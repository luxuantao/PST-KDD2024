<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Metropolis-Hastings Data Augmentation for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyeonjin</forename><surname>Park</surname></persName>
							<email>hyeonjin961030@korea.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Seunghun</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sihyeon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinyoung</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jisu</forename><surname>Jeong</surname></persName>
							<email>jisu.jeong@navercorp.com</email>
						</author>
						<author>
							<persName><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
							<email>kyungmin.kim.ml@navercorp.com</email>
						</author>
						<author>
							<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
							<email>jungwoo.ha@navercorp.com</email>
						</author>
						<author>
							<persName><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
							<email>hyunwoojkim@korea.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">†</forename><surname>Korea University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Naver</forename><surname>Clova</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Naver</forename><forename type="middle">Ai</forename><surname>Lab</surname></persName>
						</author>
						<title level="a" type="main">Metropolis-Hastings Data Augmentation for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) often suffer from weak-generalization due to sparsely labeled data despite their promising results on various graph-based tasks. Data augmentation is a prevalent remedy to improve the generalization ability of models in many domains. However, due to the non-Euclidean nature of data space and the dependencies between samples, designing effective augmentation on graphs is challenging. In this paper, we propose a novel framework Metropolis-Hastings Data Augmentation (MH-Aug) that draws augmented graphs from an explicit target distribution for semi-supervised learning. MH-Aug produces a sequence of augmented graphs from the target distribution enables flexible control of the strength and diversity of augmentation. Since the direct sampling from the complex target distribution is challenging, we adopt the Metropolis-Hastings algorithm to obtain the augmented samples. We also propose a simple and effective semi-supervised learning strategy with generated samples from MH-Aug. Our extensive experiments demonstrate that MH-Aug can generate a sequence of samples according to the target distribution to significantly improve the performance of GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b12">[1]</ref> have been widely used for representation learning on graphstructured data due to their superior performance in various applications such as node classification <ref type="bibr" target="#b13">[2]</ref><ref type="bibr" target="#b14">[3]</ref><ref type="bibr" target="#b15">[4]</ref>, link prediction <ref type="bibr" target="#b16">[5]</ref><ref type="bibr" target="#b17">[6]</ref><ref type="bibr" target="#b18">[7]</ref> and graph classification <ref type="bibr" target="#b19">[8,</ref><ref type="bibr" target="#b20">9]</ref>. They have been proven effective by achieving impressive performance for diverse datasets such as social networks <ref type="bibr" target="#b21">[10]</ref>, citation networks <ref type="bibr" target="#b15">[4]</ref>, physics <ref type="bibr" target="#b22">[11]</ref>, molecules <ref type="bibr" target="#b23">[12]</ref>, and knowledge graphs <ref type="bibr" target="#b21">[10]</ref>. However, GNNs often suffer from weakgeneralization due to their small and sparsely labeled graph datasets. One prevalent remedy to address the problem is data augmentation. Data augmentation increases the diversity of data and improves the generalization power of machine learning models trained on randomly augmented samples. It is widely used to enhance the generalization ability of models in many domains. For instance, in image recognition, advanced methods like <ref type="bibr" target="#b24">[13]</ref><ref type="bibr" target="#b25">[14]</ref><ref type="bibr" target="#b26">[15]</ref> as well as simple transformations such as random cropping, cutout, Gaussian noise, or blurring have been used to achieve competitive performance.</p><p>However, unlike image recognition, designing effective and label-preserving data augmentation for individual samples on graphs is challenging due to their non-Euclidean nature and the dependencies between data samples. In image recognition, it is straightforward to identify operations that preserve labels. For instance, human can verify that rotation, translation, and small color jittering do not change the labels in image classification. In contrast, graphs are less interpretable and it is non-trivial for even human to check whether the augmented samples belong to the original class or not. In addition, due to the dependencies between nodes and edges in a graph, it is hard to control the degree of augmentation for individual samples. For instance, a simple operation on a graph, e.g., dropping a node, may result in a completely different degree of augmentation depending on the graph structure. If a hub node is removed, the single perturbation affects a substantial amount of other nodes, which are data samples in node classification. To address these challenges, learning-based data augmentation methods for graphs have been proposed. AdaEdge <ref type="bibr" target="#b27">[16]</ref> optimizes the graph topology based on the model prediction. <ref type="bibr" target="#b28">[17]</ref> proposes GAug-M and GAug-O that generate augmented graphs via a differentiable edge predictor. GraphMix <ref type="bibr" target="#b29">[18]</ref> presents interpolation-based regularization by jointly train a fully connected network and graph neural networks. However, they require additional models for augmentation and more importantly do not explicitly guarantee that augmentation has a proper strength and diversity.</p><p>In this paper, we proposed a novel framework called Metropolis-Hastings Data Augmentation (MH-Aug) that draws augmented graphs from an 'explicit' target distribution with the desired strength and diversity for semi-supervised learning. Since the direct sampling from the complex distribution is challenging, we adopt the Metropolis-Hastings algorithm to obtain the augmented samples. Recently, the importance of leveraging unlabeled data as well as adopting advanced augmentation has emerged <ref type="bibr" target="#b30">[19,</ref><ref type="bibr" target="#b31">20]</ref>. Inspired by that, we also adopt the consistency training by utilizing the regularizers for unlabeled data. Our extensive experiments demonstrate that MH-Aug can generate a sequence of samples according to the desired distribution and be combined with the consistency training and it significantly improves the performance of graph neural networks.</p><p>Our contributions are summarized as follows:</p><p>• We proposed a novel framework Metropolis-Hastings Data Augmentation that draws augmented samples from an 'explicit' target distribution. To the best of our knowledge, this is the first work that studies data augmentation for graph-structured data from a perspective of a Markov chain Monte Carlo sampling. • We theoretically and experimentally prove that our MH-Aug generates the augmented samples according to the desired distribution with respect to the strength and diversity. • We propose a target distribution that flexibly controls the strength and diversity of augmentation. This includes an efficient way to measure the strength of augmentation reflecting the structural changes of ego-graphs (or samples in node classification). • Lastly, we propose a simple and effective semi-supervised learning strategy leveraging sequentially generated samples from our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Semi-Supervised Learning on Graphs. GNNs have been widely adopted in representation learning on graphs <ref type="bibr" target="#b13">[2]</ref><ref type="bibr" target="#b14">[3]</ref><ref type="bibr" target="#b15">[4]</ref>. However, existing works only utilize a small subset of nodes. To fully utilize a large amount of unlabeled data, recent studies for semi-supervised learning have emerged inspired by semisupervised frameworks in other domains <ref type="bibr" target="#b30">[19,</ref><ref type="bibr" target="#b31">20]</ref>. GraphMix <ref type="bibr" target="#b29">[18]</ref> is a regularization method based on semi-supervised learning by linear interpolation between two data on graphs, and SSL <ref type="bibr" target="#b32">[21]</ref> proposes self-supervised learning strategies to exploit available information from graph structure. BVAT <ref type="bibr" target="#b33">[22]</ref> promotes the smoothness of GNNs by generating virtual adversarial perturbations. Likewise, we follow semi-supervised strategy to leverage unlabeled data while considering sequentially generated samples from our augmentation.</p><p>Data Augmentation on Graphs. Data augmentation is an effective technique to improve generalization by increasing the diversity of data. It is becoming the de facto necessity for model training to employ simple data augmentation (e.g., image rotation, flipping, translation, and so on). Despite the effectiveness of data augmentation, few approaches have been explored in graph domain due to its non-Euclidean nature and dependencies between data samples. Simple approaches exist such as DropEdge <ref type="bibr" target="#b34">[23]</ref> to randomly remove a certain number of edges and AdaEdge <ref type="bibr" target="#b27">[16]</ref> to adaptively control the inter-class/intra-class edges. Similarly, a method to propagate the perturbed node features by randomly dropping on a node-based was proposed in <ref type="bibr" target="#b35">[24]</ref>. GAug <ref type="bibr" target="#b28">[17]</ref> proposes the neural edge predictors as an augmentation module. Unlike existing methods employing simple perturbation <ref type="bibr" target="#b34">[23]</ref> or extra augmentor model <ref type="bibr" target="#b28">[17,</ref><ref type="bibr" target="#b36">25]</ref>, we propose the sampling-based augmentation, where a sequence of augmented samples are drawn from the explicitly designed target distribution for augmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We present a novel data augmentation framework for graph-structured data via Metropolis-Hastings algorithm. MH-Aug is a sampling-based augmentation, where a sequence of augmented samples are drawn from the explicit target distribution that enables flexible control of strength and diversity of augmentation. The overall sampling process of MH-Aug is described in Figure <ref type="figure" target="#fig_0">1</ref>. In this section, we first summarize the basics for our framework and delineate the components of MH-Aug. Then, we outline the training procedure with proposed consistency regularizers for semi-supervised learning. Lastly, we theoretically prove the distribution of augmented samples by MH-Aug converges to the desired target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Ego-graph, G i . A graph is denoted as G = (V, E), where V and E are the sets of nodes and edges.</p><p>A k-hop ego-graph G i <ref type="bibr" target="#b37">[26]</ref> is a subgraph of G centered at node v i ∈ V, consisting of neighbors within k hops from node v i and all edges between the neighbors including v i . In other words, the k-hop ego-graph of a node v i is defined as</p><formula xml:id="formula_0">G i = (V i , E i ), V i = {u|S(u, v i ) ≤ k, u ∈ V}, E i = {(u, v)|(u, v) ∈ E and u, v ∈ V i }, where S(u, v)</formula><p>is the length of the shortest path between nodes u and v. In this paper, we do not explicitly specify k for ego-graphs since 2-hop ego-graphs are used in all experiments.</p><p>Change ratio of graph, ∆G . The change ratio of graph G to G = (V , E ) is measured by the number of added/deleted edges (or nodes) divided by the number of original edges (or nodes), i.e.,</p><formula xml:id="formula_1">∆G E = (|E −E|+|E −E |)/|E| and ∆G V = (|V −V|+|V −V |)/|V|.</formula><p>Since, in this work, we consider only subgraphs of the original input graph as augmented samples, which is similar to DropEdge <ref type="bibr" target="#b34">[23]</ref> and DropNode <ref type="bibr" target="#b35">[24]</ref>, the change ratio can be equivalently written as</p><formula xml:id="formula_2">∆G E = 1 − |E |/|E| and ∆G V = 1 − |V |/|V|. Thereby ∆G E and ∆G V are always ranged in [0, 1].</formula><p>Metropolis-Hastings (MH) algorithm. MH algorithm is a Markov chain Monte Carlo method to draw random samples from a target distribution when direct sampling is difficult <ref type="bibr" target="#b38">[27]</ref>. The algorithm comprises three components: the target distribution P , the proposal distribution Q, and the acceptance ratio A. The MH algorithm iteratively draws samples from the target distribution P being only dependent on the current sample. The MH algorithm uses a proposal distribution Q to draw a candidate sample and evaluates the acceptance ratio A to decide whether to accept or reject the candidate sample. The accepted samples by the MH algorithm follow the target distribution P . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metropolis-Hastings Data Augmentation</head><p>Our objective is to sample the augmented graph G from the target distribution P given the original graph G and can be written as G ∼ P (G ; G).</p><p>(1) Since direct sampling from the target distribution P is challenging, we propose a novel data augmentation method based on Metropolis-Hastings algorithm.</p><p>Target Distribution. We design the target distribution P to control the strength and diversity of augmentation for effective learning. The strength and diversity can be discussed from two perspectives: a full graph and ego-graphs. In our framework, the strength of augmentation is measured by the change ratio of ego-graphs, i.e., ∆G i , since most existing GNNs with k-layers learn node representations based on their k-hop ego-graphs. On the other hand, the diversity of augmentation is controlled by ∆G and ∆G i from both full graph and ego-graph perspectives. The diversity of augmentation is adaptively adjusted for each ego-graph by the standard deviation σ( i ) that is a simple linear function of the entropy i of the prediction at node v i . Given the expected strength µ E ∈ R and ego-graph level diversity σ E ( i ) ∈ R, the target distribution P w.r.t. edges is given as follows:</p><formula xml:id="formula_3">P E (G ) ∝   |V| i exp − (∆G i,(E) − µ E ) 2 2{σ E ( i )} 2   λ1 •   1 |E| |E|•∆G E   λ2 ,<label>(2)</label></formula><p>where ∆G i,(E) is the change ratio of G i w.r.t. E and λs are hyperparamters for controlling the influence of the two components. To have various full graph change ratios ∆G E , the normalization by the number of possible augmented graphs corresponding to the same change ratio,</p><formula xml:id="formula_4">|E| |E|•∆G E , is necessary.</formula><p>As the size of graph increases, without the normalization, it becomes extremely difficult to generate augmented samples with a low (or high) full graph change ratio. For more details, see Section 4.2. Similarly, The target distribution P with respect to nodes can be written as follows:</p><formula xml:id="formula_5">P V (G ) ∝   |V| i exp − (∆G i,(V) − µ V ) 2 2{σ V ( i )} 2   λ3 •   1 |V| |V|•∆G V   λ4 ,<label>(3)</label></formula><p>where ∆G i,(V) is the change ratio of G i w.r.t. the nodes, V. With combining the two distributions, the overall target distribution is defined as:</p><formula xml:id="formula_6">P (G ) = P E (G ) • P V (G ).<label>(4)</label></formula><p>In our experiment, unlike the change ratio of the full graph ∆G E (and ∆G V ), we define the ego-graph change ratio ∆G i,(E) (and ∆G i,(V) ) with the change of the number of received messages from k-hop ego-graphs. Figure <ref type="figure">2</ref> illustrates the calculation of ∆G i,(E) regarding two different cases: (b) dropping distant (2-hop) edges and (c) dropping near (1-hop) edges. In this definition, even if the number of dropped edges is the same, dropping edges connecting nodes closer to the center node v i leads to a larger ∆G i,(E) than the case of distant nodes (0.6 &gt; 0.13), which can be regarded as a much stronger augmentation. It indicates that the amount of received messages depends on not only the number of removed edges but also which edges are dropped. This structural property can only be properly handled from ego-graph perspective. In practice, this definition allows time and memory efficient implementation using matrix multiplications as</p><formula xml:id="formula_7">∆G i,(E) = 1 − ( Ã k 1) i ( Ãk 1) i ,<label>and</label></formula><formula xml:id="formula_8">∆G i,(V) = 1 − ( Ãk m) i ( Ãk 1) i ,<label>(5)</label></formula><p>where Ã and Ã are adjacency matrices of the original graph G and the current graph G , where both graphs include a self-connection for every node, 1 ∈ R |V| is a vector of ones, and m ∈ R |V| is a mask vector for DropNode.</p><p>Proposal Distribution. For efficient sampling and a theoretical guarantee of convergence to the target distribution, a proposal distribution is crucial. A proposal distribution Q(G |G (t) ) suggests a candidate augmented sample G , given the current sample G (t) . To draw diversely augmented graphs with various edge/node change ratios ∆G E and ∆G V , a candidate augmented sample is generated by three steps: 1) change ratio sampling, 2) graph modification and 3) merging. We first independently sample change ratios ∆G (•) for edges and nodes from Gaussian distributions truncated to the range</p><formula xml:id="formula_9">[0, 1] given mean ∆G (t) (•)</formula><p>and standard deviation σ ∆,(•) . Then, we modify the original graph G to generate augmented samples G E and G V , which can be viewed as a uniform sampling from all possible augmented graphs with ∆G E and ∆G V respectively. Finally, the two graphs G E and G V are merged to construct the candidate augmented sample G . Formally, the proposal distribution is given by:</p><formula xml:id="formula_10">Q(G |G (t) ) ∝ φ(ξ E ) Φ(β E ) − Φ(α E ) • φ(ξ V ) Φ(β V ) − Φ(α V ) • 1 |E| |E|•∆G E • 1 |V| |V|•∆G V ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_11">α (•) = a−∆G (t) (•) σ∆ , β (•) = b−∆G (t) (•) σ∆ , ξ (•) = ∆G (•) −∆G (t) (•) σ∆ , φ(x) = 1 √ 2π exp(− 1 2 x 2 )</formula><p>as the probability density function of the standard normal distribution and Φ</p><formula xml:id="formula_12">(x) = 1 2 (1 + erf ( x √<label>2</label></formula><p>)) as its cumulative distribution function. a and b represent the extremes of Gaussian distribution. Since the change ratio should be ranged in [0, 1], a is 0 and b is 1. In (6), the first and second terms denote the likelihood of the change ratios ∆G E and ∆G V given ∆G (t) . The third and fourth terms are for the probability of a sample with ∆G E and ∆G V .</p><p>Acceptance Ratio. Starting with the original graph G, MH-Aug draws the candidate graph G from the proposal distribution Q. Then, with an acceptance ratio A, MH-Aug decides whether to accept or reject the candidate G . A is given as:</p><formula xml:id="formula_13">A = min 1, P (G )Q(G (t) |G ) P (G (t) )Q(G |G (t) ) .<label>(7)</label></formula><p>The computation of A with target distribution in (4) and proposal distribution in ( <ref type="formula" target="#formula_10">6</ref>) is described in the supplement. MH-Aug generates a sequence of augmented graphs {G (t) } 0≤t≤T , where T is the number of accepted samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Consistency Training with MH-Aug</head><p>Inspired by recent works <ref type="bibr" target="#b30">[19,</ref><ref type="bibr" target="#b39">28,</ref><ref type="bibr" target="#b31">20]</ref> that show the importance of advanced augmentation methods in leveraging unlabeled data, we demonstrate the effectiveness of our augmentation method in both supervised and semi-supervised settings. Similar to consistency regularization <ref type="bibr" target="#b31">[20]</ref>, we propose a simple training strategy with the following regularizers:</p><formula xml:id="formula_14">L u = 1 |V| |V| i D KL f (G (t) i ; θ) || f (G (t+1) i ; θ) , and L h = 1 |V| |V| i [−f (G i ; θ) log(f (G i ; θ))] ,<label>(8)</label></formula><p>where . L h penalizes unconfident predictions and sharpens predictions. The two regularizers can be applied to both labeled and unlabeled nodes in the node classification task. With the two regularizers and the standard cross-entropy loss L s for supervised samples, the overall loss for semi-supervised learning is given as</p><formula xml:id="formula_15">D KL (• || •) is the Kullback-Leibler divergence, f<label>(</label></formula><formula xml:id="formula_16">L = L s + γ 1 L u + γ 2 L h . (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>Our framework is outlined in Algorithm 1. Starting from original graph G with 0 change ∆G , MH-Aug generates new augmented graph data G with the change of ∆G . It decides whether to accept or reject the candidate G with acceptance score A. GNN models are trained with the accepted augmented data with our loss in <ref type="bibr" target="#b20">(9)</ref>. Then, the process is repeated until the model converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Metropolis-Hastings Data Augmentation (MH-Aug) Framework</head><p>Input:</p><formula xml:id="formula_18">target distribution P , proposal distribution Q, original graph G Output: network parameter θ Initialize t ← 0, G (0) ← G while not convergence do Draw G from Q(G |G (t) ) Q in Eq.(6) Draw u from U nif orm(0, 1) if u ≤ A then</formula><p>A in Eq.( <ref type="formula" target="#formula_13">7</ref>)</p><formula xml:id="formula_19">G (t+1) ← G Update θ with L(G, G (t) , G (t+1) ; θ)</formula><p>L in Eq.( <ref type="formula" target="#formula_16">9</ref>) t ← t + 1 end if end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical Analysis</head><p>The goal of the Metropolis-Hastings algorithm is to generate a sequence of samples according to a desired target distribution P . To accomplish this, the Metropolis-Hastings algorithm uses a Markov process, which asymptotically reaches a unique stationary distribution π(x) such that π(x) = P (x) <ref type="bibr" target="#b40">[29]</ref>. Here, we show that Markov chain of MH-Aug, which has a sequence of augmented graph as states, converges to the unique and stationary target distribution P (G ) defined in (4). Lemma 3.1. Let the sequence of augmented graphs {G (t) } 0≤t≤T be the Markov chain produced by MH-Aug. If we define the acceptance ratio A with target distribution P in (4) and proposal distribution Q in (6), the sequence converges to a unique stationary target distribution P .</p><p>This can be drawn from the Convergence theorem of Markov chain <ref type="bibr" target="#b41">[30]</ref>. The proof is in the supplement. By Lemma 3.1, we theoretically show augmented samples of MH-Aug converges to our desired target distribution. Our toy examples show a sequence of augmented graphs actually converges well to the target distribution (see Section 4.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we demonstrate the effectiveness of MH-Aug on various benchmark datasets. We start with describing datasets, baselines, and implementation details for the experiments. Next, we evaluate our framework for node classification in Section 4.1 and we offer qualitative analyses in Section 4.2 on three parts: effectiveness of ego-graph perspective for desired target distribution P , necessity of normalization term in P , and whether generated samples from MH-Aug converge to P .</p><p>Datasets. We evaluate our method on five benchmark datasets in three categories: (1) Citation networks: CORA and CITESEER <ref type="bibr" target="#b42">[31]</ref>, (2) Amazon product networks: Computers and Photo <ref type="bibr" target="#b43">[32]</ref>, and (3) Coauthor Networks: CS <ref type="bibr" target="#b43">[32]</ref>. We follow the standard data split protocol in the transductive settings for node classification, e.g., <ref type="bibr" target="#b15">[4]</ref> for CORA and CITESEER and <ref type="bibr" target="#b43">[32]</ref> for the rest.</p><p>Baselines. As backbone models to validate MH-Aug, we adopt three standard graph neural networks: GCN <ref type="bibr" target="#b15">[4]</ref>, GraphSAGE <ref type="bibr" target="#b12">[1]</ref>, and GAT <ref type="bibr" target="#b14">[3]</ref>. We compare our method with vanilla models (without Table <ref type="table">1</ref>: Node classification results. Mean accuracy and standard deviation from 10 repetitions are reported. We compare our methods with baselines of two categories: 1) supervised learning with augmentation (e.g., DropEdge and AdaEdge), which are comparable to our MH-Aug (w/o Reg) and 2) semi-supervised learning (e.g., GAug, SSL, BVAT, UDA* and GraphMix) that are comparable to our MH-Aug (w/ Reg). For each dataset and baseGNN the highest score is marked in bold. augmentation), augmentation-based supervised learning (DropEdge <ref type="bibr" target="#b34">[23]</ref>, AdaEdge <ref type="bibr" target="#b27">[16]</ref>), and semisupervised learning framework (GAug <ref type="bibr" target="#b28">[17]</ref>, SSL <ref type="bibr" target="#b32">[21]</ref>, BVAT <ref type="bibr" target="#b33">[22]</ref>, UDA* <ref type="bibr" target="#b30">[19]</ref>, GraphMix <ref type="bibr" target="#b29">[18]</ref>). In the case of DropEdge <ref type="bibr" target="#b34">[23]</ref> and AdaEdge <ref type="bibr" target="#b27">[16]</ref>, they use only cross-entropy loss (supervised setting) while the rest of models employs extra loss functions for regularization (semi-supervised setting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on Node Classification</head><p>Table <ref type="table">1</ref> shows the experimental results on node classification with five datasets compared to baseline models. We implemented all the baselines and conducted experiments for fair comparison except for the case where the performance (marked with † ) is available in the original papers <ref type="bibr" target="#b27">[16,</ref><ref type="bibr" target="#b28">17,</ref><ref type="bibr" target="#b32">21,</ref><ref type="bibr" target="#b33">22,</ref><ref type="bibr" target="#b29">18]</ref>. Also, we denote out-of-memory as OOM. MH-Aug (w/o Reg) means training the model only with the labeled data and cross-entropy loss L s whereas MH-Aug (w/ Reg) means using extra regularization losses to explicitly utilize the unlabeled data. Our full framework MH-Aug (w/ Reg), which is trained in the semi-supervised setting, consistently achieves the best performance in all datasets and the improvement against the vanilla models is 3.16% on average. In particular, we observe that MH-Aug improves the performance by 4.92% compared to the vanilla GraphSAGE on CORA. In addition, MH-Aug provides an 4.16% gain on CITESEER on average over all models (i.e., vanilla GCN, GraphSAGE and GAT).As an ablation study, we conduct experiments with MH-Aug (w/o Reg), our framework trained in the supervised setting. Table <ref type="table">1</ref> shows that MH-Aug (w/o Reg) achieves 1. </p><formula xml:id="formula_20">|V| i exp − (∆G i,(E) −µ E ) 2 2{σ E } 2</formula><p>of (2). To evaluate the effect of µ E and σ E w.r.t. egograph, we fix the full-graph change ratio ∆G E and observe the expected value of ∆G i,(E) over all possible nodes in the ego-graph, E[∆G i,(E) ]. Thus, the number of dropped edges is identical for all the augmented graphs in Figure <ref type="figure" target="#fig_2">3</ref>. It demonstrates that even if the number of dropped edges is the same, one can generate diverse samples by controlling µ E and σ E . When µ E , which controls the expected augmentation strength, is large, e.g., µ E = 1, σ E = 0.05, more important edges (e.g., edges acting as bridges between hub nodes) tend to be dropped. This observation exactly matches to our design in Section 3.2, which considers dropping edges near to the center as a strong augmentation. In addition, the mini map in the fourth cell of Figure <ref type="figure" target="#fig_2">3</ref> indicates if σ E increases, the edge-drop probability of the all edges becomes uniform, i.e. MH-Aug subsumes DropEdge as a special case. In sum, the ego-graph perspective enables the explicit control of augmentation strength and diversity to make an advanced augmentation.   , is crucial to generating ego-graphs with the desired ego-graph change ratio µ E when the number of edges is huge. We demonstrate it with a small but fully connected graph to apply MH-Aug. Figure <ref type="figure" target="#fig_4">4</ref> displays the distribution of the empirical mean of ∆G i,(E) from the augmented graph sampled from P with two different µ E = 0.1 and µ E = 0.9. With normalization (Figure <ref type="figure" target="#fig_4">4</ref>(a)), the sample mean of ∆G i,(E) of both sampling results are near to the µ E values. However, without normalization (Figure <ref type="figure" target="#fig_4">4</ref>(b)), the empirical mean of ∆G i,(E) remains the same due to overwhelmingly many possible subgraphs with a certain full graph change ratio ∆G E , e.g., 100×100 5000 for a fully connected graph with 100 nodes. But this does not mean that our MH-Aug fails to converges to the target distribution. It merely converges to the undesirable target distribution.</p><p>Convergence to Target Distribution. In Section. 3.4, we theoretically show that the distribution of samples generated by MH-Aug converges to our desired target distribution P . Now, we conduct the experiment to examine whether a sequence of augmented graphs experimentally follows the target distribution. We observe the behavior of MH-Aug with a simple toy example, i.e., a grid graph with 100 nodes. For simplicity, we only consider the change of edges and the target distribution P E in (2). In 5(b), red line denotes the probability of each edge obtained by calculating P with (2). Blue bars represent the distribution of augmented graphs generated by MH-Aug. It shows MH-Aug generates augmented graphs following the target distribution. In 5(a), we visualize drop probability on graph. Since we set µ E small, drop probability of center edges is higher than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a novel semi-supervised strategy with Metropolis-Hastings algorithm based augmentation method. This is the first work to impose data augmentation on graph-structured data from a perspective of a Markov chain Monte Carlo sampling. We theoretically and experimentally show the convergence of augmented samples to target distribution and demonstrate its consistent performance improvement over baselines across five benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sampling process of MH-Aug. MH-Aug produces augmented samples in two steps. First, it draws a candidate graph G from a proposal distribution Q (green). Then, it decides whether to accept or reject the candidate by the acceptance ratio A calculated by P (blue) and Q. The left box shows the details of sampling a candidate graph G from the proposal distribution Q(G |G (t) ) given a current sample G (t) : (1) Change Ratio Sampling draws the change ratios ∆G E and ∆G V of a candidate graph w.r.t. edges and nodes from the Gaussian distributions truncated to the range [0, 1], (2) Graph Modification generates G E and G V by applying the change ratio to the original graph G, and (3) Candidate augmented graph G is constructed by merging two augmented graphs G E and G V .</figDesc><graphic url="image-3.png" coords="3,452.42,33.30,85.78,203.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>15 Figure 2 :</head><label>152</label><figDesc>Figure 2: Calculation of ∆G i,(E) . (a) displays the message propagation of v i on ego-graph G i in order. The received message of v i is 15 in the original 2-hop G i . In case of dropping distant (2-hop) edges (b), the received message of v i is 13 and ∆G i,(E) becomes 0.13. In case of dropping same number of near (1-hop) edges (c), the received message of v i is 6 and ∆G i,(E) becomes 0.6.</figDesc><graphic url="image-25.png" coords="4,120.30,81.92,57.61,56.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Diverse G sampled by MH-Aug. The first cell is the original graph G extracted from CORA. With the fixed full graph change ratio ∆G E , augmented graphs with different µ E and σ E are generated by MH-Aug. All graphs above are 3-hop ego-graphs with a center node marked as yellow. Nodes and edges which are not in the ego-graph after augmentation are blurred. Mini-maps at the upper right corner is the edge-drop probability, where blue means higher probability and red means low probability to drop the edge. By explicitly controlling the strength and diversity from an ego-graph perspective, MH-Aug generates diverse augmentations.</figDesc><graphic url="image-32.png" coords="8,108.00,-11.13,395.99,296.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>47% improvement on average compared to vanilla models. MH-Aug (w/o Reg) provides considerable gain over all dataset and model. More specifically, it provides 3.25% performance improvement compared to the vanilla GAT model on Computers. In addition, MH-Aug (w/o Reg) beats DropEdge for all settings and mostly beats AdaEdge that optimizes the graph topology based on the model predictions. It is worth noting that even though MH-Aug (w/o Reg) does not explicitly utilize unlabeled data during training, MH-Aug (w/o Reg) achieves competitive performance compared to other semi-supervised methods, especially in the following cases: GCN on CORA (83.55%); GraphSAGE on CITESEER (72.12%); and GAT on CORA (83.49%), and Photo (90.23%). This demonstrates the effectiveness of our sampling-based augmentation. More discussion on ablation study is in the supplement.4.2 AnalysisEffectiveness of Ego-graph Perspective. To validate the effectiveness of ego-graph perspective augmentation, we qualitatively analyze augmented samples by MH-Aug on real data with various settings as shown in Figure3. An original sample (first column) is a 3-hop ego-graph from CORA. Augmented samples are generated from G in three settings: (µ E , σ E ) = (0,0.05), (µ E , σ E ) = (1,0.05) and (µ E , σ E ) = (0,1000). The mini maps at the upper right corner shows the edge-drop probability, calculated from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The effect of normalization term. The two plots above show the distribution of augmented graphs w.r.t. E[∆G i,(E) ]. We conduct the toy example with the target distribution (a) with normalization (blue) and (b) without normalization (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Drop probability of each edges (b) Visualization of drop probability on grid graphs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergence to the target distribution. To verify the convergence of MH-Aug, we simulate the sampling procedure of MH-Aug on a grid graph. (a) is the result of target distribution P given µ E = 0. We highlight the edges according to drop probability. (b) shows that the samples drawn from MH-Aug (blue) follow the target distribution P (red) that we calculate with (2). Necessity of Normalization Term. As mentioned in Section 3.2, the normalization in the target distribution P by the number of possible augmented graphs corresponding to the same change ratio, |E| |E|•∆G E</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was partly supported by NAVER Corp., National Supercomputing Center with supercomputing resources including technical support (KSC-2021-CRE-0181) and ICT Creative Consilience program (IITP-2021-2020-0-01819) supervised by the IITP.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">-M</forename><surname>Gaug</surname></persName>
		</author>
		<idno>83.60±0.50 † 73.30±1.10 † OOM 89.04±1.18 OOM SSL [21] 83.80±0.73 † 72.95±0.62 † - - - BVAT [22] 83.60±0.50 † 74.00±0.60 † 80.07±2.41 88.46±2.25 92.21±0.37</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Graphmix</surname></persName>
		</author>
		<idno>18] 83.90±0.57 † 74.70±0.59 † 80.72±1.16 89.05±1.01 91.83±0.51 † MH-Aug (w/ Reg) 85.16±0.35 75.49±0.29 82.80±2.08 90.87±1.49 92.60±0.43</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">GraphSAGE Vanilla</title>
		<idno>79.78±0.74 71.09±0.59 79.59±1.84 89.10±1.60 91.35±1.00</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adaedge</forename></persName>
		</author>
		<idno>16] 80.20±1.20 † 69.40±0.80 † 80.43±1.30 90.57±0.70 90.30±0.40 † MH-Aug (w/o Reg) 82.61±0.66 72.12±0.99 81.74±2.52 90.37±1.50 92.27±0.49</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">-M</forename><surname>Gaug</surname></persName>
		</author>
		<idno>83.20±0.40 † 71.20±0.40 † 79.84±1.99 88.72±0.97 OOM GAug-O [17] 82.00±0.50 † 72.70±0.70 † OOM 88.16±2.70 OOM BVAT [22] 83.12±0.64 72.23±0.46 78.72±2.73 89.40±1.79 92.63±0.48</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Graphmix</surname></persName>
		</author>
		<idno>18] 82.28±0.55 69.62±0.36 81.33±1.46 88.46±1.36 89.29±0.45</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mh-Aug</surname></persName>
		</author>
		<idno>84.70±0.39 75.55±0.44 83.62±2.60 92.19±1.37 93.61±0.58</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<idno>82.23±0.46 71.37±0.93 78.47±1.86 87.80±1.36 90.90±0.31</idno>
		<title level="m">GAT Vanilla</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adaedge</forename></persName>
		</author>
		<idno>16] 77.90±2.00 † 69.10±0.80 † 77.52±1.72 88.92±0.87 86.60±0.16 † MH-Aug (w/o Reg) 83.49±0.69 72.81±0.98 81.72±1.66 90.23±0.97 91.40±0.39</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">-M</forename><surname>Gaug</surname></persName>
		</author>
		<idno>82.20±0.80 † 71.60±1.10 † OOM 86.45±1.52 OOM SSL [21] 83.70±0.61 † 72.73±0.72 † - - - UDA * [19] 83.71±0.48 73.24±0.48 82.42±2.95 89.79±1.36 91.78±0.23</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Graphmix</surname></persName>
		</author>
		<idno>18] 83.32±0.18 † 73.08±0.23 † - - - MH-Aug (w/ Reg) 84.95±0.40 75.53±0.32 83.25±1.88 90.61±1.34 92.08±0.58</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">UDA* denotes our extension of UDA in the graph domain. † denotes the results reported in the original paper</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS W</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR W</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graphmix: Improved training of gnns for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Qikui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML W</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust graph representation learning via neural sparsification</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Transfer learning of graph neural networks with ego-graph information maximization</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName><forename type="first">Hastings</forename><surname>Keith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Monte Carlo statistical methods</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convergence theorem for finite markov chains</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. REU</title>
				<meeting>REU</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS W</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
