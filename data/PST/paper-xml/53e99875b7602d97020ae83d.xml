<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linking Online News and Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manos</forename><surname>Tsagkias</surname></persName>
							<email>e.tsagkias@uva.nl</email>
						</author>
						<author>
							<persName><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
							<email>w.weerkamp@uva.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Maarten de Rijke ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ISLA</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linking Online News and Social Media</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5ADB664B80BF72EFC5BB149F88CD63E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval Algorithms</term>
					<term>Experimentation Linking</term>
					<term>online news</term>
					<term>social media</term>
					<term>user generated content</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Much of what is discussed in social media is inspired by events in the news and, vice versa, social media provide us with a handle on the impact of news events. We address the following linking task: given a news article, find social media utterances that implicitly reference it. We follow a three-step approach: we derive multiple query models from a given source news article, which are then used to retrieve utterances from a target social media index, resulting in multiple ranked lists that we then merge using data fusion techniques. Query models are created by exploiting the structure of the source article and by using explicitly linked social media utterances that discuss the source article. To combat query drift resulting from the large volume of text, either in the source news article itself or in social media utterances explicitly linked to it, we introduce a graph-based method for selecting discriminative terms.</p><p>For our experimental evaluation, we use data from Twitter, Digg, Delicious, the New York Times Community, Wikipedia, and the blogosphere to generate query models. We show that different query models, based on different data sources, provide complementary information and manage to retrieve different social media utterances from our target index. As a consequence, data fusion methods manage to significantly boost retrieval performance over individual approaches. Our graph-based term selection method is shown to help improve both effectiveness and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>A symbiotic relation has emerged between online news and social media such as blogs, micro-blogs, social bookmarking sites, news comments and Wikipedia. Much of what is discussed in social media is inspired by the news (e.g., 85% of Twitter statuses are news-related <ref type="bibr" target="#b22">[22]</ref>) and, vice versa, social media provide us with a handle on the impact of news events <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b46">46]</ref>. Understanding the relationship between news and social media has become an area of significant research interest. A key ingredient is to discover and establish links between individual news articles and the social media that discuss them.</p><p>Social media utterances (such as blog posts, tweets, diggs, etc) may be linked explicitly or implicitly to a news article. In explicitly linked utterances there is a hyperlink pointing to the article; automatic discovery of such utterances is trivial. In implicitly linked utterances, however, there is no hyperlink to the source articlethe utterance is not merely about the same topic as the source news article but it directly discusses the article's content. Consider an utterance discussing the FIFA World Cup 2010 final, expressing the utterance writer's opinion on the match. This is not considered an implicitly linked utterance; would this utterance criticize the match report given in a news article, however, then it would be an implicitly linked utterance for this news article.</p><p>The task on which we focus in this paper is discovering implicitly linked social media utterances: For a given news article we discover social media utterances that discuss the article. Both the notion of relevance (detailed above) and the fact that, to address the task, one needs to cross from edited content to the unedited and strongly subjective language usage of user generated content, make the task challenging. To quantify the potential "vocabulary gap" <ref type="bibr">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">11]</ref> we conducted an exploratory experiment. We considered a set of news articles plus a range of social media platforms; for each news article we computed the (symmetric) Kullback-Leibler (KL) divergence between the article and the social media utterances explicitly linked to it (grouped by platform) as a way of approximating the difference in vocabularies; see Fig. <ref type="figure" target="#fig_0">1</ref> for a visualization. We observe (varying levels of) difference in vocabulary between news and social media. The vocabularies of blog posts, Digg and Twitter seem relatively close to that of the news articles-anecdotal evidence suggests that this is due to these sources copying parts of the original news article. Moreover, the social media platforms show varying degrees of difference between their vocabularies.</p><p>When attempting to link social media utterances to a given news article, the main question is: how do we represent the article as a query? Typically, the article itself has a fielded structure (title, lead, body, headers, etc) that can be exploited <ref type="bibr" target="#b2">[2,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b23">23]</ref>. Which of these is helpful in identifying implicitly linked social media utterances? Alternatively, one can try to identify a selection of "representative" terms from the article <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b17">17]</ref>. Given the noisy or unedited character of many social media utterances, the selection procedure needs to be very robust. There is a third alternative, based on the observation that there may be many social media utterances that explicitly reference a given news article. For a sample of news articles (described in §5.3), Table <ref type="table" target="#tab_5">6</ref> displays the number of articles that are explicitly referenced by the six social media platforms considered above. What if we used representations of a news article generated from social media utterances that explicitly link to it? Given these options, we approach the task of discovering implicitly linked social media utterances for a news article as a data fusion problem. We generate multiple query models for an article, based on three strategies: (i) its internal document structure, (ii) explicitly linked social media utterances, and (iii) term selection strategies. This yields ranked lists per strategy and these ranked lists are then merged using data-fusion methods. The research questions we aim to answer are the following:</p><p>1. Does the internal document structure of a news article help to retrieve implicitly linked social media utterances? 2. Do query models derived from social media models outperform models based on internal document structure? 3. Is implicilt link discovery effectiveness affected by using reduced query models that only use a limited selection of words? 4. How can ranked lists from individual strategies be fused to improve performance? 5. Does it help effectiveness when making the fusion strategy dependent on the news article for which we are seeking implicitly linked utterances?</p><p>When talking about effectiveness of a method, we consider the performance of the method in terms of recall or precision-oriented metrics. Efficiency on the other hand deals with a method's performance in terms of speed.</p><p>Our main contributions are the following: (a) we introduce the task of discovering social media utterances implicitly linked to a news article; (b) we offer a comparison of query models derived from (i) the document itself and (ii) auxiliary social media platforms in terms of the effectiveness of finding implicitly linked utterances; (c) we propose a robust graph-based term selection method, apply it to document and social media models, and compare the effectiveness and efficiency of these reduced models to the original models; and (d) we compare three types of late data fusion methods for combining ranked lists: (i) without training, (ii) query independent training, and (iii) query dependent training.</p><p>The rest of the paper is organized as follows: We report on related work in §2, we present our approach in §3, our models are presented in §4, our experimental setup is described in §5, we report on results and discuss our findings in §6, and conclude in §7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>News and social media.</head><p>Much of what is discussed in social media is inspired by the news <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b45">45]</ref>. Even search in social media is to an important degree influenced by news events <ref type="bibr" target="#b36">[36]</ref>. As a consequence, the text analysis and retrieval communities have begun to examine the relationship between the twonews and social media-from a range of angles. Recent emerging interest concerns work on predicting the response to a news article in social media <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b46">46]</ref>.</p><p>We consider a task that is different from the ones mentioned so far: discovering implicitly linked social media utterances that discuss a news article. Link identification has been used to track short information cascades through the blogosphere <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24]</ref>. Of particular relevance to us, though, is the work by Ikeda et al. <ref type="bibr" target="#b17">[17]</ref> who use similarity between term vectors that represent news articles and blog posts to decide on the existence of links between the two. On top of that, Takama et al. <ref type="bibr" target="#b44">[44]</ref> use the difference between publication times of news articles and blog posts to decide on the existence of a link. Gamon et al. <ref type="bibr" target="#b12">[12]</ref> use a graph-based approach to create context for news articles out of blog posts. We are interested in discovering utterances that implicitly link to a specific news article and not to the news event(s) that the article is about.</p><p>Blog post retrieval. Viewed abstractly, the task we considerdiscovering social media utterances that are (implicitly) linked to a given news article-is similar to the (topical) blog post finding that has been examined at the TREC Blog track between 2006 and 2009 <ref type="bibr" target="#b38">[38]</ref>. There are important differences, though, that motivate approaches to the task of discovering social media utterances that are technically and conceptually different from existing approaches to the blog post finding task. For a start, the information need, and therefore the notion of relevance is different: instead of posts that discuss a topic, we seek to identify utterances that reference a specific article-not a different article that is possibly about the same topic. Among other things, this leads to a dramatically different technical setting, with elaborate information needs (the source article) as opposed to the typical two or three word queries or two or three line narratives. Moreover, relevant utterances are necessarily published after the source news article and tend to be published reasonably shortly after the source article <ref type="bibr" target="#b25">[25]</ref>. Conceptually, we are crossing genres, from edited news (on the query side) to user generated content (on the result side).</p><p>One of the themes that has emerged around blog (post) retrieval is the use of non-content features. Timeliness is one such feature that is particularly relevant for our setting. Another one concerns quality indicators; we use the credibility indicators in <ref type="bibr" target="#b47">[47]</ref>.</p><p>Combining multiple representations. From work on the topical blog post retrieval task mentioned above, we borrow the insight that social media retrieval benefits from elaborate query modeling <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b48">48]</ref>. One pertinent type of query representation considered in the literature exploits query structure-we use the structure of the source article, "our query," to obtain different ways of representing the source news article for which we are seeking to identify linked social media utterances. Another pertinent type of query representation known concerns the use of "external corpora," where terms are sampled from documents that are not in the target collection from which items need to be retrieved; a prominent example of such an external corpus in the setting of blog retrieval is Wikipedia.</p><p>The idea of using multiple representations of a query or its underlying information need has a long history; Belkin et al. <ref type="bibr" target="#b7">[7]</ref> summa-rize work on the theme that builds off the early TREC collections. More broadly, combinations of approaches-either at the level of queries, sources, or result rankings-have been met with different degrees of success. Snoek et al. <ref type="bibr" target="#b42">[42]</ref> identify two types of combination approaches depending on whether the combination occurs at the query level (early fusion) or at the result level (late fusion). In the setting of blog post retrieval, Weerkamp et al. <ref type="bibr" target="#b48">[48]</ref> show that the use of multiple query representations (in the form of complex query models) helps improve blog post retrieval effectiveness. Interestingly, Beitzel et al. <ref type="bibr" target="#b6">[6]</ref> find that combinations of highly effective systems hurt performance as compared to the performance of the individual approaches. McCabe et al. <ref type="bibr" target="#b28">[28]</ref> find that combinations of a poorly performing approach with a good system, using weights where the good system is weighted highly, leads to performance gains over the good system. We apply standard (late) data fusion approaches <ref type="bibr" target="#b41">[41]</ref>, re-examine insights on data fusion from the literature and shed new light on the effectiveness of combinations in the context of our finding linked utterances task; see §4.3, 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPROACH</head><p>Starting from a source news article, we need to identify, in a target index, utterances that reference the source article. We view this task as a data fusion problem: starting from the source article, we derive and apply query models to generate multiple queries, which are then used to retrieve utterances from the target index, resulting in multiple ranked lists that we then merge into a single result list; see Fig. <ref type="figure" target="#fig_3">2</ref>. Let us motivate these steps.   Most of our attention in this paper will be devoted to the query modeling step. Importantly, in the process of identifying social media utterances that reference a given source news article, we are crossing genres, from news to social media. When crossing genres, the vocabulary gap between source article ("the query") and target utterances ("the documents") is wider than within genres, especially when one of the genres involved is a social media genre <ref type="bibr" target="#b48">[48]</ref>. To bridge the gap, we follow multiple alternative routes: starting from the source article, we consider multiple query modeling strategies, i.e., ways of arriving at a query to be fired against the target index. First, we consider different representations of the source news article itself. It is a semi-structured document that features elements such as title, lead and body. Derived representations such as the named entities mentioned in the article and quotations from interviews are also used to represent the article and generate queries. Second, to help generate queries that represent the source article we also use auxiliary social media. Intuitively, to bridge between the language usage of the source article and that of the utterances in the target index, we can exploit social media where the two types of language usage are, to some degree, mixed. E.g., a Digg story usually consists of the news article title and summary (edited content)</p><p>and the user comments (unedited content), tweets mix the article title (edited ) with the twitterer's opinion/comment (unedited). The textual representations from which queries are derived may be quite long as compared to, for example, article titles. E.g., when focusing on the source news article, the entire title and body of the article can be used as a query <ref type="bibr" target="#b31">[31]</ref>; such long queries, however, are costly to process and may introduce noise and cause topic drift. For this reason, we identify and extract terms that are discriminative and characteristic of language usage pertinent to the source article (or auxiliary social media) and use these to derive a query.</p><p>In the retrieval step, we submit queries representing the source news article to an index of social media utterances, and retrieve ranked lists for each of these queries.</p><p>In the fusion step, we use late data fusion methods <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> to merge results lists produced by alternative query modeling methods. For the methods that support weighted fusion, we investigate two approaches for weight optimization: query independent and query dependent. In the former approach, the system learns weights for each query model from a training set so a given metric is maximized, and then these weights are used for fusing ranked lists in response to future articles. In the latter approach, weights are learned per source article ("query") so the given metric is maximized for an article-specific training ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHODS</head><p>We describe the methods used in addressing the three steps identified in the approach outlined in §3: retrieving social media utterances, query modeling and data fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Retrieval model</head><p>For the retrieval step, we use a language modeling approach. We compute the likelihood of generating a news article a from a language model estimated from an utterance u:</p><formula xml:id="formula_0">P lm (a|u) = Q w∈a P (w|u) n(w,a) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where w is a query term in a, n(w, a) the term frequency of w in a, and P (w|u) the probability of w estimated using Dirichlet smoothing:</p><formula xml:id="formula_2">P (w|u) = n(w, u) + μP (w) |u| + μ , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where μ is the smoothing parameter, |u| is the utterance length in words, and P (w) is the probability of w in the collection.</p><p>We impose two constraints on our content-based model expressed in Eq. 1. The first is on the publication date of utterances potentially discussing the source news article. The second is on the "quality" of utterances being retrieved. Both are modeled in a probabilistic fashion so they can be incorporated in our content-based model.</p><p>As to the first constraint, we want to favor utterances published close to the source news article, mainly due to the volatility of the news; most social media utterances are generated around the news article publication date <ref type="bibr" target="#b25">[25]</ref>. Given a date range t of length |t| in days, an utterance can or cannot appear in t, therefore:</p><formula xml:id="formula_4">P date (u|t) = ( 1 n(u,t) , if u occurs in t 0, otherwise,<label>(3)</label></formula><p>where r is a time unit in t, n(u, •) is the number of utterances occurring in r or in t. We want to avoid discarding potentially relevant utterances that occur outside t, while still favoring those published in t. Therefore, we follow the language modeling paradigm and </p><formula xml:id="formula_5">(u) = log(n(r, u)) p emoticons(u) = 1 -n(e, u) • |u| -1 p post_length (u) = log(|u|) p pronouns(u) = 1 -n(o, u) • |u| -1 p shouting (u) = 1 -n(a, u) • |u| -1 p spelling (u) = 1 -n(m, u) • |u| -1</formula><p>derive an estimate for P date (u|t) based on Dirichlet smoothing:</p><formula xml:id="formula_6">Pdate (u|t) = 1 + μP (u) n(u, t) + μ , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where μ is a smoothing parameter as in Eq. 2, and</p><formula xml:id="formula_8">P (u) = 1/n(u)</formula><p>is the a priori probability of an utterance to occur anytime and n(u) is the total number of utterances in the collection.</p><p>Our second refinement of the retrieval model aims to account for adversarial social media utterances and for utterances that do not provide informative context for the article. We incorporate the credibility factors introduced in <ref type="bibr" target="#b47">[47]</ref> as quality indicators. Specifically, we implement the following topic independent factors on the level of utterances: comments, emoticons, post length, pronouns, shouting, and spelling; Table <ref type="table" target="#tab_0">1</ref> shows the model for each factor. All factors are given equal importance and are put together for the estimation of a global credibility prior probability P cred (u) for an utterance u:</p><formula xml:id="formula_9">P cred (u) = 1 |F | P f ∈F p f (u),<label>(5)</label></formula><p>where F = {comments, emoticons, post_length, pronouns, shouting, spelling}.</p><p>Finally, the content-based, recency and credibility models are combined through their geometric mean in one score for ranking an utterance u given a source news article a and a date range t:</p><formula xml:id="formula_10">Score(u, a, t) = 3 p P lm (a|u) • P date (a|t) • P cred (u) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query modeling</head><p>Most of our methodological contributions concern query modeling: building a representation of news article a to be used for retrieval (see Eq. 1). We explore three families of query models, for which we consider (i) the source news article itself as a "generator" of query models, (ii) social media as such a generator, and (iii) "reducing" the sources from which we generate query models to single out target terms.</p><p>Exploiting the source article. Obviously, the source news article itself is an important source of information for creating query models that represent it. News articles typically feature a title, lead and body as structural elements. The title is indicative of the article's main topic and summarizes the article. The lead consists of a few sentences, gives insight on what will follow and includes the main actors of the article. The body is the main content. Following the probabilistic model in <ref type="bibr" target="#b31">[31]</ref>, the contents of these structural elements are mapped to queries in a straightforward manner: we use the entire contents of a selected element. For article title, we tested the effectiveness of using exact phrases for modeling, however, plain title content outperformed exact phrases and, hence, we use the plain title content to model title. In addition to structural elements, we use two extra features as a source for query modeling: named entities and quotations. A great majority of articles refer to and discuss people, organizations, and locations. Given a news article a, we identify named entities in a by extracting sequences of capitalized words. Quotations are text passages from interviews with people inside the article and as such are likely to remain intact throughout information spread <ref type="bibr" target="#b25">[25]</ref>. This characteristic renders them viable surrogates for an article. Starting from the two extra features, we arrive at query models by constructing exact phrases from the named entities and the quotations.</p><p>As a final step, we model article metadata, consisting of the byline that represents authorship, and the news agent. The byline consists of the first and last name of the author. For the news agent, we create a basic list of potential synonyms by examining how social media refer to the news agent. For example, New York Times is mapped with three synonyms: "New York Times," NYTimes, NYT. Content from the byline is combined with list of synonyms to produce the final query.</p><p>Table <ref type="table" target="#tab_1">2</ref> (top) lists query models derived from the source article.</p><p>Exploiting social media. We consider a second family of query models, obtained from social media platforms that explicitly link to the source article. Examples include Digg stories (that have a URL), tweets that include a URL, etc. Consequently, it is possible to track a source news article to social media utterances via its URL. The idea is to create query models by aggregating content from a range of social media sources, for two reasons:</p><p>1. not all sources cover all news articles with the same intensity; 2. different social media may exhibit different language usage around the same source article.</p><p>By sampling content from multiple social media sources we increase the possibility of capturing the creativity in the language usage. We use a small number of social media platforms with frequent explicit links to news articles: Digg, Delicious, Twitter and NYT Community (NYTC); see §5 for details. We also use content from blog posts that hyperlink to a source article and Wikipedia articles relevant to the article <ref type="bibr" target="#b48">[48]</ref>. Data harvested from social media platforms that explicitly links to a source news article is used as follows for the purposes of query modeling. Similarly to how we modeled internal structure elements, we use the entire contents from all elements in a source to model the news article. E.g., for a Digg story that links to a news article, we take all text from the story title, from the story descrip-tion and from all comments, if any, attached to the story. For blog posts that include a hyperlink to the article, we consider the text of the post in the blog's feed. For Wikipedia, we use the source article's title to retrieve the ten most relevant Wikipedia articles from a Wikipedia index and use their content to model the news article.</p><p>Using social media for query modeling purposes raises issues. First, accumulating content from multiple blog posts and Wikipedia articles can lead to noisy queries. We reduce the model size by applying a graph-based term selection method (see below). Second, looking at other social media platforms, some news articles are "comment magnets," accumulating thousands of comments. Third, with platforms that allow for the creation of hierarchical discussion threads, the relevancy of a comment to the source news article is dependent on its level in the thread. To limit potential topical noise, we perform comment selection (dependent on the platform) based on comment metadata. Next, we look at two methods for ranking comments for Digg and NYTC.</p><p>For a Digg comment dc, we consider the number of positive (up) and negative (down) votes, the number of replies (replies) to the comment and the depth (level) of the comment in the thread:</p><formula xml:id="formula_11">Rank(dc) = (replies + 1) • (up -down) e level</formula><p>The formula rewards comments with a high number of positive votes that triggered further discussion (replies) and that are more likely to be about the article than about other comments (level).</p><p>For a NYT comment nc, we consider the number of recommendations (rec), and whether nc was selected from the editors (se):</p><formula xml:id="formula_12">Rank(nc) = 2 • (se + 1) • rec</formula><p>where se is a binary variable of value 1 when the comment is selected by the editors and 0 otherwise. The formula biases comment selection to highly recommended comments that are boosted further when selected from the NYT editors.</p><p>Table <ref type="table" target="#tab_1">2</ref> (bottom) lists query models derived using social media.</p><p>Reduced query models. So far, we have used any and all the data identified for a data source above as "the query model generated from the source." As a consequence, these query models (when viewed as lists of words) may be lengthy, which may have a negative impact on retrieval efficiency and potentially also on effectiveness; see Table <ref type="table" target="#tab_5">6</ref> (top half) for the average query length per source. Next, we aim to identify and extract terms that are discriminative, either for the source news article at hand or for the discussion surrounding it. To this end we introduce TH-Rank ("TextHitsRank"), a variation of TextRank <ref type="bibr" target="#b34">[34]</ref>. TextRank and other graph-based ranking methods are based on the idea of "recommendation," where the importance of a vertex within a word-graph is computed using global information recursively drawn from the entire graph. Our modifications to TextRank are three-fold: how the graph is constructed, the scoring algorithm, and the cutoff threshold for the returned terms.</p><p>To construct a directed (word) graph for a document, the text is tokenized and stemmed and multi-word named entities are collapsed into a single word. Unlike TextRank (where only nouns are considered for constructing the graph), we use all terms due to the low recognition accuracy of nouns in noisy text <ref type="bibr" target="#b10">[10]</ref>. For each token a vertex is created and an edge is added between tokens that co-occur within a window of two words. Intuitively, the edges are weighted according to the number of occurrences of a pair of tokens in the text. Words at sentence boundaries are not connected to avoid accidental recommendations.</p><p>We are not only interested in the most discriminative words, but also in their context. For this purpose, instead of the PageRank Round-robin weighted algorithm used by TextRank, we use the HITS algorithm, which makes a distinction between "authorities" and "hubs" <ref type="bibr" target="#b19">[19]</ref>, for scoring. In our setting, the authority score determines how important a word is for the article (proceeded by how many words) and the hub score reflects the word's contribution to the article's context (how many words follow it).</p><p>We use a document-dependent threshold for which terms to select: from each set (authorities or hubs), we only return terms whose score is of the same magnitude as the highest scored term.</p><p>In §6, we apply TH-Rank to the following models: full, digg, nytc, wikipedia, and blogposts (Table <ref type="table" target="#tab_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Late fusion</head><p>Different query models potentially give rise to different ranked result lists. To arrive at a single merged result list, we use late data fusion methods. In particular, we consider the methods listed in Table <ref type="table" target="#tab_2">3</ref>; see <ref type="bibr" target="#b41">[41]</ref> for a survey of these and other methods.</p><p>Let N be the set of all ranked lists ni resulting from different query models. Let sn i (a, u) be the score of an utterance u (from the target index) given a source news article a, wn i a weight assigned to ni and Nret a subset of N consisting of ranked lists that returned u. Then, combMAX considers the highest score from N , combMIN considers the lowest score from N , WcombSUM sums up all scores factored by their weight <ref type="bibr" target="#b16">[16]</ref>:</p><formula xml:id="formula_13">score WcombSUM (a, u) = P |N | i=1 wn i • sn i (a, u)</formula><p>if wn i = 1 (for all ni), it becomes combSUM. WcombWW is similar to WcombSUM except that final scores are multiplied by the sum of weights of the runs that returned the utterance:</p><formula xml:id="formula_14">score WcombWW (a, u) = P m∈N ret wm × P |N | i=1 wn i • sn i (a, u)</formula><p>for the special case where wm = 1 (for all m), we get Wcomb-MNZ. If we further assume wn i = 1 (for all ni), we arrive at combMNZ. combANZ is similar to combMNZ but final scores are averaged over the number of runs that return the utterance |Nret|:</p><formula xml:id="formula_15">score combANZ (a, u) = 1 |N ret | • P |N | i=1 sn i (a, u)</formula><p>Round-robin (RR) chooses one utterance from each ranked list, deleting any utterance if it has occurred before. Weighted roundrobin (RR-W) is similar except that not all ranked lists are available at each round. Each ranked list is assigned a sampling frequency, defining every how many rounds it will be sampled. Normalization of scores between ranked lists is required before producing the final rankings <ref type="bibr" target="#b37">[37]</ref>. A standard practice is to first normalize the document scores per run and then merge them:</p><formula xml:id="formula_16">s normed,n i (a, u) = sn i (a, u) -min(sn i (a)) max(sn i (a)) -min(sn i (a))</formula><p>.</p><p>We also consider a second normalization method, based on z-scoring, inspired from work in topic detection and tracking <ref type="bibr" target="#b2">[2]</ref>:</p><formula xml:id="formula_17">sz-score,n i (a, u) = sn i (a, u) -μ σ ,</formula><p>where μ is the mean of the document score distribution for source news article a in ranked list ni, and σ is the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL SETUP</head><p>We present our research questions, experiments, dataset and evaluation method. For the purpose of finding social media utterances that reference individual news articles, we choose to focus on a single target collection in our experimental evaluation, namely the blogosphere. Nothing depends on this particular choice, though. Our choice is based on the observation that blogs, unlike many other social media, are not limited to a single dominant platform like Digg or Twitter. Content found on individual social media platforms can be biased according to the platform's user demographics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments</head><p>To answer our research questions in §1 we conduct two sets of experiments, aimed at (i) query modeling and (ii) late fusion.</p><p>Performance of three families of query models In this set of experiments we answer research questions 1-3. For each of the three families (document structure, social media, and reduced models) we construct queries, and submit them to an index of blog posts. We measure the performance of each model individually, and compare the results. Analysis of the results reveals differences in performance between the individual models, and the families of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of three late fusion types</head><p>The second set of experiments is aimed at answering research questions 4 and 5.</p><p>Here, late fusion techniques are applied to the ranked lists produced by the individual models. We experiment with 10 fusion methods from three types: (i) no training required, (ii) query independent training, and (iii) query dependent training. Finally, we test the utility of two different score normalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data set and data gathering</head><p>The data set that we use as our target social media collection is the Blogs08 collection provided by TREC; the collection consists of a crawl of feeds, permalinks, and homepages of 1.3M blogs during early 2008-early 2009. This crawl results in a total of 28.4M blogs posts (or permalinks). We only used feed data, the textual content of blog posts distributed by feeds and ignored the permalinks. Two main reasons underly this decision: (i) our task is precision-oriented and benefits from a clean collection; and (ii) using feed data requires almost no preprocessing of the data. Extracting posts from the feed data gave us a coverage of 97.7% (27.8M posts extracted). As a second preprocessing step we perform language detection and remove all non-English blog posts from the corpus, leaving us with 16.9M blogs posts. Our index is constructed based on the full content of blog posts.</p><p>Our news article dataset is based on the headline collection from the top stories task in TREC 2009. This is a collection of 102,812 news headlines from the New York Times and include the article title, byline, publication date, and URL. For our experiments we extended the dataset by crawling the full body of the articles.</p><p>As auxiliary collections used in our query modeling experiments, we use data gathered from the following five platforms: Digg: A collaborative news platform where people submit URLs that they find interesting. <ref type="foot" target="#foot_0">1</ref> We collected 19,608 Digg stories corresponding to the same number of articles. On average each story is associated with 26 comments.</p><p>Delicious: A social bookmarking site, where people can store the addresses of web sites they want to keep. <ref type="foot" target="#foot_1">2</ref> We collected 7,275 tagged articles with an average of 3 unique tags per article, summing up to 3,906 unique tags.</p><p>Twitter: We use Topsy, a real-time search engine that indexes content from Twitter, a microblogging platform where people can submit short snippets of text 140 characters long. <ref type="foot" target="#foot_2">3</ref> We collected tweets that mention 21,550 news articles, with each article being mentioned in 3 tweets on average. <ref type="foot" target="#foot_3">4</ref>NYT Community: A web service from New York Times for retrieving comments registered on their site. <ref type="foot" target="#foot_4">5</ref> We collected comments for 2,037 articles with an average of 150 comments per article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia:</head><p>The collaborative online encyclopedia. We use the Wikipedia dump that is included in the Clueweb09 collection, <ref type="foot" target="#foot_5">6</ref> containing almost 6 million pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation</head><p>The ideal ground truth for our task would consist of tuples consisting of news articles and social media utterances. As a proxy, we follow <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b35">35]</ref> and use items that are explicitly linked to a given news source. We then remove the explicit links and test our link generation methods by examining to which extent they succeed at identifying those explicit links. The reason for choosing this evaluation scheme is twofold: (i) the generation of such ground truth is cheaper than having human assessors judge whether a blog post is about a news article, and (ii) in this paper we are interested in examining the relative effectiveness of the suggested approaches, not in absolute numbers.</p><p>Our ground truth is assembled in two phases. First, for each news article we find blog posts that include the article's URL. Second, for each discovered blog post we look for other blog posts that include its URL. The process continues recursively until no more blog posts are discovered. For our experiments we sample headlines with more than ten explicit links and where social media possibly plays a role. For each news article, we take the temporally first five explicitly linked blog posts for using them in modeling. The remaining blog posts form the article's ground truth. This selection procedure results in 411 news articles with an average of 14 explicitly linked ("relevant") blog posts per article. <ref type="foot" target="#foot_6">7</ref>In our experiments we use the Indri framework <ref type="bibr" target="#b32">[32]</ref>. Each experimental condition returns the top 1,000 results. We report on standard IR measures: recall, mean reciprocal rank (MRR), mean average precision (MAP), and r-precision. Statistical significance is tested using a two-tailed paired t-test and is marked as (or ) for significant differences for α = .01, or (and ) for α = .05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Weight optimization for late fusion</head><p>For late fusion methods that allow for weighted fusion, we estimate a weight wn i for each ranked list ni using query independent and query dependent approaches.</p><p>Query independent weight optimization. Given a set of news articles and a et of ground truth assessments, we seek weights that maximize MAP over a set of source articles. For this, we conduct two fold cross-validation and split our ground truth in two sets of equal size: training (205 articles) and testing (206 articles). First, we learn weights that maximize MAP on the training set and then use these for evaluation on the test set. For estimating wn i , we follow He and Wu <ref type="bibr" target="#b16">[16]</ref>. First, for each ranked list ni in the training set, the MAP score mapn i is computed. Then, mapn i is used as weight for ni in the test set: wn i = mapn i . He and Wu suggest that the weight for the best individual run should be factored several times its MAP score. Fig. <ref type="figure" target="#fig_4">3</ref> shows that, in our setting, increasing the weight of the best individual run hurts performance.</p><p>Query dependent weight optimization. Given a news article and a ground truth, we seek weights wn i that maximize average precision (AP). Since the weights are dependent on the query, the ground truth for training and testing should be different. For building the training ground truth, we look for good surrogates of implicitly linked blog posts to use as proxy. For this purpose, for an article's training ground truth, we consider the temporally first five explicitly linked blog posts. The testing ground truth is kept the same as in query independent optimization for the results to remain comparable. In the training step, the system learns weights such that the blog posts in the training ground truth rank at the top. Then, in the testing step, we report on MAP for the testing ground truth. For estimating wn i we use maximum AP training and line search <ref type="bibr" target="#b13">[13]</ref>, where wn 1 , . . . , wn n is considered a set of directions in the range [0, 1]. We move along the first direction in steps of 0.2 so that AP is maximized; then move from there along the second direction to its maximum, and so on. We cycle through the whole set of directions as many times as necessary, until AP stops increasing.</p><p>For query dependent and query independent fusion, we combine all available ranked lists except from the blogposts model. The later is excluded because it exploits the same explicitly linked blog posts to model the news article with those used as training ground truth in query dependent fusion. Also, for models that have reduced counterparts, we select the one performing the best. This selection leads to 11 models: title, lead, ne, quote, metadata, full, diggcomm, delicious, nyt-comm, twitter, and wikipedia-graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS AND ANALYSIS</head><p>We report on our results from the experiments in §5 for query modeling and late fusion and conduct an analysis on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Query modeling</head><p>We turn to the results of our query modeling approach; each paragraph discusses one of the research questions in §1. Next, we perform an analysis of the results to gain more insight. RQ1: Internal document structure vs. article title. Our baseline is set to the query model derived from an article's title only. This Table <ref type="table">4</ref>: System performance for retrieving blog posts relevant to a source article using credibility priors and models derived from internal document structure and social media, and their reduced counterparts using TH-Rank. Significance tested against baseline (title). 0.0691 0.0300 0.0122 0.0077 (Q) wikipedia-graph 0.0412 0.0142 0.0030 0.0020 (R) blogposts-graph 0.4170 0.4448 0.1727 0.1362 choice is supported by two reasons: First, the article's title is the most compact representation of the entire article and second, the article's title was chosen in prior research for ranking news headlines according to their mentions in the blogosphere <ref type="bibr" target="#b26">[26]</ref>.</p><p>Table <ref type="table">4</ref> (top) reports on results from models derived from the article's internal document structure. The best performing model is the one that uses the full article, namely, content from the article's title and body. The high performance of full is possibly due to blog posts picking up different aspects of the article that are not available in more compact representations such as title and lead. Both ne and quotes show a precision-enhancing effect over the baseline, at the cost of a drop in recall. Depending on the application, these representations could be an efficient alternative to full. RQ2: Comparison of social media models over internal document structure models. Turning into models derived from social media, Table <ref type="table">4</ref> (middle) shows that digg-comm, the model from Digg using only five comments (using Appendix 4.2), is performing the best among all social media models and significantly improves over title on all metrics. delicious shows a high recall possibly due to the nature of tags which are more likely to capture the article's theme rather than precisely identify it.</p><p>In general, social media models using all available content from the underlying source perform worse than models based on article internal structure. This is possibly due to noise found in user generated content, a claim supported by the improved performance of digg-comm and nytc-comm (which exploit only a subset of available content using comment selection methods) over their respective baselines (using all comments). RQ3: Reduced query models using TH-Rank. For most query models, TH-Rank leads to improved performance. Among all reduced models, full-graph and blogposts-graph perform the best; both show significant improvements on precision-oriented metrics, without hurting recall. For full-graph, when compared to full, per- formance drops by 33% due to a significant reduction (97%) in query size. Given the low noise levels in edited text, TH-Rank sees to discard more words than required. For blogposts-graph, performance increases by an order of magnitude following a 80% reduction in query size. For blog posts, TH-Rank manages to remove noise and select terms helpful to retrieval. In both cases, TH-Rank offers a good balance between efficiency (shorter models are less computationally expensive) and effectiveness.</p><p>Next, we take a close look at the query modeling results and we perform an analysis in four directions: (i) uniqueness, (ii) silent models, (iii) NYT comments, (iv) TH-Rank, and (v) opinionatedness of articles.</p><p>Uniqueness: Besides looking at the results in terms of precision and recall, we also explore the uniqueness of runs: how many linked utterances are identified by one model in the top X, and not by any of the other models? We do so for the top 10 results and top 1,000 results. First, we observe that all models have unique results; Second, quotes-#1 is able to capture unique results in the top 10, whereas delicious does so in the top 1,000. Finally, title, full, and digg-comm capture most unique results.</p><p>Silent models: Table <ref type="table" target="#tab_5">6</ref> shows that certain models, like Twitter and NYT, are silent for a large number of queries, and it is therefore difficult to assess their utility when looking at overall performance. Table <ref type="table" target="#tab_4">5</ref> reports on the performance for articles present in the baseline and the social media model (twitter or nytc-comm); results show that the Twitter model significantly outperforms the baseline on recall metrics.</p><p>NYT comments: An interesting observation from the results is the low performance of nytc and nytc-comm, despite their strong connection to the source news article (see Tables <ref type="table">4</ref> and<ref type="table" target="#tab_4">5</ref>). This strong connection could be the reason for their failure: news comments are usually displayed on the same page as the news article and come after it. Consequently, when people comment, there is no need to explain what news event they are referring to, give context to their opinion, or write full names of entities. This leads to a lack of discriminative and descriptive terms for that news article in the comments, potentially explaining the poor performance of news comments-based query models.</p><p>TH-Rank: Why does TH-Rank help performance for blogposts but not for other social media? Comment threads are prone to topic drift as the discussion goes on, while explicitly linked blog posts are more likely to be focusing on one topic, that of the article. Topical focus is likely to enable TH-Rank in one case to reduce noise and improve performance and in the other to capture the "general theme" of the discussion which can be far away from what triggered the discussion initially.</p><p>The same can hold for models using comment selection methods which are found to outperform their TH-Rank counterparts. Highly recommended comments are more likely to reflect what is also published in the blogosphere. On the other hand, when TH-Rank is ran on all available data from a source it proves unable to capture accurately discriminative terms for the news article, although it returns more terms for digg and nytc (lower reduction ratio, see Table <ref type="table" target="#tab_5">6</ref>).</p><p>Opinionatedness: We measure how opinionatedness of news articles affect the performance of individual models. In order to do so, we split our ground truth of 411 articles into 131 opinionated and 280 non-opinionated articles depending on whether the article title contains the term "OP'ED" (e.g., columns, editorials, . . . ).</p><p>We perform a two-tailed independent t-test between the opinionated and non-opinionated scores for each model. For most models, performance is stable across the two articles types with full and digg-comm performing the best. In terms of recall, six of the models drop significantly, when crossing from non-opinionated to opinionated articles. title is amongst them, possibly due to static titles assigned to opinionated articles, which usually consist of the column or editorial name with only few additional terms. We also notice that digg-comm sees the highest recall on non-opinionated articles over all models, whereas this is full for opinionated articles. An interesting case is the metadata model for opinionated articles: When compared to non-opinionated articles, the recall shows a large significant increase, which is due to blog posts referring to the article author's name (and agency).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Late fusion</head><p>As before, each paragraph corresponds to one of the remaining research questions in §1. Then, we take a closer look at the results. RQ4: Query independent late fusion. We experiment with 10 fusion methods and two document score normalization methods for the combination of 11 individual models; see §5.4. Table <ref type="table" target="#tab_6">7</ref> shows that the best performing method in terms of MAP is Wcomb-MNZ, which yields statistically significant improvements over the full model. WcombSUM, WcombWW, combSUM and combMNZ perform similar to, but slightly less than WcombMNZ, and RR-W outperforms RR.</p><p>We investigated the effect on MAP for a range of scale factors of the best individual run (full), when using z-scoring and linear  RQ5: Query dependent late fusion. For this experiment we use the best performing late fusion method from RQ4: WcombMNZ with z-scoring. The goal here is to learn weights that maximize average precision for a training ground truth. From Table <ref type="table" target="#tab_6">7</ref> we can see that query dependent fusion significantly outperforms full, but performs slightly worse than query independent fusion. One reason for this can be that the nature of relevant blog posts is evolving as we move farther in time from the source article publication date.</p><p>Next, we proceed with an analysis of: (i) query dependent vs. independent fusion, (ii) an oracle run, and (iii) early fusion.</p><p>Query dependent vs. independent fusion: In theory, query dependent fusion was expected to outperform other methods because of how weights were optimized. For each individual article, weights were estimated to maximize average precision. However, query independent fusion showed to perform better. The two methods differ in the training set. For query independent fusion each article in the training set was on average associated with 14 blog posts. For query dependent fusion, weights were estimated for a ground truth of 5 blog posts per article. It is, therefore, interesting to explore the utility of a larger sample of explicitly linked blog posts as training ground truth or to seek time dependent evolution patterns in the weights assigned to each ranked list.</p><p>Oracle run: For each source article we take the ranked list produced from the best performing model according to average precision, and combine these into a final "oracle" run. Since we only use one model per source news article and no mixture of models, this run does not achieve the maximum performance possible. Still, the oracle run gives an indication of what scores are achievable. Comparing the performance of the oracle run (Table <ref type="table" target="#tab_6">7</ref>) to WcombMNZ, the best performing query independent fusion method, we observe that the latter arrives remarkably close to the oracle run.</p><p>Early fusion: Belkin et al. <ref type="bibr" target="#b7">[7]</ref> conducted thorough experiments comparing performance of early and late fusion techniques. They found that combining individual models at query time performance increased compared to individual models. As a check we use the best performing model from each family of query models and combine them in a query. For the reduced models of Wikipedia and blog posts, each term is weighted according to its hub or authority score. The performance of this run (again, Table <ref type="table" target="#tab_6">7</ref>) is less than the best individual run (i.e., full) and the query independent and dependent fusion methods (i.e., WcombMNZ). The lower performance is likely due to noise brought in after combing all models and suggests that term selection and term weighting methods on the combined query hold potential for improving retrieval effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>Most of the methodological contributions of this paper lie in news article modeling for the task of discovering implicitly linked social media utterances. We study retrieval effectiveness of multiple query models that exploit content from individual elements in article's internal structure and from explicitly linked utterances from six social media platforms. Experimental evidence shows that query models based on the entire article perform the best. However, query models from social media bring in previously unseen utterances. Query models trained on anchor text from explicitly linked blog posts are interesting to explore, however our current experimental setup constraints us from further investigating their utility. For lengthy models, we introduce TH-Rank, an unsupervised graph-based method for selecting the most discriminative terms from each model. We demonstrate that content selection helps to improve both effectiveness and efficiency. Next, we study the effect of combining ranked lists from individual query models and we experiment with ten late data fusion methods and two document score normalization methods. We also study the impact on effectiveness of query dependent and query independent weight optimization schemes. We found that fusion methods improve significantly when using z-scoring normalization. Query independent weight optimization helped WcombMNZ to outperform all individual and fusion runs and to achieve performance remarkably close to an oracle run.</p><p>In future work we plan stricter recency conditions to our retrieval model, study the potential of query dependent fusion in more detail, compare our models to typical IR approaches such as BM25F, and experiment with additional target indexes such as Twitter. Results from this work and its future extensions lay the ground work for discovering social media utterances related to a topic of a group of news stories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Avg. symmetric KL-divergence between New York Times articles and explicitly linked social media utterances from Digg, Twitter, blog posts, New York Times comments, Delicious, Wikipedia. Larger circles indicate a higher degree of divergence and hence a bigger difference in vocabulary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Approach to finding linked social media utterances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MAP scores for combination of 11 individual runs when increasing the weight of the best individual run for WcombMNZ and WcombSUM methods and using linear and z-score normalization of document scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note><p>Models for individual credibility factors. |u| is the utterance length in words, n(X, u) is the number of X for utterance u, where X = {r, e, o, z, l}, and r is comments, o is pronouns, e is emoticons, z is capitalized words, and l is misspelled (or unknown) words. pcomments</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Query models grouped by source; in addition, TH-Rank is applied to the following models: full, digg, and nytc.</figDesc><table><row><cell cols="2">Query Model Source</cell><cell>Elements</cell></row><row><cell cols="2">Exploiting the source article</cell><cell></cell></row><row><cell>title</cell><cell>Article</cell><cell>Title</cell></row><row><cell>lead</cell><cell>Article</cell><cell>Lead</cell></row><row><cell>body</cell><cell>Article</cell><cell>Body</cell></row><row><cell>metadata</cell><cell>Article</cell><cell>Author (byline), news agent</cell></row><row><cell>ne</cell><cell>Article</cell><cell>Named entities</cell></row><row><cell>quote</cell><cell>Article</cell><cell>Quotations</cell></row><row><cell>full</cell><cell>Article</cell><cell>Title and body</cell></row><row><cell cols="2">Exploiting social media</cell><cell></cell></row><row><cell>digg</cell><cell>Digg</cell><cell>Title, description and comments</cell></row><row><cell>delicious</cell><cell>Delicious</cell><cell>Title, tags and their frequency</cell></row><row><cell>twitter</cell><cell>Topsy</cell><cell>Tweet</cell></row><row><cell>nytc</cell><cell>NYTC</cell><cell>Comment title and body</cell></row><row><cell>wikipedia</cell><cell cols="2">Wikipedia Full article</cell></row><row><cell>blogposts</cell><cell>Blogs</cell><cell>Feed item in RSS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Data fusion methods used in the paper.</figDesc><table><row><cell>Method</cell><cell>Gloss</cell></row><row><cell>combMAX</cell><cell>Maximum of individual scores</cell></row><row><cell>combMIN</cell><cell>Minimum of individual scores</cell></row><row><cell>combSUM</cell><cell>Sum of individual scores</cell></row><row><cell>combMNZ</cell><cell>combSUM × number of nonzero scores</cell></row><row><cell>combANZ</cell><cell>combSUM ÷ number of nonzero scores</cell></row><row><cell cols="2">WcombSUM weighted sum of individual scores</cell></row><row><cell cols="2">WcombMNZ WcombSUM × number of nonzero scores</cell></row><row><cell>WcombWW</cell><cell>WcombSUM × sum of individual weights</cell></row><row><cell>RR</cell><cell>Round-robin</cell></row><row><cell>RR-W</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>System performance for articles present in either twitter or nytc-comm and the baseline.</figDesc><table><row><cell>runID</cell><cell>Recall</cell><cell>MRR</cell><cell>Rprec</cell><cell>MAP</cell></row><row><cell cols="4">110 common topics between baseline and Twitter</cell><cell></cell></row><row><cell>title</cell><cell>0.4165</cell><cell>0.3876</cell><cell>0.1667</cell><cell>0.1192</cell></row><row><cell>twitter</cell><cell>0.5741</cell><cell>0.4206</cell><cell>0.2024</cell><cell>0.1654</cell></row><row><cell cols="4">197 common topics between baseline and NYTC</cell><cell></cell></row><row><cell>title</cell><cell>0.4091</cell><cell>0.3576</cell><cell>0.1293</cell><cell>0.0951</cell></row><row><cell cols="2">nytc-comm 0.1979</cell><cell>0.1345</cell><cell>0.0334</cell><cell>0.0261</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Number of queries per news article model, and their average length for query terms, phrases, and both.</figDesc><table><row><cell>runID</cell><cell># queries</cell><cell cols="3">Average query length Terms Phrases Total</cell></row><row><cell cols="4">Query based on: Internal document structure</cell><cell></cell></row><row><cell>title</cell><cell>411</cell><cell>8</cell><cell>0</cell><cell>8</cell></row><row><cell>lead</cell><cell>411</cell><cell>23</cell><cell>0</cell><cell>23</cell></row><row><cell>metadata</cell><cell>411</cell><cell>8</cell><cell>1</cell><cell>9</cell></row><row><cell>ne</cell><cell>410</cell><cell>0</cell><cell>18</cell><cell>18</cell></row><row><cell>quote-#1</cell><cell>398</cell><cell>0</cell><cell>10</cell><cell>10</cell></row><row><cell>full</cell><cell>411</cell><cell>912</cell><cell>0</cell><cell>912</cell></row><row><cell cols="2">Query based on: Social media</cell><cell></cell><cell></cell><cell></cell></row><row><cell>delicious</cell><cell>411</cell><cell>47</cell><cell>0</cell><cell>47</cell></row><row><cell>digg</cell><cell>411</cell><cell>1,476</cell><cell>0</cell><cell>1,476</cell></row><row><cell>digg-comm</cell><cell>411</cell><cell>225</cell><cell>0</cell><cell>225</cell></row><row><cell>nyt</cell><cell>197</cell><cell>15,048</cell><cell>0</cell><cell>15,048</cell></row><row><cell>nytc-comm</cell><cell>197</cell><cell>288</cell><cell>0</cell><cell>288</cell></row><row><cell>twitter</cell><cell>111</cell><cell>48</cell><cell>0</cell><cell>48</cell></row><row><cell>wikipedia</cell><cell>409</cell><cell>6,912</cell><cell>1,316</cell><cell>8,229</cell></row><row><cell>blogposts</cell><cell>408</cell><cell>617</cell><cell>41</cell><cell>658</cell></row><row><cell cols="3">Query based on: Reduced using TH-Rank</cell><cell></cell><cell></cell></row><row><cell>full-graph</cell><cell>411</cell><cell>27</cell><cell>2</cell><cell>29</cell></row><row><cell>digg-graph</cell><cell>395</cell><cell>37</cell><cell>1</cell><cell>38</cell></row><row><cell>nytc-graph</cell><cell>197</cell><cell>131</cell><cell>1</cell><cell>132</cell></row><row><cell>wikipedia-graph</cell><cell>409</cell><cell>117</cell><cell>10</cell><cell>127</cell></row><row><cell>blogposts-graph</cell><cell>408</cell><cell>23</cell><cell>1</cell><cell>25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>System performance for query independent fusion using 10 late fusion techniques on the test set using z-score normalization and combining 11 individual runs for the best scale factor (2) of full. Query dependent fusion results are reported for the best fusion method. Significance tested against full. Results from an oracle run and early fusion are also reported.</figDesc><table><row><cell>Method</cell><cell>Recall</cell><cell>MRR</cell><cell>Rprec</cell><cell>MAP</cell></row><row><cell>full</cell><cell>0.5860</cell><cell>0.6196</cell><cell>0.3323</cell><cell>0.2522</cell></row><row><cell cols="2">Query independent</cell><cell></cell><cell></cell><cell></cell></row><row><cell>combMAX</cell><cell>0.7214</cell><cell>0.5871</cell><cell>0.2820</cell><cell>0.2283</cell></row><row><cell>combMIN</cell><cell>0.3308</cell><cell>0.0766</cell><cell>0.0195</cell><cell>0.0131</cell></row><row><cell>combSUM</cell><cell>0.7194</cell><cell>0.6083</cell><cell>0.3202</cell><cell>0.2665</cell></row><row><cell>combMNZ</cell><cell>0.7265</cell><cell>0.6130</cell><cell>0.3252</cell><cell>0.2722</cell></row><row><cell>combANZ</cell><cell>0.6821</cell><cell>0.4547</cell><cell>0.1574</cell><cell>0.1256</cell></row><row><cell cols="2">WcombSUM 0.7190</cell><cell>0.6141</cell><cell>0.3317</cell><cell>0.2772</cell></row><row><cell cols="2">WcombMNZ 0.7248</cell><cell>0.6123</cell><cell>0.3422</cell><cell>0.2788</cell></row><row><cell>WcombWW</cell><cell>0.7169</cell><cell>0.6129</cell><cell>0.3315</cell><cell>0.2723</cell></row><row><cell>RR</cell><cell>0.7328</cell><cell>0.3990</cell><cell>0.2095</cell><cell>0.1664</cell></row><row><cell>RR-W</cell><cell>0.7298</cell><cell>0.3999</cell><cell>0.2358</cell><cell>0.1882</cell></row><row><cell cols="2">Query dependent</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WcombMNZ 0.7011</cell><cell>0.6148</cell><cell>0.3277</cell><cell>0.2646</cell></row><row><cell>Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Oracle</cell><cell>0.6388</cell><cell>0.7727</cell><cell>0.3645</cell><cell>0.3141</cell></row><row><cell>Early fusion</cell><cell>0.5331</cell><cell>0.5356</cell><cell>0.5220</cell><cell>0.1956</cell></row><row><cell cols="5">normalization of document scores. Fig. 3 illustrates that, in our</cell></row><row><cell cols="5">setting, WcombMNZ with z-scoring and the scale factor set to 2</cell></row><row><cell cols="4">achieves the best MAP among all fusion methods.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.digg.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.delicious.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.topsy.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Topsy limits access to the ten most recent tweets for a URL. Consequently, the reported average might not reflect reality.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p><ref type="bibr" target="#b5">5</ref> http://developer.nytimes.com/docs/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>community_api 6 http://boston.lti.cs.cmu.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Data/ clueweb09/<ref type="bibr" target="#b7">7</ref> The complete ground truth may be retrieved from http://ilps.science.uva.nl/resource/ linking-online-news-and-social-media.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research was supported by the European Union's ICT Policy Support Programme as part of the Competitiveness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430 (GALATEAS), by the 7th Framework Program of the European Commission, grant agreement no. 258191 (PROMISE), by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, by the Netherlands Organisation for Scientific Research (NWO) under project nrs 612.-066.512, 612.061.814, 612.061.815, 640.004.802, 380-70-011 and by the Center for Creation, Content and Technology (CCCT).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Implicit structure and dynamics of blogspace</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lukose</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In WWE &apos;04</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Topic detection and tracking: event-based information organization</title>
		<editor>J. Allan</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Acad. Publ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Document representation and query expansion models for blog recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Arguello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In ICWSM &apos;08</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Why are they excited?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="207" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning similarity metrics for event identification in social media</title>
		<author>
			<persName><forename type="first">H</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fusion of effective retrieval strategies in the same information retrieval system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beitzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Soc. Inform. Sci. and Techn</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="859" to="868" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining the evidence of multiple query representations for information retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Proc. and Manag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="431" to="448" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Language of News Media</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language in Society. Blackwell</title>
		<imprint>
			<date type="published" when="1991-09">September 1991</date>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative systems: Solving the vocabulary problem</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<idno type="ISSN">0018-9162</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="58" to="66" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Studying the effects of noisy text on text mining applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K M</forename><surname>Haque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AND &apos;09</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dumais. The vocabulary problem in human-system communication</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="964" to="971" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Blews: Using blogs to provide context for news articles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>König</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>In ICWSM &apos;08</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linear discriminant model for information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yun Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;05</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
		<ptr target="http://www.inex.otago.ac.nz/" />
		<title level="m">INEX 2010 Link-The-Wiki Track</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information diffusion through blogspace</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gruhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;04</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="491" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward a robust data fusion for document retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE NLP-KE&apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatically linking news articles to blog entries</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fujiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symp</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Why we twitter: understanding microblogging usage and communities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebKDD/SNA-KDD &apos;07</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Click-through prediction for news queries</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;09</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structure and evolution of blogspace</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Com. ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="35" to="39" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What is twitter, a social network or a news media?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;10</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The New York Times Annotated Corpus</title>
		<author>
			<persName><surname>Ldc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cascading behavior in large blog graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcglohon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Glance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meme-tracking and the dynamics of the news cycle</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;09</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2009 blog track</title>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC &apos;09</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Early online identification of attention gathering items in social media</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathioudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;10</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">System fusion for improving performance in information retrieval systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">News article ranking: Leveraging the wisdom of bloggers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RIAO &apos;10</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">State of the blogosphere</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mclean</surname></persName>
		</author>
		<ptr target="http://technorati.com/blogging/article/state-of-the-blogosphere-2009-introduction" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Similarity measures for tracking information flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;05</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Acm</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Combining the Language Model and Inference Network Approaches to Retrieval IPM Special Issue on Bayesian Networks and Information Retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="735" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wikify!: linking documents to encyclopedic knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into texts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;04</title>
		<imprint>
			<date type="published" when="2004-07">July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to link with Wikipedia</title>
		<author>
			<persName><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A study of blog search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3936</biblScope>
			<biblScope unit="page" from="289" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relevance score normalization for metasearch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;01</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="427" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Overview of the Trec-2006 blog track</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC 2006. NIST</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using twitter to recommend real-time topical news</title>
		<author>
			<persName><forename type="first">O</forename><surname>Phelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys &apos;09</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="385" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Event detection and tracking in social streams</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sayyadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maykov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In ICWSM &apos;09</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combination of multiple searches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC 1992</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MULTIME-DIA &apos;05</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="399" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Predicting the popularity of online content</title>
		<author>
			<persName><forename type="first">G</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
		<idno>CoRR, abs/0811.0405</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualization of news distribution in blog space</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Takama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kajinami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WI-IATW &apos;06</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="413" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bloggers during the london attacks: Top information sources and topics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWE &apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Predicting the volume of comments on online news stories</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;09</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Credibility improves topical blog post retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-08: HLT. ACL</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A generative blog post retrieval model that uses query expansion based on external collections</title>
		<author>
			<persName><forename type="first">W</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-ICNLP 2009</title>
		<imprint>
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
