<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Locating Data Sources in Large Distributed Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Leonidas</forename><surname>Galanis</surname></persName>
							<email>lgalanis@cs.wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Wang</surname></persName>
							<email>yuanwang@cs.wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Shawn</forename><forename type="middle">R</forename><surname>Jeffery</surname></persName>
							<email>jeffery@cs.wisc.edu</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
							<email>dewitt@cs.wisc.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Sciences Department</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin</orgName>
								<orgName type="institution" key="instit2">Madison</orgName>
								<address>
									<addrLine>1210 W Dayton St Madison</addrLine>
									<postCode>53706</postCode>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>2003</postCode>
									<settlement>Berlin, Germany</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Locating Data Sources in Large Distributed Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1B91F6EA7A1BDA42585E62DD20EFA63A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Querying large numbers of data sources is gaining importance due to increasing numbers of independent data providers. One of the key challenges is executing queries on all relevant information sources in a scalable fashion and retrieving fresh results. The key to scalability is to send queries only to the relevant servers and avoid wasting resources on data sources which will not provide any results. Thus, a catalog service, which would determine the relevant data sources given a query, is an essential component in efficiently processing queries in a distributed environment. This paper proposes a catalog framework which is distributed across the data sources themselves and does not require any central infrastructure. As new data sources become available, they automatically become part of the catalog service infrastructure, which allows scalability to large numbers of nodes. Furthermore, we propose techniques for workload adaptability. Using simulation and real-world data we show that our approach is valid and can scale to thousands of data sources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our vision is demonstrated by the following scenario: At some computer terminal of a large distributed system a user issues a query. Based on the query, the system determines where to look for answers and contacts each node containing relevant data. Upon completion of the query, regardless of the number of results or how they are ranked and presented, the system guarantees that all the relevant data sources known at query submission time have been contacted. The naïve way to implement our vision would be to send a query to each of the participating nodes in the network. While this approach would work for a small number of data providers it certainly does not scale. Hence, when a system incorporates thousands of nodes, a facility is needed that allows the selection of the subset of nodes that will produce results, leaving out nodes that will definitely not produce results. Such a facility implies the deployment of catalog-like functionality.</p><p>A catalog service in a large distributed system can be used to determine which nodes should receive queries based on query content. Additionally it can be used to perform other tasks such as query optimization in a distributed environment. There are three basic designs for building a catalog service for a distributed system: 1) A central catalog service, 2) a fully-replicated catalog on each participating node, or 3) a fully distributed catalog service. A centralized design implies a resource exclusively dedicated to servicing catalog requests. Existing technology allows the construction of such servers that could sufficiently handle thousands of nodes. Such a solution, however, requires a central infrastructure and a scheme to share expenses among the participating peers. To avoid this each node in the system can take over the burden of catalog maintenance. To this end, one simple design is the use of a fully replicated catalog on each peer (as practiced in distributed database systems <ref type="bibr" target="#b17">[18]</ref>). When a new peer joins the system it downloads the catalog from any existing peer and it can immediately query the entire community. Nevertheless, maintenance of the catalogs requires O(n 2 ) number of messages for the formation of a network of n nodes. Clearly, this is not scalable to thousands of nodes.</p><p>We focus on a fully distributed architecture motivated by recent advances in peer-to-peer computing (P2P). P2P systems research has proposed a number of new distributed architectures with desirable traits, including no central infrastructure, better utilization of distributed resources, and fault tolerance. Particular attention has been paid into making these systems scalable to large numbers Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment of nodes, avoiding shortcomings of the early P2P pioneers such as file sharing systems like Gnutella <ref type="bibr" target="#b8">[9]</ref> and Napster <ref type="bibr" target="#b16">[17]</ref>. Representatives of scalable location and routing protocols are CAN <ref type="bibr" target="#b20">[21]</ref>, Pastry <ref type="bibr" target="#b21">[22]</ref>, Chord <ref type="bibr" target="#b24">[25]</ref> and Tapestry <ref type="bibr" target="#b31">[32]</ref>, henceforth referred to as Distributed Hash Tables or DHTs. Each of these protocols, however, allows only simple key based lookup queries.</p><p>This paper studies the feasibility of using existing P2P technology as the basis for efficiently facilitating complex queries over an arbitrary large number of data repositories. Given an arbitrary query q and a large number of data repositories, our goal is to send q only to the repositories that have data relevant to q without relying on a centralized catalog infrastructure. Additionally, data repositories must be able to join the P2P system and make their data available for queries. Our design builds on current P2P technologies. The contributions of this work are:</p><p>• A catalog framework for locating data sources • A fully decentralized design of a distributed catalog service that allows data providers to join and make their data query-able by all existing peers. • Techniques to adapt to the query workload and distribute the catalog service load fairly across the participating nodes. • An experimental evaluation of a distributed catalog for locating data sources in large distributed XML repositories.</p><p>The rest of this paper is organized as follows: The system model of our envisioned catalog service is described in Section 2 where a simple example demonstrates its application. Section 3 discusses the desired features of the data summaries for our distributed catalog and proposes two designs. In Section 4 we show how our system evolves as new nodes join. Section 5 describes how the catalog service is used in order to direct queries to the relevant data sources. Section 6 points out load balancing issues and proposes effective solutions. Section 7 presents our experiments. Related work and Conclusions (Sections 8 and 9) follow at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Model</head><p>Conceptually the system allows an arbitrary number of data providers or nodes to join and make their data available. Let N i (1 &lt; i n) denote the n nodes, each of which publishes a set D i of data objects. When a node N i wants to join the system it creates catalog information which is the set</p><formula xml:id="formula_0">C i = {(k j , S ij ) | S ij is a summary of k j on node N i }.</formula><p>The items k j are present in the data objects D i . In an XML repository if D i is a set of documents, the k j 's will be a subset of the attribute and element names in D i . Each S ij is summary catalog information (or data summary) corresponding to k j and depends on the data on node N i . For example, a data summary for the element price on node N i might contain all the unique paths that lead to price as well as a histogram of price's values.</p><p>The catalog service determines which nodes a query Q should execute on using the functions query_parts() and map(). The function query_parts extracts a set of k j 's from a query. Given a query Q. The function map:</p><formula xml:id="formula_1">{Q}×{{C i | 1 &lt; i n}} → {N i | 1 &lt; i n}, uses<label>query_parts</label></formula><p>() to examine the relevant sets of data summaries S ij in order to determine the nodes storing data relevant to Q. Of course the catalog service may contain additional information but this paper focuses on the implementation of map when the number of nodes in the system becomes very large.</p><p>One possible map function would be the constant function map(Q, {C i }) = {N i | 1&lt; i n}. However, such an implementation would not scale for large values of n since it would require contacting every node for every query. The study in <ref type="bibr" target="#b9">[10]</ref> demonstrates on a real system that the key to scalability is minimizing the number of messages in the distributed system. Hence, our goal is to implement map(Q, {C i }) = {N | P 1 ∨ P 2 } where P 1 : Executing Q on N yields a non-empty results set and P 2 : Executing part of Q on N is required to produce the final result set for Q. Proposition P 2 covers the case in which Q requires a join or an intersection of data across different nodes. To achieve our goal, our implementation of map employs a fully distributed catalog design based on DHTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">DHT Background</head><p>The DHTs have very desirable characteristics. Their goal is to provide the efficient location of data items in a very large and dynamic distributed system without relying on any centralized infrastructure. Thus, given a key, the corresponding data item can be efficiently located using only O(logn) network messages ( <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>) where n is the total number of nodes in the system. Moreover, the distributed system evolves gracefully and can scale to very large numbers of nodes. Hence, current DHT designs provide a means to create large fully distributed dynamic networks of nodes for storage and efficient retrieval of objects. Our work leverages this functionality to provide a scalable fully distributed catalog service. Chord, which serves as the experimental substrate of our work, is publicly available and has been successfully used in other projects such as CFS <ref type="bibr" target="#b6">[7]</ref>. Nevertheless, our design does not depend on the specific DHT implementation and can work with any of the aforementioned DHT protocols.</p><p>The Chord protocol supports just one lookup operation: It maps a given key to a node. Depending on the application this node is responsible for associating the key with the corresponding data item (object). Chord uses hashing to map both keys and node identifiers (such as IP address and port) onto the identifier ring (Figure <ref type="figure" target="#fig_0">1</ref>). Each key is assigned to its successor node, which is the nearest node traveling the ring clockwise. Nodes and keys may be added or removed at any time, while Chord maintains efficient lookups using just O(logn) state on each node in the system. For a detailed description of Chord and its algorithms refer to <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Keys and Objects</head><p>One of the design challenges is to determine how to map the catalog information to the DHT. Based on our conceptual catalog model the obvious choice for keys are the items k j , henceforth also referred to as keys. The objects stored are sets of data summaries S ij , and not just single data summaries since mapping from k j to S ij is not 1 to 1.</p><p>An example using a simple XPath <ref type="bibr" target="#b28">[29]</ref> query illustrates how the outlined concepts can be put into practice. Consider four XML repositories which contain the data shown in Table <ref type="table" target="#tab_0">1</ref>. Assume that element tags are chosen as keys k j and that each summary S ij contains a set of all the possible paths in the data that lead to k j . For example S 1, author ={library/catalogs/book, library/reservation/book} while S 2, author = {bookstore/book}. Table <ref type="table" target="#tab_1">2</ref> shows how the DHT assigned the keys to the nodes that joined the network. The summary sets are stored along with the keys. Query Q 1 =/library/reservation//book illustrates how the data summaries can be used. Q 1 , submitted on N 3 , asks for all reserved books from all the library nodes in the P2P network (Figure <ref type="figure" target="#fig_4">2</ref>). Determining which nodes to send Q 1 to is done as follows: The tag name book serves as the DHT lookup key. Q 1 is sent by the DHT layer to N 4 (step 1), which stores the portion of the catalog which contains book information. On N 4 , the path in Q 1 is matched against S i,book , (1&lt;i&lt;4). N 4 replies to N 3 with the node set {N 1 } (step 2) since only S 1,book matches the given query and so N 1 is the only node that stores at least one XML document that contains a book element with a library ancestor. Finally, N 3 sends Q 1 to N 1 for execution (step 3). The example illustrates one possible way to use DHTs by appropriately defining k j and S ij for XML repositories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paths in XML Data</head><p>DHT Index N 1 author:{(S 1,author ), (S 2,author ), (S 3,author ), (S 4,author )} N 2 reservation: {(S 1,res. )} N 3 --N 4 book: {(S 1,book ), (S 2,book ), (S 3,book ), (S 4,book )}, price: {(S 2,price ), (S 3,price ), (S 4,price )} Flexibility in data summarization is also an essential requirement for our system since there are various data value domains and schemas. There are many different data summarization techniques and naturally none of them is suitable in every case. Consequently, data providers that join the system should enjoy the flexibility of choosing from a variety summarization techniques that fit their data. For example histograms <ref type="bibr" target="#b12">[13]</ref> are suitable for numeric values while Bloom filters <ref type="bibr" target="#b2">[3]</ref> are more suitable for web addresses. In some cases no summarization is necessary, such as the domain of all states in the USA.</p><p>Note a fine distinction between the data summaries provided to the system by a data source N s and the summaries actually stored on some other node N c in the sys- tem. Conceptually, N c receives a set {(k j , S ij )} from the other nodes in the system. However, it is not required to store each S ij as provided by the data source. N c can store the summary information however it desires, but should not degrade its accuracy or otherwise change its content.</p><formula xml:id="formula_2">N 1 N 2 N 3 N 4 Q 1 (1) Q 1 , book (2) {N 1 } (3) execute Q 1 Q 1 Figure 2: Execution of Q 1 N 1 Nodes Keys N 2 successor of N 1</formula><p>In this paper we evaluate our design on large networks of independent XML data repositories. Two path summarization methods for XML data are presented that are suitable for the purposes of our experiments. For both designs we adopt the choice of keys and summaries made in the initial example (Section 2.2): XML element tags and attribute names are our keys k j . For each k j the corresponding data summary S ij stores all possible unique paths to k j , which, in essence, corresponds to structural ancestor information. During a lookup operation, the last step of an XPath branch is used, which eliminates false positives from the structural summaries. Other configurations are also possible.</p><p>The DHT layer distributes the set of keys across the nodes in the system. The catalog CT N on node N holds the set of the data summaries for keys assigned to N. In essence CT N implements the map function for XPath queries for all keys assigned to N. For our study we implemented CT N using the methods described below. The performance of these implementations was then measured and the results were used in our simulation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generic B+-Tree</head><p>This method for implementing CT N is the most generic in the sense that it is both simple and independent of the structure of the data summaries. The keys of the B+-Tree are the pairs (k, N) where k corresponds to the element tag or the attribute name and N is the node that contributed a summary S for k. Only a prefix lookup on k is required but having N in the key helps speed up the algorithm in Section 5. S contains two parts: The structural summary SS and the value summary VS, both of which are optional.</p><p>The simplest way to implement SS is to store all unique paths that lead to k in the data of N. The space required for this will be usually small. Let c denote the total number of paths that lead to k; let l denote the average depth of each path and finally, let t denote the size in bytes allocated for storing each tag on the path. The size of SS is then approximately s = c⋅l⋅t. To make s independent of the length of the element names, tags are hashed to 32-bit integers. Using reasonable values of 10, 4, and 4 ( <ref type="bibr" target="#b4">[5]</ref>), for c, l, and t, respectively, the size of SS will be about 160 bytes. If the size of SS becomes a performance limiting issue in pathological cases (very large c or l) more advanced summarization techniques ( <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>) can be applied. VS depends on the value domain of k. If k has numeric values simple ranges suffice. Bloom filters <ref type="bibr" target="#b2">[3]</ref> can be used when we are interested only in equality queries on the value domain of k.</p><p>Given a simple XPath query Q: /a 1 /a 2 /.../a n /k op x the nodes that must receive Q are determined as follows: Each S i that corresponds to k and node N i is examined. Q will be sent to N i only if SS i contains /a 1 /a 2 /.../a n and VS i op x =TRUE (op is an operator such as '=' or '&lt;'). As expected, the set {N i } returned by this procedure will contain some false positives, mainly because of the accuracy of the value summary. The structural summary has a very low probability of producing false positives, when a good hash function is used, because hash collisions must occur in every element of a path to produce a false positive. Nevertheless, false negatives can always be avoided, as long as the summaries are updated regularly. Note that SS can also handle XPath queries with wildcards and "//".</p><p>One problem arises when the number of B+-tree entries for k becomes large, in which case the system may need to match a path against a large number of potentially very similar structural summaries. The solution is to replace all keys (k, N i ) with keys (k, C i ) that correspond to data items CS i . C i corresponds to a cluster of nodes with similar structure and CS i is a new compound summary constructed by merging the structural summaries of the nodes in C i . The paths contained in all S i s are merged and to each path p, a subset RN is assigned. RN contains the nodes that either contain or don't contain p, depending on which choice yields a smaller set. The other alternative for handling large numbers of nodes is to use an index keyed by the path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Path Index</head><p>A path index is an alternative which is less flexible than the previous method but more efficient for elements that appear on large numbers of nodes. Flexibility is limited because this design requires that the structural summaries contain paths while the previous method works for any structural summary. Processing one summary per node is, however, no longer necessary. Consider again the query Q: /a 1 /a 2 /…/a n /k. The DHT will relay Q based on k to N c for the purposes of catalog lookup. On N c , a B+-Tree keyed on inverse paths will guide the lookup. The data items in the tree are lists of nodes and each node is specially annotated if it has provided a value summary for the specific path. Thus, if the B+-tree contains the key k/a n /…/a 2 /a 1 , retrieving the corresponding data item will yield the nodes in the system that contain /a 1 /a 2 /…/a n /k. In order to make the key sizes of the index less variable the tags are replaced with constant size integers using hashing. Processing queries with wildcards is also possible. For example, Q: //a 1 //*/a n /k can be processed by initiating a range scan using the B+-Tree key k/a n , and evaluating each subsequent B+-Tree key for at least one element of any tag above a n and an element a 1 somewhere in the path above all the others.</p><p>With this structure a node must be associated with each path it contributes and consequently with each corresponding key in the B+-Tree. For example if node N con-tributes 10,000 different paths for tag k, a reference to N has to appear in 10,000 data items in the B+-Tree. Similarly, if a path is present on 10,000 nodes its associated data item must contain references to all these nodes. Size concerns stemming from these cases can be easily addressed by either clustering nodes with similar documents into groups or by simply compressing the node lists. Our study assumes that a scalable, efficient and reasonably sized index is available on each participating node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">System Evolution</head><p>System evolution, such as bootstrapping and node arrival and departures, is defined by the Chord protocol. The specification, however, refers only to the keys and not to the objects corresponding to those keys. It is up to the application to manage storage and retrieval of the objects corresponding to the Chord keys. This section describes how the distributed catalog service evolves when nodes join, leave and update their data, and how objects, which are the sets of data summaries, are stored and accessed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Arriving Nodes</head><p>Section 2 outlines a model according to which nodes create the data they provide to the P2P network. Each new node N n that joins the system creates the set C n which makes the data of N n query-able by the nodes already in the system. First N n contacts any node N c in the system (Step 1, Figure <ref type="figure" target="#fig_1">3</ref>). Chord finds N n 's successor N s in the identifier ring. The new node N n , now part of the identifier ring, injects each (k nj , S nj ) ∈ C n into the system and the Chord protocol decides which nodes should receive the new catalog information (Step 2). Since uploading N n 's catalog information may take some time, not all the data on C n becomes immediately query-able by existing nodes in the system. Only after all the summaries in C n have found their way to their hosts is N n 's entire data visible to other nodes in the system.</p><p>Additionally, N n becomes part of the catalog infrastructure and is available to share the load of the catalog service by hosting parts of the summary sets already in the network. The Chord protocol will assign to N n keys k j for which N n is the successor in the identifier ring. These keys are located on N s (k 1 and k 2 , Figure <ref type="figure" target="#fig_1">3</ref>). Our design co-locates keys and summary sets in the interest of avoiding one extra network message during lookup. Therefore, it follows that when N n joins the system both keys and data summaries need to be moved from N s to N n (Step 3).</p><p>Moving the keys from one node to another happens much faster than moving the summaries, because the former are smaller. Thus, each k j for which N n becomes responsible initially points to N s until the corresponding data summary S ij is fully transferred to N n (Step 2). During this transitional period lookups will be relayed from N n to N s . Figure <ref type="figure" target="#fig_1">3</ref> shows the time line of N n joining the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Updates and Departures</head><p>Catalog information stored in the system may need to be updated as the data on individual nodes changes. Update requests are handled in a similar way as insertion of information during join (Step 2, Figure <ref type="figure" target="#fig_1">3</ref>). Updates to summaries follow the "single writer/multiple readers" model; only a node that has created a data summary (the owner) is allowed to change its content. Nodes that store the data summaries do not alter their content, although they may alter the way they are stored.</p><p>When a node N decides to leave the system, it must hand over the catalog information to its successor according to the Chord protocol. Furthermore, it notifies the nodes that hold N's catalog data. To achieve this, N uses the keys it inserted into the system to find the nodes that currently hold N's catalog information. Another valid approach is one where N does not do anything upon leaving the system and lets the remaining nodes detect its absence over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Expected System Volatility</head><p>The system volatility, the rate at which nodes join and leave the system, is a key factor to consider when designing such a system. If this rate is expected to be high the design should avoid moving large amounts of data across nodes as part of system maintenance. DHTs generally try to minimize the amount of state stored on each node in order to be able to facilitate any level of volatility. Catalog information, which is moved across nodes, is not large, but its size would probably be an issue in an environment similar to those of other file-sharing systems like Gnutella and Napster, in which there is a high turnover of nodes. Our target, however, is a community of independent data providers that are interested in making their data widely query-able and therefore are highly unlikely to have the same behavior as individuals sharing music files. We expect that nodes, having joined, will leave only for scheduled maintenance and then rejoin. In this case they do not need to reinsert their catalog information into the system as they do during the initial join phase. Therefore, the goal is to achieve high throughput of catalog lookup requests and not to mitigate the impact of system volatility by minimizing the amount of catalog data stored on each node. Nevertheless, the dynamic maintenance algorithms of DHT designs are important even in a low volatility environment, since they allow new nodes to join in a scalable way.</p><formula xml:id="formula_3">N s Join Request k 1 , k 2 , k 3 N n N s N c N n k 1 , {S i1 } k 2 , {S i2 } k 3 , {S i3 } k 1 k 2 k 3 k 1 k 2 N n inserts (k nj , S ni ) 1 2 Identifier ring N s N n k 3 , {S i3 } k 1 k 2 k 3 k 1 , {S i1 } k 2 , {S i2 } 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Processing Catalog Queries</head><p>Given an XPath query, a catalog lookup determines which nodes in the system should receive the query. The system can handle general branching XPath queries. Consider the following example which is based on the sample network introduced in Section 2.2 and the query Q 2 : //book/[author="J. Smith"]/price. Q 2 retrieves the prices of books by author "J. Smith" and has two branches: Q 21 : //book/price Q 22 : //book/author ="J. Smith" Both branches must be satisfied and thus the query will be sent only to those nodes that have both paths in their repositories. Figure <ref type="figure">4</ref> shows the steps for each query branch. Q 2 is sent to N 4 (step1), where, based on price and Q 21 , the set N 21 ={N 2 , N 3 , N 4 } is produced. Q 2 and N 21 are then forwarded to N 1 , which is responsible for author elements (step 2). Q 22 contains a value predicate on author. Hence, both structural and value summaries are utilized to select relevant nodes. Assuming only N 2 has authors named "J. Smith", the lookup using Q 22 yields the set N 22 ={N 2 } (step 3). Finally, N 3 sends Q 2 to N 2 for execution (step 4), since it is the only node appearing in</p><formula xml:id="formula_4">N 21 ∩ N 22 .</formula><p>The class of XPath queries that the system can handle are of the form p = /a 1 [b 1 ]/a 2 [b 2 ]/…/a n [b n ] op value. Each b i is in turn a path. The path steps can include wildcards and the '//' navigation operator. The structural part of the XPath query is handled using the structural catalog information. The value predicate uses the value summaries. The implementation of query_parts selects the target tag name of each branch of the query. The algorithm for resolving this general query is as follows:</p><p>1: Extract all the N simple paths sp i from p. Simple paths have the form sp i =/a i1 /a i2 /…/a i,Mi op value (1&lt;i&lt;N). Let the set of candidate nodes be N = ∅. 2: Pick the next a i,Mi in the set T of targets of the simple paths. 3: Visit the node N c responsible for a i,Mi and retrieve the set of candidate nodes N i for sp i using the catalog information on N c . 4:</p><formula xml:id="formula_5">Set N = N ∩ N i , or N = N i if N = ∅. T = T -{a i,Mi } 5: If T ≠ ≠ ≠</formula><p>≠ ∅ go to 2. 6: N contains the nodes on which p should be submitted.</p><p>Caching catalog query results or even summaries on nodes that submit queries is feasible. Of course this would increase performance by offloading frequently contacted nodes, but also would raise cache maintenance and invalidation issues. Our study does not deal with caching catalog query results or data summaries since accessing the primary copy of a data summary is guaranteed to fetch the most up to date results. Incorporating caching as an additional layer over the present design is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Catalog Lookup Scalability Issues</head><p>Current implementations of DHTs balance the load across participating nodes by distributing keys uniformly under the assumption that the keys are accessed uniformly. Our system cannot be load balanced by relying solely on the provisions of the DHT layer because some elements will be used in queries more frequently than others. The result will be a higher processing load for catalog queries on the nodes that are responsible for frequently accessed elements, which invalidates the uniformity assumption of key accesses present in DHT designs. Furthermore, it can lead to scalability limits unless the processing load for popular elements is distributed across more nodes in the system. Even if the processing load of catalog lookups on nodes is negligible compared to the processing of the actual queries on each node, such lookups still consume resources such as available connections. This section describes techniques that allow the redistribution of the catalog query load dynamically based on the global query workload. It is assumed that nodes are willing assist with load balancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Structure Based Splitting</head><p>The last step in a branch of an XPath query is used as the lookup key for locating the corresponding summary set. When the request rate for a key exceeds a specific threshold, the affected node initiates a key split which forms new keys. This has the effect of transferring part of the load to other nodes. The following example illustrates how this technique works (Figure <ref type="figure" target="#fig_2">5</ref>). Consider the tag price and for illustration purposes, assume bike, car, boat Once the tag price has been split, query processing changes slightly. When some node N q submits a query q 1 =//car/price &lt; $10,000, q 1 will initially arrive on N which is responsible for price summaries. N will then respond to N q that price has been split and that new keys of level 2 should be used when submitting catalog lookups. N q will cache this information and will resubmit the catalog lookup request using the new key car/price. Eventually all nodes that request price catalog information will find out about the split and replace the key price with its corresponding level 2 key for subsequent queries. The level of a key indicates the number of path steps contained in it. Initially all keys are level 1.</p><formula xml:id="formula_6">N 1 N 2 N 3 N 4 Q 2 (1) Q 2 ,Q 21 , price (2) Q 2 ,Q 22 , author, {N 2 , N 3 , N 4 } Q 2 (4) execute Q 2 (3) {N 2 ,}<label>Figure</label></formula><p>There is, however, a class of queries that will not be able to use a level 2 key. Consider the query q 2 =//store[name="A"]//price &lt; $1000 submitted by node N q in which the parent of price is not defined. If N followed the split-replicate policy during the splitting of price, it will process q 2 itself. If N followed the split-toss policy, N q being aware of this fact will have to submit four catalog queries using the four possible keys and then merge the results from nodes N 1 , N 2 , N 3 and N 4 .</p><p>Query q 2 makes one trade-off between split-toss and split-replicate apparent: data replication versus more mes-sages per query. Another arises in when the split is no longer necessary because of changes in the global workload. Using split-replicate, any of the nodes N 1 , N 2 , N 3 and N 4 can safely discard its price summaries when it notices that it does not receive any significant traffic. Using, split-toss, however, all nodes involved in the split need to coordinate their actions.</p><p>In general, the structure-based splitting (SBS) algorithm takes as input the key k to split and the set K k = {(p, v, N)} where p is a path that leads to k, v is the value summary associated with k and N is the owner node of the pair (p, v). Note that k itself can be the product of a split. In this case the owner node is the node of the original unsplit key. The output of SBS is the set {(k i , K i )} and the mapping (k, {k i }). If a split is necessary but the key cannot be split, the solution is replication described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Replication</head><p>Another method for balancing the catalog query load is to replicate sets of summaries on other nodes. When a node N detects that queries of one of its keys k exceeds a specific rate, it contacts one or more nodes in the system and requests that they replicate the summary data for k. N also creates a mapping for k's new catalog data locations which it hands over to nodes that request lookups on k. A node N q queries the specified new locations in a round-robin fashion, after it is notified that key k has been replicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Splitting vs. Replication</head><p>There are several important differences between the two methods of load balancing. Replication is oblivious to the content of the data summaries and works with any summarization method. SBS exploits the structure summaries to create as many partitions as there are level 2 keys. Thus if there are a level 2 keys which contain the key k being split, the initial request r rate is roughly divided by (a + 1) in the case of split-replicate. Furthermore, if a new partition on a node is not receiving a significant rate of requests it can be safely discarded, if split-replicate is used. On the contrary, plain replication cannot, a priori, decide the optimal number of new replicas. One choice is to create only one replica on one other node in the system and let the new node replicate further if the rate of requests is still too high. The other choice is to create many replicas at once, which will cause the system to react to the increased load more rapidly. Another difference is that splitreplicate replicates the information only once per split. Also, a single update in the case of SBS needs to be propagated to only one other summary per split, whereas in the case of replication it must go to all replicas. Problems such as those described in <ref type="bibr" target="#b10">[11]</ref> are not an issue since only the owner of a data summary set requests updates to catalog information. In any case, replication is the last resort for load balancing if no further splitting is possible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Results</head><p>The goal of our experiments is to demonstrate the viability of our approach. Thus, certain traits of the DHT designs are taken for granted. For instance we do not evaluate how our system works under various degrees of volatility, or how different parameters of the underlying DHT affect the number of messages in the network. The focus is to facilitate scalable catalog lookups during query processing. A simulator is used to verify the scalability of catalog lookups in very large networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Experimental Data</head><p>Our goal is to use data that will be very similar to the data one could expect in a real world deployment of our catalog service. This can be achieved using data from potential data providers that would find the catalog service useful. Additionally, we opt for using XML data since this is the de facto standard for platform independent data representation and information exchange. The website www.xml.org contains a registry in which organizations that use XML data can register their schemas. We believe that this data is very close to reality and therefore we use it in our experiments. We found 30 usable schemas that draw from various domains such as Banking, Transportation, Arts and many others. The data collection contains 3500 unique element and attribute names and 16,000 different paths from root elements to leaves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1.">Data Generation</head><p>The purpose of data generation is to assign the available schemas to the nodes in the system. Each node N is assigned a number of schemas NS which is drawn from a normal distribution with mean MNS and standard deviations SNS. Each schema is chosen with probability proportional to a weight WS. Changing WS allows regulation of the popularity of each schema. For each chosen schema we create a node-specific schema as follows: All the paths that are required in the schema are also included in the node specific schema. The paths that are not required are included with probability PP, in order to create a more realistic situation. Each node must create data summaries to insert into the catalog service. To this end each possible path that leads to leaf is enumerated and, for each element and attribute found in the node's assigned schemas, a structural summary is built. Note that commonly used element names that appear in multiple schemas will contain paths from different schemas in their summaries. In our study we ignored namespaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">Query Load Generation</head><p>The query load is generated concurrently with data generation and is done in such a way that makes the generated queries follow the distribution of the generated data across all nodes. Each schema is assigned query credits QC which is proportional to WS. While paths are enumerated, each path is picked with some probability QP to contribute to the query pool. For each chosen path p the size of the XPath query it yields is between two and four 80% of the time (a similar assumption is made in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b18">[19]</ref>). The other 20% of the time the size of the query is uniformly distributed between one and the size of the path p. The target of the XPath query is chosen to be one element or attribute of path p as follows: The target is the eth item from the leaf with e exponentially distributed with mean 2. This gives preference to tags closer to the leaf nodes, which is more realistic. Once a query is created the QC corresponding to the schema is decreased by one. Thus, popular schemas contribute more queries to the query pool than less popular ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Simulator Architecture</head><p>The simulator used for evaluating our catalog service is based on the Chord protocol simulator found on the Chord project website. The DHT substrate is used unmodified and contains some additional optimizations (such as LRU finger tables) not found in <ref type="bibr" target="#b24">[25]</ref>. These optimizations further decrease the number of network messages required but do not affect our study. Each message between nodes incurs a network delay exponentially distributed with an average of 50ms. Nodes do not unexpectedly die and so all nodes that join are available for catalog queries. The average local processing time for Chord maintenance tasks is uniformly distributed between 50 ms and 200 ms.</p><p>An additional layer translates an XPath query Q to the necessary Chord lookups, based on the algorithms described earlier. Once the query reaches the node with the appropriate summary it is evaluated and the set N of candidate nodes that match the structure of the query is produced. To simulate the effect of value summaries a percentage (CT) of the nodes in N are discarded before N is sent back to the query origin. Upon receiving N the origin sends the XPath query to the nodes found in N.</p><p>Catalog related processing is simulated in more detail than Chord specific tasks. Each node simultaneously serves up to CR catalog requests using the processor-sharing discipline <ref type="bibr" target="#b13">[14]</ref> while additional requests are queued in a FCFS queue that can hold at most CQ requests. The time it takes to process each catalog request is 101 ms on average and follows the distribution of the measured times. Measurements were taken on a Pentium III at 800 MHz, running Linux 7.2 with an IDE disk on implementations of the structures presented in Section 3 using the data generated. These numbers represent the time it takes if a request is processed alone and are increased according to the processor-sharing discipline. The time for XPath queries was set to be uniformly distributed with a mean of 500 ms and a standard deviation of 500 ms and represents the background load on the system.</p><p>The system is driven by simulated users, who pick a query from the query pool and submit it on a random node in the system. The number of users U is a multiple of the number of nodes in the system so that the system becomes loaded. Each user submits a new query after all generated XPath queries have finished executing and after a think time of 5 seconds and a typing time of 3 seconds as specified in the TPC benchmark specifications <ref type="bibr" target="#b25">[26]</ref>. Note that by setting the CT parameter to 100% no XPath queries are generated and thus the system runs in catalog-only mode. This setting simulates the case where each data provider uses a dedicated catalog processor or assigns a maximum bandwidth to catalog queries (in conjunction with appropriately setting CQ).</p><p>The discussion on load-balancing left the triggering of splitting or replication unspecified. Our opinion is that in a real system the rate of requests that will cause a load balancing reaction is a local decision. For our simulations however, the nodes in the system are all equivalent and there is a common policy for all of them. A load balancing action on a node is triggered once the number of requests has reached CR and new requests are put on the FCFS queue. This indicates that the node is receiving more requests than desired. To remedy the situation a key must be either split or replicated. The key chosen is the most frequently occurring key in catalog queries among the CR requests that are served using Processor Sharing. The load incurred by a load balancing action is also simulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Nodes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>All Setups MNS=5, SNS=5, PP=90%, WS=10 (for all schemas), QP=80%, CT=100%, CR=20, CQ=500 500 U=5,000, NQ=400,000 1000 U=10,000, NQ=800,000 2000 U=20,000, NQ=1,600,000 3000 U=30,000, NQ=2,000,000 5000 U=50,000, NQ=4,000,000</p><p>Table <ref type="table">3</ref>: Experimental parameters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Experiments</head><p>In our experiments we focus on processing catalog queries in a system that has formed, stabilized. Updates are not considered since the update traffic is assumed to be orders of magnitude less intense than the query traffic and so does not affect load balancing. Furthermore, the performance of data summarization techniques is not tested since it is not the focus of this paper. Replication (R) one replica at a time The values for the parameters described previously are taken from the Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1.">Performance</head><p>This section presents the performance of the catalog service for various settings. The response times measured show that using the DHT alone, without any provisions for adapting to the query workload does not scale (Graph 1). (All measured 95% confidence intervals are within at least 0.5% of the means). SR shows the best scalability characteristics when adapting to the query load, holding the average response time below 2.1 sec. The increase in response times for SR can be attributed to an increase in the average hops per catalog request (Table <ref type="table" target="#tab_4">4</ref>) and to the increased load because of insertions caused by splits. ST does not scale as well because it discards the split summary set. This causes the generation of more messages per query and more splits (Graph 6), thus increasing response time. Replication performs better than ST but worse than SR since it cannot adapt quickly to the query load because only one replica is created each time a node reaches the request limit CR. SR splits a key as many times as there are next level keys and so distributes the overload faster. In both SR and ST there were no cases where splitting a key was not possible. The throughput results in Graph 2 once again show that Chord alone cannot handle the increasing rate of requests as the system grows because some nodes are overloaded. In the 500 node case SR, ST and R achieve the maximum throughput rate which is 624 queries/sec. To achieve this, only a very small number of splits and replications were required. SR also works very well for all five sizes of networks. Both ST and R do not perform as well as SR in the large networks of 2000, 3000 and 5000 nodes. The performance of R vs. ST is, however, reversed relative to the response time numbers. The explanation is that, on average, ST generates more requests and splits. However, it reacts to the increased rate on the large networks faster than R because more new keys are generated per split. Table <ref type="table" target="#tab_6">5</ref> shows the number of catalog lookups rejected by nodes in the system because they were overloaded. Some individual nodes drop more than 50% of the requests if no load balancing takes place. In any case, using SR provides double the throughput of C without any significant loss of requests and with at least a five times improvement in response times.   Obviously, if no actions are taken, the distribution of requests across nodes is not balanced. Therefore load balancing is necessary for scalability. Using splitting and replication the system achieves a fairer distribution of requests which reflects in better throughput and response times (Section 7.3.1). Using load balancing on the 500 node network leaves the first 50 nodes handling about 27% of the query load. While not ideal, the redistribution seems adequate to handle the overall rate of requests, which is 624 queries/sec. If the rate were higher more load balancing actions would distribute the load more evenly. This is evident in the larger network configurations. The higher request rate causes more splits and replications and thus the first 50 nodes receive only about 11% and 7% of the users' requests in the 2000 and 5000 node networks respectively. It is important to note that strategy ST generates about 1.5-2.5 times more catalog requests than SR because it discards the split summary. As expected the network of 500 nodes required the fewest splits since the nodes in it are subjected to the lowest request rate of all networks. Furthermore, as stated, the ST strategy incurs more splits than SR, since in the former, the initial summary is discarded and all the requests that could be answered using the initial summary go to the split replicas. This causes higher level splitting. In the 5000 node case ST causes almost twice as many splits as SR. Graph 6 shows the increasing number of load balancing actions for SR and R as the networks grow larger. In order to keep up with the request rate the system adapts to the query load.</p><p>The number of new keys created by the load balancing methods is an indicator of how fast each policy reacts to the query load characteristics. Consider the case of 2000 nodes: R causes 149 replications. This corresponds to 149 new keys. SR causes just 111 splits but creates 1793 new keys, which balances the query load faster. The small number of splits relative to the total number of elements and attributes clearly shows that dynamic splitting is preferred to splitting keys in advance. Note that in the 5000 node configuration, SR creates 5300 new keys with only 415 splits. Cascading effects were also observed when nodes became overloaded as a result of accepting new keys. Cascading, however, stopped once requests were dispersed across a sufficient number of nodes.</p><p>To conclude, our experiments indicate that SR is the most suitable strategy for load balancing a distributed catalog based on our framework. It achieves good scalability, fast reaction to the query load characteristics while incurring a small overhead for creating new keys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4.">Alternate Query Load</head><p>We experimented with a different query workload in which elements at all levels are the targets of paths with equal probability. The results are similar to those obtained with the previous workload which favors deeper paths. As expected the distribution of the load is slightly less skewed leading to a smaller number of load balancing actions. The relative performance of C, SR, ST and R remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related Work</head><p>Distributed databases ( <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b27">[28]</ref>) use catalogs to store fragmentation information. Each site has its own replica of the catalog and determines where to execute a query or parts of a query. The fact that all the sites of a DDBMS are under the control of a single authority makes this design feasible since the number of nodes in the system is not large. The problem our design tries to address is different in that there is no central authority, no controlled data fragmentation and a much larger number of data providers. More recent distributed query processor designs such as the one in <ref type="bibr" target="#b3">[4]</ref> recognize the need for scalable catalog services and advocate distributed catalogs. A distributed catalog proposal based on multiple hierarchies can be found in <ref type="bibr" target="#b19">[20]</ref>. This approach is not automatic since it relies on manual extraction of categories from the data.</p><p>Our work builds upon recent peer-to-peer DHT protocols that were inspired by earlier pioneers such as FreeNet and Gnutella ( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>). They guarantee a definite answer to a lookup query within a bounded number of network hops, while Gnutella and FreeNet and other peer-to-peer studies opt for returning the first 10 or 100 matches to a query, if any, are found without any guarantees. The difference of the earlier proposed LH* <ref type="bibr" target="#b15">[16]</ref> from current DHT designs is that it allows a hash table to grow by expanding on servers taken from a large preexisting pool, rather than allowing nodes to join and become part of an existing distributed hash table.</p><p>Distributed file systems based on DHT protocols can be found in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b22">[23]</ref>. All those studies recognize the need for performing additional load balancing on top of the DHT layer to achieve scalability. They resort to replication of popular files across nodes in the system, assuming that sufficient storage is available.</p><p>The study in <ref type="bibr" target="#b11">[12]</ref> advocates traditional query processing techniques over data stored in the DHTs. In contrast, our approach uses the DHTs for storing metadata, which it uses to guide queries to the relevant data sources.</p><p>Other studies of peer-to-peer systems ( <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b29">[30]</ref>) use metadata on each peer to efficiently route searches to other peers or answer searches on behalf of other peers in the network. Their query satisfaction criterion (first 100 results render a query satisfactorily answered) is, however, geared more towards the need of file sharing individuals.</p><p>Sun's JXTA Search <ref type="bibr" target="#b26">[27]</ref> provides searches of data sources that actively produce data (such as news sites). The system builds indices on the queries a data source can answer and distributes them across JXTA hubs to which data sources connect. Thus, the way catalog information is distributed across the hubs is determined by where the data providers connect. It is anticipated that providers of similar topics will connect to the same hubs. In our case catalog information location is independent of where providers join the system.</p><p>DNS <ref type="bibr" target="#b1">[2]</ref> is a distributed hierarchical catalog service which is widely used. The naming service it provides is very a similar concept to our mapping of queries to data repositories. The analogy is that our system identifies relevant servers from a query instead of a symbolic name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions</head><p>We presented a design based on established technology that allows the implementation of completely decentralized catalog services for large numbers of nodes. In addition to leveraging existing technology we identified application-specific circumstances that require enhancements in the form of load-balancing in order to achieve scalability. Using simulation we demonstrate that our approach is valid and has good scalability characteristics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Chord identifier ring</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Node N n joining the network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Splitting of price</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Graph 1 :</head><label>1</label><figDesc>Average response times for catalog queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Graph 2 :</head><label>2</label><figDesc>Combined throughput of queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Graph 6 :</head><label>6</label><figDesc>Number of load balancing actions for SR (all levels) and R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Nodes with sample data</head><label>1</label><figDesc></figDesc><table><row><cell>N 1 library/catalogs/book/author,</cell></row><row><cell>library/reservation/book/author</cell></row><row><cell>N 2 bookstore/book/price, bookstore/book/author</cell></row><row><cell>N 3 bookstore/book/price, bookstore/book/author</cell></row><row><cell>N 4 bookstore/book/price, bookstore/book/author</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Part of the DHT index on each node 3. Data Summaries</head><label>2</label><figDesc></figDesc><table><row><cell>The extraction of the data summaries S ij from the set D i is</cell></row><row><cell>the next step in the design process. Considering how the</cell></row><row><cell>summaries could be used can provide insights into how</cell></row><row><cell>they should be extracted from the data on each node. As-</cell></row><row><cell>sume that our input queries are XPath expressions and the</cell></row><row><cell>data on the participating nodes are XML documents. Con-</cell></row><row><cell>sider the path expression //book//price. Assuming that</cell></row><row><cell>book is chosen as the lookup key for this path, the set of</cell></row><row><cell>data summaries {S i,book } should enable the catalog to find</cell></row><row><cell>nodes that have documents in which price is a descendant</cell></row><row><cell>of book. Consequently data summaries should contain</cell></row><row><cell>descendant information for book. Alternatively, if price is</cell></row><row><cell>used as the lookup key, ancestor information is required.</cell></row><row><cell>Besides structural summaries, summarizing values is</cell></row><row><cell>equally important. Consider the example query</cell></row><row><cell>//book[category = "Peer-to-Peer"]. If there are 100,000</cell></row><row><cell>nodes that store book information but only 20 of them</cell></row><row><cell>specialize in "Peer-to-Peer" books, it would be much</cell></row><row><cell>more efficient to send such a query only to the 20 relevant</cell></row><row><cell>nodes and not to all nodes that have books.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>4: Processing strategy for Q 2 and</head><label></label><figDesc>house are its possible parent elements. Let N hold the data summaries for price. Once N detects that requests for price exceed its capacity for catalog requests it initiates a split of price. Thus, N creates four new keys: k 1 = bike/price, k 2 = car/price, k 3 = boat/price and k 4 = house/price. Then N creates a new set of data summaries by appropriately splitting the existing data summaries of price. The new keys k 1 , k 2 , k 3 , k 4 and the corresponding partial summaries are handed to the DHT layer and eventually end up on nodes N 1 , N 2 , N 3 and N 4 respectively. Care is taken that N does not receive any of the new keys. Finally, N has two choices for what to do with the initial data summaries for price: it keeps them (splitreplicate) or it discards them (split-toss). Both approaches are valid since, combined, the new summaries contain the same information as the old summaries. Updates to the price summary requested by price's owners have to be propagated to the affected new summaries in either case.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 : Average number of network hops per request</head><label>4</label><figDesc></figDesc><table><row><cell>Number of Nodes</cell><cell>500</cell><cell cols="4">1000 2000 3000 5000</cell></row><row><cell>Average hops per request</cell><cell>3.5</cell><cell>4.1</cell><cell>5.0</cell><cell>5.7</cell><cell>7.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : Droped requests across all configurations 7.3.2. Data Summary Sizes</head><label>5</label><figDesc></figDesc><table><row><cell>The increased performance observed using load-balancing</cell></row><row><cell>comes at a very low cost in terms of storage using the</cell></row><row><cell>path index described in Section 3.2. The average size of</cell></row><row><cell>the catalog on each node prior to load balancing actions is</cell></row><row><cell>about 12KB-15KB (for all approaches). The maximum</cell></row><row><cell>size of the catalog information on any node for various</cell></row><row><cell>network sizes is shown in the following table</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating the Selectivity of XML Path Expressions for Internet Scale Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aboulnaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Albitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DNS and BIND</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Space/time trade-offs in hash coding with allowable errors</title>
		<imprint>
			<date type="published" when="1970-07">July 1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ObjectGlobe: Ubiquitous query processing on the Internet</title>
		<author>
			<persName><forename type="first">R</forename><surname>Braumandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seltzsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="71" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What are Real DTDs Like</title>
		<author>
			<persName><forename type="first">B</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Routing Indices for Peer-to-Peer Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDCS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wide-area cooperative storage with CFS</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SOSP</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="http://www.freenetproject.org" />
		<title level="m">FreeNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="http://gnutella.wego.com/" />
		<title level="m">Gnutella Resources</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Processing Queries in a Large Peer-to-Peer System</title>
		<author>
			<persName><forename type="first">L</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Jeffery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAiSE</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Dangers of Replication and a Solution. Readings In Database Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Helland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O' Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>3 rd edition p372</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Complex Queries in DHT-based Peer-to-Peer Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huebsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Histogram-Based Solutions to Diverse Database Estimation Problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Poosala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Queueing Systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kleinrock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Theory, John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">OceanStore: An Architecture for Global-Scale Persistent Storage</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASPLOS</title>
		<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LH* -Linear Hashing for Distributed Files</title>
		<author>
			<persName><forename type="first">W</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neimat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Napster</surname></persName>
		</author>
		<ptr target="http://www.napster.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Principles of Distributed Database Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Structure and Value Synopses for XML Data Graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Garofalakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distributed Query Processing and Catalogs for Peer-to-Peer Systems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papadimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tufte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Scalable Content-Addressable Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications</title>
		<meeting>of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable, distributed object location and routing for large-scale peer-to-peer systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><surname>Pastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFIP/ACM Intl. Conference on Distributed Systems Platforms</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SOSP</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploring the Design Space of Distributed and Peer-to-Peer Systems: Comparing the Web, TRIAD, and Chord/CFS</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saroiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gribble</surname></persName>
		</author>
		<idno>IPTPS &apos;02</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCOMM</title>
		<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><surname>Tpc-C</surname></persName>
		</author>
		<title level="m">Benchmark Standard Specification Revision 5.0</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">JXTA Search: Distributed Search for Distributed Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Waterhouse</surname></persName>
		</author>
		<ptr target="http://search.jxta.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">R*: An Overview of the Architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Obermarck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wilms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yost</surname></persName>
		</author>
		<idno>RJ3325</idno>
		<imprint/>
	</monogr>
	<note type="report_type">IBM Research Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Language</forename><surname>Xml Path</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/xpath20/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Search in peer-to-peer networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDCS</title>
		<meeting>ICDCS</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Designing a Super-Peer Network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tapestry: An Infrastructure for Fault-tolerant Wide-area Location and Routing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<idno>UCB/CSD-01-1141</idno>
		<imprint/>
	</monogr>
	<note type="report_type">UCB Tech. Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
