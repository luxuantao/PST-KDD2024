<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EasyMKL: a scalable multiple kernel learning algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Aiolli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics Via Trieste</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>63</addrLine>
									<postCode>35121</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michele</forename><surname>Donini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics Via Trieste</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>63</addrLine>
									<postCode>35121</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EasyMKL: a scalable multiple kernel learning algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F25973BC5A407F7BFD18E94B5611DFC</idno>
					<idno type="DOI">10.1016/j.neucom.2014.11.078</idno>
					<note type="submission">Received 1 July 2014 Received in revised form 18 October 2014 Accepted 5 November 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multiple kernel learning Kernel methods Feature selection Feature learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of Multiple Kernel Learning (MKL) is to combine kernels derived from multiple sources in a data-driven way with the aim to enhance the accuracy of a target kernel machine. State-of-the-art methods of MKL have the drawback that the time required to solve the associated optimization problem grows (typically more than linearly) with the number of kernels to combine. Moreover, it has been empirically observed that even sophisticated methods often do not significantly outperform the simple average of kernels. In this paper, we propose a time and space efficient MKL algorithm that can easily cope with hundreds of thousands of kernels and more. The proposed method has been compared with other baselines (random, average, etc.) and three state-of-the-art MKL methods showing that our approach is often superior. We show empirically that the advantage of using the method proposed in this paper is even clearer when noise features are added. Finally, we have analyzed how our algorithm changes its performance with respect to the number of examples in the training set and the number of kernels combined.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large margin kernel based algorithms are recognized state-ofthe-art algorithms for data mining applications. Besides the good performance they generally offer, the definition of a kernel allows a convenient way to easily inject into the classifier any available background knowledge one can have about a particular domain. Any positive definite kernel matrix implicitly specifies an inner product in a Hilbert space where large-margin techniques can be used for learning.</p><p>Recently, there has been a growing interest of researchers devoted to investigate on how these kernels can be learned from data. This is a big challenge and can potentially lead to significant improvements on a classifier. It is in fact quite established that the choice of the right features (i.e. the kernel) dramatically influences the performance of a classifier more than the classifier itself.</p><p>Kernel learning is a paradigm which is often adopted within a semi-supervised learning setting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The goal of kernel learning is to learn the kernel matrix using available data (labeled and possibly unlabeled examples) optimizing an objective function that enforces the agreement between the kernel and the set of i.i.d. labeled data, e. g., by maximizing their alignment <ref type="bibr" target="#b2">[3]</ref>. On the other hand, unlabeled data are typically used to regularize the generated models by constraining the discriminant function to be smooth (that is, it should not vary too much on similar examples).</p><p>Multiple Kernel Learning (MKL), see for example <ref type="bibr" target="#b3">[4]</ref> for a recent and quite exhaustive survey, is a popular paradigm used to learn kernels. The kernel computed by these techniques are (generally linear) combinations of previously defined weak kernels. The main rationale behind this kind of methods is that they can alleviate the effort of the user on defining good kernels for a given problem. Using the MKL framework, the algorithm itself can be able to select the best combination among a battery of predefined and reasonable weak kernels.</p><p>In this paper, we focus on multiple kernel learning with positive and linear combination parameters, that is, MKL in the form</p><formula xml:id="formula_0">K ¼ X R r ¼ 1 η r K r ; η r Z 0:</formula><p>This typology of algorithm is based on several theoretical results that bound the estimation error (i.e. the difference between the true error and the empirical margin error). These bounds exploit the Rademacher complexity bounds for the linear combination of kernels <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>According to <ref type="bibr" target="#b3">[4]</ref>, the majority of existing MKL approaches can be divided into the following two categories. Fixed or Heuristic rule techniques apply some fix rule, like simple summation or product of the kernels, or simple heuristic to find the combination parameters. The result usually obtained by these methods is scalable with respect to the number of kernels combined but their effectiveness will critically depend on the domain at hand. On the other side, Optimization based approaches learn the combination parameters by solving an optimization problem that can be integrated in the learning machine (e.g. structural risk based target function) or formulated as a different model (e.g. alignment, or other kernel similarity maximization).</p><p>Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom Generally speaking, best performing approaches to MKL necessitate of complex optimization including semidefinite programming (SDP), or quadratically constrained quadratic programming (QCQP), just to name a few, thus making these approaches unpractical when the number of kernels to combine is large, say in the order of hundreds. Further, these methods often need to keep in memory the whole set of weak kernels, or at least the submatrices corresponding to the training data pairs. This will exhaust the memory soon when coping with hundreds of examples and hundreds of weak kernels.</p><p>The same MKL idea can be used in two different scenarios. In the first and more popular case, a small number of strong kernels have to be combined each representing a different (possibly orthogonal) view of the same task. Typically these kernels are individually well designed by experts and their optimal combination hardly leads to a significant improvement of the performance with respect to, for example, a simple averaging combination. Alternatively, the MKL paradigm can be exploited to combine a very large set of weak kernels aiming at boosting their accuracy. In this way, the final combination will represent a weighted combination of the different subsets of features. In this paper, we focus on this second approach because, we think, at least in principle, it can really boost the performance as it basically performs a datadriven feature learning/selection/weighting.</p><p>With this second view in mind, we see that for MKL to be effective we need to combine many kernels. For this reason, being non-scalable becomes a stringent issue for a MKL method. On the other side, simpler fixed rule algorithms are definitely more scalable in general but they are far less flexible and tend to become ineffective when coping with many kernels and noise. In this paper, we propose a very efficient algorithm which is able to cope with thousands of kernels efficiently and we empirically demonstrate the effectiveness of the methods proposed that almost always is more accurate than the baselines especially when noise is present in the weak kernels.</p><p>A summary of the paper is the following. In Section 2 we show the notation used in this paper for classification problems. In Section 3, the Kernel Optimization of the Margin Distribution (KOMD) classification algorithm, is presented. In Section 4 the proposed multiple kernel learning algorithm, namely EasyMKL, is presented. In Section 5, experiments are presented with respect to different dimensions, namely, the effectiveness, the scalability, and the stability of the method proposed against different baselines and state-of-the-art methods. Finally, in Section 6 we draw conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Notation</head><p>Throughout this paper, we consider a classification problem with training examples defined by fðx 1 ; y 1 Þ; :::;ðx l ; y l Þg, and test examples defined by  </p><formula xml:id="formula_1">fðx l þ 1 ; y l þ 1 Þ; …; ðx L ; y L Þg; x i A R m ; y i A fÀ1; þ 1g. We use X A R LÂm</formula><formula xml:id="formula_2">Γ ¼ γ A R l þ j X i A È γ i ¼ 1; X i A ⊖ γ i ¼ 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Playing with margin and the KOMD algorithm</head><p>In <ref type="bibr" target="#b6">[7]</ref> a game theoretic interpretation as a two-player zero-sum game has been proposed for the problem of margin maximization in a classification task. Specifically, the classification task has been split into two phases. Firstly, in the ranking phase, a total order of the examples is introduced. Secondly, the pure binary classification is performed by applying a threshold to the ranking of the examples obtained in the first phase.</p><p>In particular, in the ranking phase, the task is to learn a unitary norm vector w such that w &gt; ðϕðx È ÞÀϕðx ⊖ ÞÞ 4 0 for most of positive (x È ) and negative (x ⊖ ) instance pairs in the training data. The game basically consists of one player that has to choose one vector of unitary norm w and the other that picks pairs of positive-negative examples according to two distributions γ þ and γ À over the positive and negative examples, respectively. The value of the game is the expected margin obtained, that is</p><formula xml:id="formula_3">w &gt; ðϕðx p ÞÀϕðx n ÞÞ; x p $ γ þ ; x n $ γ À .</formula><p>The goal of the first player is to maximize this value while the second player wants to minimize it. This problem is equivalent to the hard SVM and can be solved efficiently by optimizing a simple linearly constrained convex function on variables γ A Γ , namely, minimize</p><formula xml:id="formula_4">γ A Γ γ &gt; Ŷ K Ŷγ |fflfflfflfflfflffl{zfflfflfflfflfflffl} DðγÞ :</formula><p>It can be seen that the vector γ n A Γ minimizing DðγÞ identifies the two nearest points in the convex hulls of positive and negative examples, respectively, in the feature space of the kernel K.</p><p>Furthermore, a quadratic regularization over γ is introduced, namely, RðγÞ ¼ γ &gt; γ, that makes the player to prefer optimal distributions (strategies) with low variance. In fact, let p (resp. n) be the number of positive examples (resp. negative examples) in the training set, then E½γ þ ¼ 1=p and E½γ À ¼ 1=n is always true by construction, and E½v denotes the expected value of elements in a vector v.</p><formula xml:id="formula_5">It follows that Varðγ þ Þ ¼ E½γ 2 þ ÀE½γ þ 2 ¼ ‖γ þ ‖ 2 À p À 2 Varðγ À Þ ¼ E½γ 2 À ÀE½γ À 2 ¼ ‖γ À ‖ 2 À n À 2 ; obtaining RðγÞ p Varðγ þ ÞþVarðγ À Þ.</formula><p>The final best strategy for γ will be given by solving the optimization problem min γ A Γ ð1 À λÞDðγÞþλRðγÞ: The regularization parameter λ has two critical points: λ ¼ 0 and λ ¼ 1. When λ ¼ 0, as we have shown before, there is no regularization, so the solution is the same as the hard SVM one, whereas when λ ¼ 1 the minimization problem reduces to min</p><formula xml:id="formula_6">γ A Γ RðγÞ</formula><p>and the optimal solution is analytically defined by the vector of uniform distributions over positive and negative examples, that is,</p><formula xml:id="formula_7">γ u i ¼ 1=p when y i ¼ þ1</formula><p>, and γ u i ¼ 1=n when y i ¼ À1. In this case, the objective solution is the squared distance from the positive and negative centroids in feature space. The external parameter λAð0; 1Þ allows us to select the correct trade-off. Clearly, a correct selection of this parameter is fundamental if we are interested in finding the best performance for a classification task and this is usually made by validating on training data. In Fig. <ref type="figure" target="#fig_2">1</ref>, an example of the solutions found by the above algorithm for a toy problem varying the value of λ is depicted.</p><p>Once the model is learned from training data, the evaluation on a new generic example x is obtained by</p><formula xml:id="formula_8">f ðxÞ ¼ w &gt; ϕðxÞ ¼ X i y i γ i Kðx i ; xÞ ¼ K tr ðxÞ Ŷγ;</formula><p>where K tr ðxÞ ¼ ½Kðx 1 ; xÞ; …; Kðx l ; xÞ &gt; , i.e. the vector containing the kernel values with the training examples for x.</p><p>When a pure binary classification is required, which is not the case in our work, then the second phase of the algorithm is also performed. The threshold is set corresponding to the point standing in the middle between the optimal point in the convex hull of positive examples and the one in the convex hull of negative examples, that is</p><formula xml:id="formula_9">θ ¼ 1 2 X i A È γ i f ðx i Þþ X i A ⊖ γ i f ðx i Þ ! ¼ 1 2 X i γ i f ðx i Þ ¼ 1 2 γ &gt; K Ŷγ:</formula><p>Note that, when λ ¼ 0 this choice corresponds exactly to the optimal hyperplane of SVM. Finally, a new example x will be classified according to signðf ðxÞÀθÞ.</p><p>In the following we will refer to the first phase of the algorithm discussed in this section as KOMD (Kernelized Optimization of the Margin Distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EasyMKL</head><p>In MKL we want to find the best combination parameters for a set of predefined kernel matrices. In our context, this is done by learning a vector of coefficients η that forms the combined kernel according to</p><formula xml:id="formula_10">K ¼ X R r ¼ 1 η r K r ; η r Z 0:</formula><p>Clearly, we must restrict the possible choices of such a matrix K and this can be made by regularizing the learning process. So, we pose the problem of learning the kernel combination as a minmax problem over variables γ and η. Specifically, we propose to maximize the distance between positive and negative examples with a unitary norm vector η as the weak kernel combination vector, that is max</p><formula xml:id="formula_11">η: J η J ¼ 1 min γ A Γ ð1 À λÞγ &gt; Ŷ X R r η r Kr Ŷγ þ λ‖γ‖ 2 |fflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl{zfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflfflffl} Q ðη;γÞ :</formula><p>Considering dðγÞ the vector with the rth entry defined as</p><formula xml:id="formula_12">d r ðγÞ ¼ γ &gt; Ŷ Kr Ŷγ;</formula><p>we obtain that Q ðη; γÞ is equal to ð1 À λÞη &gt; dðγÞþλ‖γ‖ 2 2 and we can rewrite the original problem as min</p><formula xml:id="formula_13">γ A Γ max η: J η J ¼ 1 Q ðη; γÞ ¼ min γ A Γ max J η J ¼ 1 ð1 À λÞη &gt; dðγÞþλ‖γ‖ 2 2 :<label>ð1Þ</label></formula><p>Now, it is possible to see that the vector η n maximizing the function Q ðη; γÞ above has a simple analytic solution:</p><formula xml:id="formula_14">η n ¼ dðγÞ ‖dðγÞ‖ 2 :</formula><p>Plugging this solution into the min-max problem, we obtain min</p><formula xml:id="formula_15">γ A Γ Q ðη n ; γÞ ¼ min γ A Γ ð1 À λÞ‖dðγÞ‖ 2 þ λ‖γ‖ 2 2 :</formula><p>The optimal γ will be the (regularized) minimizer of the 2-norm of the vector of distances. This minimizer is not difficult to find as this is a convex function though not quadratic. In order to simplify the problem further we prefer to minimize an upper-bound instead corresponding to the 1-norm of the vector of distances, thus obtaining min</p><formula xml:id="formula_16">γ A Γ ð1 À λÞ‖dðγÞ‖ 1 þ λ‖γ‖ 2 2 ¼ min γ A Γ ð1 À λÞγ &gt; Ŷ X R r Kr ! Ŷγ þλ‖γ‖ 2 2 :<label>ð2Þ</label></formula><p>Interestingly, the obtained minimization problem is the same as the KOMD problem where the kernel matrix has been replaced with the simple sum of the weak kernel matrices. This will turn out to be very important for the efficiency (especially in terms of space) of the method as will be explained later on. In the following, we refer to this algorithm as EasyMKL.</p><p>From a more theoretical point of view, modifying the original problem using the 1-norm upper-bound, in fact, we are changing the optimal solution η n in the initial problem (Eq. ( <ref type="formula" target="#formula_13">1</ref>)) with a new ηn , where</p><formula xml:id="formula_17">ηn ¼ η n ‖dðγÞ‖ 1 ‖dðγÞ‖ 2 :</formula><p>Consequently, the problem that we are solving is min</p><formula xml:id="formula_18">γ A Γ ð1 À λÞ ηn dðγÞþλ‖γ‖ 2 2 ¼ min γ A Γ ð1 À λÞη n ‖dðγÞ‖ 1 ‖dðγÞ‖ 2 dðγÞþλ‖γ‖ 2 2 :<label>ð3Þ</label></formula><p>From this new formulation, we note that, in fact, we have just added a multiplicative coefficient ‖dðγÞ‖ 1 =‖dðγÞ‖ 2 to the original optimal solution η n . From Hölder's inequality this coefficient is bounded by the number of kernels and we have that</p><formula xml:id="formula_19">1 r ‖dðγÞ‖ 1 ‖dðγÞ‖ 2 r ffiffiffi R p :</formula><p>The value of the ratio between the 1-norm and the 2-norm tends to 1 if the vector dðγÞ is very sparse, while, it tends to ffiffiffi R p when the values in the vector dðγÞ are similar. In other words, using the proposed formulation, we are promoting sparse solutions of the vector dðγÞ solving the minimum problem. Then, solving the problem in Eq. (3), the vector of the weight η n ¼ dðγÞ=‖dðγÞ‖ 2 will result sparser than the solution of the original problem. Clearly, Eq. ( <ref type="formula" target="#formula_18">3</ref>) has the same solution as the EasyMKL minimum problem in Eq. <ref type="bibr" target="#b1">(2)</ref>.</p><p>A final consideration we can do here is that the quality of a kernel does not change if it is multiplied by a positive constant. However, our formulation makes particularly clear that, if kernels with different traces are present in the combination, they have unequal impact in the MKL optimization problem. In fact, most of the bounds related to the difference between the true error and the empirical margin error (i.e. estimation error) change linearly with respect to the square root of the maximal trace among the combined kernels. Moreover, if we have that (T 4 0 such that T 2 Z K r ðx; xÞ 8r ¼ 1; …; R 8 x the estimation error (fixed a specific task and without constants) is OðTÞ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. Thus, if not differently motivated, different traces should be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head><p>In this section, the experiments we have performed to validate the EasyMKL approach are presented. First of all, an introduction of the experimental setting is given in Section 5.1. In Section 5.2 we show some results confirming that the maximization of the separation in feature space is a good criterion to pursue to obtain effective kernels. Moreover, an overview of the baselines and state-of-the-art methods is given in Section 5.3. In Section 5.4 we compare our algorithm against other methods with respect to the AUC over ranking tasks. In Section 5.5 we perform experiments concerning time and memory performance of EasyMKL. We tested the stability of EasyMKL by adding different amounts of noise in the data in Section 5.6. In Section 5.7 we analyze how the cardinality of the training set influences our algorithm. Finally, in Section 5.8 we describe experiments that we have performed to understand the difference in performance obtained by changing the number of weak kernels.  <ref type="table" target="#tab_0">1</ref>. Data have been scaled to the ½À1; þ 1 interval and the number of training examples selected for the training part is approximately 10% of the dataset. We decided to use relatively small training sets obtaining a final accuracy less influenced by the classifier (on which the combined kernel is applied) and more influenced by the real quality of a kernel. Further, we preferred having larger test sets and many different splits of each single dataset aimed at improving the significance of the overall evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setting</head><p>In this work we want to demonstrate that we can efficiently perform a kind of feature selection/weighting via MKL and this can be done by selecting a very large set of weak kernels each one individually defined using a small subset of the original features.</p><p>Let d A N and β 4 0 be two parameters, then the rth kernel is constructed by random picking of a bag (replica is allowed) of features F r , such that j F r j r d, and constructing an RBF based weak kernel defined according to</p><formula xml:id="formula_20">K r ðx i ; x j Þ ¼ ∏ f A Fr e À β=j Fr j ðx ðf Þ i À x ðf Þ j Þ 2 :</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Is data separation a good criterion to maximize?</head><p>In this section, we present results showing that the maximization of the distance between positive and negative examples is a good criterion to pursue as it is positively correlated with the AUC obtained in a binary ranking task.</p><p>In particular, we designed a simple greedy algorithm which constructs a series of kernels with monotonically increasing data separation. For reader's convenience, we recall the formula of separation:</p><formula xml:id="formula_21">min γ A Γ γ &gt; Ŷ K Ŷγ:</formula><p>The algorithm starts with a null kernel and considers sequentially the available weak kernels. At each iteration, the current kernel is updated with a weak kernel only if this addition increases the separation of training data. Note that, this algorithm corresponds to have η r A f0; 1g. The algorithm is described in detail in Appendix A. Fig. <ref type="figure">2</ref>, an example of a plot obtained applying the greedy algorithm on the Splice dataset, using 10,000 weak kernels and generated as described in Section 5.1 (d¼5), is given. This experiment clearly shows the strong correlation between the data separation and the AUC obtained in the test set using the produced kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Methods</head><p>We have then considered three baseline methods for our experiments. The first method SVM is the classical SVM trained with all the features. This is only reported for the sake of completeness and just to give an idea of the difficulty of the datasets. Note that, the feature space in this case is different from MKL methods and results are not comparable. The second method, called Random Kernel (Random), consists of random picking from available kernels, which is equivalent to set only one η r equal to 1 and all the others equal to 0. We decided to add this baseline as an indicator of how much information is brought from single kernels. Intentionally, the performance of this baseline could be really poor. Finally, the last baseline method is the Average Kernel (Average) where the same weight η r is given to all the available weak kernels. Despite its simplicity, it is known (see <ref type="bibr" target="#b10">[11]</ref>) that this is a strong baseline that beats other more advanced techniques quite often and can obtain very good results. We have then considered three state-of-the-art algorithms of MKL with SVM. All these three algorithms are in the optimization based family of MKL methods. A MATLAB implementation 1 of these methods by Mehmet Gönen and Ethem Alpaydin <ref type="bibr" target="#b3">[4]</ref> has been used.</p><p>Simple MKL (SMKL): An iterative algorithm by Rakotomamonjy <ref type="bibr" target="#b11">[12]</ref> that implements a linear approach with kernel weights in a simplex. Basically SMKL works by repeating two main steps: ○ A SVM optimization problem defined on current weights. ○ Updating of the kernel weights using a gradient function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalized MKL (GMKL):</head><p>The second algorithm, by Varma and Babu <ref type="bibr" target="#b12">[13]</ref>, is called generalized multiple kernel learning (GMKL). GMKL exploits a nonlinear approach and tackles the problem with a technique that regularizes both the hyperplane weights and the kernel combination weights. This algorithm tries to optimize a non-convex problem, different from SMKL but, in general, with better results <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lasso-based MKL (GLMKL):</head><p>The third algorithm used as baseline is called Lasso-based MKL algorithm by Kloft <ref type="bibr" target="#b13">[14]</ref> and Xu <ref type="bibr" target="#b14">[15]</ref>. GLMKL considers a regularization of kernel weights with 1norm and finds the kernel weights by solving a small QP problem at each iteration.</p><p>SMO Projected Gradient Descent Generalized MKL (SPG-GMKL):</p><p>The last algorithm is inspired by the GMKL and learns simultaneously both kernel and SVM parameters. This particular algorithm <ref type="bibr" target="#b15">[16]</ref> exploits an efficient and highly scalable implementation and is a state-of-the-art method with respect to the computational performance.</p><p>The validation of the methods was performed before each type of experiment. A subset of examples (validation set) was selected for each method and dataset and was only used to select the best hyperparameters. The kernel machines (SVM and KOMD) were validated using the same subset of data to obtain a fair comparison. The performance of SVM and KOMD was similar in most of the cases, thus confirming the findings in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">AUC comparisons</head><p>The quality of the different combined kernels has been evaluated by means of AUC (Area Under Curve), which measures how good the induced ranking order is with respect to the target classification task. In this way, we do not necessitate of a threshold setting which is an additional, and useless, degree of freedom when evaluating kernels performance. Weak kernels have been generated according to the method depicted in Section 5.1. A number of 10,000 weak kernels have been constructed for each dataset. Two different values for d, that is d A f5; 10g, have been used in the generation algorithm with the RBF parameter β 0 obtained by model selection on a standard SVM. The experimental setting is summarized below:</p><p>1. Repeat for r ¼1 to R ¼10,000: pick a random p A f1; …; dg and generate a weak kernel K r using p features picked randomly (with replacement). 2. Combine fK r g R r ¼ 1 with a MKL algorithm to obtain a kernel K. 3. Rank test examples using K and SVM (KOMD for the proposed method) and evaluate the quality of the ranking with the AUC metric.  The experiment with 500 kernels of SPG-GMKL is not reported as it exceeds the limit of 2 GB of memory.</p><p>The same experiments have been repeated 1000 times averaging the AUC results in order to improve the significance of the evaluation. Aiming at setting our experiments as fair as possible, the KOMD algorithm has been used with the kernel produced by our MKL method, while SVM has been used on all the others. For what concerns the SVM baseline, RBF kernels with all the features have been used in this case. The obtained results are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p><p>The results obtained with the MKL algorithms are summarized in Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref> (for d ¼5 and d ¼10) showing that standard MKL methods do not have performances significantly better than the simple kernel averaging. On the other side, EasyMKL has significantly better AUC in four out of six datasets. Also, EasyMKL has a small standard deviation (std) with respect to the other methods and this fact highlights the stability of our algorithm. This trend is confirmed in Table <ref type="table" target="#tab_4">5</ref> where the proposed method is always significantly better than the average baseline. Unfortunately, given the relatively large number of examples of the Gisette dataset, it was not possible to run state-of-the-art MKL methods on this data without running out the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Performance in time and memory</head><p>We have also performed experiments 2 to evaluate the computational time and the memory used by EasyMKL. We compared our algorithm with a state-of-the-art algorithm called SPG-GMKL <ref type="bibr" target="#b15">[16]</ref>. A Cþ þ implementation of SPG-GMKL provided by the authors 3 has been used. SPG-GMKL with this implementation is more than 100 times faster than SimpleMKL <ref type="bibr" target="#b15">[16]</ref>. The Splice dataset has been selected for this experiment with 100 training examples. A variable number of RBF kernels have been generated with a parameter β picked randomly in ð0; 1Þ and using all the 60 features of the Splice dataset. We have studied the performance in time and memory, fixed an upper bound of 2 GB for the memory. Results are reported in Fig. <ref type="figure">3</ref>. 2 For these experiments we used a CPU Intel Core i7-3632QM @ 2.20GHz 2.20GHz.</p><p>3 http://www.cs.cornell.edu/ $ ashesh/pubs/code/SPG-GMKL/download.html Based on our experiments, the time complexity of SPG-GMKL is linear with the number of kernels, with a constant of 0.15 s per kernel. EasyMKL has a linear increase in time too, with 0.17 s per kernel. The memory used by SPG-GMKL has a sub-linear growth with the number of kernels and has reached 2 GB with only 400 kernels. Vice versa, the memory used by our algorithm is independent from the number of kernels to combine. This is due to the fact that the optimization in our algorithm consists of a simple KOMD optimization on the sum of kernels. The computation of this sum of matrices can be easily implemented incrementally and only two kernel matrices need to be stored in memory. It follows that, with our method, we can use an unlimited number of different kernels with only a small memory requirement (e.g. 47 MB for the Splice dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Stability with noise features</head><p>In this set of experiments we injected artificial noise features, that is, features uncorrelated with the label of the example, to study the robustness of EasyMKL with respect to the Average Kernel method.</p><p>Similar to <ref type="bibr" target="#b16">[17]</ref>, given a dataset of L examples, each new noise feature h is created using three simple steps:</p><p>1. Pick a feature f randomly from the set of the original features. 2. Let F ¼ fx ðf Þ i : i ¼ 1; …; Lg be the list of values of f (with replica). 3. For each example x i with i ¼ 1; …; L, define the value of x ðhÞ i as a randomly picked value from F .</p><p>Using this method we have created features with a distribution that is similar to the original features but uncorrelated with the labels. We have generated noise features in different percentages of the original features. With these new datasets we have repeated the same experiments described in Section 5.4 with the RBF weak kernels created using d equals to 5 and 10 and three datasets: Splice, Batch2, and Mush. We have fixed the percentages of noise features p A f0; 50; 100; 150; …; 900; 950; 1000g. Finally, we compared EasyMKL against the Average Kernel algorithm.</p><p>The results are summarized in Fig. <ref type="figure" target="#fig_5">4</ref>. We can note that our algorithm obtains good results even with a 1000% of additional noise features and it outperforms the baseline algorithm. EasyMKL increases the gap from the baseline monotonically with respect to the increase of the percentage of noise features. However, quite surprisingly, these results confirm that the Average Kernel algorithm is still a very strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Performance with respect to different cardinalities of the training set</head><p>In this section we have analyzed how the size of the training set influences the performance of EasyMKL. We compared our results against the Average Kernel method, that is used as baseline. We performed these experiments over the Gisette dataset using the same experimental setting as in Section 5. From these results, it is clear that our algorithm outperforms the Average Kernel algorithm when the training set is sufficiently large. This is due to the fact that EasyMKL effectively uses the training set to learn which features are more important (see experiments in Section 5.6) and, when data are abundant, it can exploit this information to outperform the baseline. Not surprisingly, when the size of the training set is small the two algorithms perform similarly (and the standard deviation is too big to claim any result).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Performance changing the number of weak kernels</head><p>We also performed experiments analyzing the performance of EasyMKL against the Average Kernel method using different numbers of weak kernels. The kernels have been generated using the same setting described in Section 5.4. The number of generated kernels R was selected ranging from R ¼ 10 to R ¼ 200 (with a step of 10 kernels). We performed the experiments fixing d ¼ 5 (i.e. very weak kernels) and using two datasets: Splice and German.</p><p>The experiments have been repeated several times for each value of R and the results are reported in Fig. <ref type="figure" target="#fig_8">6</ref>. We can observe that EasyMKL outperforms the Average Kernel method and these experiments confirm that our algorithm also works well when combining a small number of weak kernels. In particular, EasyMKL obtains a steeper gain in performance with the first weak kernels (i.e. from 10 to 30 weak kernels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we have proposed the EasyMKL algorithm which is able to cope efficiently with a very large number of different kernels. The experiments we reported have shown that the proposed method is more accurate than MKL state-of-the-art methods. We have also discussed time and memory requirements with respect to the number of combined kernels showing that EasyMKL uses only a constant amount of memory and it has only a linear time complexity. Finally, EasyMKL seems also quite robust with respect to the noise introduced by features which are not informative and works well even if used with a small number of weak kernels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>to denote the matrix where examples are arranged in rows and y A R L the vector of labels. The matrix K A R LÂL denotes the complete kernel matrix containing the kernel values of each (training and test) data pair. Further, we indicate with an hat, like for example X A R lÂm ; ŷ A R l , and K A R lÂl , the submatrices (or subvectors) obtained considering training examples only. Given a training set, Γ will denote the domain of probability distributions γ A R l þ defined over the sets of positive and negative training examples. More formally:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>:</head><label></label><figDesc>Note that any elementγ A Γ corresponds to a pair of points, the first in the convex hull of positive training examples and the second in the convex hull of negative training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. KOMD solutions of the first phase found using different λ in a simple toy classification problem. The ranking among the examples is performed evaluating the orthogonal projection of the examples over the line defined by the solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Experiments have been performed comparing the proposed methods with other state-of-the-art MKL algorithms with respect to accuracy and computational performance. A total of 7 benchmark datasets with different characteristics and 7 different MKL methods have been considered. Namely, the datasets considered are Diabetes [8] (8 features, 768 examples), Australian [8] (14 features, 690 examples), German [8] (24 features, 1000 examples), Splice [8] (60 features, 1000 examples), Batch2 [9] (128 features, 1244 examples), Mush [8] (112 features, 2000 examples), Gisette [10] (5000 features, 13,500 examples), that Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>7 0: 11 %Fig. 3 .</head><label>113</label><figDesc>Fig.3. Time and memory used by SPG-GMKL and EasyMKL with different amounts of kernels combined. The kernels are created using all the features of the Splice dataset. The experiment with 500 kernels of SPG-GMKL is not reported as it exceeds the limit of 2 GB of memory.</figDesc><graphic coords="6,127.70,356.31,330.24,367.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. AUC comparisons with standard deviation of EasyMKL against the Average Kernel algorithm with respect to different percentages of additional noise features using different datasets (Splice, Batch2 and Mush) and two different values of d (5 and 10).</figDesc><graphic coords="7,80.56,58.64,444.00,429.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. AUC comparisons (left) with standard deviation (right) of EasyMKL against the Average Kernel algorithm with respect to different cardinality of the training set using the Gisette dataset and three different values of d (5,10 and 20).</figDesc><graphic coords="8,67.73,58.64,450.00,418.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>4 but with different sizes of the training set. Specifically, we have selected the size of the training set varying from 10 to 200 examples. For each size, Fig. 5 has been obtained by evaluating the average on a large number of runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. AUC comparisons with standard deviation of EasyMKL against the average kernel algorithm with respect to different number of weak kernels using the German dataset (24 features) and the Splice dataset (60 features) with d ¼ 5.</figDesc><graphic coords="9,131.54,58.64,342.00,402.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Datasets information: name, source, number of features, number of examples and cardinality of the training set.</figDesc><table><row><cell>Data set</cell><cell>Source</cell><cell>Features</cell><cell>Examples</cell><cell>Ntr</cell></row><row><cell>Diabetes</cell><cell>UCI</cell><cell>8</cell><cell>768</cell><cell>77</cell></row><row><cell>Australian</cell><cell>Statlog</cell><cell>14</cell><cell>690</cell><cell>69</cell></row><row><cell>German</cell><cell>Statlog</cell><cell>24</cell><cell>1000</cell><cell>100</cell></row><row><cell>Splice</cell><cell>UCI</cell><cell>60</cell><cell>1000</cell><cell>100</cell></row><row><cell>Batch2</cell><cell>UCI</cell><cell>128</cell><cell>1244</cell><cell>124</cell></row><row><cell>Mush</cell><cell>UCI</cell><cell>112</cell><cell>2000</cell><cell>200</cell></row><row><cell>Gisette</cell><cell>NIPS03</cell><cell>5000</cell><cell>13,500</cell><cell></cell></row></table><note><p>1350 Fig. 2. Value of the quality function (separation) versus the AUC obtained using the greedy algorithm over the Splice dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>AUC results on three datasets of SVM (RBF all features).</figDesc><table><row><cell cols="3">Average AUC (RBF all features)</cell><cell></cell></row><row><cell>Algorithm</cell><cell>Diabetes</cell><cell>Australian</cell><cell>German</cell></row><row><cell>SVM</cell><cell>80.39%</cell><cell>91.37%</cell><cell>70.79%</cell></row><row><cell cols="3">Average AUC (RBF all features)</cell><cell></cell></row><row><cell>Algorithm</cell><cell>Splice</cell><cell>Batch2</cell><cell>Mush</cell></row><row><cell>SVM</cell><cell>86.12%</cell><cell>95.20%</cell><cell>98.52%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>AUC 7 std results on three datasets of various MKL methods and baselines (RBF feature subset d ¼ 5).</figDesc><table><row><cell cols="2">Average AUC (RBF subset d ¼ 5)</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>Diabetes</cell><cell>Australian</cell><cell>German</cell></row><row><cell>Random</cell><cell>62:13 7 6:20 %</cell><cell>65:56 7 21:32 %</cell><cell>49:59 7 11:68 %</cell></row><row><cell>Average</cell><cell>63:01 7 0:41 %</cell><cell>92:09 7 0:51 %</cell><cell>73:04 7 0:37 %</cell></row><row><cell>SMKL</cell><cell>77:52 7 2:11 %</cell><cell>91:64 7 3:18 %</cell><cell>69:71 7 2:78 %</cell></row><row><cell>GMKL</cell><cell>75:43 7 3:23 %</cell><cell>90:37 7 4:01 %</cell><cell>69:53 7 3:87 %</cell></row><row><cell>GLMKL</cell><cell>70:95 7 2:75 %</cell><cell>85:27 7 3:56 %</cell><cell>68:61 7 3:02 %</cell></row><row><cell>EasyMKL</cell><cell>79:91 7 0:13 %</cell><cell>92:17 7 0:11 %</cell><cell>72:71 7 0:21 %</cell></row><row><cell cols="2">Average AUC (RBF subset d ¼ 5Þ</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>Splice</cell><cell>Batch2</cell><cell>Mushrooms</cell></row><row><cell>Random</cell><cell>52:61 7 10:22 %</cell><cell>81:14 7 11:83 %</cell><cell>28:58 7 24:56 %</cell></row><row><cell>Average</cell><cell>86:56 7 0:25 %</cell><cell>95:35 7 0:43 %</cell><cell>98:45 7 0:49 %</cell></row><row><cell>SMKL</cell><cell>83:48 7 2:15 %</cell><cell>94:82 7 2:18 %</cell><cell>97:49 7 3:41 %</cell></row><row><cell>GMKL</cell><cell>83:70 7 4:24 %</cell><cell>92:53 7 3:99 %</cell><cell>97:39 7 4:89 %</cell></row><row><cell>GLMKL</cell><cell>85:36 7 1:98 %</cell><cell>94:54 7 3:34 %</cell><cell>96:38 7 4:72 %</cell></row><row><cell>EasyMKL</cell><cell>87:87 7 0:18 %</cell><cell>98:98 7 0:21 %</cell><cell>97:91 7 0:24 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>AUC 7 std results on three datasets of various MKL methods and baselines (RBF feature subset d ¼ 10).</figDesc><table><row><cell cols="2">Average AUC (RBF subset d ¼ 10)</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>Diabetes</cell><cell>Australian</cell><cell>German</cell></row><row><cell>Random</cell><cell>71:44 7 7:05 %</cell><cell>70:36 7 12:59 %</cell><cell>56:93 7 7:14 %</cell></row><row><cell>Average</cell><cell>71:94 7 0:34 %</cell><cell>92:11 7 0:42 %</cell><cell>73:33 7 0:36 %</cell></row><row><cell>SMKL</cell><cell>75:21 7 3:41 %</cell><cell>87:94 7 2:12 %</cell><cell>70:92 7 2:56 %</cell></row><row><cell>GMKL</cell><cell>72:01 7 5:76 %</cell><cell>86:88 7 3:17 %</cell><cell>70:89 7 3:01 %</cell></row><row><cell>GLMKL</cell><cell>71:85 7 5:01 %</cell><cell>84:80 7 2:09 %</cell><cell>69:75 7 2:78 %</cell></row><row><cell>EasyMKL</cell><cell>79:61 7 0:16 %</cell><cell>92:17 7 0:11 %</cell><cell>73:21 7 0:11 %</cell></row><row><cell cols="2">Average AUC (RBF subset d ¼10)</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>Splice</cell><cell>Batch2</cell><cell>Mushrooms</cell></row><row><cell>Random</cell><cell>56:76 7 8:52 %</cell><cell>85:30 7 8:24 %</cell><cell>44:58 7 37:26 %</cell></row><row><cell>Average</cell><cell>90:42 7 0:34 %</cell><cell>94:70 7 0:53 %</cell><cell>98:86 7 0:19 %</cell></row><row><cell>SMKL</cell><cell>86:84 7 3:42 %</cell><cell>95:07 7 4:87 %</cell><cell>97:48 7 2:37 %</cell></row><row><cell>GMKL</cell><cell>84:48 7 6:02 %</cell><cell>94:92 7 6:97 %</cell><cell>96:99 7 4:32 %</cell></row><row><cell>GLMKL</cell><cell>86:41 7 5:51 %</cell><cell>94:18 7 6:09 %</cell><cell>95:25 7 3:12 %</cell></row><row><cell>EasyMKL</cell><cell>91:19 7 0:18 %</cell><cell>97:08 7 0:16 %</cell><cell>98:29 7 0:12 %</cell></row></table><note><p>1 http://www.cmpe.boun.edu.tr/ $ gonen/mkl.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Average AUC 7 std on the Gisette (NIPS2003 feature selection challenge) of EasyMKL (with RBF subset) and comparison with the Average baseline.</figDesc><table><row><cell>Algorithm</cell><cell>RBF d ¼ 5</cell><cell>RBF d ¼ 10</cell><cell>RBF d ¼ 20</cell></row><row><cell>Average</cell><cell>95:52 7 0:15 %</cell><cell>94:72 7 0:13 %</cell><cell>93:65 7 0:13 %</cell></row><row><cell>EasyMKL</cell><cell>95:83 7 0:12 %</cell><cell>95:40 7 0:10 %</cell><cell>94:64</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: F. Aiolli, M. Donini, EasyMKL: a scalable multiple kernel learning algorithm, Neurocomputing (2015), http: //dx.doi.org/10.1016/j.neucom.2014.11.078i</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>F. Aiolli, M. Donini / Neurocomputing ∎ (∎∎∎∎) ∎∎∎-∎∎∎</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Greedy Algorithm</head><p>In this appendix we present in detail the greedy algorithm (Algorithm 1) used in Section 5.2. Require: λA½0; 1 Ensure: A kernel matrix K n and a vector γ n</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Semi-Supervised</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="〈http://www.kyb.tuebingen.mpg.de/ssl-book〉" />
		<editor>O. Chapelle, B. Schölkopf, A. Zien</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple kernel learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2211" to="2268" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalization bounds for learning kernels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<ptr target="〈http://www.icml2010.org/papers/179.pdf〉" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">June 21-24, 2010. 2010</date>
			<biblScope unit="page" from="247" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved loss bounds for multiple kernel learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<ptr target="〈http://www.jmlr.org/proceedings/papers/v15/hussain11a/hussain11a.pdf〉" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04-11">2011. April 11-13, 2011, 2011</date>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A kernel method for the optimization of the margin distribution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICANN</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="〈http://archive.ics.uci.edu/ml〉" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vembu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Homer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huerta</surname></persName>
		</author>
		<title level="m">Chemical gas sensor drift compensation using classifier ensembles</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Result analysis of the nips 2003 feature selection challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS 17</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Soft margin multiple kernel learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="761" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><surname>Simplemkl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2491" to="2521" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Danyluk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2009</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</editor>
		<meeting>ICML 2009<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">June 14-18, 2009. 2009</date>
			<biblScope unit="volume">382</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Non-sparse regularization and efficient training with multiple kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<idno>abs/1003.0079</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simple and efficient multiple kernel learning by group lasso</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ICML</publisher>
			<biblScope unit="page" from="1175" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spg-gmkl: generalized multiple kernel learning with a million kernels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Result analysis of the nips 2003 feature selection challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aiolli received a Master&apos;s Degree and a PhD in Computer Science both from the University of Pisa. He was Post-doc at the University of Pisa, Paid Visiting Scholar at the University of Illinois at Urbana-Champaign (IL), USA, and Post-doc at the University of Padova. He is currently Assistant Professor at the University of Padova</title>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research activity is mainly in the area of Machine Learning and Information Retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Donini received his Bachelor&apos;s degree and Master&apos;s degrees in Mathematics from the University of Padova in 2010 and 2012, respectively. He is currently a PhD student in Machine Learning at the University of Padova. His research interests include kernel methods, multiple kernel learning and machine learning in general</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
