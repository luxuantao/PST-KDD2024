<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Cross-domain Recommendation in Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-02">2 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
							<email>ruobingxie@tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liangdong</forename><surname>Wang</surname></persName>
							<email>ldwang@tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Shukai</forename><surname>Liu</surname></persName>
							<email>shukailiu@tencent.com</email>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">WeChat</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">WeChat</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">WeChat</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">WeChat</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">WeChat</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">WeChat</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Cross-domain Recommendation in Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-02">2 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2112.00999v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>recommendation</term>
					<term>contrastive learning</term>
					<term>cross-domain recommendation</term>
					<term>matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-domain recommendation (CDR) aims to provide better recommendation results in the target domain with the help of the source domain, which is widely used and explored in real-world systems. However, CDR in the matching (i.e., candidate generation) module struggles with the data sparsity and popularity bias issues in both representation learning and knowledge transfer. In this work, we propose a novel Contrastive Cross-Domain Recommendation (CCDR) framework for CDR in matching. Specifically, we build a huge diversified preference network to capture multiple information reflecting user diverse interests, and design an intra-domain contrastive learning (intra-CL) and three inter-domain contrastive learning (inter-CL) tasks for better representation learning and knowledge transfer. The intra-CL enables more effective and balanced training inside the target domain via a graph augmentation, while the inter-CL builds different types of cross-domain interactions from user, taxonomy, and neighbor aspects. In experiments, CCDR achieves significant improvements on both offline and online evaluations in a real-world system. Currently, we have deployed CCDR on a well-known recommendation system, affecting millions of users. The source code will be released in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Personalized recommendation aims to provide attractive items for users according to their profiles and historical behaviors, which has been widely implemented in various fields of our lives. Real-world large-scale recommendation systems usually adopt the classical two-stage architecture containing ranking and matching (i.e., candidate generation) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>. The matching module focuses more on the efficiency and diversity, which first retrieves a small subset of (usually hundreds of) item candidates from the million-level large corpora. Next, the ranking module gives the specific ranks of items for the final display. With the increase of recommendation scale and the expansion of recommendation scenarios, real-world recommendations usually need to bring in additional data sources (i.e., domains) as supplements to improve their content coverage and diversity. These cold-start items of new data sources only have very few user behaviors at their warm-up stage. Hence, it is difficult to recommend these cold-start items appropriately. Cross-domain recommendation (CDR), which aims to make full use of the informative knowledge from the source domain to help the target domain's recommendation <ref type="bibr" target="#b4">[5]</ref>, is proposed to solve this issue. EMCDR <ref type="bibr" target="#b22">[23]</ref> is a classical CDR method, which focuses on building user mapping functions via aligned user representations in the source and target domains. CoNet <ref type="bibr" target="#b13">[14]</ref> proposes another approach that jointly models feature interactions in two domains via a cross connection unit. However, existing CDR methods often heavily rely on aligned users for crossdomain mapping (e.g., EMCDR), ignoring other rich information in recommendation such as taxonomy. It will harm the knowledge transfer between different domains, especially in cold-start scenarios. Moreover, lots of CDR methods are designed for ranking that consider complicated cross-domain user-item interactions (e.g., CoNet), which cannot be directly adopted in matching due to the online efficiency. CDR in the matching module should consider not only recommendation accuracy, but also diversity and efficiency.</p><p>In this work, we aim to improve the matching module's performance on new (few-shot or strict cold-start) domains via the CDR manner. Fig. <ref type="figure" target="#fig_0">1</ref> shows an illustration of this task. Precisely, CDR in matching mainly has the following three challenges:</p><p>(1) How to address the data sparsity and popularity bias issues of CDR in matching? Real-world recommendation usually suffers from serious data sparsity issues when modeling the interactions between million-level users and items. Moreover, these sparse interactions are even highly skewed to popular items with high exposure owing to the Matthew effect <ref type="bibr" target="#b25">[26]</ref>, which makes hot items become hotter. These two issues inevitably harm the representation learning of cold-start and long-tail items, whose damages will even be multiplied in matching where all items should be considered.</p><p>(2) How to conduct more effective knowledge transfer for the (coldstart) target domain with few user behaviors? As stated above, conventional CDR methods strongly depend on aligned users and their behaviors. The performance of CDR in matching will be greatly reduced, if most users and items have few interactions and models cannot learn reliable representations in cold-start domains. Moreover, other heterogeneous information (e.g., taxonomy) should also be fully considered in CDR to bridge different domains. We should build more effective and robust cross-domain knowledge transfer paths to well learn both popular and long-tail objects.</p><p>(3) How to balance the practical demands of accuracy, diversity and efficiency of CDR in matching? Online efficiency requirements need to be strictly followed. Moreover, matching is more responsible for the diversity than ranking, for it determines the inputs of ranking.</p><p>A good CDR matching model should comprehensively transfer user diverse preferences via multiple paths to the target domain.</p><p>To address these issues, we propose a novel Contrastive Cross-Domain Recommendation (CCDR) to transfer user preferences in matching. Specifically, we build two global diversified preference networks for two domains, containing six types of objects to enhance diversity and cross-domain connections. We conduct a GNN aggregator with a neighbor-similarity based loss on heterogeneous interactions to capture user diverse interests. To strengthen the cross-domain knowledge transfer, we design the intra-domain contrastive learning (intra-CL) and inter-domain contrastive learning (inter-CL) in CCDR. The intra-CL conducts an additional self-supervised learning with sub-graph based data augmentations to learn more reliable representations for matching in the target domain. In contrast, the inter-CL designs three contrastive learning tasks focusing on the cross-domain mapping between aligned users, taxonomies, and their heterogeneous neighbors. The mutual information maximization with different types of objects multiplies the effectiveness of cross-domain knowledge transfer. Finally, all additional CL losses are combined with the original CDR losses under a multi-task learning (MTL) framework. We conduct a cross-domain multi-channel matching to further improve the diversity in online. CCDR has the following three advantages: (1) The intra-CL brings in self-supervised learning for long-tail users and items, which can alleviate the data sparsity and popularity bias issues in matching. <ref type="bibr" target="#b1">(2)</ref> The inter-CL introduces new CL-based cross-domain interactions, which enables more effective and robust knowledge transfer for CDR in matching. <ref type="bibr" target="#b2">(3)</ref> The diversified preference network, multiple CL tasks, and the cross-domain multi-channel matching cooperate well to capture user diverse preferences, which meets the requirements of diversity and efficiency in online system.</p><p>In experiments, we compare CCDR with competitive baselines on real-world recommendation domains, and achieve significant improvements on both offline and online evaluations. Moreover, we also conduct some ablation tests and parameter analyses to better understand our model. The contributions are concluded as:</p><p>• We propose a novel contrastive cross-domain recommendation for CDR in matching. To the best of our knowledge, we are the first to conduct contrastive learning to improve both representation learning and knowledge transfer in CDR. • We propose the intra-CL task with a sub-graph based data augmentation to learn better node representations inside the single domain, which can alleviate the data sparsity issue in CDR of matching. • We also creatively design three inter-CL tasks via aligned users, taxonomies, and their neighbors in our diversified preference networks, which enable more effective and diverse knowledge transfer paths across different domains. • We achieve significant improvements on both offline and online evaluations. CCDR is effective, efficient, and easy to deploy. Currently, it has been deployed on a real-world recommendation system, affecting millions of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Matching in Recommendation. The matching (i.e., candidate generation) module aims to retrieve a small subset of (usually hundreds of) items from large corpora efficiently <ref type="bibr" target="#b41">[42]</ref>. Conventional recommendation matching algorithms often rely on information retrieval based models <ref type="bibr" target="#b29">[30]</ref>. Recently, embedding-based retrieval <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> is also widely used in practical systems with the help of fast retrieval servers <ref type="bibr" target="#b15">[16]</ref>. Due to the need for efficiency, embeddingbased methods usually adopt a two-tower architecture, which conducts two different towers for building user and item representations separately. Different feature interaction methods such as FM <ref type="bibr" target="#b28">[29]</ref>, Youtube candidate generation <ref type="bibr" target="#b3">[4]</ref>, DeepFM <ref type="bibr" target="#b7">[8]</ref>, AutoInt <ref type="bibr" target="#b31">[32]</ref>, ICAN <ref type="bibr" target="#b41">[42]</ref>, AFN <ref type="bibr" target="#b2">[3]</ref>, DFN <ref type="bibr" target="#b39">[40]</ref>, and AFT <ref type="bibr" target="#b9">[10]</ref> could be used in these towers. In contrast, tree-based matching models <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b55">56]</ref> give another way to address the matching problem with structured item trees. Graph-based matching models <ref type="bibr" target="#b40">[41]</ref> are also proposed to learn user/item representations. However, few works focus on the CDR in matching, which often exists in practical recommendations.</p><p>Cross-domain Recommendation. Cross-domain recommendation attempts to learn useful knowledge from the source domain to help the target domain's recommendation <ref type="bibr" target="#b4">[5]</ref>. EMCDR <ref type="bibr" target="#b22">[23]</ref> is a classical embedding mapping approach, which builds the mapping function via aligned users' representations. SSCDR <ref type="bibr" target="#b16">[17]</ref> designs a semi-supervised manner to learn item mapping based on EMCDR. In contrast, CoNet <ref type="bibr" target="#b13">[14]</ref> is another classical type of CDR method that uses the cross connection unit to model domain interactions. Zhao et al. <ref type="bibr" target="#b48">[49]</ref> combines items of both source and target domains in one graph to learn representations. Neural attentive transfer <ref type="bibr" target="#b5">[6]</ref>, dual transfer <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b52">53]</ref>, source-target mixed attention <ref type="bibr" target="#b24">[25]</ref>, dual generator <ref type="bibr" target="#b47">[48]</ref>, and meta-learning <ref type="bibr" target="#b54">[55]</ref> are also proposed for CDR. Additional review information is also used to enhance the attentive knowledge transfer <ref type="bibr" target="#b49">[50]</ref>. Some works explore non-overlapping CDR <ref type="bibr" target="#b45">[46]</ref>. However, most of these CDR models are specially designed for the ranking module (which involve user-item interactions in cross-domain modeling). ICAN <ref type="bibr" target="#b41">[42]</ref> is the most related work, which captures field-level feature interactions to improve matching in multiple domains. In CCDR, we introduce several novel CL tasks. To the best of our knowledge, we are the first to conduct CL to jointly improve representation learning and knowledge transfer in CDR. Contrastive Learning. Contrastive learning (CL) is a representative self-supervised learning (SSL) method, which aims to learn models by contrasting positive pairs against negative pairs <ref type="bibr" target="#b8">[9]</ref>. Contrastive predictive coding (CPC) <ref type="bibr" target="#b23">[24]</ref> designs the widely-used InfoNCE loss. MoCo <ref type="bibr" target="#b10">[11]</ref> builds a large dynamic dictionary with a queue and a moving-averaged momentum encoder. SimCLR <ref type="bibr" target="#b1">[2]</ref> designs a simple contrastive learning framework with a composition of data augmentations and projectors for CL. BYOL <ref type="bibr" target="#b6">[7]</ref> relies on its online and target networks, which iteratively bootstraps the outputs of a network to serve as targets for learning. Qiu et al. <ref type="bibr" target="#b27">[28]</ref> proposes a graph contrastive coding for GNN pre-training. CL in recommendation. Recently, SSL and CL are also verified in recommendation <ref type="bibr" target="#b46">[47]</ref>. S 3 -Rec <ref type="bibr" target="#b51">[52]</ref> builds contrastive learning tasks among items, attributes, sentences, and sub-sentences in sequential recommendation. UPRec focuses on user-aware SSL <ref type="bibr" target="#b38">[39]</ref>. Xie et al. <ref type="bibr" target="#b43">[44]</ref> designs an adversarial and contrastive VAE for sequence modeling. Moreover, CL has also been used in disentangled recommendation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b50">51]</ref>, session modeling <ref type="bibr" target="#b37">[38]</ref>, social recommendation <ref type="bibr" target="#b44">[45]</ref>, and cold-start recommendation <ref type="bibr" target="#b34">[35]</ref>. For graph-based CL, Wu et al. <ref type="bibr" target="#b36">[37]</ref> introduces embedding, node, edge dropouts to graphbased recommendation. Differing from these works, we build three CL tasks to facilitate the user preference transfers between different domains in cold-start cross-domain recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this work, we propose CCDR to enhance the cross-domain recommendation in matching via contrastive learning. In this section, we first describe our task and the overall framework of CCDR (Sec.</p><p>3.1). Second, we introduce the diversified preference networks and the single-domain GNN-based aggregator (Sec. 3.2 and Sec. 3.3). Next, we introduce the intra-domain and inter-domain contrastive learning (Sec. 3.4 and Sec. 3.5). Finally, we combine three losses with a multi-task optimization (Sec. 3.6). The online system and deployment of CCDR will be introduced in Sec. 4. More detailed discussions are given in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Overall Framework</head><p>CDR in matching. We concentrate on the matching module of the classical two-stage recommendation systems <ref type="bibr" target="#b3">[4]</ref>. Matching is the first step before ranking, which attempts to efficiently retrieve hundreds of items from million-level item candidates. It cares more about whether good items are retrieved (often measured by hit rate), not the specific top item ranks which should be considered by the following ranking module (often measured by NDCG or AUC) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>. The CDR in matching task attempts to improve the target domain's matching module with the help of the source domain.</p><p>Overall framework. CCDR is trained with three types of losses, including the original source/target single-domain losses, the intradomain CL loss, and the inter-domain CL loss. <ref type="bibr" target="#b0">(1)</ref> We first build a huge global diversified preference network separately for each domain as the sources of user preferences. This diversified preference network contains various objects such as user, item, tag, category, media, and word with their interactions to bring in user diverse preferences from different aspects. ( <ref type="formula" target="#formula_2">2</ref>) Next, we train the single-domain matching model via a GNN aggregator and the neighbor-similarity based loss. (3) Since the cold-start domain lacks sufficient user behaviors, we introduce the intra-domain CL inside the target domain to train more reliable node representations with a sub-graph based data augmentation. (4) To enhance the cross-domain knowledge transfer, we design three inter-domain CL tasks via aligned users, taxonomies, and their neighbors between two domains, which cooperate well with the diversified preference network. All three losses are combined under a multi-task learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Diversified Preference Network</head><p>Conventional matching <ref type="bibr" target="#b3">[4]</ref> and CDR <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> models usually heavily rely on user-item interactions to learn CTR objectives and crossdomain mapping. However, it will decrease the diversity of matching due to the popularity bias issue. Moreover, it does not take full advantage of other connections (e.g., tags, words, medias) besides users between different domains, which is particularly informative in cross-domain knowledge transfer. Therefore, inspired by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>, we build a global diversified preference network for each domain, considering 6 types of important objects in recommendation as nodes and their heterogeneous interactions as edges. Specifically, we use item, user, tag, category, media, and word as nodes. Tags and categories are item taxonomies that represent users' fine-and coarse-granularity interests. Media indicates the item's producer. Words reflect the semantic information of items extracted from items' titles or contents. To alleviate data sparsity and accelerate our offline training, we also gather users into user groups according to their basic profiles (all users having the same gender-age-location attributes are clustered in the same user group). These user groups are viewed as user nodes in CCDR.</p><p>As for edges, we consider the following six item-related interactions: (a) User-item edge (U-I). This edge is generated if an item is interacted by a user group at least 3 times. We jointly consider multiple user behaviors (i.e., click, like, share) to build this edge with different weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Item-item edge (I-I). The I-I edge introduces sequential information of user behaviors in sessions. It is built if two items appear in adjacent positions in a session. (c) Tag-item edge (T-I). The T-I edges connect items and their tags. It captures items' fine-grained taxonomy information. (d) Category-item edge (C-I). It records items' coarse-grained taxonomy information. (e)</head><p>Media-item edge (M-I). It links items with their producers/sources. (f) Word-item edge (W-I). It highlights the semantic information of items from their titles. Each edge is undirected but empirically weighted according to the edge type and strength (e.g., counts for U-I edges). Comparing with conventional U-I graphs, our diversified preference network tries its best to describe items from different aspects via these heterogeneous interactions. The advantages of this diversified preference network are as follows: (1) it brings in additional information as supplements to user-item interactions, which jointly improve both accuracy and diversity (see Sec. 3.3.2). ( <ref type="formula" target="#formula_2">2</ref>) It can build more potential bridges between different domains via users, tags, categories, and words, which cooperates well with the inter-CL tasks and the online multi-channel matching in CDR (see Sec. 3.5 and Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Single-domain GNN Aggregator</head><p>3.3.1 GNN-based Aggregator. Inspired by the great successes of GNN, we adopt GAT <ref type="bibr" target="#b33">[34]</ref> as the aggregator on the diversified preference network for simplicity and universality. Precisely, we randomly initialize 𝒆 0 𝑖 for all heterogeneous nodes. For a node 𝑒 𝑖 and its neighbor 𝑒 𝑘 ∈ 𝑁 𝑒 𝑖 (𝑁 𝑒 𝑖 is the neighbor set of 𝑒 𝑖 after a weighted sampling), we have 𝑒 𝑖 's node representation 𝒆 𝑥 𝑖 at the 𝑥-th layer as:</p><formula xml:id="formula_0">𝒆 𝑥 𝑖 = 𝜎 ( ∑︁ 𝑒 𝑘 ∈𝑁 𝑒 𝑖 𝛼 𝑥 𝑖𝑘 𝑾 𝑥 𝒆 𝑥−1 𝑘 ). (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>𝑾 𝑥 is the weighting matrix, 𝜎 is the sigmoid function. 𝛼 𝑥 𝑖𝑘 represents the attention between 𝑒 𝑖 and 𝑒 𝑘 in 𝑥-th layer noted as:</p><formula xml:id="formula_2">𝛼 𝑥 𝑖𝑘 = exp(𝑓 (𝒘 𝑥 ⊤ [𝑾 𝑥 𝒆 𝑥−1 𝑖 ||𝑾 𝑥 𝒆 𝑥−1 𝑘 ])) 𝑒 𝑙 ∈𝑁 𝑒 𝑖 exp(𝑓 (𝒘 𝑥 ⊤ [𝑾 𝑥 𝒆 𝑥−1 𝑖 ||𝑾 𝑥 𝒆 𝑥−1 𝑙 ])) ,<label>(2)</label></formula><p>where 𝑓 (•) indicates a LeakyReLU activation and || indicates the concatenation. 𝒘 𝑥 is the weighting vector. Note that the 𝑁 𝑒 𝑖 is a dynamic neighbor set which is randomly generated based on the edge weight in Sec. 3.2. We conduct a two-layer GAT to generate the aggregated node representations 𝒆 𝑖 for all nodes (𝒆 𝑠 𝑖 and 𝒆 𝑡 𝑖 for the source and target domains). It is also not difficult to conduct other GNN models such as LightGCN <ref type="bibr" target="#b11">[12]</ref> in this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Neighbor-similarity Based Optimization. In practical CDR scenarios, users often have fewer historical behaviors on items in (cold-start) target domains. Conventional embedding-based matching methods such like Matrix factorization (MF) <ref type="bibr" target="#b17">[18]</ref> cannot get sufficient supervised information from the sparse user-item interactions, and thus cannot learn reliable user and item representations for matching. To capture additional information from behavior, session, taxonomy, semantics, and data source aspects, we conduct the neighbor-similarity based loss <ref type="bibr" target="#b19">[20]</ref> on the diversified preference network. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, this loss projects all nodes into the same latent space, making all nodes similar with their neighbors. It regards all types of edges as unsupervised information to guide the training besides user-item interactions. Formally, the neighborsimilarity based loss 𝐿 𝑁 is defined as follows:</p><formula xml:id="formula_3">𝐿 𝑁 = − ∑︁ 𝑒 𝑖 ∑︁ 𝑒 𝑘 ∈𝑁 𝑒 𝑖 ∑︁ 𝑒 𝑗 ∉𝑁 𝑒 𝑖 (− log(𝜎 (𝒆 ⊤ 𝑖 𝒆 𝑗 )) + log(𝜎 (𝒆 ⊤ 𝑖 𝒆 𝑘 ))). (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>𝒆 𝑖 is the 𝑖-th aggregated node representation, and 𝑒 𝑘 is a sampled neighbor of 𝑒 𝑖 . 𝑒 𝑗 is a randomly selected negative sample of 𝑒 𝑖 . We choose the neighbor-similarity based loss for the following advantages: (1) 𝐿 𝑁 makes full use of all types of interactions between heterogeneous objects in matching, which contain significant information from user behaviors (U-I edges), sessions (I-I edges), item taxonomies (T-I and C-I edges), data sources (M-I edges) and semantics (W-I edges). It helps to capture user diverse preferences to balance accuracy and diversity in matching. If we only consider U-I edges, this loss will degrade into the classical MF. (2) CDR in matching should deal with long-tail items. 𝐿 𝑁 brings in additional information for long-tail items that can benefit cold-start domains.</p><p>(3) We conduct a cross-domain multi-channel matching strategy in online for diversity. This embedding-based retrieval strategy also depends on heterogeneous node embeddings optimized by 𝐿 𝑁 to retrieve similar items in the (cold-start) target domain (see Sec. <ref type="bibr" target="#b3">4</ref> for more details). The 𝐿 𝑁 loss exactly fits for the online multi-channel matching, and also well cooperates with the diversified preference network and the inter-CL losses. We cannot conduct complicated user-item interaction calculations in Eq. ( <ref type="formula" target="#formula_3">3</ref>), since we rely on the fast embedding-based retrieval in matching for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Intra-domain Contrastive Learning</head><p>Contrastive learning is a widely-used SSL method that can make full use of unlabelled data via its pair-wise training. In CCDR, we conduct two types of CL tasks. The intra-domain contrastive learning (intra-CL) is conducted inside the target domain to learn better node representations, while the inter-domain contrastive learning (inter-CL) is adopted across the source and target domains to guide a better knowledge transfer. In intra-CL, we conduct a sub-graph based data augmentation for each node aggregation, which could be regarded as a dynamic node/edge dropout in classical graph augmentation <ref type="bibr" target="#b36">[37]</ref>. Precisely, for a node 𝑒 𝑖 , we sample two neighbor set 𝑁 𝑒 𝑖 and 𝑁 ′ 𝑒 𝑖 to conduct the GNN aggregation, and receive two node representations 𝒆 𝑖 and 𝒆 ′ 𝑖 . 𝒆 ′ 𝑖 is regarded as the positive instance of 𝒆 𝑖 in intra-CL, with a different sub-graph sampling focusing on different neighbors of 𝑒 𝑖 . Similar to <ref type="bibr" target="#b1">[2]</ref>, we randomly sample from other examples 𝒆 ′ 𝑗 in the same batch 𝐵 of 𝑒 𝑖 to get the negative samples 𝑒 𝑗 . We do not use all examples in 𝐵 as negative samples for efficiency. In this case, the popularity bias is partially solved <ref type="bibr" target="#b35">[36]</ref>. Formally, we follow the InfoNCE <ref type="bibr" target="#b23">[24]</ref> to define the intra-CL loss 𝐿 𝑖𝑛𝑡𝑟𝑎 as follows:</p><formula xml:id="formula_5">𝐿 𝑖𝑛𝑡𝑟𝑎 = − ∑︁ 𝐵 ∑︁ 𝑒 𝑖 ∈𝐵 log exp(sim(𝒆 𝑖 , 𝒆 ′ 𝑖 )/𝜏) 𝑒 ′ 𝑗 ∈𝑆 𝐵𝑖 exp(sim(𝒆 𝑖 , 𝒆 ′ 𝑗 )/𝜏) . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>𝑆 𝐵𝑖 indicates the negative samples of 𝑒 𝑖 in 𝐵. 𝜏 is the temperature. sim(𝒆 𝑖 , 𝒆 ′ 𝑗 ) measures the similarity between 𝒆 𝑖 and 𝒆 ′ 𝑗 , which is calculated as their cosine similarity. With the intra-CL loss, longtail nodes can also get training opportunities via SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inter-domain Contrastive Learning</head><p>The inter-CL aims to improve the knowledge transfer across different domains via various types of nodes and edges in the diversified preference network. Precisely, we design three inter-domain CL tasks via aligned users, taxonomies, and neighbors as in Fig. <ref type="figure">3</ref>. <ref type="bibr" target="#b22">[23]</ref> take aligned users as their dominating mapping seeds across domains. We follow this idea and conduct a user-based inter-CL task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">User-based Inter-CL. Most conventional CDR methods</head><formula xml:id="formula_7">. (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>𝑆 𝑢 𝑖 is the sampled negative set collecting from all other users except 𝑢 𝑖 . sim(•, •) indicates the cosine similarity.</p><p>3.5.2 Taxonomy-based inter-CL. Differing from some classical CDR methods <ref type="bibr" target="#b16">[17]</ref>, CCDR builds a diversified preference network that introduces more bridges across different domains. We assume that the same tag/category/word in different domains should have the same meanings. Hence, we design a taxonomy-based inter-CL similar to the user-based CL. we take the aggregated node representation pair (𝒕 𝑠 𝑖 , 𝒕 𝑡 𝑖 ) of the same taxonomy 𝑡 𝑖 in two domains as the positive pair, where 𝑡 𝑖 could be tags, categories, and words. We have:</p><formula xml:id="formula_9">𝐿 𝑖𝑛𝑡𝑒𝑟 𝑡 = − ∑︁ 𝑡 𝑖 log exp(sim(𝒕 𝑠 𝑖 , 𝒕 𝑡 𝑖 )/𝜏) 𝑡 𝑗 ∈𝑆 𝑡 𝑖 exp(sim(𝒕 𝑠 𝑖 , 𝒕 𝑡 𝑗 )/𝜏) ,<label>(6)</label></formula><p>𝑆 𝑡 𝑖 is the sampled negative set of 𝑡 𝑖 from all other taxonomies with the same type. We can set different temperatures for taxonomies and users if we want to sharpen the differences of some types. 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑡 functions as a supplement to the original user-based mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Neighbor-based inter-CL.</head><p>Besides the explicit alignments of users and taxonomies across domains, there are also some essential objects such as items that do not have explicit mapping. We aim to bring in more implicit cross-domain knowledge transfer paths between unaligned nodes in two domains. We suppose that similar nodes in different domains should have similar neighbors (e.g., similar items may have similar users, taxonomies, and producers </p><p>In 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑛 , for an aligned node's representation 𝒆 𝑠 𝑖 in the source domain, its target-domain neighbor's representation 𝒆 𝑡 𝑘 is the positive instance, while other target-domain representations 𝒆 𝑡 𝑗 are negative. It is reasonable since related objects should be connected in the diversified preference network and learned to be similar under the neighbor-similarity based loss in Eq. <ref type="bibr" target="#b2">(3)</ref>. It is also convenient to extend the current positive samples 𝑒 𝑘 ∈ 𝑁 𝑡 𝑒 𝑖 to multi-hop neighbors for better generalization and diversity in CDR.</p><p>This neighbor-based inter-CL greatly multiplies the diversified knowledge transfer paths between two domains, especially for the cold-start items. For example, through the 𝑡𝑎𝑔 𝑠 𝑖 → 𝑡𝑎𝑔 𝑡 𝑖 → 𝑖𝑡𝑒𝑚 𝑡 𝑗 path, the cold-start 𝑖𝑡𝑒𝑚 𝑗 's representation in the target domain can be directly influenced by fully-trained representations in the source domain. Moreover, the similarities between different types of source-domain node representations and the target-domain item representations are directly used in the online multi-channel matching for diversified retrieval, which will be discussed in Sec. 4. Finally, we combine all three CL losses from aligned user, taxonomy, and neighbor aspects to form the inter-CL loss 𝐿 𝑖𝑛𝑡𝑒𝑟 as:</p><formula xml:id="formula_11">𝐿 𝑖𝑛𝑡𝑒𝑟 = 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑢 + 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑡 + 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑛 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Multi-task Optimization</head><p>Following classical CL-based recommendation models <ref type="bibr" target="#b44">[45]</ref>, we also conduct a multi-task optimization combining the source-domain matching loss 𝐿 𝑁 𝑠 , the target-domain matching loss 𝐿 𝑁 𝑡 , the intra-CL loss 𝐿 𝑖𝑛𝑡𝑟𝑎 , and the inter-CL loss 𝐿 𝑖𝑛𝑡𝑒𝑟 as follows:</p><formula xml:id="formula_12">𝐿 = 𝜆 1 𝐿 𝑁 𝑠 + 𝜆 2 𝐿 𝑁 𝑡 + 𝜆 3 𝐿 𝑖𝑛𝑡𝑟𝑎 + 𝜆 4 𝐿 𝑖𝑛𝑡𝑒𝑟 .<label>(9)</label></formula><p>𝜆 1 , 𝜆 2 , 𝜆 3 , 𝜆 4 are loss weights set as 1.0, 1.0, 1.5, 0.6 according to the grid search (see Sec. 5.7 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ONLINE DEPLOYMENT</head><p>We have deployed CCDR on the cold-start matching module in a well-known recommendation system named WeChat Top Stories. A good CDR-based cold-start matching module should have the following key characteristics: (1) making full use of user behaviors and item features in the source and target domains, (2) capturing user diverse preferences from different aspects, and (3) balancing accuracy, diversity and efficiency. To achieve these, we propose a new cross-domain multi-channel matching in online. Specifically, we conduct six channels including user, item, tag, category, media, and word channels to retrieve items in the target domains via node representations learned by Eq. ( <ref type="formula" target="#formula_12">9</ref>). We rely on the user historical behavior sequence 𝑠𝑒𝑞 = {𝑑 1 , 𝑑 2 , • • • , 𝑑 𝑛 } to capture user's interests, where 𝑑 𝑖 is the 𝑖-th clicked item and 𝑛 is the max length. In the item channel, we directly use the node representations of all items in 𝑠𝑒𝑞 to retrieve similar items in the target domain. Formally, we define the score 𝑠 𝑑 𝑖 of the 𝑖-th target-domain item d𝑖 in the item channel as follows:</p><formula xml:id="formula_13">𝑠 𝑑 𝑖 = ∑︁ 𝑗=1 𝑠𝑖𝑚( d𝑖 , 𝒅 𝑗 ) × 𝑠𝑎𝑡𝑖𝑠 𝑓 𝑗 × 𝑟𝑒𝑐𝑒𝑛𝑐𝑦 𝑗 × 𝑧 𝑑 (𝑖, 𝑗). (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>𝑠𝑖𝑚( d𝑖 , 𝒅 𝑗 ) is the cosine similarity between the clicked item 𝑑 𝑗 in user historical behaviors and the item candidate d𝑖 in the target domain, where d𝑖 and 𝒅 𝑗 are aggregated item embeddings trained by Eq. ( <ref type="formula" target="#formula_12">9</ref>). 𝑠𝑎𝑡𝑖𝑠 𝑓 𝑗 measures the posterior user satisfaction on 𝑑 𝑗 , which is roughly calculated as the complete rate of item contents. 𝑟𝑒𝑐𝑒𝑛𝑐𝑦 𝑗 models the temporal factors of historical items, which decays exponentially from the short term to the long term (𝑟𝑒𝑐𝑒𝑛𝑐𝑦 𝑗 = 0.95 𝑛−𝑗 ). For online efficiency, each item in 𝑠𝑒𝑞 only recommends its top 100 nearest items. 𝑧 𝑑 (𝑖, 𝑗) equals 1 only if the target-domain item d𝑖 appears in the top 100 nearest items of 𝑑 𝑗 , and otherwise 𝑧 𝑑 (𝑖, 𝑗) = 0. We pre-calculate the similarities and index the top nearest items for all nodes in offline to further accelerate the online matching.</p><p>To capture user diverse preferences from different aspects, we further conduct the tag, category, media and word channels similar to the item channel. Taking the tag channel for instance, we build a historical tag sequence</p><formula xml:id="formula_15">𝑠𝑒𝑞 𝑡 = {𝑇 1 ,𝑇 2 , • • • ,𝑇 𝑛 } according to the item sequence {𝑑 1 , 𝑑 2 , • • • , 𝑑 𝑛 },</formula><p>where 𝑇 𝑗 is the tag set of 𝑑 𝑗 . All tags in 𝑠𝑒𝑞 𝑡 retrieve top 100 nearest items in the target domains as candidates. Similar to Eq. ( <ref type="formula" target="#formula_13">10</ref>), the score of the 𝑖-th target-domain item d𝑖 in the tag channel 𝑠 𝑡 𝑖 is defined as follows:</p><formula xml:id="formula_16">𝑠 𝑡 𝑖 = 𝑛 ∑︁ 𝑗=1 ∑︁ 𝑡 𝑘 𝑠𝑖𝑚( d𝑖 , 𝒕 𝑘 ) × 𝑠𝑎𝑡𝑖𝑠 𝑓 𝑗 × 𝑟𝑒𝑐𝑒𝑛𝑐𝑦 𝑗 × 𝑧 𝑡 (𝑖, 𝑗, 𝑘). (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>𝑠𝑖𝑚( d𝑖 , 𝒕 𝑘 ) is the cosine similarity between d𝑖 and the aggregated tag representation 𝒕 𝑘 . 𝑧 𝑡 (𝑖, 𝑗, 𝑘) is the tag's indicator. 𝑧 𝑡 (𝑖, 𝑗, 𝑘) = 1 only if the tag 𝑡 𝑘 belongs to the 𝑗-th item 𝑑 𝑗 in 𝑠𝑒𝑞, and d𝑖 locates in the top 100 nearest items of 𝑡 𝑘 . Other category, media and word channels are the same as the tag channel, generating their corresponding scores 𝑠 𝑐 𝑖 , 𝑠 𝑚 𝑖 and 𝑠 𝑤 𝑖 . As for the user channel, we directly depend on the user's gender-age-location attribute triplet's (i.e., the user group in Sec. 3.2) node representations to retrieve nearest items according to the cosine similarity score 𝑠 𝑢 𝑖 for d𝑖 . Finally, all top items retrieved by six heterogeneous channels are combined and reranked via the aggregated score 𝑠 𝑖 as:</p><formula xml:id="formula_18">𝑠 𝑖 = 𝑠 𝑢 𝑖 + 𝑠 𝑑 𝑖 + 𝑠 𝑡 𝑖 + 𝑠 𝑐 𝑖 + 𝑠 𝑚 𝑖 + 𝑠 𝑤 𝑖 . (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>It is easy to set and adjust the hyper-parameters of heterogeneous channels' weights for the practical demands and the preferences of systems. We rank top target-domain items via 𝑠 𝑖 , and select top 500 items as the final output of our multi-channel matching, considering both matching accuracy and memory/computation costs. We conclude the feasibility and advantages of our cross-domain multi-channel matching as follows: (1) These multiple matching channels rely on the similarities between the target-domain items and heterogeneous nodes, which is consistent with the neighborsimilarity based loss and the inter-CL losses. (2) The multi-channel matching makes full use of all heterogeneous information to generate diversified item candidates, which is essential in cold-start matching. (3) We pre-calculate the indexes for the top nearest items of all nodes, which greatly reduces the online computation costs. The online computation complexity of CCDR is 𝑂 (log(600𝑛)+600𝑛) (𝑛 is the length of user historical behavior). More details of online deployment and efficiency are given in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct sufficient experiments to answer the following research questions: (RQ1): How does CCDR perform against different types of competitive baselines on metrics of matching (see Sec. 5.4)? (RQ2): How does CCDR perform in online evaluation on real-world recommendation systems (see Sec. 5.5)? (RQ3): What are the effects of different components in CCDR (see Sec. 5.6)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Large-scale CDR Matching Dataset</head><p>CCDR relies on item-related taxonomy, semantic, and producer information for CDR in matching, while no large-scale public CDR dataset is capable for this setting. Therefore, we build a new CDR dataset CDR-427M extracted from a real-world recommendation system named WeChat Top Stories, which contains a source domain and two target domains. Specifically, we randomly select nearly 63 million users, and collect their 427 million behaviors on 3.0 million items. We split these behaviors into the train set and the test set using the chronological order. We also bring in 187 thousand tags, 356 categories, 56 thousand medias, and 207 thousand words as additional item-related information. All data are preprocessed via data masking to protect the user's privacy.</p><p>To simulate different CDR scenarios, we evaluate on two target domains having different cold-start degrees. The first is a few-shot target domain, where most users only have several behaviors. The second is a strict cold-start domain, which is more challenging since all user behaviors on items in the train set are discarded <ref type="bibr" target="#b26">[27]</ref>. Following Sec. 3.2, we build three diversified preference networks for all domains separately via the train set and all item-related information. The statistics of diversified preference networks in three domains are in Table <ref type="table" target="#tab_2">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Competitors</head><p>We implement several competitive baselines focusing on the matching module and cross-domain recommendation for comparisons.</p><p>Classical Matching Methods. We implement three competitive matching models as baselines. We do not compare with tree-based models <ref type="bibr" target="#b53">[54]</ref>, for they cannot be deployed in cold-start domains. All user behaviors of two domains are considered in these models.</p><p>• Factorization Machine (FM) <ref type="bibr" target="#b28">[Rendle 2010</ref>]. FM <ref type="bibr" target="#b28">[29]</ref> is a classical embedding-based matching model. It captures the feature interactions between users and items for embeddingbased retrieval under the two-tower architecture <ref type="bibr" target="#b3">[4]</ref>. • AutoInt <ref type="bibr" target="#b31">[Song et al. 2019]</ref>. AutoInt <ref type="bibr" target="#b31">[32]</ref> is a recent method that utilizes self-attention to model feature interactions. It also adopt the two-tower architecture for matching. • GraphDR+ <ref type="bibr">[Xie et al. 2021a</ref>]. GraphDR <ref type="bibr" target="#b40">[41]</ref> is an effective graph-based matching model. The single-domain model of CCDR could be considered as an enhanced GraphDR with differences in node aggregation and multi-channel matching specially designed for CDR. We directly conduct the singledomain model of CCDR on the joint network containing both source and target domains, noted as GraphDR+.</p><p>Table <ref type="table">2</ref>: Results of matching-related metrics on CDR-427M. All improvements of CCDR are significant (t-test with 𝑝 &lt; 0.01). * indicates that these models are based on the same single-domain model (noted as GraphDR+) in CCDR.</p><p>Model few-shot target domain strict cold-start target domain HIT@50 HIT@100 HIT@200 HIT@500 HIT@50 HIT@100 HIT@200 HIT@500 FM <ref type="bibr">(</ref> Cross-domain/Multi-domain Methods. We also implement two representative CDR models and one multi-domain matching model as baselines. We do not compare with CDR models like CoNet <ref type="bibr" target="#b13">[14]</ref>, since they cannot be directly used in matching for efficiency.</p><p>• EMCDR+ <ref type="bibr" target="#b22">[Man et al. 2017]</ref>. EMCDR <ref type="bibr" target="#b22">[23]</ref> is a classical CDR model that directly learns the embedding mapping of users between two domains. For fair comparisons, we use the same single-domain model and multi-channel matching in CCDR for learning and serving, noted as EMCDR+.</p><p>• SSCDR+ <ref type="bibr" target="#b16">[Kang et al. 2019]</ref>. SSCDR <ref type="bibr" target="#b16">[17]</ref> is regarded as an enhanced EMCDR, which adopts a semi-supervised loss to learn the mapping of items. We also follow the same settings of EMCDR to get SSCDR+. Since the strict cold-start domain has no user-item behaviors, we use aligned taxonomies to learn cross-domain mappings in EMCDR+ and SSCDR+.</p><p>• ICAN <ref type="bibr">[Xie et al. 2020b]</ref>. ICAN <ref type="bibr" target="#b41">[42]</ref> is the SOTA model in multi-domain matching, which is the most related work of our task. It highlights the interactions between feature fields in different domains for cold-start matching.</p><p>Knowledge Distillation/Contrastive Learning Methods. We further propose two enhanced versions of the single-domain matching model in CCDR (i.e., GraphDR+) for more challenging comparisons.</p><p>• Sub-graph CL. We build a sub-graph CL method based on GraphDR+. It considers the intra-CL loss with a sub-graph augmentation in Eq. ( <ref type="formula" target="#formula_5">4</ref>) inspired by <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref>. It can be viewed as an ablation version of CCDR without the inter-CL. • Cross-domain KD. We further propose a cross-domain knowledge distillation (KD) model. This model also follows the single-domain model of CCDR, learning the cross-domain mapping via the Hint loss <ref type="bibr" target="#b30">[31]</ref> between aligned nodes in two domains (i.e., user, tag, category, and word).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Settings</head><p>In the single-domain model of CCDR, the input dimensions of all nodes are 128, and the output dimensions are 100. We conduct a weighted neighbor sampling to select 25 and 10 neighbors for the first and second layers' aggregations. The edge weight is proportional to the mutual information between its two nodes to make sure that different types of interactions can have comparable frequencies. In online matching, we use the top 200 most recent behaviors. All graph-based models have the same online matching strategy.</p><p>The batch sizes and the negative sample numbers of the intra-CL, inter-CL, and neighbor-similarity based losses are 4, 096 and 10. The temperature 𝜏 is set to be 1. For all models, we conduct a grid search to select parameters. Parameter analyses of CL loss weights are given in Sec. 5.7. All models share the same experimental settings and multi-domain behaviors for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation of CDR in Matching (RQ1)</head><p>5.4.1 Evaluation Protocols. We evaluate on the few-shot and strict cold-start domains separately. All models select top 𝑁 items from the overall corpora for each test instance. Following classical matching models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, we utilize the top 𝑁 hit rate (HIT@N) as our evaluation metric. To simulate the real-world matching systems, we concentrate on larger 𝑁 as 50, 100, 200, and 500 (we retrieve top 500 items in online). We should double clarify that CCDR focuses on CDR in matching, which cares whether good items are retrieved, not the specific ranks that should be measured by the ranking module. Hence, HIT@N is much more suitable for matching than ranking metrics such as AUC and NDCG. We also evaluate the diversity via a classical aggregate diversity metric named item coverage <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Experimental Results</head><p>. From Table <ref type="table">2</ref> we can observe that:</p><p>(1) CCDR achieves significant improvements over all baselines on all HIT@N in both two domains, with the significance level 𝑝 &lt; 0.01 (the deviations of CCDR are within ±0.0004 in HIT@500). It indicates that CCDR can learn high-quality matching embeddings and well transfer useful knowledge to the target domain via CL. The improvements of CCDR mainly derive from three aspects: (a) The intra-CL enables more sufficient and balanced training via SSL with selected negative samples, which successfully alleviates the data sparsity and popularity bias issues. (b) The inter-CL builds interactions across different domains via three CL tasks, which multiplies the knowledge transfer via heterogeneous bridges. (c) The diversified preference network, CL losses, and multi-channel matching cooperate well with each other. The similarities used in online matching are directly optimized via losses in Eq. ( <ref type="formula" target="#formula_12">9</ref>).</p><p>(2) CCDR has large improvement percentages on the challenging strict cold-start domain (55% improvement on HIT@500 over SS-CDR+), where users have no behaviors on target items. It is natural since the combination of the diversified preference network and user/taxonomy/neighbor based inter-CL tasks can transfer more diversified preferences via more cross-domain bridges. Moreover, comparing with different CCDR models, we find that both intra-CL and inter-CL are effective, while inter-CL plays a more important role in CDR. We also find that CCDR has 4.2% and 6.0% improvements on the diversity metric item coverage <ref type="bibr" target="#b12">[13]</ref> compared to the best-performing GraphDR+ in two domains. It indicates that CCDR has better performances on the diversity via CL tasks.</p><p>(3) Among baselines, we find that ICAN performs better in the few-shot domain, while SSCDR+ performs better in the strict coldstart domain. It is because that ICAN strongly relies on the feature field interactions between behaviors in different domains, which are extremely sparse or even missing in the strict cold-start scenarios. In contrast, SSCDR+ benefits from cross-domain mapping. Moreover, classical matching methods such as GraphDR+ perform worse than CCDR. It implies that simply mixing behaviors in two domains may not get good performances, since the unbalanced domain data will confuse the user preference learning in the target domain.</p><p>(4) Comparing with different CDR methods, we observe that the CL-based methods are the most effective compared to knowledge distillation (e.g., cross-domain KD) and embedding mapping (e.g., EMCDR+ and SSCDR+). It is because that (a) contrastive learning can provide a sufficient and balanced training via SSL, and (b) CCDR conducts knowledge transfer via not only aligned users, but also taxonomies and neighbors. In this case, the popularity bias and data sparsity issues in the CDR part can be largely alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Online Evaluation (RQ2)</head><p>5.5.1 Evaluation Protocols. To verify the effectiveness of CCDR in real-world scenarios, we conduct an online A/B test on a wellknown online recommendation system named WeChat Top Stories. Precisely, we deploy CCDR and several competitive baselines in the matching module of a relatively cold-start domain as in Sec. 4, with the ranking module unchanged. The online baseline is the GraphDR+ (target) model trained solely on the target domain. In online evaluation, we focus on the following three online metrics in the target domain: (1) CTR, (2) average user duration per capita, and (3) average share rate per capita. We conduct the A/B test for 8 days, with nearly 6 million users influenced by our online evaluation.   <ref type="table" target="#tab_5">3</ref> shows the online improvement percentages of all models. We can find that:</p><p>(1) CCDR significantly outperforms all models in three metrics with the significance level 𝑝 &lt; 0.01. Note that all models are based on the same single-domain model in CCDR (i.e., GraphDR+). It reconfirms the effectiveness of the intra-domain and inter-domain contrastive learning. We jointly consider multiple behaviors such as click, share and like to build the diversified preference network, and use a neighbor-similarity based loss to learn user diverse preferences. Hence, CCDR has improvements on different metrics, which reflects user's real satisfaction more comprehensively.</p><p>(2) Comparing with the base model that only considers the target domain, we know that the source domain's information is essential. Looking into the differences among GraphDR+, Sub-graph CL (i.e., CCDR (intra-CL)), and CCDR (inter-CL), we can find that both intra-CL and inter-CL are effective in online scenarios. Moreover, CCDR models outperform GraphDR+ and Cross-domain KD, which also verifies the advantages of inter-CL over simple multi-domain mixing and cross-domain knowledge distillation in knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study (RQ3)</head><p>In this subsection, we further compare CCDR with its several ablation versions to show the effectiveness of different CL tasks. Table <ref type="table" target="#tab_6">4</ref> displays the HIT@N results on both few-shot and strict cold-start domains. We find that: (1) Both intra-CL and inter-CL are essential in few-shot and cold-start domains. Inter-CL contributes the most to the CDR performances, since it is strongly related to the knowledge transfer task in CDR and fits well with the neighbor-similarity based loss of the single-domain model. <ref type="bibr" target="#b1">(2)</ref> The intra-CL task also significantly improves the matching in CDR, while it just achieves slight improvements on the strict cold-start domain. The power of intra-CL will be multiplied when there are more user behaviors in the target domain. (3) From the second ablation group, we observe that all three inter-CL tasks can provide useful information for CDR. We observe that the user-based inter-CL functions well in the few-shot domain (since it has more user-related interactions), while taxonomy-based inter-CL achieves higher improvements in the cold-start domain. Note that CCDR does not conduct 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑢 and use user channel in the strict cold-start domain, since the cold-start user nodes are isolated in the target domain with no behaviors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Model Analyses (RQ4)</head><p>We further conduct two model analyses on different weights of the intra-CL and inter-CL losses. Fig. <ref type="figure" target="#fig_2">4</ref> displays the HIT@500 results of different intra-and inter-CL weights (𝜆 3 and 𝜆 4 ) on both fewshot and strict cold-start domains. We can find that: (1) as the loss weight increases, the HIT@500 results of both intra-CL and inter-CL losses first increase and then decrease. The best parameters are 𝜆 3 = 1.5, 𝜆 4 = 0.6. Note that the parameter analysis is carried out around the optimal parameter point. <ref type="bibr" target="#b1">(2)</ref> The performance trends of two CL loss weights are relatively consistent on both few-shot and strict cold-start domains. Moreover, CCDR models with different CL loss weights still outperform all baselines, which verifies the robustness and usability of CCDR in real-world scenarios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose a novel CCDR framework to deal with CDR in matching. We adopt the intra-CL to alleviate the data sparsity and popularity bias issues in matching, and design three inter-CL tasks to enable more diverse and effective knowledge transfer. CCDR achieves significant offline and online improvements on different scenarios, and is deployed on real-world systems. In the future, we will explore more sophisticated inter-CL tasks to further improve the effectiveness and diversity of knowledge transfer. We will also try to introduce the idea of inter-CL to other cross-domain tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of CDR in matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The neighbor-similarity loss and the intra-CL loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of different intra-/inter-CL loss weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Each user 𝑢 𝑖 has two user representations 𝒖 𝑠 𝑖 and 𝒖 𝑡 𝑖 in the source and target domains learned in Sec. 3.3. Although users may have different preferences and behavior patterns in two domains, it is still natural that the source-domain representation 𝒖 𝑠 𝑖 should be more similar with its target-domain 𝒖 𝑡 𝑖 than any other representations 𝒖 𝑡 𝑗 . We define the user-based inter-CL loss 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑢 as follows: 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑢 = − ∑︁</figDesc><table><row><cell>u i s</cell><cell>aligned users</cell><cell></cell><cell>u i t</cell><cell></cell><cell></cell><cell cols="3">sim(u i s ,u i User-based inter-CL: t )&gt;sim(u i s ,u j t )</cell></row><row><cell>t i s</cell><cell>aligned tags/</cell><cell></cell><cell>t t i</cell><cell></cell><cell></cell><cell cols="3">Taxonomy-based inter-CL:</cell></row><row><cell></cell><cell cols="2">categories/words</cell><cell></cell><cell></cell><cell></cell><cell cols="2">sim(t i s ,t i</cell><cell>t )&gt;sim(t i s ,t j</cell><cell>t )</cell></row><row><cell>e i s</cell><cell>aligned any</cell><cell></cell><cell>t e i</cell><cell cols="2">e j t</cell><cell cols="3">Neighbor-based inter-CL:</cell></row><row><cell></cell><cell>types of nodes</cell><cell></cell><cell>neigh</cell><cell>e k</cell><cell>t</cell><cell>sim(e i</cell><cell cols="2">s ,e k</cell><cell>t )&gt;sim(e i s ,e j</cell><cell>t )</cell></row><row><cell cols="2">source domain</cell><cell cols="3">target domain</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Figure 3: Three inter-CL tasks across different domains.</cell></row><row><cell></cell><cell cols="2">𝑢 𝑖</cell><cell>log</cell><cell cols="5">exp(sim(𝒖 𝑠 𝑖 , 𝒖 𝑡 𝑖 )/𝜏) 𝑢 𝑗 ∈𝑆 𝑢 𝑖 exp(sim(𝒖 𝑠 𝑖 , 𝒖 𝑡 𝑗 )/𝜏)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>𝑖𝑛𝑡𝑒𝑟 𝑛 is formalized with all aligned nodes 𝑒 𝑖 ∈ 𝐸 𝐴 and 𝑒 𝑖 's neighbor set 𝑁 𝑡 𝑒</figDesc><table><row><cell>).</cell></row><row><cell>Hence, we propose a neighbor-based inter-CL, which builds indi-</cell></row><row><cell>rect (multi-hop) connections between objects in different domains.</cell></row><row><cell>Precisely, we define 𝐸 𝐴 as the overall aligned node set (including</cell></row><row><cell>users, tags, categories, and words). The neighbor-based inter-CL</cell></row><row><cell>loss 𝐿</cell></row></table><note>𝑖 in the target domain as follows: 𝐿 𝑖𝑛𝑡𝑒𝑟 𝑛 = − ∑︁ 𝑒 𝑖 ∈𝐸 𝐴 ∑︁ 𝑒 𝑘 ∈𝑁 𝑡 𝑒 𝑖 log exp(sim(𝒆 𝑠 𝑖 , 𝒆 𝑡 𝑘 )/𝜏) 𝑒 𝑗 ∉𝑁 𝑡 𝑒 𝑖 exp(sim(𝒆 𝑠 𝑖 , 𝒆 𝑡 𝑗 )/𝜏) .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of three domains in CDR-427M.</figDesc><table><row><cell>domain</cell><cell>user</cell><cell>item</cell><cell>edge</cell><cell>train</cell><cell>test</cell></row><row><cell>source domain</cell><cell cols="4">63.1M 2.04M 175M 406M</cell><cell>/</cell></row><row><cell cols="6">few-shot target domain 2.38M 0.39M 29.8M 10.8M 4.99M</cell></row><row><cell cols="4">strict cold-start domain 2.23M 0.57M 15.0M</cell><cell>/</cell><cell>5.48M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Online A/B tests on WeChat Top Stories.5.5.2 Experimental Results. Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation tests on CDR-427M.</figDesc><table><row><cell>Model</cell><cell cols="2">few-shot</cell><cell cols="2">strict cold-start</cell></row><row><cell></cell><cell cols="4">HIT@50 HIT@500 HIT@50 HIT@500</cell></row><row><cell>CCDR</cell><cell>0.0225</cell><cell>0.1442</cell><cell>0.0161</cell><cell>0.0987</cell></row><row><cell>w/o intra-CL</cell><cell>0.0213</cell><cell>0.1327</cell><cell>0.0154</cell><cell>0.0931</cell></row><row><cell>w/o inter-CL</cell><cell>0.0174</cell><cell>0.1099</cell><cell>0.0118</cell><cell>0.0732</cell></row><row><cell>w/o 𝐿 𝑖𝑛𝑡𝑒𝑟𝑢</cell><cell>0.0211</cell><cell>0.1376</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>w/o 𝐿 𝑖𝑛𝑡𝑒𝑟𝑡</cell><cell>0.0219</cell><cell>0.1403</cell><cell>0.0149</cell><cell>0.0901</cell></row><row><cell>w/o 𝐿 𝑖𝑛𝑡𝑒𝑟𝑛</cell><cell>0.0221</cell><cell>0.1411</cell><cell>0.0157</cell><cell>0.0948</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>HIT@500 with inter-CL loss weights strict cold-start domain few-shot target domain</head><label></label><figDesc></figDesc><table><row><cell>source</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>target</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>intra</cell><cell>0.2</cell><cell>0.6</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>1.5</cell><cell>1.5</cell><cell>1.5</cell><cell>1.5</cell><cell>1.5</cell></row><row><cell>inter</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.6</cell><cell>0.2</cell><cell>0.6</cell><cell>1</cell><cell>1.5</cell><cell>2</cell></row><row><cell cols="4">HIT@500_strict 0.0948 0.0954 0.0967</cell><cell>0.0987</cell><cell>0.0971</cell><cell cols="5">0.0972 0.0987 0.0966 0.0959 0.0943</cell></row><row><cell cols="4">HIT@500_few 0.1306 0.1325 0.1345</cell><cell>0.1442</cell><cell>0.1357</cell><cell cols="5">0.1401 0.1442 0.1391 0.1373 0.1351</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.13</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell cols="3">strict cold-start domain</cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.11</cell><cell></cell><cell cols="3">few-shot target domain</cell><cell>0.11</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.09</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 . 2</cell><cell>0 . 6</cell><cell>1</cell><cell>1 . 5</cell><cell>2</cell><cell>0 . 2</cell><cell>0 . 6</cell><cell>1</cell><cell>1 . 5</cell><cell>2</cell></row><row><cell></cell><cell cols="5">(a) HIT@500 with intra-CL loss weights</cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Online Deployment and Efficiency</head><p>A.1.1 Online Deployment. We have deployed CCDR on WeChat Top Stories (a well-known recommendation system widely used by tens of millions of users) for more than three months. In practical systems, the quality, novelty, and diversity of item contents are the dominating factors that strongly impact user experiences. Hence, real-world recommendation systems often need to introduce new data sources as cold-start domains to attract users <ref type="bibr" target="#b42">[43]</ref>. CCDR is deployed on the matching module to rapidly retrieve high-quality items. The relatively few-shot domain is viewed as the target domain, while other many-shot domains are combined as the source domain. We retrieve top 500 items for each target domain in matching. These retrieved items of CCDR are combined with candidates of other domains, and are then fed into the following ranking module for the final display. In offline, we spend nearly 8 hours to train CCDR on 8 GPUs, which is acceptable for the offline training in real-world large-scale systems. CCDR is a straightforward, effective, and efficient industrial implementation for CDR in matching, which can also be easily adopted in other new domains.</p><p>A.1.2 Online Efficiency. Real-world matching should deal with more than million-level candidates, where a linear prediction complexity w.r.t. the huge corpus size is unacceptable <ref type="bibr" target="#b53">[54]</ref>. In CCDR, all embedding-based retrievals are conducted via the node embedding similarities learned in Eq. ( <ref type="formula">9</ref>), which can be pre-calculated in offline as stated in Sec. 4. The top-100 nearest items for all heterogeneous nodes are also pre-indexed in offline for efficiency. Therefore, we do not need to calculate 𝑠𝑖𝑚(•, •) in online, and the online computation mainly locates in the indexing (finding top-100 nearest item embeddings via indexes) and sorting part (selecting the final top-500 items via 𝑠 𝑖 ) of Eq. <ref type="bibr" target="#b9">(10)</ref> to Eq. <ref type="bibr" target="#b11">(12)</ref>. We have an industrylevel online indexer with cache technologies, which could largely accelerate the indexing part to 𝑂 (1) and make sure it won't become a computation bottleneck. Since we have six channels and select top-100 nearest items for each heterogeneous object, the computation complexity of the indexing part is nearly 𝑂 (600𝑛) (𝑛 is the length of the user historical behavior). Similarly, the sorting part is 𝑂 (log(6 × 100 × 𝑛)). In conclusion, the overall online computation complexity of CCDR is approximately 𝑂 (log(600𝑛) + 600𝑛), and the maximum user behavior length 𝑛 is 200. As for the memory cost, CCDR only needs space to store the nearest neighbor indexes for all nodes in the diversified preference network, which is acceptable for an industrial large-scale recommendation system.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Controllable multi-interest framework for recommendation</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions</title>
		<author>
			<persName><forename type="first">Weiyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linpeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys</title>
				<meeting>RecSys</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A multi-view deep learning approach for cross domain user modeling in recommendation systems</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mamdouh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elkahky</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-domain recommendation without sharing user-relevant data</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepFM: a factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial Feature Translation for Multi-domain Recommendation</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaikai</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating collaborative filtering recommender systems</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><forename type="middle">G</forename><surname>Terveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conet: Collaborative cross networks for cross-domain recommendation</title>
		<author>
			<persName><forename type="first">Guangneng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Embeddingbased retrieval in facebook search</title>
		<author>
			<persName><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuying</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pronin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janani</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Big Data</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semisupervised learning for cross-domain recommendation to cold-start users</title>
		<author>
			<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DDTCDR: Deep dual transfer cross domain recommendation</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
				<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Neural Network for Tag Ranking in Tag-enhanced Video Recommendation</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SDM: Sequential deep matching model for online large-scale recommender system</title>
		<author>
			<persName><forename type="first">Fuyu</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfred</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangled Self-Supervision in Sequential Recommenders</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-Domain Recommendation: An Embedding and Mapping Approach</title>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Tong Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MiNet: Mixed Interest Network for Cross-Domain Click-Through Rate Prediction</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuwu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving regularized singular value decomposition for collaborative filtering</title>
		<author>
			<persName><forename type="first">Arkadiusz</forename><surname>Paterek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
				<meeting>KDD cup and workshop</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attribute Graph Neural Networks for Strict Cold Start Recommendation</title>
		<author>
			<persName><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yile</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM</title>
				<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autoint: Automatic feature interaction learning via selfattentive neural networks</title>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Contrastive Learning for Cold-Start Recommendation</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MM</title>
				<meeting>MM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Noise contrastive estimation for one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ga</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee</forename><surname>Loong Soon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-Supervised Hypergraph Convolutional Networks for Sessionbased Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10989</idno>
		<title level="m">UPRec: User-Aware Pre-training for Recommender Systems</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Feedback Network for Recommendation</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving Accuracy and Diversity in Matching of Recommendation with Diversified Preference Network</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Internal and Contextual Attention Network for Cold-start Multi-channel Matching in Recommendation</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical Reinforcement Learning for Integrated Recommendation</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial and Contrastive Variational Autoencoder for Sequential Recommendation</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nguyen Quoc Viet Hung, and Xiangliang Zhang</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Proceedings of WWW</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A mixed heterogeneous factorization model for non-overlapping cross-domain recommendation</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junpcyeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Knowledge transfer via pre-training for recommendation: A review and prospect</title>
		<author>
			<persName><forename type="first">Zheni</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in big Data</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning personalized itemset mapping for cross-domain recommendation</title>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihong</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-domain recommendation via preference propagation GraphNet</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CATN: Cross-domain recommendation for cold-start users via aspect transfer network</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DTCDR: A framework for dual-target cross-domain recommendation</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning Tree-based Deep Model for Recommender Systems</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Leyu Lin, and Qing He. 2021. Transfer-Meta Framework for Cross-domain Recommendation to Cold-Start Users</title>
		<author>
			<persName><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaikai</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbo</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
				<meeting>SIGIR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning Optimal Tree Models under Beam Search</title>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziru</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
