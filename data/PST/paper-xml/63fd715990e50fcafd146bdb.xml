<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ChatAug: Leveraging ChatGPT for Text Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-25">25 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haixing</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhengliang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenxiong</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoke</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dajiang</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongmin</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">ChatAug: Leveraging ChatGPT for Text Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-25">25 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.13007v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large language model</term>
					<term>few-shot learning</term>
					<term>nature language processing</term>
					<term>data augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation on the training data to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can not ensure the correct labeling of the generated data (lacking faithfulness) or can not ensure sufficient diversity in the generated data (lacking completeness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named ChatAug). ChatGPT is trained on data with unparalleled linguistic richness and employs a reinforcement training process with large-scale human feedback, which endows the model with affinity to the naturalness of human language. Our text data augmentation approach ChatAug rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning text classification tasks show the superior performance of the proposed ChatAug approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE effectiveness of natural language processing (NLP)   heavily relies on the quality and quantity of the training data. With limited training data available, which is a common issue in practice due to privacy concerns or the high cost of human annotation, it can be challenging to train an accurate NLP model that generalizes well to unseen samples. The challenge of training data insufficiency is especially prominent in few-shot learning (FSL) scenarios, where the model trained on the original (source) domain data is expected to generalize from only a few examples in the new (target) domain <ref type="bibr" target="#b0">[1]</ref>. Many FSL methods have shown promising results in overcoming this challenge in various tasks <ref type="bibr" target="#b1">[2]</ref>. Existing FSL methods mainly focus on improving the learning and generalization capability of the model via better architectural design <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, leveraging pre-trained language models as the basis and then finetuning it using limited samples <ref type="bibr" target="#b5">[6]</ref> with meta-learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref> or prompt-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. However, the performance of these methods is still intrinsically limited by the data quality and quantity in both the source and target domains.</p><p>Besides model development, text data augmentation can also overcome the sample size limit and work together with other FSL methods in NLP <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Data augmentation is usually model-agnostic and involves no change to the underlying model architecture, which makes this approach particularly practical and applicable to a wide range of tasks. In NLP, there are several types of data augmentation methods. Traditional text-level data augmentation methods rely on direct operations on the existing sample base. Some frequently used techniques include synonym replacement, random deletion, and random insertion <ref type="bibr" target="#b13">[14]</ref>. More recent methods utilize language models to generate reliable samples for more effective data augmentation, including backtranslation <ref type="bibr" target="#b14">[15]</ref> and word vector interpolation in the latent space <ref type="bibr" target="#b15">[16]</ref>. However, existing data augmentation methods are limited in the accuracy and diversity of the generated text data, and human annotation is still mandatory in many application scenarios <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>The advent of (very) large language models (LLMs) such as the GPT family <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b18">[19]</ref> brings new opportunities for generating text samples that resemble human-labeled data, which significantly alleviates the burden of human anno-tators <ref type="bibr" target="#b19">[20]</ref>. LLMs are trained in self-supervised manners, which scale up with the near-infinite amount of text corpus available in the open domains. The large parameter space of LLMs also allows them to store a large amount of knowledge, while large-scale pre-training (e.g., the autoregressive objective in training GPTs) enables LLMs to encode rich factual knowledge for language generation. Furthermore, the training of ChatGPT follows that of Instruct-GPT <ref type="bibr" target="#b20">[21]</ref>, which utilizes reinforcement learning with human feedback (RLHF), thus enabling it to produce more informative and impartial responses to input.</p><p>Inspired by the success of applying language models in text generation, we propose a new data augmentation method named ChatAug, which leverages ChatGPT to generate auxiliary samples for few-shot text classification. We have tested the performance of ChatAug via experiments on both general domain and medical domain datasets. Performance comparison of the proposed ChatAug approach with existing data augmentation methods shows double-digit improvements in sentence classification accuracy. Further investigation into the faithfulness and completeness of the generated text samples reveals that ChatAug can generate more diversified augmented samples while simultaneously maintaining their accuracy (i.e., semantic similarity to the data labels). We envision that the development of LLMs will lead to human-level annotation performance, thus revolutionizing the field of few-shot learning and many tasks in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Augmentation</head><p>Data augmentation, the artificial generation of new text through transformations, is widely used to improve model training in text classification. In NLP, existing data augmentation methods work at different granularity levels: characters, words, sentences and documents.</p><p>Data augmentation at the character level refers to the method of randomly inserting, exchanging, replacing or deleting some characters in the text <ref type="bibr" target="#b21">[22]</ref>, which improves the robustness of the NLP model against noises in text data. Another method called optical character recognition (OCR) data augmentation generates new text by simulating the errors that occur when using OCR tools to recognize text from pictures. Spelling augmentation <ref type="bibr" target="#b22">[23]</ref> deliberately misspells some frequently misspelled words. Keyboard augmentation <ref type="bibr" target="#b21">[22]</ref> simulates random typo errors by replacing a selected key with another key close to it on the QWERTY layout keyboard.</p><p>Data augmentation also works at the word level. Random swap augmentation randomly exchanges two words in the text, and random deletion augmentation randomly deletes some words <ref type="bibr" target="#b23">[24]</ref>. Synonym augmentation uses synonym databases such as PPDB <ref type="bibr" target="#b24">[25]</ref> to replace randomly selected words <ref type="bibr" target="#b25">[26]</ref>. WordNet <ref type="bibr" target="#b27">[27]</ref> is also widely used as a reference for synonym augmentation. This method maintains semantic consistency in samples and is suitable for text classification tasks. Wang et al. <ref type="bibr" target="#b28">[28]</ref> proposed a data augmentation method based on word embeddings, which replaces selected words with their top-n similar words to create a new sentence. Different pre-trained word embeddings are considered (e.g., GoogleNews Lexical Embeddings <ref type="bibr" target="#b29">[29]</ref>). This method is based on the principle that words close to each other in the embedding space often appear in similar contexts, which might help with maintaining grammatical consistency.</p><p>However, a serious limitation of word embedding-based methods is that close words in the embedding space are not necessarily semantically similar, yet semantic changes can affect the classification results. For example, "hot" and "cold" usually appear in similar contexts, so their word embeddings are close, but they have exactly opposite semantic meanings. The counter-fitting embedding augmentation <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref> solves this problem by using a synonym dictionary and an antonym dictionary to adjust the initial word embeddings. Specifically, the distance between embeddings of synonyms will be shortened, and the distance between embeddings of antonyms will become enlarged.</p><p>Contextual augmentation <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref> is another wordlevel data augmentation method, which uses masked language models (MLMs) such as BERT <ref type="bibr" target="#b34">[34]</ref>, DistilBERT <ref type="bibr" target="#b35">[35]</ref> and RoBERTA <ref type="bibr" target="#b36">[36]</ref> to generate new text based on the context. Specifically, they insert &lt; mask &gt; tokens in some positions of the text, or replace some words in the text with &lt; mask &gt; tokens, and then let the MLM predict what words should be put in these masked positions. Since MLMs are pre-trained on a large number of texts, contextual augmentation can usually generate meaningful new texts. Some text data augmentation methods work at the sentence and document level. For example, back translation augmentation <ref type="bibr" target="#b37">[37]</ref> uses language translation models for data augmentation. Specifically, the language model first translates the text into another language, and then translates it back to the original language. Due to the randomness of the translation process, the augmented text is different from the original text, but semantic consistency is maintained. At the document level, Gangal et al. <ref type="bibr" target="#b38">[38]</ref> proposed a method to paraphrase the entire document to preserve document level consistency.</p><p>In general, regardless of the granularity level or the text generation backbone (i.e., rule-based methods or language models), the goal of data augmentation is to produce sensible and diverse new samples that maintain semantic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-shot Learning</head><p>Deep learning has achieved remarkable success in various data-intensive applications. However, the performance of deep models could be affected if the dataset size is small in the downstream tasks. Few-shot Learning is a branch of science that focuses on developing solutions to address the challenge of small sample sizes <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b39">[39]</ref>. FSL research aims to leverage prior knowledge to rapidly generalize to new tasks that contain only a few labeled samples. A classic application scenario for few-shot learning is when obtaining supervised examples is difficult or not possible due to privacy, safety, or ethical considerations. The development of few-shot learning enables practitioners to improve the efficiency and accuracy of text classification in various scenarios and deploy practical applications.</p><p>Recent advances in few-shot learning have shown promising results in overcoming the challenges of limited training data for text classification. For example, a common approach in NLP is to use a pre-trained language model such as BERT <ref type="bibr" target="#b5">[6]</ref> as a starting point and then fine-tune it with limited samples. Some of the most recent methodological developments <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b40">[40]</ref> approaches that have gained traction include prompt-tuning <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and metalearning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In general, existing FSL methods target either architectural design <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, data augmentation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> or the training process <ref type="bibr" target="#b41">[41]</ref>.</p><p>Despite the recent development of prompt-tuning and meta-learning methods, they suffer from some major limitations. For example, prompt engineering is a cumbersome art that requires extensive experience and manual trial-anderrors <ref type="bibr" target="#b42">[42]</ref>. Meta-learning, on the other hand, suffers from problems such as training instability <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref> and sensitivity to hyper-parameters <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>. In addition, all these FSL pipelines demand deep machine learning expertise and acquaintance with complex model architectures and training strategies, which are not attainable by common practitioners and general developers. As discussed in section 2.1, data augmentation is an effective solution for FSL and can be combined with other FSL models. Thus, the ChatAug method proposed in this paper, which has demonstrated the capability to generate accurate and comprehensive training samples, can overcome the issues of current FSL methods and potentially change the landscape of few-shot learning in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Very Large Language Models</head><p>Pre-trained language models (PLMs) based on the transformer architecture, such as the BERT <ref type="bibr" target="#b5">[6]</ref> and GPT <ref type="bibr" target="#b46">[46]</ref> model families, have revolutionized natural language processing. Compared to previous methods, they deliver stateof-the-art performance on a wide range of downstream tasks and contribute to the rising popularity and democratization of language models. In general, there are three classes of pretrained language models: autoregressive language models (e.g., the decoder-based GPT), masked language models (e.g., the encoder-based BERT) and encoder-decoder models(e.g., BART <ref type="bibr" target="#b47">[47]</ref> and T5 <ref type="bibr" target="#b48">[48]</ref>). These models typically contain between 100M and 1B parameters <ref type="bibr" target="#b18">[19]</ref>.</p><p>In recent years, NLP communities have witnessed the rise of very large language models such as GPT-3 (175B parameters) <ref type="bibr" target="#b7">[8]</ref>, PaLM (540B parameters) <ref type="bibr" target="#b49">[49]</ref>, Bloom (176B parameters) <ref type="bibr" target="#b50">[50]</ref>, OPT (up to 175B parameters) <ref type="bibr" target="#b51">[51]</ref>, and the FLAN series (FLAN has 137B parameters) <ref type="bibr" target="#b52">[52]</ref>. At their core, these large language models are transformer models inspired by BERT and GPT, albeit at a much larger scale.</p><p>Large language models aim to learn accurate latent feature representations of input text. These representations are often context-dependent and domain-dependent. For example, the vector representation of the word "treat" might be vastly different between medical domains and the general domain. For smaller pre-trained language models, it is often necessary to continuously pre-train and fine-tune such models to attain acceptable performance <ref type="bibr" target="#b53">[53]</ref>. However, very large language models can potentially eliminate the need for fine-tuning while maintaining competitive performance <ref type="bibr" target="#b7">[8]</ref>, The follow-up rate after 5 years was 85%.</p><p>1. The study showed a follow-up rate of 85% after 5 years.</p><p>2. The 5-year follow-up rate was found to be significantly high at 85%. 3. The participants had an impressive follow-up rate of 85% after 5 years. 4. The data indicated a follow-up rate of 85% 5 years post-study. 5. The 5-year follow-up rate was recorded as 85%, according to the findings. 6. The results of the study showed a follow-up rate of 85% after a 5-year period.</p><p>This means that the maturation of calluses was 27% faster in the LIPUS group.</p><p>1. The LIPUS group showed a 27% acceleration in callus maturation.</p><p>2. The results revealed that the callus maturation process was 27% faster in the LIPUS group.</p><p>3. The study found that the callus maturation rate in the LIPUS group was 27% quicker. 4. Callus maturation was 27% more rapid in the LIPUS group compared to others. 5. The callus maturation process in the LIPUS group was 27% more efficient, according to the results.</p><p>Test Registration: IRCT.ir IRCT2012071010230N1.</p><p>1. The trial was registered with the IRCT under the identifier IRCT2012071010230N1.  <ref type="bibr" target="#b54">[54]</ref>.</p><p>Existing studies indicate that pre-trained language models can help augment a dataset with new samples with similar semantic meaning <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, which is of significant practical value to real-world applications. In this study, we aim to use ChatGPT, a popular LLM to conduct data augmentation. ChatGPT is based on GPT-3 <ref type="bibr" target="#b7">[8]</ref>, which was trained on massive web data with diverse and rich information. Furthermore, ChatGPT was trained through Reinforcement learning from Human Feedback (RLHF). During RLHF, human feedback is incorporated into the process of generating and selecting the best results. More specifically, a reward model is trained based on human annotators' ranking or generated results. In turn, this reward model rewards model outputs that are most aligned with human preference and human values. We believe these innovations make ChatGPT the best candidate for generating humanlevel quality data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">ChatGPT: Present and Future</head><p>ChatGPT is a game changer in natural language processing. Indeed, for the first time in human history, the power of large language models is accessible to the general public through a user-friendly chatbot interface. In turn, this common accessibility contributes to ChatGPT's unprecedented popularity. Millions of users further unlock the potential of language models, which introduces myriad possibilities for new use cases.</p><p>ChatGPT has emerged as a general-purpose problem solver for many NLP applications <ref type="bibr" target="#b55">[55]</ref>. Qin et al. <ref type="bibr" target="#b55">[55]</ref> evaluated ChatGPT on a comprehensive set of NLP tasks, including common benchmarks in natural language inference, arithmetic reasoning, named entity recognition, sentiment analysis, question answering, dialogue and summarization. They conclude that ChatGPT excels in most tasks, except for tasks that focus on specific details (e.g., sequence tagging).</p><p>ChatGPT is also a valuable solution for multilingual tasks. A recent empirical study <ref type="bibr" target="#b56">[56]</ref> reports that ChatGPT excels at tasks involving high-resource languages (various European languages and Chinese) and is comparable with Google Translate, DeepL Translate and Tencent TranSmart. Nonetheless, ChatGPT performs poorly on low-resource languages and faces extra challenges handling distant language translation (i.e., English-German translation is considered to be less "distant", compared to English-Hindi translation). A later study <ref type="bibr" target="#b57">[57]</ref> confirms that ChatGPT struggles with low-resource languages, although the authors observe that ChatGPT does better in understanding non-Latin scripts than generating them.</p><p>In addition, it is also possible to use the purely textbased ChatGPT to interact with multimodal data. A group of researchers <ref type="bibr" target="#b57">[57]</ref> use HTML Canvas and Python Turtle graphics as media for text-to-image generation. ChatGPT can faithfully generate HTML and Python code, which can be then used to generate desired images. The authors designed a flag drawing task that required ChatGPT to generate code that can generate country flags. It was found that ChatGPT could generate better flags when the prompt for code was preceded by a prompt that queries ChatGPT for the flag's description. In other words, descriptive text prompts could improve multimodal task performance.</p><p>Beyond computer science, ChatGPT can be readily applied to medical report generation and comprehension <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>, education <ref type="bibr" target="#b60">[60]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>, rigorous math research <ref type="bibr" target="#b63">[63]</ref> and finance <ref type="bibr" target="#b64">[64]</ref>. Overall, ChatGPT is a versatile tool that promotes general AI usage.</p><p>However, researchers are also cautious about the possible negative impact of ChatGPT. Some of the more prominent concerns are related to bias <ref type="bibr" target="#b65">[65]</ref>, <ref type="bibr" target="#b66">[66]</ref>, ethics <ref type="bibr" target="#b67">[67]</ref>, <ref type="bibr" target="#b68">[68]</ref>, plagiarism <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b70">[70]</ref> and job replacement en masse <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b72">[72]</ref>. In response, a commentary published in Nature advocates for urgent attention to accountability, open-source large language models and societal embrace of AI <ref type="bibr" target="#b65">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET</head><p>In this work, we use clinical natural language processing (clinical NLP) as the task and carry out our experiments on two popular public benchmarks. Data augmentation is particularly in demand in clinical NLP, because the significant burden of expert annotation and stringent privacy regulations make large-scale data labeling infeasible. We will describe these datasets in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Symptoms Dataset</head><p>This dataset is published on Kaggle 1 . It contains the audio data of common medical symptom descriptions over 8 hours. We use the text transcripts corresponding to the audio data and perform sample de-duplication. The dataset after preprocessing includes 231 samples of 7 symptom categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PubMed20k Dataset</head><p>PubMed20K is a widely used dataset in natural language processing (NLP) and text mining research. It consists of approximately 20,000 scientific abstracts from the biomedical domain that have been annotated with task-specific labels, such as named entities (e.g., genes, diseases, chemicals), relations between entities, and other semantic roles. The dataset has been used for developing and evaluating machine learning models for various NLP tasks, such as named entity recognition, relation extraction, and text classification.</p><p>PubMed20K is constructed based on the PubMed database, which is a large collection of biomedical literature maintained by the US National Library of Medicine. The abstracts in PubMed20K cover a wide range of topics in biomedicine, including genomics, pharmacology, and clinical medicine. Due to its size, diversity, and high-quality annotations, PubMed20K has become a popular benchmark dataset for evaluating the performance of machine learning models in biomedical NLP <ref type="bibr" target="#b73">[73]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Augmentation with ChatGPT</head><p>Similar to GPT <ref type="bibr" target="#b46">[46]</ref>, GPT-2 <ref type="bibr" target="#b74">[74]</ref>, and GPT-3 <ref type="bibr" target="#b7">[8]</ref>, ChatGPT belongs to the family of autoregressive language models and uses transformer decoder blocks <ref type="bibr" target="#b75">[75]</ref> as the model backbone.</p><p>During pre-training, ChatGPT is regarded as an unsupervised distribution estimation from a set of samples X = {x 1 , x 2 , ..., x n }, and sample x i composed of m tokens is defined as x i = (s 1 , s 2 , ..., s m ). The objective of pretraining is to maximize the following likelihood:</p><formula xml:id="formula_0">L(x i ) = m i=1 log P (s i |s 1 , ..., s i-1 ; ?)<label>(1)</label></formula><p>where ? represents the trainable parameters of ChatGPT. The tokens are represented by token embedding and position embedding:</p><formula xml:id="formula_1">h 0 = x i W e + W p<label>(2)</label></formula><p>where W e is the token embedding matrix and W p is the position embedding matrix. Then N transformer blocks are used to extract the features of the sample:</p><formula xml:id="formula_2">h n = transf ormer blocks(h n-1 )<label>(3)</label></formula><p>where n ? [1, N ]. Finally, the target token is predicted:</p><formula xml:id="formula_3">s i = sof tmax(h N W T e )<label>(4)</label></formula><p>where h N is the output of top transformer blocks. After pre-training, the developers of ChatGPT apply Reinforcement Learning from Human Feedback (RLHF) <ref type="bibr" target="#b20">[21]</ref> to fine-tune the pre-trained language model. The RLHF aligns language models with user intent on a wide range of tasks by fine-tuning them according to human feedback. The RLHF of ChatGPT contains three steps: Supervised Fine-tuning (SFT): Unlike GPT, GPT-2, and GPT-3, ChatGPT uses labeled data for further training. The AI trainers play as users and AI assistants to build the answers based on prompts. The answers with prompts build as supervised data for further training the pre-trained model. After further pre-training, SFT model can be obtained.</p><p>Reward Modeling (RM): Based on the SFT method, a reward model is trained to input a prompt and response, and output a scalar reward. The labelers rank the outputs from best to worst to build a ranking dataset. The loss function between two outputs is defined as follows: loss(? r ) = E (x,yw,y l )?Dc [log (? (r ?r (x, y w ) -r ?r (x, y l )))]</p><p>(5) where ? r is the parameters of reward model; x is the prompt, y w is the preferred completion out of the pair of y w and y l ; D c is the dataset of human comparisons.</p><p>Reinforcement Learning (RL): By using reward models, ChatGPT can be fine-tuned using Proximal Policy Optimization (PPO) <ref type="bibr" target="#b76">[76]</ref>. In order to fix the performance regressions on public NLP datasets, the RLHF mix the pretraining gradients into the PPO gradients, which also known as PPOptx:</p><formula xml:id="formula_4">objective(?) = ?E x?Dpretrain log ? RL ? (x) + E (x,y)?D ? RL ? r ?r (x, y) -? log ? RL ? (y | x)/? SFT (y | x)<label>(6</label></formula><p>) where ? RL ? is the learned RL policy, ? SFT is the supervised trained model, and D pretrain is the pretraining distribution. The ? is the pre-training loss coefficient that controls the strength of pre-training gradients, and the ? is the KL (Kullback-Leibler) reward coefficient that controls the strength of the KL penalty.</p><p>Compared with previous data augmentation methods, ChatGPT is more suitable for data augmentation because of the following reasons:</p><p>? ChatGPT is pre-trained with large-scale corpus, so it has a broader semantic expression space, and is helpful to enhance the diversity of data augmentation.</p><p>? Since the fine-tuning stage of ChatGPT introduces a large number of manual annotation samples, the language generated by ChatGPT is more in line with human expression habits.</p><p>? Through reinforcement learning, ChatGPT can compare the advantages and disadvantages of different expressions and ensure that the augmentative data with higher quality.</p><p>Under the BERT framework, we introduce ChatGPT as the data augmentation tool for few-shot text classification. Specifically, ChatGPT is applied to rephrase each input sentence into six additional sentences, thereby augmenting the few-shot samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Few-shot Text Classification</head><p>We apply BERT <ref type="bibr" target="#b77">[77]</ref> to train a few-shot text classification model. The output features h of the top layer of BERT can be written as:</p><formula xml:id="formula_5">z = [z c , z 1 , z 2 , ..., z n ],<label>(7)</label></formula><p>where the z c is the representation of the class special token CLS. For text classification, the z c is usually fed into a taskspecific classifier header for final prediction. However, in the scenario of FSL, it is difficult to achieve satisfactory performance through fine-tuning BERT because few-shot samples will easily lead to over-fitting and lack of generalization ability.</p><p>To effectively address the challenge of few-shot text classification, many approaches have been proposed. Generally, there are four categories of methods for few-shot text classification based on large language models: metalearning, prompt-tuning, model design, and data augmentation. meta-learning refers to the process of learning to learn with tasks that update meta-parameters <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Promptbased methods guide large language models to predict correct results by designing templates <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Model design methods guide the model to learn from few-shot samples by changing the structure of the model <ref type="bibr" target="#b78">[78]</ref>. Data augmentation uses similar characters <ref type="bibr" target="#b21">[22]</ref>, similar word semantics <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, or knowledge base <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b79">[79]</ref> to expand samples. Our method directly data augmentation through the language capabilities of large language models, which is a simple and efficient data augmentation method.</p><p>Objective Function: Our objective function of few-shot learning consists of two parts: cross entropy and contrastive learning loss. We feed z c into a fully connected layer as the classifier for the final prediction:</p><formula xml:id="formula_6">? = W T c z c + b c ,<label>(8)</label></formula><p>where W c and b c are trainable parameters, and take crossentropy as one of the objective functions:</p><formula xml:id="formula_7">L CE = - d?D C c=1 y dc ln ?dc , (<label>9</label></formula><formula xml:id="formula_8">)</formula><p>where C is the output dimension, which is equal to the union of label spaces of the base dataset and novel dataset, and y d is the ground truth. Then, to make full use of the prior knowledge in the base dataset to guide the learning of the novel dataset, we introduce the contrastive loss function to make the sample representation of the same category more compact, and the sample representation of different categories more separate. The contrastive loss between pairs of samples in the same batch is defined as follows:</p><formula xml:id="formula_9">L CL = -log e cos(vi,v i ) e cos(vi,v i ) + e cos(vi,vj ) , (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where v i and v i are the z c of samples that belong to the same category; v i and v j are the z c of samples belong to different categories; cos(?; ?) is the cosine similarity.</p><p>In the BERT fine-tuning stage on the base dataset, we only use cross entropy as the objective function. In the few-shot learning stage, we combine cross entropy and contrastive learning loss as the objective function:</p><formula xml:id="formula_11">L = L CE + ?L CL .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baseline Methods</head><p>In the experiment section, we compared our method with other popular data augmentation methods. For these methods, we use the implementation in open source libraries including nlpaug <ref type="bibr" target="#b80">[80]</ref> and textattack <ref type="bibr" target="#b81">[81]</ref>.</p><p>? InsertCharAugmentation. This method inserts random characters at random locations in text, which improves the generalization ability of the model by injecting noise into the data.</p><p>? SubstituteCharAugmentation. This method randomly replaces selected characters with other ones.</p><p>? SwapCharAugmentation <ref type="bibr" target="#b21">[22]</ref>. This method randomly exchanges two characters.</p><p>? DeleteCharAugmentation. This method randomly deletes characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>OCRAugmentation. OCRAugmentation simulates possible errors during OCR recognition. For example, OCR tool may wrongly identify "0" as "o", and wrongly identify "I" as "l".</p><p>? SpellingAugmentation <ref type="bibr" target="#b22">[23]</ref>. It creates new text by deliberately misspelling some words. The method uses a list of English words that are most likely to be misspelled provided by Oxford Dictionary, for example, misspelling "because" as "becouse".</p><p>? KeyboardAugmentation <ref type="bibr" target="#b21">[22]</ref>. It simulates typo error by replacing randomly selected characters with the adjacent characters in the QWERTY layout keyboard. For example, replacing 'g' with 'r', 't', 'y', 'f', 'h', 'v', 'b' or 'n'.</p><p>? SwapWordAug <ref type="bibr" target="#b23">[24]</ref>. It randomly exchanges words in text. This method is a submethod of Easy Data Augmentation (EDA) proposed by Wei et al.</p><p>? DeleteWordAug. DeleteWordAug randomly deletes words in the text, which is also a submethod of EDA.</p><p>? PPDBSynonymAug <ref type="bibr" target="#b25">[26]</ref>. It replaces words with their synonym in PPDB thesaurus. Synonym replacement can ensure semantic consistency and is suitable for classification tasks.</p><p>? WordNetSynonymAug. It replaces words with their synonym in WordNet thesaurus.</p><p>? SubstituteWordByGoogleNewsEmbeddings <ref type="bibr" target="#b28">[28]</ref>. It replaces words with their top-n similar words in the embedding space. The word embeddings used are pre-trained with GoogleNews corpus.</p><p>? InsertWordByGoogleNewsEmbeddings <ref type="bibr" target="#b80">[80]</ref>. It randomly selects word from vocabulary of GoogleNews corpus and inserts it the random position of the text.</p><p>? CounterFittedEmbeddingAug <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>. It replaces words with their neighbors in counter-fitting embedding space. Compared with GoogleNews word vectors used by SubstituteWordByGoogleNewsEmbeddings, counter-fitting embedding introduces the constraint of synonyms and antonyms, that is, the embedding between synonyms will be pulled closer, and vice versa.</p><p>? ContextualWordAugUsingBert(Insert) <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>. This method uses BERT to insert words based on context, that is, add &lt; mask &gt; token at random position of the input text, and then let BERT predict the token at that position.</p><p>? ContextualWordAugUsingDistilBERT(Insert). This method uses DistilBERT to replace BERT for prediction, and the rest is the same as ContextualWordAu-gUsingBert(Insert).</p><p>?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ContextualWordAugUsingRoBERTA(Insert).</head><p>This method uses RoBERTA to replace BERT for prediction, and the rest is the same as ContextualWordAugUsingBert(Insert).</p><p>? ContextualWordAugUsingBert(Substitute). This method <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref> uses BERT to replace words based on context, that is, replace randomly selected words in text with &lt; mask &gt; token, and then let BERT predict the token at that position.</p><p>? ContextualWordAugUsingDistilBERT(Substitute). This method uses DistilBERT to replace BERT for prediction, and the rest is the same as ContextualWordAugUsingBert(Substitute).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ContextualWordAugUsingRoBERTA(Substitute).</head><p>This method uses RoBERTA to replace BERT for prediction, and the rest is the same as ContextualWordAugUsingBert(Substitute).</p><p>? BackTranslationAug. The method <ref type="bibr" target="#b37">[37]</ref> translates the text into German and then into English, resulting in a new text that is different from the original but has the same semantics. We use wmt19-en-de and facebook/wmt19-de-en language translation models <ref type="bibr" target="#b82">[82]</ref> developed by Facebook for translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Metrics</head><p>We employed cosine similarity and TransRate <ref type="bibr" target="#b83">[83]</ref> as metrics to assess the completeness (i.e., whether features contain sufficient information about a target task) and compactness (i.e., whether features of each class are compact enough for good generalization) of our augmented data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Embedding Similarity</head><p>To evaluate the semantic similarity between the samples generated by data augmentation methods and actual samples, we adopt embedding similarity between the generated samples and the actual samples of the test dataset. Some of the most common similarity metrics include Euclidean distance, cosine similarity and dot product similarity. In this study, we select cosine similarity to capture the distance relationship in the latent space. The cosine similarity measures the cosine value of the angle between two vectors. This value increases when two vectors are more similar, and is bounded by a range between 0 and 1. We input sample into pre-trained BERT, and use the representation of the CLS token as sample embedding. The cosine similarity metric is commonly used in NLP <ref type="bibr" target="#b84">[84]</ref> and we follow this convention.</p><formula xml:id="formula_12">cos(?) = A ? B A 2 B 2 ,<label>(12)</label></formula><p>where A and B denote the two embedding vectors in comparison, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">TransRate</head><p>TransRate is a metric that quantifies transferability based on the mutual information between the features extracted by a pre-trained model and their labels, with a single pass through the target data. The metric achieves a minimum value when the data covariance matrices of all classes are identical, making it impossible to distinguish between the data from different classes and preventing any classifier from achieving better than random guessing. Thus, a higher TransRate could indicate better learnability of the data. More specifically, knowledge transfer from a source task T s to a target task T t is measured as shown below:</p><formula xml:id="formula_13">T rR Ts?Tt (g) = H(Z) -H(Z|Y ),<label>(13)</label></formula><p>where Y represents the labels of augmented examples, and Z denotes the latency embedding features extracted by the pre-trained feature extractor g. T rR means the TransRate value. H(?) denotes the Shannon entropy <ref type="bibr" target="#b85">[85]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT RESULTS</head><p>In our experiments, we use BERT as the base model. First we train our model on the base dataset to get the pretrained model. Then we fine-tune the model with the few-shot samples, where we employ different data augmentation methods to generate the augmented samples. We feed those samples into BERT model to fine-tune the pretrained models.   on the PubMed20K dataset, we adopt the same training configuration, with the maximum sequence length set to 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification Performance Comparison</head><p>Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table">3</ref> show that ChatAug achieves the highest accuracy for both Symptoms and PubMed20K datasets.</p><p>In the PubMed20K dataset, ChatAug achieves accuracies of 83.5% for both BERT and BERT with contrastive loss, whereas without data augmentation, the accuracy is only 79.2% and 79.8%, respectively. In the Symptoms dataset, the accuracy for BERT without data augmentation is only 63.6%, and 60.6% with Contrastive loss. However, our ChatAug approach significantly improves the accuracy to 88.9% and 89.9%, respectively. These results suggest that data augmentation using ChatGPT is more effective for enhancing the performance of machine learning models in various applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of Augmented Datasets</head><p>In this section, we evaluate the performance of our augmented data in the latent space and visualize the results in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND DISCUSSION</head><p>In this paper, we proposed a novel data augmentation approach for few-shot classification. Unlike other methods, our model expands the limited data at the semantic level to enhance data consistency and robustness, which results in a better-performing trained model. Although ChatAug has shown promising results in data augmentation, it has certain limitations. For example, in recognizing and expanding medical texts, it may produce incorrect augmentation data due to the lack of domain knowledge. In future research, we may fine-tune the original model first and then perform data augmentation to address this issue.</p><p>The proposed ChatAug method has shown promising results in text classification. A promising direction for future research is to investigate the effectiveness of ChatAug on a wider range of downstream tasks. For example, given the strong ability of ChatGPT to extract key points and understand sentences, we can foresee potential promising results in text summarization. Specifically, ChatGPT might be valuable for domain-specific science paper summarization <ref type="bibr" target="#b86">[86]</ref> and clinical report summarization <ref type="bibr" target="#b87">[87]</ref>. Publicly available domain-specific science paper summarization datasets and clinical report datasets are rare and are often provided at small scales due to privacy concerns and the need for expert knowledge to generate annotated summaries. However, ChatGPT could address this challenge by generating diverse augmented summarization samples in different representation styles. The data generated from ChatGPT are typically concise, which can be valuable for further enhancing the generalization capabilities of the trained model.</p><p>The dramatic rise of generative image models such as DALLE2 <ref type="bibr" target="#b88">[88]</ref> and Stable Diffusion <ref type="bibr" target="#b89">[89]</ref> provides opportunities for applying ChatAug to few-shot learning tasks in computer vision. For example, accurate language descriptions may be used to guide the generative model to generate images from text or to generate new images based on existing images as a data augmentation method for fewshot learning tasks, especially when combined with efficient fine-tuning methods <ref type="bibr" target="#b90">[90]</ref>, <ref type="bibr" target="#b91">[91]</ref> such as LoRA for Stable Diffusion. Thus, prior knowledge from a large language model can facilitate faster domain adaptation and better few-shot learning of generative models in computer vision.</p><p>Recent research shows that large language models (LLMs), such as GPT-3 and ChatGPT, are capable of solving Theory of Mind (ToM) tasks, which were previously thought to be unique to humans <ref type="bibr" target="#b92">[92]</ref>. While the ToM-like capabilities of LLMs may be an unintended byproduct of improved performance, the underlying connection between cognitive science and the human brain is an area ripe for exploration. Advancements in cognitive and brain science can also be used to inspire and optimize the design of LLMs. For example, it has been suggested that the activation patterns of the neurons in the BERT model and those in the human brain networks may share similarities and could be coupled together <ref type="bibr" target="#b93">[93]</ref>. This presents a promising new direction for developing LLMs by utilizing prior knowledge from brain science. As researchers continue to investigate the connections between LLMs and the human brain, we may discover new means to enhance the performance and capabilities of AI systems, leading to exciting breakthroughs in the field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of ChatAug. a (top panel): First, we apply ChatGPT for data augmentation. We input samples of all classes into ChatGPT and prompt ChatGPT to generate samples that preserves semantic consistency with existing labelled instance. b (bottom panel): In the next step, we train a BERT-based sentence classifier on the few-shot samples and the generated data samples and evaluate the model's classification performance.</figDesc><graphic url="image-1.png" coords="3,75.60,43.70,460.81,283.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1 Algorithm 1</head><label>11</label><figDesc>Overall Framework 1. https://www.kaggle.com/datasets/paultimothymooney/medicalspeech-transcription-and-intent The framework of ChatAug for few-shot text classification. Input: base dataset D b and novel dataset D n Initialize: Initialized pre-trained BERT model Definition: D is the dataset with the base dataset D b and augmented dataset D aug n , and chatGPT aug is the data augmentation method based on ChatGPT Parameters: Fine-tuning epochs of base dataset epoch b , finetuning epochs of FSL epoch f for epoch in epoch b do train(model, D b ) end for D aug n = chatGPT aug(D n ) D = D n ? D aug n for epoch in epoch f do train(model,D ) end for Given a base dataset D b = {(x i , y i )} N b i=1 with a label space y i ? Y b , a novel dataset D n = {(x j , y j )} Nn j=1 with a label space y j ? Y n , and Y b ? Y n = ?. In the few-shot classification scenario, the base dataset D b has a relatively larger set of labeled samples, while the novel dataset D n has only a few labeled samples. The performance of fewshot learning is evaluated on the novel dataset. Our goal is to train a model with both base and limited novel datasets, while achieving satisfying generalizability on the novel dataset. The overall framework of ChatAug is shown in Fig 1, and the training steps are shown in Algorithm 1. First of all, we fine-tune BERT on D b . Then, the D aug n is generated by data augmentation with ChatGPT. Finally, we fine-tune BERT with D = D n ? D aug n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. We employed two evaluation metrics to assess the completeness and compactness of our newly augmented data. For the top left plot, we displayed the cosine similarity metric and final accuracy of all data augmentation methods on the Symptoms dataset. For the top right plot, we showed the TransRate metric and final accuracy of all data augmentation methods on the Symptoms dataset. In the bottom panel, we plotted the cosine similarity and TransRate values of all data augmentation methods on the PubMed20K dataset. And on the right side of the picture, we listed all the augmented methods with different colors and shapes.</figDesc><graphic url="image-4.png" coords="9,85.40,199.61,152.79,116.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 2 .</head><label>2</label><figDesc>Latent embeddings are evaluated using cosine similarity and the TransRate metric (see section 4.5 for more details). The horizontal axis represents the cosine similarity values and Transrate values, and the vertical axis describes the classification accuracy. Since embedded similarity measures the similarity between the augmentative data and the test dataset, the higher similarity means that the augmentative data more matched with the real data, and with higher completeness and compactness. As higher TransRate could indicate better learnability of the data, the higer TransRate means the augmentative data with higer quality. The most ideal candidate method should be positioned at the top-right of the visualization. As shown in Fig 2, ChatAug produces high-quality samples in terms of both completeness and compactness on the Symptoms dataset and the PubMed20K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Haixing Dai, Zhengliang Liu, Zihao Wu, Lin Zhao, Ninghao Liu and Tianming Liu are with the School of Computing, University of Georgia, Athens, GA, USA. (e-mail: {hd54134, zl18864 ,zw63397, lin.zhao, ninghao.liu, tliu}@uga.edu). ? Wenxiong Liao, Xiaoke Huang, Hongmin Cai are with the School of Computer Science and Engineering, South China University of Technology, China. (e-mail: {cswxliao@mail.scut.edu.cn, csxkhuang@mail.scut.edu.cn, hmcai@scut.edu.cn). ? Wei Liu is with the Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA. (e-mail: liu.wei@mayo.edu) ? Sheng Li is with the School of Data Science, University of Virginia, Charlottesville, VA, USA. (email: shengli@virginia.edu) ? Dajiang Zhu is with the Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA. (e-mail: dajiang.zhu@uta.edu)</figDesc><table /><note><p>? * Co-first authors. ? ? Quanzheng Li and Xiang Li are with the Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, Massachusetts, USA. (e-mail: li.quanzheng@mgh.harvard.edu, xiangli.shaun@gmail.com) ? D. Shen is with School of Biomedical Engineering, ShanghaiTech University, Shanghai 201210, China. He is also with Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200230, China, and Shanghai Clinical Research and Trial Center, Shanghai, 201210, China. (e-mail: Dinggang.Shen@gmail.com)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 Generated</head><label>1</label><figDesc>Samples From ChatGPT on PubMed20K</figDesc><table><row><cell>Original Text</cell><cell>Altered Text</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2. The study was officially registered with IRCT.ir, with the registration number IRCT2012071010230N1. 3. The trial was documented with IRCT.ir with the identification code IRCT2012071010230N1. 4. IRCT.ir served as the official registrar for the trial, with the registration number IRCT2012071010230N1. 5. The study was recorded with IRCT under the registration number IRCT2012071010230N1. 6. IRCT.ir recorded the trial with the identifier IRCT2012071010230N1.</figDesc><table><row><cell>While behavioral and technological</cell></row><row><cell>interventions can lead to some modest</cell></row><row><cell>improvements in glycemic control,</cell></row><row><cell>these interventions have not performed</cell></row><row><cell>much better than conventional prevention</cell></row><row><cell>in achieving glycemic control.</cell></row></table><note><p>1. The study found that although behavioral and technological interventions led to some slight improvements in glycemic control, they were not significantly more effective than typical care. 2. Despite the modest improvement in glycemic control through behavioral and technological interventions, they did not perform better than the standard care. 3. The results showed that while behavioral and technological interventions resulted in some minimal gains in glycemic control, they did not surpass the usual care in achieving glycemic control. 4. Although behavioral and technological interventions showed some improvement in glycemic control, they were not found to be significantly superior to the usual care. 5. The study showed that the usual care was not outperformed by behavioral and technological interventions in terms of achieving glycemic control, despite some small improvements.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 Data</head><label>2</label><figDesc>Augmentation Ablation Study on Symptoms</figDesc><table><row><cell>Data Augmentation</cell><cell cols="2">BERT BERT+Constractive</cell></row><row><cell>Raw</cell><cell>0.636</cell><cell>0.606</cell></row><row><cell>BackTranslationAug</cell><cell>0.778</cell><cell>0.747</cell></row><row><cell>ContextualWordAugUsingBert(Insert)</cell><cell>0.697</cell><cell>0.677</cell></row><row><cell>ContextualWordAugUsingBert(Substitute)</cell><cell>0.626</cell><cell>0.667</cell></row><row><cell>ContextualWordAugUsingDistilBERT(Insert)</cell><cell>0.707</cell><cell>0.747</cell></row><row><cell cols="2">ContextualWordAugUsingDistilBERT(Substitute) 0.667</cell><cell>0.646</cell></row><row><cell>ContextualWordAugUsingRoBERTA(Insert)</cell><cell>0.758</cell><cell>0.707</cell></row><row><cell>ContextualWordAugUsingRoBERTA(Substitute)</cell><cell>0.727</cell><cell>0.667</cell></row><row><cell>CounterFittedEmbeddingAug</cell><cell>0.667</cell><cell>0.626</cell></row><row><cell>InsertCharAugmentation</cell><cell>0.404</cell><cell>0.475</cell></row><row><cell>InsertWordByGoogleNewsEmbeddings</cell><cell>0.636</cell><cell>0.677</cell></row><row><cell>KeyboardAugmentation</cell><cell>0.545</cell><cell>0.505</cell></row><row><cell>OCRAugmentation</cell><cell>0.768</cell><cell>0.778</cell></row><row><cell>PPDBSynonymAug</cell><cell>0.697</cell><cell>0.758</cell></row><row><cell>SpellingAugmentation</cell><cell>0.697</cell><cell>0.707</cell></row><row><cell>SubstituteCharAugmentation</cell><cell>0.535</cell><cell>0.586</cell></row><row><cell>SubstituteWordByGoogleNewsEmbeddings</cell><cell>0.727</cell><cell>0.727</cell></row><row><cell>SwapCharAugmentation</cell><cell>0.475</cell><cell>0.485</cell></row><row><cell>SwapWordAug</cell><cell>0.687</cell><cell>0.727</cell></row><row><cell>WordNetSynonymAug</cell><cell>0.616</cell><cell>0.758</cell></row><row><cell>ChatAug</cell><cell>0.889</cell><cell>0.899</cell></row><row><cell>TABLE 3</cell><cell></cell><cell></cell></row><row><cell cols="3">Data Augmentation Ablation Study on PubMed20K</cell></row><row><cell>Data Augmentation</cell><cell cols="2">BERT BERT+Constractive</cell></row><row><cell>Raw</cell><cell>0.792</cell><cell>0.798</cell></row><row><cell>BackTranslationAug</cell><cell>0.812</cell><cell>0.830</cell></row><row><cell>ContextualWordAugUsingBert(Insert)</cell><cell>0.802</cell><cell>0.811</cell></row><row><cell>ContextualWordAugUsingBert(Substitute)</cell><cell>0.815</cell><cell>0.830</cell></row><row><cell>ContextualWordAugUsingDistilBERT(Insert)</cell><cell>0.796</cell><cell>0.796</cell></row><row><cell cols="2">ContextualWordAugUsingDistilBERT(Substitute) 0.797</cell><cell>0.800</cell></row><row><cell>ContextualWordAugUsingRoBERTA(Insert)</cell><cell>0.815</cell><cell>0.814</cell></row><row><cell>ContextualWordAugUsingRoBERTA(Substitute)</cell><cell>0.782</cell><cell>0.782</cell></row><row><cell>CounterFittedEmbeddingAug</cell><cell>0.805</cell><cell>0.805</cell></row><row><cell>InsertCharAugmentation</cell><cell>0.826</cell><cell>0.831</cell></row><row><cell>InsertWordByGoogleNewsEmbeddings</cell><cell>0.786</cell><cell>0.784</cell></row><row><cell>KeyboardAugmentation</cell><cell>0.809</cell><cell>0.815</cell></row><row><cell>OCRAugmentation</cell><cell>0.789</cell><cell>0.789</cell></row><row><cell>PPDBSynonymAug</cell><cell>0.795</cell><cell>0.829</cell></row><row><cell>SpellingAugmentation</cell><cell>0.808</cell><cell>0.811</cell></row><row><cell>SubstituteCharAugmentation</cell><cell>0.816</cell><cell>0.821</cell></row><row><cell>SubstituteWordByGoogleNewsEmbeddings</cell><cell>0.807</cell><cell>0.822</cell></row><row><cell>SwapCharAugmentation</cell><cell>0.797</cell><cell>0.801</cell></row><row><cell>SwapWordAug</cell><cell>0.798</cell><cell>0.794</cell></row><row><cell>WordNetSynonymAug</cell><cell>0.761</cell><cell>0.757</cell></row><row><cell>ChatAug</cell><cell>0.835</cell><cell>0.835</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (csur)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on few-shot learning in natural language processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="294" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical attention prototypical networks for few-shot text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing</title>
		<meeting>the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="476" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Meta-learning for few-shot natural language processing: A survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09604</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transprompt: Towards an automatic transferable prompting framework for fewshot text classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2792" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Meta learning for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01500</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ptr: Prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="182" to="192" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards unified prompt tuning for few-shot text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05313</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Eda: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11196</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closer look at feature space data augmentation for few-shot intent classification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glaude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Lichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</meeting>
		<imprint>
			<date type="published" when="2019">DeepLo 2019. 2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A survey of data augmentation approaches for nlp</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03075</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Augmenting nlp models using latent feature interpolations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Didolkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6931" to="6936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on data augmentation for text classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Kaufhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reuter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recent advances in natural language processing via large pre-trained language models: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P B</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01243</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Survey on natural language processing in medical image analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yi xue ban= Journal of Central South University. Medical Sciences</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="981" to="993" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Zhong nan da xue xue bao</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02173</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Coulombe</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1812.04718" />
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D19-1670" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">Nov. 2019</date>
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Belgium</forename><surname>Brussels</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/K18-1047" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">That&apos;s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2557" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Counter-fitting Word Vectors to Linguistic Constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mrk?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ga?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N16-1018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="142" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating Natural Language Adversarial Examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D18-1316" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2890" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N18-2072" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Data Augmentation Using Pre-trained Transformer Models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02245</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/1910.01108</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P16-1009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">Aug. 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nareor: The narrative reordering problem</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alikhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">653</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fewshot learning for medical text: A systematic review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Garadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14081</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Few-shot text classification with triplet networks, data augmentation, and curriculum learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5493" to="5500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How to train your maml</title>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09502</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modelagnostic multi-stage loss optimization meta learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2349" to="2363" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bloom: A 176b-parameter open-access multilingual language model</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ili?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Castagn?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gall?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13688</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Clinicalradiobert: Knowledge-infused few shot learning for clinical notes named entity recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rezayi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hebbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI 2022</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-18">September 18, 2022. 2022</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Is chatgpt a general-purpose natural language processing task solver?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06476</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Is chatgpt a good translator? a preliminary study</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08745</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lovenia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04023</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Chatgpt and other large language models are doubleedged swords</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Hentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">230163</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evaluating the performance of chatgpt in ophthalmology: An analysis of its successes and shortcomings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Antaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Touma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Milad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>El-Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2024" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheatham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Medenilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elepa ?o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Madriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aggabao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diaz-Candido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maningo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Digital Health</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e0000198</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Collaborating with chatgpt: Considering the implications of generative artificial intelligence for journalism and media education</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Pavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism &amp; Mass Communication Educator</title>
		<imprint>
			<biblScope unit="page">10776958221149577</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Education in the era of generative artificial intelligence (ai): Understanding the potential benefits of chatgpt in promoting teaching and learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baidoo-Anu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Owusu</forename><surname>Ansah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available at SSRN</title>
		<imprint>
			<biblScope unit="volume">4337484</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Mathematical capabilities of chatgpt</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-R</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.13867</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Chatgpt for (finance) research: The bananarama conjecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Finance Research Letters</title>
		<imprint>
			<biblScope unit="page">103662</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Chatgpt: five priorities for research</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Van Dis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bockting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">614</biblScope>
			<biblScope unit="issue">7947</biblScope>
			<biblScope unit="page" from="224" to="226" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Is chat gpt biased against conservatives? an empirical study</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Mcgee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-02-15">February 15, 2023</date>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
	<note>An Empirical Study</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Breaking chatgpt with dangerous questions understanding how chatgpt prioritizes safety, context, and obedience</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Co-authoring with an ai? ethical dilemmas and artificial intelligence</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Jabotinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ethical Dilemmas and Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2022-12-15">December 15, 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Chatgpt: The end of online exam integrity?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Susnjak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09292</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Will chatgpt get you caught? rethinking of plagiarism detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Er</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04335</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Are chatgpt and alphacode going to replace programmers?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Castelvecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Economics of chatgpt: A labor market view on the occupational impact of artificial intelligence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zarifhonarvar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">4350925</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="308" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Bert: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Mask-guided bert for few shot text classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10447</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Agribert: Knowledge-infused agricultural language models for matching food and nutrition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rezayi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dhakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">July 23-29, 2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Nlp augmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://github.com/makcedward/nlpaug" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lifland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grigsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Facebook fair&apos;s wmt19 news translation task submission</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Frustratingly easy transferability estimation</title>
		<author>
			<persName><forename type="first">L.-K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9201" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Measurement of text similarity: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">421</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Chestxraybert: A pretrained language model for chest radiology report summarization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Covidsum: A linguistically enriched scibert-based summarization model for covid-19 scientific papers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1532046422000156" />
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">103999</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">695</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.12242</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Theory of mind may have spontaneously emerged in large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.02083</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Coupling artificial neurons in bert and biological neurons in the human brain</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI</title>
		<meeting>the 37th AAAI Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
