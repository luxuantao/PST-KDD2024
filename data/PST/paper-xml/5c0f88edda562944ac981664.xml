<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Convolutional Neural Networks for Segmentation of Breast Tumors in MRI with Application to Radiogenomics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<email>xdzhangjun@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashirbani</forename><surname>Saha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Zhu</surname></persName>
							<email>zhe.zhu@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
							<email>maciej.mazurowski@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Radi-ology</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Convolutional Neural Networks for Segmentation of Breast Tumors in MRI with Application to Radiogenomics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F7FD5845603913056077C35C1891ADBD</idno>
					<idno type="DOI">10.1109/TMI.2018.2865671</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2018.2865671, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dynamic contrast-enhanced magnetic resonance imaging</term>
					<term>breast tumor</term>
					<term>segmentation</term>
					<term>molecular subtype classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Breast tumor segmentation based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a challenging problem and an active area of research. Particular challenges, similarly as in other segmentation problems, include the class-imbalance problem as well as confounding background in DCE-MR images. To address these issues, we propose a mask-guided hierarchical learning (MHL) framework for breast tumor segmentation via fully convolutional networks (FCN). Specifically, we first develop an FCN model to generate a 3D breast mask as the region of interest (ROI) for each image, to remove confounding information from input DCE-MR images. We then design a two-stage FCN model to perform coarse-to-fine segmentation for breast tumors. Particularly, we propose a Dice-Sensitivity-like loss function and a reinforcement sampling strategy to handle the class-imbalance problem. To precisely identify locations of tumors that underwent a biopsy, we further propose an FCN model to detect two landmarks located at two nipples. We finally select the biopsied tumor based on both identified landmarks and segmentations. We validate our MHL method on 272 patients, achieving a mean Dice similarity coefficient (DSC) of 0.72 which is comparable to mutual DSC between expert radiologists. Using the segmented biopsied tumors, we also demonstrate that the automatically generated masks can be applied to radiogenomics and can identify luminal A subtype from other molecular subtypes with the similar accuracy with the analysis based on semi-manual tumor segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Cancer is the second-leading cause of death in the United States and a major public health problem worldwide <ref type="bibr" target="#b0">[1]</ref>. Excluding skin cancers, breast cancer is the most common cancer diagnosed among US women, accounting for nearly one in three cancers <ref type="bibr" target="#b1">[2]</ref>. Recent research and clinical studies have shown that dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is an effective tool for breast cancer diagnosis because of its capability to visualize both physiological tissue characteristics and anatomical structures <ref type="bibr" target="#b2">[3]</ref>. Very recent studies have also shown that algorithmically-extracted features can be used to identify genomic composition of breast cancer tumors <ref type="bibr" target="#b3">[4]</ref> and to predict patient outcomes <ref type="bibr" target="#b4">[5]</ref>. Several features have been proposed to characterize DCE-MR images in this context, such as tumor morphology, texture, and enhancement kinetics <ref type="bibr" target="#b5">[6]</ref>. These features highly rely on an accurate tumor segmentation. Hence, the accurate segmentation of breast tumors in DCE-MR images is a critically significant task for automated breast cancer analysis.</p><p>Although many methods have been developed for general tumor segmentation, only a few are designed for breast tumor segmentation in DCE-MRI. There are four types of commonlyused approaches for tumor segmentation: 1) manual annotation, 2) atlas-based approaches, 3) semi-automatic methods, and 4) learning-based approaches. The most straightforward way is the manual annotation for tumor regions by radiologists <ref type="bibr" target="#b6">[7]</ref>, which is not only time-consuming but also error-prone. Although various atlas-based methods (via image registration) have achieved promising results <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, such methods cannot accurately identify breast tumors, since breast tumors generally do not have fixed positions and regular morphological shapes. Several semi-automatic methods have been developed for breast tumor segmentation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, these methods require pre-defined regions (e.g., bounding boxes) for tumors drawn by radiologists, due to the challenge in identifying breast tumors from confounding organ or vessels directly.</p><p>As an alternative, learning-based methods that treat tissue segmentation as a supervised classification problem have achieved remarkable performance in many medical applications, such as vessel segmentation <ref type="bibr" target="#b12">[13]</ref>, organ segmentation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, tumor segmentation <ref type="bibr" target="#b15">[16]</ref>, and computer-aided disease analysis <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. More recently, deep-learning-based methods, e.g., convolutional neural network (CNN) and fully convolutional networks (FCN), have achieved state-of-the-art performance in medical imaging analysis <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. In the literature, a common challenge in tumor segmentation via FCN is that the number of voxels in the tumor region (positive class) is much smaller than that in the background (negative class), leading to a serious classimbalance problem <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. However, such problem is seldom considered in previous deep-learning methods for breast tumor segmentation. Also, as illustrated in Fig. <ref type="figure">1</ref>, the confounding background (e.g., vessel structures and organs) in DCE-MRI making the task of breast tumor segmentation more challenging.</p><p>To this end, we propose a Mask-guided Hierarchical Learning (MHL) framework for breast tumor segmentation via FCN. Our strategy is first decomposing the original difficult segmen-Fig. <ref type="figure">1</ref>. Illustration of DCE-MRI for 3 subjects, with each row corresponding to a particular subject. The left column denotes pre-contrast images scanned before contrast-enhancement, the second column shows post-contrast images scanned after contrast-enhancement, and the last column denotes subtraction images between post-contrast and pre-contrast images.</p><p>tation problem into several relatively easier sub-problems and then hierarchically solving those sub-problems. Specifically, in the first step, we develop an FCN model (denoted as FCN-1) to generate a breast mask for each input image. In the second step, we propose another FCN (denoted as FCN-2) to generate over-segmented tumor-like regions, using the context information (i.e., breast masks) generated via FCN-1. In FCN-2, we also develop a Dice-Sensitivity-like loss function for dealing with the class-imbalance problem and detecting all possible tumor-like regions. In the third step, we refine the segmentation results via an additional FCN (i.e., FCN-3), using a Dice-like loss function and a reinforcement sampling strategy. Furthermore, we propose a landmark detection based method to distinguish biopsied tumors from all detected tumors, with landmarks defined at locations of two nipples.</p><p>The overall contribution of this paper is the development of a multi-stage system for segmentation of breast tumors that incorporates a variety of algorithms, aiming at addressing the common issues in breast DCE-MR images, including classimbalance problem and confounding background. The more specific contributions and novelties are the following. First, we develop mask-guided hierarchical deep learning framework to perform coarse-to-fine segmentation for breast tumors. Specifically, the first-stage FCN employs a Dice-Sensitivity-like loss function to include important tumor pixels as many as possible. And the second-stage FNC uses a reinforcement sampling strategy to focus on the tumor-like regions. Second, we develop a landmark based biopsied tumor localization method, with two landmarks located at two nipples. Third, we have extensively evaluated the proposed method on DCE-MRIs for 272 patients with breast tumors in three tasks, including 1) multitumor segmentation, 2) biopsied tumor segmentation, and 3) molecular subtype classification. Of particular novelty in our evaluation is its radiogenomic aspect in which the imaging phenotypes are associated with biological characteristics of the tumor. Results in the first two tasks demonstrate that our method can accurately identify multiple tumors from MR images as well as tumors that underwent biopsy. In the third task, the results show that our method can distinguish luminal A subtype from other tumor subtypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Semi-automatic Method</head><p>There are only a few studies focusing on breast tumor segmentation in DCE-MRI, and most of them are semi-automatic methods. As a typical clustering-based method, e.g., fuzzy cmeans (FCM), is often used for tumor segmentation <ref type="bibr" target="#b26">[27]</ref>. This method usually requires a rough localization (e.g., bounding boxes) for tumors defined by radiologists, since it is challenging to distinguish breast tumors from confounding tissues, vessels, and other organs. Another semi-automatic method that has been applied to breast tumor segmentation is the graphcut based method <ref type="bibr" target="#b9">[10]</ref>, where tumors are first specified and roughly segmented by manual annotation. Ashraf et al. <ref type="bibr" target="#b10">[11]</ref> proposed a multichannel Markov random field framework for breast tumor segmentation, where 2D slices containing tumor regions selected from 3D MRI are used in their experiments. Vignati et al. <ref type="bibr" target="#b11">[12]</ref> developed a lesion detection system for breast DCE-MRI, consisting of four major components, i.e., 1) breast segmentation to identify breast size and location, 2) registration to correct for patient movements, 3) lesion detection to extract contrast-enhanced regions, and 4) false positive reduction to exclude contrast-enhanced regions other than lesions. Renz et al. <ref type="bibr" target="#b27">[28]</ref> proposed a computer-assisted diagnosis method, including registration of DCE-MR images, detection of lesions by segmentation, extraction of dynamic and morphologic features, and classification of lesions. Sun et al. <ref type="bibr" target="#b28">[29]</ref> classify malignant masses from benign masses using a semi-supervised deep convolutional neural network, using preselected ROIs from mammogram images (with each ROI containing a mass extracted based on the radiologists' notations). Rasti et al. <ref type="bibr" target="#b29">[30]</ref> employed pre-selected slices of DCE-MRI to perform an automatic MR tumor segmentation and ROI generation. Specifically, a sequence of image processing techniques and active contour segmentation were first performed, followed by a deep neural network for breast cancer diagnosis based on the pre-selected ROI. In summary, previous semiautomatic methods highly rely on pre-defined slices or regions (e.g., bounding boxes) for tumors, and such regions require labor-intensive annotation by radiologists <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning-based Method</head><p>Different from semi-automatic methods, learning-based methods can perform automatic tumor segmentation via supervised learning algorithms <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Learning-based methods aim to train voxel-wise classification models, where voxels in tumor regions are regarded as positive samples and the rest are used as negative ones. Specifically, these methods usually first extract a cubic patch centered at a particular voxel, and then learn a patch-wise binary classifier to distinguish voxels in tumor regions from the remaining ones <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. For instance, Dinsha et al. <ref type="bibr" target="#b33">[34]</ref> used the support vector machine (SVM) classifier for breast tumor segmentation in thermogram images. Jiang et al. <ref type="bibr" target="#b32">[33]</ref> used the AdaBoost classifier and Haar-like features to detect rough tumor regions, followed by an SVM to refine the segmentation results. Bria et al. <ref type="bibr" target="#b25">[26]</ref> proposed an approach for computer-aided detection which faces the class imbalance with a cascade of boosting classifiers where each node is trained by a learning algorithm based on ranking instead of classification error. The novelty of this method is in handling the class imbalance in each node through a boosting algorithm designed to maximize the Area under the ROC curve (AUC). Gubern-Mérida et al. <ref type="bibr" target="#b31">[32]</ref> proposed an automated localization method for breast cancers in DCE-MR images, by first extracting blob and relative enhancement voxel features for locating lesion candidates and then computing a malignancy score for each lesion candidate using region-based morphological and kinetic features computed on segmented lesion candidates. McClymont et al. <ref type="bibr" target="#b34">[35]</ref> developed an automatic breast lesion segmentation method based on mean-shift clustering and graph-cuts, using multimodal (T1, T2, DCE-MRI) breast MRI data. Maicas et al. <ref type="bibr" target="#b35">[36]</ref> proposed to generate a shape prior based on the semantic segmentation computed from a convolutional neural network, and then concatenated the outputs of last layers together to form the input to a random forest classifier to produce the final detection results for breast mass. The traditional learning-based methods often treat feature extraction and model training as two stand-alone tasks, ignoring the possible heterogeneous property between human-engineered features and subsequent learning models.</p><p>Different from conventional learning-based methods, deep neural networks, such as fully convolutional networks (FCN), can perform segmentation tasks in an end-to-end manner, where feature extraction and model training are incorporated into a unified learning framework <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In particular, without using any fully connected layers, FCN can perform a voxel-wise (or pixel-wise) segmentation for a whole image with an arbitrary size. For instance, Long et al. <ref type="bibr" target="#b36">[37]</ref> adopted FCN for semantic segmentation. Ronneberger et al. developed a coarse-fine-connected shortcut architecture of FCN (i.e., U-Net) for biomedical image segmentation <ref type="bibr" target="#b39">[40]</ref>. Similar to U-Net, Brosch et al. <ref type="bibr" target="#b40">[41]</ref> proposed an FCN architecture with shortcuts for multiscale feature integration for the segmentation of multiple sclerosis lesions. Chen et al. <ref type="bibr" target="#b41">[42]</ref> developed a cascaded deep learning method for 3D image segmentation, based on a combination of a fully convolutional network (including a sequence of U-Nets denoted as kU-Net) and a recurrent neural network (RNN) for exploiting the intra-slice and inter-slice contexts, respectively. Christ et al. <ref type="bibr" target="#b24">[25]</ref> developed a two cascaded FCNs for the combined segmentation of the liver and its lesions. Specifically, they first trained an FCN to segment the liver as an ROI input to a second FCN, and then performed lesion segmentation within the predicted liver ROIs using the second FCN. Fakhry et al. <ref type="bibr" target="#b42">[43]</ref> proposed a residual deconvolutional network for electron microscopy image segmentation, consisting of two information pathways that capture full-resolution features and contextual information. Milletari et al. <ref type="bibr" target="#b43">[44]</ref> developed a fully convolutional network (called V-Net) for end-to-end 3D image segmentation, using a Dice coefficient based objective function. Dalmıs ¸et al. <ref type="bibr" target="#b44">[45]</ref> proposed a two consecutive U-Nets for breast and fibroglandular tissue segmentation, while only 2D axial slices of MR images were used as samples for network training. However, most of the existing deep learning based methods seldom consider both the effects of confounding regions and the class-imbalance problem that are prevalent in breast tumor segmentation from DCE-MRI.</p><p>Besides, several deep neural networks have been recently proposed for breast image analysis, based on mammography, MRI, ultrasound images, and whole-slide histology images. Using mammography, Kooi et al. <ref type="bibr" target="#b45">[46]</ref> trained CNN on a large dataset to detect mammographic lesions, with results showing that CNN outperforms traditional methods and the performance can be further improved by adding location information and context from manually designed features to the CNN model. With the same image modality, the U-Net was adopted to detect and segment soft tissue lesion in digital mammography <ref type="bibr" target="#b46">[47]</ref>, while Kallenberg et al. used a sparse autoencoder for breast density segmentation and mammographic risk scoring <ref type="bibr" target="#b47">[48]</ref>. Besides, Becker et al. <ref type="bibr" target="#b48">[49]</ref> developed a software based on neural networks for breast cancer detection in mammography dataset. Using MRI, deep reinforcement learning was employed for detecting active breast lesion in DCE-MRI <ref type="bibr" target="#b49">[50]</ref>, while a hybrid mass-detection algorithm was developed to combine unsupervised candidate detection with deep-learning-based classification <ref type="bibr" target="#b50">[51]</ref>. Using ultrasound images, Yap et al. <ref type="bibr" target="#b51">[52]</ref> employed three deep learning methods for breast lesion detection in ultrasound images, and obtained superior performance compared with four existing state-of-the-art lesion detection algorithms. Using whole-slide histology images the breast, Albarqouni et al. <ref type="bibr" target="#b52">[53]</ref> proposed an additional crowdsourcing layer (AggNet) to handle the data aggregation issue for mitosis detection in breast cancer histology images. With the similar image modality, Saha et al. <ref type="bibr" target="#b53">[54]</ref> proposed a Her2Net method for identifying, segmenting, and classifying cell membranes and nuclei from human epidermal growth factor receptor-2 (HER2) stained breast cancer images. More recently, Hamidinekoo et al. <ref type="bibr" target="#b54">[55]</ref> have reviewed various approaches of deep learning in mammography and breast histology and made an overview of future trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Radiogenomics</head><p>Recently, studies using a variety of imaging techniques have improved a characterization of cancers <ref type="bibr" target="#b55">[56]</ref>. Additionally, a new discipline has emerged that aims at correlating tumor genomic, molecular, and related characteristics with their imaging phenotypes. For example, Yamamoto et al. performed a preliminary study to define the radiogenomic landscape, suggesting radiogenomic analysis with MRI is a novel approach to understanding the underlying molecular biology of breast cancers <ref type="bibr" target="#b56">[57]</ref>. Then, Yamamoto et al. also extracted 47 computational quantitative features from DCE-MRI, and found that the enhancing rim fraction score (a quantitative radiogenomic </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MATERIALS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Image Annotation</head><p>Data used in this study were scanned during September 2007 and June 2009. Consecutive preoperative DCE-MRIs of 400 patients were collected with the Institutional Review Board approval including a waiver of informed consent. As indicated in <ref type="bibr" target="#b30">[31]</ref>, by excluding the patients with reduction mammoplasty, implants, or missing sequences, there were 285 patients left. Six radiologists further manually checked all data and excluded 13 patients that have implants or no valid tumor. We have 272 cases that were included in this study.</p><p>All MRIs were acquired in axial plane by using a 1.5 T or 3.0 T scanner (GE Healthcare and Siemens) in prone position with TR in the range of <ref type="bibr">[3.5, 7</ref>.0] ms and TE in the range of [1.0, 3.0] ms. All included studies contained a fat-saturated gradient echo T1-weighted pre-contrast sequence and typically three post-contrast T1-weighted sequences acquired after the IV administration of contrast agent [gadopentetate dimeglumine (Magnevist, BayerHealth Care, Berlin, Germany)] or [gadobenate dimeglumine (MultiHance, Bracco, Milan, Italy)] using a weight based protocol (0.2ml/kg). The image size ranged from 320 × 320 × 144 to 512 × 512 × 200 with the resolution from</p><formula xml:id="formula_0">1.1 × 1.1 × 1.2 mm 3 to 0.6 × 0.6 × 1.0 mm 3 .</formula><p>For each image, we performed two types of image normalization, including intensity normalization and spatial normalization. In the intensity normalization stage, we removed the noise of the image by selecting top 0.1% maximum values as maximum value and bottom minimum 0.1% values as the minimum value. Then, we performed the z-score normalization using mean µ = 0 and standard deviation σ = 1. Note that, for the normalization of subtraction image, we first obtained the subtraction and then performed the intensity normalization. In the spatial normalization stage, we resampled all images with the resolution of 0.7 × 0.7 × 0.7 mm 3 (using spline interpolation), since most of the images have the resolution from 0.6 × 0.6 × 1.0 mm 3 to 1.1 × 1.1 × 1.2 mm 3 . Since the general structure of the breast tissue is not rotation invariant we did not perform the image rotation for data augmentation. Our data have been re-oriented to the same direction according to the DICOM header information. We slightly resized the images with the ratio from 0.8 to 1.2 randomly to augment the training data. Since we trained the model with subimages cropped from the original images, translation can be achieved by sampling. The whole dataset is split into two subsets:</p><p>1) A training set consisting of 224 cases (denoted as D 1 ): Six radiologists with 6 months-20 years of experience annotated each case respectively, and completed the annotation together for the whole 224 cases. Three sequences were displayed to the reader: the pre-contrast and the first post-contrast sequence, as well as the subtracted sequences (obtained by subtracting the post-contrast from the pre-contrast sequences). The radiologist manually annotated all tumors with the smallest cuboid bounding box covering each tumor region. A clustering process via fuzzy c-means (FCM) was then performed to generate the gold standard. For occasional cases where the generation of gold standard appeared flawed due to imperfections of the FCM algorithms, manual corrections were made by a researcher.</p><p>2) A test set consisting of 48 subjects (denoted as D 2 ). For the testing set, we have two sets of different annotations. The first set of annotation contains the multiple tumor masks,  Similarly, final segmentation ground truth for biopsied tumors was obtained via FCM based on the bounding boxes for tumors defined by the radiologists. In this way, each case was annotated four times by four radiologists, and therefore there were four tumor masks for each of the cases. This repeated annotation will allow us for evaluating the algorithm vs. radiologist error in the context of radiologist vs. radiologist differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ground Truth Generation for Breast Masks and Landmarks</head><p>For generating ground-truth breast masks, we employ a curve fitting and active contour based method <ref type="bibr" target="#b60">[61]</ref> to obtain breast masks for training the proposed breast segmentation model. The breast masks are not required to be very accurate, since such masks are only used as the context information to remove confounding vessels and organs located outside of the breast. To accurately localize tumors, we further defined two landmarks for the left and right breasts. Specifically, we manually annotated the right and left nipples as two landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head><p>We propose an automatic method for breast tumor segmentation via a mask-guided hierarchical learning (MHL) framework (see Fig. <ref type="figure" target="#fig_0">2</ref>). We first train an FCN model (i.e., FCN-1) for estimating the region of interest (i.e., breast mask). With the breast mask as guidance, we train additional two FCN models (FCN-2 and FCN-3) to estimate rough segmentation results and to refine the initial results, respectively. In this way, we can detect all tumor regions in the input image. To identify biopsied tumors from all detected tumors, we further develop a landmark detection model (i.e., FCN-4) to detected two landmarks for biopsied tumor selection and following radiogenomics. In the following sections, we present these three major components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Estimation of Breast Masks</head><p>Since breast tumors appear only in breast regions, we first generate a region of interest (ROI) that includes breast only. The most straightforward way is to use a breast mask as an ROI, which can remove most of the confounding organs. Inspired by previous studies <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, we develop a fully convolutional network (i.e., FCN-1) with a 3D U-Net architecture <ref type="bibr" target="#b39">[40]</ref> to learn a breast mask for each input pre-contrast image. To speed up both the training and testing processes, we adopt down-sampled pre-contrast images (normalized into the resolution of 1.5 × 1.5 × 1.5mm 3 ) to train FCN-1. The architecture of FCN-1 is shown in Fig. <ref type="figure">S1</ref> in the Supplementary Materials. A binary cross-entropy loss function is used in FCN-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hierarchical Learning for Tumor Segmentation</head><p>1) Rough Segmentation: Although we can roughly exclude background organs via breast masks learned in FCN-1, there may still exist a lot of confounding information (e.g., vessels) in breast regions. It is easy to remove some tumor regions, if we would like to remove these confounding information thoroughly. Furthermore, the number of voxels in positive regions (i.e., tumor regions) is generally much smaller than that of negative regions (i.e., background regions), leading to a serious class-imbalance problem. These are common problems in medical image segmentation. To this end, we develop an FCN model (i.e., FCN-2, see Fig. <ref type="figure" target="#fig_1">3</ref>) to obtain rough segmentation results for breast tumors with high sensitivity, by including as many as possible tumor-like voxels.</p><p>The FCN-2 utilizes the breast mask as the guidance for the segmentation task. A straightforward way to make use of breast masks would be to treat the breast mask as an ROI to mask the original input images. In doing so, most of the enhanced organs can be simply removed. Another way is to use the breast mask as an ROI in the post-processing stage, where segmentation results are further masked by the breast mask. However, given a possibility that a mask is inaccurate, these two hard-masking strategies could lead to unrecoverable errors, especially for tumors that appear in the boundary of breast regions. Note that the breast masks used in this study are generated by our proposed FCN-1 model (training with inaccurate breast masks) rather than by manual annotation, and thus these masks may not be fully accurate. To this end, as opposed to the two hard-masking strategies mentioned above, we use a soft-masking strategy in this work. That is, the breast mask is used as the input of FCN-2 to guide the process of tumor segmentation, to avoid un-recoverable errors introduced by inaccurate masks. As shown in the top part of Fig. <ref type="figure" target="#fig_1">3</ref>, the input of FCN-2 contains three channels that are corresponding to the breast mask generated by FCN-1, the post-contrast image, and the subtraction image, respectively. The output of FCN-2 is a probability map for tumor segmentation. That is, FCN-2 is used to learn a non-linear mapping from input images to probability maps for tumors, where the breast mask generated by FCN-1 is used as context information to guide the tumor segmentation.</p><p>Similar to FCN-1, FCN-2 adopts a U-Net architecture <ref type="bibr" target="#b39">[40]</ref> to capture both global and local structural information of input images. Specifically, there exists a contracting path and an expanding path in FCN-2. Every step in the contracting path consists of a 3 × 3 × 3 convolution, followed by a batch normalization, a rectified linear unit (ReLU), and a 2 × 2 × 2 max pooling operation with a stride 2 for down-sampling. Each step in the expanding path consists of a 3 × 3 × 3 upconvolution, followed by a concatenation process with the corresponding feature map from the contracting path, and one 3 × 3 × 3 convolution (followed by a ReLU function). Note that the activation function of the last layer in FCN-2 is a sigmoid function and therefore the output is normalized into [0, 1]. That is, the output of FCN-2 is the probability that each voxel belongs to a tumor region or background. Due to both contracting and expanding paths, such network can grasp a large image area using small kernel sizes while still keeping high localization accuracy <ref type="bibr" target="#b39">[40]</ref>.</p><formula xml:id="formula_1">Let X n,v represent the v-th (v = 1, • • • , V ) voxel of the n- th input data X n .</formula><p>Given three channels, each voxel contains 3 elements, with each element corresponding to a particular channel. We denote the ground-truth segmentation probability map for X n as S n and its v-th element as S n,v . It is worth noting that voxels in tumor regions (i.e., the positive class) occupies only a very small portion of the scan (compared with voxels in the background belonging to the negative class), which is a typical class-imbalance problem. This often causes the prediction of learned networks to be biased towards the background regions, and the tumor regions are only partially detected or missing. To alleviate the class-imbalance problem by establishing the right balance between foreground and background voxels <ref type="bibr" target="#b43">[44]</ref>, as well as obtain a high sensitivity of tumor regions, we propose the following Dice-Sensitivity-like loss function. Both components of the Dice-Sensitivity-like loss function are contributing to addressing the class imbalance problem. Dice coefficient, unlike the measurements such as the traditional overall accuracy, mean squared error, or crossentropy, highly focuses on the tumor class and penalizes the missed voxels as well as false positives. Sensitivity adds an additional bias toward detection of tumor voxels and therefore addresses the issue of imbalance by shifting the focus toward the minority class (i.e., tumor voxels). The formal definition of the Dice-Sensitivity-like loss function is</p><formula xml:id="formula_2">Ω1(W2) =2 - 1 N N n=1 2 V v=1 Sn,vf (Xn,v, W2) V v=1 S 2 n,v + V v=1 f (Xn,v, W2) 2 DSC - 1 N N n=1 V v=1 Sn,vf (Xn,v, W2) V v=1 S 2 n,v SEN ,<label>(1)</label></formula><p>where f (X n,v , W 2 ) is the estimated probability for X n,v by using the network coefficients W 2 of FCN-2, and N is the number of training images in a batch. In Eq. ( <ref type="formula" target="#formula_2">1</ref>), the second term is the Dice coefficient, and the last one is the sensitivity term. That is, the loss function defined in Eq. ( <ref type="formula" target="#formula_2">1</ref>) aims to segment tumor regions, as well as keep those tumorlike regions as much as possible. In this way, we can only generate a rough segmentation result via FCN-2, because many false positive voxels will be included via the proposed Dice-Sensitivity-like loss function. The training process is performed in a sliding window manner with a fixed window size of 96 × 96 × 96, because of the memory problem and different image sizes.</p><p>2) Segmentation Refinement: To refine segmentation results generated by FCN-2, we develop an additional FCN model (FCN-3) to accurately locate tumor regions. The architecture of FCN-3 is shown in the bottom part of Fig. <ref type="figure" target="#fig_1">3</ref>. Specifically, the input of FCN-3 contains three channels, i.e., probability map of rough segmentation generated via FCN-2, post-contrast image, and subtraction image. Since the focus of FCN-3 is to estimate tumor locations accurately, we adopt the following Dice-like loss function <ref type="bibr" target="#b43">[44]</ref> in FCN-3:</p><formula xml:id="formula_3">Ω2(W3) =1 - 1 N N n=1 2 V v=1 Sn,vg(X n,v , W3) V v=1 S 2 n,v + V v=1 g(X n,v , W3) 2 DSC ,<label>(2)</label></formula><p>where g(X n,v , W 3 ) is the estimated probability map by using the network coefficients W 3 of FCN-3, and X n,v is the v-th</p><formula xml:id="formula_4">(v = 1, • • • , V</formula><p>) voxel of the input data X n with 3 channels.</p><p>To further alleviate the class-imbalance problem, we propose a reinforcement sampling strategy to extract patches from both tumor regions and the background for training FCN-3. Specifically, we sample training patches according to the probability map generated via FCN-2. Mathematically, let P represent the probability map of rough segmentation from FCN-2 with the size of A × B × C. Given a patch size of × P × P , there are (A -P + 1) × (B -P + 1) × (C -P + 1) candidate patches that can be sampled. We can generate a salience map Q = P * 1, where * is the convolutional operation with valid strategy, and 1 is a kernel of all 1 with the size of P × P × P . Accordingly, the size of Q is (A -P + 1) × (B -P + 1) × (C -P + 1). Based on Q, we can calculate a sampling probability map V as follows: where ω is a coefficient used to directly adjust the sampling probability and also control the total number of sampled patches. Therefore, each patch centered at (x, y, z) can be sampled with the probability of Overall, in our proposed hierarchical learning strategy, we first use FCN-1 to generate breast masks to exclude those confounding organs that are not in breast regions. With breast masks as the context information, we then adopt FCN-2 with a Dice-Sensitivity-like loss function in Eq. ( <ref type="formula" target="#formula_2">1</ref>) to roughly segment tumor regions, while keeping as many as possible tumor-like regions. Finally, we adopt FCN-3 to refine the segmentation results, based on both reinforcement sampling strategy and Dice-like loss function defined in Eq. ( <ref type="formula" target="#formula_3">2</ref>). In this way, we decompose the original tumor segmentation problem into several easy sub-problems, and perform coarse-to-fine breast tumor segmentation in a hierarchical learning manner.</p><formula xml:id="formula_5">V = exp(- ω Q ),<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Landmark-based Biopsied Tumor Selection</head><p>An important application of breast tumor segmentation in MRI is to identify associations between imaging and genomics, by selecting biopsied tumors from all detected tumors. This is done with the help of the recorded rough position of the biopsied tumor in the clinical records. For example, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>, the tumor is marked as L10, which means the tumor is in the left breast (right part of the image) and at the 10 O'clock position in coronal view, where the clock center is the left nipple. Note that some other supervised learning based methods (e.g., <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>) and simple unsupervised methods can be potentially used for detecting nipple landmarks. However, the unsupervised methods may require context information, e.g., the position of the heart, to accurately determine the position (i.e., the left or right breast) of landmarks. To locate biopsied tumors, in this work, we used the tumor position information (e.g., L10) and landmarks to determine which out of all detected tumors is the one that underwent biopsy. Specifically, we develop a new method for identifying the biopsied tumor based on landmark detection, including the following three steps.</p><p>1) Tumor Labeling: We first label all possible tumors in the segmented image generated by FCN-3. Specifically, we perform image erosion to remove possible noise, followed by image dilation to connect possible non-mass tumor regions. Then, we compute connected components on the image after these morphological operations to obtain the mask for each tumor. Finally, we use the tumor mask to locate the corresponding tumor from the original segmented image.</p><p>2) Landmark Detection: To calculate the clock position for each tumor automatically, we define two landmarks located at the left and the right nipples, i.e., N l and N r as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Similar to <ref type="bibr" target="#b67">[68]</ref>, we train a landmark detector (i.e., FCN-4) with a U-Net architecture, where a pre-contrast image is treated as the input and heatmaps for two landmarks are regarded as the output. The architecture of FCN-4 is shown in Fig. <ref type="figure" target="#fig_0">S2</ref> in the Supplementary Materials. In this way, we can automatically detect two nipple landmarks for each precontrast image.</p><p>3) Biopsied Tumor Selection: With the detected nipple landmarks as clock centers, we can calculate the clock position for each detected tumor. The left and right position information for tumors is defined by their distances to two landmarks. That is, if the distance from a particular tumor to the left nipple landmark is smaller than that to the right nipple landmark, we consider this tumor is located in the left breast. The clock information is defined by both positions of nipple landmarks and tumors, where the nipple landmark is used as the clock center. Based on the estimated clock position of each of the tumors and clock position of the biopsied tumor from the clinical records, we can eventually identify the biopsied tumor from all detected tumors. The detailed steps are shown in Algorithm 1, and the computational cost of our proposed method is reported in Section J of the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Biopsied Tumor Selection</head><p>Input: Pre-contrast image, segmentation of multiple tumors, and clock position of biopsied tumors. <ref type="bibr" target="#b0">1</ref> Step 1: Tumor Labeling.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we first introduce experimental settings, evaluation criteria, and competing methods. Then, we present results of multi-tumor segmentation, biopsied tumor segmentation, and molecular subtype classification based on features extracted from biopsied tumors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>As mentioned in Section III, we use D 1 as the training set, and D 2 as the test set. To obtain the rough segmentation (input of FCN-3) for each subject in the training set, we perform twofold cross-validation to obtain rough segmentation via FCN-2. Specifically, we randomly split the training data into two subsets and perform the two-fold cross validation to generate valid rough segmentation (with high recall) for all training data. In doing so, we can obtain the valid prediction for each training subject. Then, we train FCN-3 using all training images and rough segmentation using reinforcement sampling strategy, and apply it to testing subjects to obtain accurate tumor segmentation results for each test subject.</p><p>We perform three groups of experiments to evaluate the effectiveness of the proposed method. Specifically, in the first group, we validate the proposed MHL method in multi-tumor segmentation. Here, we focus on evaluating the ability of our method in identifying any tumor in a particular DCE-MR image regardless of whether the tumor underwent biopsy. In the second group, we performed biopsied tumor segmentation, to test the capability of our method in identifying tumors that underwent biopsy. Also, segmentation data from multiple expert readers are used to evaluate the performance of our method in the context of inter-reader variability. In the third group of experiments, we apply our tumor segmentation method to a radiogenomic analysis. That is, we first extract imaging features from segmented tumor regions, and then test whether these features can predict molecular subtype of the tumors. In this task, we aim to distinguish the luminal A subtype from the other three subtypes, including 1) luminal B, 2) human epidermal growth factor receptor 2 (HER2) enriched, and 3) basal-like <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>. The experimental results of these three groups of experiments are shown in Sections V-B, V-C, and V-D, respectively.</p><p>In the proposed MHL method, the programming language we used is Python, and the convolutional neural networks are implemented on the platform of Pytorch <ref type="bibr" target="#b68">[69]</ref>. Also, our program was run on a single GPU (i.e., NVIDIA GTX 1080 8GB). In this study, we adopt Dice Similarity Coefficient (DSC), Sensitivity (SEN), and Positive Predictive Value (PPV) as evaluation criteria. The definitions of these measurements are introduced in Section E of the Supplementary Materials. In addition, to evaluate the classification performance of a classifier predicting tumor subtype, we compute the area under the receiver operating characteristic curve (AUC), based on the features extracted from segmented biopsied tumor regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results of Multi-tumor Segmentation</head><p>We compare our mask-guided hierarchical learning (MHL) method with six state-of-the-art segmentation methods for breast tumor segmentation, including 1) Random Forest based method <ref type="bibr" target="#b69">[70]</ref>, 2) Patch-based CNN <ref type="bibr" target="#b70">[71]</ref>, 3) 2D U-Net <ref type="bibr" target="#b39">[40]</ref>, 4) 3D U-Net method <ref type="bibr" target="#b39">[40]</ref>, 5) 3D V-Net method <ref type="bibr" target="#b43">[44]</ref>, and 6) 3D kU-Net <ref type="bibr" target="#b41">[42]</ref> with two cascaded fully convolutional networks. The detailed implementations of these methods are introduced in Section C of the Supplementary Materials.</p><p>We first report segmentation results for multiple breast tumors achieved by our MHL method and four competing methods in Table <ref type="table" target="#tab_2">I</ref>, where multiple tumors were annotated by radiologists (see Section III-A). We also perform paired t-test between results generated by our method and each competing method in a subject-wise manner. For each of the measures, we report it in bold if it was statistically significantly different from our method at the significance level of 0.05 in Table <ref type="table" target="#tab_2">I</ref>.</p><p>For the interested reader, we show the specific p-values in Table <ref type="table">S5</ref> in the Supplementary Materials. From Table <ref type="table" target="#tab_2">I</ref>, we make the following observations. First, our MHL method always performs better on average than all four competing methods in terms of all the evaluation metrics (DSC, SEN, and PPV). Almost all differences are statistically significant. For instance, MHL achieves a DSC value of 71.76%, which is notably higher than the second best result (68.63%) yielded by 3D U-Net. Second, MHL achieves promising SEN value (75.04%) in multi-tumor segmentation, indicating its strong capability in identifying tumor regions. In addition, our MHL method achieves more than 6% improvement in terms of PPV compared with the other four methods, implying that MHL can effectively discard false positives. Possible reasons may be that the proposed hierarchical learning strategy, where we first adopt FCN-2 to roughly discard those confounding voxels and keep as many as possible tumor-like regions, and then resort to FCN-3 to accurately detect tumor regions. Another reason could be that the proposed reinforcement sampling strategy helps alleviate the class-imbalance problem. The differences are all statistically significant except for sensitivity difference between our method and 3D U-Net (p = 0.23) and 3D kU-Net (p = 0.29). Please note that the overall Dice coefficient and PPV of our method are significantly better than 3D U-Net and 3D kU-Net, reflecting that the main strength of our method is the reduction of false positive findings.</p><p>To qualitatively evaluate the performances of different methods in multi-tumor segmentation, we illustrate the segmentation results achieved by our MHL method and six competing methods in Fig. <ref type="figure">5</ref>. For clarity, we also show the ground truth in the last column of Fig. <ref type="figure">5</ref>. Tumors detected by our method in formats of 2D slices and 3D rendering with breast mask are also provided in Fig. <ref type="figure" target="#fig_1">S3</ref> of the Supplementary Materials. From Fig. <ref type="figure">5</ref>, we can see that the segmentation results obtained by our MHL method are very similar to ground truth, while the other four methods often yield many false positive regions. For instance, in the first row of Fig. <ref type="figure">5</ref>, the segmentation results achieved by Random forest, Patch-based CNN, and 2D CNN include many confounding regions. In contrast, 3D U-Net and our MHL method generate more accurate segmentation results, and MHL performs slightly better than 3D U-Net. Note that 3D U-Net adopts conventional Dice-like loss function, while we use the proposed Dice-Sensitivity-like loss function in the proposed FCN-2 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results of Biopsied Tumor Segmentation</head><p>In this work, we propose a landmark detection based method (see Section IV-C) to identify biopsied tumors from all detected tumors. In Table <ref type="table" target="#tab_3">II</ref>, we show the segmentation results for biopsied tumor achieved by our method and four competing methods, where the ground truth for biopsied tumor segmentation (see Section III-A) are annotated by four different radiologists (denoted as R 1 , R 2 , R 3 , and R 4 ). We also show the specific p-values in Table <ref type="table">S6</ref> in the Supplementary Materials. For a fair comparison, those four competing methods share the same landmark detection scheme for biopsied tumor identification that is one of the novel contributions of our paper. From Table <ref type="table" target="#tab_3">II</ref>, we can observe that our MHL method outperforms those four competing methods regarding in all three evaluation criteria (i.e., DSC, SEN, and PPV) when compared to annotations provided by each of the radiologists except for one measure for one other method for annotations of one radiologist (almost identical performance). Almost all improvements are statistically significant at the 0.05 level. Similarly, as in the evaluation of our algorithm to detect multiple tumors, some of the differences between our algorithm and some other methods are not statistically significant in terms of sensitivity while remaining significant for Dice similarity coefficient and positive predictive value. This further validates that the main strength of our method as compared to some recently proposed models is its ability to reduce false positive predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results of Molecular Subtype Classification</head><p>We further perform molecular subtype classification based on features extracted from segmented biopsied tumors. Specifically, we identify luminal A from four types of tumor subtypes, including luminal A, luminal B, human epidermal growth factor receptor 2 enriched, and basal-like <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>. We use three types of morphological features from biopsied tumors segmented by our MHL method and four radiologists (i.e., R 1 , R 2 , R 3 , and R 4 ). To be specific, two heterogeneity kinetic features <ref type="bibr" target="#b3">[4]</ref> are variance and proportion of time to peak (TTP) of the characteristic kinetic curve, denoted as f 1 and f 2 respectively. The third type of feature <ref type="bibr" target="#b71">[72]</ref> is signal enhancement ratio (SER) partial tumor volume, denoted as f 3 . A detailed description of three features can be found in Section C of Supplementary Materials. Since the focus of this study is breast tumor segmentation, we only evaluate the classification performance of luminal A vs. the other three genome subtypes by calculating the area under the receiver operating characteristic curve (AUC) using each feature individually.</p><p>Table <ref type="table" target="#tab_3">III</ref> shows the AUC values in molecular subtype classification based on three types of features. It can be observed from Table <ref type="table" target="#tab_3">III</ref> that, based on the first type of feature (i.e., f 1 ), our MHL method yields much higher AUC value (i.e., 69.01%), compared with methods using features extracted from ground-truth tumor regions annotated by four radiologists. Using the other two representations, MHL achieves reasonable results in molecular subtype classification, implying that biopsied tumor regions generated by our automatic segmentation method are useful in radiogenomic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Influence of Hierarchical Learning Strategy</head><p>In this group of experiments, we show the results of multitumor segmentation yielded by only FCN-2 and the proposed  hierarchical (i.e., "FCN-2 + FCN-3") learning method in Fig. <ref type="figure" target="#fig_8">6</ref>. From Fig. <ref type="figure" target="#fig_8">6</ref> (a), we can see that our proposed "FCN-2 + FCN-3" method achieves better results than the method using only FCN-2. In particular, "FCN-2 + FCN-3" yields much higher PPV value, compared with FCN-2. That implies that the refinement process via FCN-3 helps discard false positive voxels generated by FCN-2. We further illustrate the cumulative distribution of each measurement in Fig. <ref type="figure" target="#fig_8">6 (b-d</ref>). Similarly, Fig. <ref type="figure" target="#fig_8">6 (b-d</ref>) shows that "FCN-2 + FCN-3" usually outperforms FCN-2, which further demonstrates the effectiveness of the proposed hierarchical learning strategy. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, FCN-2 aims to detect all possible tumors, using the proposed Dice-Sensitivity-like loss function to include tumor-like regions as many as possible. Thus, we can only obtain rough segmentation results because many false positive voxels will be included. In FCN-3, we focus on accurate tumor segmentation via a Dice-like loss function, by further refining the segmentation results of FCN-2. This could partly explain why our hierarchical learning strategy can improve the performance compared with the firststage (i.e., FCN-2) only. Besides, we study the influence of another two strategies (i.e., using breast mask as guidance and reinforcement sampling) on the performance of our method in Section I of the Supplementary Materials.</p><p>Training multiple FCNs that constitute our hierarchical model with limited data (224) poses a risk of over-fitting the models. In our study, we applied two strategies to alleviate the issue. First, we held out 10% of the training dataset during the training process and used it as a validation set in order to stop network training. Second, we have used networks with a moderate number of parameters for each step of our algorithm to find a balance between a sufficient complexity to perform an accurate segmentation and a relatively low likelihood of over-training. These two are common strategies to avoid overtraining in developing machine learning models. Note that our method could not perform the exactly end-to-end training, since several FCN models were trained separately. However, our method can perform end-to-end segmentation for new testing subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inter-rater Error</head><p>In biopsied tumor segmentation, for each subject, we have four ground-truth segmentations annotated by four radiologists. In regard to comparing results with multiple readers, there are several studies <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref> introducing how to generate average boundaries based on multiple annotations and perform the statistical evaluation. However, our annotations may have no overlap in certain situations, such as different radiologists marked different regions as tumors. Thus, it is difficult to generate average boundaries. Therefore, we compare our results with four radiologists separately. As indicated in Table <ref type="table" target="#tab_3">II</ref>, automatic segmentation methods can obtain different performance based on different ground truth. In this subsection, we analyze possible errors among different radiologists. Specifically, since four radiologists can be combined into six possible pairs, we can calculate DSC for each pair in the task of biopsied tumor segmentation, and the mean DSC is 78.65 ± 29.82 (%). By comparing the segmentation results achieved by our method with each of four ground-truth results, we can obtain a specific DSC value, and then get the mean DSC of 71.83 ± 28.46 (%). Besides, Fig. <ref type="figure" target="#fig_9">7</ref> (a) shows six cumulative distribution of interrater DSC in blue curves. Moreover, we also show the results of biopsied tumor segmentation achieved by our MHL method with the ground truth annotated by four radiologists, respectively, in red curves. These curves indicate that most of the red and blue curves are stacked together. Fig. <ref type="figure" target="#fig_9">7 (b)</ref> shows the average of inter-rater DSC (blue curve) and that of our method (red curve). We can see from Fig. <ref type="figure" target="#fig_9">7</ref> (b) that the average result of our method is very close to the average ground truth annotated by four radiologists.</p><p>On the other hand, we can see from Table <ref type="table" target="#tab_3">II</ref> and Fig. <ref type="figure" target="#fig_9">7</ref> that there are large variances in both inter-observer accuracy and DSC achieved by our method. This is a reflection of the difficulty of the task and a certain level of subjectivity associated with the task as demonstrated by the limited interreader agreement. Therefore, it is expected that the agreement for different cases (between the reader and the algorithm, and among different readers) will vary substantially.</p><p>In addition, for subject, we compare our result with that annotated by 4 radiologists, and select the best fit (i.e., maximum DSC) one as the ground truth to calculate the evaluation measurements. If our segmentation at least fits one of those radiologists, our segmentation could be reasonable. In doing so, our mean DSC reaches 78.51±26.25 (%). Moreover, in Fig. <ref type="figure" target="#fig_9">7</ref> (b), we show the cumulative distribution of the best fit result in the green curve. The overlap of three curves in Fig. <ref type="figure" target="#fig_9">7</ref> (b) suggests that the results of our method is comparable to that annotated by radiologists, and thus could be useful in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limitations and Future Work</head><p>Although our method achieves very promising results in breast tumor segmentation, there are still several limitations to be considered. First, our method cannot effectively deal with tumors that are associated with hematoma and extremely small tumors. For instance, as shown in Fig. <ref type="figure" target="#fig_10">8</ref> (a), our method fails to accurately segment tumors with hematoma because such regions are affected by complex morphology and texture of tumors. Also, as shown in Fig. <ref type="figure" target="#fig_10">8</ref> (b), our method might not be able to segment tumors in very small regions, since these small tumors could be regarded as confounding regions and could be discarded by our method. Second, in the current study, we only adopt molecular subtype classification as additional evaluation for our segmentation method. Intuitively, if the shape/texture information is related to a particular molecular subtype, the joint learning of molecular subtype classification and breast tumor segmentation could be a better solution for performance improvement, which will be our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we propose a mask-guided hierarchical learning (MHL) framework for breast tumor segmentation. We first generate breast masks to exclude confounding organs. With breast masks as guidance, we develop two cascaded fully convolutional networks (FCNs) to accurately detected tumor regions. We also develop a landmark detection method to select biopsied tumor from all detected tumors. Experimental results of breast tumor segmentation and molecular subtype classification demonstrate the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the proposed mask-guided hierarchical learning (MHL) framework for breast tumor segmentation. There are three components, including 1) breast mask generation via a fully convolutional network (FCN-1), 2) a two-stage FCN (FCN-2 and FCN-3) for tumor segmentation, and 3) landmark detection based biopsied tumor selection using FCN-4. biomarker) is associated with early metastasis and expression of the known predictor of metastatic progression [58]. Mazurowski et al. extracted 23 imaging features from lesions indicated by a breast radiologist on MR images, and showed that the luminal B subtype of breast cancer is associated with MR imaging features [59]. Sutton et al. investigated the association between a gene-expression-based breast cancer subtype and morphological and texture-based image features extracted from MRI, suggesting that image-based features could predict the likelihood of recurrence and magnitude of chemotherapy benefit [60]. However, these methods rely on features extracted from the entire MR image, without focusing on tumor regions.</figDesc><graphic coords="4,496.85,194.21,60.55,60.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architecture of our hierarchical network, including the first-stage fully convolutional network (i.e., FCN-2) for rough segmentation and the secondstage FCN (i.e., FCN-3) for segmentation refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of clock position of tumor with biopsy. Nr denotes the landmark for the right nipple, while N l represents the landmark for the left nipple. L10 means the tumor located in 10 O'clock position of the left breast in coronal view, where the center of the clock is a nipple.</figDesc><graphic coords="7,95.76,50.33,156.60,78.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 1. 1 :</head><label>21</label><figDesc>Morphological operation: image erosion and image dilation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 1. 2 : 5 2. 1 :</head><label>3251</label><figDesc>Connected components computation and labeling.<ref type="bibr" target="#b3">4</ref> Step 2: Landmark Detection Prediction of nipple landmarks using pre-trained landmark detector FCN-4.<ref type="bibr" target="#b5">6</ref> Step 3: Biopsied Tumor Selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 3. 1 :</head><label>71</label><figDesc>Define the left-right position of each tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>8 3. 2 :</head><label>82</label><figDesc>Calculate the clock position of each tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>9 3. 3 :</head><label>93</label><figDesc>Find the nearest tumor according to the given clock position. 10 if there exist multiple nearest tumors then 11 Select the one with maximum volume. Output: Tumor mask with biopsy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Multi-tumor segmentation results achieved by FCN-2 and the proposed hierarchical (i.e., "FCN-2 + FCN-3") learning strategy. (a) Multitumor segmentation performance in terms of DSC, SEN, and PPV. (b)-(d) Cumulative distribution for DSC, SEN, and PPV, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7.Inter-rater error analysis in biopsied tumor segmentation. (a) Cumulative distribution achieved by 6 pairs of inter-rater (blue curves) and our method (red curves). (b) Overall segmentation performance of radiologists (blue curve), our method (red curve), and the best fit result of our method (green curve).</figDesc><graphic coords="11,319.56,129.63,233.44,57.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Illustration of failed segmentation for tumors by the proposed MHL method. (a) The failed case where tumors are with hematoma, and (b) the failed case where tumors are in small regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,54.10,54.76,504.90,194.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SEGMENTATION</head><label>I</label><figDesc>PERFORMANCE OF MULTIPLE BREAST TUMORS ACHIEVED BY SEVEN DIFFERENT METHODS (%).</figDesc><table><row><cell>Methods</cell><cell>DSC</cell><cell>SEN</cell><cell>PPV</cell></row><row><cell>Random Forest [70]</cell><cell>64.14±27.89</cell><cell>72.36±26.50</cell><cell>67.54±30.96</cell></row><row><cell>Patch-based CNN [71]</cell><cell>65.00±25.65</cell><cell>73.01±26.57</cell><cell>68.75±27.96</cell></row><row><cell>2D U-Net [40]</cell><cell>67.47±26.19</cell><cell>73.11±25.54</cell><cell>69.98±27.18</cell></row><row><cell>3D U-Net [40]</cell><cell>68.63±25.88</cell><cell>74.27±25.01</cell><cell>71.24±25.95</cell></row><row><cell>3D V-Net [44]</cell><cell>67.80±25.50</cell><cell>72.54±24.79</cell><cell>70.97±26.52</cell></row><row><cell>3D kU-Net [42]</cell><cell>68.72±25.47</cell><cell>74.18±25.73</cell><cell>71.32±26.04</cell></row><row><cell>MHL (Ours)</cell><cell>71.76±24.19</cell><cell>75.04±23.12</cell><cell>77.33±21.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II SEGMENTATION</head><label>II</label><figDesc>PERFORMANCE OF BIOPSIED TUMORS ACHIEVED BY SEVEN DIFFERENT METHODS (%). NOTE THE GROUND-TRUTH IS ANNOTATED BY 4 RADIOLOGISTS (i.e., R 1 , R 2 , R 3 , AND R 4 ).</figDesc><table><row><cell>Radiologist</cell><cell>Measure</cell><cell>Our framework+ Random Forest [70]</cell><cell>Our framework+ Patch-based CNN [71]</cell><cell>Our framework+ 2D U-Net [40]</cell><cell>Our framework+ 3D U-Net [40]</cell><cell>Our framework+ 3D V-Net [44]</cell><cell>Our framework+ 3D kU-Net [42]</cell><cell>MHL (Ours)</cell></row><row><cell></cell><cell>DSC</cell><cell>64.48±29.23</cell><cell>64.88±31.21</cell><cell>65.24±31.15</cell><cell>66.94±30.51</cell><cell>65.75±30.54</cell><cell>67.12±29.80</cell><cell>70.11±28.34</cell></row><row><cell>R1</cell><cell>SEN</cell><cell>67.86±31.08</cell><cell>67.46±32.39</cell><cell>67.53±30.86</cell><cell>68.84±27.92</cell><cell>67.72±28.72</cell><cell>68.52±28.12</cell><cell>68.76±27.76</cell></row><row><cell></cell><cell>PPV</cell><cell>71.91±28.07</cell><cell>75.01±28.58</cell><cell>75.81±29.32</cell><cell>77.36±27.61</cell><cell>76.18±27.38</cell><cell>77.45±27.21</cell><cell>85.59±24.09</cell></row><row><cell></cell><cell>DSC</cell><cell>66.50±30.16</cell><cell>66.77±28.46</cell><cell>67.50±28.99</cell><cell>68.95±28.89</cell><cell>68.84±28.73</cell><cell>70.01±28.95</cell><cell>72.94±28.50</cell></row><row><cell>R2</cell><cell>SEN</cell><cell>68.82±30.59</cell><cell>68.45±30.11</cell><cell>68.22±29.48</cell><cell>69.05±29.71</cell><cell>68.95±29.99</cell><cell>69.04±29.70</cell><cell>70.35±28.89</cell></row><row><cell></cell><cell>PPV</cell><cell>75.49±25.60</cell><cell>77.37±25.45</cell><cell>78.00±26.93</cell><cell>79.68±24.72</cell><cell>79.47±25.13</cell><cell>79.75±24.83</cell><cell>87.99±20.05</cell></row><row><cell></cell><cell>DSC</cell><cell>66.23±28.87</cell><cell>67.31±29.81</cell><cell>68.32±29.77</cell><cell>70.52±28.22</cell><cell>70.14±29.30</cell><cell>71.19±28.56</cell><cell>73.41±25.89</cell></row><row><cell>R3</cell><cell>SEN</cell><cell>70.41±30.51</cell><cell>70.10±29.24</cell><cell>71.57±33.10</cell><cell>71.36±31.26</cell><cell>70.87±29.84</cell><cell>71.08±30.93</cell><cell>73.31±30.38</cell></row><row><cell></cell><cell>PPV</cell><cell>71.46±27.64</cell><cell>74.03±28.41</cell><cell>75.36±29.74</cell><cell>77.01±27.93</cell><cell>76.55±27.37</cell><cell>77.45±26.68</cell><cell>84.33±24.04</cell></row><row><cell></cell><cell>DSC</cell><cell>63.97±29.25</cell><cell>64.89±29.77</cell><cell>65.27±29.79</cell><cell>66.50±28.48</cell><cell>66.25±29.96</cell><cell>66.79±29.80</cell><cell>70.89±26.85</cell></row><row><cell>R4</cell><cell>SEN</cell><cell>68.12±30.84</cell><cell>68.70±29.54</cell><cell>68.86±31.73</cell><cell>68.91±30.00</cell><cell>68.16±29.80</cell><cell>68.73±29.88</cell><cell>70.01±29.44</cell></row><row><cell></cell><cell>PPV</cell><cell>70.92±30.27</cell><cell>72.66±28.19</cell><cell>73.52±31.57</cell><cell>74.83±29.82</cell><cell>74.08±29.07</cell><cell>74.94±28.97</cell><cell>81.94±27.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>TABLE III AUC (%) IN GENOME SUBTYPE CLASSIFICATION BASED ON THREE TYPES OF FEATURES (i.e., f 1 , f 2 , AND f 3 ).</figDesc><table><row><cell>AUC</cell><cell>R1</cell><cell>R2</cell><cell>R3</cell><cell>R4</cell><cell>MHL (Ours)</cell></row><row><cell>f1</cell><cell>53.85</cell><cell>54.29</cell><cell>62.20</cell><cell>54.29</cell><cell>69.01</cell></row><row><cell>f2</cell><cell>67.03</cell><cell>64.18</cell><cell>65.93</cell><cell>67.91</cell><cell>63.98</cell></row><row><cell>f3</cell><cell>65.05</cell><cell>64.40</cell><cell>67.91</cell><cell>60.44</cell><cell>66.59</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Forest</head><p>Patch-based CNN 2D U-Net 3D U-Net MHL (Ours) Ground Truth Besides the segmentation results for multiple tumors, we further report the tumor detection results based on the segmented regions. Specifically, to evaluate the tumor detection performance, we define the labeled tumor regions with more than 50% overlap with ground-truth tumor regions as true positive detections; and otherwise, false positive detections. We employ both measures of Recall (i.e., sensitivity) and Precision (i.e., positive predictive value) by tumors to evaluate the detection performance of our proposed method and the competing methods. The results are shown in Table <ref type="table">S2</ref> of the Supplementary Materials.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CA: A</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Journal for Clinicians</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="30" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>Cancer Statistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Desantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Fedewa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convergence of incidence rates between black and white women</title>
		<imprint>
			<date type="published" when="2015">2015. 2016</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
	<note>Breast cancer statistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Assessing heterogeneity of lesion enhancement kinetics in dynamic contrast-enhanced MRI for breast cancer diagnosis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karahaliou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vassiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arikidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanavou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Costaridou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The British Journal of Radiology</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identification of intrinsic imaging phenotypes for breast cancer tumors: Preliminary associations with gene expression profiles</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Daye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gavenonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kontos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="374" to="384" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mammographic tumor features can predict long-term outcomes reliably in women with 1-14-mm invasive breast carcinoma</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H. Tony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amy Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1745" to="1759" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cancerous breast lesions on dynamic contrast-enhanced MR images: Computerized characterization for image-based prognostic markers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bhooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Giger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="680" to="690" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Medical image data and datasets in the era of machine learning-Whitepaper from the 2016 C-MIMI meeting dataset session</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Geis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic brain tumor segmentation by subject specific modification of atlas priors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Leemput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic Radiology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1341" to="1348" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-atlas segmentation without registration: A supervoxel-based approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Segmentation and classification of breast tumor using dynamic contrastenhanced MR images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Englander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="393" to="401" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multichannel Markov random field framework for tumor segmentation with an application to classification of gene expressionbased breast cancer recurrence risk</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Gavenonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Daye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kontos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="637" to="648" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance of a fully automatic lesion detection system for breast DCE-MRI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vignati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Giannini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Persano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Carbonaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bertotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martincich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Regge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1341" to="1351" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured learning for 3d perivascular spaces segmentation using vascular features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate segmentation of CT male pelvic organs via regression-based deformable models and multi-task random forests</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Localization and segmentation of 3D intervertebral discs in MR images by data driven estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Armbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Felsenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1719" to="1729" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An automatic learning-based framework for robust nucleus segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="550" to="566" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Landmark-based deep multiinstance learning for brain disease diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="157" to="168" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting anatomical landmarks from limited medical imaging data using two-stage task-oriented deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4753" to="4764" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relationship induced multi-template learning for diagnosis of Alzheimer&apos;s disease and mild cognitive impairment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1463" to="1474" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-channel multi-scale fully convolutional network for 3D perivascular spaces segmentation in 7T MR images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="106" to="117" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pairwise constraint-guided sparse learning for feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="298" to="310" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in MRI images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">View-aligned hypergraph learning for Alzheimer&apos;s disease diagnosis with incomplete multimodality data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="123" to="134" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic liver and tumor segmentation of CT and MRI volumes using cascaded fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ettlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E A</forename><surname>Elshaera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lipkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schlecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmaddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tatavarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05970</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from unbalanced data: A cascade-based approach for detecting clustered microcalcifications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tortorella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="252" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fuzzy c-means (fcm)-based approach for computerized segmentation of breast lesions in dynamic contrast-enhanced MR images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Giger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic Radiology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detection and classification of contrast-enhancing masses by a fully automatic computer-assisted diagnosis system for breast MRI</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Böttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diekmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Streitparth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Collettini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1077" to="1088" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhancing deep convolutional neural network scheme for breast cancer diagnosis with unlabeled data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><forename type="middle">B</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="4" to="9" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Breast cancer diagnosis in DCE-MRI using mixture ensemble of convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teshnehlab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="381" to="390" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interobserver variability in identification of breast tumors in MRI and its implications for prognostic biomarkers and radiogenomics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Ghate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4558" to="4564" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated localization of breast cancer in DCE-MRI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gubern-Mérida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Platel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning-based automatic breast tumor detection and segmentation in ultrasound images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Megalooikonomou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1587" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Breast tumor segmentation and classification using SVM and Bayesian from thermogram images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dinsha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Manikandaprabu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Unique Journal of Engineering and Advanced Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="151" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully automatic lesion segmentation in breast MRI using mean-shift and graph-cuts on a region adjacency graph</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcclymont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trakic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Crozier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="795" to="804" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Globally optimal breast mass segmentation from DCE-MRI using deep semantic segmentation as shape prior</title>
		<author>
			<persName><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="305" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic skin lesion segmentation using deep fully convolutional networks with Jaccard distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint craniomaxillofacial bone segmentation and landmark digitization by context-guided fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>-F. Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep 3D convolutional encoder networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Traboulsee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1239" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combining fully convolutional and recurrent neural networks for 3D biomedical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3036" to="3044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Residual deconvolutional networks for brain electron microscopy image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fakhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="456" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using deep learning to segment breast and fibroglandular tissue in MRI volumes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Dalmıs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¸</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gubern-Mérida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="533" to="546" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large scale deep learning for computer aided detection of mammographic lesions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gubern-Mérida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Den Heeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="303" to="312" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Automated soft tissue lesion detection and segmentation in digital mammography using a u-net deep learning network</title>
		<author>
			<persName><forename type="first">T</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teuwen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06865</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning applied to breast density segmentation and mammographic risk scoring</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kallenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Vachon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Winkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1322" to="1331" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning in mammography: accuracy of a multipurpose image analysis software in the detection of breast cancer</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghafoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Wurnig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frauenfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative Radiology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="434" to="440" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for active breast lesion detection from DCE-MRI</title>
		<author>
			<persName><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hybrid mass detection in breast MRI combining unsupervised saliency analysis and deep learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hadad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tlusty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automated breast ultrasound lesions detection using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sentís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zwiggelaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aggnet: Deep learning from crowds for mitosis detection in breast cancer histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Achilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1313" to="1321" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Her2net: A deep framework for semantic segmentation and classification of cell membranes and nuclei in breast cancer evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2189" to="2200" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep learning in mammography and breast histology, an overview and future trends</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hamidinekoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rampun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Honnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zwiggelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="45" to="67" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Radiogenomics: Creating a link between molecular diagnostics and diagnostic imaging</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Radiology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="241" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Radiogenomic analysis of breast cancer using MRI: A preliminary study to define the landscape</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Roentgenology</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="654" to="663" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Breast cancer: Radiogenomic biomarker reveals associations among dynamic contrast-enhanced MR imaging, long noncoding RNA, and metastasis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="384" to="392" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Radiogenomic analysis of breast cancer: Luminal b molecular subtype is associated with enhancement dynamics at MR imaging</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Silber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="372" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Breast cancer subtype intertumor heterogeneity: MRI-based features predict results of a genomic assay</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Dashevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Apte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Deasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Magnetic Resonance Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1398" to="1406" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recurrence-free survival in breast cancer is associated with MRI tumor enhancement dynamics quantified using computer algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Marcom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Ghate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Radiology</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2117" to="2122" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<title level="m">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A fully automated scheme for mammographic segmentation and classification based on breast density and asymmetry</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Tzikopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mavroforakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Evaluation and comparison of 3D intervertebral disc localization and segmentation methods for 3D T2 MR data: A grand challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Belavỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ibragimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Korez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vrtovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Everson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Andrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="327" to="344" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automatic X-ray landmark detection and shape segmentation via datadriven joint estimation of image displacements</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grutzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="487" to="499" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Regression forests for efficient anatomy detection and localization in CT studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Workshop on Medical Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Regressing heatmaps for multiple landmark localization using CNNs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Payer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Štern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Pytorch: Tensors and dynamic neural networks in Python with strong GPU acceleration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Automatic craniomaxillofacial landmark digitization via segmentation-guided partially-joint regression forest model and multiscale statistical features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1820" to="1829" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Effective semantic pixel labelling with convolutional networks and conditional random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Can signal enhancement ratio (SER) reduce the number of recommended biopsies without affecting cancer yield in occult MRI-detected lesions?</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Newitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Hylton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Joe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic Radiology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="721" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A methodology for evaluation of boundary detection algorithms on medical images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chalana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="642" to="652" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Comments on: A methodology for evaluation of boundary detection algorithms on medical images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Alberola-Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martín-Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Alzola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="658" to="660" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
