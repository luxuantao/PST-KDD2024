<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ming</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF9D617A7C3E9CAF7D3866EC6009B8C4</idno>
					<idno type="DOI">10.1109/TIP.2015.2442920</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Perceptual Quality Assessment for Multi-Exposure Image Fusion Kede Ma, Student Member, IEEE, Kai Zeng, and Zhou Wang, Fellow, IEEE Abstract-Multi-exposure image fusion (MEF) is considered an effective quality enhancement technique widely adopted in consumer electronics, but little work has been dedicated to the perceptual quality assessment of multi-exposure fused images. In this paper, we first build an MEF database and carry out a subjective user study to evaluate the quality of images generated by different MEF algorithms. There are several useful findings. First, considerable agreement has been observed among human subjects on the quality of MEF images. Second, no single state-of-the-art MEF algorithm produces the best quality for all test images. Third, the existing objective quality models for general image fusion are very limited in predicting perceived quality of MEF images. Motivated by the lack of appropriate objective models, we propose a novel objective image quality assessment (IQA) algorithm for MEF images based on the principle of the structural similarity approach and a novel measure of patch structural consistency. Our experimental results on the subjective database show that the proposed model well correlates with subjective judgments and significantly outperforms the existing IQA models for general image fusion. Finally, we demonstrate the potential application of the proposed model by automatically tuning the parameters of MEF algorithms. <ref type="foot" target="#foot_0">1</ref>Index Terms-Multi-exposure image fusion (MEF), image quality assessment, structural similarity, luminance consistency, subjective evaluations, perceptual image processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ULTI-EXPOSURE image fusion (MEF) is considered an effective quality enhancement technique that is widely adopted in consumer electronics <ref type="bibr" target="#b0">[1]</ref>. MEF takes a sequence of images with different exposure levels as inputs and synthesizes an output image that is more informative and perceptually appealing than any of the input images <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. MEF fills the gap between high dynamic range (HDR) natural scenes and low dynamic range (LDR) pictures captured by normal digital cameras. Comparing with typical HDR imaging techniques which first construct an HDR image from the source sequence and then tone-map it to an LDR image, MEF bypasses the intermediate HDR image construction step and directly yields an LDR image that can be displayed on standard viewing devices.</p><p>Since first introduced in 1980's <ref type="bibr" target="#b1">[2]</ref>, MEF has been an active research topic and attracted an increasing amount of attention in recent years <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b12">[13]</ref>. With many MEF algorithms at hand, it becomes pivotal to compare their performance, so as to find the best algorithm as well as directions for further advancement. Because the human visual system (HVS) is the ultimate receiver in most applications, subjective evaluation is a straightforward and reliable approach to evaluate the quality of fused images <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. Although expensive and time consuming <ref type="bibr" target="#b16">[17]</ref>, a comprehensive subjective user study has several benefits. First, it provides useful data to study human behaviors in evaluating perceived quality of fused images. Second, it supplies a test set to evaluate and compare the relative performance of classical and state-of-the-art MEF algorithms. Third, it is useful to validate and compare the performance of existing objective image quality assessment (IQA) models in predicting the perceptual quality of fused images. This will in turn provide insights on potential ways to improve them.</p><p>Over the past decade, substantial effort has been made to develop objective IQA models for image fusion applications <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Most of them are designed for generalpurpose image fusion applications, not specifically for MEF, and some of them can only work with the case of two input images. Furthermore, little has been done to compare them with (or calibrate against) subjective data that contains a wide variety of source sequences and MEF algorithms.</p><p>In this work, we aim to tackle the problem of perceptual quality assessment of MEF images. We build one of the first databases dedicated to subjective evaluation of MEF images. The database contains 17 source sequences with multiple exposure levels (≥ 3) and the fused images generated by 8 classical and state-of-the-art MEF algorithms. Based on the database, we carry out a subjective user study to evaluate and compare the quality of the fused images. We observe considerable agreement between human subjects, and not a single MEF algorithm produces the best quality for all test images. More importantly, we find that existing objective quality models for general image fusion are very limited in predicting perceived quality of MEF images. This motivates us to develop a novel objective IQA model for MEF images. Our model is inspired by the structural similarity (SSIM) index <ref type="bibr" target="#b26">[27]</ref>, whose philosophy is that the HVS is highly adapted for extracting structural information from natural scenes. To compare the structures of multiple patches from different exposures, we introduce a novel measure of patch structural consistency. Furthermore, to balance between finer-scale detail preservation and coarserscale luminance consistency <ref type="bibr" target="#b15">[16]</ref>, we adopt a multi-scale approach <ref type="bibr" target="#b27">[28]</ref>, where with the scale shifting from fine to coarse, SSIM-based structural comparison captures image distortions from fine details to large-scale luminance variations. Experimental results show that the proposed model well correlates with subjective judgments and significantly outperforms existing objective IQA models for general image fusion. The value of objective models are beyond measuring and comparing MEF images and algorithms. A reliable objective model can play a key role in the design and optimization of novel MEF algorithms. To demonstrate this potential, we apply the proposed model to automatic parameter tuning of an advanced MEF algorithm. Numerical experiments show that the proposed model provides a useful tool to exploit the parameter space and to pick the optimal parameter that produces MEF images of the best perceptual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The problem of MEF can be generally formulated as</p><formula xml:id="formula_0">Y(i ) = K k=1 W k (i )X k (i ), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where K is the number of multi-exposure input images in the source sequence, X k (i ) and W k (i ) represent the luminance value (or the coefficient amplitude in the transform domain) and the weight at the i -th pixel in the k-th exposure image, respectively. Y denotes the fused image. The weight factor W k (i ) is often spatially adaptive and bears information regarding the relative structural detail and perceptual importance at different exposure levels. Depending on the specific models for structural information and perceptual importance, MEF algorithms differ in the computation of W k . A significant number of MEF algorithms have been proposed, ranging from simple weighted averaging to sophisticated methods based on advanced statistical image models. Local and global energy weighting approaches are the simplest ones, which employ the local or global energy in the image to determine W k . Dated back to 1984, Burt <ref type="bibr" target="#b1">[2]</ref> first employed Laplacian pyramid decomposition for binocular image fusion. Later in 1994, <ref type="bibr">Burt and</ref> Kolczynski applied this decomposition to MEF, where they selected the local energy of pyramid coefficients and the correlation between pyramids within the neighborhood to compute W k . Laplacian pyramid turns out to be an effective scheme in image fusion to avoid unnatural appearance and unwanted artifacts introduced by fusion in the spatial domain <ref type="bibr" target="#b4">[5]</ref>. Goshtasby <ref type="bibr" target="#b3">[4]</ref> partitioned each source image into several non-overlapping blocks and selected the block with the highest entropy to construct the fused image. Due to the non-overlapping partition, the method inevitably suffers from blocking artifacts. Mertens et al. <ref type="bibr" target="#b4">[5]</ref> adopted proper contrast, high saturation and well exposure as quality measures to guide the fusion process in a multiresolution fashion. Using the same weighting map to guide the fusion in the spatial domain, the method tends to introduce artificial edges and color distortions in the fused image. Bilateral filter is used in <ref type="bibr" target="#b5">[6]</ref> to calculate edge information, which is subsequently employed to compute the weights. This method puts no constraints on global luminance consistency and often produces dark appearance of the fused image. Song et al. <ref type="bibr" target="#b6">[7]</ref> firstly estimated the initial image by maximizing the visual contrast and scene gradient, and synthesized the fused image by suppressing reversals in image gradients. In practice, this method tends to produce color saturated images. Zhang and Cham <ref type="bibr" target="#b7">[8]</ref> constructed visibility and consistency measures from gradient information and used them as the weighting factors. The adoption of gradient direction enables the method to fuse a source sequence captured in a dynamic scene that has moving objects. A similar gradient-based MEF method is proposed in <ref type="bibr" target="#b8">[9]</ref>. Based on <ref type="bibr" target="#b4">[5]</ref>, Li et al. <ref type="bibr" target="#b9">[10]</ref> enhanced the details of a given fused image by solving a quadratic optimization problem. A median filter and recursive filter based MEF method is developed in <ref type="bibr" target="#b10">[11]</ref> by taking local contrast, brightness and color dissimilarity into consideration. The use of median filter also enables the method to handle dynamic scenes. More recently, Li et al. <ref type="bibr" target="#b11">[12]</ref> proposed a guided filter to control the roles of pixel saliency and spatial consistency when constructing W k . Shen et al. <ref type="bibr" target="#b12">[13]</ref> embedded perceived local contrast and color saturation into a conditional random field and derived W k based on maximum a posteriori estimation.</p><p>Despite the increasing interests in developing objective IQA models for various image fusion applications, systematic and comprehensive evaluation and comparison of these models have been largely lacking. To validate the performance of objective IQA models, subjective user study is necessary. Toet and Franken <ref type="bibr" target="#b13">[14]</ref> examined the perceptual quality of multi-scale image fusion schemes, where only night-time outdoor scenes and very simple fusion methods were included in the study. Petrović <ref type="bibr" target="#b14">[15]</ref> reported subjective assessment results for multi-sensor image fusion algorithms. However, the number of input images was limited to 2 and most test images were monochrome aerial pictures. Moreover, state-of-the-art image fusion algorithms are missing from the experiment. To demonstrate the effectiveness of their fusion algorithm, Song et al. <ref type="bibr" target="#b6">[7]</ref> conducted two groups of paired comparison tests through both on-site and Web platforms, where the subjective experimental results only include few examples. Shen et al. <ref type="bibr" target="#b12">[13]</ref> reported subjective evaluation results to verify the performance of their algorithm. However, the number of test images involved is also limited. To the best of our knowledge, a comprehensive subjective user study that contains sufficient test sequences and compares a wide variety of MEF algorithms has not been reported in the literature.</p><p>An excellent survey on objective IQA models for image fusion applications can be found in <ref type="bibr" target="#b28">[29]</ref>. Here we only provide a brief overview: Qu et al. <ref type="bibr" target="#b17">[18]</ref> combined the mutual information between the fused and multiple input images to evaluate image quality. Xydeas and Petrovic <ref type="bibr" target="#b18">[19]</ref> extracted edge information using the Sobel operator and employed edge strength as the main feature in assessing the quality of fused images. A similar idea was employed in <ref type="bibr" target="#b19">[20]</ref>, where Wang and Liu retrieved edge strength using a two-scale Haar wavelet. Zheng et al. <ref type="bibr" target="#b20">[21]</ref> computed spatial frequency using multi-directional gradient filters and estimated the quality of fused images based on activity levels. Inspired by the SSIM index <ref type="bibr" target="#b26">[27]</ref> for general-purpose IQA, Piella and Heijmans <ref type="bibr" target="#b21">[22]</ref> developed three models to predict fused image quality based on the universal quality index <ref type="bibr" target="#b29">[30]</ref>. Cvejic et al. <ref type="bibr" target="#b22">[23]</ref> and Yang et al. <ref type="bibr" target="#b23">[24]</ref> also built their quality measures upon structural information theory. Chen and Varshney <ref type="bibr" target="#b24">[25]</ref> estimated local saliency based on edge intensities and combined saliency with global contrast sensitive function. Chen and Blum <ref type="bibr" target="#b25">[26]</ref> applied contrast sensitivity filter in the frequency domain and then pool local information preservation scores to produce a global quality measure. All aforementioned models are designed for general-purpose image fusion applications, not specifically for MEF. In most cases, the performance of existing models were demonstrated using limited examples only, without being thoroughly validated based on subjective user studies. Most of the algorithms were elaborated with the source sequence containing two input images only. However, most of these algorithms can be extended to the cases of multiple input images except <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SUBJECTIVE QUALITY ASSESSMENT OF MEF IMAGES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Database and Subjective User Study</head><p>Seventeen high-quality natural source image sequences are selected to cover diverse image content including indoor and outdoor views, natural sceneries, and man-made architectures. All source image sequences are shown in Fig. <ref type="figure" target="#fig_0">1</ref> and listed in Table <ref type="table" target="#tab_0">I</ref>. All of them contain at least 3 input images that represent underexposed, overexposed, and in-between cases. For visualization purpose, in Fig. <ref type="figure" target="#fig_0">1</ref>, we select the best quality fused image in terms of subjective evaluations to represent each source sequence.</p><p>Eight MEF algorithms are selected, which include simple operators such as 1) local energy weighted linear combination and 2) global energy weighted linear combination, as well as advanced MEF algorithms such as 3) Raman09 <ref type="bibr" target="#b5">[6]</ref>, 4) Gu12 <ref type="bibr" target="#b8">[9]</ref>, 5) ShutaoLi12 <ref type="bibr" target="#b10">[11]</ref>, 6) ShutaoLi13 <ref type="bibr" target="#b11">[12]</ref>, 7) Li12 <ref type="bibr" target="#b9">[10]</ref>, and 8) Mertens07 <ref type="bibr" target="#b4">[5]</ref>. These algorithms are chosen to cover a diverse types of MEF methods in terms of methodology and behavior. In all cases, default parameter settings are adopted without tuning for better quality. Eventually, a total of 136 fused images are generated, which are divided into 17 image sets of 8 images each, where the images in the same set are created from the same source sequence. An example is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, which includes a source sequence at three exposure levels (Fig. <ref type="figure" target="#fig_2">2(a1-a3</ref>)) and eight fused images (Fig. <ref type="figure" target="#fig_1">2(b-i</ref>)). Note that different MEF algorithms produce substantially different fused images in terms of perceptual appearance and quality. Thus, quality assessment of fused images are desirable to pick the one with the best quality.</p><p>The subjective testing environment was setup as a normal indoor office workspace of ordinary illumination level, with no reflecting ceiling walls and floor. All images are displayed on an LCD monitor at a resolution of 2560 × 1600 pixel with Truecolor (32bit) at 60Hz. The monitor was calibrated in accordance with the recommendations of ITU-T BT.500 <ref type="bibr" target="#b30">[31]</ref>. A customized MATLAB figure window was used to render the images on the screen. During the test, all 8 fused images from the same set are shown to the subject at the same time on one computer screen at actual pixel resolution but in random spatial order. The study adopted a multi-stimulus quality scoring strategy without showing the reference sequence. A total of 25 naïve observers, including 15 male and 10 female subjects aged between 22 and 30, participated in the subjective experiment. The subjects are allowed to move their positions to get closer or further away from the screen for better observation. All subject ratings were recorded with pen and paper during the study. To minimize the influence of visual fatigue, the length of a session was limited to a maximum of 30 minutes. For each image set, the subject was asked to give an integer score that best reflects the perceptual quality of each fused image. The score ranges from 1 to 10, where 1 denotes the worst quality and 10 the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subjective Data Analysis</head><p>After the subjective experiment, 2 outlier subjects were removed based on the outlier removal scheme in <ref type="bibr" target="#b30">[31]</ref>, resulting in 23 valid subjects. The final quality score for each individual image is computed as the average of subjective scores, namely the mean opinion score (MOS), from all valid subjects. Considering the MOS as the "ground truth", the performance of individual subjects can be evaluated by calculating the correlation coefficient between individual subject ratings and MOS values for each image set, and then averaging the correlation coefficients of all image sets. Pearson linear correlation coefficient (PLCC) and Spearman's rand-order correlation coefficient (SRCC) are employed as the evaluation criteria <ref type="bibr" target="#b31">[32]</ref>. Both criteria range from 0 to 1, where higher values indicate better performance. The mean and standard deviation (std) of the results are depicted in Fig. <ref type="figure" target="#fig_2">3</ref>. It can be seen that each individual subject performs quite consistently with relatively low variations for different image content. The average performance across all individual subjects is also given in the rightmost columns of Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of Existing MEF Algorithms</head><p>We use the MOS values of the 8 MEF algorithms described in Section III-A to evaluate and compare their performance. The mean and std of MOS values over all 17 image sets are summarized in Fig. <ref type="figure" target="#fig_3">4</ref>. It is worth mentioning that this only provides a rough comparison of the relative performance of the MEF algorithms, where default parameters are used without fine tuning. Besides, computational complexity is not a factor under consideration.  From the subjective test results, we have several observations. First, from the sizes of the error bars, we observe that subjects agree with each other to a significant extent on the performance of any individual MEF algorithm, but the performance difference between different MEF algorithms is sometimes small (when compared with the error bars). Second, Mertens's method <ref type="bibr" target="#b4">[5]</ref> achieves the best performance on average, while Li's method <ref type="bibr" target="#b9">[10]</ref>, which is the second best on average, is actually a detail-enhanced algorithm built upon Mertens's method <ref type="bibr" target="#b4">[5]</ref>. It has very similar average performance and a larger error-bar than Mertens's method <ref type="bibr" target="#b4">[5]</ref>. This suggests that detail enhancement might be useful to create perceptually appealing results on some images, but may also create unwanted artifacts in some other images, and the overall performance gain is not always guaranteed in the current approaches. Third, comparing local energy weighting with global energy weighting approaches, the former focuses more on enhancing local structures while the latter emphasizes more on global luminance consistency. The large performance gap between them indicates that maintaining large-scale luminance consistency may be an important factor in determining the quality of the fused images. Fourth, not a single algorithm produces fused images with the best perceptual quality for all image sets. This suggests that there is still room for future improvement, and proper combination of the ideas used in different MEF algorithms has the potential to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance of Existing Objective IQA Models</head><p>Using the above database, we test the performance of 9 existing objective IQA models for image fusion. Since the source sequences in the database consist of at least 3 input images, models that can only work with the case of 2 input images <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> are excluded from the comparison. For the purpose of fairness, all models are tested using their default parameter settings. Note that to obtain a reasonable result, we take the absolute value of the objective score in <ref type="bibr" target="#b20">[21]</ref>. Three implementations of the algorithm in <ref type="bibr" target="#b21">[22]</ref> were proposed and the one with the best performance is reported here.</p><p>Tables II and III summarize the evaluation results, which is somewhat disappointing because state-of-the-art IQA models do not seem to provide adequate predictions of perceived quality of fused images. Even the models with the best performance, such as Xydeas's <ref type="bibr" target="#b18">[19]</ref> and Wang's <ref type="bibr" target="#b19">[20]</ref> methods, are only moderately correlated with subjective scores.</p><p>The above test results also provide some useful insights regarding the general approaches used in IQA models. First, models based on entropy computations of pixel intensity values and transform coefficients <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> have poor correlation with perceptual quality. The reason may be that the quality of fused images is highly content dependent and only entropy of image intensity/coefficient histogram is insufficient in capturing the perceptual distortions introduced by MEF processes. Second, local structure-preservation based models, such as SSIM and gradient based approaches applied in spatial or transform domain <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>, provide the most promising results so far. However, they are often unsuccessful in capturing the degradations of luminance consistency across the image space. This suggests that more accurate objective IQA models may be developed by achieving a good balance between assessing local structure preservation and evaluating large-scale luminance consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OBJECTIVE QUALITY ASSESSMENT OF MEF IMAGES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proposed Objective Quality Assessment Model</head><p>Following the general construction of SSIM <ref type="bibr" target="#b26">[27]</ref>, we first examine how the information in the multi-exposure image sequence is preserved in the fused image at each spatial location. Direct use of the SSIM algorithm <ref type="bibr" target="#b26">[27]</ref>, however, is impossible, which requires a single perfect quality reference image. How to work with multiple input images in MEF is </p><formula xml:id="formula_2">} = {x k |1 ≤ k ≤ K } denote</formula><p>the set of image patches extracted from the same spatial location in the source image sequence of K multi-exposure images, and let y be the corresponding patch in the fused image, respectively. Here x k for all k and y are N dimensional column vectors, where N is the number of pixels in the patch and each entry is given by the intensity value of a pixel in the patch. Given the patch set {x k } as a reference, the goal here is to define a quality measure of the fused image patch y.</p><p>A useful approach we learn from the SSIM approach is to look at an image patch from three separate aspects: luminance, contrast and structure. An easy approach to implement this is to decompose any given image patch into three components:</p><formula xml:id="formula_3">x k = x k -μ x k • x k -μ x k x k -μ x k + μ x k = xk • xk xk + μ x k = c k • s k + l k , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where • denotes the l 2 norm of a vector, μ x k is the mean value of the patch, and xk = x k -μ x k is a mean-removed patch, or a zero-mean patch that contains the contrast and structure information only. The scalar l k = μ x k , the scalar c k = xk , and the unit-length vector s k = xk / xk roughly represent the luminance, contrast and structure components of x k , respectively. In the case of MEF, direct preservation of the luminance of the local source image patches (that are all somewhat under-or over-exposed) is of low relevance with regard to the overall image quality, and thus we exclude it from the local patch comparison described below. This differentiates our model from other SSIM-based quality models such as <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, and the more critical differences are in the way we work with the contrast and structure components of multiple source images.</p><p>The visibility of the local patch structure largely depends on local contrast. On one hand, the higher the contrast, the better the visibility. On the other hand, too large contrast may lead to unrealistic representation of the local structure. Considering all input source image patches as realistic capturing of the scene, the patch that has the highest contrast among them would correspond to the best visibility under the realisticity constraint. Therefore, the desired contrast of the fused image patch is determined by the highest contrast of all source image patches:</p><formula xml:id="formula_5">ĉ = max {1≤k≤K } c k = max {1≤k≤K } xk .</formula><p>(</p><p>Different from contrast, the structures of local image patches are denoted by unit-length vectors s k for 1 ≤ k ≤ K , each of which points to a different direction in the vector space. The desired structure of the fused image patch corresponds to another direction in the same vector space that best represents the structures of all source image patches. A simple model to account for this relationship is given by</p><formula xml:id="formula_7">s = K k=1 w (x k ) s k K k=1 w (x k ) and ŝ = s s , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where w(•) is a weighting function that determines the contribution of each source image patch in the structure of the fused image patch. Intuitively, the contribution should increase with the strength of the image patch. A straightforward approach that conforms with such intuition is to employ a power weighting function given by</p><formula xml:id="formula_9">w(x k ) = xk p , (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>where p ≥ 0 is an exponent parameter. With various choices of the value of p, this general formulation leads to a family of weighting functions with different physical meanings. The larger the p value, the more emphasis is put on the patches that have relatively larger strength. Specifically, p = 0 corresponds to straightforward direction average (where low and high contrast source patches are accounted for equally); p = 1 corresponds to length-weighted direction average; p = 2 corresponds to energy-weighted direction average; and p = ∞ corresponds to picking the direction corresponding to the patch that has the largest vector length among all patches. It remains to determine the value of p. Instead of fixing p to be a constant, here we propose an automatic approach that chooses p at each spatial location adaptively. The motivation is to adjust the relative weighting factors in (4) based on the consistency between the structures of the source image patches. To implement the idea, we first need to quantify the consistency between the set of structural vectors {x k }. Each of the vectors points to a specific direction in the vector space. In the extreme case, when one vector is a contrast enhanced variation of another (i.e., there is no structural changes between the vectors), both vectors will point to the same direction. Therefore, we define a structure consistency measure between a set of vectors based on the degree of direction agreement between them. In particular, we compute</p><formula xml:id="formula_11">R({x k }) = K k=1 xk K k=1 xk . (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>By triangular inequality, 0 ≤ R ≤ 1, and a lager R indicates stronger consistency between the set of vectors. To better understand this, two examples are illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>. The sum of a set of source patch vectors corresponds to connecting the starts and ends of these vectors one by one, which creates a new vector x = K k=1 xk pointing from the origin to the end of the last patch vector. The length of the new vector x (the numerator of Eq. ( <ref type="formula" target="#formula_11">6</ref>)) is usually smaller than adding the length of all patch vectors together (the denominator of Eq. ( <ref type="formula" target="#formula_11">6</ref>)), but when all the source patch vectors point to exactly the same direction, these two quantities are equal, leading to the maximal possible value of R = 1. Fig. <ref type="figure" target="#fig_4">5</ref>(a) and 5(b) are two examples showing the cases of stronger (larger R) and weaker (smaller R) patch structure consistencies, respectively. When the value of R is small, the multiple source patches convey different structures with similar strength, and thus it is more appropriate to assign them similar weights, resulting in a smaller desired p value. On the other hand, when R is large and the structures of the patches are similar, a stronger patch would have higher contrast and be more resistent to distortions such as noise, and thus should be given a higher weight, leading to a larger desired p value. We propose an empirical approach to account for this, which is given by</p><formula xml:id="formula_13">p = tan π R 2 . (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>In the extreme case when all patches agree with each other (R = 1), a value of p = ∞ is chosen, while at the other extreme when there is no consistency in patch structures (R = 0), p = 0 is selected.</p><p>Once the value of p is determined at each spatial location, Eq. ( <ref type="formula" target="#formula_7">4</ref>) is employed to compute ŝ, which is subsequently combined with ĉ in Eq. ( <ref type="formula" target="#formula_6">3</ref>) to yield a new vector</p><formula xml:id="formula_15">x = ĉ • ŝ.<label>(8)</label></formula><p>Following the construction of the SSIM approach <ref type="bibr" target="#b26">[27]</ref>, we use a simplified definition of the SSIM index to evaluate local image quality:</p><formula xml:id="formula_16">S({x k }, y) = 2σ xy + C σ 2 x + σ 2 y + C , (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>where σ 2 x , σ 2 y and σ xy denote the local variances of x and y, and the local covariance between x and y, respectively. C is a small positive stabilizing constant that accounts for the saturation effects of the visual system at low contrast <ref type="bibr" target="#b26">[27]</ref>. Note that since luminance is not considered a highly relevant component in the current scenario, the luminance term is not included in the above structural comparison assessment.  The local structure comparison of Eq. ( <ref type="formula" target="#formula_16">9</ref>) is applied using a window across the entire image, resulting in a quality map indicating how the structural details are preserved at each spatial location. Fig. <ref type="figure" target="#fig_6">7</ref> provides a visual demonstration of the quality maps of two fused images created from the same source sequence, where brighter regions in the maps indicate better structure preservation. It appears that the quality maps accurately predict local structure preservations in the fused images. For instance, Fig. <ref type="figure" target="#fig_6">7</ref>(a) generated by Mertens' algorithm <ref type="bibr" target="#b4">[5]</ref> fails to preserve the fine details in the tower and the brightest cloud region in the left part of the image. Such detail loss is well reflected in the corresponding quality maps. By contrast, Fig. <ref type="figure" target="#fig_6">7(c</ref>) computed by the local energy weighting algorithm creates unnatural artifacts near the strong edges of the Tower. Such structural distortion is again clearly indicated in the corresponding structural quality maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MODEL IS STATISTICALLY BETTER THAN THAT OF THE COLUMN MODEL, A SYMBOL "0" MEANS THAT THE ROW MODEL IS STATISTICALLY WORSE, AND A SYMBOL "-" MEANS THAT THE ROW AND COLUMN MODELS ARE STATISTICALLY INDISTINGUISHABLE</head><p>The quality map is averaged to obtain an overall structural quality measure of the fused image</p><formula xml:id="formula_18">Q(Y) = 1 M M j =1 S({x k }( j ), y( j )), (<label>10</label></formula><formula xml:id="formula_19">)</formula><p>where j is the spatial patch index and M is the total number of patches.</p><p>Based on the subjective test results, analysis and discussions presented in Section III, an ideal quality MEF image should not only preserve as much as possible the local structural details in the source sequence, but also achieve good luminance consistency across space. Although it is difficult to directly determine the exact luminance value at each spatial location in the fused image, the relative luminance/brighness values of different regions across space in the source input images provide useful references. For example, Fig. <ref type="figure" target="#fig_1">2</ref>(a1)-(a3) show the three input images of the source sequence "Venice", where the absolute local luminance of the sky, the buildings and the water areas in the right part of the image is difficult to decide, but their relative luminance values are apparentthe sky is brighter than the buildings, and the buildings are brighter than the water area. Such larger-scale relative luminance structure cannot be properly captured if only a local windowing approach like the one described above is adopted. Instead, the image content needs to be observed in larger windows or at coarser scales. The principle behind the multiscale SSIM (MS-SSIM) approach <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> provides a natural fit to what we need, where fine-scale structure comparisons capture local structural detail loss and distortions, while at the coarse scales, the same comparisons reveal the consistency of large-range luminance patterns. As illustrated in Fig. <ref type="figure" target="#fig_5">6</ref>, we iteratively apply a low-pass filter followed by downsampling the filtered images by a factor of 2. We index the original scale of the image or the finest scale as Scale 1, and the coarser scale after l -1 iterations as Scale l. At the l-th scale, the structure comparison ( <ref type="formula" target="#formula_16">9</ref>) is conducted and denoted as S l (x, y). When this is computed for all scales, a set of multi-scale quality maps are obtained, as exemplified in Fig. <ref type="figure" target="#fig_6">7</ref>. By pooling the quality map at each scale using Eq. ( <ref type="formula" target="#formula_18">10</ref>), we obtain a set of scale-level quality scores {Q l (Y)}. The overall quality is then computed by combining scale-level quality scores using a similar method as in <ref type="bibr" target="#b27">[28]</ref>:</p><formula xml:id="formula_20">Q(Y) = L l=1 [Q l (Y)] β l , (<label>11</label></formula><formula xml:id="formula_21">)</formula><p>where L is the total number of scales and β l is the weight assigned to the l-th scale. The proposed model does not involve any training process or introduce any new parameter. All parameters are inherited from previous publications. These include C = (0.03D) 2 from <ref type="bibr" target="#b26">[27]</ref>, where D is the dynamic range of intensity values (For 8 bits/pixel gray-scale images, D = 255), L = 3, and the normalized fine-to-coarse scale weights are given by {β 1 , β 2 , β 3 } = {0.0710, 0.4530, 0.4760} <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Validation</head><p>We validate the proposed objective model using the database presented in Section III and compare its performance against the nine objective IQA models for image fusion described in Section II. Tables II and III summarize the PLCC and SRCC evaluation results, where we have also included the performance of an average subject (as discussed in Section III). It can be seen that the proposed method delivers the best performance in predicting subjective quality of fused images on almost every set of test images. On average, it not only performs much better than existing IQA models (which are designed for quality assessment of general image fusion), but also outperforms an average subject.</p><p>To ascertain that the improvement of the proposed model is statistically significant, we carried out a statistical significance analysis by following the approach introduced in <ref type="bibr" target="#b35">[36]</ref>. First, a nonlinear regression function is applied to map the objective quality scores to predict the subjective scores. We observe that the prediction residuals all have zero-mean, and thus the model with lower variance is generally considered better than the one with higher variance. We conduct a hypothesis testing using F-statistics. Since the number of samples exceeds 40, the Gaussian assumption of the residuals approximately hold based on the central limit theorem <ref type="bibr" target="#b36">[37]</ref>. The test statistic is the ratio of variances. The null hypothesis is that the prediction residuals from one quality model come from the same distribution and are statistically indistinguishable (with 95% confidence) from the residuals from another model. After comparing every possible pairs of objective models, the results are summarized in Table <ref type="table" target="#tab_2">IV</ref>, where a symbol "1" means the row model performs significantly better than the column model, a symbol "0" means the opposite, and a symbol "-" indicates that the row and column models are statistically indistinguishable. It can be observed that most existing IQA models for image fusion are statistically indistinguishable from each other, while the proposed model is statistically better than all other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Potential Application: Automatic Parameter Tuning of MEF Algorithms</head><p>The application scope of objective IQA models is much broader than performance assessment and comparison of MEF algorithms. A more interesting application area is to use them to guide the design and optimization of novel MEF algorithms. Here we use automatic parameter tuning of MEF models as an example to demonstrate the potential values of the proposed IQA model.</p><p>Many MEF algorithms include one or more free parameters whose best values largely depend on the image content. In practice, these values often need to be selected manually, which is difficult, inconvenient, and time-consuming. Objective quality models that can automatically tune the parameters in MEF algorithms are highly desirable, especially when the volume of images being processed is large. To demonstrate the usefulness of the proposed method, here we apply the proposed model to automatically tune the parameters of the MEF algorithm proposed in <ref type="bibr" target="#b10">[11]</ref>.</p><p>The algorithm in <ref type="bibr" target="#b10">[11]</ref> extracts local contrast and brightness for static scenes and color dissimilarity for dynamic scenes to compute the initial weighting map. A recursive filter is then employed to smooth the map while preserving the edge information. The space and range supports of the recursive filter are controlled by two parameters σ s and σ r . The default values of σ s and σ r are chosen under the guidance of the model in <ref type="bibr" target="#b18">[19]</ref>, which results in a fixed set of values given by σ s = 100 and σ r = 4. In Fig. <ref type="figure" target="#fig_7">8</ref>, we plot the quality score of the proposed model as a function of σ s and σ r for the "Cave" sequence fused by the chosen algorithm <ref type="bibr" target="#b10">[11]</ref>, where higher surface in the plot indicates a higher quality of the fused image in terms of the proposed model. From the figure, we have several interesting observations. First, the perceptual quality of the fused image varies significantly with σ s and σ r when they are relatively small. Second, careful inspections and comparisons of the fused images together with their corresponding quality scores suggest that the proposed model is a good perceptual quality indicator and provides a useful tool to automatically choose the optimal values of σ s and σ r . Third, the upperright image corresponds to the default parameters chosen by the model in <ref type="bibr" target="#b18">[19]</ref>, while the lower-right image corresponds to the optimal parameter values chosen by the proposed model. Comparing the two images, we can see that the best quality image indicated by the proposed model better preserves the fine detail and color information of the tree trunks and the rocks in the middle part of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND DISCUSSION</head><p>In this work, we tackle the problem of perceptual quality assessment of MEF images. Our major contributions are threefold. First, we construct a database dedicated to MEF IQA, which is perhaps the first of its kind that contains a wide variety of image content and fused images created by different kinds of MEF algorithms. We carry out subjective experiment and data analysis on the database, which lead to three useful findings: 1) although the behaviors of individual subjects varies, there is generally a considerable agreement between them on the quality of fused images; 2) there is still room for designing better MEF algorithms that are more robust to the variation of image content; 3) state-of-the-art IQA models for general image fusion applications do not seem to provide adequate predictions of perceived quality of fused images. Second, we propose an objective IQA model for MEF images based on the multiscale SSIM principle <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> and a novel measure of structural patch consistency. The proposed model measures the local structure preservation at the fine scales and also captures luminance consistency at coarser scales. Experiments on the benchmark database show that the proposed model significantly outperforms existing IQA models designed for image fusion. Third, we demonstrate the usefulness of the proposed model by applying it to automatic parameter tuning of a state-of-the-art MEF algorithm and obtained promising initial results.</p><p>For future research, MEF IQA still has many challenging and interesting problems to be solved. First, the proposed metric is mainly based on the principle of structural similarity. Other principles that have been successfully used in full-reference IQA may be exploited in the context of MEF. These include information theoretic and natural scene statistical approaches <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref>, the adaptive linear system decomposition framework <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, the feature similarity method <ref type="bibr" target="#b43">[44]</ref>, and visual attention and saliency-based approaches <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b46">[47]</ref>. Second, most existing objective quality models, including the proposed one, work with the luminance component only. Proper accounting for color distortions have great potentials to improve the performance of the objective quality model <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Third, all source sequences that constitute the subjective database are nearly static, but the natural scenes we encounter in practice are often dynamic and contain moving objects <ref type="bibr" target="#b48">[49]</ref>. It is useful to generalize the proposed model to account for dynamic scenes. Fourth, how to integrate the quality model into consumer electronics to capture high quality fused image in real-time is another challenging problem yet to be explored.</p><p>Besides quality assessment of MEF images, objective quality models for other image fusion applications are largely lacking. These include images fused from multi-focus images in photography applications <ref type="bibr" target="#b49">[50]</ref>; images fused from hyperspectral images in remote sensing applications; and images generated by merging different imaging modalities in medical imaging applications. They are related to MEF IQA but also have fundamentally interesting differences. It is interesting to explore the common characteristics shared by these applications as well as the unique features of each individual application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Input source image sequences contained in the database. Each image sequence is represented by one image, which is a fused image of the sequence that has the best quality in the subjective test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of multi-exposure source image sequence (a1, a2, a3) and fused images (b)-(i) created by different MEF algorithms.</figDesc><graphic coords="4,56.99,421.73,164.66,109.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.PLCC and SRCC between individual subject rating and MOS. Rightmost column: performance of an average subject.</figDesc><graphic coords="5,61.60,57.69,225.78,294.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Mean and std of subjective rankings of individual image fusion algorithms across all image sets.</figDesc><graphic coords="5,56.07,397.95,236.43,151.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of structure consistency measure in the vector space. The vectors in (a) have much stronger structure consistency than those in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Diagram of multi-scale structure comparison. LPF: low-pass filtering; 2 ↓: downsampling by a factor of 2.</figDesc><graphic coords="8,112.43,175.61,132.26,198.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples of multi-scale structural quality maps. (a) Fused image created by Mertens' algorithm [5]. (b) The structural quality maps of (a). (c) Fused image created by local energy weighting. (d) The structural quality maps of (c). In (b) and (d), higher brightness indicates better quality.</figDesc><graphic coords="8,112.43,393.53,132.26,198.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. "Cave" image sequence fused by Li's method<ref type="bibr" target="#b10">[11]</ref> over a wide range of σ s and σ r values. Higher surface in the middle plot indicates a higher value of the proposed objective quality model, suggesting better quality of the fused image.</figDesc><graphic coords="10,57.11,171.05,134.30,100.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I INFORMATION</head><label>I</label><figDesc>ABOUT SOURCE INPUT IMAGE SEQUENCES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV STATISTICAL</head><label>IV</label><figDesc>SIGNIFICANCE MATRIX BASED ON QUALITY PREDICTION RESIDUALS. A SYMBOL "1" MEANS THAT THE PERFORMANCE OF THE ROW</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The subjective database and the MATLAB code of the proposed model will be made available online. Preliminary results of Section III were presented at the 6th International Workshop on Quality of Multimedia Experience, Singapore,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2014" xml:id="foot_1"><p></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Dr. Z. Liu for providing the codes of <ref type="bibr" target="#b28">[29]</ref>, Dr. B. Gu and Dr. Z. Li for generating fused images of their algorithms.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Myszkowski</surname></persName>
		</author>
		<title level="m">High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting</title>
		<meeting><address><addrLine>San Mateo, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The pyramid as a structure for efficient computation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiresolution Image Processing and Analysis</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhanced image capture through fusion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kolczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th IEEE Int. Conf. Comput. Vis</title>
		<meeting>4th IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fusion of multi-exposure images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Goshtasby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="611" to="618" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exposure fusion: A simple and practical alternative to high dynamic range photography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilateral filter based compositing for variable exposure photography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics</title>
		<meeting>Eurographics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic exposure fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="341" to="357" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-directed multiexposure composition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2318" to="2323" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient field multi-exposure images fusion for high dynamic range image visualization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="604" to="610" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detail-enhanced exposure fusion</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahardja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4672" to="4676" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast multi-exposure image fusion with median filter and recursive filter</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consum. Electron</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="626" to="632" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image fusion with guided filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2013-07">Jul. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">QoE-based multi-exposure fusion in hierarchical multivariate Gaussian CRF</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2469" to="2478" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of different image fusion schemes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Franken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Displays</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="37" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Subjective tests for image fusion evaluation and objective metric validation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="216" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of multi-exposure image fusion algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Workshop Quality Multimedia Exper</title>
		<meeting>6th Int. Workshop Quality Multimedia Exper</meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Information measure for performance of image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="313" to="315" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Objective pixel-level image fusion performance measure</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Xydeas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Petrovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Sensor Fusion, Archit</title>
		<meeting>SPIE, Sensor Fusion, Archit</meeting>
		<imprint>
			<date type="published" when="2000-04">Apr. 2000</date>
			<biblScope unit="volume">4051</biblScope>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A novel image fusion metric based on multi-scale analysis</title>
		<author>
			<persName><forename type="first">P.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 9th Int. Conf. Signal Process</title>
		<meeting>IEEE 9th Int. Conf. Signal ess</meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="965" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new metric based on extended spatial frequency and its application to DWT based fusion algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Essock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Haun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="192" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new quality metric for image fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heijmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2003-09">Sep. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A similarity metric for assessment of image fusion algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cvejic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Canagarajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Signal Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="178" to="182" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel similarity based quality metric for image fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="160" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A human perception inspired quality metric for image fusion based on regional information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="207" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new automated quality assessment algorithm for image fusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1421" to="1432" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th IEEE Asilomar Conf. Signals, Syst., Comput</title>
		<meeting>37th IEEE Asilomar Conf. Signals, Syst., Comput</meeting>
		<imprint>
			<date type="published" when="2003-11">Nov. 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Objective assessment of multiresolution image fusion algorithms for context enhancement in night vision: A comparative study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laganiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="109" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A universal image quality index</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Methodology for the Subjective Assessment of the Quality of Television Pictures, document Rec. ITU-R BT</title>
		<imprint>
			<date type="published" when="1993-11">Nov. 1993</date>
			<biblScope unit="volume">500</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><surname>Vqeg</surname></persName>
		</author>
		<ptr target="http://www.vqeg.org" />
		<title level="m">Final Report From the Video Quality Experts Group on the Validation of Objective Models of Video Quality Assessment</title>
		<imprint>
			<date type="published" when="2000-04">Apr. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comments on &apos;information measure for performance of image fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Creighton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1066" to="1067" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image fusion metric based on mutual information and Tsallis entropy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cvejic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Canagarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="626" to="627" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance evaluation of image fusion techniques</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Fusion: Algorithms and Applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="469" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Applied Statistics and Probability for Engineers, 6th ed</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Montgomery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image information and visual quality</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="444" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Information content weighting for perceptual image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1185" to="1198" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Objective quality assessment of tone-mapped images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="657" to="667" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An adaptive linear system framework for image distortion analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno>III-1160-III-1163</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2005-09">Sep. 2005</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Quantifying color image distortions based on adaptive spatio-chromatic signal decompositions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th IEEE Int. Conf. Image Process</title>
		<meeting>16th IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="2213" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contextually adaptive signal representation using conditional principal component analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Figueras I Ventura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<date type="published" when="2008-04">Mar./Apr. 2008</date>
			<biblScope unit="page" from="877" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial pooling strategies for perceptual image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2006-10">Oct. 2006</date>
			<biblScope unit="page" from="2945" to="2948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visual importance pooling for image quality assessment</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">VSI: A visual saliency-induced index for perceptual image quality assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4270" to="4281" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of color images using adaptive signal representation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Human Vis</title>
		<meeting>SPIE, Human Vis</meeting>
		<imprint>
			<date type="published" when="2010-02">Feb. 2010</date>
			<biblScope unit="volume">7527</biblScope>
			<biblScope unit="page">75271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Selectively detail-enhanced fusion of differently exposed images with moving objects</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4372" to="4382" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Objective quality assessment for multiexposure multifocus image fusion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Salama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2712" to="2724" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
