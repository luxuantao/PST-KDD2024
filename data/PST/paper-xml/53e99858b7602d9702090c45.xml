<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Compressibility of N P Instances and Cryptographic Applications ‡</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Danny</forename><surname>Harnik</surname></persName>
							<email>harnik@cs.technion</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<settlement>Technion, Haifa</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
							<email>moni.naor@weizmann.ac.il.</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution" key="instit1">Judith Kleeman Professorial Chair</orgName>
								<orgName type="institution" key="instit2">Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Compressibility of N P Instances and Cryptographic Applications ‡</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D5CAB25E99001EBA1969A08801F0A587</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study compression that preserves the solution to an instance of a problem rather than preserving the instance itself. Our focus is on the compressibility of N P decision problems. We consider N P problems that have long instances but relatively short witnesses. The question is, whether one can efficiently compress an instance and store a shorter representation that maintains the information of whether the original input is in the language or not. We want the length of the compressed instance to be polynomial in the length of the witness and polylog in the length of original input. We discuss the differences between this notion and similar notions from parameterized complexity. Such compression enables succinctly storing instances until a future setting will allow solving them, either via a technological or algorithmic breakthrough or simply until enough time has elapsed.</p><p>We give a new classification of N P with respect to compression. This classification forms a stratification of N P that we call the VC hierarchy. The hierarchy is based on a new type of reduction called W-reduction and there are compression-complete problems for each class.</p><p>Our motivation for studying this issue stems from the vast cryptographic implications of compressibility. We describe these applications, for example, based on the compressibility of SAT. We say that SAT is compressible if there exists a polynomial p(•, •) so that given a formula consisting of m clauses over n variables it is possible to come up with an equivalent (w.r.t satisfiability) formula of size at most p(n, log m). Then given a compression algorithm for SAT we provide a construction of collisionresistant hash functions from any one-way function. This task was shown to be impossible via black-box reductions [77], and indeed our construction is inherently non-black-box. A second application of a compression algorithm for SAT is a construction of a one-way function from any samplable distribution of N P instances that is hard on the average. Using the terminology of Impagliazzo <ref type="bibr" target="#b48">[49]</ref>, this would imply that Pessiland=Minicrypt. Another application of SAT compressibility is a cryptanalytic result concerning the limitation of everlasting security in the bounded storage model when mixed with (time) complexity based cryptography. In addition, we study an approach to constructing an Oblivious Transfer Protocol from any one-way function. This approach is based on compression for SAT that also has a property that we call witness-retrievability. However, we manage to prove severe limitations on the ability to achieve witness-retrievable compression of SAT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In order to deal with difficult computational problems several well-established options were developed, including: approximation algorithms, subexponential algorithms, parametric complexity and average-case complexity. In this paper we explore our favorite approach for dealing with problems: postpone them (hopefully without cluttering our desk or disk). We initiate the study of the compressibility of N P problems for their resolution in some future setting and in particular the cryptographic significance of such compression. Rather than solving a given instance, we ask whether a shorter instance with the same solution can be found efficiently. We emphasize that we are not interested in maintaining the information about the original instance (as is the case in typical notions of compression), but rather maintaining the solution only. The solution can possibly be much shorter than the input (as short as a yes/no answer), thus the potential of such a compression is high.</p><p>While the question of compressibility is interesting with respect to problems both inside and out of N P, our focus is mostly on a special case, that of N P problems that have long instances but relatively short witnesses. An N P language L is defined by an efficiently computable relation R L such that an input (or instance) x is in L if and only if there exists a witness w such that R L (x, w) = 1. Throughout the paper, an N P instance is characterized by two parameters m and n: The length of the instance x is denoted by m and the length of the witness w is denoted by n. The problems of interest are those having relatively short witnesses, i.e. n m, but not too short (m 2 n ). Traditionally, the study of N P languages evolves around the ability or inability to efficiently decide if an instance is in the language or not, or to find a witness w for an instance x ∈ L within polynomial time. We introduce the question of compressibility of such instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example of Compressing SAT Instances:</head><p>To illustrate the relevant setting, we use the well known example of SAT. An instance Φ for SAT consists of a CNF formula over n variables and we define that Φ ∈ SAT if there exists an assignment to the n variables that satisfies all the clauses of Φ. We begin with compressibility with respect to decision, and discuss the search variant of compressibility later in the paper. In this example we consider the question of compressibility of SAT instances to shorter SAT instances<ref type="foot" target="#foot_0">1</ref> :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 1.1 (Compression of SAT instances)</head><p>Does there exist an efficient algorithm and a polynomial p(•, •) with the following input and output? Input: A CNF formula Φ with m clauses over n variables. Output: A formula Ψ of size p(n, log m) such that Ψ is satisfiable if and only if Φ is satisfiable.</p><p>The idea is that the length of Ψ should not be related to the original length m, but rather to the number of variables (or in other words, to the size of the witness). Typically, we think of the parameters m and n as related by some function, and it is instructive (but not essential) to think of m as larger than any polynomial in n. So potentially, the length of Ψ can be significantly shorter than that of Φ. <ref type="foot" target="#foot_1">2</ref>In general, one cannot expect to compress all the formulas, or else we would have an efficient algorithm for all N P problems. 3 However, once we restrict the attention to the case of a shorter witness, then compression becomes plausible. Note that if P = N P then compression becomes trivial, simply by solving the satisfiability of Φ and outputting 1 if Φ ∈ SAT and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation for Compression:</head><p>Compressing for the future is an appealing notion for various settings. There are numerous plausible scenarios that will give us more power to solve problems in the future. We could potentially find out that P = N P and solve all our N P problems then. We may have faster computers or better means of computing such as quantum computers or some other physical method for solving problems (see Aaronson <ref type="bibr" target="#b0">[1]</ref> for a list of suggestions). Above all, the future entails lots and lots of time, a resource that is usually scarce in the present. Saving the problems of today as they are presented is wasteful, and compression of problems will allow us to store a far greater number of problems for better days.</p><p>Our interest in studying the issue of compression stems from the vast cryptographic implications of compressibility. We demonstrate three questions in cryptography that compression algorithms would resolve (see <ref type="bibr">Section 1.3)</ref>. We are confident that the notion of compressibility will be found to have further applications both within and outside of cryptography. For example, in subsequent works, Dubrov and Ishai <ref type="bibr" target="#b25">[26]</ref> show the relevance of the notion of compression to derandomization and Dziembowski <ref type="bibr" target="#b27">[28]</ref> shows that compression is related to the study of forward-secure storage (see Section 1.4 on related work). We note that a notion similar to compression has been useful (and well studied) in the context of parameterized complexity (see a comparison and discussion in Section 1.4). The concept of compression of problems is also interesting beyond the confines of N P problems, and makes sense in any setting where the compression requires much less resources than the actual solution of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Compression of NP instances</head><p>We define the notion of compression with respect to an N P language L. We associate with L a specific fixed N P relation R L that defines it (as mentioned above) as well as a function n(x) that defines an upper bound on the length of a potential witness for an instance x. 4 At times, for simplicity, we abuse notations and simply refer to the language L and omit the reference to the underlying relation R L . In essence, a compression algorithm is a specialized Karp-reduction that also reduces the length of the instance. Definition 1.2 (Compression Algorithm for N P Instances) Let L = (R L , n(•)) be an N P language. Denote by m and n the instance length and the witness length respectively. A compression algorithm for L is a polynomial-time machine Z along with a language L and a polynomial p(•, •) such that for all large enough m:</p><p>1. For all x ∈ {0, 1} m with parameter n the length of Z(x) is at most p(n, log m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Z(x) ∈ L if and only if x ∈ L</head><p>The above definition is of an errorless compression. We also consider a probabilistic variant called εcompression for some real function ε : N → [0, 1]. The probabilistic definition is identical to the errorless one except that Z is a probabilistic polynomial-time machine and the second property is augmented to: 2'. For large enough n, for all x ∈ {0, 1} m with parameter n it holds that:</p><formula xml:id="formula_0">Pr[(Z(x) ∈ L ) ⇔ (x ∈ L)] ≥ 1 -ε(n)</formula><p>where probability is over the internal randomness of Z. By default we require ε(•) to be negligible (i.e., ε(n) = n -ω (1) ). 5   The paper consists of two parts: Part I is a study of the concept of compression of N P instances from a complexity point of view. Part II introduces the cryptographic applications of compression algorithms.</p><p>How much to compress: Definition 1.2 (of compression algorithms) requires a very strong compression, asking that the length of the compression be polynomial in n and log m. For the purposes of part I of the paper (the complexity study), it is essential that the length of the compression be at least sub-polynomial in m in order to ensure that the reductions defined with respect to compressibility (See Section 2.2) do compose. For clarity we choose a polynomial in log m, although this may be replaced by any sub-polynomial function m (.) (i.e., m = m o (1) ). We note that in many natural cases one can assume that n ≥ log m and then one can replace the polynomial p(n, log m) in Definition 1.2 by a polynomial in n alone (in Sections 1.4 and 2.1 we compare this definition to the notion of polynomial kernelization). However, we choose not to restrict the scope of our discussion by making this assumption. Moreover, for part II (the applications) Definition 1.2 may be significantly relaxed, where even a compression to length m 1-ε for some constant ε suffices for some applications.</p><p>The Complexity of L : In Definition 1.2 there is no restriction on the complexity of the language L . All that is required is that there is enough information in Z(x) to determine whether x ∈ L or not. However, it is worth noting that if the compression is errorless then the language L must be in a class of nondeterministictime poly(m) that we denote N P(poly(m)). That is, languages that are verifiable in time poly(m) when given a non-deterministic hint (in order for poly(m) to be well defined we assume that the parameter m is also encoded in the instance Z(x)). This fact follows simply from the definition of compression. <ref type="foot" target="#foot_3">6</ref> In some cases it is essential to restrict L to be in N P(poly(m)), such as when defining the witness-retrievability property (Definition 1.6). Moreover, in some cases it is natural to further restrict L to actually be in N P (that is in N P(poly(n, log m)). For instance, this is the case in the example for compression of SAT (Example 1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper organization:</head><p>In the rest of the introduction we survey the results of this paper, including part I (the complexity study) and part II (the cryptographic applications). In section 1.4 we discuss related and subsequent works. The main complexity study of the compressibility of N P problems appears in Section 2. The Cryptographic applications are in Sections 3,5 and 6. In Section 3 we describe the application of compression to constructing collision-resistant hash functions (CRH) from any one-way function. Section 5 presents the implication to the hybrid bounded storage model, while Section 6 discusses witness-retrievable compression and its application to the construction of oblivious transfer (OT) from any one-way function. We conclude with a discussion and some open problems (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Part I: Classifying N P Problems with Respect to Compression</head><p>We are interested in figuring out which N P languages are compressible and, in particular, whether important languages such as SAT and Clique are compressible. For starters, we demonstrate some non-trivial languages that do admit compression (Section 2.1): we show compression for the well-known N P-complete problem of vertex cover and for another N P-complete language known as minimum-fill-in. We show a generic compression of sparse languages (languages containing relatively few words from all possible instances). As specific examples we mention the language consisting of strings that are the output of a cryptographic pseudorandom generator and also consider the sparse subset sum problem. In addition we show compression for the promise problem GapSAT. <ref type="foot" target="#foot_4">7</ref> However, these examples are limited and do not shed light on the general compression of other N P problems. Moreover, it becomes clear that the traditional notions of reductions and completeness in N P do not apply for the case of compression (i.e., the compression of an N P-complete language does not immediately imply compression for all of N P). This is not surprising, since this is also the case with other approaches for dealing with N P-hardness such as approximation algorithms or subexponential algorithms (see for example <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b50">51]</ref>) and parameterized complexity (see <ref type="bibr" target="#b23">[24]</ref> and further discussion in Section 1.4 on related work). For each of these approaches, appropriate new reductions were developed, none of which is directly relevant to our notion of compression.</p><p>We introduce W-reductions in order to study the possibility of compressing various problems in N P. These are reductions that address the length of the witness in addition to membership in an N P language. Wreductions have the desired property that if L W-reduces to L , then any compression algorithm for L yields a compression algorithm for L. Following the definition of W-reductions we define also the corresponding notion of compression-complete and compression-hard languages for a class.</p><p>The VC classification: We introduce a classification of N P problems with respect to compression. The classification presents a structured hierarchy of N P problems, that is surprisingly different from the traditional view and closer in nature to the W hierarchy of parameterized complexity (see <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b33">[34]</ref>). We call our hierarchy VC, short for "verification classes", since the classification is closely related to the verification algorithm of N P languages when allowed a preprocessing stage. We give here a very loose description of the classes, just in order to convey the flavor of the classification. Formal definitions appear in Section 2.3. In the following definition, when we use the term "verification" we actually mean "verification with preprocessing":</p><p>• For k ≥ 2, the class VC k is the class of languages that have verification that can be presented as a depth k circuit of unbounded fan-in and polynomial size (polynomial in n and m). For example, the language SAT is compression-complete for the class VC 2 . Other examples include Integer-Programming which resides in VC log n and Dominating-Set which is in VC 3 . Both of these are shown to be compression-hard for VC 2 .</p><p>• VC 1 is the class of languages that have local verification. That is, languages that can be verified by testing only a small part (of size poly(n, log m)) of the instance. This class contains many natural examples such as the Clique language or Long-path.</p><p>• VC OR is the class of languages that have verification that can be presented as the OR of m small instances of SAT (each of size n). This class contains the languages that are relevant for the cryptographic applications. The Clique language is compression-hard for this class (Claim 2.23).</p><p>• VC 0 is the class of compressible languages. In particular it includes vertex cover, sparse languages and GapSAT.</p><p>We show that the classes described form a hierarchy (see Lemma 2.17 and Claim 2.22). That is:</p><formula xml:id="formula_1">VC 0 ⊆ VC OR ⊆ VC 1 ⊆ VC 2 ⊆ VC 3 . . .</formula><p>We discuss some of the more interesting classes in the VC hierarchy, classify some central N P problems and mention compression-complete problems for the classes. The existence of a compression algorithm for a complete problem for some class entails the collapse of the hierarchy up to that class into VC 0 .</p><p>In addition, we study the compression of N P search problems. That is, compressing an instance in a way that maintains all the information about a witness for the problem. We show that the compression of a class of decision problems also implies compression for the corresponding search problems. Formally: Theorem 1.3 If a class VC k has a compression algorithm, then for any L ∈ VC k there is a compression algorithm for the corresponding search problem. This theorem turns out to be useful for the cryptanalytic result regarding the bounded storage model we present in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Part II: Implications to Cryptography</head><p>As the main motivation for the study of compression, we provide some strong implications of compressibility to cryptography. The implications described are of contrasting flavors. On the one hand we show constructions of cryptographic primitives using compression algorithms, while on the other hand we show a cryptanalysis using compression algorithms. Alternatively, this shows that the incompressibility of some languages is necessary for some applications. For simplicity, we discuss the implications with respect to the compression of SAT. We note however, that the same statements can actually be made with compression of languages from the class VC OR (see <ref type="bibr">Definition 2.20)</ref>. This class is the lowest class in our VC hierarchy, and potentially easier to compress than SAT. Moreover, the instances that we need to compress for our applications are further limited in the sense that (i) the relevant instances have a witness to either being in the language or to not being in the language and (ii) the (positive and negative) instances have a unique witness.</p><p>Basing Collision-Resistant Hash Functions on Any One-Way Function: Collision-Resistant Hash functions (CRH) are important cryptographic primitives with a wide range of applications, e.g. <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b4">5]</ref>. Loosely speaking, a CRH is a family H of length-reducing functions, such that no efficient algorithm can find collisions induced by a random hash function from the family. Currently there is no known construction of CRH from general one-way functions or one-way permutations, and moreover, Simon <ref type="bibr" target="#b76">[77]</ref> showed that basing CRH on one-way permutations cannot be achieved using "black-box" reductions. We show how a general compression algorithm may be used to bridge this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1.4 If there exists an errorless 8 compression algorithm for SAT then there exists a construction of collision-resistant hash functions based on any one-way function.</head><p>The construction of the CRH in Theorem 1.4 (if the hypothesis were true) would be inherently non-blackbox and uses the program of the one-way function via Cook's Theorem <ref type="bibr" target="#b16">[17]</ref>. This is essential to the validity of this approach, in light of the black-box impossibility result <ref type="bibr" target="#b76">[77]</ref>.</p><p>An interesting corollary of this result is a construction of statistically hiding bit commitment from any one-way function. Moreover, the construction would require only a single round of interaction. Such a construction was recently shown by <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b43">44]</ref> but requires a large number of rounds of interaction.</p><p>Basing One-Way Functions on Hard Instances: The next application shows that compression may be used in order to prove, in the terminology of <ref type="bibr" target="#b48">[49]</ref>, that Pessiland does not exist. Impagliazzo <ref type="bibr" target="#b48">[49]</ref> summarizes five possibilities for how the world may look like based on different computational assumptions. Pessiland is the option where it is easy to generate hard on the average instances yet no one-way functions exist (or in other words one cannot efficiently generate solved hard instances). We show that compression may be used to overrule this possibility and place us in the setting of Minicrypt in which one-way functions do exist. More precisely, given a language (not necessarily in N P) that is hard on the average for non-uniform machines over a samplable distribution and a compression algorithm for a related language, one can construct a one-way function. A clean statement in the case that the language is in N P is the following: Theorem 1.5 let L ∈ N P and let D be a samplable distribution such that any polynomial size circuit has only negligible advantage in deciding membership in L of samples drawn from D. If there exists a compression algorithm for SAT then there is a construction of a one-way function. If in addition the compression is errorless then there is also a construction of collision resistant hash functions.</p><p>This result also employs non-black-box techniques which are essential as it was shown that there is no black box construction of a one-way function from any hard on the average language (over a samplable distribution). This was shown initially by Impagliazzo and Rudich (in unpublished work) and formally by Wee <ref type="bibr" target="#b82">[83]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On Everlasting Security and the Hybrid Bounded Storage Model:</head><p>The bounded storage model (BSM) of Maurer <ref type="bibr" target="#b61">[62]</ref> provides the setting for the appealing notion of everlasting security <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. Loosely speaking, two parties, Alice and Bob, that share a secret key in advance, may use the BSM to encrypt messages in a way that the messages remain secure against an adversary which has storage limitations (yet is computationally unbounded), even if the shared secret key is eventually revealed.</p><p>However, if the parties do not meet in advance to agree on a secret key then everlasting security requires high storage requirements from Alice and Bob <ref type="bibr" target="#b28">[29]</ref>, rendering encryption in this model less appealing. Hoping to overcome this, it was suggested to combine the BSM with computational assumptions; we refer to this as the hybrid BSM. Specifically, the suggestion is to run a computational key agreement protocol in order to agree on a shared secret key, and then run one of the existing BSM encryption schemes. Dziembowski and Maurer <ref type="bibr" target="#b28">[29]</ref> showed that this idea does not necessarily work in all cases, by showing an attack on a protocol consisting of the combination of a specific (artificial) computational key agreement protocol with a specific BSM encryption scheme.</p><p>We show that compression of N P instances can be used to attack all hybrid BSM schemes. Or in other words, if a compression of SAT exists (even one that allows errors), then the hybrid BSM is no more powerful than the standard BSM. One interpretation of this result is that in order to prove everlasting security for a hybrid BSM scheme without further conditions, one must prove that there exists no compression algorithm for SAT or at least make a reasonable incompressibility assumption regarding the resulting formulae. Note however that a straightforward assumption of the form "this distribution on SAT formulae is incompressible" is not efficiently falsifiable, in the sense of Naor <ref type="bibr" target="#b67">[68]</ref>, that is, it is not clear how to set up a challenge that can be solved in case the assumption is false.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On Random Oracles:</head><p>The authors of this paper show in <ref type="bibr" target="#b44">[45]</ref> that if all parties are given access to a random oracle, then there actually exists everlasting security in the hybrid BSM without an initial key and with low storage requirements from Alice and Bob <ref type="foot" target="#foot_6">9</ref> . Therefore, finding a compression algorithm for SAT would show an example of a task that is achievable with random oracles but altogether impossible without them. <ref type="foot" target="#foot_7">10</ref> This would constitute an argument against relying (blindly) on random oracles to determine whether a task is feasible at all. This is different than previous results such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b5">6]</ref>, which show specific protocols that becomes insecure if the random oracle is replaced by a function with a small representation. Model separation results were discussed by Nielsen <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b73">74]</ref> (for non-interactive non-committing encryption) and Wee <ref type="bibr" target="#b81">[82]</ref> (for obfuscating point functions), but the separations there are between the programmable and non-programmable random oracle models. In contrast, the hybrid BSM result in <ref type="bibr" target="#b44">[45]</ref> holds also if the oracle is non-programmable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Witness-retrievable compression and the existence of Minicrypt:</head><p>The two top worlds that Impagliazzo considers in his survey <ref type="bibr" target="#b48">[49]</ref> are Minicrypt, where one-way functions exist but oblivious transfer protocols do not exist (in this world some interesting cryptographic applications are possible, and in particular shared key cryptography exists) and Cryptomania where Oblivious Transfer (OT) protocols do exist (and hence also a wide range of cryptographic applications like secure computation and public key cryptography). The last application we discuss is an attempt to use compression in order to prove that Minicrypt=Cryptomania. Whether oblivious transfer can be constructed from any one-way function is a major open problem in cryptography. Impagliazzo and Rudich <ref type="bibr" target="#b51">[52]</ref> addressed this problem and proved that key agreement protocols (and hence also oblivious transfer) cannot be constructed from any one-way function using "black-box" reductions.</p><p>We explore an approach of using compression in order to bridge the gap between the two worlds. In order to do so we introduce an additional requirement of a compression algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1.6 (Witness-retrievable Compression)</head><p>Let Z, L and L define a compression algorithm as in Definition 1.2 and let R L and R L be N P relations for L and L respectively. The compression is said to be witness-retrievable with respect to R L and R L if there exists a probabilistic polynomial-time machine W such for every input x, if x ∈ L then for every witness w x for x with respect to R L it holds that w y = W (w x , Z(x)) is a witness for Z(x) ∈ L with respect to R L . We allow a negligible error in the success of W (where probability is over the internal randomness of Z and W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1.7 (Informal) If there exists a witness-retrievable compression algorithm for a certain type of SAT formulas, then there exists an Oblivious Transfer protocol based on any one-way function.</head><p>As in the CRH construction (Theorem 1.4), the conditional construction of oblivious transfer in Theorem 1.7 is inherently non-black-box. Unfortunately we show that this approach cannot work with a compression algorithm for the general SAT problem, due to the following theorem:<ref type="foot" target="#foot_8">11</ref> Theorem 1.8 If one-way functions exist then there is no witness-retrievable compression of SAT.</p><p>Furthermore, we also rule out the possibility of other types of witness-retrievable compression that may be sufficient for Theorem 1.7. More precisely, the impossibility of witness-retrievable compression does not change when allowing an error in the retrieval, or when dealing with a case where there is a unique witness (see Corollary 6.7). These developments rule out basing the approach of Theorem 1.7 on the compression of a general and standard language. The approach may still work out with a witness-retrievable compression algorithm for the specific CNF formulas as constructed in the proof of Theorem 1.7.</p><p>Finally, we point out that almost all of the examples of compression algorithms in this paper (in Sections 2.1 and 2.10) are in fact witness-retrievable. This demonstrates that these examples fall short of the general compression that we are seeking. In fact a major obstacle in achieving compression for problems such as SAT seems to be that most natural approaches would be witness-retrievable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related Work</head><p>The relationship between compression and complexity in general is a topic that has been investigated since the early days of Complexity Theory (i.e. Kolmogorov Complexity <ref type="bibr" target="#b59">[60]</ref>). However, the feature that we are studying in this work is compressibility with respect to the solution (witness) rather than the instance. This distinguishes our work from a line of seemingly related works about notions of compression <ref type="bibr">([27, 78, 81]</ref> to name a few), all of which aim at eventually retrieving the input of the compression algorithm.</p><p>There are several examples of other relaxations of solving N P problems in polynomial time. Each of these relaxations yields a corresponding classifications of N P. These classifications, like ours, are subtle and usually turn out to be different than the traditional N P classification. For example, Papadimitriou and Yannakakis <ref type="bibr" target="#b74">[75]</ref> introduce L-reductions and the classes MAX NP and MAX SNP, with respect to approximation algorithms. Impagliazzo, Paturi and Zane <ref type="bibr" target="#b50">[51]</ref> define reductions with respect to solution in sub-exponential time.</p><p>The classification most related to ours is that of parameterized complexity (see the monographs on this subject by Downey and Fellows <ref type="bibr" target="#b23">[24]</ref>, Niedermeier <ref type="bibr" target="#b71">[72]</ref> and Flum and Grohe <ref type="bibr" target="#b33">[34]</ref>). Parameterized complexity studies the tractability of problems when one of the parameters is considered to be fixed or very small (this is called fixed parameter tractability (FPT)). One of the basic techniques of acquiring efficient algorithms in this context is the method of "kernelization" that may yield natural compression algorithms (see examples in Section 2.1). The kernelization method first shrinks the instance to a smaller instance whose size is only a function of the parameter and then solves it in brute force. However, in spite of the similarities between kernelization and compression, there are important differences. At a high level, kernelization is geared towards getting closer to a solution of the original instance. Our notion, on the other hand, requires compression per se, disregarding whether it is much harder to solve the compressed instance than the original one (in fact, in our main applications for constructing collision-resistant hashing and one-way functions in Sections 3 and 4, the compressed instance never has to be solved). Indeed we expect that new methods of compression that would resolve the problems we raise in this paper will utilize this property (that the compressed instance is harder to solve). That being said, a version of this notion, namely polynomial kernelization is equivalent to deterministic compression to size poly(n). The question of polynomial kernelization has been raised independently from our work in the parameterized complexity community (e.g. <ref type="bibr" target="#b33">[34]</ref>, Definition 9.1). See a further discussion on kernelization in Section 2.1. In addition, due to the above mentioned similarities, the Weft hierarchy of parameterized complexity is reminiscent of the VC-hierarchy: both being defined by reductions to circuits of bounded depth. However, as discussed above, our study of compression yields quite a different classification.</p><p>A related notion to parameterized complexity that is reminiscent of our work is limited non-determinism, which started with the work of Kintala and Fischer <ref type="bibr" target="#b57">[58]</ref>, see the survey by Goldsmith, Levy and Mundheck <ref type="bibr" target="#b39">[40]</ref>. This was further studied by Papadimitriou and Yannakakis <ref type="bibr" target="#b75">[76]</ref> who defined a few syntactic classes within the class of polylog non-determinism (LOGN P and LOGSN P ). The interesting point is that several natural problems are complete for these classes. The notion of reduction used is the usual polynomial reduction and hence the classifications arising from this study are very different from our VC hierarchy. A related classification is the EW-hierarchy defined by Flum, Grohe and Weyer <ref type="bibr" target="#b34">[35]</ref>. This hierarchy is a similar to the Weft classification of parameterized complexity but limits the running time to be only exponential in the witness length, thus being geared towards problems with polylogarithmic size parameters (as in LOGN P ).</p><p>Subsequent Works: Dubrov and Ishai <ref type="bibr" target="#b25">[26]</ref> discussed the compression of problems and showed that a certain incompressibility assumption has an application to derandomization. Specifically they construct a pseudorandom generator that fools procedures that use more randomness than their output length. Their work was mostly conducted independently of ours, following conversations regarding an early phase of our work. In addition, inspired by our CRH construction, they prove that any one-way permutation can either be used for the above mentioned derandomization, or else can be used to construct a weak version of CRH.</p><p>Dziembowski <ref type="bibr" target="#b27">[28]</ref> shows the relevance of our notion of witness-retrievable compression to achieving forward-secure storage. He shows a cryptanalytic result of such compression. Furthermore, following our approach for construction of OT from one-way functions, he shows that for every one-way function either a specific storage scheme is forward-secure, or there exists an (infinitely often) OT protocol based on this one-way function.</p><p>Recently some strong negative results about compression were shown. Fortnow and Santhanam <ref type="bibr" target="#b35">[36]</ref> show that an errorless compression algorithm for SAT (or even for the class VC OR ) entails the collapse of the polynomial hierarchy. Chen and Müller <ref type="bibr" target="#b14">[15]</ref> notice that this generalizes to compression with a one-sided error. These results limit the application to constructing collision resistant hash functions (Theorem 3.1). The application may still be valid given a relaxed compression algorithm. For example, it suffices if the compression is successful only on instances that either have a witness to being satisfiable or have a witness to not being satisfiable. Note that the applications in Sections 4 and 5 allow an error in the compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Part I: On the Compression of N P Instances</head><p>Attempting to compress N P instances requires a different approach than solving N P problems. Intuitively, a solution for compression might arise while trying to solve the problem. While a full solution of an N P problem may take exponential time, it is plausible that the first polynomial number of steps leaves us without an explicit solution but with a smaller instance. Indeed, some algorithms in the parameterized complexity world work like this (see some examples in the next section). On the other hand, we allow the possibility that the compressed version is actually harder to solve (computational time-wise) than the original one (and may require a somewhat longer witness altogether).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Examples of Compression Algorithms for some Hard Problems</head><p>We start by showing several examples of compression algorithms for problems that are conjectured not to be in P. Two of these example are N P-complete problems, while the third is taken from cryptography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vertex Cover:</head><p>The well studied N P-complete problem of vertex cover receives as input a graph G = (V, E) and asks whether there exists a subset of vertices S ⊆ V of size at most k such that for every edge (u, v) ∈ E either u or v are in S. The parameters are the instance length m, which is at most O(|E| log |V |), and the witness length n = k log |V |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.1 There exists a witness-retrievable compression algorithm for vertex cover.</head><p>Proof: We are following the parameterized complexity algorithm for vertex cover (presented in <ref type="bibr" target="#b23">[24]</ref> and attributed to S. Buss). If a vertex cover S of size k exists, then any vertex of degree greater than k must be inside the set S. The compression algorithm simply identifies all such vertices and lists them in the cover, while removing all their outgoing edges (that do not need to be covered by other vertices). The graph left after this process has maximal degree k, and furthermore all edges have at least one end in the cover. Thus, if the original graph has a k vertex cover then the total number of edges left is at most k 2 (at most k vertices in the cover with at most k edges each). If there are more than k 2 edges then the answer to the problem is NO, otherwise, such a graph can be represented by the list of all edges, which takes k 2 log k bits. The compression can be made witness-retrievable since if we use the original labels of vertices to store the new graph, then the original cover is also a cover for the new compressed graph. 2</p><p>It is in fact possible to get the compressed instance to be a graph with 2k edges, rather than k 2 edges, as shown in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b15">[16]</ref> (see <ref type="bibr" target="#b71">[72]</ref> Chapter 7). It is interesting to note that we do not know of a compression algorithm for the Clique problem or the Dominating Set problem, which are strongly linked to the vertex cover problem in the traditional study of N P, and in fact, in Theorems 3.1, 5.2 and 6.1 we show strong implications of a compression algorithm for these languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On parameterized complexity and compression:</head><p>The use of an algorithm from parameterized complexity for compression is not a coincidence. The "problem kernel" or "kernelization" method (see <ref type="bibr" target="#b23">[24]</ref>, Chapter 3 or <ref type="bibr" target="#b71">[72]</ref> Chapter 7) is to first reduce the problem to a small sub-instance that, like compression, contains the answer to the original problem. Then the algorithm runs in time that is a function only of the sub-instance, e.g. exponential in this small instance size. As was discussed in Section 1.4, if the running time and output size of the first reduction happens to be only polynomial in the parameter (a class formally defined in <ref type="bibr" target="#b7">[8]</ref>), then the first phase of the algorithm is a compression algorithm. Downey, Fellows and Stege <ref type="bibr" target="#b24">[25]</ref> (Lemma 4.7) show that kernelization (with arbitrary functions of the witness) captures precisely fixed parameters problems. Further restricting the attention to polynomial kernelization (e.g., <ref type="bibr" target="#b33">[34]</ref>, Definition 9.1) introduces a question that is equivalent to deterministic compression to size poly(n).</p><p>In this context, it is important to note that a compression algorithm for a language does not necessarily give a parameterized complexity algorithm for the same language. At first glance it seems that one can first run the compression algorithm, and then solve the compressed problem by brute force, thus getting a fixed parameter algorithm. However, such a strategy does not necessarily work, since in the compression algorithm there is no restriction on the size of the witness of the compressed language, which may in fact grow arbitrarily. Therefore solving the compressed problem by brute force may require a super-polynomial time in m. The same holds also for definitions of polynomial kernelization in which one does not restrict the witness size of the kernel(note that the witness can potentially be larger than the instance itself). Moreover, even if the witness does not grow, in many cases the witness size depends on the instance size and not on the parameter alone (e.g. in the Clique problem if the parameter is the clique size k then the witness length is n = k log m), in which case the above strategy is irrelevant with respect to the fixed parameter tractability of such problems.</p><p>Chapter 7 of the monograph of Niedermeier <ref type="bibr" target="#b71">[72]</ref> contains several examples of polynomial size kernelizations (e.g. for the languages 3-Hitting Set and Dominating Set on planar graphs). These algorithms yield compression algorithms for the respective languages. We describe one additional example of a compression algorithm that is derived in this manner. Minimum Fill-In: The minimum fill-in problem is an N P-hard problem that takes as input a graph G and a parameter k, and asks whether there exist at most k edges that can be added to the graph that would turn it into a chordal graph, i.e. one with no induced cycles of length more than 3. This problem has applications in ordering a Gaussian elimination of a matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.2 The minimum fill-in problem with parameter k has witness-retrievable compression.</head><p>Proof: Kaplan, Shamir and Tarjan <ref type="bibr" target="#b53">[54]</ref> prove that this problem is fixed-parameter tractable. Their algorithm partitions the graph into two sets of nodes A and B where A is of size k 3 and there are no chordless cycles (i.e. an induced cycle of length greater than 3) in G that contain vertices in B. The complexity of this partition is O(k 2 |V ||E|). They then prove that G has a k edge fill-in if and only if the graph induced by A has a k edge fill-in.</p><p>Thus the compression algorithm follows the same partitioning and stores only the graph induced by the small set A. The new graph has at most k 3 vertices and thus can be presented by only poly(k) log |k| bits. The fill-in for the new instance is exactly that of the original instance and thus the compression can be witness-retrievable if the original labels of the vertices are used for the compressed graph as well. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Sparse languages</head><p>We call a language sparse if the language contains only of a small fraction of the words of any given length. More precisely: Definition 2.3 (Sparse Language) Let L be an N P language with instance length m and parameter n and denote</p><formula xml:id="formula_2">L m,n = {x ∈ {0, 1} m | x ∈ L with witness of length ≤ n}, then L is sparse if there exists a polynomial p(•) such that for all sufficiently large m (with corresponding n) it holds that |L m,n | ≤ 2 p(n) .</formula><p>We show that all sparse languages can be compressed to a size that is dominated by the number of words that are actually in the language. This is shown by a generic compression algorithm for any sparse language. The idea is to apply a random (pairwise independent) hash function to the instance where the output of the hash is of length 2p(n) and thus substantially smaller than m. The new language contains all words that are hashed values of a word in the original language. We note that the compressed language L lies in N P(poly(m)) (recall that N P(poly(m)) stands for nondeterministic-time poly(m)). In particular, it is not necessarily witness-retrievable.</p><p>Rather than formally presenting the method for a general sparse language, we describe the method via a sparse language that we call PRG-output. Note that for this language the method is witness-retrievable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2.4 (PRG-Output)</head><p>Let G be a pseudorandom generator stretching an n bit seed to an m bit output (with m an arbitrary polynomial in n). Define the language PRG-output over inputs y ∈ {0, 1} m as L G = {y| there exists an x s.t. G(x) = y} As long as the underlying PRG is secure then it is hard to decide whether an instance was taken randomly from L(G) or from {0, 1} m . Yet this language has a simple compression algorithm. Note that simply saving, say, the first 2n bits of the instance y is insufficient because if y only differs from G(x) in one bit, then this bit may be anywhere in the m bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.5 There exists a witness-retrievable compression algorithm for PRG-output.</head><p>Proof: Let H be a family of almost pairwise independent hash functions from m bits to 2n bits. The compression algorithm simply chooses a random h ∈ H and outputs (h(y), h). The new language is</p><formula xml:id="formula_3">L G = {(z, h)| there exists an x s.t. h(G(x)) = z}.</formula><p>Naturally, if y ∈ L G then also (h(y), h) ∈ L G with the same witness (and thus the witness-retrievability). On the other hand, if y / ∈ L G then by the properties of H, for every seed x we have that Pr</p><formula xml:id="formula_4">h [h(G(x)) = h(y)] &lt; O(2 -2n</formula><p>), and by a union bound over all x ∈ {0, 1} n , we get</p><formula xml:id="formula_5">Pr h [h(y) ∈ L G ] &lt; O(2 -n ).</formula><p>Finally, since there are almost pairwise independent hash functions whose description is of length O(n) + log m (for example see <ref type="bibr" target="#b65">[66]</ref>), then the algorithm is indeed compressing. Note that the compression algorithm described above is probabilistic and carries an error probability of 2 -Ω(n) and also that the compressed language L in this case is in N P(poly(m)). 2 Sparse subset sum: We show another example of a compressible language called sparse subset sum that is sparse in a different sense than that of Definition 2.3. While the generic compression for sparse languages does not work for this language, it is compressible in its own right. Moreover, the compression algorithm for sparse subset sum is better than the generic algorithm in the sense that the compressed language in the specialized algorithm is in N P(poly(n, log m)) (or actually in N P) rather than in N P(poly(m)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2.6 (Sparse Subset Sum)</head><p>The language sparse subset sum takes as input n values x 1 , . . . x n each in {0, 1} m (with m &gt;&gt; n) and a target value T ∈ {0, 1} m . An input is in the language if there is a subset S ⊆ [n] where i∈S x i = T (the sum is taken modulo 2 m ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.7 There exists a witness-retrievable compression algorithm for sparse subset sum.</head><p>Proof: To compress an instance of sparse subset sum simply pick a large random prime 2 n &lt; P &lt; 2 2n+log m and store the numbers y i = x i mod P (for every i ∈ [n]), the target T P = T mod P and P (the idea of picking a prime P and working modulo P has been useful various applications, e.g. in the Karp-Rabin string matching algorithm <ref type="bibr" target="#b55">[56]</ref>). The compressed instance is of length O(n(n + log m)) and the compressed language is also subset sum (modulo P ). If there exists a set S for which i∈S x i = T then also i∈S y i = T P mod P (hence the witness-retrievability). On the other hand, we want that if the original instance was not in the language then for any subset S it will hold that i∈S y i = T P . In order to get i∈S y i = T P it is required that P is a divisor of D = i∈S x i -T . However D has at most m/n prime divisors that are greater than 2 n , while the prime P is taken from a range containing O(2 2n m/n) primes (we assume n ≥ log m in the calculations). Therefore, for every S it holds that Pr P [ i∈S y i = T P ] ≤ 2 -2n and by a union bound over all sets S, the probability of an error is bounded by 2 -n . 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">W-Reductions and Compression-Completeness</head><p>The few examples of compression that we have showed clearly indicate that the study of N P problems with respect to compression gives a distinct perspective, different from the traditional study of N P. The reason is that the typical Karp-reduction between N P problems does not distinguish between the length of the witness and the length of the instance. For example, when reducing SAT to the Clique problem, one builds a large graph from a CNF formula and asks whether or not it has a Clique of size k. However, in this new instance, the witness size <ref type="foot" target="#foot_9">12</ref> is a polynomial in m (the length of the SAT formula) rather than n (the number of variables in the formula). Thus, it is not clear how to use a compression algorithm for Clique to get a compression algorithm for SAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W-reductions and compression-completeness:</head><p>In order to show that a compression algorithm for L implies a compression algorithm for L, a more restricted type of reduction is needed. We call this a Wreduction and it is similar to a Karp-reduction but imposes an extra property on the length of the witness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.8 (W-Reduction)</head><p>For two N P languages L and L we say that L W-reduces to L if there exist polynomials p 1 and p 2 and a polynomial-time computable function f that takes an instance x for L and outputs an instance f (x) for L such that:</p><formula xml:id="formula_6">1. f (x) ∈ L if and only if x ∈ L.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If</head><p>x is of length m with witness length n, then f (x) is of length at most p 1 (m) with witness length at most p 2 (n, log m).</p><p>We first note that this reduction composes, that is: Claim 2.9 If L W-reduces to L and L W-reduces to L then L W-reduces to L .</p><p>We next claim that W-reduction indeed fulfills its goal with respect to compression: Claim 2.10 Let L and L be N P languages such that L W-reduces to L. Then given a compression algorithm for L, one can obtain a compression algorithm for L .</p><p>Proof: Suppose that x is an instance for language L of length m with witness length n. The compression algorithm for L runs as follows: First use the W-reduction to L and get an instance f (x) for L, and then run the compression algorithm for L on f (x). By the properties of the reduction f (x) is of length m ≤ p 1 (n, m) with witness length n ≤ p 2 (n, log m). The outcome Z(f (x)) of the compression is therefore of length poly(n , log m ) = poly(n, log m). Furthermore, if L is the language that Z compresses to, then Z(f (x)) ∈ L if and only if f (x) ∈ L which in turn happens if and only if x ∈ L . Thus the combined process gives a compression algorithm for instances of L . 2</p><p>We remark that in the complexity discussion of compression we choose to ignore the issue of witnessretrievability. Nevertheless, in order for the W-reduction to relay this property, the reduction itself must also have a witness-retrievability property. That is, given a witness w for x ∈ L then one can efficiently compute w for f (x) ∈ L (without the knowledge of x). We define complete problems with respect to compression: these are defined similarly to the standard notion, but with respect to W-reductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.11 (Compression-Complete</head><formula xml:id="formula_7">) A problem L is compression-complete for class C if: 1. L ∈ C 2. For every L ∈ C the language L W-reduces to L.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A language is called compression-hard for class C if requirement 2 holds (requirement 1 may or may not hold).</head><p>The relevance of compression-complete problems is stated in the following simple claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.12 Let L be compression-complete for class C, then given a compression algorithm for L, one can obtain a compression algorithm for any L ∈ C.</head><p>The proof follows directly from the definition of completeness and Claim 2.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The VC Classification</head><p>We now introduce the new classification arising from the study of compressibility of N P problems. For this we define a series of N P languages. Some notation: by a circuit of depth k we mean a depth k alternating AND-OR circuit where the fan-in of the gates is bounded only by the size of the circuit and negations are only on the input variables (no NOT gates). The next language, LocalCircuitSAT, is a less natural one. It is designed to capture computations that do not need to access the whole input, but can rather check only a sub-linear fraction of the input (a good example is verifying that a set of vertices in a graph is indeed a Clique). Let x be a string of length m. If I = (i 1 , . . . , i n ) is a list of n locations in x then we denote by x(I) the values of x at these locations. We can now introduce our classification of N P problems: Definition 2.15 (The VC classification of N P problems) Consider N P problems where m denotes the instance size and n denotes the witness size. We define the class VC k for every k ≥ 0. The definition is divided into three cases:</p><p>• k = 0: The class VC 0 is the class of all languages that admit compression algorithms. There are two possible versions here, one considering errorless compression and the other allowing probabilistic compression with errors. We typically refer to the later, depending on the context.</p><p>• k = 1: The class VC 1 is the class of all languages that W-reduce to LocalCircuitSAT.</p><p>• k ≥ 2: The class VC k is the class of all languages that W-reduce to Depth k CircuitSAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For any function</head><formula xml:id="formula_8">k(m, n) (where k(m, n) ≤ m) also define VC k(m,n) as the class of all languages that W-reduce to Depth k(m,n) CircuitSAT. Finally, define VC = VC m (the class for k(m, n) = m).</formula><p>A first observation is that simply by definition, the languages LocalCircuitSAT and Depth k CircuitSAT are compression-complete for their respective classes. The most notable example of a complete language is for the class VC = N P where the complete problem is CircuitSAT (satisfiability of a polynomial size circuit). When discussing a W-reduction to a depth k circuit, we can actually assume without loss of generality that the top gate of this circuit is an AND gate (as we will show in the next claim). An immediate corollary is that SAT (that is, satisfiability of CNF formulas) is compression complete for the class VC 2 . Formally, let Depth k CircuitSAT AN D denote the language Depth k CircuitSAT when restricted to circuits where the top gate is an AND gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.16</head><p>For any k ≥ 2, we have that a language L ∈ VC k if and only if L W-reduces to the language Depth k CircuitSAT AN D .</p><p>Proof: We show that any instance that contains a circuit where the top gate is an OR W-reduces to an instance with top gate AND. We prove this first for k ≥ 3. Denote the input circuit by C = j t C j,t where each C j,t is a top OR depth (k -2) circuit. If C is satisfiable then t C j,t is satisfiable for at least one choice of j. Add to the witness the index i of this satisfiable sub-circuit (i is given by the boolean variables i 1 , ..., i where is logarithmic in poly(m, n)). For each j, denote C j,t = C j,t ∨ i j1 1 ∨ ... ∨ i j , where i j denotes i ⊕ j. Notice that C j,t is always satisfied for j = i, and for j = i is satisfied if and only if C i,t is satisfied. Thus the circuit can now be written as C = j,t C j,t that is satisfiable if and only if the original circuit was. The top OR gate of C is therefore removed in the new instance C while adding only a small number of variables, thus the input to the circuit witness remains of order poly(n, log m) as required.</p><p>In the case k ≥ 3, the depth of the new instance becomes k -1. In the case that k = 2, the bottom level that included only variables is transformed into an OR of variables, thus the new circuit is simply a CNF formula (and the depth remains k = 2). 2</p><p>The VC Hierarchy: The VC classification indeed defines a hierarchical structure. That is:</p><formula xml:id="formula_9">VC 0 ⊆ VC 1 ⊆ VC 2 ⊆ VC 3 • • • ⊆ VC.</formula><p>And in general, for every two polynomially bounded functions k(n, m), (n, m) such that for all n, m we have k(n, m) ≤ (n, m) then VC k (m, n) ⊆ VC (m, n). Furthermore, VC = N P by the definition of N P. These observations follow trivially by the definitions, the only non-trivial part being the fact that</p><formula xml:id="formula_10">VC 1 ⊆ VC 2 , that is proved next. Lemma 2.17 VC 1 ⊆ VC 2</formula><p>Proof: We need to show a W-reduction from LocalCircuitSAT to SAT. The input is therefore a long string x and small circuit C on n + n log m variables. Let i 1 , ...i n denote the potential locations in the string that the circuit C receives as inputs. Also define the variables y 1 , ..., y n to indicate the values of x in the corresponding locations (that is y t = x it for t ∈ [n]). Thus the circuit C runs on the variables y 1 , ..., y n and the bits of i 1 , ..., i n .</p><p>We first note that C is of size p(n, log m) = (n + n log m) and may be reduced (via Cook's Theorem <ref type="bibr" target="#b16">[17]</ref>) to a CNF formula Φ C over O(p(n, log m)) variables and of size O(p(n, log m)) that is satisfiable if and only if C is satisfiable.</p><p>Thus we have a CNF formula over the variables y 1 , ..., y n , i 1 , ...i n and some extra variables. This formula's satisfiability will be equivalent to the membership of the LocalCircuitSAT instance if we manage to force the variables of y to take the values y t = x it . This is done by adding additional clauses to the CNF in the following manner: For simplicity we describe this only for y 1 , where the same is repeated for every other y t for t ∈ [n]. Define for each j ∈ [m] a formula Φ j = (y 1 = x j ) ∨ (i 1 = j). Notice that Φ i 1 = 1 if and only if y 1 = x i 1 . Denote the bits of i 1 by i 1,1 , ..., i 1,d where d = log m . An alternative way to write Φ j is as the following CNF (recall that i j denotes i ⊕ j):</p><formula xml:id="formula_11">Φ j = (y i ∨ x j ∨ i j1 1,1 ∨ ... ∨ i jd 1,d ) ∧ (y i ∨ x j ∨ i j1 1,1 ∨ ... ∨ i jd 1,d )</formula><p>Finally, to force y 1 = x i 1 we simply take the new CNF to be Φ C ∧ j∈[m] Φ j . The same is repeated to force</p><formula xml:id="formula_12">y t = x it for all t ∈ [n]. 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The VC Classification and Verification with Preprocessing</head><p>We now discuss the VC hierarchy from a different angle, that of the verification complexity of a language. This approach, though slightly more cumbersome than the definition via W-reductions, gives more intuition as to what it means to be in a class VC k . The new view defines the VC hierarchy with respect to the verification algorithm for L, that is, the efficient procedure that takes a witness w for x ∈ L and verifies that it is indeed a true witness. We point out that the nature of verification algorithms may vary when discussing different N P problems. For example, in the k-Clique problem the verification algorithm needs to check only O(k 2 ) edges in the graph, and thus can read only a sub-linear part of the instance. In SAT, on the other hand, all the clauses in the formula must be checked when verifying a witness. Simply looking at the verification algorithm of a language is not sufficient. For starters, classification according to verification does not distinguish between problems in P that are trivially compressible and N P-complete languages. Instead, we consider the notion of verification with preprocessing. This is the process for verifying that x ∈ L when given a witness, that also allows a preprocessing stage to the instance. Formally: Definition 2.18 (Verification with Preprocessing) Let L be an N P language with instances of length m and witness length n. A pair of polynomial-time algorithms (P, V ) are called a verification with preprocessing for L if the following two step verification holds:</p><p>1. Preprocessing: P gets an instance x and outputs a new instance P (x).</p><p>2. Verification: There exists a polynomial p(•, •) such that x ∈ L if and only if there exists a witness w of length at most p(n, log m) such that V (P (x), w) = 1.</p><p>Notice that when allowing for preprocessing, then all problems in P have a pair (P, V ) where P solves the problem and stores the answer while V simply relays this answer. Thus when considering the complexity of V in this definition, then easy problems indeed have very low complexity.</p><p>The VC Classification via Verification with Preprocessing: An alternative and equivalent way to view the classes in the VC hierarchy is based on the verification algorithm V in a verification with preprocessing pair (P, V ). The problems are divided into two families:</p><p>• The class VC 1 is the set of the languages that have very efficient verification (i.e. poly(n, log m) rather than poly(n, m)). We assume random access to the instance (suppose that the verification algorithm is a RAM), thus such a verification algorithm only accesses a sub-linear fraction of the instance.</p><p>• The languages whose verification is not very efficient (run in time poly(n, m)). This family is further classified into sub categories. The class VC k is the class of languages where the verification algorithm V has a representation as a depth k polynomial size circuit (polynomial in n and m).</p><p>This definition is equivalent to the definition via W-reductions since the W-reduction to the complete problem can simply be viewed as the preprocessing stage. In the other direction, every preprocessing stage is actually a W-reduction to the language defined by V .</p><p>It is interesting to note that Buss and Islam <ref type="bibr" target="#b8">[9]</ref> give an alternative view with similar flavor to the Weft hierarchy of parameterized complexity. They call it "prepare, guess and check" in which they essentially add a preprocessing phase to a previous approach of Cai and Chen <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Within VC 1 -The Class VC OR</head><p>Arguably, the most interesting class in the hierarchy is the bottom class VC 1 . It contains many natural problems such as Clique or small subset-sum <ref type="foot" target="#foot_11">14</ref> that only test local properties of the input. Furthermore, it is presumably the easiest to find compression algorithms for. We further refine our hierarchy within the class VC 1 by introducing another class, the class VC OR . Consider the language OR(L) that take a large OR of small instances of a language L. Formally:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.19 (OR(L))</head><p>Let L be an N P language. Define the language OR(L) as follows Input: m instances x 1 , ..., x m to the language L, each of length n.</p><formula xml:id="formula_13">Membership: (x 1 , ..., x m ) ∈ OR(L) if there exists i ∈ [m] such that x i ∈ L.</formula><p>Specifically the language OR(CircuitSAT ) is defined as: Input: m circuits C 1 , . . . , C n where each circuit is of size n. Membership: (C 1 , ..., C m ) ∈ OR(CircuitSAT ) if at least one of the m circuits is satisfiable. This language is used to define the following class: Definition 2. <ref type="bibr" target="#b19">20</ref> The class VC OR is the class of all languages that W-reduce to OR(CircuitSAT ).</p><p>We first note that in each of the m small instances, the instance length and witness length are polynomially related. So unlike the general case where we focused only on short witness languages, when talking about OR(L), any language L ∈ N P \ P is interesting. For example, the language OR(3 -SAT ) is not trivially compressible. Moreover, it is compression-complete for VC OR . Claim 2.21 Let L be any N P-complete language, then OR(L) is compression-complete for VC OR .</p><p>Proof: The W-reduction from OR(CircuitSAT ) to OR(L) simply runs the standard Karp reduction from CircuitSAT to L for each of the m circuits independently. The witness for each circuit was of length at most n and is now of size p(n) for some polynomial p. In addition the witness contains an index of the instance of L that is satisfied, thus the total witness length is p(n) + log m. 2</p><p>For example, the problem OR(Clique) that gets m small graphs (over n vertices) and asks whether at least one of the graphs has k sized clique (where k = O(n)) is also compression-complete for VC OR .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.22 VC OR ⊆ VC 1</head><p>Proof: This is best seen by W-reducing OR(Clique) to LocalCircuitSAT. Given graphs G 1 , ..., G m for OR(Clique), generate the instance x = G 1 , ..., G m and a circuit C that receives the locations of a clique in one of the graphs and checks whether they are indeed the edges in these locations form a clique (all belong to the same graph and are the edges induced by k vertices). The size of the circuit is p(n, log m) for some polynomial p since it checks only locations in x that belong to one graph (of size n). Finally, add p(n, log m) dummy variables to the circuit so that the circuit C has size equal to the number of input variables (this is a technical requirement in the definition of LocalCircuitSAT). 2 Furthermore, VC 0 ⊆ VC OR , since any compressible language can be W-reduced by the compression algorithm to a language with instance size p(n, log m) and this instance can be reduced to CircuitSAT and viewed as an OR of a single small circuit and hence is in VC OR . Note that here too, one may need to add dummy variables to make the circuit of the same size as its input. Altogether we have that:</p><formula xml:id="formula_14">VC 0 ⊆ VC OR ⊆ VC 1 .</formula><p>Finally, we show a language that is compression-hard for VC OR . This claim is also relevant to our cryptographic applications (in Sections 3, 4, 5 and 6): <ref type="bibr" target="#b22">23</ref> Clique is compression-hard for VC OR .</p><formula xml:id="formula_15">Claim 2.</formula><p>Proof: The language OR(Clique) W-reduces to Clique simply by taking one graph that is the union of all the small graphs in the OR(Clique) instance. Clearly there is a clique in the union if and only if there is a clique in at least one sub-graph. 2 A similar claim is true for all problems involving searching for a connected subgraph of size n in a graph of size m as long as the problem of deciding whether a graph of size p(n) contains such a subgraph is NP-Hard for some polynomial p(•). This is true, for instance, for the problem of whether there is a path of length n. <ref type="foot" target="#foot_12">15</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">The VC Classification and some N P Problems</head><p>In general, most of the VC classification focuses on W-reductions to depth k circuits. The reasoning for this is that there is a certain tradeoff between depth and the number of variables. More precisely, we can reduce the depth of a verification circuit, but only at the price of adding additional variables (this is done using methods from Cook's Theorem <ref type="bibr" target="#b16">[17]</ref> and requires adding a variable for each gate in one intermediate level of the circuit). Since the number of variables is the focal point when discussing compression (as it coincides with the witness size), then depth turns out to be central in our classification.</p><p>Given our current state of knowledge, there are a few plausible views of the world. The two endpoint scenarios are (i) there is compression for every language in N P (as would be implied by a compression algorithm for CircuitSAT), (ii) there is only compression for a few select problems, such as the examples in section 2.1. A third option is that there is a compression algorithm for some compression-complete problem in the hierarchy (say for VC k ), which would imply the collapse of all the classes below VC k to VC 0 .</p><p>We will briefly go over a few key classes in the hierarchy and a few examples of natural N P problems and their classification (as we know it) within the VC hierarchy. We note that all the statements in this section apply also to compression with possible error (negligible in n).</p><p>The class VC 0 : Currently we know that this class contains all the languages in P, languages that are already compressed by definition (such as 3-SAT), and the languages that we showed compression algorithms to (Vertex cover, PRG-output and Minimum-fill-in).</p><p>The class VC OR : This class contains all languages OR(L) for an N P language L. One natural example is the OR(SAT ) problem which is actually a depth 3 circuit where the fan-in at the two bottom levels is bounded by n and only the top gate is allowed to be of greater fan-in. Some important languages in this class are those that need to be compressed in the cryptographic applications in Sections 3, 5 and 6.</p><p>The class VC 1 : Since we are only interested in problems where the witness size n is much smaller than the instance size m, then many natural problems with this restriction are in VC 1 . For example, graph problems that ask whether a small graph can be embedded in a large graph are all in VC 1 . The Clique problem (with a clique of size n), or Long-Path (a path of length n that does not hit any vertex twice) are such small graph embedding problems. Small Subset-Sum is another natural language in VC 1 . This language receives a set of m values and a target sum and asks whether there is a small (size n) subset for which the values add up exactly to the target sum (see also footnote in Section 2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dominating Set:</head><p>The problem asks, given a graph, whether there is a set of k vertices such that all the graph is in its neighbor set. Dominating set is in the class VC 3 as implied by the following verification: the witness is a set S and the algorithm tests that ∀ vertex v ∃ vertex u ∈ S such that (u, v) is in the graph. The ∀ translates to and AND gate and the ∃ translates to an OR gate. Finally, testing that an edge is in the graph requires an AND over the O(log m) bits representing this edge. In total, this is a depth 3 circuit. Note that a straightforward verification of vertex cover will also yield a depth 3 circuit. However, while vertex cover is compressible and in VC 0 , for dominating set we are unaware of a better method. In addition, dominating set is compression-hard for VC 2 . This is seen by a standard reduction of SAT to dominating set in which a SAT formula with n variables and m clauses is transformed into a graph with m + 3n vertices with the property that the graph has a dominating set of size n iff the SAT formula is satisfiable. <ref type="foot" target="#foot_13">16</ref>Weighted-SAT: Given a CNF formula of length m the problem asks if it has a satisfying assignment of weight at most k (at most k variables are assigned the value 1). Unlike our previous discussions of SAT, the number of variables here is only bounded by m and the short witness simply consists of the list of all variables that receive the value 1 (that is, the witness is of length n = k log m). This problem, with constant clause size, serves as the basic complete problem for the parameterized complexity class W <ref type="bibr" target="#b1">[2]</ref>, which is at the bottom of the W-hierarchy (see <ref type="bibr" target="#b23">[24]</ref>). However, with regards to compressibility, we only know how to place it in the class VC 4 . This is shown by the following verification procedure (using the same logic as with Dominating-Set): For every witness (list) L, the algorithm tests that ∀ clauses C either ∃ a variable x ∈ C such that x ∈ L or ∃ a negated variable x ∈ C such that x ∈ L. The verification of x ∈ L adds up to total depth 3 by testing that ∃y ∈ L such that x = y (where x = y is tested by an AND over the bits of x and y). On the other hand, verifying that x ∈ L requires total depth 4 as it runs ∀y ∈ L we have x = y. The overall depth is thus dominated by the negated variables and is thus 4.</p><p>OR of (large) instances: Consider the Or of CNF formulas over few variables (each CNF formula may be large, unlike in the language OR(SAT ) where the CNF formulas are considerably smaller than the fan-in of the OR gate). In other words, instances of this language are depth three circuits where the top gate is an Or gate. Yet the language is actually in VC 2 , as implied by Claim 2.16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integer Programming (IP):</head><p>An instance of integer programming consists of a list of m linear constraints on n integer variables with the goal of maximizing a linear target function over these n variables (under the list of constraints). Unlike its counterpart of linear programming, where the variables may take real values and is polynomial-time solvable, integer programming is N P-hard even when the variables are restricted to taking only the values 0 and 1 (one of Karp's original problems <ref type="bibr" target="#b54">[55]</ref>). Thus, the decision variant of integer programming, where the number of constraints is much larger than the number of variables, is interesting with respect to compression. First, compressing it is at least as hard as compressing SAT: given a SAT instance with n variables and m constraints it is simple to come up with a corresponding IP instance with 2n variables and m constraints, i.e. IP is VC 2 -hard. On the other hand, a straightforward verification of a witness for this problem takes the proposed assignment for the n variables and checks if it satisfies each of the constraints. The verification of a linear constraint can be achieved in logarithmic depth (in n), placing IP in VC k (n) for k(n) = Ω(log n). We are unaware of a (significantly) better classification (of lower depth) for integer programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">On Reducing the Error in Compression Algorithms</head><p>The error of a compression algorithm can be reduced substantially at the expense of a worse compression rate (the output length of the compression algorithm will be longer). The idea is simply to run and store the outcome of many executions of the compression, each time with a fresh and independent randomness. For example, by storing n independent executions and using a Chernoff bound we arrive at the following claim:</p><p>Claim 2. <ref type="bibr" target="#b23">24</ref> Let Z be a compression algorithm for language L with outcome length p(n, log m) and q, δ &gt; 0 be such that (i) if x ∈ L then Z(x) ∈ L with probability q, and (ii) if x / ∈ L then Z(x) / ∈ L with probability q + δ. Then there is a compression algorithm Z with error 2 -Ω(δ 2 n) and outcome length np(n, log m).</p><p>Note that this technique is limited by the growth of the output and, in particular, one cannot use this method to achieve an error that is exponentially small in m (rather than n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">On Compression of Search Problems</head><p>So far, the N P problems that we discussed were all decision problems, that is, they ask if x ∈ L, and are answered by YES or NO. When considering a specific N P relation R L associatedwith L, then the above decision problem has a natural search problem associated with it, which is to actually find a witness to x ∈ L with respect to the relation R L . A solution to such a problem is an n bit string rather than just a single bit.</p><p>Loosely speaking, a compression algorithm for the search instance should produce a shorter output that contains enough information about some witness for the original problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.25 (Compression for search problem)</head><p>A compression algorithm for an N P search problem L (with respect to R L ) is a pair of algorithms (Z, E) with a polynomial p(•, •), where Z is a polynomialtime compression algorithm and E is an unbounded extraction algorithm. Given an instance x with witness parameter n we should have that:</p><p>1. Z(x) is of length at most p(n, log m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If</head><p>x ∈ L and there is a witness of length n, then E(Z(x)) = w where w is a witness to x ∈ L with respect to R L .</p><p>It is natural to consider the relationship between the difficulty of decision and search for a given problem, as was done in other settings such as average-case complexity by Ben-David et al. <ref type="bibr" target="#b6">[7]</ref>. We show that for any problem a compression for the decision variant also yields a compression for the search variant, without an increase in the V C hierarchy.</p><p>Theorem 2.26 For any k ≤ 1, if the class VC k has a compression algorithm, then there is a compression algorithm for the search problem of a relation R L of L ∈ VC k . This is true also for VC OR .</p><p>Note that Theorem 2.26 holds also when a small error in the compression is allowed. The error in the resulting compression for search algorithm grows by a polynomial factor (by factor n 3 ) with respect to the error of the underlying compression for decision algorithm. This follows in a straightforward manner from the proof (by a union bound).</p><p>The technique of the proof below also comes in handy in proving Theorem 5.4, regarding the application of the ability to compress, say SAT, to cryptanalysis in hybrid bounded storage model. In the following proof, a witness to x ∈ L refers to a witness according to the specific relation R L associated with L.</p><p>Proof: Given an instance x to a language L, for any i ∈ [n], consider the N P problem L i that asks whether there exists an n bit witness w to x ∈ L such that w i = 1 (the i th bit of w is 1). The language L i is also in VC k since its verification circuit is the same as the one for L with an additional AND to the variable w i (this AND gate is incorporated into the top level AND of the circuit thus the depth remains k).</p><p>Our first attempt is to compress the instance x for every i ∈ [n] with respect to the language L i (denote such a compression by Z L i (x)). Thus we store Z L i (x) for all i ∈ [n], which amounts to n • p(n, log m) bits, for some polynomial p(n, log m) (this is also in poly(n, log m)). Now suppose that there is only a single witness w to x; then one can extract w bit by bit, by solving the compressed instance of each bit. However, this fails when w is not the only witness, and we might obtain inconsistent answers for the different bits.</p><p>The natural idea now is to use the reduction of Valiant and Vazirani <ref type="bibr" target="#b79">[80]</ref> to a unique witness, as was done by Ben-David et al. <ref type="bibr" target="#b6">[7]</ref> for showing that average NP being in BPP implies also a randomized search algorithm for average NP. The idea is to choose a pairwise-independent hash function h that is appropriately shrinking, and add to the language the requirement that h(w) = 0. We use the following lemma: Lemma 2.27 ([80]) Let L be an N P language and for every x denote by W x the set of all witnesses to x ∈ L. Let be such that 2 ≤ |W | ≤ 2 +1 . Let H +2 be a family of pairwise independent hash functions with h : {0, 1} n → {0, 1} +2 for all h ∈ H +2 . Then</p><formula xml:id="formula_16">Pr h∈H +2 [|{w : w ∈ W x and h(w) = 0}| = 1] ≥ 1 8</formula><p>Let H be a family of pairwise independent hash functions. Consider the N P language L H whose elements are of the form (x, h) where h ∈ H maps strings of length n to some shorter length. We have that (x, h) ∈ L H if there is a witness w for x ∈ L and h(w) = 0. We note that this language is also in VC k , since the additional requirement that h(w) = 0 can be verified efficiently over n variables (the hash function h computation is efficient). By Cook's theorem this computation may be represented as a CNF formula φ h over these variables plus only poly(n) additional variables. Thus adding the requirement of the hash does not add to the depth of the verification circuit for L. This is easy to for VC k , and for VC OR note that we can add (conjunction) the CNF formula φ h to each instance of CircuitSAT, while keeping the problem in VC OR . Now, if we enumerate on all values of , then with probability at least 1  8 , for the correct we will get that L H has a unique witness; storing Z L H i (x, h) for all i suffices to maintain the information about this witness. This can be repeated sufficiently many times (say O(n) times), so that with overwhelming probability one of the attempts will indeed give a unique witness. However, this solution is also insufficient, since we have stored a list of O(n 2 ) compressed values (O(n) repetitions for each value of ∈ [n]) with the guarantee that with overwhelming probability one of them is a witness for x, but we do not known which one (recall that we cannot store the original instance and thus cannot verify that a witness is correct).</p><p>Our final attempt succeeds in reducing the list of potential witnesses into a unique and true witness. This compression is as follows: Denote by Lī the language that asks whether there exists an n bit witness w to x ∈ L such that w i = 0 (similar to L i but with w i negated). The compression of an instance x to the search problem L goes as follows: For every ∈ [n] repeat the following n times:</p><formula xml:id="formula_17">• Choose h ∈ R H +2 . • For all i ∈ [n] store Z L H i (x, h) and Z L H ī (x, h).</formula><p>The extraction procedure is as follows: For all and h ∈ H +2 , solve all the compressed instance pairs. For every pair Z L H i (x, h) and Z L H ī (x, h), if both are negative or both are positive, then ignore all values that are compressed with this h. Only if for all i we have that exactly one of the instances being correct, then output the i th bit of w according to the result.</p><p>The above algorithm indeed compresses, since it only adds a factor of n 3 to the overall storage. With probability at least 1 -2 -Ω(n) at least one of the chosen h's is successful in leaving exactly one witness to x ∈ L h , and this witness will be extracted. Finally, if h did not leave exactly one witness, then this will be identified: If there are no witnesses then Z L H i (x, h) and Z L H ī (x, h) will both be negative for all i. If there is more than one witness, then for at least one choice of i both Z L H i (x, h) and Z L H ī (x,h) will be positive. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">On Maintaining Other Information</head><p>We have seen that compression may maintain much more than just a yes/no answer. A natural question to ask is what other types of information may be maintained through compression algorithms. The following are some examples:</p><p>Number of witnesses: The compression described above actually maintains an approximation of the number of witnesses to x ∈ L (with respect to R L ). Once the chosen k is too large, there will be a sharp drop in the probability of having a witness and this can be observed when extracting the witnesses and indicate what is the right k.</p><p>An almost random witness: The compression above also outputs a witness that is almost uniformly distributed over W x . Or more accurately, the probability of getting each witness is bounded by a constant times 1/|W x |.</p><p>On maintaining all witnesses: As opposed to maintaining a single witness or the number of witnesses, a compressed instance cannot always maintain the information about all of the witnesses of an input instance. This is shown by the following simple information theoretic argument: encode an m bit string s with a DNF circuit C by constructing for each position j ∈ [m] a formula C j on log m variables. If s[j] = 1 then take C j to be circuit that is satisfied iff the variables encode the index j. If s[j] = 0 then C j is the non-satisfiable circuit C j = 0. The circuit C is formed by taking an OR of all these circuits (C = j∈[m] C j ). The satisfying assignments of C correspond exactly to the 1's in s. Consider C as an input to the language as CircuitSAT <ref type="foot" target="#foot_14">17</ref> . Suppose that there exists a compression algorithm that maintains all of the witnesses of a circuit C. In particular, this means that the m bit string s may also be extracted from the compressed instance. But this is clearly impossible information theoretically, since m random bits may not be represented by poly(n, log m) &lt; m bits. So we conclude that if our goal is come up with a compression algorithm for SAT then we must come up with a way of losing information about the witnesses.</p><p>In the examples of compression that we have seen in Section 2.1, the compression algorithms for vertex cover, PRG-output and Minimum fill-in actually maintain all the witnesses. On the other hand, the compression for GapSAT (which we will see in Section 2.10) does not necessarily maintain this information, as it is based on sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">Speculation on Compression</head><p>We give two arguments that may be viewed as evidence to the existence and non-existence of compression respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Optimistic View -Compression of a promise problem and the PCP Theorem:</head><p>Consider the promise problem GapSAT that takes as input a CNF formula Φ of size m over n variables and the guarantee that either Φ is satisfiable or it is at most (1 -1 2n )-satisfiable (no assignment satisfies more than (1 -1 2n ) of its clauses). The task is to decide if Φ is satisfiable or far from satisfiable.</p><p>Such a problem has a simple and witness-retrievable compression. The idea is to choose O(n 2 ) random clauses from Φ and take the AND of these clauses to be the compressed formula Ψ. This compression works because if Φ is far from satisfiable then for every assignment the formula Ψ is satisfied with probability at most 2 -2n (Ψ does not contain one of the 1 2n m unsatisfied clauses). Taking a union bound over all assignments, we get that with probability (1 -2 -n ) the formula Ψ has no satisfying assignment. On the other hand, if Φ is satisfiable then the same assignment also satisfies Ψ (and hence the witness-retrievability). Note that our definition of GapSAT is robust in the sense that GapSAT is compressible whenever the gap is (1 -1 p(n) ) for every choice of a polynomial p(•). The above simple compression algorithm is especially interesting in light of the PCP Theorem. One way to view the PCP Theorem is as an efficient reduction from an instance of SAT to an instance of GapSAT. Thus one can hope to combine the PCP reduction with the above compression and get a compression for general SAT. However, reducing general SAT to GapSAT via the PCP is not a W-reduction as the witness size grows to the order of the instance size. For starters, the PCP Theorem is typically defined over 3-CNF formulas, and the reduction of a general size m CNF to a 3-CNF adds O(m) variables. In order for this approach to achieve compression for SAT, we require a new PCP Theorem that is actually a W-reduction.</p><p>GapSAT is just one example of a gap problem that admits compression. For instance, one can consider the promise problem GapClique where a graph of size m either has a Clique of size m/n or contains no Clique of size n. As in the case of GapSAT, GapClique is compressible by sampling a subset of its vertices. Thus, coming up with a W-reduction from a general (n , m )-Clique problem (the graph of size m either contains a clique of size n or not) to (n, m)-GapClique would enable the compression of Clique. We view finding PCPs that are also W-reductions as a major research direction, especially in light of the recent new proof to the PCP Theorem of Dinur <ref type="bibr" target="#b22">[23]</ref>.</p><p>This connection to succinct PCPs was subsequently studied by Fortnow and Santhanam <ref type="bibr" target="#b35">[36]</ref>. They derive negative results on PCPs from the negative results on compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Pessimistic View -On Oblivious Compression:</head><p>We have seen in Section 2.9 that it is impossible to maintain all of the information in an instance when compressing it and some information is necessarily lost (for example the list of all witnesses cannot be kept). On the other hand, we show that if compression exists then it is not likely to lose too much information about the original instance. Such a result would entail the collapse of the polynomial hierarchy to its second level. More formally:</p><p>Let Z be a compression algorithm for SAT. We consider it as a two input algorithm taking a formula Φ and local randomness r ∈ {0, 1} . Denote by Z(Φ, U ) the random variable taking the output of Z with fixed input Φ and random r ∈ R {0, 1} . Let X be a distribution over formulas. The random variable Z(X, U ) denotes the output of Z under a choice of random r and a random Φ from the distribution X.</p><p>The compression Z is said to be ε-oblivious if for every m, n there exists a samplable distribution X over satisfiable formulas of length m and with n variables, such that for every satisfiable instance Φ (with parameters m and n) the distribution Z(Φ, U ) and the distribution Z(X, U ) are ε-statistically close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 2.28 If there exists an ε-oblivious compression for SAT (with ε ≤ 1</head><p>3 ), then the polynomial hierarchy collapses to its second level.</p><p>Proof: We show that if oblivious compression of SAT instances exists then Co-SAT ∈ AM. Consider the following interactive proof that an instance Φ ∈ SAT. The verifier chooses a random satisfiable formula Ψ ∈ X randomness r ∈ U and flips a random coin c. If c = 0 then the verifier sends ξ = Z(Φ, r) to the prover, if c = 1 he sends ξ = Z(Ψ, r). The prover then answers 1 if the compressed instance is satisfiable and 0 otherwise. The verifier accepts if the prover's answer equals his bit c and rejects otherwise.</p><p>Completeness: If indeed Φ ∈ SAT, then the prover will be able to tell whether the verifier used a coin c = 0 or c = 1, simply by testing the satisfiability of ξ and replying correctly. Soundness: Suppose that Φ ∈ SAT, then by the obliviousness property of Z the message ξ is from nearly the same distribution whether c = 0 or c = 1 and the prover is bound to error with probability 1  2 + ε. It should be noted also that the above impossibility result does not rely on the fact that the algorithm Z actually compresses but rather on the obliviousness property. 2</p><p>We note that the negative result of Fortnow and Santhanam <ref type="bibr" target="#b35">[36]</ref> regarding deterministic compression of SAT can be viewed as a further development of these ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part II: Cryptographic Applications 3 Basing Collision-Resistant Hash Functions on Any One-Way Function</head><p>Loosely speaking, a family of length-reducing functions H is called collision-resistant hash functions (CRH) if no efficient algorithm can find collisions induced by a random member of the family. That is, no PPTM can find for a randomly chosen h ∈ R H, a pair of input strings x and x such that x = x but h(x) = h(x ). In addition we want (i) An efficient algorithm for sampling from H using (possibly secret) randomness (the secret coins approach is potentially more powerful than when only public coins are used <ref type="bibr" target="#b47">[48]</ref>) and (ii) An efficient evaluation algorithm that given the description of h ∈ H and x produces h(x). As mentioned in the introduction, CRHs have wide cryptographic applications, see discussion and formal definitions in, for example, <ref type="bibr" target="#b52">[53]</ref>. We are interested in basing CRH on as general assumption as possible. There is no known construction of CRH from general one-way functions or one-way permutations. Moreover, Simon <ref type="bibr" target="#b76">[77]</ref> showed that basing CRH on one-way permutations cannot be achieved using black-box reductions <ref type="foot" target="#foot_15">18</ref> . We show that compression can be used to bridge this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.1 If there exists an errorless compression algorithm for SAT, or for any problem that is compressionhard for VC OR , then there exists a construction of a family of Collision-Resistant Hash functions (CRH) based on any one-way function.</head><p>Proof: Let (COMMIT, VERIFY) be a statistically binding computationally hiding commitment scheme based on the one-way function f (see, for instance, <ref type="bibr" target="#b36">[37]</ref> for formal definitions of commitments). Recall that the protocol COMMIT takes from the sender a string S and randomness r and after an interaction the receiver gets a commitment σ. The polynomial-time algorithm VERIFY takes the commitment σ and a possible opening to value S with randomness r and verifies that S , r are consistent with σ. One could take for example the commitment scheme of Naor <ref type="bibr" target="#b66">[67]</ref> based on the one-way function f . <ref type="foot" target="#foot_16">19</ref> In our setting we can work under the assumption that the sender (in the commitment) is honest, and in such a case, the commitment may be achieved without interaction at all <ref type="foot" target="#foot_17">20</ref> .</p><p>The CRH construction is inspired by the approach of Ishai, Kushilevitz and Ostrovsky <ref type="bibr" target="#b52">[53]</ref> for constructing collision-resistant hash functions from Private Information Retrieval (PIR). A high level description is: choose a hash function from a naive hash family with no computational hardness guarantees; in the construction below we use the selection function, i.e. a random position i. The new hash function is defined by a computationally hiding commitment to the naive hash function, and the output of the new hash function is a compression maintaining the information of the committed naive hash function when applied to the input (i.e. compression of the formula that checks that the value is what it claimed to be). Intuitively, finding a collision would require guessing with non-negligible advantage the naive hash function (the position i). The actual construction is given in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRH family H f :</head><p>Description of the hash function: Let Z be a compression algorithm for SAT. A function in the CRH collection is denoted h σ,r Z and defined by a commitment σ to a value i ∈ [m], and randomness r Z for Z. The commitment uses security parameter n.</p><formula xml:id="formula_18">Input to h σ,r Z : a string x ∈ {0, 1} m</formula><p>The CNF formula Φ σ,x is defined as follows:</p><p>• Denote by VERIFY σ the algorithm VERIFY with the input σ fixed. That is, VERIFY σ takes as inputs y and r and accepts if and only if they form a legal opening of the commitment σ (and in particular this means that y = i).</p><p>• Translate VERIFY σ into a CNF formula Φ σ (using Cook's reduction) over the variables y 1 , ..., y of y, the bits of r and dummy variables added in the reduction.</p><p>• For every j ∈ [m] define the clause C j,x = (y j1 1 ∨ y j2 2 ∨ .... ∨ y j ) if x j = 0 (where y 0 denotes ȳ and y 1 denotes y) and C j,x = 1 if x j = 1.</p><formula xml:id="formula_19">• Set Φ σ,x = Φ σ ∧ j∈[m] C j,x</formula><p>The hash function:</p><formula xml:id="formula_20">h σ,r Z (x) = Z(Φ σ,x , r Z )</formula><p>Figure <ref type="figure">1</ref>: The construction of Collision-Resistant Hash from any one-way function.</p><p>By the compressing properties of Z we get that h σ,r Z indeed shrinks its input (note that shrinkage by a single bit allows further shrinking by composition). We also have that sampling h σ,r Z from H can be done efficiently (with secret coins).</p><p>As for collisions, let x = x be two strings in {0, 1} m that form a collision, i.e., h σ,r Z (x) = h σ,r Z (x ). This equality implies, by the property of the compression, that Φ σ,x is satisfiable iff Φ σ,x is satisfiable (here we use the fact that the compression is errorless). Due to the binding property of the commitment we have that any assignment satisfying Φ σ must have y = i (recall that i is the index that σ is a commitment to). Thus the first part of Φ σ,x is only satisfied when y = i. But the second part is only satisfied if x y = 1, thus Φ σ,x is satisfied if and only if x i = 1. We get that Φ σ,x is satisfiable if and only if x i = 1 and Φ σ,x is satisfiable if and only if x i = 1. Therefore it must be the case that x i = x i , since otherwise one of them is 0 and the other one is 1 and the satisfiability of Φ σ,x is different than that of Φ σ,x . But for some j we have x j = x j and for that j we deduce that σ is not a commitment to j.</p><p>Suppose now that we have an efficient procedure that finds a collision x and x for a given (σ, r Z ) with relatively high probability (an inverse polynomial in n). Whenever the procedure indeed finds a collision, pick any j such that x j = x j . For this j we can deduce that σ is not a commitment to j. This procedure can be used to break the hiding properties of the commitment scheme, since it yields an efficient method that distinguishes the commitment value from random with advantage 1/m: given (the real) i and a random one i ∈ [m] in a random order, run the above procedure to obtain j. If j equals one of the two values i or i , then guess this one as the random one and otherwise flip a coin. This contradicts our assumptions on building blocks (namely, the one-way function).</p><p>To prove the result when using compression for any language that is compression-hard for VC OR , a similar construction is defined based on the OR of small circuits rather than CNF formulas: For every j ∈ [m] let C σ,j be the circuit that outputs one if and only if there exists randomness r such that σ is consistent with (j, r) (that is σ is a possible commitment to the value j using randomness r). Let C σ,x be the circuit that takes the OR of all C σ,j such that x j = 1 and let Z be a compression algorithm for the language OR(CircuitSAT ). We define h σ,r Z (x) = Z(C σ,x , r Z ). The proof is identical to the case of SAT. 2</p><p>Note that instead of an errorless compression we can do away with an error probability slightly smaller than 2 -m . That is, for all x we want the probability that Z(Φ σ,x , r Z ) preserves the satisfiability of Φ σ,x to be at least 1 -2 -m+u where the probability is over σ and r Z and u ≈ log m. In this case we can argue (using a union bound) that with probability at least 1 -2 -u no x exists violating the preservation of satisfiability.</p><p>We also note that the construction is inherently non-black box as it uses the code of the one-way function (via the commitment) in the application of Cook's Theorem. This is essential for the validity of the whole approach in light of the black-box impossibility of Simon <ref type="bibr" target="#b76">[77]</ref>. Theorem 3.1 implies the following corollary:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 3.2 If there exists an errorless compression algorithm for SAT or for any problem that is compressionhard for VC OR , then there exist statistically hiding, computationally binding commitment schemes based on any one-way function. The scheme requires two rounds of interaction.</head><p>The corollary follows since CRH imply statistically hiding bit commitment, see Naor and Yung <ref type="bibr" target="#b69">[70]</ref> (and Damgård, Pedereson and Pfitzman <ref type="bibr" target="#b18">[19]</ref> for commitment to many bits). Until recently, the known minimal assumptions for constructing statistically hiding bit commitments were the existence of one-way permutations <ref type="bibr" target="#b68">[69]</ref> and the more general one-way functions with known pre-image size <ref type="bibr" target="#b42">[43]</ref>. Since the publication of the earlier version of this paper statistically hiding bit commitments based on any one-way function were shown to exist <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b43">44]</ref>. However, all of these protocols <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> require many rounds of interaction -at least linear in the security parameter (this was shown to be an inherent limitation of the technique <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>). The commitments based on CRHs, on the other hand, are non-interactive, at least after the initial phase where the function h ∈ H is chosen. Such a non-interactive CRH also allows for commitment schemes with very low communication <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Basing One-Way Functions on Hard Instances</head><p>In this section we consider a method for constructing one-way functions from problems that are hard on the average over a samplable distribution. We start by defining the notion of hardness that we discuss. Denote by (x ∈ L) the boolean value which corresponds to whether x is in L or not. Definition 4.1 A language L is hard for polynomial-size circuits over a distribution D if for every family of polynomial-size circuits {C n }, for every polynomial p(•) and for all large enough n, it holds that:</p><formula xml:id="formula_21">Pr x←D(1 n ) [C n (x) = (x ∈ L)] ≤ 1 2 + 1 p(n)</formula><p>Let L be a language (not necessarily in N P). Recall that the language OR(L) with parameters m and n is defined as follows:</p><formula xml:id="formula_22">OR(L) m,n = {(x 1 , . . . , x m ) | ∀i|x i | ≤ n and ∃i such that x i ∈ L}.</formula><p>The following theorem demonstrates how compression of OR(L) can be used to construct one-way functions.</p><p>Theorem 4.2 Given a language L that is hard for polynomial size circuits over a samplable distribution D and a compression algorithm Z for OR(L), 1. If Z is errorless then there is a construction of collision resistant hash functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If Z allows a negligible error (negligible in n) there is a construction of a one-way function.</head><p>Note that there is no restriction on the complexity of recognizing L, other than it being hard for circuits over a samplable distribution. In particular L need not be in N P at all. If L does happen to be in N P, then the above statement can use a general compression of a VC OR -complete language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 4.3 let L ∈ N P be hard for polynomial size circuits over a samplable distribution D (as in Definition 4.1). If there exists a compression algorithm for SAT, or for any problem that is compressionhard for VC OR , then there is a construction of a one-way function. If the compression is errorless then there is also a construction of collision resistant hash functions.</head><p>Proof: (of Theorem 4.2) The proof follows by defining a family of hash functions h S based on a compression algorithm. The claim is that, in the errorless case, h S is a family of collision resistant hash functions (see Section 3). If Z is error prone then we define a modified hash h S and prove that it is a family of distributional collisions resistant hash functions. That is, it is hard to find a random collision for h S . This implies that h S naturally defines a distributional one-way function, which, in turn, implies the existence of one-way functions.</p><p>We begin by proving the statement in the case of errorless compression. Define a family of hash functions h S as follows. Each hash function is defined by S = (σ 0 1 , σ 1 1 , . . . , σ 0 m , σ 1 m ), a 2m-tuple of instances of length n from the domain of the distribution D. Let Z be a compression algorithm for the language OR(L). Define the hash function h S (x) = Z(σ x 1 1 , . . . , σ xn n ). Suppose there exists an efficient procedure A that finds collisions for h S over random S ∈ D 2m . More precisely, there exists a polynomial p(•) such that for infinitely many n,</p><formula xml:id="formula_23">Pr S∈D 2m [A(S) = (x, x ) such that x = x and h S (x) = h S (x )] ≥ 1 p(n) .</formula><p>Denote by D 0 the restriction of the distribution D to instances σ ∈ L. Note that D 0 is not necessarily samplable. We show that if there exists a procedure A that finds collisions over S ∈ D 2m 0 (rather than D 2m ) then A can be used to break the hardness of the language L over D. To complete the proof we then show that if A is successful over D 2m then it is also successful over D 2m 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.4</head><p>Let A be an efficient algorithm and p(•) be a polynomial such that for infinitely many n,</p><formula xml:id="formula_24">Pr S∈D 2m 0 [A(S) = (x, x ) such that x = x and h S (x) = h S (x )] ≥ 1 p(n) ,</formula><p>then there exists a family of polynomial-size circuits C A such that for infinitely many n,</p><formula xml:id="formula_25">Pr σ∈D [C A (σ) = (σ ∈ L)] ≥ 1 2 + 1 2np(n)</formula><p>.</p><p>Proof: (of lemma 4.4) By the assumption the procedure A finds a collision with probability at least 1 p(n)</p><p>(over D 2m 0 ). Therefore, there exists an index i ∈ [m] such that A finds a collision x, x such that x i = x i (x and x differed on the i th bit) with probability at least 1 np(n) (since every collision must differ in at least one bit). This index i is used in the reduction described next.</p><p>The strategy of C A for determining membership in L is as follows: Given an input σ drawn from the distribution D, create a 2m-tuple S by putting σ in the i th pair in S (for example, define σ 1 i = σ) and fill the other entries by random instances from the distribution D 0 . The non-uniform hint is used to determine i and to supply the random samples from D 0 . Now run the algorithm A on the tuple S and retrieve a collision x, x (if A was successful). If x i = x i . then answer σ / ∈ L. Otherwise, answer according to a random coin flip.</p><p>Under the restriction that σ / ∈ L, the tuple S is distributed precisely as the distribution D 2m 0 . Therefore, with probability at least 1 np(n) the algorithm A returns a collision with x i = x i and C A answers correctly that σ / ∈ L. On the other hand, under the restriction that σ ∈ L, the algorithm A cannot return a collision with x i = x i . This is due to the fact that the outcome of h S (x) corresponds to whether (σ x 1 1 , . . . σ xn n ) is in OR(L) or not (by the correctness of the compression algorithm). But membership in OR(L) is determined solely by the i th pair (all of the other pairs are not in L), and more precisely by the value of the bit x i . Therefore, a collision can only occur if the i th bit is the same in x and x . Thus, in this case the procedure C A answers "not in L" with probability exactly 1  2 . Altogether, the procedure C A answers correctly whenever x i = x i (happens with probability 1 np(n) ) and with probability 1  2 otherwise. This amounts to a success probability of</p><formula xml:id="formula_26">1 2 + 1 2np(n) . 2</formula><p>It is left to show that A is as successful on D 2m as it is on D 2m 0 . For this we define an event under which A is considered successful. In our case it is the cases that A running on S returns a collision under h S (i.e., A(S) = (x, x ) such that x = x and h S (x) = h S (x )). We say that an algorithm's success can be efficiently verified if there exists a polynomial-time computable relation R such that R(A(S), S) = 1 if and only if A was successful on S. This is clearly the case with collision finding since one can verify efficiently whether the two outputs of A are distinct and collide under h S . We conclude the first part of the theorem using the following claim: Claim 4.5 Let A be a polynomial time algorithm whose success can be verified efficiently and let D and D 0 be defined as above. Then for every polynomial p(•) and all large enough n:</p><formula xml:id="formula_27">|Pr S←D 2m [A succeeds on S] -Pr S←D 2m 0 [A succeeds on S]| &lt; 1 p(n)</formula><p>Proof Sketch: Claim 4.5 is proved by a standard hybrid argument (see e.g., <ref type="bibr" target="#b36">[37]</ref>, Section 3.2.3 ). Namely, one can use a distinguisher between D 2m and D 2m 0 in order to distinguish between D and D 0 . This in turn is enough to break the hardness of L over D. Note that non-uniformity is used in the reduction (for constructing hybrid distributions) and so this only achieves a contradiction if L is hard against non-uniform adversaries (circuits) even if the distinguisher between D 2m and D 2m 0 is actually uniform. 2 This concludes the proof for the errorless case. We now turn to the case of error-prone compression. In this case we also incorporate the string r of random coins used by Z into the hash. Define</p><formula xml:id="formula_28">h S (x, r) = (Z r (σ x 1 1 , . . . σ xn n ), r).</formula><p>Unlike the errorless case, we do not know that h S forms a CRH family (since the errors may form collisions that are easy to find). Rather, we first show that h S is a family of distributional collision resistant hash functions (DCRH) (a similar primitive was defined in <ref type="bibr" target="#b25">[26]</ref>). Loosely speaking, this is a family such that for a randomly chosen hash in the family, no efficient algorithm can find a random collision of the hash. A DCRH is useful since such a family translates to a collection of distributional one-way functions which in turn imply the existence of standard full-fledged one-way functions. A distributional one-way function is a function for which it is hard to find a random inverse of an output element (rather than just a single pre-image as in standard one-way functions). This notion was defined by Impagliazzo and Luby <ref type="bibr" target="#b49">[50]</ref>, who showed that the existence of distributional one-way functions implies the existence of standard one-way functions. We use a straightforward generalization of distributional one-way functions to collections rather than a single function.</p><p>Note, however, that we only show that h S is a DCRH when S is sampled according to the distribution D 2m 0 . In particular, the key to the hash function cannot necessarily be sampled in an efficient manner. This eventually translates to a one-way function over a domain that might not be efficiently samplable. Unfortunately, one cannot apply Claim 4.5 to show that h S forms a DCRH also when S is taken from D 2m , since the property of finding a random collision is not efficiently verifiable. Instead, we first construct a collection of one-way functions (via distributional one-way functions) in which the keys are chosen from D 2m 0 , and then apply Claim 4.5 to show that the one-wayness holds also for a collection chosen from D 2m (using the fact that finding a single inverse is an efficiently verifiable property).</p><p>More formally, as in the case of CRH, a collection of functions consists of algorithms for sampling a key S and evaluating a hash function h S over the generated key (in our context we only require that the evaluation algorithm be efficient). For a fixed key S, suppose that h S takes inputs of length . For every such key S define the distribution C S over pairs (y, y ) such that y ∈ U (n) and y is taken uniformly from the collection of the siblings of y (that is, from the set {y | h S (y) = h S (y )}). A collection is said to be a distributional collision resistant hash family (DCRH) if for every efficient algorithm A and every negligible function ε(•) the probability over the keys Pr S [A(S) is ε(n)-close to C S ] is negligibly small (i.e., n -o (1) ). We will first show that h S as defined above is a DCRH when S is sampled from D 2m 0 . This is implied directly from the following lemma (proof appears after the proof of Theorem 4.2): Lemma 4.6 Let A be an efficient algorithm, ε A (•) be a negligible function and p(•) be a polynomial such that for infinitely many n:</p><formula xml:id="formula_29">Pr S∈D 2m 0 [A(S) is ε A (n)-close to C S ] ≥ 1 p(n) ,</formula><p>then there exists an efficient circuit C A such that for infinitely many n,</p><formula xml:id="formula_30">Pr σ∈D [C A (σ) = (σ ∈ L)] ≥ 1 2 + 1 3np(n)</formula><p>.</p><p>We now show that a DCRH implies a collection of distributional one-way functions and start by defining this notion. As before, a collection of functions consists of algorithms for sampling a key S (given security parameter) and evaluating a keyed function f S over the generated key (where the sampling algorithm is not necessarily efficient in our case). A collection is said to be distributional one-way if the probability Pr S [(A(f S (U ), S), f S (U )) is ε(n)-close to (U , f S (U ))] is negligibly small (i.e., n -o (1) ). The distributions are taken over the choice of the input in U and the random coins of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim 4.7 Any DCRH also forms a collection of distributional one-way functions.</head><p>Proof Sketch: This is shown by demonstrating that a procedure A for breaking the distributional onewayness of f S can be used to break the distributional collision-resistance of this function. Define the procedure B A as follows: (i) choose a random x ∈ U (ii) compute x = A(f S (x), S) and (iii) if x = x then output (x, x ), otherwise repeat from (i). If, for a given S, the procedure A is such that</p><formula xml:id="formula_31">(A(f S (U ), S), f S (U )) is ε-close to (U , f S (U )) then the output of B A is ε-close to C S . 2</formula><p>We now use the result of <ref type="bibr" target="#b49">[50]</ref> that constructs standard one-way function from a distributional one-way function. The same transformation holds also for collections of functions (the notion that we use), since the proof holds separately for each function in the family. Thus we derive standard collections of one-way functions (for definition, see e.g., <ref type="bibr" target="#b36">[37]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4.8 (From [50], Lemma 1) If there is a collection of distributional one-way functions then there is a collection of one-way functions.</head><p>At this point we have a collection of one-way functions f S in which the key S is sampled from the distribution D 2m 0 , (which is not necessary efficiently samplable). We can now apply Claim 4.5 to show that this holds also when S is sampled from the distribution D 2m (which is efficiently samplable). We use the fact that the success of an adversary in finding an inverse of f S (x) is efficiently verifiable (unlike the success in finding a random inverse). The final step is a standard transformation from a collection of oneway function to a single one-way function (e.g., see <ref type="bibr" target="#b36">[37]</ref>, Section 2.7.4, Exercise 18). This concludes the proof of Theorem 4.2 2 Proof: (of Lemma 4.6) The proof resembles that of the errorless case (Lemma 4.4) and in fact the circuit C A is essentially the same circuit (barring the minor technicality of ignoring the r part of the inputs).</p><p>Recall that the construction in Lemma 4.4 identifies an index i for which a collision with x i = x i is found with probability at least 1 np(n) . Given an instance σ ∈ D it generates a 2m-tuple S with σ in the i th pair and the rest filled with random instances from D 0 . In Lemma 4.4 when one was given a collision with x i = x i we could immediately deduce that σ / ∈ L. This is not the case when an error is allowed, since for all we know, the algorithm A might always return an x, x , r such that Z with randomness r errs on either x or x . What we show is that if A returns a collision according to the required distribution C S , then with all but negligible probability this collision is a "good" collision (good in the sense that Z r errs on neither), in which case we can safely deduce that if</p><formula xml:id="formula_32">x i = x i then σ / ∈ L.</formula><p>Claim 4.9 Let Z be a compression algorithm for OR(L) with error probability ε Z then for any S ∈ D 2m , Pr (x,x ,r)←C S [Z r errs on either x or x ] &lt; 2ε Z .</p><p>By the assumption on A we get that with probability at least 1 np(n) the algorithm A returns a collision with x i = x i and by Claim 4.9 we have that with all but probability 2ε Z + ε A (a negligible probability) this collision implies that σ / ∈ L (recall ε A is the statistical distance of the output of a successful A from C S ). Thus the circuit C A distinguishes between σ ∈ L and σ / ∈ L with advantage at least 1 2np(n) -ε Z -ε A 2 (and in particular with advantage 1 3np(n) ). This concludes the proof of Lemma 4.6. 2</p><p>Proof: (of Claim 4.9) When sampling from C S , the first value (x, r) in the collision is simply taken according to the uniform distribution. In particular r is sampled independently of x and by the definition of compression, for every x, at most an ε Z fraction of the r's yield an error. Moreover, when ignoring the first pair, the second value (x , r) is also uniformly distributed. This is because the probability of getting a value (x , r) as the second element in a collision is the probability of hitting a sibling of (x , r) (according to h S ) as the first element and then the probability of choosing it out of all siblings. Denote the sibling set of (x , r)</p><p>by Sib (x ,r) and the combined length |x | + |r| by . Then the probability of getting (x , r) is</p><formula xml:id="formula_33">|Sib (x ,r) | 2</formula><p>for hitting Sib (x ,r) times 1 |Sib (x ,r) | for hitting (x , r) within the set. Thus each element appears as the second element with probability 1  2 . Therefore, the probability of Z r having an error on at least one of the values in the collision is at most 2ε Z (by a union bound). 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">On Everlasting Security and the Hybrid Bounded Storage Model</head><p>The bounded storage model, introduced by Maurer <ref type="bibr" target="#b61">[62]</ref>, bounds the space (memory size) of dishonest players rather than their running time. The model is based on a long random string R of length m that is publicly transmitted and accessible to all parties. Security relies on the assumption that an adversary cannot possibly store all of the string R in his memory. The requirement is that the honest parties Alice and Bob can interact using a small local storage of size n (where n is significantly smaller than m) while security is guaranteed against an eavesdropper Eve with a much larger, yet bounded storage space.</p><p>This model has enjoyed much success for the task of private key encryption. It has been shown that Alice and Bob who share a short private key can exchange messages secretly using only a very small amount of storage <ref type="foot" target="#foot_18">21</ref> , while an eavesdropper who can store up to a constant fraction of R (e.g. 1  2 m bits) learns essentially nothing about the messages (this was shown initially by Aumann and Rabin <ref type="bibr" target="#b3">[4]</ref> and improved in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b60">61]</ref> and ultimately in Vadhan <ref type="bibr" target="#b78">[79]</ref>). These encryption schemes have the important property called everlasting security (put forward in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>). Once the broadcast is over and R is no longer accessible then the message remains secure even if the private key is exposed and Eve gains larger storage capacity.</p><p>In contrast, the situation is less desirable when Alice and Bob do not share any secret information in advance. The solution of Cachin and Maurer <ref type="bibr" target="#b9">[10]</ref> for this task requires Alice and Bob to use storage of size at least n = Ω( √ m), which is not so appealing in this setting. Dziembowski and Maurer <ref type="bibr" target="#b28">[29]</ref> proved that this is also the best one can do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Hybrid Bounded Storage Model:</head><p>The inability to achieve secure encryption in the bounded storage model with memory requirements smaller than n = √ m has lead to the following suggestion that we call the hybrid BSM: Let Alice and Bob agree on their secret key using a computationally secure key agreement protocol (e.g. the Diffie-Hellman protocol <ref type="bibr" target="#b20">[21]</ref>). The rationale being that while an unbounded eavesdropper will eventually break the key, if this happens after the broadcast had already occurred, then the knowledge of the shared key would be useless by then (this should be expected from the everlasting security property where getting the shared key after the broadcast has ended is useless). This hybrid model is very appealing as it attempts to achieve everlasting security by adding assumptions on the ability of an adversary that has a strict time limit. Assumptions of this sort are generally very reasonable since all that we require is that the computational protocol is not broken in the short time period between its execution and the transmission of R. For instance, an assumption such as the Diffie Hellman key agreement <ref type="bibr" target="#b20">[21]</ref> cannot be broken within half an hour, can be made with far greater degree of trust than actually assuming the long term security of this protocol.</p><p>Somewhat surprisingly, Dziembowski and Maurer <ref type="bibr" target="#b28">[29]</ref> showed that this rationale may fail. They introduce a specific computationally secure key agreement protocol (containing a non-natural modification based on private information retrieval (PIR) protocols). If this key agreement protocol is used in the hybrid BSM setting with a specific private key scheme, then the eavesdropper can completely decrypt the encrypted message. However, their result does not rule out the possibility that the hybrid idea will work with some other key agreement protocol. For instance, using the plain Diffie Hellman key agreement may still work.</p><p>In this work we show that if compression of SAT exists then there exists an attack on the everlasting security of any hybrid BSM scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Two Possible Models</head><p>The notation we use for the storage bounds of the honest parties is n A and n B (respectively) and for Eve's bound it is m E . For simplicity we take n A = n B = n and use an abuse of notations by setting m E = m (where actually it should be that m E = 1 2 m). We define the hybrid BSM <ref type="foot" target="#foot_19">22</ref> as a setting where the running time of the eavesdropper Eve is polynomially bounded up until and during the broadcast of R, and unbounded after that. We discuss two variants of a BSM scheme. We first discuss these in the standard BSM where the eavesdropper is unbounded over time, and then compare them to the hybrid setting where computational restrictions are imposed:</p><p>• The Basic BSM Scheme: The basic scheme allows interaction only up to the start of the broadcast of R (after that only the encrypted message is sent). Thus the key is fully determined by the time the broadcast has ended. Such a scheme is fully breakable in the standard (non-hybrid) BSM (without an initial secret key) since the unbounded adversary can find some randomness consistent with Alice's view, and simulates Alice's actions and thus recover the encryption key <ref type="foot" target="#foot_20">23</ref> . Basic schemes in the hybrid BSM are interesting as they include any combination of a key agreement protocol with a private key scheme (such as the one described by <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b44">[45]</ref>). We show that if sufficiently strong compression exists then there exist attacks on any such scheme.</p><p>• The General BSM Scheme: Alice and Bob interact both before and after the broadcast of R. Dziembowski and Maurer <ref type="bibr" target="#b28">[29]</ref> show that such a scheme is breakable unless n &gt; Ω( √ m) (without initial secret keys). For the hybrid BSM, we show that if compression exists then there exists an attack on any such scheme as long as n &gt; Ω( m/p(n, log m)), for some polynomial p (related to the polynomial of the compression algorithm and to the running time of the protocol that Alice and Bob use).</p><p>Thus we prove that if compression of SAT (or of any VC OR -hard language) is feasible then the hybrid BSM is essentially no more powerful than the standard BSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Basic Hybrid BSM</head><p>Definition 5.1 (Basic hybrid BSM scheme) A basic hybrid BSM scheme consists of the following: Alice and Bob with storage bound n run a protocol Π that is polynomial in n (this could be a key agreement scheme with security parameter n). Denote by T the transcript of this protocol. Alice and Bob use their respective views of the protocol Π (i.e. the transcript T and their local randomness) to agree on at most n locations of bits from the broadcast string R that they should store. They store these bits and then use the stored bits to generate an encryption key K (the scheme requires that they agree on the same key). <ref type="foot" target="#foot_21">24</ref>We show that sufficiently strong compression of SAT can be used to break any hybrid BSM scheme. For such a scheme to be secure it is required that the key K remains secret in presence of an eavesdropper that runs in polynomial time up until and during the broadcast, but is unbounded after it. We refer the reader to <ref type="bibr" target="#b45">[46]</ref> for rigorous definitions of security (the attack presented below is not sensitive to the actual definition).</p><p>For the discussion here take K to be a one bit key. The general idea is that while the eavesdropper may not figure out in time what locations to store, he can use this transcript to save a relatively short (compressed) CNF formula whose satisfiability coincides with the value of the key K. Later, when he is given unbounded computational power, he will be able to extract this bit from the compressed formula. Theorem 5.2 If there exists a compression algorithm for SAT or for any compression-hard language for VC OR , with polynomial p 1 , then any basic hybrid BSM scheme can be broken using memory p 2 (n, log m) (where p 2 is a polynomial related to p 1 and the running time of the protocol Π).</p><p>Proof: Denote the locations of the bits that Alice and Bob store by i 1 , ..., i n . Consider the algorithm V that takes the transcript T Π and the broadcast string R as inputs and Alice's local randomness, and locations i 1 , ..., i n as a witness. The algorithm should check if the witness and inputs are indeed consistent with one another (for example, V should verify that a key agreement with the randomness of Alice, the transcript T indeed chooses the indices i 1 , ..., i n to store) and output 1 if and only if they are consistent and generate an encryption key K = 1. The main observation is that the N P language defined by this relation V is in VC 1 . Thus, if SAT has a compression algorithm then there is also a compression algorithm for all of VC 1 (from Lemma 2.17) including the language defined by V .</p><p>The attack of the eavesdropper Eve is as follows: Eve generates the verification program V and feeds the instance (T, R) to the compression algorithm for the language V . By the properties of the compression, the output is a CNF formula that is satisfiable if and only if K = 1. The length of the output is of some polynomial length p 2 (n, log m). If the polynomial p 2 is sufficiently small then the compressed instance is shorter than Eve's space bound 1  2 m, and he stores this output. Finally, at a later stage, Eve can use her unbounded powers to solve the compressed problem and retrieve the bit K.</p><p>We note that a slightly more involved argument works also with compression for VC OR . The idea is to use independent compression for the bit R(i j ) for every j ∈ [n]. Every such R(i j ) may be presented as the OR of m circuits of size p(n) each, for some polynomial p. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The General Hybrid BSM</head><p>The general scheme is like the basic one but the encryption key K is not necessarily fully defined by the end of the broadcast. In addition, the parties are allowed to interact after the broadcast is over. We note that the bounded storage key exchange scheme of Cachin and Maurer <ref type="bibr" target="#b9">[10]</ref> requires such late interaction. Definition 5.3 (General hybrid BSM scheme) The general hybrid BSM scheme consist of the following: Alice and Bob with storage bound n engage in a protocol Π 1 that runs in time polynomial in n. Denote by T 1 the transcript of this protocol. Each of the two parties Alice and Bob uses its respective view of the protocol Π 1 to determine at most n locations in the broadcast string R and stores the bits in these locations. After the broadcast they interact in a second protocol Π 2 (with transcript T 2 ) at the end of which they both agree on encryption key K (with all but negligible error probability). Theorem 5. <ref type="bibr" target="#b3">4</ref> If there exists a compression algorithm for SAT or for any compression-hard language for VC OR with compression p 1 (n, log m), then there exists an attack on any general hybrid BSM scheme where n 2 &gt; m/p 2 (n, log m) (where p 2 is a polynomial related to p 1 and the running time of the protocol Π 1 ).</p><p>However, in order to invoke Lemma 5.5, we need the samples to be taken according to the distribution S A (T 1 , R), which is defined by a uniform distribution over r A ∈ A T 1 . We will show that while sampling via the compression of search problems does not give the desired distribution exactly, it is still sufficiently close to be useful.</p><p>A closer inspection of our compression for search technique from Section 2.8 shows that we do not necessarily sample uniformly from A T 1 . However, we do sample close to uniformly, in the sense that no element in A T 1 gets more than double the probability of another element in A T 1 . We then show that taking a constant times many samples as was originally needed guarantees that amongst the stored bits we have n random samples of the random variable S A (T 1 , R), and thus we have stored enough bits from R to break the scheme.</p><p>Recall from Section 2.8 that the compression algorithm for search problems chooses a random pairwiseindependent hash function h and saves only a witness (r A , s A ) that is uniquely hashed to the value 0 by h. Since r A fully determines s A (when given T 1 and R), then without loss of generality we view the witness simply as r A . Furthermore, assume w.l.o.g. that r A is of length n. Suppose that</p><formula xml:id="formula_34">∈ [n] is such that 2 &lt; |A T 1 | ≤ 2 +1</formula><p>. Let H +2 be a family of pairwise independent hash functions with h : {0, 1} n → {0, 1} +2 for all h ∈ H +2 . Then for every r A ∈ A T 1 the probability that a random h ∈ H +2 uniquely maps r A to zero is at most 2 -( +2) (since Pr h∈H +2 [h(r A ) = 0] = 2 -( +2) ). By the pairwise independence of H it holds that for all other r A ∈ A T 1 with r A = r A we have that +2) . By a union bound over all r A ∈ A T 1 with r A = r A , combined with the probability that h(r A ) = 0, we get:</p><formula xml:id="formula_35">Pr h∈H +2 [h(r A ) = 0|h(r A ) = 0] = 1 -2 -(</formula><formula xml:id="formula_36">Pr h∈H +2 [h uniquely maps r A to 0] ≥ 2 -( +2) • 1 2 = 2 -( +3) .</formula><p>Altogether, for all r A ∈ A T 1 it holds that</p><formula xml:id="formula_37">2 -( +2) ≥ Pr h∈H +2 [h uniquely maps r A to 0] ≥ 2 -( +3) .</formula><p>Thus whenever the output of h is indeed of length + 2, the probability of sampling r A ∈ A T 1 is almost uniform (up to a factor of 2 for each element). <ref type="foot" target="#foot_23">26</ref> is no Since we repeat the compression for every choice of ∈ [n], then in particular samples are stored for the correct . By Lemma 2.27 we know that at least 1  8 of the repeated compressions indeed store information about a valid witness (a sample of r A ∈ A T 1 ). Thus, choosing, say, 9n independent h ∈ H +2 guarantees at least n samples (by a Chernoff bound, as the choices are independent). But as mentioned above, these samples are just close to uniform over A T 1 rather than truly uniform. The solution is to simply run more instances of this process, say, for 25n independent choices of h ∈ H +2 . This would guarantee that with overwhelming probability, at least 3n of these choices have a valid witness. We show that from these slightly biased samples we can extract n truly uniform samples of witnesses. This last argument follows by a method for generating uniformly distributed samples from A T 1 . At a first stage, 3n samples are taken using the unique hashing method. Now a diluting second stage is in order run to extract the actual samples: Suppose that the least likely element to be sampled gets probability p min . For any element r A that is sampled with probability p r A , keep the sample with probability p min pr A and delete it otherwise. Thus every element is eventually chosen with the same probability p min , and since p min pr A ≥ 1 2 then at least n samples are eventually chosen (with overwhelming probability). Note that the diluting stage is not necessarily efficiently computable. However, the probability p r A can be computed using the adversaries unbounded running time, since these probabilities are fully defined by the transcript T 1 which can be stored in its entirety (as it is of length polynomial in n). Therefore an unbounded eavesdropper may indeed extract n uniform samples from her view. 2 Note: In the two models that we consider we limit the honest parties to access and store at most n actual bits from the broadcast string R. This is in contrast to storing some function of R with a bound on the function's output length (an ability that the adversary is entitled to). This is a legitimate requirement as the honest parties should run algorithms that are considerably more efficient than the adversary's. It should be noted, however, that our Theorems (5.2 and 5.4) hold also if the honest players can store functions, albeit they then call for a compression algorithm for all of N P (rather than just for the lowest class VC OR ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">On Witness Retrievable Compression and Public Key Cryptography Based</head><p>on Any One-Way Function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">On Oblivious Transfer from any One-Way Function</head><p>As mentioned in the introduction, whether one-way functions are sufficient for public key cryptography is a long standing open problem. In fact, many researchers view the black-box impossibility result of Impagliazzo and Rudich <ref type="bibr" target="#b51">[52]</ref> as an indication general one-way functions are insufficient for public key cryptography. We now describe an approach to bridging this gap using witness-retrievable compression of a specific language. More precisely, we demonstrate a construction of an oblivious transfer protocol (see definition in, for instance <ref type="bibr" target="#b37">[38]</ref>) from any one-way function using such a compression algorithm.</p><p>Theorem 6.1 There exists a distribution D over CNF formulas such that given a witness-retrievable compression algorithm for formulas from the distribution D one can construct an Oblivious Transfer (OT) from any one-way function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>The construction actually builds a Private Information Retrieval (PIR) protocol, and then uses the construction of Di Crescenzo, Malkin and Ostrovsky <ref type="bibr" target="#b19">[20]</ref> to build an OT protocol from the PIR protocol.</p><p>Recall that a PIR protocol has a sender with a database of size m and a receiver that chooses to learn one entry from the database (see precise definition in, e.g <ref type="bibr" target="#b19">[20]</ref>). It is required that the receiver learns the bit of his choice, but a computationally bounded sender learns essentially nothing about this choice. In addition, the total communication should be strictly smaller than m.</p><p>Let f be a one-way function and take (COMMIT, VERIFY) to be a commitment based on the one-way function f (as in Section 3). In this proof we work under the assumption that the parties are semi-honest (that is, the parties follow the protocol as prescribed and are only allowed to try and infer extra information from the transcript of the protocol). The semi-honest assumption is justified by the compiler of Goldreich, Micali and Wigderson <ref type="bibr" target="#b38">[39]</ref> that showed how to transform a semi-honest protocol into one against malicious parties (again, the only needed cryptographic assumption is the existence of a one-way function). Consider the protocol in Figure <ref type="figure" target="#fig_1">2</ref>. 1. Bob commits to i: Bob commits to i with randomness r B , Alice receives σ = COMMIT(i, r B ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Alice computes Φ:</head><p>The CNF formula Φ is defined as follows:</p><p>• Denote by VERIFY σ the algorithm VERIFY with the input σ fixed. That is, VERIFY σ takes as inputs and r and accepts if and only if they form a legal opening of the commitment σ (and in particular this means that x = i).</p><p>• Translate VERIFY σ into a CNF formula Φ σ (using Cook's reduction) over the variables x 1 , ..., x of x, the bits of r and dummy variables added in the reduction.</p><p>• For every j ∈ [m] define the clause  It remains to show that the protocol P IR f is indeed a PIR protocol. Due to the fact that the commitment is binding (up to a negligible error), an assignment satisfying Φ σ must have x = i (recall that i is the index that Bob committed to). Thus the first part of Φ is only satisfied when x = i. But the second part is only satisfied if D[x] = 1, thus Φ is satisfied if and only if D[i] = 1. By the property of the compression algorithm, also Ψ is satisfiable iff D[i] = 1. Hence, using the witness-retrievable properties of the compression, Bob figures out whether or not Ψ is satisfiable, and learns the bit D[i] (up to a negligible error).</p><formula xml:id="formula_38">C j = (x j1 1 ∨ x j2 2 ∨ .... ∨ x j ) if D[j] = 0 (where x 0 denotes x and x 1 denotes x) and C j = 1 if D[j] = 1. • Set Φ = Φ σ ∧ j∈[m] C j<label>3</label></formula><p>The second property is that the sender Alice learns no computational information about Bob's choice. This follows directly from the guarantees of the commitment scheme (note that Bob does not send any information outside of the commitment). The third and final requirement regards the length of the communication. But the length of the communication is a fixed polynomial in p(n) (depending on the commitment protocol and the parameter of the compression algorithm). So choosing a large enough databases with m &gt; p(n) guarantees a non trivial PIR protocol and hence an OT protocol. 2 Note that the OT protocol derived in Theorem 6.1 is a one-round protocol (that is, one message sent from the receiver followed by one message from the sender). This follows from the construction of the PIR protocol and the construction of <ref type="bibr" target="#b19">[20]</ref> that preserves the number of rounds. One implication of this fact is that such an OT protocol may be used to construct a two round key agreement scheme, that in turn maybe used to construct a public key encryption. In general, this is achieved by fixing the first message of the protocol to be as the public key. Formally: Corollary 6.2 If there exists a witness-retrievable compression algorithm for a specific type of SAT instances, then based on any one-way function one can construct a public key encryption scheme (PKE) that is semantically secure against chosen plaintext attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">On the Limitation of the Witness Retrievability Property</head><p>Witness-retrievable compression is defined (Definition 1.6) as a compression with an additional PPT algorithm W such that for every witness w x for R L it holds that w y = W (w x , Z(x)) is a witness for Z(x) ∈ L . Recall that nearly all of the examples of compression algorithms (in Sections 2.1 and 2.10) are in fact witness-retrievable (the exception being compression of general sparse languages, Definition 2.3). This property is essential to the success of the construction of the OT protocol in Theorem 6.1 (without it the receiver would have to run in time that is super-polynomial). In this section we show that if one-way functions exist then a compression algorithm for SAT cannot be witness-retrievable (this regards the general language SAT rather than a specific distribution of instances as generated in Theorem 6.1). Moreover, this statement also holds for other general languages mentioned in Theorem 6.1 (that are potentially easier to compress than SAT). In particular, there is no witness-retrievable compression for the Clique language or for the language OR(SAT ) (that is complete for VC OR ). We give the formal statements below with respect to the language OR(SAT ) and deduce the statements for SAT and Clique as corollaries.</p><p>We also rule out other natural definitions of witness-retrievability that would have been sufficient for the proof of Theorem 6.1 to go through. Suppose we relax the witness-retrievability requirement to hold only with some probability ε, then we show that if one-way functions exist then this probability ε has to be very low, at most an inverse polynomial in m. Such a low probability of success is not sufficient for the OT construction in Theorem 6.1 to follow (we note though, that witness-retrievability with this low success probability is still sufficient for the cryptanalytic result in <ref type="bibr" target="#b27">[28]</ref>). We then show that the same situation also holds for languages that are guaranteed to have unique witnesses (i.e. unique-SAT and unique-OR(SAT )). This is of relevance since the instances being compressed in the proof of Theorem 6.1 all have at most a single witness. <ref type="foot" target="#foot_24">27</ref>We emphasize again that the OT construction may still be successful under the compression of formulas of the specific type that are generated in the proof. However, we cannot generalize this method to work with compression of a more standard language.</p><p>On the Impossibility of Perfect Witness Retrieval: Recall that the language OR(SAT ) takes as an input a list of m CNF formulas (each of length n) and accepts if at least one of the formulas is satisfiable. Consider the following way of generating an instance of OR(SAT ). Take m bit commitments σ 1 , . . . , σ m , each with security parameter n (see proof of Theorem 3.1 for definition and discussion of commitments in our context). For each commitment σ i , generate using Cook's Theorem a CNF formula φ σ i that is satisfiable if and only if σ i is a commitment to 1. As an instance of OR(SAT ) we take the OR of the m CNF formulas φ σ 1 , ..., φ σm . We denote this instance by φ(σ 1 , . . . , σ m ). Denote by w σ i a satisfying assignment for φ σ i (such an assignment can be generated by an opening of σ i to the value 1). The assignment w σ i also serves as a witness for φ(σ 1 , . . . , σ m ) ∈ OR(SAT ). Our first impossibility result is for compression of OR(SAT ) with errorless witness-retrievability. ¿From the hiding property of commitment schemes it holds that these two distributions are indistinguishable, i.e. given a list L of m -1 commitments, no computationally bounded distinguisher can tell with nonnegligible bias whether L was generated by L 0 or L 1 . We will show that if the premise of the claim is false, it is possible to distinguish the two distributions (without knowledge of the openings to any of the commitments in the list).</p><p>Given a list L of m -1 commitments, the distinguisher generates σ 0 i and σ<ref type="foot" target="#foot_25">1</ref> i and the corresponding witnesses. He then generates a formula φ by adding σ 0 i to the i th position in the list L, and runs the compression on φ. The distinguisher then runs w = W (Z(φ), w σ 1 i ) and checks whether w is a witness to Z(φ). By the assumption, w will indeed be a witness every time that φ is satisfiable. On the other hand, w cannot be a witness if φ is not satisfiable, simply by the properties of the compression. Thus if w is indeed a witness for Z(φ) then it must be that φ ∈ OR(SAT ) and there is some commitment to 1 in the list and thus L was generated from L 1 . Otherwise, it means that φ ∈ OR(SAT ) and the original list was from L 0 (ignoring the negligible probability that L 1 generates a list containing only commitments to 0).</p><formula xml:id="formula_39">Now if Pr[Y i = 1|X i = 0] ≥ 1 q(n)</formula><p>for some polynomial q(•), then the distinguisher follows the same procedure with the difference that:</p><formula xml:id="formula_40">• If w = W (Z(φ), w σ 1 i</formula><p>) is a witness for Z(φ) then output L 1 .</p><p>• If w is not a witness flip a coin and output either L 0 or L 1 accordingly.</p><p>In case w was indeed a witness, the distinguisher is guaranteed to be correct. Therefore, the above procedure gives an advantage 1 2q(n) in distinguishing between L 0 and L 1 , contradicting the hiding properties of the commitment scheme. 2 Note that the distributions L 0 and L 1 will be useful also in the discussion of the unique witnesses case (Lemma 6.6). 2</p><p>On Non-Perfect Witness Retrievability: We now show that the witness-retrieval procedure is possible only if its success probability is sufficiently low (we denote the success probability by 1 q(n,m) ). We upper bound the success probability by a function of the rate of compression that the algorithm Z achieves (we denote by p(n, m) the polynomial that bounds the length of the output of Z, i.e. the compressed instance). Lemma 6.5 Suppose one-way functions exist and suppose that (Z, W ) is a witness-retrievable compression for OR(SAT ) such that for every φ with parameters m, n the following holds:</p><p>1. The compression parameter |Z(φ)| ≤ p(n, m) 2. The success probability of W is at least 1 q(n,m) where probability is over the random coins of Z and W as well as the choice of the witness.</p><p>Then q(n, m) ≥ Ω( m p(n,m) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof:</head><p>The proof uses the same setting as in the proof of Lemma 6.3. Once more, the sender sends a compressed value Z(φ x ) to the receiver that runs the procedure Rec and we view this process as a channel between a sender who holds the random variables X = X 1 , ..., X m to a receiver who gets the random variables Y = Y 1 , ..., Y m . Only this time if X i = 1 it is not guaranteed that also Y i = 1 (since the witnessretrievability is no longer perfect). Instead, our assumption on the success probability of W translates to</p><formula xml:id="formula_41">Pr[Y i = 1 | X i = 1] ≥ 1 q(n,m) for a random i. Since X i is a uniformly distributed bit then Pr[Y i = 1] ≥</formula><p>In addition, Claim 6.4 states that for every i it holds that Pr[Y i = 1 | X i = 0] ∈ neg(n). Thus, if Y i = 1 then X i = 1 with overwhelming probability and therefore H(X i | Y i = 1) ∈ neg(n) for every i (where H denotes the Shannon entropy). We use the above mentioned facts to provide an upper bound on the average entropy of X i (average over i) when given Y :</p><formula xml:id="formula_42">E i [H(X i | Y )] = E i [P r(Y i = 1)H(X i | Y i = 1) + P r(Y i = 0)H(X i | Y i = 0)] ≤ 1 2q(n, m) • neg(n) + (1 - 1 2q(n, m) ) • 1 ≤ 1 - 1 2q(n, m) + neg(n)</formula><p>The first inequality is true since H(X i | Y i = 0) ≤ 1 for every i. We deduce an upper bound on the entropy of X when given Y :</p><formula xml:id="formula_43">H(X|Y ) ≤ i H(X i | Y ) = mE i [H(X i | Y )] ≤ m(1 - 1 2q(n, m) + neg(n))</formula><p>Hence, when the receiver gets Z(φ x ) (and can generate Y ), the receiver's entropy of X deteriorates by</p><formula xml:id="formula_44">H(X) -H(X | Y ) ≥ Ω( m q(n, m)</formula><p>).</p><p>This can only happen if the sender sent at least Ω( m q(n,m) ) bits to the receiver, and thus p(n, m) ≥ Ω( m q(n,m) ) as required. 2</p><p>Note that the construction of OT protocols from one-way functions in Theorem 6.1 requires that the compression rate p(n, m) ≤ O(m 1-) for some constant &gt; 0. Thus, when put in the context of constructing OT protocols, the above lemma states that a useful compression algorithm for OR(SAT ) cannot have witness-retrievability with probability that is better than O( 1 m ). In order to achieve non-trivial PIR protocols (via Theorem 6.1), one would require witness-retrievability with a better success probability.</p><p>On Witness Retrieval with a Unique Witness: The limitations on witness-retrievability hold also when there is only a single witness, which is the case in our cryptographic applications. For this we consider the promise problem OR(SAT ) U that is OR(SAT ) with a guarantee that every instance has at most one satisfying assignment. We generate the interesting instances of OR(SAT ) U as above, from sets of commitments. In this case the set of commitments should be such that at most one of the commitments is to the value 1. For simplicity we also assume that each commitment has a unique opening (this may be achieved using one-way permutation), so overall such instances have the unique witness property. Lemma 6.6 Suppose one-way permutations exist and suppose that (Z, W ) is a witness-retrievable compression for OR(SAT ) U such that for every input φ with parameters m, n the following holds:</p><p>1. The compression parameter is |Z(φ)| ≤ p(n, m) 2. The success probability of W is at least 1 q(n,m) for a polynomial q(•, •) where probability is over the random coins of Z and W . Then 1 q(n,m) -p(n,m) m ∈ neg(n).</p><p>Proof: Suppose that there is a witness-retrievable compression (Z, W ) for OR(SAT ) U that succeeds with probability 1 q(n,m) . In similar fashion to the proof of Claim 6.4 we will show that in such a case one can efficiently distinguish if a list of m -1 commitments was generated by the distribution L 0 or by the distribution L 1 . Recall that the distribution L 0 is a random choice of m -1 commitments to 0 while the distribution L 1 is a choice of m -1 random commitments (commitments to either 0 or 1). The distinguisher works without knowledge of the openings to any of the commitments, thus contradicting the hiding properties of the commitment scheme.</p><p>The distinguisher generates a random commitment σ 1 to 1 along with its witness w σ 1 . Now, given a list L of m -1 commitments, the distinguisher creates an instance φ by adding σ 1 in a random position in the list L, and runs the compression on φ. The distinguisher then tries to retrieve a witness to Z(φ) using the opening w σ 1 . In the case that L ∈ L 0 then φ is an instance of OR(SAT ) U and thus by the assumption the distinguisher will retrieve a witness with probability at least 1 q(n,m) . On the other hand, if L ∈ L 1 then the instance φ is a general instance of OR(SAT ) (without the promise of the unique witness). Lemma 6.5 states that there exists a φ for which the witness-retrieval succeeds with probability at most p(n,m) m . A more careful inspection of the proof of Lemma 6.5 shows that this statement also holds for a randomly chosen φ (generated by choosing m random commitments not all of which are to 0). Thus, if L ∈ L 1 then the witness-retrieval succeeds on φ with probability at most p(n,m) m (with probability taken over the choice of L ∈ L 1 and the randomness of the distinguisher). Overall, the distinguisher accepts with probability at least 1 q(n,m) when L is from L 0 and at most p(n,m) m when L is from L 1 . So if 1 q(n,m) -p(n,m) m is larger than a polynomial fraction in n, then this procedure has a distinguishing advantage between L 0 and L 1 , contradicting the security of the commitment scheme. 2 All our results have been stated for the language OR(SAT ). However, they may be applied for other languages such as SAT and Clique. In particular, we get the statement with respect to SAT as a corollary (since a compression for SAT can be used as a compression for OR(SAT ) via the same reduction as in Lemma 2.17). Corollary 6.7 Suppose one-way functions exist and let (Z, W ) be a witness-retrievable compression for SAT (or for Unique-SAT), such that for every input φ with parameters m, n the following holds:</p><p>1. The compression parameter |Z(φ)| ≤ p(n, m) 2. The success probability of W is at least 1 q(n,m) where probability is over the random coins of Z and W as well as the choice of the witness.</p><p>Then q(n, m) ≥ Ω( m p(n,m) ).</p><p>7 Discussion and Open Problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Discussion -A Unified Perspective of the Applications</head><p>In sections 3,4 and 6 we presented three separate applications of compression that have a similar flavor: A CRH from one-way functions using perfect compression (Section 3), a CRH/one-way function from hardon-average language using perfect/imperfect compression (Section 4), and PIR/OT from one-way function using witness-retrievable compression (Section 6). These constructions have a common underlying principle and can be viewed as variants on this main theme. The basic observation is that compression of OR(L), where L is a "hard on average" language, can be used to construct private information retrieval (PIR) protocols in which the receiver is unbounded. This construction follows by generalizing a standard approach in the design of PIR protocols (e.g. <ref type="bibr" target="#b58">[59]</ref>). In this method the receiver generates a sequence of n commitments hiding the characteristic vector of its selection, and the server computes an encoding of the XOR (alternatively, OR) of all of the committed values which correspond to the 1-entries of the database. When decoded, this value amounts to the bit that the receiver was seeking. The non-triviality in the PIR protocol stems from the fact that the length of the latter encoding can be made shorter than the length of the database. Typically this is achieved by using homomorphic properties of specific commitment schemes.</p><p>In our case, this is achieved via the compression of OR(L) (where L is the language defined by the commitment scheme). Thus the use of compression here can be viewed as a relaxation of the traditional use of homomorphic commitments. The result of Section 3 follows from this general scheme combined with the observation that PIR with an unbounded receiver implies CRH (via the reduction of <ref type="bibr" target="#b52">[53]</ref>). Section 6 observes that the receiver in the PIR protocol can be made efficient if the underlying compression is witness-retrievable. The results of Section 4 follow by further observing that the CRH construction doesn't require the committed vector to be known to anyone, and moreover this construction remains collision resistant even if the committed vector is uniformly random (otherwise one could break the semantic security of the commitment). Thus the commitments can be replaced by random instances of a hard-on-average language. When compression is imperfect, the CRH is relaxed to a distributional variant which still implies a one-way function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Future Directions and Open Problems</head><p>The issue of compressibility and the corresponding classification introduced in this work raise many open problems and directions. The obvious one is to come up with a compression algorithm for a problem like SAT or Clique (or some VC OR -complete or hard problem). Note that the new impossibility results of Fortnow and Santhanam <ref type="bibr" target="#b35">[36]</ref> do not rule out the possibility of error prone compression for these languages. We have seen compressibility of some interesting N P languages and hence the question is where exactly is boundary between compressibility and incompressibility. We tend to conjecture that it is in the low levels of the VC hierarchy. We view PCP amplification methods such as the recent result of Dinur <ref type="bibr" target="#b22">[23]</ref> as potential leads towards achieving compression. This is because these results show a natural amplification of properties on a graph, and could potentially be combined with a simple compression of promise problems (such as the example for GapSAT in Section 2.10). The main issue is doing the PCP amplification without introducing many new variables. Due to the recent results of <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b14">[15]</ref> the underlying PCP in such an approach must also introduce some level of errors.</p><p>In particular, the following task would suffice for achieving non-trivial compression: given CNF formulae φ 1 and φ 2 (not necessarily with short witnesses) come up with a CNF formula φ that is (1) satisfiability of the new formula coincides with very high probability with the satisfiability of φ 1 ∨ φ 2 and (2) shorter than the combined lengths of φ 1 and φ 2 ; By shorter we mean of length (1 -)(|φ 1 | + |φ 2 |). The reason this is sufficient is that we can apply it recursively and obtain non-trivial compression for OR(SAT ), which implies the cryptographic applications.</p><p>Short of showing a compression for general complexity classes, it would be interesting to come up with further interesting compression algorithms as well as to obtain more hardness results. For instance, is Clique or any other embedding problem complete for VC 1 ? Is there a natural and simple complete problem for VC 1 ? Also, the VC hierarchy is by no means the ultimate classification with respect to compressibility. One can hope to further refine this classification, especially within the confines of VC 1 . Moreover, it would be interesting to find connections of the VC hierarchy to other classifications (e.g., in the style of Feige <ref type="bibr" target="#b30">[31]</ref> for average case complexity and approximation algorithms and Chen et al. <ref type="bibr" target="#b12">[13]</ref> for parameterized complexity and subexponential algorithms).</p><p>Regarding the cryptographic application of getting a CRH from one-way functions (Theorem 3.1), one issue is how essential is the requirement that the compression will be errorless (this question is even more interesting due to the new impossibility results of <ref type="bibr" target="#b35">[36]</ref>). We know that this requirement can be relaxed to hold with an error that is exponentially small in m. However it is unknown whether a CRH can be constructed from any one-way function using a compression algorithm that errs with probability that is, say, exponentially small in n and log m. Note that using typical amplification techniques for CRH is unsuccessful. For example, taking a concatenation of several independently chosen hash functions on the same input fails, since reducing the adversary's success probability to allow using the a union bound requires using too many (Ω(m)) independent functions for the overall hash to still be shrinking. Another question in this regard is whether compression of languages outside of N P is possible. For example, applications such as the construction of a CRH (in sections 3 and 4) can work also with compression of the language AN D(L) (which may not have a short witness) or XOR(L) (not in N P) rather than OR(L).</p><p>Especially in light of the apparent hardness of compression, it is valuable to understand what are the implications of incompressibility. We have seen the necessity of incompressibility for the security of schemes in the hybrid bounded storage model. Other examples are the previously mentioned works of Dubrov and Ishai <ref type="bibr" target="#b25">[26]</ref> regarding derandomization and Dziembowski <ref type="bibr" target="#b27">[28]</ref> with respect to forward-secure storage. In order to gain confidence in an incompressibility assumption when used in a cryptographic setting it is important to come up with an efficiently falsifiable assumption<ref type="foot" target="#foot_27">28</ref> of this nature (see <ref type="bibr" target="#b67">[68]</ref>).</p><p>Finally we feel that we have just scratched the surface of an important topic and in the future there will be other implications of compressibility or the impossibility of compression, whether in cryptography or in other areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 2 . 13 (</head><label>213</label><figDesc>Depth k CircuitSAT) For any k ≥ 2 consider the N P problem called Depth k CircuitSAT: Input: a circuit C of size m and depth at most k over n variables. Membership: C ∈ Depth k CircuitSAT if there exists a satisfying assignment to C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 .</head><label>2</label><figDesc>14 (LocalCircuitSAT) Input: A string x of length m and a circuit C over n + n • log m variables and of size (n + n • log m). 13 Membership: (x, C) ∈ LocalCircuitSAT if there exists a list I of n locations in x such that C(x(I), I) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Protocol</head><label></label><figDesc>P IR f : Alice's input: database D of m bits. Let D[i] denote the ith bit in D. Bob's input: index i ∈ [m]. Denote the bits of i by i 1 , ..., i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The construction of a PIR protocol from any one-way function.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This example comes only as an illustration. We later consider the more general question of compression to instances that are not necessarily of the same language.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that since our requirement for compression is only relevant for problems where m n, an N P-complete problem such as</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p><ref type="bibr" target="#b2">3</ref>-SAT (where all clauses have exactly 3 literals) is irrelevant for compression as m is already at most O(n 3 ) in such formulas.<ref type="bibr" target="#b2">3</ref> Suppose that every formula can be compressed by a single bit, then sequentially reapplying compression to the input will result in a very short formula that may be solved by brute enumeration.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Suppose that there exists an errorless compression algorithm Z for L then define L to be the language of all Z(x) such that x ∈ L. Then, for every y ∈ L a verification algorithm takes as a nondeterministic witness a value x along with a witness to x ∈ L and verifies that indeed y = Z(x). Thus L is in N P(poly(m)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>I.e. a promise problem were either the formula is satisfiable or every assignment does not satisfy a relatively large number of clauses.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>The construction of CRH requires that the error probability of compression algorithm will be zero. This can be slightly relaxed to an error that is exponentially small in m (rather than n).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>This does not contradict the compressibility of SAT, since the cryptanalytic result in the hybrid BSM model is not black-box and thus is not preserved in the presence of a random oracle.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>Note that finding an algorithm that actually solves SAT would render more natural tasks (e.g., symmetric encryption) possible in the random oracle model and impossible without it. Of course finding a compression algorithm seems more likely and does not rule out (most of) cryptography.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>The first version of this paper<ref type="bibr" target="#b45">[46]</ref> (datedFeb 17, 2006)  did not contain this theorem and was hence more optimistic on the possibility of finding a witness preserving compression algorithm for SAT.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>The witness for Clique is a choice of k vertices from the graph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10"><p>The choice of the circuit to be of size n (over n variables) is arbitrary and other polynomial functions suffice as well. Furthermore, such a circuit of small size may be meaningful since not all the variables have to be used and some might be just dummy variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11"><p>This problem takes m values and a target value and asks if there is a small (size n) subset of the values that adds up to the target.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12"><p>It is interesting to note that whereas the problem of finding a path of length n is fixed parameter tractable<ref type="bibr" target="#b1">[2]</ref>, Feige and Kilian<ref type="bibr" target="#b31">[32]</ref> give indications that the Clique problem is hard for small n (via subexponential simulations). This illustrates that such differences in parameterized complexity are not necessarily preserved in the classification of compression.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_13"><p>In a nutshell, the reduction creates a triangle for each variable xi of the formula. One of the nodes of the triangle is identified with the positive variable and another with its negation while the third is connected only to the other two. In addition, a vertex is created for each clause in the formula. Now, each literal is connected with all of the clauses that it appears in. The generated graph has a dominating set of size n iff the formula is satisfiable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_14"><p>The circuit C is actually an instance for the language OR(CircuitSAT ).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_15"><p>Simon's black-box impossibility result<ref type="bibr" target="#b76">[77]</ref> is actually stated for the public coins version of CRH rather than the secret coins variant that we discuss. However this separation also holds for the case of secret coins (as pointed out in<ref type="bibr" target="#b47">[48]</ref>).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_16"><p>To be more exact, the commitment of<ref type="bibr" target="#b66">[67]</ref> can be based on the pseudorandom generator of Håstad et al.<ref type="bibr" target="#b46">[47]</ref> which in turn can be based on the function f .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_17"><p>In the scheme of Naor<ref type="bibr" target="#b66">[67]</ref>, the receiver is required to provide the sender with a (public) random string. Certainly, an honest sender can generate this string by himself without harming the properties of the commitment. Thus in such a setting, the sender can generate the commitment without interaction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_18"><p>Alice and Bob only require n = O( + log m + log 1 ε ) bits of memory to exchange an bit message with error ε.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_19"><p>The hybrid BSM model and notions of everlasting security in this model are formally defined in<ref type="bibr" target="#b44">[45]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_20"><p>Since Alice must be able to decrypt the message then simulating Alice with any randomness that is consistent with the transcript must output the same key.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_21"><p>The discussion is also valid if the parties are required to reach an agreement with all but negligible probability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_22"><p>When considering nA and nB that are not necessarily identical, the actual requirement is for Eve to store nB samples of SA(T1, R) (each sample is of length nA). Subsequently the scheme is insecure as long as nA • nB &lt; O(mE).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_23"><p>Note that the almost uniformity of the samples actually holds for every choice of the parameter . Therefore, this property can be relied on even if the correct choice of is unknown.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_24"><p>The relevant instances in Theorem 6.1 actually have a unique witness only if there exists a commitment scheme that has only a unique opening. As this is not necessarily the case when given any one-way function, we consider for simplicity the case of one-way permutations (that guarantee a unique opening commitment scheme).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_25"><p></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_26"><p>2q(n,m) for a random i.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_27"><p>An efficiently falsifiable assumption is one for which it is possible to create verifiable challenges so that if the assumption is false then the challenges can be solved.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We thank Yuval Ishai for many helpful comments and specifically for pointing out that the CRH construction does not require witness-retrievability. We are also grateful to Alon Rosen, Ronen Shaltiel and Gillat Kol for their comments on the presentation and Hovav Shacham for conversations regarding witness-retrievable compression. Finally we thank the anonymous referees for FOCS and SICOMP, Salil Vadhan and Mike Langston for their helpful comments and suggestions and to Mike Fellows for pointing out some references.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> ‡  <p>Research supported by a grant from the Israel Science Foundation. A short version of this paper appeared in FOCS 2006.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof: Denote by A T 1 the set of all possible random strings r A of Alice that are consistent with the transcript T 1 (recall that T 1 is executed in full before the string R is broadcast and therefore A T 1 is fully determined by T 1 ). Let s A = S A (T 1 , R, r A ) denote the bits that Alice stores at the end of the broadcast when running with randomness r A , transcript T 1 and broadcast string R. Finally, denote by S A (T 1 , R) the random variable that is S A (T 1 , R, r A ) for a uniform choice of r A ∈ A T 1 . That is, S A (T 1 , R) is distributed over all possible s A 's that Alice may store when running with transcript T 1 and broadcast string R. Similarly we denote by S B (T 1 , R) the corresponding possible view of Bob.</p><p>The proposed strategy for Eve is to store n independent samples from the random variable S A (T 1 , R). For this purpose we denote by S E (T 1 , R) (for any R and T 1 ) the random variable that consists of n independent samples of S A (T 1 , R). An important observation due to Maurer <ref type="bibr" target="#b62">[63]</ref> is that the uncertainty of Eve regarding the underlying key is upper bounded by the mutual information between the views of Alice and Bob given Eve's view. Formally, the relevant quantity is</p><p>The success of Eve's strategy follows from the two lemmata below, the first due to Dziembowski and Maurer <ref type="bibr" target="#b28">[29]</ref> and the second due to Maurer <ref type="bibr" target="#b62">[63]</ref>: .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5.5 ([29])</head><p>and S E (T 1 , R) be defined as above. Then:</p><p>. Lemma 5.6 ([63], Theorem 3) Let V A , V B and V E be random variables denoting the respective views of Alice, Bob and Eve. Let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>be procedures of Alice and Bob to extract a mutual secret key from their respective views, such that K</head><p>A strategy for an eavesdropper is therefore to store n independent samples of the random variable S A (T 1 , R). Lemmata 5.5 and 5.6 assert that Eve's entropy of the encryption key K is at most n 2 /m in such a case. A crucial point is that an encryption key that has entropy significantly lower than 1 (from Eve's point of view) can be predicted with high probability by an unbounded Eve, rendering the scheme insecure. Thus if an eavesdropper has O(m) storage capacity then the scheme is insecure as long as n 2 = O(m). 25  Lemma 5.5 was used in <ref type="bibr" target="#b28">[29]</ref> in a setting where the eavesdropper is unbounded and can hence sample the random variable S A (T 1 , R). However, in our setting the eavesdropper is computationally bounded and does not have the power to generate this distribution. Instead, we use compression to store information about samples of S A (T 1 , R) to be extracted after the broadcast is over (when the eavesdropper is computationally unbounded).</p><p>The main idea is to use compression for search problems, as was discussed in Section 2.8. Define the N P language L A as follows:</p><p>The first thing to note is that L A is in VC OR . This is shown once more by the same argument as in Theorems 5.2 or 3.1, and based on the fact that the protocol Π 1 is polynomial-time in n. Once this is established, then given a compression algorithm for VC OR we invoke Theorem 2.26 to get a compression algorithm to the search problem associated with L A . Running this compression once, allows us to extract a witness to L A and in particular to get one sample s A of a consistent view of Alice. Running this n times supposedly gives n samples of such a view, which suffices to break the scheme by Lemma 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 6.3 If one-way functions exist then there is no witness-retrievable compression for OR(SAT ) with perfect witness-retrieval.</head><p>Proof: The proof follows by showing that a witness-retrievable compression Z for OR(SAT ) can be used to transmit an m bit string between two parties with sub-linear communication. As a setup stage, the receiver generates m random commitments to 1 and m random commitments to 0 and sends them to the sender. Denoted these by (σ 1 1 , . . . , σ 1 m ) and (σ 0 1 , . . . , σ 0 m ) respectively. For every string x ∈ {0, 1} m denote φ x = φ(σ x 1 1 , . . . , σ xm m ) (where x i denotes the i th bit of x). In order to send string x ∈ {0, 1} m the sender sends Z(φ x ) to the receiver. We claim that the receiver can, with overwhelming probability, learn the string x, thus contradicting the fact that the message sent is significantly shorter than m. Note that the receiver knows witnesses w σ 1 i for all i and that a witness for φ x ∈ OR(SAT ) consists of a witness w σ 1 i of a φ σ 1 i that is included in φ x . The receiver extracts x as follows:</p><p>Procedure Rec on input Z(φ x ):</p><p>• For every i ∈ [m]:</p><p>If w is a witness for Z(φ x ) then set y i = 1, otherwise, set y i = 0.</p><p>• Output y = y 1 , ..., y m .</p><p>Denote by X i the random variable of the i th bit of x and by Y i the random variable of the corresponding output of Rec. We view the process as a channel between a sender who holds the random variables X = X 1 , ..., X m to a receiver who gets the random variables Y = Y 1 , ..., Y m and claim that with overwhelming probability Y = X.</p><p>If X i = 1 then the opening of σ 1 i should yield a witness for Z(φ x ), from the perfect witness-retrievability, and thus Y i = 1. We should show that if X i = 0, then indeed Y i = 0 (up to a negligible error). Note that X is uniformly distributed over {0, 1} m , whereas Y is determined by the random choice of commitments (σ 1 1 , . . . , σ 1 m ) and (σ 0 1 , . . . , σ 0 m ), the random coins of Z and W and the random variable X. Claim 6. <ref type="bibr" target="#b3">4</ref> Let X and Y be the random variables described above. Then for every i ∈ [m] (possibly related to m, n) and every polynomial q(•) and all sufficiently large n,</p><p>Note that the Claim 6.4 holds also if the underlying witness-retrieval algorithm is non-perfect. This will be used in the proof of Lemma 6.5.</p><p>Proof: Suppose that the claim is false, that is, for some q(•), for infinitely many n and some i (possibly related to n), Pr[Y i = 1|X i = 0] ≥ 1/q(n). For simplicity we first deal with the case that Pr[Y i = 1|X i = 0] = 1. In other words, W (Z(φ x ), w σ 1 i</p><p>) always outputs a witness for Z(φ x ). Consider the two distributions L 0 and L 1 on lists of m -1 commitments:</p><p>• Distribution L 0 is defined by a random and independent choice of m -1 commitments to 0.</p><p>• Distribution L 1 is defined by first choosing at random a string V 1 , V 2 , . . . , V m-1 ∈ {0, 1} m-1 and then generating m -1 independent commitments to V 1 , V 2 , . . . , V m-1 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NP-complete problems and physical reality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aaronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGACT News</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="52" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Color-coding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="844" to="856" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Everlasting security in the bounded storage model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Rabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1668" to="1680" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information theoretically secure communication in the limited storage space model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Rabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;99</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1666</biblScope>
			<biblScope unit="page" from="65" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How to go beyond the black-box simulation barrier</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">42nd IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An uninstantiable random-oracle-model scheme for a hybridencryption problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boldyreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Palacio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -EUROCRYPT</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3027</biblScope>
			<biblScope unit="page" from="171" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the theory of average case complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences (JCSS)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="219" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The undirected feedback vertex set problem has a poly() kernel</title>
		<author>
			<persName><forename type="first">K</forename><surname>Burrage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Estivill-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fellows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rosamond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parameterized and Exact Computation, Second International Workshop (IWPEC 2006)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4169</biblScope>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simplifying the weft hierarchy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="313" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unconditional security against memory-bound adversaries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cachin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;97</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1294</biblScope>
			<biblScope unit="page" from="292" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the amount of nondeterminism and the power of verifying</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="733" to="750" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The random oracle methodology, revisited</title>
		<author>
			<persName><forename type="first">R</forename><surname>Canetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Halevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="557" to="594" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tight lower bounds for certain parameterized NP-hard problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fellows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kanj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="231" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vertex cover: Further observations and further improvements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kanj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="280" to="301" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">SAT is unlikely to be compressible. Manuscript</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linear kernels in linear time, or how to save k colors in O(n 2 ) steps</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fellows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juedes</surname></persName>
		</author>
		<idno>WG 04</idno>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3353</biblScope>
			<biblScope unit="page" from="257" to="269" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The complexity of theorem-proving procedures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A design principle for hash functions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Damgård</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;89</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="page" from="416" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the existence of statistically hiding bit commitment schemes and fail-stop signatures</title>
		<author>
			<persName><forename type="first">I</forename><surname>Damgård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfitzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;93</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">773</biblScope>
			<biblScope unit="page" from="250" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single database private information retrieval implies oblivious transfer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Di Crescenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ostrovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -EUROCRYPT &apos;2000</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1807</biblScope>
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">New directions in cryptography</title>
		<author>
			<persName><forename type="first">W</forename><surname>Diffie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Information Theory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="644" to="654" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hyper-encryption and everlasting security</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Rabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Theoretical Aspects of Computer Science (STACS)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2285</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The PCP theorem by gap amplification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dinur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Parameterized Complexity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fellows</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parameterized complexity: a systematic method for confronting computational intractability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fellows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contemporary Trends in Discrete Mathematics</title>
		<title level="s">AMS DIMACS Proceedings Series</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="49" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the randomness complexity of efficient sampling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dubrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="711" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Digital signets: Self-enforcing protection of digital information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lotspiech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On forward-secure storage</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dziembowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;06</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4117</biblScope>
			<biblScope unit="page" from="251" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On generating the initial key in the bounded-storage model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dziembowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -EUROCRYPT</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3027</biblScope>
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimal randomizer efficiency in the bounded-storage model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dziembowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="26" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relations between average case complexity and approximation complexity</title>
		<author>
			<persName><forename type="first">U</forename><surname>Feige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="534" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On limited versus polynomial nondeterminism</title>
		<author>
			<persName><forename type="first">U</forename><surname>Feige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Chicago Journal of Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">1997</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the impossibility of constructing non-interactive statistically-secret protocols from any trapdoor one-way function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topics in Cryptology -CT-RSA 2002, The Cryptographer&apos;s Track at the RSA Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Flum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<title level="m">Parameterized Compleixity Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bounded fixed-parameter tractability and log 2 n nondeterministic bits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Flum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Colloquium on Automata, Languages and Programming (ICALP)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">3142</biblScope>
			<biblScope unit="page" from="555" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Infeasibility of instance compression and succinct PCPs for NP</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fortnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Santhanam</surname></persName>
		</author>
		<idno>TR07-096</idno>
	</analytic>
	<monogr>
		<title level="j">Electronic Colloquium on Computational Complexity (ECCC)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<title level="m">Foundations of Cryptography</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<title level="m">Foundations of Cryptography</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Proofs that yield nothing but their validity, or all languages in NP have zero-knowledge proof systems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="691" to="729" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Limited nondeterminism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mundhenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGACT News</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the (in)security of the Fiat-Shamir paradigm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Tauman</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Finding collisions in interactive protocols -a tight lower bound on the round complexity of statistically-hiding commitments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Segev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">48th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="669" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reducing complexity assumptions for statistically-hiding commitment</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shaltiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -EUROCRYPT &apos;2005</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3494</biblScope>
			<biblScope unit="page" from="58" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistically-hiding commitment from any one-way function</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On everlasting security in the hybrid bounded storage model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Colloquium on Automata, Languages and Programming (ICALP) 2006, Part II</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4052</biblScope>
			<biblScope unit="page" from="192" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the compressibility of NP instances and cryptographic applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<idno>TR06-022</idno>
	</analytic>
	<monogr>
		<title level="m">Electronic Colloquium on Computational Complexity (ECCC)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A pseudorandom generator from any one-way function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Håstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1364" to="1396" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Finding collisions on a public road, or do secure hash functions need secret coins?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reyzin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;04</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3152</biblScope>
			<biblScope unit="page" from="92" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A personal view of average-case complexity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Annual Structure in Complexity Theory Conference</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="134" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">One-way functions are essential for complexity based cryptography</title>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="230" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Which problems have strongly exponential complexity?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="653" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Limits on the provable consequences of one-way permutations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rudich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="44" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sufficient conditions for collision-resistant hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ostrovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Theory of Cryptography Conference -(TCC &apos;05)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3378</biblScope>
			<biblScope unit="page" from="445" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tractability of parameterized completion problems on chordal, strongly chordal, and proper interval graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1906" to="1922" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reducibility among combinatorial problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complexity of Computer Computations</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Miller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Thatcher</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1972">1972</date>
			<biblScope unit="page" from="85" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient randomized pattern-matching algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A note on efficient zero-knowledge proofs and arguments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Refining nondeterminism in relativized polynomial-time bounded computations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Replication is not needed: Single database, computationally-private information retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ostrovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">An Introduction to Kolmogorov Complexity and Its Applications, 2nd Edition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vitányi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Encryption against space-bounded adversaries from on-line strong extractors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="42" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Conditionally-perfect secrecy and a provably-secure randomized cipher</title>
		<author>
			<persName><forename type="first">U</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="66" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Secret key agreement by public discussion</title>
		<author>
			<persName><forename type="first">U</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Information Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="733" to="742" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Indifferentiability, impossibility results on reductions, and applications to the random oracle methodology</title>
		<author>
			<persName><forename type="first">U</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Renner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Theory of Cryptography Conference -(TCC &apos;04)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2951</biblScope>
			<biblScope unit="page" from="21" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">CS proofs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="436" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Small-bias probability spaces: Efficient constructions and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="856" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Bit commitment using pseudorandomness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="158" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On cryptographic assumptions and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;03</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2729</biblScope>
			<biblScope unit="page" from="96" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Perfect zero-knowledge arguments for NP using any one-way permutation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="108" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Universal one-way hash functions and their cryptographic applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Statistical zero-knowledge arguments for NP from any one-way function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Invitation to Fixed Parameter Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Niedermeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Separating random oracle proofs from complexity theoretic proofs: The non-committing encryption case</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -CRYPTO &apos;02</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2442</biblScope>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On protocol security in the cryptographic model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">BRICS Dissertation Series DS-03</title>
		<imprint>
			<date type="published" when="2003-08-08">8 August, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Optimization, approximation, and complexity classes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th ACM Symposium on the Theory of Computing</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On limited nondeterminism and the complexity of the V-C dimension</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences (JCSS)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="170" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Finding collisions on a one-way street: Can secure hash functions be based on general assumptions?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Cryptology -EUROCRYPT &apos;1998</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1403</biblScope>
			<biblScope unit="page" from="334" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Compression of samplable sources</title>
		<author>
			<persName><forename type="first">L</forename><surname>Trevisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zuckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computational Complexity</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Constructing locally computable extractors and cryptosystems in the bounded storage model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cryptology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="77" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">NP is as easy as detecting unique solutions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">On pseudoentropy versus compressibility</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computational Complexity</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">On obfuscating point functions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Finding pessiland</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography Conference -(TCC &apos;06)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3876</biblScope>
			<biblScope unit="page" from="429" to="442" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
