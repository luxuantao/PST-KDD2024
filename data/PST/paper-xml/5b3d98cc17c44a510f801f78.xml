<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Playing hard exploration games by watching YouTube</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
							<email>yusufaytar@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Nando de Freitas DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
							<email>tpfaff@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Nando de Freitas DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Budden</surname></persName>
							<email>budden@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Nando de Freitas DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Le Paine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Nando de Freitas DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Nando de Freitas DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Playing hard exploration games by watching YouTube</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">10EBEA284575BBD9928590D4803BE93D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games MONTEZUMA'S REVENGE, PITFALL! and PRIVATE EYE for the first time, even if the agent is not presented with any environment rewards. * denotes equal contribution 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>People learn many tasks, from knitting to dancing to playing games, by watching videos online. They demonstrate a remarkable ability to transfer knowledge from the online demonstrations to the task at hand, despite huge gaps in timing, visual appearance, sensing modalities, and body differences. This rich setup with abundant unlabeled data motivates a research agenda in AI, which could result in significant progress in third-person imitation, self-supervised learning, reinforcement learning (RL) and related areas. In this paper, we show how this proposed research agenda enables us to make some initial progress in self-supervised alignment of noisy demonstration sequences for RL agents, enabling human-level performance on the most complex and previously unsolved Atari 2600 games.</p><p>Despite the recent advancements in deep reinforcement learning algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> and architectures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>, there are many "hard exploration" challenges, characterized by particularly sparse environment rewards, that continue to pose a difficult challenge for existing RL agents. One epitomizing example is Atari's MONTEZUMA'S REVENGE <ref type="bibr" target="#b9">[10]</ref>, which requires a human-like avatar to navigate a series of platforms and obstacles (the nature of which change substantially room-toroom) to collect point-scoring items. Such tasks are practically impossible using naive -greedy exploration methods, as the number of possible action trajectories grows exponentially in the number of frames separating rewards. For example, reaching the first environment reward in MONTEZUMA'S REVENGE takes approximately 100 environment steps, equivalent to 100 18 possible action sequences. Even if a reward is randomly encountered, γ-discounted RL struggles to learn stably if this signal is backed-up across particularly long time horizons. Successful attempts at overcoming the issue of sparse rewards have fallen broadly into two categories of guided exploration. First, intrinsic motivation methods provide an auxiliary reward that encourages the agent to explore states or action trajectories that are "novel" or "informative" with respect to some measure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref>. These methods tend to help agents to re-explore discovered parts of state space that appear novel or uncertain (known-unknowns), but often fail to provide guidance about where in the environment such states are to be found in the first place (unknown-unknowns). Accordingly, these methods typically rely on an additional random component to drive the initial exploration process. The other category is imitation learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>, whereby a human demonstrator generates state-action trajectories that are used to guide exploration toward areas considered salient with respect to their inductive biases. These biases prove to be a very useful constraint in the context of Atari, as humans can immediately identify e.g. that a skull represents danger, or that a key unlocks a door.</p><p>Among existing imitation learning methods, DQfD by Hester et al. <ref type="bibr" target="#b19">[20]</ref> has shown the best performance on Atari's hardest exploration games. Despite these impressive results, there are two limitations of DQfD <ref type="bibr" target="#b19">[20]</ref> and related methods. First, they assume that there is no "domain gap" between the agent's and demonstrator's observation space, e.g. variations in color or resolution, or the introduction of other visual artifacts. An example of domain gap in MONTEZUMA'S REVENGE is shown in Figure <ref type="figure" target="#fig_0">1</ref>, considering the first frame of (a) our environment compared to (b) YouTube gameplay footage. Second, they assume that the agent has access to the exact action and reward sequences that led to the demonstrator's observation trajectory. In both cases, these assumptions constrain the set of useful demonstrations to those collected under artificial conditions, typically requiring a specialized software stack for the sole purpose of RL agent training.</p><p>To address these limitations, this paper proposes a method for overcoming domain gaps between the observation sequences of multiple demonstrations, by using self-supervised classification tasks that are constructed over both time (temporal distance classification) and modality (cross-modal temporal distance classification) to learn a common representation (see Figure <ref type="figure" target="#fig_1">2</ref>). Unlike previous approaches, our method requires neither (a) frame-by-frame alignment between demonstrations, or (b) class labels or other annotations from which an alignment might be indirectly inferred. We additionally propose a new unsupervised measure (cycle-consistency) for evaluating the quality of such a learnt embedding.</p><p>Using our embedding, we propose an auxiliary imitation loss that allows an agent to successfully play hard exploration games without requiring the knowledge of the demonstrator's action trajectory. Specifically, providing a standard RL agent with an imitation reward learnt from a single YouTube video, we are the first to convincingly exceed human-level performance on three of Atari's hardest exploration games: MONTEZUMA'S REVENGE, PITFALL! and PRIVATE EYE. Despite the challenges of designing reward functions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref> or learning them using inverse reinforcement learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>, we also achieve human-level performance even in the absence of an environment reward signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Imitation learning methods such as DQfD have yielded promising results for guiding agent exploration in sparse-reward tasks, both in game-playing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref> and robotics domains <ref type="bibr" target="#b40">[41]</ref>. However, these methods have traditionally leveraged observations collected in artificial conditions, i.e. in the absence of a domain gap (see Figure <ref type="figure" target="#fig_0">1</ref>) and with full visibility over the demonstrator's action and reward trajectories. Other approaches include interacting with the environment before introducing the expert demonstrations <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31]</ref> and goal conditioned policies for high fidelity imitation <ref type="bibr" target="#b28">[29]</ref>, although these papers typically do not assume domain gap or operate in sparse reward settings.</p><p>There are several methods of overcoming the domain gap in the previous literature. In the simple scenario of demonstrations that are aligned frame-by-frame <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>, methods such as CCA <ref type="bibr" target="#b1">[2]</ref>, DCTW <ref type="bibr" target="#b38">[39]</ref> or time-contrastive networks (TCN) <ref type="bibr" target="#b33">[34]</ref> can be used to learn a common representation space. However, YouTube videos of Atari gameplay are more complex, as the actions taken by different demonstrators can lead to very different observation sequences lacking such an alignment. In this scenario, another common approach for domain alignment involves solving a shared auxiliary objective across the domains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. For example, Aytar et al. <ref type="bibr" target="#b4">[5]</ref> demonstrated that by solving the same scene classification task bottlenecked by a common decision mechanism (i.e. using the same network), several very different domains (i.e. natural images, line drawings and text descriptions) could be successfully aligned. Similarly, domain adaptive meta-learning <ref type="bibr" target="#b43">[44]</ref> uses a shared policy network for addressing the domain gap for robotic tasks, though they require both first person robot demonstrations and third person human demonstrations. Our work differs from the above approaches in that we do not make use of any category-guided supervision or first person demonstrations. Instead, we define our shared tasks using self-supervision over unlabeled data. This idea is motivated by several recent works in the self-supervised feature learning literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Other related approaches include single-view TCN <ref type="bibr" target="#b33">[34]</ref>, which is another self-supervised task that does not require paired training data. We differ from this work by using temporal classification instead of triplet-based ranking, which removes the need to empirically tune sensitive hyper parameters (local neighborhood size, ranking margin, etc). Another approach <ref type="bibr" target="#b32">[33]</ref> performs temporal classification but limits its categories to frames close or far away in time. With respect to our use of cross-modal data, another similar existing method in the feature learning literature is L 3 -net <ref type="bibr" target="#b2">[3]</ref>. This approach learns to align vision and sound modalities, whereas we learn to align multiple audio-visual sequences (i.e. demonstrations) using multi-modal alignment as a self-supervised objective. We adapt both TCN and L 3 -net for domain alignment and provide an evaluation compared to our proposed method in Section 6. We also experimented with third-person imitation methods <ref type="bibr" target="#b36">[37]</ref> that combine the ideas of generative adversarial imitation learning (GAIL) <ref type="bibr" target="#b20">[21]</ref> and adversarial domain confusion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>, but were unable to make progress using the very long YouTube demonstration trajectories.</p><p>Considering the imitation component of our work, one perspective is that we are learning a reward function that explains the demonstrator's behavior, which is closely related to inverse reinforcement learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>. There have also been many previous studies that consider supervised <ref type="bibr" target="#b3">[4]</ref> and few-shot methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref> for imitation learning. However, in both cases, our setting is more complex due to the presence of domain gap and absence of demonstrator action and reward sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Closing the domain gap</head><p>Learning from YouTube videos is made difficult by both the lack of frame-by-frame alignment, and the presence of domain-specific variations in color, resolution, screen alignment and other visual artifacts.</p><p>We propose that by learning a common representation across multiple demonstrations, our method will generalize to agent observations without ever being explicitly exposed to the Atari environment.</p><p>In the absence of pre-aligned data, we adopt self-supervision in order to learn this embedding. The rationale of self-supervision is to propose an auxiliary task that we learn to solve simultaneously across all domains, thus encouraging the network to learn a common representation. This is motivated by the work of Aytar et al. <ref type="bibr" target="#b4">[5]</ref>, but differs in that we do not have access to class labels to establish a supervised objective. Instead, we propose two novel self-supervised objectives: temporal distance classification (TDC), described in Section 3.1 and cross-modal temporal distance classification (CMC), described in Section 3.2. We also propose cycle-consistency in Section 3.3 as a quantitative measure for evaluating the one-to-one alignment capacity of an embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Temporal distance classification (TDC)</head><p>We first consider the unsupervised task of predicting the temporal distance ∆t between two frames of a single video sequence. This task requires an understanding of how visual features move and transform over time, thus encouraging an embedding that learns meaningful abstractions of environment dynamics conditioned on agent interactions.</p><p>We cast this problem as a classification task, with K categories corresponding to temporal distance intervals,</p><formula xml:id="formula_0">d k ∈ {[0], [1], [2], [3 -4], [5 -20], [21 -200]}.</formula><p>Given two frames from the same video, v, w ∈ I, we learn to predict the interval d k s.t. ∆t ∈ d k . Specifically, we implement two functions: an embedding function φ : I → R N , and a classifier τ tdc : R N × R N → R K , both implemented as neural networks (see Section 5 for implementation details). We can then train τ tdc (φ(v), φ(w)) to predict the distribution over class labels, d k , using the following cross-entropy classification loss:</p><formula xml:id="formula_1">L tdc (v, w, y) = - K j=1 y j log(ŷ j ) with ŷ = τ tdc (φ(v), φ(w)) ,<label>(1)</label></formula><p>where y and ŷ are the true and predicted label distributions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-modal temporal distance classification (CMC)</head><p>In addition to visual observations, our YouTube videos contain audio tracks that can be used to define an additional self-supervised task. As the audio of Atari games tends to correspond with salient events such as jumping, obtaining items or collecting points, a network that learns to correlate audio and visual observations should learn an abstraction that emphasizes important game events. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model selection through cycle-consistency</head><p>A challenge of evaluating and meta-optimizing the models presented in Section 3 is defining a measure of the quality of an embedding φ. Motivated by the success of cyclic relations in CycleGAN <ref type="bibr" target="#b47">[48]</ref> and for matching visual features across images <ref type="bibr" target="#b46">[47]</ref>, we propose cycle-consistency for this purpose. Assume that we have two length-N sequences, V = {v 1 , v 2 , ...v n } and W = {w 1 , w 2 , . . . , w n }.</p><p>We also define the distance, d φ , as the Euclidean distance in the associated embedding space,</p><formula xml:id="formula_2">d φ (v i , w j ) = ||φ(v i ) -φ(w j )|| 2 .</formula><p>To evaluate cycle-consistency, we first select v i ∈ V and determine its nearest neighbor, w j = argmin w∈W d φ (v i , w). We then repeat the process to find the nearest neighbor of w j , i.e. v k = argmin v∈V d φ (v, w j ). We say that v i is cycle-consistent if and only if |i -k| ≤ 1, and further define the one-to-one alignment capacity, P φ , of the embedding space φ as the percentage of v ∈ V that are cycle-consistent. Figure <ref type="figure" target="#fig_3">4</ref>(a) illustrates cycle-consistency in two example embedding spaces. The same process can be extended to evaluate the 3-cycle-consistency of φ, P 3 φ , by requiring that v i remains cycle consistent along both paths V → W → U → V and V → U → W → V , where U is a third sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">One-shot imitation from YouTube footage</head><p>In Section 3, we learned to extract features from unlabeled and unaligned gameplay footage, and introduced a measure to evaluate the quality of the learnt embedding. In this section, we describe how these features can be exploited to learn to play games with very sparse rewards, such as the infamously difficult PITFALL! and MONTEZUMA'S REVENGE. Specifically, we demonstrate how a sequence of checkpoints placed along the embedding of a single YouTube video can be presented as a reward signal to a standard reinforcement learning agent (IMPALA for our experiments <ref type="bibr" target="#b13">[14]</ref>), allowing successful one-shot imitation even in the complete absence of the environment rewards.</p><p>Taking a single YouTube gameplay video, we simply generate a sequence of "checkpoints" every N = 16 frames along the embedded trajectory. We can then represent the following reward:</p><formula xml:id="formula_3">r imitation = 0.5 if φ(v agent ) • φ(v checkpoint ) &gt; α 0.0 otherwise<label>(2)</label></formula><p>where φ(v) are the zero-centered and l 2 -normalized embeddings of the agent and checkpoint observations. We also require that checkpoints be visited in soft-order, i.e. if the last collected checkpoint is at  v (n) , then v checkpoint ∈ {v (n+1) , . . . , v (n+1+∆t) }. We set ∆t = 1 and α = 0.5 for our experiments (except when considering pixel-only embeddings, where α = 0.92 provided the best performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation Details</head><p>The visual embedding function, φ, is composed of three spatial, padded, 3x3 convolutional layers with <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">64</ref>) channels and 2x2 max-pooling, followed by three residual-connected blocks with 64 channels and no down-sampling. Each layer is ReLU-activated and batch-normalized, and the output fed into a 2-layer 1024-wide MLP. The network input is a 128x128x3x4 tensor constructed by random spatial cropping of a stack of four consecutive 140x140 RGB images, sampled from our dataset. The final embedding vector is l 2 -normalized.</p><p>The audio embedding function, ψ, is as per φ except that it has four, width-8, 1D convolutional layers with <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256</ref>) channels and 2x max-pooling, and a single width-1024 linear layer. The input is a width-137 (6ms) sample of 256 frequency channels, calculated using STFT. ReLU-activation and batch-normalization are applied throughout and the embedding vector is l 2 -normalized.</p><p>The same shallow network architecture, τ , is used for both temporal and cross-modal classification. Both input vectors are combined by element-wise multiplication, with the result fed into a 2-layer MLP with widths (1024, 6) and ReLU non-linearity in between. A visualization of these networks and their interaction is provided in Figure <ref type="figure" target="#fig_2">3</ref>. Note that although τ tdc and τ cmc share the same architecture, they are operating on two different problems and therefore maintain separate sets of weights.</p><p>To generate training data, we sample input pairs (v i , w i ) (where v i and w i are sampled from the same domain) as follows. First, we sample a demonstration sequence from our three training videos. Next, we sample both an interval, d k ∈ {[0], <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr">[3 -4]</ref>, <ref type="bibr">[5 -20]</ref>, <ref type="bibr">[21 -200]</ref>}, and a distance, ∆t ∈ d k . Finally, we randomly select a pair of frames from the sequence with temporal distance ∆t. The model is trained with Adam using a learning rate of 10 -4 and batch size of 32 for 200,000 steps.</p><p>As described in Section 4, our imitation loss is constructed by generating checkpoints every N = 16 frames along the φ-embedded observation sequence of a single YouTube video. We train an agent using the sum of imitation and (optionally) environment rewards. We use the distributed A3C RL agent IMPALA <ref type="bibr" target="#b13">[14]</ref> with 100 actors for our experiments. The only modification we make to the published network is to calculate the distance (as per Equation( <ref type="formula" target="#formula_3">2</ref>)) between the agent and the next two checkpoints and concatenate this 2-vector with the flattened output of the last convolutional layer. We also tried re-starting our agent from checkpoints recorded along its trajectory, similar to Hosu et al. <ref type="bibr" target="#b22">[23]</ref>, but found that it provided minimal improvement given even our very long demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis and Experiments</head><p>In this section we analyze (a) the learnt embedding spaces, and (b) the performance of our RL agent. We consider three Atari 2600 games that are considered very difficult exploration challenges: MON-TEZUMA'S REVENGE, PITFALL! and PRIVATE EYE. For each, we select four YouTube videos (three training and one test) of human gameplay, varying in duration from 3-to-10 minutes. Importantly, Figure <ref type="figure">6</ref>: For each embedding method, we visualize the t-SNE projection of four observation sequences traversing the first room of MONTEZUMA'S REVENGE. Using pixel space alone fails to provide any meaningful cross-domain alignment. Purely cross-modal methods perform better, but produce a very scattered embedding due to missing long-range dependencies. The combination of temporal and cross-modal objectives yields the best alignment and continuity of trajectories. none of the YouTube videos were collected using our specific Arcade Learning Environment <ref type="bibr" target="#b9">[10]</ref>, and the only pre-processing that we apply is keypoint-based (i.e. Harris corners) affine transformation to spatially align the game screens from the first frame only. The dataset used and additional experiments can be found in the supplemental material to this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Embedding space evaluation</head><p>To usefully close the domain gap between YouTube gameplay footage and our environment observations, our learnt embedding space should exhibit two desirable properties: (1) one-to-one alignment capacity and (2) meaningful abstraction. We consider each of these properties in turn.</p><p>First, one-to-one alignment is desirable for reliably mapping observations between different sequences. We evaluate this property using the cycle-consistency measure introduced in Section 3.3. The features from earlier layers in φ (see Figure <ref type="figure" target="#fig_5">5</ref>) are centered and l 2 -normalized before computing cycleconsistency. Specifically, we consider both (a) the 2-way cycle-consistency, P φ , between the test video and the first training video, and (b) the 3-way cycle-consistency, P 3 φ , between the test video and the first two training videos. These results are presented in Figure <ref type="figure" target="#fig_5">5</ref>, comparing the cycle-consistencies of our TDC, CMC and combined methods to a naive l 2 -distance in pixel space, single-view timecontrastive networks (TCN) <ref type="bibr" target="#b33">[34]</ref> and L 3 -Net <ref type="bibr" target="#b2">[3]</ref>. Note that we implemented single-view TCN and L 3 -Net in our framework and tuned the hyperparameters to achieve the best P 3 φ cycle-consistency. As expected, pixel loss performs worst in the presence of sequence-to-sequence domain gaps. Our TDC and CMC methods alone yield improved performance compared to TCN and L 3 -Net (particularly at deeper levels of abstraction), and combining both methods provides the best results overall.</p><p>Next, Figure <ref type="figure">6</ref> shows the t-SNE projection of observation trajectories taken by different human demonstrators to traverse the first room of MONTEZUMA'S REVENGE. It is again evident that a pixel-based loss entirely fails to align the sequences. The embeddings learnt using purely cross-modal alignment (i.e. L 3 -Net and CMC) perform better but still yield particularly scattered and disjoint trajectories, which is an undesirable property likely due to the sparsity of salient audio signals. TDC and our combined TDC+CMC methods provide the more globally consistent trajectories, and are less likely to produce false-positives with respect to the distance metric described in Section 4.</p><p>Finally, a useful embedding should provide a useful abstraction that encodes meaningful, high-level information of the game while ignoring irrelevant features. To aid in visualizing this property, Figure <ref type="figure" target="#fig_7">7</ref> demonstrates the spatial activation of neurons in the final convolutional layer of the embedding network φ, using the visualization method proposed in <ref type="bibr" target="#b45">[46]</ref>. It is compelling that the top activations are centered on features including the player and enemy positions in addition to the inventory state, which is informative of the next location that needs to be explored (e.g. if we have collected the key required to open a door). Important objects such as the key are emphasized more in the cross-modal and combined embeddings, likely due to the unique sounds that are played when collected (see figure <ref type="figure" target="#fig_7">7</ref>  We observe that use of the audio signal in CMC results in more emphasis being placed on key items and their location in the inventory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Solving hard exploration games with one-shot imitation</head><p>Using the method described in Section 4, we train an IMPALA agent to play the hard exploration Atari games MONTEZUMA'S REVENGE, PITFALL! and PRIVATE EYE using a learned auxiliary reward to guide exploration. For each game, the embedding network, φ, was trained using just three YouTube videos, and an additional video was embedded to generate a sequence of exploration checkpoints. Videos of our agent playing these games can be found here<ref type="foot" target="#foot_0">2</ref> .</p><p>Figure <ref type="figure" target="#fig_8">8</ref> presents our learning curves for each hard exploration Atari game. Without imitation reward, the pure RL agent is unable to collect any of the sparse rewards in MONTEZUMA'S REVENGE and PITFALL!, and only reaches the first two rewards in PRIVATE EYE (consistent with previous studies using DQN variants <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>). Using pixel-space features, the guided agent is able to obtain 17k points in PRIVATE EYE but still fails to make progress in the other games. Replacing a pixel embedding with our combined TDC+CMC embedding convincingly yields the best results, even if the agent is presented only with our TDC+CMC imitation reward (i.e. no environment reward).</p><p>To test the impact of the choice of expert trajectory, we generate checkpoints from two additional videos of MONTEZUMA'S REVENGE from our set, and train agents with those sequences (figure <ref type="figure" target="#fig_8">8</ref>, left). While all three agents manage to clear the first level, expert 1 achieves the highest score. Out of the three expert sequence considered, expert 1 also has the biggest domain shift. This is in line with our findings from section 6.1 that our embedding space can sufficiently align our sequences. Domain shift in the expert trajectories is therefore not a significant factor on performance.</p><p>Finally, in Table <ref type="table" target="#tab_0">1</ref> we compare our best policies for each game to the best previously published results; Rainbow <ref type="bibr" target="#b18">[19]</ref> and ApeX DQN <ref type="bibr" target="#b21">[22]</ref> without demonstrations, and DQfD <ref type="bibr" target="#b19">[20]</ref> using expert demonstrations. Unlike DQfD our demonstrations are unaligned YouTube footage without access to action or reward trajectories. Our results are calculated using the standard approach of averaging over 200 episodes initialized using a random 1-to-30 no-op actions. Importantly, our approach is the first to convincingly exceed human-level performance on all three games -even in the absence of an environment reward signal. We are the first to solve the entire first level of MONTEZUMA'S REVENGE and PRIVATE EYE, and substantially outperform state-of-the-art on PITFALL!.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a method of guiding agent exploration through hard exploration challenges by watching YouTube videos. Unlike traditional methods of imitation learning, where demonstrations are generated under controlled conditions with access to action and reward sequences, YouTube videos contain only unaligned and often noisy audio-visual sequences. We have proposed novel self-supervised objectives that allow a domain-invariant representation to be learnt across videos, and described a one-shot imitation mechanism for guiding agent exploration by embedding checkpoints throughout this space. Combining these methods with a standard IMPALA agent, we demonstrate MONTEZUMA'S REVENGE PITFALL! PRIVATE EYE Rainbow <ref type="bibr" target="#b18">[19]</ref> 384.0 0.0 4,234.0 ApeX <ref type="bibr" target="#b21">[22]</ref> 2,500.0 -0. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the domain gap that exists between the Arcade Learning Environment and YouTube videos from which our agent is able to learn to play MONTEZUMA'S REVENGE. Note different size, resolution, aspect ratio, color and addition of visual artifacts such as text and avatars.</figDesc><graphic coords="2,108.00,72.00,396.01,73.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: For the path shown in (a), t-SNE projections [25] of observation sequences using (c) our embedding, versus (d) raw pixels. Four different domains are compared side-by-side in (b) for an example frame of MONTEZUMA'S REVENGE: (purple) the Arcade Learning Environment, (cyan/yellow) two YouTube training videos, and (red) an unobserved YouTube video. It is evident that all four trajectories are well-aligned in our embedding space, despite (purple) and (red) being held-aside during training. Using raw pixel values fails to achieve any meaningful alignment.</figDesc><graphic coords="3,108.00,72.00,396.00,107.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the network architectures and interactions involved in our combined TDC+CMC self-supervised loss calculation. The final layer FC2 of φ is later used to embed the demonstration trajectory to imitate. Although the Arcade Learning Environment does not expose an audio signal to our agent at training time, the audio signal present in YouTube footage made a substantial contribution to the learnt visual embedding function φ.</figDesc><graphic coords="4,108.00,72.00,395.99,120.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Visualization of two embedding spaces with low and high cycle-consistency. Note that the selected point in sequence V (left) fails and (right) succeeds at cycling back to the original point. (b) Demonstration of one shot imitation through RL visualized in the embedding space.</figDesc><graphic coords="5,108.00,72.00,395.98,92.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Cycle-consistency evaluation considering different embedding spaces. We compare naive l 2 pixel loss to temporal methods (TDC and single-view TCN) and cross-modal methods (CMC and L 3 -Net). Combining TDC and CMC yields the best performance for both 2 and 3-cycle-consistency, particularly at deeper levels of abstraction (e.g. no performance loss using FC1 or FC2).</figDesc><graphic coords="6,106.98,72.00,249.46,95.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(d)  and (e)). Notably absent are activations associated with distractors such as the moving sand animation, or video-specific artifacts indicative of the domain gap we wished to close.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (Left) Visualization of select activations in the final convolutional layer. Individual neurons focus on e.g. (a) the player, (b) enemies, and (c) the inventory. Notably absent are activations associated with distractors or domain-specific artifacts. (Right) Visualization of activations summed across all channels in the final layer. We observe that use of the audio signal in CMC results in more emphasis being placed on key items and their location in the inventory.</figDesc><graphic coords="8,108.00,72.00,396.01,87.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Learning curves of our combined TDC+CMC algorithm with (purple) and without (yellow) environment reward, versus imitation from pixel-space features (blue) and IMPALA without demonstrations (green). The red line represents the maximum reward achieved using previously published methods, and the brown line denotes the score obtained by an average human player.</figDesc><graphic coords="9,108.00,72.00,396.01,109.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our best policy (mean of 200 evaluation episodes) to previously published results across MONTEZUMA'S REVENGE, PITFALL! and PRIVATE EYE. Our agent is the first to exceed average human-level performance on all three games, even without environment rewards.the first human-level performance in the infamously difficult exploration games MONTEZUMA'S REVENGE, PITFALL! and PRIVATE EYE.</figDesc><table><row><cell>6</cell><cell>49.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://www.youtube.com/playlist?list=PLZuOGGtntKlaOoq_8wk5aKgE_u_Qcpqhu</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We would like to thank the team, especially Serkan Cabi, Bilal Piot and Tobias Pohlen, for many fruitful discussions. We thank the reviewers for their comments, which helped in making this a better paper. And finally, we say 'thank you' to all the amazing Atari players on Youtube and Twitch, which inspired this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An introduction to multivariate statistical analysis</title>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of robot learning from demonstration</title>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Brenna D Argall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and autonomous systems</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="469" to="483" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cross-modal scene networks</title>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00932</idno>
		<title level="m">See, hear, and read: Deep aligned representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed distributional deterministic policy gradients</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unifying count-based exploration and intrinsic motivation</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1471" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">One-shot imitation learning</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradly</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR, abs/1802.01561</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">One-shot visual imitation learning via meta-learning</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04905</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The reactor: A sample-efficient actor-critic architecture</title>
		<author>
			<persName><forename type="first">Audrunas</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inverse reward design</title>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Hadfield-Menell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smitha</forename><surname>Milli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6768" to="6777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep q-learning from demonstrations</title>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matej</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Sendonaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Dulac-Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Distributed prioritized experience replay</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Playing atari games with deep reinforcement learning and human checkpoint replay</title>
		<author>
			<persName><forename type="first">Ionel-Alexandru</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Traian</forename><surname>Rebedea</surname></persName>
		</author>
		<idno>CoRR, abs/1607.05077</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Imitation from observation: Learning to imitate behaviors from raw video via context translation</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03374</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07608</idno>
		<title level="m">Deep exploration via randomized value functions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="801" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">One-shot high-fidelity imitation: Training large-scale deep nets with rl</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Le Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05017</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parsa</forename><surname>Mahmoudieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yide</forename><surname>Shentu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08606</idno>
		<title level="m">Zero-shot visual imitation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Observe and look further: Achieving consistent performance on atari</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Večerík</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11593</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00653</idno>
		<title level="m">Semi-parametric topological memory for navigation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Time-contrastive networks: Selfsupervised learning from multi-view observation</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Where do rewards come from</title>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual conference of the cognitive science society</title>
		<meeting>the annual conference of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2601" to="2606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bradly</surname></persName>
		</author>
		<author>
			<persName><surname>Stadie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01703</idno>
		<title level="m">Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Faraz</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrett</forename><surname>Warnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01954</idno>
		<title level="m">Behavioral cloning from observation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep canonical time warping for simultaneous alignment and representation learning of sequences</title>
		<author>
			<persName><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><forename type="middle">W</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1128" to="1138" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Večerík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Rothörl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08817</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Anticipating the future by watching unlabeled video</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08023</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">One-shot imitation from observing humans via domain-adaptive meta-learning</title>
		<author>
			<persName><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeep</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01557</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andrew Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
