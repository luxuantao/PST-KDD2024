<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study</title>
				<funder ref="#_DXVEgwM #_NSPnH8t #_6s2NKvG #_CceHrHW">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_b5pkAu9">
					<orgName type="full">INCAS</orgName>
				</funder>
				<funder ref="#_aJ74KQQ">
					<orgName type="full">IIS</orgName>
				</funder>
				<funder ref="#_EStCTVD">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-07">7 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>205 97 135, 247 1, 253 Engineering Journal 270, 683 100 430, 867,276 Psychology Journal 372, 641 100 460, 123 2, 701 Computer Science Conference 263, 393 13, 084,440 Journal 410, 603 15, 540 96 634, 996 Geology Journal 431, 883 100 471, 216 1, 762 Mathematics Journal 490, 551 14, 271 98 404, 150,584 Materials Science Journal 1,337, 802 99 1, 773 Physics Journal 1, 664 91 1, 761 Biology Journal 1, 267 100 2, 131 Chemistry Journal 1, 538 100 2, 637,438 Medicine Journal 2, 105 36, 619 100 4</addrLine>
									<postCode>042, 006 10, 046 1, 954 7, 313, 613 75 331, 582 1, 506 2, 751, 834 7, 753, 066 2, 731 6, 904, 549 5, 457, 369, 983 16, 392, 070 3, 641, 588, 778 64, 730, 547 7, 086, 849, 956 35, 721, 253 8, 646, 345, 385 7, 405, 779</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Jin</surname></persName>
							<email>bowenj4@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>205 97 135, 247 1, 253 Engineering Journal 270, 683 100 430, 867,276 Psychology Journal 372, 641 100 460, 123 2, 701 Computer Science Conference 263, 393 13, 084,440 Journal 410, 603 15, 540 96 634, 996 Geology Journal 431, 883 100 471, 216 1, 762 Mathematics Journal 490, 551 14, 271 98 404, 150,584 Materials Science Journal 1,337, 802 99 1, 773 Physics Journal 1, 664 91 1, 761 Biology Journal 1, 267 100 2, 131 Chemistry Journal 1, 538 100 2, 637,438 Medicine Journal 2, 105 36, 619 100 4</addrLine>
									<postCode>042, 006 10, 046 1, 954 7, 313, 613 75 331, 582 1, 506 2, 751, 834 7, 753, 066 2, 731 6, 904, 549 5, 457, 369, 983 16, 392, 070 3, 641, 588, 778 64, 730, 547 7, 086, 849, 956 35, 721, 253 8, 646, 345, 385 7, 405, 779</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>205 97 135, 247 1, 253 Engineering Journal 270, 683 100 430, 867,276 Psychology Journal 372, 641 100 460, 123 2, 701 Computer Science Conference 263, 393 13, 084,440 Journal 410, 603 15, 540 96 634, 996 Geology Journal 431, 883 100 471, 216 1, 762 Mathematics Journal 490, 551 14, 271 98 404, 150,584 Materials Science Journal 1,337, 802 99 1, 773 Physics Journal 1, 664 91 1, 761 Biology Journal 1, 267 100 2, 131 Chemistry Journal 1, 538 100 2, 637,438 Medicine Journal 2, 105 36, 619 100 4</addrLine>
									<postCode>042, 006 10, 046 1, 954 7, 313, 613 75 331, 582 1, 506 2, 751, 834 7, 753, 066 2, 731 6, 904, 549 5, 457, 369, 983 16, 392, 070 3, 641, 588, 778 64, 730, 547 7, 086, 849, 956 35, 721, 253 8, 646, 345, 385 7, 405, 779</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<email>yumeng5@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>205 97 135, 247 1, 253 Engineering Journal 270, 683 100 430, 867,276 Psychology Journal 372, 641 100 460, 123 2, 701 Computer Science Conference 263, 393 13, 084,440 Journal 410, 603 15, 540 96 634, 996 Geology Journal 431, 883 100 471, 216 1, 762 Mathematics Journal 490, 551 14, 271 98 404, 150,584 Materials Science Journal 1,337, 802 99 1, 773 Physics Journal 1, 664 91 1, 761 Biology Journal 1, 267 100 2, 131 Chemistry Journal 1, 538 100 2, 637,438 Medicine Journal 2, 105 36, 619 100 4</addrLine>
									<postCode>042, 006 10, 046 1, 954 7, 313, 613 75 331, 582 1, 506 2, 751, 834 7, 753, 066 2, 731 6, 904, 549 5, 457, 369, 983 16, 392, 070 3, 641, 588, 778 64, 730, 547 7, 086, 849, 956 35, 721, 253 8, 646, 345, 385 7, 405, 779</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><forename type="middle">2023</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<addrLine>205 97 135, 247 1, 253 Engineering Journal 270, 683 100 430, 867,276 Psychology Journal 372, 641 100 460, 123 2, 701 Computer Science Conference 263, 393 13, 084,440 Journal 410, 603 15, 540 96 634, 996 Geology Journal 431, 883 100 471, 216 1, 762 Mathematics Journal 490, 551 14, 271 98 404, 150,584 Materials Science Journal 1,337, 802 99 1, 773 Physics Journal 1, 664 91 1, 761 Biology Journal 1, 267 100 2, 131 Chemistry Journal 1, 538 100 2, 637,438 Medicine Journal 2, 105 36, 619 100 4</addrLine>
									<postCode>042, 006 10, 046 1, 954 7, 313, 613 75 331, 582 1, 506 2, 751, 834 7, 753, 066 2, 731 6, 904, 549 5, 457, 369, 983 16, 392, 070 3, 641, 588, 778 64, 730, 547 7, 086, 849, 956 35, 721, 253 8, 646, 345, 385 7, 405, 779</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-07">7 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3543507.3583354</idno>
					<idno type="arXiv">arXiv:2302.03341v1[cs.DL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>scientific literature tagging</term>
					<term>metadata</term>
					<term>text classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the exponential growth of scientific publications on the Web, there is a pressing need to tag each paper with fine-grained topics so that researchers can track their interested fields of study rather than drowning in the whole literature. Scientific literature tagging is beyond a pure multi-label text classification task because papers on the Web are prevalently accompanied by metadata information such as venues, authors, and references, which may serve as additional signals to infer relevant tags. Although there have been studies making use of metadata in academic paper classification, their focus is often restricted to one or two scientific fields (e.g., computer science and biomedicine) and to one specific model. In this work, we systematically study the effect of metadata on scientific literature tagging across 19 fields. We select three representative multi-label classifiers (i.e., a bag-of-words model, a sequence-based model, and a pre-trained language model) and explore their performance change in scientific literature tagging when metadata are fed to the classifiers as additional features. We observe some ubiquitous patterns of metadata's effects across all fields (e.g., venues are consistently beneficial to paper tagging in almost all cases), as well as some unique patterns in fields other than computer science and biomedicine, which are not explored in previous studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Digital libraries and archives; Data mining; World Wide Web.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>: A scientific paper with metadata from Microsoft Academic <ref type="bibr" target="#b37">[38]</ref>. The goal of scientific literature tagging is to predict its related topics.</p><p>PubMed <ref type="bibr" target="#b25">[26]</ref>, are available on the Web with great attention received. One major goal of these platforms is to help researchers query and track academic information and resources. Meanwhile, the volume of scientific publications is growing exponentially, doubling every 12 years <ref type="bibr" target="#b11">[12]</ref> and reaching 240,000,000 by 2019 <ref type="bibr" target="#b41">[42]</ref>. In such an information explosion era, it becomes more important than ever to accurately tag each scientific paper with its relevant topics so that researchers can track their interested fields of study instead of getting overwhelmed by the whole literature. Figure <ref type="figure">1</ref> shows an example of the scientific literature tagging task, which aims to predict the tags such as "World Wide Web", "Webgraph", and "Link Farm" given the paper "Graph structure in the Web".</p><p>Previous studies <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b58">59]</ref> have pointed out that scientific literature tagging is beyond a multi-label text classification task because academic papers are accompanied by metadata, which make them more complex than plain text sequences. Such metadata information, including venues, authors, and references, can be strong indicators of each paper's related topics. For example, in Figure <ref type="figure">1</ref>, the venue "WWW " implies the paper's relevance to "Computer Science" and "World Wide Web", while the authors and references may further indicate fine-grained tags such as "Webgraph" and "Link Farm".</p><p>Although existing studies on scientific literature tagging <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref> have proposed to incorporate metadata features into the tagger, they still have two major limitations. First, their focus is restricted to one or two scientific fields only (e.g., computer science and biomedicine). Empirically examining the effect of metadata in other fields (e.g., art, economics, mathematics, physics, etc.) has remained elusive, mainly owing to the lack of benchmark datasets for fine-grained paper tagging in these fields. Second, the proposed metadata-aware taggers mainly train an RNN <ref type="bibr" target="#b4">[5]</ref> or Transformer <ref type="bibr" target="#b40">[41]</ref> architecture from scratch. It is still unclear whether the proposed usage of metadata can be generalized to other approaches, such as bag-of-words classifiers (which are still broadly studied in large-scale multi-label text classification when efficiency is concerned <ref type="bibr" target="#b22">[23]</ref>) and pre-trained language models. Contributions. To address the aforementioned two limitations of previous studies, in this work, we conduct a systematic cross-field cross-model study on the effect of metadata on scientific literature tagging. First, we construct a large-scale scientific literature tagging benchmark, Maple (Metadata-Aware Paper colLEction), from the Microsoft Academic Graph <ref type="bibr" target="#b37">[38]</ref>. Maple covers 19 scientific fields and consists of more than 11.9 million papers. The number of candidate tags in each field ranges between ?700 and ?64,000. Then, we consider three major types of multi-label classification approaches: bag-of-words classifiers <ref type="bibr">[2, 16, 19, 31-34, 39, 49, 50, 54]</ref>, sequence-based classifiers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59]</ref>, and pre-trained language models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60]</ref>. We select one representative model from each of the three categories, namely Parabel <ref type="bibr" target="#b32">[33]</ref>, Transformer <ref type="bibr" target="#b43">[44]</ref>, and OAG-BERT <ref type="bibr" target="#b23">[24]</ref>, that can be modified in a straightforward way to jointly take text and metadata as input for classification. Based on Maple, we explore the effect of venues, authors, and references on paper tagging in the 19 fields when using the three selected classifiers. We have the following major observations:</p><p>? The effect of metadata varies significantly across different fields and classifiers. In general, venues are consistently beneficial in almost all cases, while the benefit of authors and references is highly dependent on the field and the classifier. For example, author information is evidently beneficial to paper tagging in the Philosophy field while harmful in Geology; references are helpful in Mathematics when we use Parabel but become disadvantageous in the same field when Transformer is adopted. ? The effect of metadata tends to be similar in two fields that belong to the same high-level scientific area. For example, Biology and Medicine are both life sciences (according to <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51]</ref>), and the effects of venues, authors, and references are largely aligned in the two fields. This finding implies that the experience of using metadata in one field can be extrapolated to a similar field. ? We also study the effect of metadata when predicting tags at different granularity levels. In a number of fields, venues improve the performance for not only coarse-grained tags but also very fine-grained ones, which may be unusual in the Computer Science field. The major reason is that some venues in these fields can indicate very fine tags (e.g., "Journal of Roman Archaeology" in History, "Mediaeval Studies" in Philosophy).</p><p>To summarize, this work makes the following contributions: (1) We construct a large-scale benchmark, Maple, for scientific literature tagging across 19 fields, whose field coverage is much broader than the datasets used in previous paper tagging studies <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59]</ref>. <ref type="bibr" target="#b1">(2)</ref> We comprehensively evaluate the performance of different types of multi-label classifiers in scientific literature tagging after incorporating metadata features. (3) Our empirical findings demonstrate some ubiquitous patterns of metadata's effects across all fields, as well as some unique patterns in fields other than computer science and biomedicine, which are not explored in previous studies. Our systematic studies are meant for providing insights to practitioners to build scientific literature taggers that can benefit all fields. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Text and Metadata. We represent the text information of a paper ? as a single text sequence T ? = ? 1 ? 2 ...? | T ? | by concatenating its title and abstract. The metadata of a paper ? is represented as a set M ? = {? 1 , ? 2 , .., ? |M ? | } consisting of its venue, author(s), and reference(s).</p><p>Problem Definition. The scientific literature tagging task can be cast as a large-scale multi-label classification problem, where all candidate tags (e.g., "Organic Chemistry", "Coupling Reaction", "Suzuki Reaction") constitute a large label space L (e.g., with 10 3 -10 5 labels). Given a scientific paper with its text and metadata information, the task is to find a set of labels from L that are relevant to the paper. Formally, the problem is defined as follows.</p><p>Definition 2.1. (Problem Definition) Given a training corpus D and the label space L, where each paper ? ? D is associated with its text T ? , metadata information M ? , and relevant tags L ? ? L, our objective is to learn a multi-label text classifier ? class that can map an unlabeled paper ? ? ? D to its relevant tags L ? ? ? L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET CONSTRUCTION</head><p>Previous studies on scientific literature tagging <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref> mainly use computer science and biomedicine papers to evaluate their proposed model, meanwhile paying less attention to other scientific fields. To bridge this gap, we construct Maple, a multifield benchmark for evaluating scientific literature tagging. Maple is built upon data from the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b37">[38]</ref>, which has been widely adopted in scientific text mining <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref>. MAG covers 19 academic fields, which are listed in Table <ref type="table" target="#tab_0">1</ref>. For each field, we conduct the following steps to construct a dataset.</p><p>Venue Selection. MAG maintains a list of the top-100 journals in each field according to the ?-index <ref type="bibr" target="#b12">[13]</ref>. When constructing Maple, we focus on papers published in these top journals. Note that some preprint services (e.g., "arxiv", "bioRxiv", "SSRN ", "NBER Working Paper") <ref type="bibr" target="#b42">[43]</ref> are also viewed as top journals in MAG, but we exclude them from our consideration. As a result, in Table <ref type="table" target="#tab_0">1</ref>, some of the constructed datasets contain less than 100 venues. Among the 19 fields, computer science (CS) has a unique publication culture: CS papers often appear first or exclusively in conferences rather than journals. Thus, for the Computer Science field, besides a collection of journal papers, we construct another dataset with papers from 75 top conferences according to CSRankings<ref type="foot" target="#foot_0">1</ref> . That being said, we will construct 20 datasets in total for the 19 fields.</p><p>Label Space Construction. For each field, we need a set of candidate labels for paper tagging. MAG has a directed acyclic graph (DAG)-structured label taxonomy L MAG <ref type="bibr" target="#b36">[37]</ref>. The taxonomy has 6 levels and more than 10 5 labels, where each of the 19 fields is a Layer-0 label (i.e., the most coarse-grained label). Given a field ? , we extract its descendant labels in the taxonomy L MAG as the label space L ? of this field, but we exclude ? itself from L ? because the root label is trivial to predict in classification. Since a label may have more than one parent in the DAG-structured taxonomy, it may appear in the label space of two or more fields. For example, the label "Anatomy" is a child of both "Biology" and "Medicine", so it is a candidate label in both the Biology and the Medicine datasets in Maple.</p><p>Paper Selection. Each paper ? in MAG is tagged with its relevant labels L ? ? L MAG <ref type="bibr" target="#b36">[37]</ref> <ref type="foot" target="#foot_1">2</ref> . To be included in Maple (given a field ? ), a paper ? needs to satisfy the following two criteria: (1) ? is published in a selected venue of ? ; (2) ? is labeled with ? and at least one of ? 's descendants (i.e., ? ? L ? and |L ? ? L ? | ? 1). When studying the scientific literature tagging task in a field ? , we focus on labels related to that field only. Therefore, the ground truth labels of ? are defined as L ? |? = L ? ? L ? . Note that a paper may appear in more than one field, and its ground truth labels are different when we consider different fields. For example, if a paper is tagged with "Medicine", "Polypharmacy", "Computer Science", and "Graph Embedding", given that "Polypharmacy" is a candidate label in the Medicine field and "Graph Embedding" is a candidate label in the Computer Science field, the paper will appear in both the Medicine and the Computer Science datasets in Maple. However, when we perform tagging in the Medicine field, the ground-truth label of the paper is "Polypharmacy"; when we perform tagging in the Computer Science field, the ground-truth label of the paper is "Graph Embedding".</p><p>Text and Metadata Extraction. For each selected paper ?, we extract its title, abstract, venue, author(s), and reference(s) from MAG. The title and abstract are concatenated as text information T ? . The venue, author(s), and reference(s) constitute metadata features M ? of the paper.</p><p>Training-Validation-Testing Split. Maple contains academic papers published between Jan. 1, 1981 and Dec. 31, 2020. We use papers from 1981 to 2015 for training and validation, and papers from 2016 to 2020 for testing.</p><p>Statistics of the constructed 20 datasets in Maple can be found in Table <ref type="table" target="#tab_0">1</ref>. More details are shown in Appendix A.1. From now on, for convenience of discussion, we use the terms "field" and "dataset" interchangeably if there is no ambiguity. (In other words, we treat "Computer Science (Conference)" and "Computer Science (Journal)" as different fields.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODELS</head><p>Large-scale multi-label text classification (LMTC) has been extensively studied over the past decade. Various approaches are proposed and can be applied to scientific literature tagging. Based on how text is used as features in the classifier, existing LMTC approaches can be divided into three major categories: (1) Bag-ofwords classifiers <ref type="bibr">[2, 16, 19, 31-34, 39, 49, 50, 54]</ref> treat each document as a multiset of tokens while disregarding word position and order. Trees, embeddings, and linear layers are commonly used in these models. (2) Sequence-based classifiers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59</ref>] take each document as a sequence of tokens and train a CNN, RNN, or Transformer architecture from scratch to build a multi-label classifier. (3) Pre-trained language model classifiers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60]</ref> aim at transferring the knowledge learned from web-scale corpora (e.g., Wikipedia and PubMed) to the LMTC task, which can complement the text information from the training corpus. Since the goal of this paper is to study the effect of metadata, we select one LMTC model from each of the three categories that can be easily augmented with metadata information. Now we introduce the three selected models -Parabel <ref type="bibr" target="#b32">[33]</ref> (bag-of-words), Transformer <ref type="bibr" target="#b43">[44]</ref> (sequence-based), and OAG-BERT <ref type="bibr" target="#b23">[24]</ref> (pre-trained language model) -and how they can take text and metadata features as input for classification.</p><p>4.1 Bag-of-Words Classifier: Parabel <ref type="bibr" target="#b32">[33]</ref> 4.1.1 Using Text Only. In general, bag-of-words classifiers represent each document ? as a |V D |-dimensional vector ? ? , where V D is the vocabulary of the training corpus D. Given a word ? ? V D , its corresponding entry in ? ? is defined using the tf-idf score:</p><formula xml:id="formula_0">? ?,? = tf (?, ?) ? idf (?, D).<label>(1)</label></formula><p>Here, tf (?, ?) is the term frequency of ? in document ?; idf (?, D) = log</p><formula xml:id="formula_1">| D | | {? ? ?D: ? ?T ? ? } |</formula><p>is the inverse document frequency of ?. The relevant labels of each document are represented by an |L|dimensional vector ? ? , where ? ?,? = 1 if ? is a tag relevant to ? (i.e., ? ? L ? ), and ? ?,? = 0 otherwise.</p><p>To perform multi-label text classification, Parabel learns an ensemble of multiple label trees, each of which is obtained by recursively partitioning the labels into two balanced groups until each node contains less than a certain number of labels. The partition process is implemented by spherical 2-means clustering based on the label representation ? ? , which is a unit vector in the direction of the mean of the training points containing label ?. After label space partitioning, Parabel learns a hierarchical discriminative classifier Pr(? ? |? ? ). Specifically, at each non-leaf node, a distribution is learned to determine which child nodes should be traversed; at each leaf node, a distribution is learned to predict the set of relevant tags. For more technical details, one can refer to <ref type="bibr" target="#b32">[33]</ref>, but we omit them here as they are not directly related to the usage of metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Using Text + Metadata.</head><p>It is straightforward to generalize bag-of-words classifiers to take metadata features. Let U D denote the set of metadata instances appearing in D. Given a metadata instance ? ? U D and a paper ?, we define tf (?, ?) and idf (?, D) as follows:</p><formula xml:id="formula_2">tf (?, ?) = 1(? ? M ? ), idf (?, D) = log | D | | {? ? ? D : ? ? M ? ? } | ,<label>(2)</label></formula><p>where 1(?) is the indicator function. One can find that the definitions in Eq. ( <ref type="formula" target="#formula_2">2</ref>) are well aligned with the definitions of tf (?, ?) and idf (?, D), except that a metadata instance does not appear multiple times in a document. Now we can have a "bag-of-metadata" representation x? for each paper. x? is a |U D |-dimensional vector, where x?,? = tf (?, ?) ? idf (?, D).</p><p>(</p><p>Finally, we represent each paper ? as a (|U D | + |V D |)-dimensional vector, that is, the concatenation of bag-of-words and "bag-ofmetadata" representations ? ? || x? . This vector is fed into Parabel for training and prediction.</p><p>4.2 Sequence-based Classifier: Transformer <ref type="bibr" target="#b43">[44]</ref> 4.2.1 Using Text Only. To apply Transformer <ref type="bibr" target="#b40">[41]</ref> to LMTC, we follow the architecture proposed in <ref type="bibr" target="#b43">[44]</ref>, which adds multiple "[CLS]" tokens in front of document text T ? as the input sequence. Formally, given</p><formula xml:id="formula_4">T ? = ? 1 ? 2 ...? | T ? |</formula><p>, the input sequence of paper ? is</p><formula xml:id="formula_5">I ? = [CLS 1 ] [CLS 2 ] ... [CLS ? ] ? 1 ? 2 ... ? |T? |<label>(4)</label></formula><p>The motivation here is that when the label space is large (e.g., with 10 4 tags), the output representation of one "[CLS]" token (e.g., a vector with several hundred dimensions) may not carry enough information to predict relevant labels. Therefore, multiple "[CLS]" tokens are needed to probe the semantics of text from different perspectives.</p><p>After Transformer encodes the input sequence I ? , each token ? ? I ? will have an output representation ? ? . We concatenate the representations of all "[CLS]" tokens together as the paper embedding:</p><formula xml:id="formula_6">? ? = ? [CLS 1 ] | | ? [CLS 2 ] | | ... | | ? [CLS ? ] . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>? ? is further fed into a fully connected layer to perform multi-label classification:</p><formula xml:id="formula_8">?? = Sigmoid(? ? ? ? + ?).<label>(6)</label></formula><p>Here, ?? is an |L|-dimensional vector, in which ??,? is the predicted probability that paper ? is relevant to label ?. The classifier is trained to minimize the following binary cross-entropy (BCE):</p><formula xml:id="formula_9">- ?? ? ?L (? ?,? log ??,? + (1 -? ?,? ) log(1 -??,? )).<label>(7)</label></formula><p>4.2.2 Using Text + Metadata. The fully connected attention mechanism in Transformer paves an easy way to incorporate metadata. To be specific, following <ref type="bibr" target="#b58">[59]</ref>, we can directly insert metadata tokens into the input sequence. Given paper text</p><formula xml:id="formula_10">T ? = ? 1 ...? | T ? | and metadata M ? = {? 1 , ..., ? | M ? | }, the input sequence is I ? = [CLS 1 ] ... [CLS ? ] ? 1 ... ? |M? | [SEP] ? 1 ... ? |T? |<label>(8)</label></formula><p>Unlike in CNN <ref type="bibr" target="#b20">[21]</ref> or RNN <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b52">53]</ref> classifiers, in Transformer, the order of metadata instances does not matter much because each pair of (metadata, word) or (metadata, metadata) can interact with each other via the fully connected attention mechanism during encoding. In our experiments, when listing authors in I ? , we follow the authorship order (i.e., 1 st author, 2 nd author, ...); when listing references in I ? , we adopt a random order because the citation order and inline contexts are not stored in MAG. Following <ref type="bibr" target="#b58">[59]</ref>, we treat each metadata instance ? ? ? I ? as one token during Transformer encoding. For example, the venue "WWW " is represented as one token "[Venue_1135342153]" instead of its textual name "the web conference" containing multiple tokens.   After I ? is fed to Transformer, the remaining designs of the metadata-aware classifier exactly follow Eqs. ( <ref type="formula" target="#formula_6">5</ref>)-( <ref type="formula" target="#formula_9">7</ref>).</p><formula xml:id="formula_11">(0, 0) (0, 1) (0, 2) ? (1, 0) (1, 1) ? (2, 0) (2, 1) (3, 0) (3, 1) (3, 2) (3, 3) (3, 4)<label>(4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-trained Language Model Classifier:</head><p>OAG-BERT <ref type="bibr" target="#b23">[24]</ref> Previous classifiers built upon pre-trained language models (PLMs) mainly utilize BERT <ref type="bibr" target="#b9">[10]</ref>, BioBERT <ref type="bibr" target="#b19">[20]</ref>, SciBERT <ref type="bibr" target="#b2">[3]</ref>, XLNet <ref type="bibr" target="#b45">[46]</ref>, or RoBERTa <ref type="bibr" target="#b24">[25]</ref> to derive document representations. However, these PLMs mostly focus on text information during pre-training and are not specifically designed to deal with metadata. To bridge this gap, we adopt OAG-BERT <ref type="bibr" target="#b23">[24]</ref>, an entity-augmented academic language model, which can jointly encode scientific text and venue/author information.</p><p>The pre-training process of OAG-BERT is briefly illustrated in Figure <ref type="figure" target="#fig_2">2</ref>(a). It places text and metadata information in a single sequence for masked language modeling. Besides, it proposes three strategies to deal with metadata entities: (1) heterogeneous entity type embedding makes the model aware of different metadata types;</p><p>(2) span-aware entity masking selects a continuous span within long entities (e.g., the venue "knowledge discovery and data mining");</p><p>(3) 2-dimensional positional embedding jointly models inter and intra-entity token orders. The model is first trained on 5 million full-text papers and then on 120 million paper titles/abstracts with metadata from the Open Academic Graph <ref type="bibr" target="#b54">[55]</ref>. </p><formula xml:id="formula_12">? ? = OAG-BERT( T ? ).<label>(9)</label></formula><p>Here, ? ? = [? ?,1 , ..., ? ?,? ] contains the output vectors of all tokens.</p><p>We then adopt mean pooling to obtain the paper representation ? ? .</p><formula xml:id="formula_13">? ? = 1 ? ? ?? ?=1 ? ?,? .<label>(10)</label></formula><p>Directly fine-tuning the PLM using a BCE loss (i.e., Eq. ( <ref type="formula" target="#formula_9">7</ref>)) is very time-consuming, especially when the label space is large. Considering efficiency and model simplicity, we fix the paper representation ? ? to train a Parabel classifier ? Parabel (? ? ) = Pr(? ? |? ? ).</p><p>During the prediction stage, given a paper ? ? , the trained classifier predicts the probability that ? ? is relevant to each label ?:</p><formula xml:id="formula_14">?? ? = ? Parabel (? ? ? ).<label>(11)</label></formula><p>Then, we can rank the labels according to ?? ? ,? to predict a list of the most relevant labels. However, in practice, we find this strategy does not yield competitive prediction accuracy. This finding is consistent with the design in related studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref> that discrete lexical features should be considered together with continuous representations during classification. To add lexical features, motivated by <ref type="bibr" target="#b59">[60]</ref>,</p><p>we propose a simple heuristic to re-rank the labels: Given a paper ? ? , all labels whose name appears in the paper text T ? ? will be ranked higher than those not appearing in T ? ? . This heuristic is equivalent to ranking the labels according to a modified score ?? ? ,? = ?? ? ,? + 1(? ? ? T ? ? ), where ? ? stands for the textual name of label ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Using</head><p>Text + Metadata. When metadata information is available, we use OAG-BERT to jointly encode paper text and metadata (as illustrated in Figure <ref type="figure" target="#fig_2">2(c))</ref>.</p><formula xml:id="formula_15">? ? = OAG-BERT( T ? , M ? ).<label>(12)</label></formula><p>The remaining steps exactly follow the text-only model. It is worth noting that since references are not involved during the pre-training of OAG-BERT, we can only use venues and authors as metadata here. Also, unlike the Transformer classifier that views each venue/author as one token, OAG-BERT tokenizes the textual name of venues/ authors based on its vocabulary during encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Setup</head><p>Datasets and Compared Methods. We have introduced the 20 datasets in Section 3. For each of the three classifiers, we test its performance when using text only, text+venue, text+author, and text+reference in order to check the effect of each metadata type separately. (Recall that OAG-BERT cannot take references as metadata features, so it does not have the text+reference variant.) For detailed hyperparameter settings, one can refer to Appendix A.2.</p><p>Evaluation Metrics. Following previous studies on multi-label text classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59</ref>], we adopt P@? and NDCG@? as evaluation metrics, where ? = 1, 3, 5. Given a paper ?, let rank(?) be the index of the ?-th highest predicted label according to each classifier, then</p><formula xml:id="formula_16">P@? = 1 ? ? ?? ?=1 ? ?,rank(? ) . DCG@? = ? ?? ?=1 ? ?,rank(? ) log(? + 1) , NDCG@? = DCG@? min(?,||?? || 0 ) ?=1 1 log(?+1) . (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Analysis</head><p>Table <ref type="table">2</ref> shows the P@? and NDCG@? scores of the three classifiers on the 20 datasets. We run each experiment 5 times with the average score reported. We conduct two-tailed t-tests to check the statistical significance of metadata's effect. To be specific, given a field and a classifier, if a score is significantly improved (with p-value &lt; 0.05) after using a certain type of metadata in comparison with using text We represent each field with a 24-dimensional vector based on the effect of venue, author, and reference information on the three classifiers. Then, we apply t-SNE <ref type="bibr" target="#b26">[27]</ref> to visualize these fields in a 2-dimensional space. The color scheme highlights several high-level scientific areas, following the major clusters of science detected by <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51]</ref> and suggesting similar effects of metadata within each area.</p><p>only, we mark the score as blue in Table <ref type="table">2</ref>; if a score significantly deteriorates (with p-value &lt; 0.05), we mark the score as red .</p><p>From Table <ref type="table">2</ref>, we can observe that: (1) The effect of metadata varies remarkably across different fields. For instance, author information is significantly helpful in Computer Science and Engineering when Parabel is utilized as the classifier but becomes harmful in Biology and Medicine when the same classifier is adopted. References are useful in Business and Economics when Parabel is employed but become disadvantageous in the same fields if Transformer is the tagger. (2) Venues are consistently beneficial to scientific literature tagging in almost all cases. To be specific, all 20 fields significantly benefit from venues <ref type="foot" target="#foot_2">3</ref> when Parabel is used, 18 fields when Transformer is used, and 18 fields when OAG-BERT is used. In the remaining 4 cases where venue information is not beneficial, neither is it harmful. (3) Authors are helpful in a majority of (15 and 17, respectively) fields when Parabel and OAG-BERT are the taggers but rarely help when Transformer is adopted. References are useful in even fewer cases. The overall performance of the three types of metadata is reflected by the macro average of P@? and NDCG@? scores over the 20 datasets, which are shown in Table <ref type="table">3</ref>. The reason why authors and references do not work in the Transformer classifier is that their embeddings need to be trained from scratch without good initialization. In contrast, leveraging venues only incurs a small number of additional parameters. Therefore, if one aims to utilize author and reference information, some embedding pre-training techniques <ref type="bibr" target="#b58">[59]</ref> may help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect in Different Fields</head><p>In this section, we examine the effect of metadata in different fields. Given a field ? , to describe the effect of metadata in ? , we construct a 24-dimensional vector in the following way: There are three classifiers (i.e., Parabel, Transformer, and OAG-BERT) and three types of metadata (i.e., Venue, Author, and Reference) studied Table 2: P@? and NDCG@? scores on the 20 datasets. Blue : significantly better than using text only (p-value &lt; 0.05). Red : significantly worse than using text only (p-value &lt; 0.05). "-": OAG-BERT cannot take references as metadata signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field Input</head><p>Parabel <ref type="bibr" target="#b32">[33]</ref> Transformer <ref type="bibr" target="#b43">[44]</ref> OAG-BERT [24] P@1 P@3 P@5 N@3 N@5 P@1 P@3 P@5 N@3 N@5 P@1 P@3 P@5 N@3 N@5   <ref type="table">3</ref>: Macro average of P@? and NDCG@? over the 20 datasets. Blue , Red , and "-": the same meaning as in Table <ref type="table">2</ref>.</p><p>Input Parabel <ref type="bibr" target="#b32">[33]</ref> Transformer <ref type="bibr" target="#b43">[44]</ref> OAG-BERT [24] P@1 P@3 P@5 N@3 N@5 P@1 P@3 P@5 N@3 N@5 P@1 P@3 P@5 N@3 N@5 in our experiments, so there are 3 ? 3 -1 = 8 (classifier, metadata) combinations, since (OAG-BERT, Reference) is not applicable.</p><p>For each of the 8 (classifier, metadata) combinations, we calculate the relative performance change of P@1, P@3, and P@5 by comparing the classifier using text only and the classifier using text together with the metadata. For example, given the (Parabel, Venue) combination, we calculate the following 3 values: P@? (Parabel, Text + Venue) -P@? (Parabel, Text) P@? (Parabel, Text) , ? = 1, 3, 5. ( <ref type="formula">14</ref>)</p><p>In total, we will have 3 ? 8 = 24 values, which can form a 24dimensional vector ? ? . We compute ? ? for all 20 fields and then apply t-SNE <ref type="bibr" target="#b26">[27]</ref> to visualize these vectors in a 2-dimensional space.</p><p>The visualization result is shown in Figure <ref type="figure" target="#fig_4">3</ref>, where we color each field according to high-level scientific areas/subareas detected by <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51]</ref>. To be specific, Rosvall and Bergstrom <ref type="bibr" target="#b34">[35]</ref> cluster scientific fields into four major areas -social sciences, physical sciences, life sciences, and ecology &amp; earth sciences -based on a large-scale paper citation network. The cluster of physical sciences is further split into two subareas, one of which is related to mathematics and computer science, and the other to physics and chemistry; Yin et al. <ref type="bibr" target="#b50">[51]</ref> observe a similar partitioning of the 19 fields when studying public uses of scientific papers in each field.</p><p>In Figure <ref type="figure" target="#fig_4">3</ref>, we find that fields with the same color are often embedded closer, indicating that metadata have similar effects in fields belonging to the same area/subarea. This finding is very useful when we need to extrapolate the experience of using metadata in one field to a similar field. For example, one may have known that references are beneficial and authors are harmful when using Parabel in the Medicine field. Based on our finding, the same pattern can be deduced when using Parabel in the Biology field because Medicine and Biology are both life sciences. According to Table <ref type="table">2</ref>, this deduction is correct.</p><p>Meanwhile, we observe two exceptions: (1) The area of social sciences is split into two clusters, one of which contains Art, History, Philosophy, Sociology, and Political Science, and the other contains Economics and Business. The effects of metadata in the Business and Economics fields are more similar to those in Mathematics and Computer Science. This is possibly because a large proportion of Economics and Business papers rely on quantitative analysis of massive data to draw conclusions, which is different from the paradigm in most Art, History, and Philosophy papers. (2) Geography is embedded closer to social sciences than it is to Geology and Environmental Science. This is possibly because Geography is an interdisciplinary field. The physical geography subfield is more related to earth sciences while the human geography subfield is closer to social sciences. Overall, we still observe commonalities in the effects of metadata within high-level areas/subareas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect at Different Label Granularities</head><p>Now we examine the effect of metadata when predicting labels at different granularity levels. The MAG taxonomy <ref type="bibr" target="#b36">[37]</ref> has 6 layers (from the most coarse-grained Layer 0 to the most fine-grained  The effect of metadata on Layer-? P@? scores (averaged over the fields that significantly benefit from leveraging that type of metadata using the classifier). Table <ref type="table">4</ref>: The fields that benefit the most from leveraging venue information in terms of Layer-1 P@1, Layer-2 P@1, and Layer-3 P@1. "T": text only. "+V": text+venue. "?": absolute performance improvement. L1 P@1 L2 P@1 L3 P@1 Parabel <ref type="bibr">[</ref> Layer 5). Layer-0 labels are the 19 fields excluded from our label space. Across all 20 datasets, 85.2%, 86.5%, 70.4%, 35.0%, and 13.2% of the papers have ground-truth Layer-1, Layer-2, Layer-3, Layer-4, and Layer-5 labels, respectively. Due to the low proportions of papers related to Layer-4 and Layer-5 tags, we only study the effect of metadata on predicting Layer-1, Layer-2, and Layer-3 tags.</p><p>Given one classifier and one type of metadata, we consider all fields that significantly benefit from the metadata using the classifier. In these fields, we calculate the Layer-? P@? scores (( ?, ?) = (1, 1), (2, 1), (2, 3), (3, 1), <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b2">3)</ref>). The definition of Layer-? P@? is very similar to that of P@? except that Layer-? P@? considers the ? most confident labels at Layer ? instead of in the whole label space. When calculating Layer-? P@?, we focus on papers with at least one ground-truth Layer-? label. Then, we compute the absolute performance change of Layer-? P@? scores after incorporating the considered type of metadata. The results are shown in Figure <ref type="figure" target="#fig_6">4</ref>.</p><p>From Figure <ref type="figure" target="#fig_6">4</ref>, we find that: (1) On average, venues can help scientific literature tagging for not only coarse-grained tags but also fine-grained ones. This may be counterintuitive from computer scientists' perspective because CS venues (e.g., "WWW ") can hardly indicate very fine tags (e.g., "Link Farm"). Indeed, in the Computer Science (Conference) dataset, the contribution of venues on Layer-3 P@1 is subtle (e.g., 0.19% when using Parabel, -0.11% when using Transformer). However, in fields other than Computer Science, some venues do carry very fine-grained signals. For example, there are two venues "Journal of Roman Archaeology" and "Mediaeval Studies" in History and Philosophy, respectively. These two venues may strongly imply a paper's relevance to "Classical Archaeology" and "Medievalism", which are Layer-2 and Layer-3 tags, respectively. In Table <ref type="table">4</ref>, we list the fields that benefit the most from leveraging venue information in terms of Layer-1 P@1, Layer-2 P@1, and Layer-3 P@1, where we do observe that Philosophy and History are the biggest beneficiaries in terms of Layer-3 P@1.</p><p>(2) Different from venues, authors are beneficial to fine-grained tagging but harmful to coarse-grained prediction. This observation consistently holds across all three classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Extreme Multi-Label Text Classification. In keeping with our discussion in Section 4, we divide related studies on extreme multilabel classification into three major categories. (1) Bag-of-words classifiers <ref type="bibr">[2, 16, 19, 31-34, 39, 49, 50, 54]</ref> take sparse tf-idf features as input. To improve model efficiency, 1-vs-all approaches such as DiSMEC <ref type="bibr" target="#b1">[2]</ref> and PPDSparse <ref type="bibr" target="#b48">[49]</ref> explore parallelism and model size reduction via model weight truncation. In another direction, tree-based approaches apply various partitioning techniques on the large label space. For example, Parabel <ref type="bibr" target="#b32">[33]</ref> partitions the labels to a balanced tree structure using 2-means clustering; Bonsai <ref type="bibr" target="#b18">[19]</ref> improves Parabel by allowing multi-way and unbalanced partitions; XR-Linear <ref type="bibr" target="#b53">[54]</ref> improves Parabel by incorporating various hard negative sampling schemes; AnnexML <ref type="bibr" target="#b38">[39]</ref> partitions the labels via graph-based nearest neighbor indices. (2) Sequence-based classifiers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59</ref>] employ deep neural architectures such as CNNs (e.g., XML-CNN <ref type="bibr" target="#b20">[21]</ref>), RNNs (e.g., MeshProbeNet <ref type="bibr" target="#b44">[45]</ref> and AttentionXML <ref type="bibr" target="#b52">[53]</ref>), and Transformers (e.g., BertXML <ref type="bibr" target="#b43">[44]</ref>) to learn semantic representations of input text sequences for classification. There are also studies aggregating shallow word embeddings and/or applying MLP layers to obtain document embeddings, such as Slice <ref type="bibr" target="#b14">[15]</ref>, DeepXML <ref type="bibr" target="#b7">[8]</ref>, DECAF <ref type="bibr" target="#b28">[29]</ref>, GalaXC <ref type="bibr" target="#b35">[36]</ref>, and ECLARE <ref type="bibr" target="#b29">[30]</ref>. <ref type="bibr" target="#b2">(3)</ref> Pre-trained language model classifiers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60]</ref> propose to transfer the knowledge learned by PLMs from web-scale corpora to the classification task. For example, X-Transformer <ref type="bibr" target="#b3">[4]</ref> complements the output of BERT <ref type="bibr" target="#b9">[10]</ref>, XLNet <ref type="bibr" target="#b45">[46]</ref>, or RoBERTa <ref type="bibr" target="#b24">[25]</ref> with sparse tf-idf features; LightXML <ref type="bibr" target="#b16">[17]</ref> adopts PLMs as the text encoder and performs label shortlist and re-ranking with the same PLM; XR-Transformer <ref type="bibr" target="#b55">[56]</ref> further proposes fast multi-resolution PLM finetuning. Despite the success of these models in extreme multi-label classification, they mainly focus on classifying plain text sequences and are less aware of document metadata. In contrast, our work proposes several straightforward ways to enhance text classifiers with metadata signals.</p><p>Scientific Literature Tagging. Classifying academic papers is a common evaluation task in text mining <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58]</ref> and graph mining <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> studies. However, most studies consider coarse-grained paper classification only (e.g., with 5-20 categories in the label space), the result of which is not subdivided enough to satisfy users' fine-grained interests. To tag papers on PubMed with fine-grained medical subject headings, the task of MeSH indexing has been extensively studied <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52]</ref>. However, these models only use text or text+venue as input, leaving the effect of authors and references unexplored. Recently, Zhang et al. <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> and Ye et al. <ref type="bibr" target="#b46">[47]</ref> make use of metadata to tag papers with fields of study in MAG. Still, these studies are restricted to computer science and biomedicine fields only. In comparison, our work conducts a systematic study across 19 fields and three major types of classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we examine the effect of metadata on scientific literature tagging in 19 fields using three classifiers. Our results provide the following insights to practitioners aiming at building accurate scientific literature taggers: First, while previous metadata-aware approaches often directly use all types of available metadata, we show that not all of them are always beneficial. It is important to select useful metadata features based on the classifier's type, the field, and the granularity level of predicted tags. Second, although the state-of-the-art models for text sequence modeling have been dominated by Transformer-based models, we demonstrate that simple bag-of-words classifiers work comparably well or even better in many cases for large-scale fine-grained paper tagging, and may be more effective in leveraging different types of metadata. Third, despite the varying effects of different metadata types in different fields, the gain or loss induced by each metadata type is rather consistent across similar fields.</p><p>This study explores each type of metadata separately to avoid confounders. However, different types of metadata (e.g., a venue and an author) may interact with each other and provide additional hints for classification. In the future, it is of our interest to study the composite effect of multiple types of metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Datasets</head><p>Supplementary to Table <ref type="table" target="#tab_0">1</ref>, Table <ref type="table" target="#tab_5">5</ref> summarizes more dataset statistics in Maple, including the average numbers of authors, references, and labels per paper as well as the numbers of training, validation, and testing papers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameters</head><p>In each of the 20 datasets, we remove metadata instances that appear in less than 5 papers. We adopt the same hyperparameter configuration when using text only, text+venue, text+author, and text+reference as input. The code source and hyperparameters of each classifier are introduced below.</p><p>A.2.1 Parabel. <ref type="foot" target="#foot_3">4</ref> We remove words appearing in less than 5 papers. All parameters are set by default. Specifically, the number of threads ? = 1; the number of trees ? = 3; the maximum number of labels in a leaf node ? = 100; the beam search width in prediction ? = 10.</p><p>A.2.2 Transformer. <ref type="foot" target="#foot_4">5</ref> The number of Transformer layers is 1; the number of attention heads is 2; the number of [CLS] tokens ? = 10; the embedding dimension is 100; the maximum sequence length is 500; the batch size is 256. We adopt GloVe.6B.100d as initialized word embeddings. For (the number of training epochs, the number of warm-up epochs), we use (100, 20) for Art, Philosophy, and Geography, (80, 16) for Business, Sociology, History, and Political Science, <ref type="bibr" target="#b59">(60,</ref><ref type="bibr" target="#b11">12)</ref> for Environmental Science, Economics, Engineering, and Computer Science, <ref type="bibr" target="#b39">(40,</ref><ref type="bibr" target="#b7">8)</ref> for Psychology, Geology, and Mathematics, <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b3">4)</ref> for Materials Science, Physics, and Biology, and (15, 3) for Chemistry and Medicine. Other hyperparameters are set by default.</p><p>A.2.3 OAG-BERT. <ref type="foot" target="#foot_5">6</ref> We use "oagbert-v2" as the PLM. After PLM encoding, we fix paper embeddings to train a Parabel classifier. All parameters of Parabel are set by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More on the Effect of Metadata at Different Label Granularities</head><p>Supplementary to Table <ref type="table">4</ref>, Table <ref type="table">6</ref> shows the fields that benefit the most from leveraging author or reference information in terms of Layer-1 P@1, Layer-2 P@1, and Layer-3 P@1. We find that authors and references are beneficial to the Art, Philosophy, and History fields in many cases. The possible reason is that each paper in these fields has a small number of authors and references (according to the statistics in Table <ref type="table" target="#tab_5">5</ref>). Therefore, the author or reference list may contain fewer confounding signals and be more topic-indicative.</p><p>Table <ref type="table">6</ref>: The fields that benefit the most from leveraging author or reference information in terms of Layer-1 P@1, Layer-2 P@1, and Layer-3 P@1. "T": text only. "+A": text+author. "+R": text+reference. "?": absolute performance improvement.</p><p>L1 P@1 L2 P@1 L3 P@1 Parabel <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Effect of Metadata on Efficiency</head><p>Now we report the effect of metadata on model efficiency. Table <ref type="table" target="#tab_7">7</ref> shows the average relative training time increase of the three classifiers across the 20 datasets after incorporating venues, authors, and references, respectively. All models are run on Intel Xeon E5-2680 v2 @ 2.80GHz and one NVIDIA GeForce GTX 1080 Ti GPU (if a GPU is needed). We can observe a significant increase in training time after adding references as features. When Parabel is the classifier, the increase is due to a much longer bag-of-words vector used to represent a paper; when Transformer is the classifier, the increase is caused by a large number of additional parameters (i.e., reference embeddings), which make the model converge more slowly.   Table <ref type="table">9</ref>: P@? and NDCG@? scores on the three additional datasets. Blue , Red , and "-": the same meaning as in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Field Input</head><p>Parabel <ref type="bibr" target="#b32">[33]</ref> Transformer <ref type="bibr" target="#b43">[44]</ref> OAG-BERT [24] P@1 P@3 P@5 N@3 N@5 P@1 P@3 P@5 N@3 N@5 P@1 P@3 P@5 N@3 N@5 small datasets, but the situation is reversed on moderate-sized and large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Additional Datasets with MeSH Labels</head><p>To further strengthen our findings, we construct three datasets with Medical Subject Headings (MeSH) terms <ref type="bibr" target="#b6">[7]</ref> as their labels, which are curated by experts from the National Library of Medicine.</p><p>To be specific, we take the three datasets, Biology, Chemistry, and Medicine, from Maple and obtain the ground-truth MeSH labels of each paper<ref type="foot" target="#foot_6">7</ref> . After removing papers not having MeSH labels, we get three new datasets, Biology-MeSH, Chemistry-MeSH, and Medicine-MeSH. Their statistics are shown in Table <ref type="table" target="#tab_8">8</ref>. We run Parabel, Transformer, and OAG-BERT on the three new datasets. When running Transformer, for (the number of training epochs, the number of warm-up epochs), we use <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b3">4)</ref> for all three datasets. When running OAG-BERT, we need to rerank those labels appearing in the paper text higher than those not. Note that a MeSH label may have multiple label names (i.e., one canonical name and 0, 1, or several entry terms, see the MeSH label "COVID-19"<ref type="foot" target="#foot_7">8</ref> as an example). Given a MeSH label, if any of its label names appears in the paper text, we view it as occurring in the paper. All other hyperparameters are the same as in Appendix A.2.</p><p>The P@? and NDCG@? scores of Parabel, Transformer, and OAG-BERT on the three new datasets are demonstrated in Table <ref type="table">9</ref>. In general, we still find that venues are beneficial to scientific</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>MASK] ? Jie Tang ? data mining knowledge [MASK] [MASK] [MASK] [MASK] Tsinghua</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The pre-training process of OAG-BERT<ref type="bibr" target="#b23">[24]</ref> Encoding a paper when we use text + metadata</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The pre-training process and our usage of OAG-BERT [24].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 3 . 1</head><label>31</label><figDesc>Using Text Only. When considering paper text T ? only for classification, we first use OAG-BERT to encode the text sequence (as illustrated in Figure 2(b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: We represent each field with a 24-dimensional vector based on the effect of venue, author, and reference information on the three classifiers. Then, we apply t-SNE<ref type="bibr" target="#b26">[27]</ref> to visualize these fields in a 2-dimensional space. The color scheme highlights several high-level scientific areas, following the major clusters of science detected by<ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51]</ref> and suggesting similar effects of metadata within each area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The effect of metadata on Layer-? P@? scores (averaged over the fields that significantly benefit from leveraging that type of metadata using the classifier).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 Figure 5 :</head><label>55</label><figDesc>Figure 5 shows the training time of the three classifiers on the 20 datasets. The reported training time is an average over 20 runs (i.e., 5 runs ? {text only, text+venue, text+author, text+reference}) when Parabel and Transformer are tested, or 15 runs (i.e., 5 runs ? {text only, text+venue, text+author}) when OAG-BERT is tested. Among the three classifiers, Parabel is consistently the most efficient across the 20 datasets; Transformer spends more time than OAG-BERT on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the 20 datasets in Maple across 19 fields.There are 2 datasets in the Computer Science field, one of which is collected from top conferences and the other from top journals.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Text 0.7513 0.5811 0.4678 0.7076 0.6977 0.7510 0.5673 0.4507 0.6896 0.6712 0.6983 0.5354 0.4275 0.6549 0.6429 +Venue 0.7554 0.5858 0.4717 0.7130 0.7033 0.7599 0.5753 0.4572 0.6995 0.6812 0.7004 0.5391 0.4318 0.6588 0.</figDesc><table><row><cell>Macro Average</cell><cell>+Author</cell><cell cols="6">6480 0.7512 0.5817 0.4687 0.7079 0.6983 0.7442 0.5594 0.4433 0.6809 0.6617 0.6984 0.5374 0.4305 0.6569 0.6461</cell></row><row><cell></cell><cell cols="2">+Reference 0.7487 0.5809 0.4689 0.7065 0.6977</cell><cell>0.7277 0.5469 0.4328 0.6652 0.6459</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b32">33]</ref> </figDesc><table><row><cell></cell><cell>T</cell><cell>0.7178</cell><cell></cell><cell>T</cell><cell>0.7846</cell><cell></cell><cell>T</cell><cell>0.6118</cell></row><row><cell>Physics</cell><cell>+V</cell><cell>0.7284</cell><cell>Philosophy</cell><cell>+V</cell><cell>0.8170</cell><cell>Philosophy</cell><cell>+V</cell><cell>0.7209</cell></row><row><cell></cell><cell>?</cell><cell>1.06%</cell><cell></cell><cell>?</cell><cell>3.24%</cell><cell></cell><cell>?</cell><cell>10.91%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Transformer [44]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>T</cell><cell>0.7393</cell><cell></cell><cell>T</cell><cell>0.6728</cell><cell></cell><cell>T</cell><cell>0.4105</cell></row><row><cell>Physics</cell><cell>+V</cell><cell>0.7495</cell><cell>Philosophy</cell><cell>+V</cell><cell>0.7504</cell><cell>Philosophy</cell><cell>+V</cell><cell>0.6071</cell></row><row><cell></cell><cell>?</cell><cell>1.02%</cell><cell></cell><cell>?</cell><cell>7.76%</cell><cell></cell><cell>?</cell><cell>19.66%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">OAG-BERT [24]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>T</cell><cell>0.6626</cell><cell></cell><cell>T</cell><cell>0.6747</cell><cell></cell><cell>T</cell><cell>0.3455</cell></row><row><cell>Geography</cell><cell>+V</cell><cell>0.6679</cell><cell>Art</cell><cell>+V</cell><cell>0.6879</cell><cell>History</cell><cell>+V</cell><cell>0.4130</cell></row><row><cell></cell><cell>?</cell><cell>0.53%</cell><cell></cell><cell>?</cell><cell>1.32%</cell><cell></cell><cell>?</cell><cell>6.75%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>More statistics of the 20 datasets in Maple.</figDesc><table><row><cell></cell><cell>#Authors/</cell><cell>#References/</cell><cell>#Labels/</cell><cell>#Train</cell><cell>#Valid</cell><cell>#Test</cell></row><row><cell>Field</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Paper</cell><cell>Paper</cell><cell>Paper</cell><cell>Papers</cell><cell>Papers</cell><cell>Papers</cell></row><row><cell>Art</cell><cell>1.314</cell><cell>2.813</cell><cell>2.425</cell><cell>39,901</cell><cell>9,975</cell><cell>8,497</cell></row><row><cell>Philosophy</cell><cell>1.117</cell><cell>8.121</cell><cell>3.715</cell><cell>40,696</cell><cell>10,174</cell><cell>8,426</cell></row><row><cell>Geography</cell><cell>3.625</cell><cell>31.793</cell><cell>2.231</cell><cell>44,710</cell><cell>11,178</cell><cell>17,995</cell></row><row><cell>Business</cell><cell>2.346</cell><cell>37.635</cell><cell>3.556</cell><cell>49,481</cell><cell>12,370</cell><cell>23,007</cell></row><row><cell>Sociology</cell><cell>1.592</cell><cell>24.474</cell><cell>2.318</cell><cell>60,815</cell><cell>15,204</cell><cell>14,189</cell></row><row><cell>History</cell><cell>1.218</cell><cell>4.405</cell><cell>1.938</cell><cell>80,830</cell><cell>20,208</cell><cell>12,109</cell></row><row><cell>Political Science</cell><cell>1.496</cell><cell>10.198</cell><cell>3.107</cell><cell>73,650</cell><cell>18,413</cell><cell>23,228</cell></row><row><cell>Environmental Science</cell><cell>4.301</cell><cell>34.156</cell><cell>2.138</cell><cell>70,479</cell><cell>17,620</cell><cell>35,846</cell></row><row><cell>Economics</cell><cell>1.966</cell><cell>26.608</cell><cell>4.961</cell><cell>119,309</cell><cell>29,827</cell><cell>29,534</cell></row><row><cell>Engineering</cell><cell>3.055</cell><cell>19.703</cell><cell>4.987</cell><cell>179,804</cell><cell>44,951</cell><cell>45,251</cell></row><row><cell>Psychology</cell><cell>3.805</cell><cell>41.201</cell><cell>5.032</cell><cell>245,288</cell><cell>61,322</cell><cell>66,344</cell></row><row><cell>CS (Conference)</cell><cell>3.389</cell><cell>17.417</cell><cell>6.089</cell><cell>136,322</cell><cell>34,081</cell><cell>92,990</cell></row><row><cell>CS (Journal)</cell><cell>3.296</cell><cell>21.968</cell><cell>5.976</cell><cell>214,616</cell><cell cols="2">53,654 142,333</cell></row><row><cell>Geology</cell><cell>3.552</cell><cell>36.510</cell><cell>5.957</cell><cell>284,022</cell><cell>71,006</cell><cell>76,806</cell></row><row><cell>Mathematics</cell><cell>2.151</cell><cell>19.068</cell><cell>6.433</cell><cell>338,454</cell><cell>84,613</cell><cell>67,484</cell></row><row><cell>Materials Science</cell><cell>4.829</cell><cell>26.838</cell><cell>4,428</cell><cell cols="3">783,289 195,822 358,620</cell></row><row><cell>Physics</cell><cell>9.841</cell><cell>23.509</cell><cell>7.271</cell><cell cols="3">905,613 226,403 237,967</cell></row><row><cell>Biology</cell><cell>5.611</cell><cell>36.137</cell><cell>8.306</cell><cell cols="3">1,125,605 281,401 181,772</cell></row><row><cell>Chemistry</cell><cell>4.293</cell><cell>28.382</cell><cell>6.288</cell><cell cols="3">1,252,531 313,133 284,292</cell></row><row><cell>Medicine</cell><cell>5.647</cell><cell>12.888</cell><cell>5.376</cell><cell cols="3">1,620,165 405,041 620,899</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc><ref type="bibr" target="#b32">33]</ref> </figDesc><table><row><cell>Mathematics</cell><cell>T +A ?</cell><cell>0.5960 0.5971 0.11%</cell><cell>Political Science</cell><cell>T +A ?</cell><cell>0.7928 0.7969 0.41%</cell><cell>Philosophy</cell><cell>T +A ?</cell><cell>0.6118 0.6160 0.42%</cell></row><row><cell></cell><cell>T</cell><cell>0.6992</cell><cell></cell><cell>T</cell><cell>0.6704</cell><cell></cell><cell>T</cell><cell>0.6124</cell></row><row><cell>Psychology</cell><cell>+R</cell><cell>0.7086</cell><cell>Business</cell><cell>+R</cell><cell>0.6801</cell><cell>Business</cell><cell>+R</cell><cell>0.6265</cell></row><row><cell></cell><cell>?</cell><cell>0.94%</cell><cell></cell><cell>?</cell><cell>0.97%</cell><cell></cell><cell>?</cell><cell>1.41%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Transformer [44]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>T</cell><cell>0.5471</cell><cell></cell><cell>T</cell><cell>0.6728</cell><cell></cell><cell>T</cell><cell>0.4859</cell></row><row><cell>History</cell><cell>+A</cell><cell>0.5489</cell><cell>Philosophy</cell><cell>+A</cell><cell>0.6900</cell><cell>Art</cell><cell>+A</cell><cell>0.4910</cell></row><row><cell></cell><cell>?</cell><cell>0.18%</cell><cell></cell><cell>?</cell><cell>1.72%</cell><cell></cell><cell>?</cell><cell>0.51%</cell></row><row><cell></cell><cell>T</cell><cell>0.5471</cell><cell></cell><cell>T</cell><cell>0.7859</cell><cell></cell><cell>T</cell><cell>0.4857</cell></row><row><cell>History</cell><cell>+R</cell><cell>0.5537</cell><cell>Art</cell><cell>+R</cell><cell>0.7953</cell><cell>History</cell><cell>+R</cell><cell>0.4890</cell></row><row><cell></cell><cell>?</cell><cell>0.66%</cell><cell></cell><cell>?</cell><cell>0.94%</cell><cell></cell><cell>?</cell><cell>0.33%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">OAG-BERT [24]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>T</cell><cell>0.6241</cell><cell></cell><cell>T</cell><cell>0.6747</cell><cell></cell><cell>T</cell><cell>0.3455</cell></row><row><cell>Art</cell><cell>+A</cell><cell>0.6254</cell><cell>Art</cell><cell>+A</cell><cell>0.6875</cell><cell>History</cell><cell>+A</cell><cell>0.3819</cell></row><row><cell></cell><cell>?</cell><cell>0.13%</cell><cell></cell><cell>?</cell><cell>1.28%</cell><cell></cell><cell>?</cell><cell>3.64%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Average relative increase in training time across 20 datasets after incorporating one type of metadata.</figDesc><table><row><cell></cell><cell cols="3">Parabel [33] Transformer [44] OAG-BERT [24]</cell></row><row><cell>+Venue</cell><cell>+0.15%</cell><cell>+0.13%</cell><cell>+0.11%</cell></row><row><cell>+Author</cell><cell>+0.72%</cell><cell>+1.86%</cell><cell>+0.11%</cell></row><row><cell>+Reference</cell><cell>+22.68%</cell><cell>+10.75%</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Statistics of the three additional datasets with MeSH labels.</figDesc><table><row><cell></cell><cell>Paper</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#Authors/</cell><cell>#References/</cell><cell>#Labels/</cell><cell>#Train</cell><cell>#Valid</cell><cell>#Test</cell></row><row><cell>Field</cell><cell cols="5">#Papers #Labels #Venues #Authors #References</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Paper</cell><cell>Paper</cell><cell>Paper</cell><cell>Papers</cell><cell>Papers</cell><cell>Papers</cell></row><row><cell>Biology-MeSH</cell><cell>Journal 1,379,393</cell><cell>25,039</cell><cell>100</cell><cell>2,486,814</cell><cell>6,876,739</cell><cell>5.686</cell><cell>40.043</cell><cell>13.870</cell><cell cols="2">985,364 246,341 147,688</cell></row><row><cell cols="2">Chemistry-MeSH Journal 762,129</cell><cell>21,585</cell><cell>87</cell><cell>1,498,358</cell><cell>5,928,908</cell><cell>4.741</cell><cell>34.344</cell><cell>10.984</cell><cell cols="2">511,814 127,954 122,361</cell></row><row><cell cols="2">Medicine-MeSH Journal 1,536,660</cell><cell>25,188</cell><cell>100</cell><cell>2,791,165</cell><cell>7,190,021</cell><cell>5.254</cell><cell>20.931</cell><cell>11.819</cell><cell cols="2">1,020,969 255,242 260,449</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Text 0.8924 0.7964 0.7088 0.8194 0.7576 0.9112 0.8098 0.7193 0.8341 0.7698 0.7506 0.6185 0.5460 0.6490 0.5945 +Venue 0.8934 0.7976 0.7101 0.8206 0.7587 0.9119 0.8105 0.7194 0.8348 0.7701 0.7520 0.6200 0.5473 0.6504 0.5958 +Author 0.8932 0.7985 0.7112 0.8212 0.7596 0.9090 0.8028 0.7093 0.8281 0.7614 0.7504 0.6184 0.5464 0.6488 0.5946 Biology-MeSH +Reference 0.8976 0.8058 0.7198 0.8279 0.7674 0.9079 0.7988 0.7034 0.8248 0.7565 .7340 0.6407 0.7603 0.6947 0.8445 0.7113 0.6082 0.7427 0.6684 0.6971 0.5804 0.5099 0.6071 0.5557 +Venue 0.8453 0.7354 0.6421 0.7615 0.6960 0.8465 0.7151 0.6121 0.7460 0.6720 0.6995 0.5826 0.5132 0.6093 0.5587 +Author 0.8450 0.7350 0.6419 0.7611 0.6957 0.8439 0.7105 0.6066 0.7419 0.6670 0.6981 0.5819 0.5126 0.6086 0.5580 Chemistry-MeSH +Reference 0.8490 0.7409 0.6491 0.7667 0.7023 0.8248 0.6802 0.5743 0.7139 0.6366 .8410 0.7454 0.8712 0.8075 0.9683 0.8567 0.7583 0.8842 0.8191 0.7343 0.6229 0.5629 0.6491 0.6089 +Venue 0.9673 0.8422 0.7472 0.8722 0.8090 0.9688 0.8585 0.7606 0.8856 0.8210 0.7370 0.6235 0.5607 0.6503 0.6081 +Author 0.9671 0.8342 0.7424 0.8656 0.8039 0.9687 0.8495 0.7496 0.8786 0.8117 0.7347 0.6185 0.5549 0.6459 0.6027</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Text</cell><cell cols="3">0.8447 0-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Text 0.9673 0Medicine-MeSH</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+Reference 0.9666 0.8382 0.7463 0.8687 0.8073</cell><cell>0.9662 0.8470 0.7458 0.8762 0.8084</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://csrankings.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The paper tags L ? come from the predictions of a system<ref type="bibr" target="#b36">[37]</ref> proposed by Microsoft Academic. The tags are accurate as checked by humans<ref type="bibr" target="#b36">[37]</ref> and have been used to support notable findings<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51]</ref>. Meanwhile, we also conduct experiments on three datasets with MeSH labels<ref type="bibr" target="#b6">[7]</ref>, which are curated by biomedical experts. Discussions on the additional three datasets can be found in Appendix A.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We say one field significantly benefits from one type of metadata (when using a certain classifier) if at least one of the five metrics is significantly improved (with p-value &lt; 0.05) after incorporating that type of metadata.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://manikvarma.org/code/Parabel/download.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/XunGuangxu/CorNet (We use the BertXML classifier in this GitHub repository. Although the model is called BertXML, it trains a Transformer architecture from scratch without BERT initialization.)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/THUDM/OAG-BERT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://learn.microsoft.com/en-us/academic-services/graph/reference-dataschema#paper-mesh</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p><ref type="bibr" target="#b7">8</ref> https://meshb.nlm.nih.gov/record/ui?ui=D000086382 literature tagging in almost all cases, while the effect of authors and references depends on the classifier's type and the field.The three additional datasets are also available in our Maple benchmark: https://doi.org/10.5281/zenodo.7611544.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank anonymous reviewers for their valuable and insightful feedback. Research was supported in part by the <rs type="programName">IBM-Illinois Discovery Accelerator Institute, US DARPA KAIROS Program</rs> No. <rs type="grantNumber">FA8750-19-2-1004</rs> and <rs type="funder">INCAS</rs> Program No. <rs type="grantNumber">HR001121C0165</rs>, <rs type="funder">National Science Foundation</rs> <rs type="grantNumber">IIS-19-56151</rs>, <rs type="grantNumber">IIS-17-41317</rs>, and <rs type="funder">IIS</rs> <rs type="grantNumber">17-04532</rs>, and the <rs type="institution">Molecule Maker Lab Institute</rs>: <rs type="programName">An AI Research Institutes program</rs> supported by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2019897</rs>, and the <rs type="institution">Institute for Geospatial Understanding</rs> through an <rs type="grantName">Integrative Discovery Environment (I-GUIDE</rs>) by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2118329</rs>. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of <rs type="affiliation">DARPA</rs> or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_b5pkAu9">
					<idno type="grant-number">FA8750-19-2-1004</idno>
					<orgName type="program" subtype="full">IBM-Illinois Discovery Accelerator Institute, US DARPA KAIROS Program</orgName>
				</org>
				<org type="funding" xml:id="_DXVEgwM">
					<idno type="grant-number">HR001121C0165</idno>
				</org>
				<org type="funding" xml:id="_EStCTVD">
					<idno type="grant-number">IIS-19-56151</idno>
				</org>
				<org type="funding" xml:id="_aJ74KQQ">
					<idno type="grant-number">IIS-17-41317</idno>
				</org>
				<org type="funding" xml:id="_NSPnH8t">
					<idno type="grant-number">17-04532</idno>
					<orgName type="program" subtype="full">An AI Research Institutes program</orgName>
				</org>
				<org type="funding" xml:id="_6s2NKvG">
					<idno type="grant-number">2019897</idno>
					<orgName type="grant-name">Integrative Discovery Environment (I-GUIDE</orgName>
				</org>
				<org type="funding" xml:id="_CceHrHW">
					<idno type="grant-number">2118329</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Construction of the Literature Graph in Semantic Scholar</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dismec: Distributed sparse machines for extreme multi-label classification</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Taming pretrained transformers for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3163" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SPECTER: Document-level Representation Learning using Citationinformed Transformers</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2270" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Medical subject headings used to search the biomedical literature</title>
		<author>
			<persName><forename type="first">H</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">L</forename><surname>Coletti</surname></persName>
		</author>
		<author>
			<persName><surname>Bleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="317" to="323" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sumeet Agarwal, and Manik Varma. 2021. Deepxml: A deep extreme multi-label learning framework applied to short text documents</title>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;21</title>
		<imprint>
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FullMeSH: improving large-scale MeSH indexing with full text</title>
		<author>
			<persName><forename type="first">Suyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1533" to="1541" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A century of science: Globalization of scientific collaborations, citations, and innovations</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1437" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An index to quantify an individual&apos;s scientific research output</title>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">E</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="16569" to="16572" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In WWW&apos;20. 2704-2710</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slice: Scalable linear extreme classifiers trained on 100 million labels for related searches</title>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhanu</forename><surname>Chunduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extreme multilabel loss functions for recommendation, tagging, ranking &amp; other missing label applications</title>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lightxml: Transformer with dynamic negative sampling for highperformance extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leilei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7987" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scientific prizes and the extraordinary growth of scientific topics</title>
		<author>
			<persName><forename type="first">Ching</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Uzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">5619</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bonsai: diverse and shallow trees for extreme multi-label classification</title>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Khandagale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Babbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="2099" to="2119" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MeSHLabeler: improving the accuracy of large-scale MeSH indexing by integrating diverse evidence</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="339" to="347" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The emerging trends of multi-label learning</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="7955" to="7974" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingnan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>KDD&apos;22. 3418-3428</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PubMed and beyond: a survey of web tools for searching biomedical literature</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="volume">2011</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">META: Metadata-Empowered Weak Supervision for Text Classification</title>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decaf: Deep extreme classification with label features</title>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheshansh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ECLARE: Extreme Classification with Label Graph Correlations</title>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheshansh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Agarwal</surname></persName>
		</author>
		<idno>WWW&apos;21. 3721-3732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Purushottam Kar, and Manik Varma</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DeepMeSH: deep semantic representation for improving large-scale MeSH indexing</title>
		<author>
			<persName><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Extreme multi-label learning with label features for warm-start tagging, ranking &amp; recommendation. In WSDM</title>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilpa</forename><surname>Gopinath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="441" to="449" />
		</imprint>
	</monogr>
	<note>Kunal Dahiya, Shrutendra Harsola, Rahul Agrawal, and Manik Varma</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising. In WWW</title>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrutendra</forename><surname>Harsola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
	<note>Rahul Agrawal, and Manik Varma</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;14</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multilevel compression of random walks on networks reveals hierarchical organization in large integrated systems</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">T</forename><surname>Bergstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2011">2011. 2011. 18209</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">GalaXC: Graph Neural Networks with Labelwise Attention for Extreme Classification. In WWW&apos;21</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Kumar Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3733" to="3744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Web-scale system for scientific knowledge exploration</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;18 System Demonstrations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An overview of microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo-June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
	<note>In WWW&apos;15 Companion</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Annexml: Approximate nearest neighbor search for extreme multi-label classification</title>
		<author>
			<persName><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="455" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
		<idno>KDD&apos;08. 990-998</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>NIPS&apos;17. 5998-6008</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Is preprint the future of science? A thirty year journey of online preprint services</title>
		<author>
			<persName><forename type="first">Boya</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09066</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Correlation networks for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishlay</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1074" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MeSHProbeNet: a self-attentive probe net for MeSH indexing</title>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishlay</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3794" to="3802" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>NeurIPS&apos;19</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3162" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pretrained generalized autoregressive model with adaptive probabilistic label clusters for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Da-Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10809" to="10819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Ppdsparse: A parallel primal-dual sparse method for extreme classification</title>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Ian Eh Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<idno>KDD&apos;17. 545-553</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Pradeep Ravikumar, Inderjit Dhillon, and Eric Xing</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3069" to="3077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Public use and public funding of science</title>
		<author>
			<persName><forename type="first">Yian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dashun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1344" to="1350" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">BERTMeSH: deep contextual representation learning for large-scale highperformance MeSH indexing with full text</title>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="684" to="692" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS&apos;</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="5820" to="5830" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">PECOS: Prediction for enormous and correlated output spaces</title>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Oag: Toward linking large-scale heterogeneous entity graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2585" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Fast multi-resolution transformer fine-tuning for extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS&apos;21. 7267-7280</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hierarchical Metadata-Aware Document Categorization under Weak Supervision</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Motifclass: Weakly supervised text classification with higher-order metadata information</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1357" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">MATCH: Metadata-Aware Text Classification in A Large Hierarchy</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3246" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boya</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. In WWW&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3162" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
