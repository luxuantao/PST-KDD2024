<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Machine Translation with Phrase-Level Universal Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingkai</forename><surname>Fang</surname></persName>
							<email>fangqingkai21b@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Intelligent Information Processing Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">ICT/CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
							<email>fengyang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Intelligent Information Processing Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">ICT/CAS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Machine Translation with Phrase-Level Universal Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal machine translation (MMT) aims to improve neural machine translation (NMT) with additional visual information, but most existing MMT methods require paired input of source sentence and image, which makes them suffer from shortage of sentence-image pairs. In this paper, we propose a phrase-level retrieval-based method for MMT to get visual information for the source input from existing sentence-image data sets so that MMT can break the limitation of paired sentence-image input. Our method performs retrieval at the phrase level and hence learns visual information from pairs of source phrase and grounded region, which can mitigate data sparsity. Furthermore, our method employs the conditional variational auto-encoder to learn visual representations which can filter redundant visual information and only retain visual information related to the phrase. Experiments show that the proposed method significantly outperforms strong baselines on multiple MMT datasets, especially when the textual context is limited.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal machine translation (MMT) introduces visual information into neural machine translation (NMT), which assumes that additional visual modality could improve NMT by grounding the language into a visual space <ref type="bibr" target="#b29">(Lee et al., 2018)</ref>. However, most existing MMT methods require additional input of images to provide visual representations, which should match with the source sentence. Unfortunately, in practice it is difficult to get this kind of pairwise input of text and images which hinders the applications of MMT. What is worse, to train an MMT model, the training data still involves the target sentence besides the source sentence and the image, which is costly to collect. As a result, the MMT model is usually trained on a small Multi30K <ref type="bibr" target="#b15">(Elliott et al., 2016)</ref> data set, which limits the performance of MMT. Therefore, it is necessary to utilize the separate image data set to obtain visual representations to break the constraints of pairwise input.</p><p>Towards this end, some researchers <ref type="bibr" target="#b52">(Zhang et al., 2020;</ref><ref type="bibr" target="#b45">Wu et al., 2021)</ref> propose to integrate a retrieval module into NMT, which retrieve images related to the source sentence from existing sentenceimage pairs as complementary input, and then use a pre-trained convolutional neural network (CNN) to encode the images. However, such sentence-level retrieval usually suffers from sparsity as it is difficult to get the images that properly match with the source sentence. Besides, visual features outputted by the CNN contain richer information (e.g., color, size, shape, texture, and background) than the source text, thus encoding them in a bundle without any filtering will introduce noise into the model.</p><p>To solve these problems, we propose a novel retrieval-based method for MMT to learn phraselevel visual representations for the source sentence, which can mitigate the aforementioned problems of sparse retrieval and redundant visual representations. For the sparsity problem, our method retrieves the image at the phrase level and only refers to the grounded region in the image related with the phrase. For the redundancy problem, our method employs the conditional variational auto-encoder to force the learned representations to properly reconstruct the source phrase so that the learned representations only retain the information related to the source phrase . Experiments on Multi30K <ref type="bibr" target="#b15">(Elliott et al., 2016)</ref> show that the proposed method gains significant improvements over strong baselines. When the textual context is limited, it achieves up to 85% gain over the text-only baseline on the BLEU score. Further analysis demonstrates that the proposed method can obtain visual information that is more related to translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Phrase-Guided Visual Representation</head><p>We use phrase-level visual representation to improve NMT. In this section, we will introduce our proposed phrase-guided visual representation. We first build a phrase-level image set, and then introduce a latent-variable model to learn a phraseguided visual representation for each image region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Phrase-Level Image Set</head><p>Our phrase-level image set is built from the training set of Multi30K, which contains about 29K bilingual sentence-image pairs. We only use the images e and source descriptions x from them, which is denoted as D = {(x i , e i )} N i=1 . We extract &lt;noun phrase, image region&gt; pairs from &lt;sentence, im-age&gt; pairs in D to build our phrase-level image set, which is denoted as D p .</p><p>For each sentence x i , we use an open-source library spaCy 1 to identify the noun phrases, which is denoted as</p><formula xml:id="formula_0">P i = (p i 1 , p i 2 , ..., p i t i )</formula><p>, where t i is the number of noun phrases in x i . For each noun phrase p i j , we detect the corresponding region r i j from the paired image e i using the visual grounding toolkit <ref type="bibr" target="#b48">(Yang et al., 2019)</ref>. Then (p i j , r i j ) is added to our phrase-level image set D p . Figure <ref type="figure" target="#fig_0">1</ref> illustrates an example.</p><p>Finally, we obtain the phrase-level image set</p><formula xml:id="formula_1">D p = {(p i , r i )} T i=1</formula><p>, where T = N i=1 t i . It contains about 102K pairs in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent-Variable Model</head><p>For an image region r, we can obtain the visual features v with a pre-trained ResNet-101 Faster R-CNN <ref type="bibr" target="#b21">(He et al., 2016;</ref><ref type="bibr" target="#b39">Ren et al., 2015)</ref>, which contains rich visual information (e.g., color, size, shape, texture, and background). However, we should not pay much attention to the visual information not mentioned in the corresponding phrase, which will introduce too much noise and even be harmful to NMT. Therefore, we further introduce a continuous latent variable to explicitly model the semantic information of image regions under the guidance of phrases. We adopt the framework of conditional variational auto-encoder (CVAE) <ref type="bibr" target="#b27">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b42">Sohn et al., 2015)</ref> to maximize the conditional marginal log-likelihood 1 https://spacy.io a black dog jumping to catch a rope toy </p><formula xml:id="formula_2">L cvae (ω, φ, θ) =E z∼q φ (z|p,v) [log p θ (p|z, v)] − KL[q φ (z|p, v) p ω (z|v)],</formula><p>(1) where p ω (z|v) is the prior, q φ (z|p, v) is an approximate posterior and p θ (p|z, v) is the decoder. The prior p ω is modeled as a Gaussian distribution:</p><formula xml:id="formula_3">p ω (z|v) = N (z; µ p (v), σ p (v) 2 I),</formula><p>(2) µ p (v) = Linear(v),</p><p>(3)</p><formula xml:id="formula_4">σ p (v) = Linear(v),<label>(4)</label></formula><p>where Linear(•) denotes linear transformation. The approximate posterior q φ is also modeled as a Gaussian distribution:</p><formula xml:id="formula_5">q φ (z|p, v) = N (z; µ q (p, v), σ q (p, v) 2 I),<label>(5)</label></formula><formula xml:id="formula_6">µ q (p, v) = Linear([RNN(p), v]), (6) σ q (p, v) = Linear([RNN(p), v]),<label>(7)</label></formula><p>where RNN(•) denotes a single-layer unidirectional recurrent neural network (RNN). The final hidden state of RNN is used to compute the mean and variance vectors.</p><p>To be able to update the parameters using backpropagation, we use the reparameterization trick (Kingma and Welling, 2014) to sample z from q φ :</p><formula xml:id="formula_7">z = µ q + σ q , ∼ N (0, I).<label>(8)</label></formula><p>The decoder p θ (p|z, v) is also implemented by a single-layer unidirectional RNN. The initial hidden state of decoder RNN is defined as:  and then the decoder will reconstruct the phrase p based on s. We refer to s as phrase-guided visual representation, since it pays more attention to the semantic information mentioned in the phrase and filters out irrelevant information. We will describe how to incorporate it into NMT in the next section.</p><formula xml:id="formula_8">s = Linear([z, v]),<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NMT with Phrase-Level Universal Visual Representation</head><p>In this section, we will introduce our retrievalbased MMT method. Specifically, we obtain visual context through our proposed phrase-level visual retrieval, and then learn a universal visual representation for each phrase in the source sentence, which is used to improve NMT. Figure <ref type="figure" target="#fig_1">2</ref> shows the overview of our proposed method, which is composed of four modules: source encoder, phraselevel visual retrieval module, multimodal aggregation module, and target decoder. The source encoder and target decoder are the same as the encoder and decoder of conventional text-only Trans-former <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>. Therefore, we will introduce the phrase-level visual retrieval module and multimodal aggregation module in detail in the rest of this section. We denote the input source sentence as x = (x 1 , x 2 , ..., x n ), the ground truth target sentence as y * = (y * 1 , y * 2 , ..., y * m ) and the generated translation as y = (y 1 , y 2 , ..., y m ). The input source sentence x will be encoded with the source encoder to obtain source sentence representation, which is denoted as H = (h 1 , h 2 , ..., h n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phrase-Level Visual Retrieval Module</head><p>To obtain the visual context of the source sentence without input paired images, we design a phraselevel visual retrieval module. Specifically, for the input sentence x = (x 1 , x 2 , ..., x n ), we identify the noun phrases P = (p 1 , p2 , ..., pt ) in x. Each phrase pi = (x l i , x l i +1 , ..., x l i +d i −1 ) is a continuous list of tokens, where l i is the index of the first token and d i is the length of pi . For each noun phrase pi , we will retrieve several relevant &lt;noun phrase, image region&gt; pairs from our phrase-level image set D p according to the semantic similarity between phrases, and then use the image regions as visual context. We design a phrase encoder to compute the phrase embedding, which is used to measure the semantic similarity between phrases.</p><p>Phrase Encoder Our phrase encoder Enc p (•) is based on a pre-trained BERT <ref type="bibr" target="#b14">(Devlin et al., 2019)</ref>. For a phrase p = (p 1 , p 2 , ..., p l ), we first use BERT to encode it into contextual embeddings:</p><formula xml:id="formula_9">c 1 , c 2 , ..., c l = BERT(p 1 , p 2 , ..., p l ),<label>(10)</label></formula><p>then the phrase embedding is the average embedding of all tokens:</p><formula xml:id="formula_10">Enc p (p) = 1 l l i=1 c i . (<label>11</label></formula><formula xml:id="formula_11">)</formula><p>Visual Retrieval For a given phrase p, we retrieve top-K relevant &lt;noun phrase, image region&gt; pairs from D p . For (p i , r i ) ∈ D p , the relevance score with given phrase p can be defined as the cosine distance between their phrase embeddings:</p><formula xml:id="formula_12">RS(p, (p i , r i )) = Enc p (p) • Enc p (p i ) Enc p (p) Enc p (p i ) ,</formula><p>(12) then we retrieve top-K relevant pairs for p:</p><formula xml:id="formula_13">{(p i k , r i k )} K k=1 = top-K i=1..T (RS(p, (p i , r i ))).<label>(13)</label></formula><p>Universal Visual Representation For every pair (p i k , r i k ), we can obtain the phrase-guided visual representation s i k through our latent-variable model as described in Section 2.2. Finally, the phrase-level universal visual representation of p is defined as the weighted sum of all {s i k }:</p><formula xml:id="formula_14">u = 1 K K k=1 RS(p, (p i k , r i k )) • s i k . (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>Our universal visual representation considers multiview visual information from several image regions, which avoids the bias caused by a single image region. Finally, for all phrases P = (p 1 , p2 , ..., pt ) in x, we obtain the corresponding universal visual representation U = (u 1 , u 2 , ..., u t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Aggregation Module</head><p>Inspired by the recent success of modality fusion in multimodal machine translation <ref type="bibr" target="#b50">(Yin et al., 2020;</ref><ref type="bibr" target="#b52">Zhang et al., 2020;</ref><ref type="bibr" target="#b17">Fang et al., 2022)</ref>, we design a simple multimodal aggregation module to fuse the source sentence representation H and phrase-level universal visual representation U. At first, we perform a phrase-level aggregation. For each phrase pi = (x l i , x l i +1 , ..., x l i +d i −1 ), we fuse the universal visual representation u i and the textual representation of corresponding tokens (h l i , h l i +1 , ..., h l i +d i −1 ):</p><formula xml:id="formula_16">m i = LayerNorm(u i + l i +d i −1 j=l i o ij h j ), (<label>15</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">o ij = sigmoid(W 1 u i + W 2 h j ),<label>(16)</label></formula><p>where denotes element-wise product. Now we obtain the multimodal phrase representation M = (m 1 , m 2 , ..., m t ). Afterwards, we apply a multi-head attention mechanism to append M to the source sentence representation:</p><formula xml:id="formula_19">S = MultiHead(H, M, M).<label>(17)</label></formula><p>We then fuse S and H with a gate mechanism:</p><formula xml:id="formula_20">S = H + λ S, (18) λ = sigmoid(W 3 H + W 4 S).<label>(19)</label></formula><p>Finally, S is fed into our target decoder for predicting the translation. The translation model is trained with a cross-entropy loss:</p><formula xml:id="formula_21">L trans = − m i=1 log p(y * i |y &lt;i , x). (<label>20</label></formula><formula xml:id="formula_22">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on the following datasets:</p><p>Multi30K Multi30K dataset contains bilingual parallel sentence pairs with image annotations, where each image is paired with one English description and the translations in German and French. Training, validation and test sets contain 29,000, 1,014, and 1,000 instances, respectively.</p><p>We also report the results on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively.</p><p>WMT16 EN-DE WMT16 EN-DE dataset contains about 4.5M sentence pairs. We choose new-stest2013 for validation and newstest2014 for test.  <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> baseline is statistically significant (p &lt; 0.05 and p &lt; 0.01, respectively).</p><p>WMT16 EN-RO WMT16 EN-RO dataset contains about 0.6M sentence pairs. We choose news-dev2016 for validation and newstest2016 for test. For all the above datasets, all sentences are tokenized and segmented into subwords units using byte-pair encoding (BPE) <ref type="bibr" target="#b41">(Sennrich et al., 2016)</ref>. The vocabulary is shared for source and target languages, with a size of 10K for Multi30K, and 40K for WMT16 EN-DE and WMT16 EN-RO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System Settings</head><p>Model Implementation For the latent-variable model, the image region is encoded with a pretrained ResNet101 Faster-RCNN <ref type="bibr" target="#b21">(He et al., 2016;</ref><ref type="bibr" target="#b39">Ren et al., 2015)</ref>. Both the phrase encoder and decoder are implemented using a single-layer unidirectional RNN with 512 hidden states. The size of the latent variable is set to 64. The batch size is 1024, and the learning rate is 5e-5. We train the model up to 200 epochs with Adam optimizer <ref type="bibr" target="#b26">(Kingma and Ba, 2015)</ref>. We adopt KL cost annealing and word dropout tricks to alleviate the posterior collapse problem following <ref type="bibr" target="#b1">Bowman et al. (2016)</ref>. The annealing step is set to 20000 and the word dropout is set to 0.1. Note that the phrases are segmented using the same BPE vocabulary as that for each source language.</p><p>For the translation model, we use Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> as our baseline. Both encoder and decoder contain 6 layers. The number of attention heads is set to 4. The dropout is set to 0.3, and the value of label smoothing is set to 0.1. For the visual retrieval module, we retrieve top-5 image regions for each phrase. We use Adam optimizer <ref type="bibr" target="#b26">(Kingma and Ba, 2015)</ref> to tune the parameters. The learning rate is varied under a warm-up strategy with 2,000 steps. We train the model up to 8,000, 20,000, and 250,000 steps for Multi30K, WMT16 EN-RO, and WMT16 EN-DE, respectively. We average the checkpoints of last 5 epochs for evaluation. We use beam search with a beam size of 4. Different from previous work, we use sacreBLEU<ref type="foot" target="#foot_0">2</ref>  <ref type="bibr" target="#b38">(Post, 2018)</ref> to compute the BLEU <ref type="bibr" target="#b37">(Papineni et al., 2002)</ref> scores and the statistical significance of translation results with paired bootstrap resampling <ref type="bibr" target="#b28">(Koehn, 2004)</ref>  All models are trained and evaluated using 2 RTX3090 GPUs. We implement the translation model based on fairseq<ref type="foot" target="#foot_2">4</ref>  <ref type="bibr" target="#b36">(Ott et al., 2019)</ref>. We train latent-variable model and translation model individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Systems</head><p>Our baseline is the text-only Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref>. Besides, we implement Imagination <ref type="bibr" target="#b16">(Elliott and Kádár, 2017)</ref> and UVR-NMT <ref type="bibr" target="#b52">(Zhang et al., 2020)</ref> based on Transformer, and compare our method with them. The details of these methods can be found in Section 6. We use the same configuration for all baseline systems as our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on Multi30K Dataset</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the results on Multi30K. Our proposed method significantly outperforms the Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> baseline, demonstrating that our proposed phrase-level universal visual representation can be helpful to NMT. Our method also surpass Imagination <ref type="bibr" target="#b16">(Elliott and Kádár, 2017)</ref> and UVR-NMT <ref type="bibr" target="#b52">(Zhang et al., 2020)</ref>. We consider it is mainly due to the following reasons. First, our phrase-level visual retrieval can obtain strongly correlated image regions instead of weakly correlated whole images. Second, our phrase-level universal visual representation considers visual information from multiple image regions and pays more attention to the semantic information mentioned in the phrases. Last, our phrase-level aggregation module makes it easier for the translation model to exploit the visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Latent-Variable Model</head><p>In Section 2.2, we introduce a latent-variable model to learn a phrase-guided visual representation for each image region. To understand how it improves the model performance compared with original visual features, we visualize the representations by reducing the dimension with Principal Component Analysis (PCA). Specifically, for all &lt;noun phrase, image region&gt; pairs in D p , we cluster the image regions by the head 5 of noun phrases. We select top-8 clusters according to their size, and randomly sample 1000 image regions for each cluster. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, the original visual features of different clusters are mixed together, indicating that they contains too much irrelevant information. In contrast, our proposed phrase-guided visual representations, which pay more attention to the semantic information, form several clusters according to their heads.</p><p>Combined with our visual retrieval module, we found that as the number of retrieved image regions K increases, the BLEU score keeps decreasing when we use original visual features, while increasing when we use our proposed phrase-guided visual representations, which is shown in Figure <ref type="figure">4</ref>. We believe the decrease of BLEU score is due to the 5 https://en.wikipedia.org/wiki/Head_ (linguistics) irrelevant information in original visual features, and thus directly sum them together will introduce too much noise. Our method filters out those irrelevant information, and multiple image regions could avoid the bias caused by a single one, which leads to the increase of BLEU score. However, we don't observe further improvements when using more image regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Source-Degradation Setting</head><p>We further conduct experiments under sourcedegradation setting, to verify the effectiveness of our method when the source textual context is limited. Following <ref type="bibr" target="#b45">Wu et al. (2021)</ref>, we mask the visually grounded tokens in the source sentence, which affects around 43% of tokens in Multi30K.</p><p>As shown in Table <ref type="table" target="#tab_2">2</ref>, our method achieves almost 85% improvements over the text-only Transformer baseline. It means our proposed phrase-level universal visual representation can fill in the missing information effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Phrase-Level vs. Sentence-Level Retrieval</head><p>To prove the effectiveness of phrase-level retrieval, we implement a sentence-level variant of our method. In this variant, we switch the latentvariable model, retrieval module and aggregation module from phrase-level to sentence-level. In this way, we retrieve several images as visual con-  text to help the translation. As shown in Table <ref type="table" target="#tab_3">3</ref>, the sentence-level variant Ours-sentence performs worse than Ours, especially in the case of source-degradation setting. We believe it is because phrase-level retrieval can obtain more relevant image regions as visual context, which contain less noise and can be integrated into textual representations more precisely. In contrast, sentence-level retrieval leads to images with much irrelevant information, and makes it difficult for the model to capture the fine-grained semantic correspondences between images and descriptions. To understand this difference more intuitively, we give an example in Figure <ref type="figure">5</ref>. As we can see, for the input sentence, phrase-level retrieval can obtain closely related image regions for noun phrases a person and a black car, while the results of sentence-level retrieval are actually weakly related with the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results on WMT News Datasets</head><p>Finally, we conduct experiments on WMT16 EN-DE and WMT16 EN-RO datasets. As shown in Table 4, we observe that both <ref type="bibr" target="#b52">Zhang et al. (2020)</ref> and our method only achieve marginal improvements compared with text-only Transformer baseline. We consider that there are two main reasons. On the one hand, most of tokens in such news text are not naturally related to specific visual contents. We found that the percentage of visual grounded tokens in the training set of WMT16 EN-DE is only 7% (vs. 43% in Multi30K), so the contribution of visual information is indeed limited. On the other hand, the news text is far from the descriptive text in Multi30K. In this way, the retrieved image regions are actually weakly correlated with the source phrase. We did some analysis to verify our hypotheses. As described in Section 3.1, we retrieve top-K pairs for each phrase according to the relevance scores. We define the average relevance scores (ARS) as follows:</p><formula xml:id="formula_23">ARS(k) = E p∈D val RS(p, (p i k , r i k )),<label>(21)</label></formula><p>which means the average relevance scores for all phrases in the validation set. As shown in Figure <ref type="figure">6</ref>, ARS on WMT news datasets are much lower than that on Multi30K, which proves that the gap between news text and descriptive text does exists. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Multimodal machine translation (MMT) aims to enhance NMT <ref type="bibr" target="#b43">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b51">Zhang et al., 2019;</ref><ref type="bibr" target="#b30">Li et al., 2021)</ref> with additional visual context. Since the release of Multi30K <ref type="bibr" target="#b15">(Elliott et al., 2016)</ref> dataset, researchers have proposed many MMT methods. Early methods <ref type="bibr" target="#b23">(Huang et al., 2016;</ref><ref type="bibr">Calixto and Liu, 2017;</ref><ref type="bibr" target="#b4">Caglayan et al., 2016;</ref><ref type="bibr" target="#b8">Calixto et al., 2016;</ref><ref type="bibr" target="#b2">Caglayan et al., 2017;</ref><ref type="bibr" target="#b31">Libovický and Helcl, 2017;</ref><ref type="bibr">Delbrouck and Dupont, 2017b,a;</ref><ref type="bibr" target="#b54">Zhou et al., 2018;</ref><ref type="bibr">Calixto et al., 2017;</ref><ref type="bibr">Helcl et al., 2018;</ref><ref type="bibr" target="#b3">Caglayan et al., 2018)</ref>   <ref type="bibr" target="#b46">(Xia et al., 2017)</ref> or capsule networks <ref type="bibr" target="#b40">(Sabour et al., 2017)</ref> to better utilize visual information during decoding. <ref type="bibr" target="#b6">Caglayan et al. (2021)</ref> propose a cross-lingual visual pre-training method and fine-tuned for MMT. It is worth noting that some of previous works <ref type="bibr" target="#b25">(Ive et al., 2019;</ref><ref type="bibr" target="#b32">Lin et al., 2020;</ref><ref type="bibr" target="#b50">Yin et al., 2020;</ref><ref type="bibr" target="#b44">Wang and Xiong, 2021;</ref><ref type="bibr" target="#b35">Nishihara et al., 2020;</ref><ref type="bibr" target="#b53">Zhao et al., 2021)</ref> adopt regional visual information like us, which shows effectiveness compared with global visual features. The major difference between our method and theirs is that our method is a retrieval-based method, which breaks the reliance on bilingual sentence-image pairs, Therefore, our method is still applicable when the input is text only (without paired images), which is unfortunately not available with those previous methods.</p><p>In addition to focusing on model design, Yang et al. ( <ref type="formula">2020</ref> All of the above methods require a specific image as input to provide visual context, which heavily restricts their applicability. To break this bottleneck, <ref type="bibr" target="#b22">Hitschler et al. (2016)</ref> propose target-side image retrieval to help the translation. <ref type="bibr" target="#b16">Elliott and Kádár (2017)</ref> propose a multitask learning framework Imagination to decomposes the multimodal translation into learning translation and learning visually grounded representation. <ref type="bibr" target="#b11">Calixto et al. (2019)</ref> introduce a latent variable and estimate a joint distribution over translations and images. <ref type="bibr" target="#b34">Long et al. (2020)</ref> predict the translation with visual representation generated by a generative adversarial network (GAN) <ref type="bibr" target="#b19">(Goodfellow et al., 2014)</ref>. The most closely related work to our method is UVR-NMT <ref type="bibr" target="#b52">(Zhang et al., 2020)</ref>, which breaks the reliance on bilingual sentence-image pairs. Like some retrieval-enhanced MT <ref type="bibr" target="#b18">(Feng et al., 2017;</ref><ref type="bibr" target="#b20">Gu et al., 2017)</ref> methods, they build a topic-image lookup table from Multi30K, and then retrieve images related to the source sentence as visual context based on the topic words. The central differences between <ref type="bibr" target="#b52">Zhang et al. (2020)</ref> and our method are as follows:</p><p>• First, their method depends on the weak correlation between words and images, which leads to much noise in the retrieved images, while our approach relies on the strong correlation between noun phrases and image regions.</p><p>• Second, our phrase-level retrieval can obtain more related visual context than their sentence-level retrieval (Section 5.4).</p><p>• Last, their method directly uses visual features extracted by ResNet <ref type="bibr" target="#b21">(He et al., 2016)</ref>, which may introduce too much noise. We adopt a latent-variable model to filter out irrelevant information and obtain a better representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a retrieval-based MMT method, which learns a phrase-level universal visual representation to improve NMT. Our method not only outperforms the baseline systems and most existing MMT systems, but also breaks the restrictions on input that hinder the development of MMT in recent years. Experiments and analysis demonstrate the effectiveness of our proposed method.</p><p>In the future, we will explore how to apply our method to other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of extracting &lt;noun phrase, image region&gt; pairs from existing &lt;sentence, image&gt; pairs.</figDesc><graphic url="image-1.png" coords="2,319.36,71.05,191.15,126.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>for future standard comparison across papers. Specifically, we measure case-insensitive detokenized BLEU for Multi30K (sacreBLEU signature: nrefs:1 | bs:1000 | seed:12345 | case:lc | eff:no | tok:13a | smooth:exp | version:2.0.0) 3 and case-sensitive detokenized BLEU for WMT datasets (sacreBLEU signature: nrefs:1 | bs:1000 | seed:12345 | case:mixed | eff:no | tok:13a | smooth:exp | version:2.0.0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of different visual representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>9152 Figure 5 :</head><label>91525</label><figDesc>Figure 5: Example of different levels of retrieval. We denote the index of retrieved image (regions) in the training set of Multi30K with #id.</figDesc><graphic url="image-16.png" coords="7,73.07,194.59,223.13,69.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>);<ref type="bibr" target="#b35">Nishihara et al. (2020)</ref>;<ref type="bibr" target="#b44">Wang and Xiong (2021)</ref> propose auxiliary loss to allow the model to make better use of visual information. Caglayan et al. (2019); Wu et al. (2021) conduct systematic analysis to probe the contribution of visual modality. Caglayan et al. (2020); Ive et al. (2021) focus on improving simultaneous machine translation with visual context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>𝐔 Source Encoder Target Decoder Multimodal Aggregation Module Phrase-Level Visual Retrieval Module</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Probabilities</cell></row><row><cell></cell><cell></cell><cell>𝐒</cell><cell></cell><cell></cell><cell>Softmax</cell><cell>ℒ %&amp;#'(</cell></row><row><cell></cell><cell></cell><cell>Gate &amp; Add</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>% 𝐒</cell><cell></cell><cell></cell><cell>Linear</cell></row><row><cell></cell><cell></cell><cell>Multi-head</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Attention</cell><cell></cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell></cell><cell>𝐌</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Phrase-level Aggregation</cell><cell></cell><cell>Feed Forward</cell></row><row><cell></cell><cell>𝐇</cell><cell></cell><cell></cell><cell>1×</cell></row><row><cell></cell><cell>Add &amp; Norm</cell><cell cols="2">CVAE &amp; Sum</cell><cell>ℒ !"#$</cell><cell>Multi-head Add &amp; Norm</cell></row><row><cell></cell><cell>Feed</cell><cell></cell><cell></cell><cell></cell><cell>Attention</cell></row><row><cell></cell><cell>Forward</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell>Add &amp; Norm</cell><cell>⋯</cell><cell>⋯</cell><cell></cell><cell>Masked</cell></row><row><cell>N×</cell><cell>Multi-head Attention</cell><cell>Visual Retrieval</cell><cell></cell><cell></cell><cell>Multi-head Attention</cell><cell>N×</cell></row><row><cell>Positional</cell><cell></cell><cell cols="3">a black dog a rope toy</cell><cell>Positional</cell></row><row><cell>Encoding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoding</cell></row><row><cell></cell><cell>Input Embedding</cell><cell cols="2">Noun Phrase Extraction</cell><cell></cell><cell>Output Embedding</cell></row><row><cell></cell><cell cols="3">Source: a black dog jumping to catch a rope toy</cell><cell></cell><cell>Target</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>BLEU scores on Multi30K dataset. * and ** mean the improvements over Transformer</figDesc><table><row><cell>Models</cell><cell>Test2016</cell><cell>EN-DE Test2017</cell><cell>MSCOCO</cell><cell>Test2016</cell><cell>EN-FR Test2017</cell><cell>MSCOCO</cell></row><row><cell>Transformer (Vaswani et al., 2017)</cell><cell>39.87</cell><cell>31.78</cell><cell>29.36</cell><cell>60.51</cell><cell>52.44</cell><cell>42.49</cell></row><row><cell cols="2">Imagination (Elliott and Kádár, 2017) 39.70 -0.17</cell><cell>32.15 +0.37</cell><cell>29.76 +0.40</cell><cell>60.88 +0.37</cell><cell>52.89 +0.45</cell><cell>42.87 +0.38</cell></row><row><cell>UVR-NMT (Zhang et al., 2020)</cell><cell>38.19 -1.68</cell><cell>31.85 +0.07</cell><cell>28.55 -0.81</cell><cell>60.02 -0.49</cell><cell>51.50 -0.94</cell><cell>43.22 +0.73</cell></row><row><cell>Ours</cell><cell cols="3">40.30 +0.43 33.45 +1.67 ** 30.28 +0.92</cell><cell cols="3">61.31 +0.80 * 53.15 +0.71 * 43.65 +1.16 *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>BLEU scores on Multi30K En-De under source-degradation setting.</figDesc><table><row><cell></cell><cell>40.2 40.3</cell><cell></cell><cell>original phrase-guided</cell><cell></cell><cell>40.12</cell><cell>40.30</cell></row><row><cell></cell><cell>40.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLEU</cell><cell>39.9 40.0</cell><cell>39.90</cell><cell>39.82</cell><cell>39.86</cell><cell></cell></row><row><cell></cell><cell>39.7 39.8</cell><cell>39.72</cell><cell>39.75</cell><cell>39.79</cell><cell>39.74</cell><cell>39.65</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3 K</cell><cell>4</cell><cell>5</cell></row><row><cell cols="7">Figure 4: BLEU scores with different number of re-</cell></row><row><cell cols="7">trieved image regions K. Phrase-guided visual repre-</cell></row><row><cell cols="7">sentations achieve better performance as K increases.</cell></row><row><cell cols="2">Models</cell><cell></cell><cell cols="4">Test2016 Test2017 MSCOCO</cell></row><row><cell cols="4">Transformer 10.42</cell><cell>8.59</cell><cell>7.08</cell></row><row><cell cols="2">Ours</cell><cell></cell><cell cols="4">19.41 +8.99 13.67 +5.08 12.23 +5.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: BLEU scores on Multi30K En-De Test2016.</cell></row><row><cell>(Mask) indicates source-degradation setting. * and **</cell></row><row><cell>mean the improvements over Transformer (Vaswani</cell></row><row><cell>et al., 2017) baseline is statistically significant (p &lt;</cell></row><row><cell>0.05 and p &lt; 0.01, respectively).</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://github.com/mjpost/sacrebleu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">This is because the official pre-processing script of Multi30K dataset lowercases the corpus, see https://github.com/multi30k/dataset/ blob/master/scripts/task1-tokenize.sh</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/pytorch/fairseq</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank all the anonymous reviewers for their insightful and valuable comments. This work was supported by National Key R&amp;D Program of China (NO. 2017YFE0192900).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR 2015</idno>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIUM-CVC submissions for WMT17 multimodal translation task</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4746</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
				<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIUM-CVC submissions for WMT18 multimodal translation task</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6438</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
				<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="597" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multimodal attention for neural machine translation</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno>CoRR, abs/1609.03976</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simultaneous machine translation with visual context</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Ive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veneta</forename><surname>Haralampieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.184</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2350" to="2361" />
		</imprint>
	</monogr>
	<note>Loïc Barrault, and Lucia Specia</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-lingual visual pretraining for multimodal machine translation</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menekse</forename><surname>Kuyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Sercan Amac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1317" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probing the need for visual context in multimodal machine translation</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1422</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4159" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DCU-UvA multimodal MT system report</title>
		<author>
			<persName><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2359</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
				<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="634" to="638" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating global visual features into attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Doubly-attentive decoder for multi-modal neural machine translation</title>
		<author>
			<persName><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1175</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1913" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent variable model for multi-modal translation</title>
		<author>
			<persName><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1642</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6392" to="6405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical study on the effectiveness of images in multimodal neural machine translation</title>
		<author>
			<persName><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1095</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="910" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for multimodal neural machine translation</title>
		<author>
			<persName><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
		</author>
		<idno>CoRR, abs/1703.08084</idno>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi30K: Multilingual English-German image descriptions</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-3210</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language</title>
				<meeting>the 5th Workshop on Vision and Language<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagination improves multimodal translation</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ákos</forename><surname>Kádár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="130" to="141" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation</title>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memory-augmented neural machine translation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1146</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1390" to="1399" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Search engine guided nonparametric neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1705.07267</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CUNI system for the WMT18 multimodal translation task</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
				<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016. 2018</date>
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal pivots for image caption translation</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1227</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2399" to="2409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention-based multimodal neural machine translation</title>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<title level="s">Shared Task Papers</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting multimodal reinforcement learning for simultaneous machine translation</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Ive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">Mingren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3222" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distilling translations with visual awareness</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Ive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1653</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6525" to="6538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>ICLR 2014</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
				<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14">2014. April 14-16, 2014</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emergent translation in multi-agent communication</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mixup decoding for diverse machine translation</title>
		<author>
			<persName><forename type="first">Jicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengzhi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.29</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
				<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="312" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention strategies for multi-source sequence-to-sequence learning</title>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic context-guided capsule network for multimodal machine translation</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413715</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia, MM &apos;20</title>
				<meeting>the 28th ACM International Conference on Multimedia, MM &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gumbel-attention for multi-modal machine translation</title>
		<author>
			<persName><forename type="first">Pengbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.08862</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generative imagination elevates machine translation</title>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2009.09654</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supervised visual attention for multimodal neural machine translation</title>
		<author>
			<persName><forename type="first">Tetsuro</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Ninomiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaro</forename><surname>Omote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4304" to="4314" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
				<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Efficient objectlevel visual context modeling for multimodal machine translation: Masking irrelevant objects helps grounding</title>
		<author>
			<persName><forename type="first">Dexin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<idno>CoRR, abs/2101.05208</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.480</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6153" to="6166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visual agreement regularized training for multi-modal machine translation</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6484</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9418" to="9425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A fast and accurate one-stage approach to visual grounding</title>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimodal transformer for multimodal machine translation</title>
		<author>
			<persName><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.400</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4346" to="4350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A novel graph-based multi-modal fusion encoder for neural machine translation</title>
		<author>
			<persName><forename type="first">Yongjing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3025" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bridging the gap between training and inference for neural machine translation</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1426</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural machine translation with universal visual representation</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Neural machine translation with semantically relevant image regions</title>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoyuki</forename><surname>Kajiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>red</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A visual attention grounding neural model for multimodal machine translation</title>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1400</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3643" to="3653" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
