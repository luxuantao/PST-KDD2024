<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robotics and Autonomous Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-03-29">29 March 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">David</forename><surname>Meger</surname></persName>
							<email>dpmeger@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Per-</roleName><forename type="first">Erik</forename><surname>Forss√©n</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><surname>Helmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sancho</forename><surname>Mccann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tristram</forename><surname>Southey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Baumann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postBox>B.C. V6T 1Z4</postBox>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robotics and Autonomous Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-03-29">29 March 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">1C2C990FEF72ED1D8AD6F9FF54A5A3C5</idno>
					<idno type="DOI">10.1016/j.robot.2008.03.008</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Object recognition Visual attention Saliency Semantic robot vision Object permanence</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art methods have recently achieved impressive performance for recognising the objects present in large databases of pre-collected images. There has been much less focus on building embodied systems that recognise objects present in the real world. This paper describes an intelligent system that attempts to perform robust object recognition in a realistic scenario, where a mobile robot moving through an environment must use the images collected from its camera directly to recognise objects. To perform successful recognition in this scenario, we have chosen a combination of techniques including a peripheral-foveal vision system, an attention system combining bottom-up visual saliency with structure from stereo, and a localisation and mapping technique. The result is a highly capable object recognition system that can be easily trained to locate the objects of interest in an environment, and subsequently build a spatial-semantic map of the region. This capability has been demonstrated during the Semantic Robot Vision Challenge, and is further illustrated with a demonstration of semantic mapping. We also empirically verify that the attention system outperforms an undirected approach even with a significantly lower number of foveations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A driving motivation behind much of cognitive robotics research today is the notion of a personal robot companion capable of aiding people in their daily activities. Special cases of this are systems to care for the elderly, robotic home and office assistants, and interactive robot toys for children. For each of these applications, the human and robot involved must perceive and represent the world in a similar fashion, so that they can collaborate effectively. In particular, a robot with the ability to visually identify objects of interest will have much of the information necessary for successful operation. A human-like visual system would help a robot with both obstacle avoidance (e.g., noticing everyday objects it might bump into, and also spotting black-yellow warning sticker tape), and for more natural human-robot interaction (e.g., "Robot, fetch me my coffee mug!").</p><p>Many of the competences required for a completely visual home assistant are beyond the boundaries of current state-of-the-art research. In particular, recognising visual objects based on their semantic meaning, often referred to as object category recognition, has recently received extensive attention from computer vision researchers <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. The focus of much of this research has been on learning appearance from large databases of static images or on indexing images from the web based on their meaning. This scenario is significantly different from the one faced by a robot in an ever-changing home environment where recognition, navigation, planning (both for robot motion and the robot's view), and interaction must all occur simultaneously. One example of a robotic system capable of object recognition in realistic settings is <ref type="bibr" target="#b5">[6]</ref>, which is similar in spirit to our system. Robotics researchers have also recently considered producing semantic maps based on the locations of objects (for example <ref type="bibr" target="#b6">[7]</ref>), but there are still many remaining challenges related to learning visual representations of objects and integrating these semantic concepts with other robot behaviours. This paper presents an integrated solution to many of these challenges and describes a system that is capable of performing real-world object recognition in realistic scenarios.</p><p>Our efforts have been motivated and directed by the Semantic Robot Vision Challenge (SRVC) <ref type="bibr" target="#b7">[8]</ref>, held at the Association for the Advancement of Artificial Intelligence (AAAI) conference in 2007. This challenge is divided into three phases. During the training phase, robots are required to build visual representations of a previously unknown list of objects in a short time frame, using only images collected from the World Wide Web. In the exploration phase, the robots examine a contest environment, which is constructed in a semi-realistic fashion, and contains the objects listed, as well as other distracting objects. The final phase is recognition, where objects must be identified with semantic labels by matching images obtained in the first two phases. Performance is evaluated by comparing the robotic system's classification output with a human's labeling of the objects.</p><p>The physical system described in this paper finished first in the robot league of the 2007 SRVC. Many of the design choices and physical specifications have been made somewhat specific to that scenario, and should be changed for a more general-purpose application. Specifically, the SRVC separated the recognition problem into three phases, whereas running all components in parallel during the operational lifetime would be desirable for a robot companion. Also, the strict time requirement meant that mapping needed to occur as quickly as possible, and that highly accurate sensors were desirable. For this reason, the mapping procedure described in Section 4 uses a laser range finder. Visual mapping, such as the method of Sim et al. <ref type="bibr" target="#b8">[9]</ref>, would be preferable in terms of cost and safety. Since it would provide the ability to sense obstacles outside the plane of the range finder.</p><p>The focus of this paper is a description of the behaviour used during environment exploration phase of the SRVC. The goal during this phase was to collect numerous, high quality views of each of the objects. Due to the time constraints of the contest, these views had to be collected without performing object recognition, but instead by quickly identifying promising objects and regions, which we will refer to as potential objects. This presemantic identification of interesting regions was inspired by the model of human visual attention proposed by Rensink <ref type="bibr" target="#b9">[10]</ref>, where proto-objects are detected subconsciously in the visual periphery, and attention shifts between these to allow more detailed consideration. Our potential object detection method can be considered a simplified version of object discovery, such as the method described by Southey et al. <ref type="bibr" target="#b10">[11]</ref>, which attempts to faithfully segment meaningful objects using numerous cues. In comparison, we produce a less precise segmentation with less computation and rely on subsequent recognition to refine the result.</p><p>The remainder of this paper will provide a detailed description for each component of our method. Section 2 describes the hardware system. Section 3 describes the potential object selection method, which serves to direct the attention for our system. This is followed by a description of the navigation, mapping and coverage algorithm in Section 4 and then by a brief description of the visual object recognition approach in Section 5. Section 6 presents results obtained during the SRVC as well as during further testing conducted in our lab, which provide validation of our approach. Finally, future work and perspectives will be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hardware</head><p>Hardware design is an important consideration when constructing a robot that is targeted at operating in a man-made environment. Many extant robot platforms are limited by height, navigation ability and fixed direction sensor platforms so that interesting objects are inaccessible. For example, objects located on desks or bookshelves in an office are often too high to be seen by a robot's cameras. Our robot platform, "Curious George", was designed to have roughly similar dimensions and flexibility to a human, so that relevant regions of the environment could be easily viewed and categorised. Our robot is an ActiveMedia PowerBot, equipped with a SICK LMS 200 planar range finder. The robot's cameras are raised by a tower with height approximately 1.5 m. The cameras are mounted on a PTU-D46-17.5 pan-tilt unit from Directed Perception which provides an effective 360 ‚Ä¢ gaze range. See Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>We employ a peripheral-foveal vision system in order to obtain the high resolution required to recognise objects while simultaneously perceiving a large portion of the surrounding region. This choice has again been modelled after the human perceptual system, and was also inspired by design choices made in <ref type="bibr" target="#b11">[12]</ref>. For peripheral vision, the robot has a Bumblebee colour stereo camera from PointGrey Research, with 1024 √ó 768 resolution, and a 60 ‚Ä¢ field-of-view which provides a low resolution survey of the environment. For foveal vision, the robot has a Canon PowerShot G7 still image camera, with 10.0 megapixel resolution, and 6√ó optical zoom which allows for high resolution imaging of tightly focused regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Attention system</head><p>The attention system identifies potential objects using the peripheral vision system, and focuses on these objects to collect detailed images using the foveal system, so that these images can be further processed for object recognition. Identifying potential objects correctly is a non-trivial problem, due to the presence of confusing backgrounds and the vast appearance and size variations amongst the items that we refer to as objects. Our system makes use of multiple cues to solve this problem. Specifically, we obtain depth from stereo to determine structures that stand out from floor or background, and we process visual information directly with a saliency measure to detect regions with distinctive appearance. This section will describe the stereo and saliency approaches in detail, and will describe the subsequent collection of foveal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stereo</head><p>The Bumblebee stereo camera is bundled with software for computing depth from stereo. We use the output disparity maps to detect obstacles and objects of interest, by detecting regions with above-floor elevations, see Fig. <ref type="figure" target="#fig_1">2</ref>. This algorithm makes use of camera tilt (variable) and elevation (static) to transform the disparities to elevation values. The elevations are then thresholded at 10 cm, and the resultant binary map is cleaned up by a series of morphological operations. This helps to remove small disparity regions, which are likely to be erroneous, and also fills in small gaps in objects. The resultant obstacle map is used both to avoid bumping into objects and tables, and in combination with saliency to determine likely locations of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Saliency</head><p>To detect potential objects we make use of the spectral residual saliency measure defined in <ref type="bibr" target="#b12">[13]</ref>. We extend the measure to colour in a manner similar to <ref type="bibr" target="#b13">[14]</ref>. That is, we compute the spectral residual on three channels: intensity, red-green, and yellow-blue. The results are then combined by summing them to form a single saliency map. Regions of multiple sizes are then detected in the saliency map using the Maximally Stable Extremal Region (MSER) detector <ref type="bibr" target="#b14">[15]</ref>. This detector is useful since it does not enforce a partitioning of the scene. Instead, nested regions can be detected, if they are deemed to be stable. Typically, MSERs are regions that are either darker or brighter than their surroundings, but, since bright in the saliency map corresponds to high saliency, we know that only bright regions are relevant here, and consequently we only need to run half the MSER detector. Bright MSERs are shown in red and green in Fig. <ref type="figure" target="#fig_2">3</ref>. Regions are required to have their smallest saliency value above a threshold proportional to the average image intensity (which is justified since spectral saliency scales linearly with intensity changes). This gives us automatic adaptation to global illumination and contrast changes. The regions are further required to be more than 20% smaller than the next larger nested region, to remove regions that are nearly identical. To ensure that the salient regions are not part of the floor, they are also required intersect the obstacle map (see Section 3.1) by 20%. Regions which pass these restrictions are shown in green in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>Compared to <ref type="bibr" target="#b13">[14]</ref>, which can be considered state-of-the-art in saliency detection, the above described detector offers three advantages:</p><p>(i) The use of spectral saliency and the MSER detector makes the algorithm an order of magnitude faster. (0.1 instead of 3.0 s in our system). (ii) The use of the MSER detector allows us to capture both objects and parts of objects, whenever they constitute stable configurations. This fits well with bottom-up object detection, since objects typically consist of smaller objects (object parts), and we would not want to commit to a specific scale before we have analysed the images further. The multiple sizes also map naturally to different zoom settings on the still image camera. (iii) The use of an average intensity-related threshold allows us to adapt the number of salient regions reported, depending on the image structure. In particular, this thresholding technique will report that there are no salient regions when analysing a highly uniform image such as a wall or floor. This is in contrast to the Walther toolbox <ref type="bibr" target="#b13">[14]</ref>, which, due to its built-in normalisation, only can order salient regions, but never decide that there is nothing interesting in the scene.</p><p>Note that the potential objects are not necessarily what one would normally call objects. They are equally likely to be distracting background features such as intersecting lines on the floor, or box corners. The purpose of saliency is merely to restrict the total number of possible gazes to a smaller set that still contains the objects we want to find. This means that it is absolutely essential that the attended potential objects are further analysed in order to reject, or verify their status as objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gaze control</head><p>In order to actually centre a potential object in the still image camera, we employ the saccadic gaze control algorithm described in <ref type="bibr" target="#b15">[16]</ref>. This algorithm learns to centre a stereo correspondence in the stereo camera. To instead centre an object in the still image camera, we centre the stereo correspondence on the epipoles (the projections of camera's optical centre) of the still image camera in the stereo camera.</p><p>In order to select an appropriate zoom level, we have calibrated the scale change between the stereo camera and the still image camera for a fixed number of zoom settings. This allows us to simulate the effect of the zoom, by applying the scale change to a detected MSER. The tightest zoom at which the MSER fits entirely inside the image is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spatial representation</head><p>An embodied recognition system must do more than simply recognising semantically meaningful objects which are directly in its field of view at a single moment in time. It must additionally move safely through its environment, record the locations of detected objects, and plan its motions to discover new objects. That is, it must be able to represent spatial-semantic information. Our system accomplishes this by: (1) building a geometric map representation of the space it has so far encountered; (2) using this map to guide further planning and exploration; (3) covering the space with the visual attention system to search for objects; (4) annotating objects in the map when they are first discovered; and (5) updating the object locations and properties over time by looking back from different viewpoints. This section will describe each of these components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Geometric mapping</head><p>Our system performs mapping with FastSLAM, a Rao-Blackwellized Particle Filter implementation <ref type="bibr" target="#b16">[17]</ref>, which builds a probabilistic occupancy grid <ref type="bibr" target="#b17">[18]</ref> based on the laser range finder readings and the robot's odometry, and tracks the robot's position within the map. An occupancy grid is well suited to guide navigation and planning tasks for a mobile robot moving on a flat surface since it mirrors the inherently 2D nature of this environment. We have implemented a layered planning architecture where goals proposed by one of the high level behaviours described below are achieved by following a lower level path produced by A * -search through the occupancy grid. Finally, the Vector-Field Histogram local planner described by Borenstein et al. <ref type="bibr" target="#b18">[19]</ref> is used for local obstacle avoidance and to adapt to dynamic changes in the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploration planning</head><p>We employ the frontier based exploration technique described by Yamauchi et al. <ref type="bibr" target="#b19">[20]</ref> to quickly cover the environment with the laser scanner and produce an initial map. As is illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>(a), a frontier is defined as the border between explored and unexplored space. For our system, these frontiers will be the locations just beyond the range of the laser scans, and in the laser shadows created behind objects or around corners. The frontier planning technique identifies candidate locations where laser scans would be most likely to uncover new regions to explore. First, one of these promising locations is chosen, then the robot moves to this location, and the map is updated. This process is iterated, until all regions have been explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Coverage planning</head><p>Each time a region of the environment is observed with the peripheral camera, the attention system has the opportunity to detect potential objects within that area. In order to maximise these opportunities, the camera should be pointed in directions that cover as much new territory as possible. We use an iterated greedy search based on visible area weighted by the number of previous observations to select favourable directions. This approach causes the camera to cover the environment roughly uniformly and give an equal chance of detecting potential objects in any location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Object permanence</head><p>The set of available object poses in visual training data collected from the Web is often incomplete. One tends to get the characteristic views <ref type="bibr" target="#b20">[21]</ref> (e.g., a shoe is normally photographed from the side, and hardly ever from the front), rather than a uniform sampling of views. In order to perform successful recognition using such limited training data, we attempt to collect numerous views of each potential object by looking back to the same locations as the robot moves. This requires awareness of an object's location even when it is not in the visual field, an ability known as object permanence. The behaviour of looking back from many views increases the likelihood that one of the collected images is taken from a similar view to that of the training data. To allow collection of highly distinctive viewpoints, the previous views of an object vote for nearby angles into a histogram with values in the range [0, 2œÄ], and histogram bins with low scores are selected. That is, views from a completely new direction are favoured over those from similar angles. We again employ greedy search over robot poses where objects were first seen, object "basketball", ‚ô¶ object "recycling bin", ‚àá object "robosapien".</p><p>histogram values and iterate the procedure to obtain roughly uniform coverage of viewing angles. Once a direction is selected, the hierarchical planning method moves the robot to the desired viewing position and a foveal image is collected. Fig. <ref type="figure" target="#fig_3">4(b)</ref> shows an example of a path produced during this behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Object recognition</head><p>While the focus of this article is robot exploration and image collection, our system also includes a method for subsequently recognising objects in the images. This section will outline our approach for training object classifiers and for evaluating these on the images collected by our robot. Our system collects its training data by submitting text-based queries to internet image search engines and storing the collections of images returned. The results of a typical query include numerous images containing the desired object, as well as some mislabelled images, cartoon representations of the object, and extensive clutter. It is a significant challenge to construct an object appearance model from such unstructured data, particularly when coupled with the time constraints of the competition. We address this challenge by heuristic re-ranking of the images to focus time and attention on more useful training images and also by employing a simple and robust local feature matching approach for recognition. Each of these components will be discussed in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image re-ranking</head><p>We re-rank the input training imagery using a number of intuitively useful cues. First, we analyse the information within single images, using colour histogram analysis to demote images with few colours, which are likely to be an artist's renderings and colour image segmentation to promote uncluttered images with homogeneous backgrounds. Next, we search for consistency across multiple images by computing pairwise similarity between each training image and searching for large cliques in the induced graph. Since the images in such cliques strongly agree upon a common visual appearance for the category, they are unlikely to be noise, and thus are ranked highly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Local feature matching</head><p>Even after re-ranking, numerous distracting and cluttered images remain. Recognition approaches that seek to build a generic category appearance model such as Zhang et al. <ref type="bibr" target="#b4">[5]</ref> were found to be ineffective in the face of such data. Instead, direct image matching based on local feature patches, as described by Lowe <ref type="bibr" target="#b21">[22]</ref>, proved more successful. In particular, our training phase consists of computing SIFT features for each training example from the Web. The classification procedure extracts the same type of features from the images collected by the robot and matches features between each training and test image. Feature matches between a pair of images are verified with a geometric consistency check. That is, each match suggests a 2D similarity transform between images and two matches that agree upon a transformation increase the likelihood that the object is present in the image. The number of these consistent feature matches provides a measure of confidence in the detection. If multiple images are labelled as containing a particular object, the system outputs the one with the highest confidence. The locations of matched features in the robot's images are used to determine the likely position and extent of the object and produce a bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Semantic mapping</head><p>The combination of techniques described in the previous sections endows a mobile agent with the ability to explore its environment and to recognise the objects it discovers. This behaviour can be easily extended to spatial-semantic mapping by back-projecting the recognised objects into the robot's map representation of the world. In our case, the probabilistic occupancy grid constructed from laser range scans fed through the FastSLAM algorithm can be augmented with the locations of visual objects. For example, Fig. <ref type="figure" target="#fig_4">5(b</ref>) and (c) illustrate the locations of objects matching the labels "robosapien", "basketball", and "recycling bin". The object recognition subsystem was provided with between 2 and 4 example views of each object, see Fig. <ref type="figure" target="#fig_4">5</ref>(a) for an example. Each object shown was identified by the attention system and observed from various locations, giving several pieces of information about its position, and allowing for collection of numerous views for recognition or future matching. We envision that the types of maps illustrated here could be easily used in a human-robot interaction system where the human operator would be able to relay commands to the robot in semantically meaningful terms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison of attention approaches</head><p>To validate the effectiveness of the saliency and structure based attention systems described in Section 3, we compared its performance against two other methods for selecting foveal views which will be described shortly. In order to ensure a fair comparison, the remaining components of our system were held constant. This suggests the following decomposition of our system into three parts:</p><p>(i) (Identical for each method) Robot motion to a location that allows coverage of the environment and collection of a peripheral image of a large region at low resolution. (ii) (Three different methods compared) Selection of a number of sub-regions and collection of foveal images. (iii) (Identical for each method) Classification of the collected foveal images by the object recognition system.</p><p>The three attention methods evaluated were the visual saliency and structure approach described in Section 3 and two comparative methods: (i) Peripheral view only. This method took only one image at each robot pose, simulating the lack of a foveal vision system. The image covered the entire peripheral region at a wide zoom setting. Recognition results from this approach should be viewed as a baseline for any more selective attention system. (ii) Random view selection. This method sampled from sub-regions of the peripheral view by randomly selecting n pan-tilt and zoom values from the view-cone visible in the peripheral camera, where n is a tuneable parameter. The space of possible images collected by this method is the same space in which the guided attention system searches. As such, there is some likelihood that these samples are identical to the interesting views obtained by the guided attention system, or are even better views. So, the recognition results from this approach can be used to evaluate whether or not our guided system is better than chance at selecting interesting views.</p><p>To additionally enforce fairness of comparison, we ran each of the three attention methods from identical robot locations. That is, once the robot had moved to a point, each one of the three attention methods was executed in sequence. The pan-tilt unit and zoom settings were reset to their defaults between each method, and the robot base was kept stationary during the process.</p><p>Fig. <ref type="figure" target="#fig_5">6</ref>(a) displays the evolution of recognition performance over time for each of the 3 approaches. In all trials attempted, the approach based on visual saliency was able to recognise more objects by the end of the trial, even when the view sampling approach took many more images. In this particular plot, the random approach uses 8 images at each location, whereas the attention method uses 3 on average. We additionally varied the parameter n for the random view selection strategy, to observe its effect on recognition performance. Fig. <ref type="figure" target="#fig_5">6</ref>(b) demonstrates that for all of 2, 4, and 8 views per pose, the collection of more data generally increases performance. However, even with twice as many view samples, the random approach does not perform as well as our guided attention measure. This is strong evidence that the visual saliency method is indeed guiding the robot to obtain promising views of objects, and that it is performing well in realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">SRVC contest performance</head><p>As mentioned earlier, the 2007 SRVC contest was composed of three phases: Web search, exploration, and classification. The abilities of the intelligent system described in this paper were demonstrated in the SRVC, where our system was the winning entry in the robot league. Fig. <ref type="figure" target="#fig_6">7</ref> demonstrates several of the objects correctly classified by our system during the final round of the contest, along with several of the misclassifications. As can be seen by the images, the contest environment was not completely realistic, but it was sufficiently complicated to present a significant challenge for current state-of-the-art recognition systems. It was impossible to view all candidate objects from any single location, so robot motion and collection of multiple views of each object was essential. Also, many of the objects were placed in highly cluttered locations such as table tops, which would cause confusion for saliency methods that do not take into account that parts of objects may also themselves be objects. The navigation and attention systems described in Sections 3 and 4 were sufficiently successful at exploring and determining the locations of interesting objects to deal with these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have described an intelligent system capable of building a detailed semantic representation of its environment. Through careful integration of components, this system demonstrates reasonably successful and accurate object recognition in a quasirealistic scenario. Significant work is still needed to produce a system which will operate successfully in more general environments such as homes, offices, and nursing homes, where personal companion robots are intended to operate. In such environments, challenges include the level of clutter, number of distinct objects, non-planar navigation, dynamic environments, and need to operate in real time, among many others. While the current implementation of our system is not sufficiently sophisticated to be successful in these environments, we believe there are several additional components which would bring this closer to reality.</p><p>The current object permanence ability has proven useful to establish the identity of objects which cannot be recognised from a particular view, but it could be extended to further leverage the available information. For example, the results of object recognition on the current set of collected views should impact the number of additional views and the locations from which these are taken.</p><p>Once the system is nearly certain of the identity of a particular object, it would be better served by moving on to objects that it is less confident about. In the future we hope to implement an integrated decision making system which would be able to capture this intuition, perhaps by extending an approach similar to <ref type="bibr" target="#b22">[23]</ref>.</p><p>Online and life-long learning will both be essential for a truly useful home companion which is able to enter a person's dwelling and continue to succeed there for a long period. It is, in many cases, an easier visual task to recognise objects when trained in the particular circumstances and on the particular objects which will be required during operation. Active training data acquisition as facilitated by object permanence is needed to extend the crude models obtained from the web, and to adapt to changing object appearances, (e.g., due to wear and tear). We believe many components in our method are well suited to such a scenario.</p><p>Context is a currently untapped source of information which can be used to aid the spatial-semantic recognition task. Contextual information such as the type of room being examined would help to prioritise recognition effort towards those objects likely to be present. Spatial context allows for preferential search based on the height and position at which an object is normally found. Some interesting attempts to incorporate context using the gist descriptor <ref type="bibr" target="#b23">[24]</ref> are given in <ref type="bibr" target="#b22">[23]</ref>.</p><p>We believe that the prospect of a useful mobile robot companion is a realistic medium term goal and that many of the components discussed in this paper will be essential to the realization of such a system. It will continue to be important to evaluate approaches that extract semantic meaning from visual scenes in realistic scenarios, and also to integrate such systems with active, mobile systems, in order to achieve robustness and generality. The system described here is one step along this path.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The "Curious George" robot platform.</figDesc><graphic coords="2,319.23,63.93,216.00,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Stereo computation. Top to bottom: Left and right input images, disparity map, and obstacle map superimposed on right input image.</figDesc><graphic coords="3,48.17,63.92,239.76,302.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Saliency computation. Top to bottom: Input image, colour opponency channels (int, R-G, Y-B), spectral saliency map, detected MSERs, and MSERs superimposed on input image. Figure best viewed in colour. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="3,316.98,63.92,240.12,416.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Paths are planned to achieve numerous goals. (a) Path towards frontier of unexplored space (indicated by blue dots) allow for exploration. (b) A path to another clear view of an object (indicated by a yellow dot) can be used to obtain multiple views. Legend: + start of path. ‚Ä¢ end of path. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc><graphic coords="4,113.99,63.93,357.48,148.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Combining the spatial awareness provided by SLAM with object recognition, meaningful object labels can be assigned to locations in the map. (a) Training data for object "robosapien". (b) Overview photo of the room the robot is exploring. (c) The map with three objects, and the locations from which they were observed. Legend:</figDesc><graphic coords="5,84.75,63.93,435.60,166.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Attention methods were compared in a number of ways. (a) Shows recognition results for the three classes of approaches. (b) Demonstrates that for the random view selection approach, recognition performance increases with sampling density. Each result is averaged over 3 separate runs of the robot.</figDesc><graphic coords="6,72.23,63.93,441.00,188.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Recognition results recorded during the official run of the 2007 SRV Contest. (a)-(d) High quality views obtained by the focus of attention system, allowing for correct recognitions. (e)-(f) The system's best guesses at objects for which no good views were obtained -these are clearly incorrect.</figDesc><graphic coords="7,88.71,63.92,427.68,487.08" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>$ This work was in part supported by the Natural Sciences and Engineering Research Council of Canada and by the Swedish Research Council through a grant for the project Active Exploration of Surroundings and Effectors for Vision Based Robots.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object recognition with many local features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Helmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Generative Model Based Vision (GMBV) (Workshop at CVPR)</title>
		<meeting>Generative Model Based Vision (GMBV) (Workshop at CVPR)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object class detection with a generative model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Putting objects in perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2137" to="2144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning object categories from googles image search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Computer Vision, ICCV</title>
		<meeting>the 10th IEEE International Conference on Computer Vision, ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1816" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local features and kernels for classification of texture and object categories: A comprehensive study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating active mobile robot object recognition and slam in natural environments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ekvall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Robotics and Automation</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>IROS06</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic modeling of places using objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Website</surname></persName>
		</author>
		<ptr target="http://www.semantic-robot-vision-challenge.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autonomous vision-based exploration and mapping using hybrid maps and Rao-Blackwellised particle filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems, IROS, IEEE/RSJ</title>
		<meeting>the IEEE/RSJ Conference on Intelligent Robots and Systems, IROS, IEEE/RSJ<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2082" to="2089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1/2/3</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object discovery through motion, appearance and shape</title>
		<author>
			<persName><forename type="first">T</forename><surname>Southey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno>WS-06-03</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Cognitive Robotics</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Strategies for object manipulation using foveal and peripheral vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bj√∂rkman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Systems ICVS&apos;06</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CVPR07</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling attention to salient proto-objects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1395" to="1407" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th BMVC</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning saccadic gaze control via motion prediction, in: 4th Canadian Conference on Computer and Robot Vision</title>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Forss√©n</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Sixteenth International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1151" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-resolution maps from wide-angle sonar</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elfes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation<address><addrLine>St. Louis, MO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The vector field histogram -fast obstacle-avoidance for mobile robots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="278" to="288" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mobile robot exploration and mapbuilding with continuous localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<meeting><address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2833" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<title level="m">Vision Science: Photons to Phenomenology</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A non-myopic approach to visual search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Canadian Conference on Computer and Robot Vision CRV</title>
		<meeting>the Fourth Canadian Conference on Computer and Robot Vision CRV<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Using the forest to see the trees: A graphical model relating features, objects and scenes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
