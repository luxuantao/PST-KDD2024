<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Viet</roleName><forename type="first">Hong-Wei</forename><surname>Ng</surname></persName>
							<email>hongwei.ng@adsc.com.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Advanced Digital Sciences Center (ADSC)</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dung</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Digital Sciences Center (ADSC)</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vassilios</forename><surname>Vonikakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Digital Sciences Center (ADSC)</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
							<email>stefan.winkler@adsc.com.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Advanced Digital Sciences Center (ADSC)</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2818346.2830593</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.5.1 [Pattern Recognition]: Models-Neural nets Emotion Classification</term>
					<term>Facial Expression Analysis</term>
					<term>Deep Learning Networks 44.1% Test: 50.5% Valid: 47.3% Test: 53.8% single finetuning Valid: 42.9% Test: 47.0% Valid: 46.9% Test: 54.0% Valid: 46.9% Test: 52.2% Valid: 44.1% Test: 51.1% Valid: 48.5% Test: 55.6%</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the techniques employed in our team's submissions to the 2015 Emotion Recognition in the Wild contest, for the sub-challenge of Static Facial Expression Recognition in the Wild. The objective of this sub-challenge is to classify the emotions expressed by the primary human subject in static images extracted from movies. We follow a transfer learning approach for deep Convolutional Neural Network (CNN) architectures. Starting from a network pre-trained on the generic ImageNet dataset, we perform supervised fine-tuning on the network in a two-stage process, first on datasets relevant to facial expressions, followed by the contest's dataset. Experimental results show that this cascading fine-tuning approach achieves better results, compared to a single stage finetuning with the combined datasets. Our best submission exhibited an overall accuracy of 48.5% in the validation set and 55.6% in the test set, which compares favorably to the respective 35.96% and 39.13% of the challenge baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Facial expression analysis (also known as emotion estimation, or analysis of facial affect) has attracted significant attention in the computer vision community during the past decade, since it lies in the intersection of many important applications, such as human computer interaction, surveillance, crowd analytics etc.</p><p>The majority of existing techniques focus on classifying 7 basic (prototypical) expressions, which have been found to be universal Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. ICMI <ref type="bibr">'15</ref> across cultures and subgroups, namely: neutral, happy, surprised, fear, angry, sad, and disgusted <ref type="bibr" target="#b27">[27]</ref>. More detailed approaches follow the Facial Action Coding System (FACS), attempting either to classify which Action Units (AU) are activated <ref type="bibr" target="#b24">[24]</ref> or to estimate their intensity <ref type="bibr" target="#b18">[18]</ref>. Fewer works follow the dimensional approach, according to which facial expressions are treated as regression in the Arousal-Valence space <ref type="bibr" target="#b29">[29]</ref>. A very detailed and recent review can be found in <ref type="bibr" target="#b21">[21]</ref>.</p><p>The Emotion Recognition in the Wild (EmotiW) contest, and its Static Facial Expression Recognition in the Wild (SFEW) subchallenge, follow the categorical approach of the 7 basic expressions. Images are selected from movies, in a semi-automated way, via a system based on subtitles <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>. The challenging characteristics of SFEW are twofold. First, imaging conditions are close to real-life, including low and uneven illumination, low resolution, occlusions, non-frontal head-poses, and motion blur. Second, the size of the dataset is relatively small (?1K/0.5K/0.3K for training/validation/testing), which makes it difficult to train large-scale models and thus, is prone to overfitting.</p><p>In order to overcome these challenges, researchers in the previous EmotiW contests have mainly used fusion of multiple features, coupled with different machine learning approaches. In <ref type="bibr" target="#b22">[22]</ref>, different kernels were learned for LPQ-TOP, audio, gist and SIFT features, and were combined in an SVM classifier. In <ref type="bibr" target="#b14">[14]</ref>, the optimal fusion of classifiers for HOG, dense SIFT, and deep convolutional features was learned based on a Riemannian manifold. In <ref type="bibr" target="#b23">[23]</ref> audio, LPQ-TOP, LBP-TOP, PHOG and SIFT features were used along with a hierarchical classifier fusion method. In <ref type="bibr" target="#b3">[3]</ref> HOG-TOP and audio features were fused using multiple kernel learning. In <ref type="bibr" target="#b13">[13]</ref> convolutional and audio features were fused using Partial Least Squares and multiple classifiers. Finally, in <ref type="bibr" target="#b11">[11]</ref> multiple Deep Convolutional Neural Network (CNN) were introduced for different data modalities (video frames, audio, human actions, mouth analysis), and different combination techniques for these models were explored.</p><p>In this paper we follow a transfer learning approach for deep CNN architectures, by utilizing a two-stage supervised fine-tuning, in the context of the SFEW sub-challenge. Starting from a generic pre-training of two different deep CNN architectures based on the ImageNet dataset, a first-stage fine-tuning is applied using the FER-2013 facial expression dataset <ref type="bibr" target="#b10">[10]</ref>, which comprises 28K/32K low resolution images of facial expressions, collected from the Internet using a set of 184 emotion-related keywords. A second-stage fine-tuning then takes place, based only on the training part of the EmotiW dataset, adapting the network weights to the characteristics of the SFEW sub-challenge. Both architectures were found to improve their performance through each of the fine-tuning stages, while the cascade fine-tuning combination resulted in the submission with the highest performance.</p><p>The rest of the paper is organized as follows. Section 2 describes previous works which are related to our study. Section 3 describes in detail the proposed approach. Section 4 discusses the experimental results. Finally, concluding remarks are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Deep Learning-based approaches, particularly those using CNNs, have been very successful at image-related tasks in recent years, due to their ability to extract good representations from data. Judging a person's emotion can sometimes be difficult even for humans, due to subtle differences in expressions between the more nuanced emotions (such as sadness and fear). As a result, efficient features, finely-tuned and optimized for this particular task are of great importance in order for a classifier to make good predictions. It comes as no surprise that CNNs have worked well for emotion classification, as evidenced by their use in a number of state-of-the-art algorithms for this task, as well as winning related competitions <ref type="bibr" target="#b10">[10]</ref>, particularly previous years' EmotiW challenge <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>However, due to the small dataset size for the EmotiW 2015 image based static facial expression recognition challenge, it is easy for complex models like CNNs to overfit the data. To work around this problem of training a high-capacity classifier on small datasets, previous works in this area have resorted to using transfer learning across tasks, where the weights of the CNN are initialized with those from a network trained for related tasks before fine-tuning them using the target dataset <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b26">26]</ref>. This approach has consistently achieved better results, compared to directly training the network on the small dataset, and is the one that we adopt in this paper as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preparation</head><p>For the EmotiW dataset, all faces were detected with OpenCV's Viola &amp; Jones face detector (frontal and profile) <ref type="bibr" target="#b25">[25]</ref>. The Intraface library <ref type="bibr" target="#b4">[4]</ref> was used in order to detect 49 facial points. The fit of the alignment model, provided by Intraface, was used to discard false positives faces; any detection with a fit lower than 0.3 was considered a non-face. The average of the 6 points of the eyes (center of the eyes) was used in order to align the image in terms of rotation and scale.</p><p>The face bounding box was defined as a ratio of the eye-to-eye distance (e2e): The side boundaries were 0.62 ? e2e (counted from the respective eye corner); the upper and lower boundaries were 0.9?e2e and 1.35?e2e, respectively. These particular values were selected for three main reasons. First, they result in an approximately square bounding box. This is important, since the DNN training platform requires images to be square. If this condition is not met, images will be stretched, thus affecting their aspect ratio. Second, they ensure that a considerable part of the forehead is included, which exhibits potentially richer visual information for facial expressions, while discarding pixels located below the mouth, with limited visual information. Third, this particular approach resembles the cropping of faces in the FER-2013 dataset, which was used for pre-training, thus increasing the consistency between the two datasets. Finally, all EmotiW images were converted to grayscale, re-sized to 256?256 and normalized using min-max intensity normalization, stretching their intensity values to [0,255]. Fig 1 depicts our cropping method in comparison with the one provided by the organizers. Regarding the FER-2013 dataset, the small size of its images (48?48 pixels) prevented the reliable detection of facial points. As a result, no alignment was used. Nevertheless, even without aligning these faces, and with their size being much smaller than those in the EmotiW target dataset (48?48 vs. 256?256), we observed a significant performance boost when using them for pre-training (see section 4). Figure <ref type="figure" target="#fig_1">2</ref> depicts a comparison between these two datasets for the 7 classes. Note the similarity in the cropping between the FER-2013 and our EmotiW images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our cropping Organizers' cropping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architectures</head><p>The success of CNN for face emotion recognition motivated us to base our models on two representative CNN architectures, which we chose because of their nice tradeoffs between speed and accuracy <ref type="bibr" target="#b2">[2]</ref>:</p><p>1. The ILSVRC-2012 <ref type="bibr" target="#b19">[19]</ref> winning entry of <ref type="bibr" target="#b12">[12]</ref> (AlexNet).</p><p>2. The CNN-M-2048 model from <ref type="bibr" target="#b2">[2]</ref> (VGG-CNN-M-2048), which is a variant of the model introduced in <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset and Training</head><p>As noted above, it is challenging to train a complex model such as a CNN using only a small amount of training data without overfitting. Our approach to tackling this problem follows recent works <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b26">26]</ref>, which consistently show that supervised fine-tuning with a relatively small dataset on a network pre-trained with a large image dataset of generic objects (e.g., ILSVRC) can lead to significant improvement in performance.</p><p>Specifically, we used the the FER-2013 face expression dataset introduced in the ICML 2013 workshop's facial expression recognition challenge <ref type="bibr" target="#b10">[10]</ref> as auxiliary data to fine-tune the respective CNNs that were trained using the ILSVRC-2012 data for the architectures mentioned above.</p><p>The FER-2013 dataset comprises 3 parts; a. The Original Training Data (OTD -28709 images), b. the Public Test Data (PTD -3589 images), used when the competition was ongoing to provide feedback on the accuracy of participant's models, and c. the Final Test Data (FTD -3589 images), used at the end of the competition to score the final models. We generated 3 variants using these parts of the FER-2013 dataset:</p><p>1. FER28. This consists of the OTD for training and the PTD for validating our fine-tuned models.</p><p>2. FER32. This consists of a combination of both the OTD and PTD (28709+3589=32298 total images) for training and the FTD for validating our fine-tuned models. We experimented with different schemes for fine-tuning the base pre-trained CNN model using these datasets in combination with the EmotiW training data. These included directly fine-tuning the EmotiW dataset on the CNNs pre-trained on ILSVRC-2012, as well as a "staged" fine-tuning, where we fine-tuned first using data from the FER-2013 dataset before fine-tuning again with the target EmotiW dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Angry</head><p>The training procedure for our CNNs closely follows that of <ref type="bibr" target="#b12">[12]</ref>. They are trained using stochastic gradient descent with hyperparameters (momentum=0.9, weight decay=0.0005, initial learning rate=0.001). Note that since we are fine-tuning CNNs pre-trained on the ILSVRC-2012 dataset <ref type="bibr" target="#b19">[19]</ref> using much smaller datasets, we set an initial learning rate of 0.001, which is lower than the typical 0.01 <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12]</ref>, so as not to drastically alter the pre-trained weights. We found that doing so delays the onset of overfitting (as observed from the point where validation loss and training loss start to diverge), allowing the CNN to achieve higher accuracy on the validation data. The learning rate is dropped by a factor of 10 following every 10 epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION</head><p>The results of our submissions to the EmotiW 2015 SFEW challenge for the validation and test set, as well as some of our experiments that were not submitted, are summarized in Figure <ref type="figure">3</ref>. The corresponding confusion matrix generated from the predictions of our classifier on the test set is shown in Figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effects of Supervised Fine-tuning</head><p>Fine-tuning our CNN models using the auxiliary FER-2013 dataset in general led to a 10% increase in accuracy over the baseline method <ref type="bibr">[7]</ref> on the test set (the exception being submission 2 with a gain of 7%). A further round of fine-tuning these models using the target EmotiW dataset typically improves the accuracy on the test data by another few percentage points. For our best model (submission 3), this was an overall increase in accuracy of more than 16% over the baseline method (55.6% vs 39.13%) on the test set.</p><p>Directly fine-tuning the networks with the EmotiW training data gave an accuracy of 37.8%. While this is slightly worse than that achieved by the baseline method, it is much lower than what we obtained using the auxiliary FER-2013 dataset, which hints at the difficulty of fine-tuning deep neural networks with small datasets, and the importance of using auxiliary data.</p><p>However, we note that if we were to train a CNN using data combined from the FER-2013 and EmotiW training set (submission 9), the performance on the test set will only be around 1.5% lower than if we had separated the two datasets and fine-tuned them successively (submission 8). This suggests that the relatively small EmotiW training set (which consists of 921 images) only had a marginal effect in improving performance of a CNN that had already been fine-tuned it on a fairly large dataset such as the FER-2013. We discuss this observation further in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effects of More Labeled Data</head><p>Interestingly, even though some of our models were fine-tuned only on the FER-2013, without any data from the target EmotiW dataset, their results on the test set were competitive with those that underwent a second round of fine-tuning on the target dataset. Often the difference in accuracy is within a few percentage points (Figure <ref type="figure">3</ref>: Submission 1 vs. 3, 6 vs. 8). Furthermore, for our best model that did not use any data from the target EmotiW dataset (Submission 1), its performance on the test set was close to 15% higher than the baseline method. This is surprising because the images in the FER-2013 dataset were thought to be less than ideal as they were not aligned, unlike the target EmotiW dataset, and much smaller in size (48?48 vs 256?256). Yet, models trained using only the FER-2013 dataset were able to give good predictions on the EmotiW dataset. This suggests that the quantity rather than quality of the data could be more important for improving the performance of CNN-based face expression classifiers at the early stages of building such classifiers.</p><p>The observation that fine-tuning with a low resolution (but larger) dataset has a positive impact on the overall performance is also inline with the latest neuroscientific findings. Mounting evidence suggests that facial expression recognition in humans is holistic rather than feature-based, especially in the case of photographs (but not for schematics) <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. As a result, even people with impaired high frequency vision can distinguish successfully between different expressions, even though they rely only on low frequency visual information <ref type="bibr" target="#b1">[1]</ref>. In our case, the lower frequency (but larger) FER-2013 dataset seems to tune the network weights in a way that allows  a better generalization over data with higher visual frequency, such as the EmotiW dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Difficulties With Certain Expressions</head><p>We observe that some classes appear to be "harder" to train in the sense that, (1) none of our models were able to score highly for them, and (2) our model predictions for those classes tend to fluctuate depending on our training scheme and architecture. This appears to be the case for classes such as "disgust", "fear", "surprise", and "sad". We believe the reasons for this could be twofold. First, these classes tend to have much fewer training samples compared to classes such as "happy", "angry", and "neutral", making it harder to train the CNN to recognize them. Figure <ref type="figure">6</ref> depicts the size of the 7 classes on the EmotiW training set, where disgust, fear and surprise are the 3 smallest classes in terms of training samples. Second, these expressions can be very nuanced, making it difficult even for humans to agree on their correct labeling <ref type="bibr" target="#b20">[20]</ref>.</p><p>We suspect that the inherent difficulty in assigning labels to some of the samples may have caused them to be "mislabeled", thereby affecting the models that were trained on them. If this was the case, it would explain why our models (and the organizer's <ref type="bibr">[7]</ref>) consistently perform better on the test data than on the validation data. A possible explanation is that samples for the "tricky" cases might (for some reason) be less ambiguous in the test data than they are in the validation data, which would make it less likely that one emotion will be "wrongly" predicted for another, hence causing the accuracy obtained for the test data to be higher than those obtained for the validation data. We arrived at this explanation by comparing the corresponding confusion matrices generated by our top 5 submissions for the EmotiW validation (Figure <ref type="figure" target="#fig_3">5</ref>) and test data (Figure <ref type="figure">4</ref>). Comparing Figures <ref type="figure" target="#fig_3">5</ref> and<ref type="figure">4</ref>, we observe a general increase in accuracy for the "tricky" classes for all 5 models except that for submission 5, usually at the expense of the "happy" class. Furthermore, we note that "surprise" class, in general, appears to have the largest gain in accuracy (between 7.4% to 33.2% improvement). We also note that this improvement in accuracy from the validation to the test data is also observed in the organizer's baseline paper <ref type="bibr">[7]</ref>, where their approach improved from an accuracy of 35.96% on the validation data to 39.13% on the test data.</p><p>Lastly, we note that all except one of our models were unable to predict a single sample labeled "disgust" correctly. A reason for this could be an imbalance in the training datasets. Indeed, as Figure <ref type="figure">6</ref> indicates, "disgust" has the fewest samples in the EmotiW training dataset (comprising 7% the whole dataset) and in our FER28 and FER32 training datasets (comprising 2% of each of these two datasets). The imbalance in the number of training samples for each class of emotions most likely caused our models to overfit on the emotions with more samples (e.g., "happy") at the expense of this class. Furthermore, the expression of disgust is very subtle, which means that it will be hard for a our CNN models to discover features to robustly distinguish this emotion from other similarly nuanced expressions such as sad, fear and neutral. This can be verified by examining the confusion matrices in Figure <ref type="figure">4</ref>, which indicates that the "happy" class is often the highest scoring class, and that the classes "disgust", "sad", "fear", "neutral" are often mistaken for each other by our models. The combination of these two factors makes it even harder to train models to predict this emotion accurately. The above observations highlight the difficulty in training CNNs using a small unbalanced dataset with classes that are not visually distinctive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have shown that it is possible to obtain a significant improvement in accuracy (up to 16%) over the baseline results for expression classification on the EmotiW dataset using CNNs fine-tuned initially on auxiliary face expression datasets, followed by a final fine-tuning on the target EmotiW dataset. We also showed that, at least for the EmotiW dataset, its small size does not favor it for being used for training CNNs. However, CNNs trained on sufficiently large auxiliary face expression datasets alone can be used to obtain results much better than the baseline, without using any data from the EmotiW dataset. Furthermore, any additional improvement from using the EmotiW dataset, when a sufficiently large face dataset such as FER-2013 is available, whether by adding it to the auxiliary dataset or another round of fine-tuning, is likely to be marginal owing to its small size. This suggests that if we were to exploit deep neural networks such as CNN for face expression recognition to achieve the significant gains seen in other domains, then having bigger datasets is crucial.</p><p>Lastly, we also noted the inherent difficulty in assigning correct labels to faces depicting some of the more nuanced emotions, and how that can affect the performance of our models. This suggests that a cost-sensitive performance measure that penalizes a model mistaking samples from one class to another in "similar" classes less harshly might be more appropriate than the binary accuracy measure used for this challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>This study is supported by the research grant for the Human-Centered Cyber-physical Systems Programme at the Advanced Digital Sciences Center from Singapore's Agency for Science, Technology and Research (A*STAR).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between our cropping method and the one provided by the organizers.</figDesc><graphic url="image-2.png" coords="2,342.74,57.57,82.48,104.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between the FER-2013 and EmotiW datasets. Top row: original size of the FER-2013 dataset (48?48 pixels). Middle row: upsampled FER-2013 dataset to 256?256 pixels. Bottom row: EmotiW dataset (256?256 pixels).</figDesc><graphic url="image-10.png" coords="3,277.33,137.26,66.15,66.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Confusion matrices generated from the EmotiW validation set for our 5 best submissions.</figDesc><graphic url="image-35.png" coords="5,170.61,74.63,75.94,75.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>,</head><label></label><figDesc>November 09-13, 2015, Seattle, WA, USA c 2015 ACM. ISBN 978-1-4503-3912-4/15/11 ...$15.00.</figDesc><table /><note><p>DOI: http://dx.doi.org/10.1145/2818346.2830593.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition of facial emotion in low vision: A flexible usage of facial features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boucart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Dinon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Despretz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Desmettre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hladiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Neuroscience</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="609" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild with feature fusion and multiple kernel learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction, ICMI &apos;14</title>
		<meeting>the 16th International Conference on Multimodal Interaction, ICMI &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="508" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><surname>Intraface</surname></persName>
		</author>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011-11">Nov 2011</date>
			<biblScope unit="page" from="2106" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MultiMedia, IEEE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2012-07">July 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: Emotiw 2015</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Multimodal Interaction, ICMI &apos;15</title>
		<meeting>the 17th International Conference on Multimodal Interaction, ICMI &apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Athanasakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romaszko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Special Issue on &apos;Deep Learning of Representations</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Froumenty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International Conference on Multimodal Interaction, ICMI &apos;13</title>
		<meeting>the 15th ACM on International Conference on Multimodal Interaction, ICMI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Partial least squares regression on grassmannian manifold for emotion recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International Conference on Multimodal Interaction, ICMI &apos;13</title>
		<meeting>the 15th ACM on International Conference on Multimodal Interaction, ICMI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="525" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction, ICMI &apos;14</title>
		<meeting>the 16th International Conference on Multimodal Interaction, ICMI &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Is facial expression processing holistic?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Omigbodun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 35th Annual Conference of the Cognitive Science Society<address><addrLine>CogSci</addrLine></address></meeting>
		<imprint>
			<publisher>CSS</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Keeping it real: Recognizing expressions in real compared to schematic faces</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Prazak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Burgund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="737" to="750" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-taught learning: Transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-sensitive dynamic ordinal regression for intensity estimation of facial action units. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="944" to="958" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04">April 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anger and disgust: Discrete or overlapping categories?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R S</forename><surname>Widen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 APS Annual Convention</title>
		<meeting>the 2004 APS Annual Convention</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial affect: A survey of registration, representation, and recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple kernel learning for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dykstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sathyanarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM on International Conference on Multimodal Interaction, ICMI &apos;13</title>
		<meeting>the 15th ACM on International Conference on Multimodal Interaction, ICMI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining multimodal features with hierarchical classifier fusion for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction, ICMI &apos;14</title>
		<meeting>the 16th International Conference on Multimodal Interaction, ICMI &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="481" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing action units for facial expression analysis. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="115" />
			<date type="published" when="2001-02">Feb 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS &apos;14)</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning realistic facial expressions from web images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2144" to="2155" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>CoRR, abs/1311.2901</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation of facial expression categories in continuous arousal-valence space: Feature and correlation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1067" to="1079" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
