<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open access to the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation is sponsored by Roller: Fast and Efficient Tensor Compilation for Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">University of Toronto ‚Ä° Renmin University of China $ Shanghai Jiao Tong University ¬∂ UCSD</orgName>
								<orgName type="institution" key="instit2">Columbia University ¬£ Tsinghua University ‚ãÑ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruofan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><roleName>UCSD</roleName><forename type="first">Shanbin</forename><surname>Ke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">University of Toronto ‚Ä° Renmin University of China $ Shanghai Jiao Tong University ¬∂ UCSD</orgName>
								<orgName type="institution" key="instit2">Columbia University ¬£ Tsinghua University ‚ãÑ Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yijia</forename><surname>Diao $‚ãÑ</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shanbin</forename><surname>Ke ¬∂‚ãÑ</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">‚ãÑ</forename><surname>Lingxiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Asaf</forename><surname>Cidon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto and Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Renmin University of China and Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Yijia Diao</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Columbia University and Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Chen Zhang</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Asaf Cidon</orgName>
								<orgName type="institution" key="instit3">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open access to the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation is sponsored by Roller: Fast and Efficient Tensor Compilation for Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite recent advances in tensor compilers, it often takes hours to generate an efficient kernel for an operator, a compute-intensive sub-task in a deep neural network (DNN), on various accelerators (e.g., GPUs). This significantly slows down DNN development cycles and incurs heavy burdens on the development of general kernel libraries and custom kernels, especially for new hardware vendors. The slow compilation process is due to the large search space formulated by existing DNN compilers, which have to use machine learning algorithms to find good solutions.</p><p>In this paper, we present ROLLER, which takes a different construction-based approach to generate kernels. At the core of ROLLER is rTile, a new tile abstraction that encapsulates tensor shapes that align with the key features of the underlying accelerator, thus achieving efficient execution by limiting the shape choices. ROLLER then adopts a recursive rTile-based construction algorithm to generate rTile-based programs (rProgram), whose performance can be evaluated efficiently with a micro-performance model without being evaluated in a real device. As a result, ROLLER can generate efficient kernels in seconds, with comparable performance to the state-of-the-art solutions on popular accelerators like GPUs, while offering better kernels on newer accelerators like IPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNN) have been used extensively in intelligent tasks like computer vision and natural language understanding. As DNN computation is known for its complexity, the compute intensive sub-tasks (e.g., matrix multiplication) in a DNN model are abstracted as operators and implemented as kernels, executed on modern accelerators (e.g., GPUs, TPUs) to speed up the computation. DNN compilers play an important role in producing high-performance kernels for the development of DNN models. It reduces the burden of *Work is done during the internship at Microsoft Research.</p><p>(often hand-crafted) library-based kernel development (e.g., cuDNN <ref type="bibr" target="#b4">[6]</ref> and cuBLAS <ref type="bibr" target="#b0">[2]</ref>) and provides a flexible way to cover the fast-growing number of custom operators, which libraries struggle to catch up with and optimize, a growing pain especially for new hardware vendors.</p><p>DNN compilers treat a DNN operator as tensor computation, which is then translated into nested multi-level loops iterated over the computation on each tensor element along different axes (dimensions). Compiler optimization techniques like loop partitioning/fusion/reordering are applied to nested loops. Due to the inherent complexity of loop rearrangement, it is a combinatorial optimization problem to find a good solution among a large search space, often with millions of choices. Therefore, advanced compilers <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b32">35]</ref> propose to adopt machine learning algorithms to search for a good solution. This usually takes thousands of search steps, each evaluated in a real accelerator, to find a reasonable solution. Our own experience shows that tuning an end-to-end DNN model using state-of-the-art compilers <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b30">33]</ref> often requires days, if not weeks. The tuning time may be even longer if the DNN model runs on less mature accelerators (e.g., AMD GPU or Graphcore IPU <ref type="bibr" target="#b2">[4]</ref>) ( ¬ß2). To make the matter worse, a DNN model need to re-compile whenever its structure, operator types, tensor shapes and configurations are changed. This is often required when trying different configurations in model training or inference. Given that an operator could have arbitrary input shapes and configurations, such compilation could significantly slow down the overall DNN model development cycle.</p><p>In this paper, we propose ROLLER, a deep learning tensor compiler that addresses the problem in a radically different way. ROLLER is built on the following insights. First, instead of multi-level nested loops, ROLLER treats the computation in a DNN operator as a data processing pipeline, where data tiles (a fraction of a tensor) are moved and processed in an abstracted hardware with parallel execution units and multilayer memory hierarchy. The goal of generating efficient kernel programs then becomes that of improving the throughput of the pipeline.</p><p>Second, for an accelerator to execute efficiently, the shape of a data tile should align with the hardware characteristics, including memory bank, memory transaction length, and minimum schedulable unit (e.g., warp size in GPUs). To achieve the full alignment across multiple hardware features, the available tile shapes are limited. More importantly, with alignment as a constraint, to maximize the throughput of a pipeline, one only needs to construct an aligned tile shape that saturates the execution unit of the accelerator. This construction process is significantly more efficient than solving the original unconstrained combinatorial optimization problem.</p><p>Third, the performance of an aligned pipeline is highly predictable. Key performance metrics under the aligned pipeline (e.g., memory throughput) can be derived from the hardware specification (or through micro-benchmarking). This greatly simplifies the performance evaluation under various aligned configurations, eliminating the need of a complex cost model and/or expensive hardware-based evaluation on each aligned configuration.</p><p>With these insights, ROLLER proposes rTile, a new abstraction that encapsulates data tile shapes that align with the key features of the hardware accelerator and the input tensor shapes ( ¬ß3.1). A data processing pipeline can then be described as an rTile-based program (a.k.a. rProgram) composed by three interfaces: Load, Store, and Compute, acted against rTile. To construct an efficient rProgram, ROLLER follows a scale-up-then-scale-out approach. It first performs the scale-up process, which adopts a recursive rTile-based construction algorithm (Figure <ref type="figure" target="#fig_8">8</ref>) to gradually increase the size of the rTile shape to construct an rProgram that saturates a single execution unit of the accelerator (e.g., an SM, a streaming multi-processor in a NVIDIA GPU). It then performs the scale-out process, which simply replicates the resulting rProgram to other parallel execution units, thanks to the homogeneity of both the computation pattern of deep learning and the parallel execution units in an accelerator.</p><p>ROLLER can evaluate the performance of different rTiles without significant overheads. The peak (saturate) compute throughput can simply be measured once per operator type. And due to the alignment, other key performance factors like memory pressure of an rTile can be derived analytically from hardware specifications. This leads to an efficient microperformance model, avoiding the expensive online profiling on each configuration required by existing DNN compilers, thereby significantly speeding up the compilation process. In addition, due to the strict alignment requirements, the recursive construction process can produce a few desired rTiles (and rProgram) quickly. Combined, ROLLER can generate efficient kernels in seconds.</p><p>We have implemented ROLLER on top of TVM <ref type="bibr" target="#b12">[15]</ref> and Rammer <ref type="bibr" target="#b23">[26]</ref>, and open-sourced the code 1 . Our evaluation on 6 types and 119 popular DNN operators from several 1 https://github.com/microsoft/nnfusion/tree/osdi22_artifact/artifacts mainstream DNN models shows that ROLLER can generate highly-optimized kernels in seconds, especially for large expensive custom operators. This achieves three orders of magnitude improvement on compilation time. The performance of ROLLER-generated kernels is comparable to and often better than the state-of-the-art tensor compilers and even vendorprovided DNN libraries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation and Key Observations</head><p>Excessive compilation time. Our own experience in a set of DNN operators (detailed setting in ¬ß5) shows that the average compile time for a single operator using Ansor <ref type="bibr" target="#b30">[33]</ref>, a stateof-the-art tensor compiler, is 0.65 hours. Among them, one convolution operator in ResNet model takes 2.17 hours. A DNN model may contain hundreds of operators, thus it easily takes days to compile the model. For example, to compile a NASNet model ( ¬ß5), we reach only 32% of the overall searching progress after tuning for 41.8 hours. Our experience also shows the compilation speed is even worse on less mature devices, the compiler takes much longer time for a kernel. Observation and insights. We observe that there exists a different view to the computation of an DNN operator. Taking matrix multiplication (MatMul), C m,n = A m,k √ó B k,n , as an example to illustrate our observation. Unlike existing compilers that treat MatMul as a 3-level loop iterated over each axis m, k, n, the computation process is also a data processing pipeline. One can Load each sub-matrix (i.e., a tile) from A and B, Compute the two tiles, and Store the resulting tile of C to memory. Thus, the performance of the computation depends on how fast one can move the data tiles in the Load-Compute-Store pipeline.</p><p>The key factor affecting the performance in all steps in the pipeline is the shape of tiles and the corresponding layout in the one-dimension memory space. Figure <ref type="figure" target="#fig_0">1</ref>(a) illustrates the computation of one element in C (in the top part) and the memory accessing pattern (in the bottom part). Assuming all matrices stored in a row-major layout, loading a column from B causes strided accesses in length of 1. Suppose the memory transaction length is 4, there will be 3/4 of total redundant data reads. Thus, the data tile shape should align with the memory transaction length for efficient memory access. In Figure <ref type="figure" target="#fig_0">1</ref>(b), when computing B in the granularity of 1 √ó 4 tile, there will be no memory bandwidth waste. Besides memory alignment, the tile shape should also align with the hardware execution unit, e.g., the parallel threads number, to avoid waste in computing cycles. Moreover, the tile shape also affects data reuse opportunities due to caching, a common feature in modern accelerators. For example, Figure <ref type="figure" target="#fig_0">1</ref>(a) needs 2mnk data reads when computing a 1 √ó 1 tile each time. However, in Figure <ref type="figure" target="#fig_0">1</ref>(b), only 1.25mnk reads are required, as one read from A can be reused 4 times. If setting the tile size along M dimension to 4√ó4, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(c), the total reads can be reduced to 0.5mnk. A 10√ó improvement over Figure <ref type="figure" target="#fig_0">1</ref>(a). These observations motivate ROLLER, a system that identifies the aligned tile shapes and constructs an efficient tile processing pipeline to improve the end-to-end throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Design</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the system overview. ROLLER takes an operator described as a tensor expression ( ¬ß3.1). The expression is generated by users or from a graph-level DNN compiler <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b30">33]</ref>, which might further fuse multiple operators into a single expression. ROLLER extracts the tensor shapes from the tensor expression and leverage hardware specifications to construct rTiles, i.e., a hardware-aligned building block ( ¬ß3.1). Based on rTiles, ROLLER proposes a scale-upthen-scale-out recursive construction algorithm to generate efficient tensor programs (named rProgram) that describes the data processing pipeline ( ¬ß3.2). When generating rProgram, the construction algorithm identifies good rTile configurations by evaluating the performance of a constructed rProgram class rTile { TensorExpr expr ; TileShape shape ; TileShape storage_padding ; vector &lt; TileShape &gt; GetInputDataTiles (); vector &lt; TileShape &gt; GetOutputDataTiles (); };  The constructed rProgram is finally realized through a code generator to emit the final kernel code corresponding to the specific device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tensor Expression and rTile</head><p>ROLLER takes input of a tensor computation as an indexbased lambda expression, i.e., tensor expression <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b24">27]</ref>. It describes how each element in the output tensor is computed based on the corresponding elements in the input tensors. For example, a MatMul operator with output tensor C of the shape M √ó N can be expressed as,</p><formula xml:id="formula_0">C = compute((M,N), lambda i,j:sum(A[i,k]*B[k,j])),</formula><p>where the element indexed by (i, j) in C is computed by a sum reduction over the elements in row i of A and column j of B, and k is the reduction axis. Such an expression can cover the majority of operators in DNN models and is widely used in existing DNN compilers including TVM <ref type="bibr" target="#b12">[15]</ref>, Ansor <ref type="bibr" target="#b30">[33]</ref>, and FlexTensor <ref type="bibr" target="#b32">[35]</ref>.</p><p>ROLLER introduces RollingTile (rTile for short) as the basic computing unit to compose a tensor computation. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, an rTile encapsulates a multi-dimensional tile shape defined along each loop axis of a given tensor expression expr. Given shape and expr, an rTile can statically infer the involved input and output data tiles. For example, a tile shape <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b0">2]</ref> along axes i, j, k denotes an rTile for the above MatMul expression, where each rTile loads a 4 √ó 2 data tile from A and a 2 √ó 4 tile from B, conducts total 4 √ó 4 √ó 2 multiply-add computations, and stores a 4 √ó 4 data tile to C, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. A unique property of an rTile is that it must align with both the underlying hardware features and the tensor shapes in a given tensor expression. All these alignments are controlled by the rTile shape and the storage_padding fields in Figure <ref type="figure" target="#fig_2">3</ref>, which represent the logical form and the physical layout of an rTile, respectively. We elaborate the detailed requirements of alignment next. Alignment with the hardware execution unit. First, the shape of an rTile must align with the parallelism of the execution unit it runs on. For example, if running on a warp of threads in a GPU, the size of shape in the rTile should be a multiple of the warp size, e.g., 32, for maximal computing efficiency. When using TensorCore in NVIDIA GPUs, the rTile shape should be a multiple of 16 √ó 16 √ó 16. Similarly, an rTile executed on a streaming multi-processor (SM) should align its size as a factor of execution unit number on the SM. Alignment with memory transaction. Second, a data tile's shape should align with the length of memory transaction for optimal memory access. Specifically, for each data tile of an rTile, we should guarantee that its leading dimension (e.g., the inner-most dimension in a row-major tensor) is a multiple of the memory transaction length, as illustrated in Figure <ref type="figure" target="#fig_4">5(a)</ref>. In ROLLER, tensors are allocated in a cache-aligned way. Thus, an rTile can avoid any wasted transaction read, as its shape is aligned with the memory transaction. Alignment with memory bank. Third, the memory layout of a data tile should align its stride with the memory bank to avoid read conflicts. For example, a <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4]</ref> data tile is kept in the memory across 4 banks and is read by an upper-memorylayer tile with a shape of <ref type="bibr" target="#b1">[3,</ref><ref type="bibr">1]</ref>, as shown in Figure <ref type="figure" target="#fig_4">5(b)</ref>. A naive approach that stores all the [3, 1] values in the same bank will result in conflicted loading. rTile avoids such inefficiency by padding a data tile. Given a data tile with a leading dimension of size N, which is read by another tile with a leading dimension of size n, we add a padding size of (BL ‚àí N%(BL) + L‚åàn/L‚åâ)%(BL) along N when storing this tile, where B and L are the bank number and the bank width, respectively. The padding sizes along each axis are calculated and stored in the storage_padding field in Figure <ref type="figure" target="#fig_2">3</ref>. For the case in Figure <ref type="figure" target="#fig_4">5</ref>(b), by a padding size of 1, all the [3, 1] values are distributed in different banks and can be read efficiently.</p><p>Alignment with tensor shape. Finally, an rTile's shape should align with the tensor shape of an input tensor expression. Otherwise, the computation cannot be evenly partitioned by the rTile, wasting compute resources or incurring heavy boundary checking overheads. A simple solution is to add a padding size P i along a tensor dimension i with size of N i , which makes N i + P i a multiple of the rTile shape's dimension size at axis i. However, a large padding might waste computation. ROLLER therefore restricts tensor padding under a range Œµ, where an rTile's shape dimension size S i has to satisfy that</p><formula xml:id="formula_1">S i ‚àíN i %S i N i ‚â§ Œµ</formula><p>, where N i is the tensor size at dimension i. This ensures the wasted percentage of computation is bounded by Œµ. With this restriction, we can enumerate all the valid rTile shapes that satisfy this condition. Deriving all rTiles. Given the above alignment requirements, for a specific tensor expression and hardware device, ROLLER incrementally derives all the conforming rTiles using the following interface:</p><p>vector&lt;int&gt; GetNextAlignedAxisSize(rTile T, Dev d), which returns the next aligned size for each axis in the shape of rTile T given the specific device specification d. This is calculated by gradually increasing the dimension size along each axis until it satisfies all the alignment requirements. The rTile abstraction allows ROLLER to be extended to support new alignment requirements (e.g., new hardware features). This is achieved by adding new alignment rules to the GetNextAlignedAxisSize interface. Calculating data reuse score. An interesting property of rTile is that we can implicitly control the memory traffic by adjusting its shape. Increasing the rTile size usually brings more data reuse opportunities at the cost of occupying more memory space. Given an rTile T and its next aligned size in each axis, we can calculate the data reuse score S i for axis i by</p><formula xml:id="formula_2">S i = Q(T )‚àíQ(T ‚Ä≤ i ) F(T ‚Ä≤ i )‚àíF(T )</formula><p>, where T ‚Ä≤ i is a newly enlarged rTile by replacing the dimension size at axis i with the next aligned size from GetNextAlignedAxisSize. Functions Q(T ) and F(T ) calculate the memory traffic and memory footprint when the computation is executed in the granularity of T , which can be directly inferred based on the given tensor expression and hardware memory specification ( ¬ß3.3). A larger S i means better cost-efficiency, i.e., more memory traffic can be saved with the same memory usage. The memory reuse score plays a critical role in constructing an efficient rProgram (using rTiles), as shown in the next subsection. rProgram: units of the accelerator, and stores the resulting data tiles back to the lowest memory. For each memory layer, a specific rTile is defined to align with the characteristics of this memory layer. Thus, ROLLER describes tensor computation as a data processing pipeline with a hierarchical rTile configuration, which is called an rTile program (i.e., rProgram).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tensor Program Construction</head><formula xml:id="formula_3">Load : L2-&gt;L1-&gt;L0 Compute: L0 Store: L0-&gt;L2 rTile : L1=[4, 8, 4], L0= [2, 2, 1] (b) ùêø 1 ùêø 2 Compute:[2, 2, 1] [4,8] [4,4] [2,1] [1,2] C [2,2]</formula><p>Figure <ref type="figure" target="#fig_5">6</ref> shows an rProgram on a device with three memory layers (L0, L1 and L2). The rProgram is described by the rTile at each layer and the rTile instructions (i.e., Load, Store, and Compute) at each memory layer. Figure <ref type="figure" target="#fig_6">7</ref>(a) shows a MatMul rProgram illustrated in Figure <ref type="figure" target="#fig_6">7</ref>(b). Figure <ref type="figure" target="#fig_6">7</ref>(c) illustrates how the rProgram is mapped to each memory layer of a device. Specifically, each time it loads a <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b2">4]</ref> data tile in A and a <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b6">8]</ref> tile in B from memory L2 to L1; and then it loads the data tiles from memory L1 to memory L0 (i.e., registers) in shapes of [2, 1] and [1, 2]. After each Compute, the resulting [2, 2] tile will be directly stored from L0 to L2.</p><p>Given a data processing pipeline, the optimization goal of the corresponding rProgram is to maximize the throughput of the pipeline. The goal can be translated into three conditions: 1) the computation and memory movement should fully leverage the hardware features; 2) the throughput should saturate the bottleneck stage; and 3) there needs to be sufficient parallelism to leverage all the parallel execution units. Thus, ROLLER proposes the following rProgram construction policy: first scale-up on one core by constructing a single-core rProgram to saturate the core's hardware utilization and then 1 Func ConstructProg(expr:TensorExpr, dev:Device):  scale-out to leverage the multi-core parallelism by replicating the constructed single-core rProgram. Scaling up an rProgram. Since the alignment properties of rTile ensure hardware efficiency, ROLLER can just focus on maximizing the throughput at each memory layer by constructing the right rTile shape. By leveraging the data reuse score defined in ¬ß3.1, the single-core rProgram construction algorithm starts from an initial rTile and gradually enlarges it towards the most cost-effective axis in the rTile (i.e., with the maximum data reuse score). Note that the construction algorithm does not require an absolute data reuse score, it just picks the largest one to maximize the throughput. During the process, the memory performance improves until it hits the computational bound or the maximal memory capacity. The above process repeats for each memory layer from top to bottom, until a desired rProgram is constructed. Note that if the data reuse score remains constant for some tensor expressions, e.g., element-wise operators, ROLLER will just construct rTiles for the top layer and loads them directly from the bottom layer memory.</p><p>Figure <ref type="figure" target="#fig_8">8</ref> shows the detailed construction algorithm. Given a tensor expression expr and a target device dev, the algo-rithm constructs an initial rTile T at the top memory layer and enlarges T recursively (EnlargeTile in line 4). At each step, it enumerates the next larger rTile T ‚Ä≤ that improves the data reuse score most (GetNextRTileShapes in line 10). If T ‚Ä≤ hits the memory capacity (line 13) or the data tile loading throughput MemPer f (T ‚Ä≤ ) exceeds the peak computing throughput MaxComputePer f (T ‚Ä≤ ) (line 17), the algorithm records the current rTile T and goes on to EnlargeTile at the next memory layer. Otherwise, it continues to enlarge T ‚Ä≤ at the current layer (line 20). The construction finishes at the lowest memory layer (line 6), producing one result and repeating, until it obtains K (e.g., 5-20) rPrograms (to tolerate the hidden factors affected by the device compiler). Note that MemPer f (T ‚Ä≤ ) and MaxComputePer f (T ‚Ä≤ ) are derived based on dev, based on the micro-performance model ( ¬ß3.3). Scaling out an rProgram. Given the homogeneity of both the computation pattern of most DNN operators and the parallel execution units in an accelerator, ROLLER simply replicates the rProgram constructed on one execution unit to other units, by uniformly partitioning the computation into rTiles of the size equals to the lowest layer rTile. We achieve this by distributing all the partitions evenly to all execution units. Note that ROLLER prefers to assign the partitions split along a reduction axis on the same execution unit, as they can share the reduction results in the higher memory layers. Note that ROLLER does not assume an rProgram will exclusively occupy all computing units, the system can explicitly control the parallelism of a rProgram when scaling out. Small operator and irregular tensor shape. The scale-out algorithm inherently favors operators with sufficient parallelism, e.g., where the partition number is significantly larger than the number of execution units. For a small operator, the overall performance of the algorithm could suffer from the low utilization of parallel execution units. In general, this can be addressed by co-scheduling with other operators in compilers like Rammer <ref type="bibr" target="#b23">[26]</ref>, if there exists sufficient interoperator parallelism. Otherwise, for each rProgram, ROLLER will try to shrink its rTiles along the axis that has the smallest data reuse score to achieve sufficient parallelism. Note that this enumerating process returns the next aligned tile size each time just like other alignment rules, which is an efficient process and incurs negligible costs compared to the overall construction process.</p><p>In addition, a large operator may contain irregular tensor shapes with small dimensions, whereas ROLLER might not generate a sufficient number of rPrograms due to the alignment requirements. To address this issue, ROLLER transforms a tensor expression into a canonical form by an axis fusion pass. Specifically, for all the involved tensors, if there exist two adjacent axes in one tensor, which are either both existing and still adjacent or both missing in all other tensors, ROLLER can safely merge these two axes. For example, an elementwise operator with the tensor shape <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b8">11,</ref><ref type="bibr" target="#b1">3]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Evaluation of an rProgram</head><p>In the construction algorithm, ROLLER needs to evaluate the performance of rProgram. Instead of evaluating the end-toend rProgram in a real hardware device, ROLLER only needs to evaluates the performance of the corresponding rTile, e.g., MemPerf and MaxComputePerf in Figure <ref type="figure" target="#fig_8">8</ref>.</p><p>To this end, ROLLER builds a micro-performance model against a device described in a hardware abstraction layer (HAL). The HAL models an accelerator as multiple parallel execution units with a hierarchical memory layer. The HAL exposes three rTile-based interfaces: Load, Compute, and Store (Figure <ref type="figure" target="#fig_9">9</ref>). An execution unit is abstracted as an rTile Execution Unit (TEU), which computes the data tiles through the Compute interface. Multiple TEUs can be organized as a group, which Load and Store tiles cooperatively. The HAL treats different memory layers, e.g., register, shared memory, DRAM, as an unified type exposing the hardware specifications that affect the performance of tile movement. The specifications include memory capacity, transaction lengths, cache line size, and number of memory banks, which can be obtained by the GetDeviceSpec interface in Figure <ref type="figure" target="#fig_9">9</ref>. Micro performance model. With the hardware abstraction layer, ROLLER can easily derive the performance of a rTile (and hence the rProgram). First, given an rTile, the incurred memory footprint (including padding) and the memory traffic volume across different layer can be statically inferred from the rTile's tensor expression expr and the shape, i.e., the MemFootprint and MemTraffic interfaces in Figure <ref type="figure" target="#fig_9">9</ref>. They are used to calculate the data reuse scores and check if an rTile exceeds the memory capacity. Second, to calculate MaxComputePerf of an rTile, ROLLER conducts a one-time profiling to measure the peak compute throughput by aggressively enlarging the compute tiles (e.g., multiple of warp size in an SM) to saturate the TEU. This performance data is cached in ROLLER for future query in the construction al-gorithm. Finally, for a given rTile, ROLLER also estimates MemPerf, the performance on loading data tiles from a memory layer to a higher layer. Given the aligned memory access in rTile, the latency of loading a regular chunk of data can be simply modeled by the division of the total traffic to the memory bandwidth. For the memory layer shared by all TEUs, we split the bandwidth evenly. For the smaller accessing sizes, ROLLER also conducts a one-time offline profiling for each device type and cache the results. It is worth noting that the micro-performance model only needs to be accurate when the tile shapes are fully aligned, a key requirement of ROLLER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>Our implementation of ROLLER is based on TVM <ref type="bibr" target="#b12">[15]</ref> and Rammer <ref type="bibr" target="#b23">[26]</ref>, two open-source DNN compilers. ROLLER's core mechanisms, including expression optimization, construction algorithm, micro-performance model, etc., are implemented with 8K lines of code. ROLLER's compilation pipeline is as follows. Its input is an ONNX graph <ref type="bibr" target="#b7">[9]</ref> or a TensorFlow frozen graph <ref type="bibr" target="#b10">[13]</ref>. ROLLER first leverages Rammer to conduct graph level optimizations (e.g., inter-and intra-operator co-scheduling). Next ROLLER derives the TVM tensor expressions for each (fused) operator extracted from the optimized graph, and generates corresponding rProgram by ROLLER's construction algorithm, and performs kernel generation. Finally, the generated kernels are injected to Rammer's runtime and generate the end-to-end model code. Code generation. Given the fixed code structure in an rProgram (in Figure <ref type="figure" target="#fig_5">6</ref>), ROLLER generates the kernel code through a predefined template, implemented as a TVM schedule with its built-in scheduling primitives. Loading and storing data tiles at each memory layer are implemented by TVM's cache_read and cache_write primitives. Partitioning on rTile is done through split and fuse. Some primitive rTile computation is implemented with TVM's intrinsic API. With the template, a given rProgram can be directly generated into device codes, e.g., CUDA kernels. Tensor padding. ROLLER relies on tensor padding to align rTiles with tensor shape. In practice, most tensors in the lowest memory (e.g., DRAM) are allocated by external program (e.g., DNN framework), thus we just apply padding in the upper layer memory (e.g., shared memory). Our tensor padding currently requires the input tensor expression to specify whether it allows to pad, as well as the default padding value (e.g., 0 for MatMul operator). For the storage padding for memory bank alignment, we leverage TVM's storage_align primitive to add padding. Performance profiling. ROLLER implements two profilers: a micro-performance profiler and a kernel profiler. The former generates device specifications, e.g., memory bandwidth, computing throughput, etc., through a set of microbenchmarks, which is a one-time offline profiling for each device type and tensor expression types (regardless of the tensor shapes). The latter profiles the fastest kernels among the top K rPrograms and is used for each compilation result if the K is larger than 1. In practice, the performance of a specific kernel code is also slightly affected by some devicecompiler and hardware related hidden factors, which ROLLER can hardly control. These factors include instruction density of different instruction types, register allocation behaviors, device compiler optimizations, warp scheduling overhead, etc. Particularly, on NVIDIA GPUs, ROLLER relies on nvcc <ref type="bibr" target="#b1">[3]</ref> to compile the generated CUDA codes into machine code. However, nvcc's proprietary optimizations might undesirably affect the program execution behaviors. Thus, ROLLER leverages the kernel profiler to quickly evaluate top performing rPrograms and select the best one. A larger K could generally increase kernel quality. After evaluating the top 10, 20, and 50 results, our experiences show that top 10 could recall the optimal results for most cases. Note that ROLLER's kernel profiler differs from the evaluation process driven by a machine learning algorithm in previous compilers <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b32">35]</ref>. The MLbased approach usually requires hundreds even thousands of sequential evaluation steps, while ROLLER only profiles tens of candidates in parallel. In future, we plan to implement assembly-level code generation to alleviate the hidden issues in a high-level device compiler.</p><p>ROLLER's HAL allows us to support different accelerators easily. User can configure the corresponding HAL for each device type. ROLLER also provides built-in configurations for most common device types. Some detailed configurations, e.g., memory bandwidth, rely on micro-benchmark profiling or derive from published device specifications. Next, we share our experiences in implementing the HAL on several popular DNN accelerators, including NVIDIA GPUs, AMD GPUs and Graphcore IPU. ROLLER on NVIDIA CUDA GPUs. An NVIDIA GPU usually employs a centralized memory architecture. We implement ROLLER on V100 and K80, two CUDA GPUs with different architectures on the streaming multi-processors (SMs). Their memory architecture contains global memory, L2 cache, L1 cache, shared memory, and register. In ROLLER's HAL, we abstract them into 3 memory layers: L2 layer for global memory and L2 cache, L1 layer for only the shared memory, and the L0 layer for register. We ignore L1 cache because it shares the space with shared memory and cannot be controlled by user programs. The memory bandwidths of all levels are measured by our micro-benchmarks. The transaction length at the global memory layer is set to 32 Bytes, i.e., 8 float elements, for both GPUs. For V100 GPUs, the bank number and the bank length of the shared memory is 32 and 4 Bytes respectively. For K80 GPUs, the bank length is 8 Bytes. The shared memory capacities are set as 48KB for both GPUs (based on deviceQuery).</p><p>We implement the TEU on CUDA GPUs as a warp of 32 threads, which is also the basic unit to execute the TensorCore WMMA instructions. The size of a TEU Group on a HAL (e.g., a SM) is set to the warp scheduler number, which is 4 for both GPUs. The SM number is 80 for V100 <ref type="bibr" target="#b18">[21]</ref> and 13 for K80. On CUDA GPUs, each thread has a limited register capacity, e.g., 255 registers for V100. Exceeding this limit will lead to register spilling, causing significant performance degradation. This sets a limit to the size of an rTile at register layer. We notice that the nvcc compiler will implicitly declare more registers (for loop variables or other purposes). Given that this behaviour is hard to predict, we reduce the register limit empirically to only 96 registers for both V100 and K80 per thread to avoid unexpected performance impacts. ROLLER on AMD ROCm GPUs. We also implement ROLLER on MI50 <ref type="bibr" target="#b9">[12]</ref>, AMD's second-generation Vega series GPU. MI50 shares a similar memory architecture as V100: the centralized global memory can be accessed by all compute units (CUs). Like SMs in NVIDIA GPU, each CU has its own scratchpad memory, registers, and computation cores. The data movement of a ROCm [1] kernel program is also similar. The memory transaction size for the global memory is set as 64 Bytes. The memory bank number is 32 and bank length is also 4 Bytes. We also implement the TEU as a warp of threads, which is 64 threads on MI50 GPUs. The maximal register size is empirically limited to 70 registers per thread. All other specifications such as the memory bandwidths at each layer, peak computing throughput, etc., are measured with our micro-benchmark. ROLLER on Graphcore IPUs The Graphcore IPU <ref type="bibr" target="#b19">[22]</ref> is a massive parallel MIMD processor with 1216 parallel processing cores. Distinct from NVIDIA and AMD GPUs, an IPU employs a distributed memory architecture. There is only 256KB on-chip local memory attached per core, and no unified global memory. When the local memory is unable to hold all the input data, by default, the initial data of a kernel program is stashed in the on-chip local memory and evenly distributed across the nodes. Thus, ROLLER's HAL for IPUs also abstracts three memory layers: L2 for all the remote memories across all cores, L2 for the local memory on each core, and L0 for the register. We take advantage of prior benchmarking work <ref type="bibr" target="#b19">[22]</ref>, which has successfully measured peak memory bandwidth and computation throughput. The size of the register files per IPU core is not publicly available. Considering that we have no prediction for behaviours of the IPU program compiler, we allow each upper-level rTile to use only 10 registers, which safely guarantee that the tiling algorithm does not emit invalid tiling configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We evaluate ROLLER on both DNN operator benchmarks and end-to-end models by comparing with state-of-the-art DNN compilers and frameworks. We first summarize our findings: 1) ROLLER achieves three orders of magnitude speedup on compilation time, compared to TVM and Ansor. On V100 GPU, the most expensive operator takes 43 seconds, while We compare ROLLER against other tensor compilers, vendor libraries and DNN frameworks, including TVM <ref type="bibr" target="#b12">[15]</ref> (v0.8) and Ansor <ref type="bibr" target="#b30">[33]</ref> (v0.8), two state-of-the-art tensor compilers; cuDNN, cuBLAS, rocBLAS (ROCm GPUs), POPLAR library (Graphcore IPU), which are vendor libraries; Tensor-Flow (v1.15), a state-of-the-art DNN framework; TensorFlow-XLA a state-of-the-art DNN full-model compilers; and Ten-sorRT (v7.0) (with TensorFlow integration version), a vendorspecific inference library for NVIDIA GPUs. We validate our compilation results by comparing them against Ansor's. Benchmarks. Our evaluation benchmark uses four typical DNN models, including ResNet-50 <ref type="bibr" target="#b16">[19]</ref> (CNN), LSTM <ref type="bibr" target="#b17">[20]</ref> (RNN), NASNet <ref type="bibr" target="#b33">[36]</ref> (a state-of-the-art CNN model obtained  <ref type="table" target="#tab_2">1</ref> lists a representative subset of operators as well as their configurations. The last column lists the corresponding abbreviation of each operator. The full list of the operator configurations is omitted due to page limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on NVIDIA GPUs</head><p>This section first evaluates ROLLER's operator performance, compilation time, and scalability on large operators by comparing against the state-of-the-art tensor compilers and vendor libraries. We also evaluate the performance of ROLLER on TensorCore. Finally, we show the end-to-end model performance compared to existing DNN compilers and framework.</p><p>Operator performance. We first evaluate the performance of ROLLER generated kernels by comparing against TVM (i.e., AutoTVM with XGBoost tuning algorithm <ref type="bibr" target="#b13">[16]</ref>), Ansor, cuBLAS (for matrix multiplication operators) and cuDNN (for convolution operators). Vendor libraries like cuBLAS and cuDNN are wrapped in TensorFlow to evaluate the performance. For the rest of operators (e.g., element-wise, reduce), we use TensorFlow's built-in kernel implementations.</p><p>To amortize the overhead of data feeds/fetches in Tensor-Flow's session, we repeat the kernel running for 1,000 times in each session and calculate the average. We set the tuning steps for TVM and Ansor to 1,000 for each operator, same as Ansor's evaluation setup <ref type="bibr" target="#b30">[33]</ref>, and report the best results. We compare both the top-1 and the best from the top-10 kernels constructed by ROLLER, the latter can tolerate some hidden performance impacts from device compilers. Figure <ref type="figure" target="#fig_0">10</ref> plots the average kernel performance for all the 119 operators in our benchmark, ordered by the operator type and ID. We plot the large operators (e.g., kernel time is larger than 5ms) in the top sub-figure in a log-scale for y-axis, and the other medium and small operators in the bottom 4 sub-figures 2 . First, compared to CUDA libraries (CudaLib), ROLLER could get comparable performance (i.e., within 10% performance) for 81.5% of the total operators, and can be even faster for 59.7% of them. We observe that the majority of operators that ROLLER performs worse are convolution operators with 3 √ó 3 or larger filters, which are usually implemented with a more efficient numerical algorithm (e.g., Winograd <ref type="bibr" target="#b20">[23]</ref>) in cuDNN and hard to be expressed by the tensor expression. This is the reason Ansor and TVM are also slower than CudaLib in these cases. Second, compared to TVM and Ansor, ROLLER could also get comparable performance for 72.3% and 80.7% of the total operators respectively. The rest 27.7% and 19.3% of them are mainly small operators or with irregular tensor shapes, which are by natural hard to align with the hardware. However, these operators usually have relatively short kernel time, e.g., only 1.65ms and 1.16ms on average. Among 54.6% and 65.5% of the total 2 Please find the complete results in our artifact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association</head><p>16th USENIX Symposium on Operating Systems Design and Implementation 241   operators, ROLLER can even produce faster kernels than TVM and Ansor, respectively. We observe that the majority of these operators are large and time-consuming ones. As it shows in the top sub-figure where operators are larger than 5ms (up to 343ms), ROLLER could achieve better performance for most of these operators, e.g., by 1.85√ó and 1.27 √ó speedup over TVM and Ansor on average. Compilation time. Given the comparable kernel performance, the major advantage of ROLLER is its fast compilation. Figure <ref type="figure" target="#fig_10">11</ref> compares ROLLER's compilation time against TVM and Ansor for all the operators. The operator ID is sorted by the compilation time for each line. The average operator compilation time for TVM is 0.65 hours and up to 7.89 hours. For the first 40 operators, which are mainly the element-wise, reduction, and pooling operators, TVM's compilation takes less than 10 seconds. This is because TVM's manually-written code templates for these operators can directly emit code without searching. However, Ansor generates search spaces for all the operators. Its compilation time takes 0.66 hours on average and up to 2.17 hours. In contrast, ROLLER's top-1 kernel results can be generated in 1 second for most operators and in 0.43s on average, which is more than three orders of magnitude faster. The major time is spent on the recursive constructing algorithm, which increases slightly with the growth of operator size, but quickly stabilizes as the recursive depth (to enlarge the rTiles) is bounded by the limited memory capacity. To get the optimal kernels from the top-10 candidates, ROLLER's average compilation time is only 13.3 seconds. The major cost comes from the kernel code compilation with the device compiler and the evaluation on target devices. Scale-out with operator size. We evaluate the scalability of ROLLER on larger operators by comparing with both CUDA   <ref type="figure" target="#fig_13">13</ref> show the performance comparisons. For the MatMul operator, both Ansor and ROLLER have a linear scalability over the batch sizes and comparable performance with CudaLib (i.e., cuBLAS). However, TVM's performance is relatively non-stable. For example, ROLLER can outperform TVM by average 11.2√ó and up to 36.1√ó for the batch size of 1024. For Conv2D operators, ROLLER can still achieve linear scalability over the batch size, and get slightly better performance than Ansor and TVM (by 1.25 and 1.54√ó on average). Note that Anosr is unable to search for a valid kernel for the batch size over 2048 using its default configurations. TVM can generate valid kernels, but the performance is scaled sublinearly for the larger batch sizes, e.g., ROLLER can achieve more than 1.9 √ó speedup for batch sizes greater than 2048.</p><formula xml:id="formula_4">M 5 1 2 M 1 K M 2 K M 4 K M 8 K M 1 6 K C 1 2 8 C 2 5 6 C 5 1 2 C 1 K C 2 K C 4 K C 8 K Compile time (s) TVM Ansor Roller-top1 Roller-top10</formula><p>Finally, Figure <ref type="figure" target="#fig_14">14</ref> compares the compilation time for the two operators with different batch sizes. The average compilation time of TVM and Ansor is 2.36 (up to 9.55) hours and 1.19 (up to 3.0) hours respectively. Moreover, their compilation time grows constantly with the growing of batch size. This is because that they are both based on ML-based search approach, whose search space usually increases exponentially with the operator size. In contrast, ROLLER produces the top-1 kernel in 1 second, and 16 seconds (up to 34 seconds) on average for the top-10 kernel. Compile on TensorCore. ROLLER could easily support hardware tensor ISAs (e.g., TensorCore) by aligning the rTile shape with the hardware instruction shape. We use the 16√ó16√ó16 WMMA instruction in ROLLER. We remove An- sor in this experiment as it does not support TensorCore to our best knowledge. We select 4 large MatMul operators that are friendly to TensorCore in this experiment. Figure <ref type="figure" target="#fig_4">15</ref> shows the performance comparisons. As it shows, by constructing from the aligned rTile shape, ROLLER can quickly produce good kernels on TensorCores, e.g., within a 43% performance gap to cuBLAS. Note that cuBLAS is highly optimized with a lot of hand-crafted optimizations on TensorCore. As a comparison, TVM fails to generate valid kernels for 3 of the 4 total operators with the default configurations. We try to increase the tuning steps from 1,000 to 10,000, it is still unable to find a legitimated kernel due to its poorly-defined search space. Small operators and irregular tensor shape. ROLLER optimizes performance for small operators by shrinking the rTile when there is insufficient parallelism. We demonstrate the performance of this optimization for the two small MatMul operators. Figure <ref type="figure" target="#fig_5">16</ref> compares the performance of the original rTile configuration without sufficient parallelism (Roller-O), and the shrunken rTile configuration (Roller-S) which matches the SM parallelism. As it shows, shrinking rTile could significantly improve performance than the original kernel, e.g., by 2.3√ó on average. However, ROLLER is still slower than Ansor, e.g., by 50% on average, on small operators, even it is significantly faster than TVM by 6.6√ó. For such operators, we can further leverage search-based approach to fine-tune the configurations to obtain a better performance. ROLLER compiles operators with irregular tensor shapes with two optimizations: i.e., axis fusion and tensor padding with bound parameter Œµ. We demonstrate their benefits on a representative set of irregular convolution operators, as shown in Figure <ref type="figure" target="#fig_16">17</ref>. We compare the performance of ROLLER without any optimizations (Roller-B), with axis fusion (Roller-F), and further with tensor padding of Œµ from 0.4 to 1.0 (Roller-P0.4 and Roller-P1.0). All ROLLER's performances are the best one selected from the top-10 candidates. First, with axis fusion optimization, ROLLER is able to have more rTiles that aligns with the tensor shapes, which improves the kernel performance by 1.5√ó on average. Moreover, with the tensor T=32,B=10 T=32,B=20 T=32,B=40 T=32,B=80 T=32,B=160 T=32,B=320 T=32,B=640 T=32,B=1280 T=32,B=2560 T=64,B=10 T=64,B=20 T=64,B=40 T=64,B=80 T=64,B=160 T=64,B=320 T=64,B=640 T=64,B=1280 T=64,B=2560 T=128,B=10 T=128,B=20 T=128,B=40 T=128,B=80 T=128,B=160 T=128,B=320 T=128,B=640 T=128,B=1280 T=128,B=2560 T=256,B=10 T=256,B=20 T=256,B=40 T=256,B=80 T=256,B=160 T=256,B=320 T=256,B=640 T=256,B=1280 T=256,B=2560 T=512,B=10 T=512,B=20 T=512,B=40 T=512,B=80 T=512,B=160 T=512,B=320 T=512,B=640 T=512,B=1280 T=512,B=2560 T=1024,B=10 T=1024,B=20 T=1024,B=40 T=1024,B=80 T=1024,B=160 T=1024,B=320 T=1024,B=640 T=1024,B=1280 T=1024,B=2560</p><p>Compute (GFLOPS)</p><p>Figure <ref type="figure" target="#fig_8">18</ref>: Memory throughput (DRAM and shared memory) and compute throughput from our micro-performance model and real measurement (X-axis: kernel configurations with different number of threads per block (T) and blocks (B)).</p><p>padding optimizations (e.g., at Œµ of 1.0), ROLLER can further improve performance than Roller-F by 1.4√ó. This is mainly because the number of legitimated kernels is very limited with smaller Œµ for irregular shapes. Increasing the Œµ allows ROLLER to have chance to select from more candidate kernels.</p><p>Micro-performance model. We conduct extensive experiments to validate the micro-performance model, including global memory throughput, shared memory throughput, and compute throughput, under different kernel configurations (i.e., different thread block and grid size). Figure <ref type="figure" target="#fig_8">18</ref> compares the performance estimated by our micro-performance model with that measured on real device. As shown, when the configuration is not aligned with the parallelism of execution units, i.e., thread number per block is less than 128 (4 warps), our model produces a relatively estimation error, especially for the DRAM throughput. This is also the case when there is insufficient parallelism (i.e., block number is less than 80). Thus, we can see our micro-performance model is accurate only for those shape-aligned configurations (i.e., rTiles), as they fully exploit hardware efficiency. This also motivates us to choose only the aligned rTiles, which greatly reduces the complexity of micro-performance model.   computation, we profile each generated kernel and compare the corresponding resource utilization at the saturated layer with the theoretical hardware limit. Table <ref type="table" target="#tab_5">2</ref> lists the distribution of the resource utilization for the total 119 operators.</p><p>The table shows most kernels saturate hardware resources, e.g., 66% of them utilize more than 90% of the theoretical limit. For the few under-utilized kernels, especially whose utilization is less than 80%, our investigation shows that they are mostly small operators with insufficient parallelisms.</p><p>To understand the impact of different alignment rules, we incrementally turn on each alignment optimization and evaluate its performance improvement. Table <ref type="table" target="#tab_6">3</ref> shows the average speedup compared with the baseline (without any optimization). For example, EUAlign shows the kernels with the alignment on execution units and memory transaction alignment (MemAlign) can together improve the performance by 1.88x than the baseline. Bank alignment (BankAlign) has relatively small improvement because kernels are already bank conflict free. End-to-end model performance. We evaluate the end-toend model performance of ROLLER by comparing against Ten-sorFlow (TF), TensorFlow-XLA (TF-XLA), TensorRT (TF-TRT), and Ansor, which represent DNN framework, graph-level compiler, vendor-provided DNN engine, and DNN compiler with tensor compilation, respectively. Note that TensorRT is also the core engine in NVIDIA Triton inference server <ref type="bibr" target="#b6">[8]</ref>. We omit TVM in this experiment as it usually requires an order of magnitude longer compilation time on tuning end-to-end models than Ansor <ref type="bibr" target="#b30">[33]</ref>. ROLLER's end-to-end model compilation is implemented in Rammer (i.e., Rammer+Roller) by feeding the generated kernels into it. To create a fair baseline, we manually feed both the TVM and Ansor generated kernels for the same set of operators into Rammer, which are denoted as Rammer+TVM and Rammer+Ansor.</p><p>Table <ref type="table" target="#tab_8">4</ref> lists the model execution time for each model compiled or executed by each compiler and framework. Note that TF-XLA fails to compile the BERT-Large and NASNet model (out-of-memory). TF-TRT also fails to run the BERT-Large model due to exceeding the maximum protobuf size limit (2GB) in its graph loading stage. For Ansor, we set the total tuning steps as 1,000 multiplied with the number of subgraphs for each model. However, Ansor also fails to produce   First, for the ResNet and NASNet models, ROLLER can only achieve comparable and mostly slower performance than TF, TF-XLA, and TF-TRT (up to 26.7% slower compared to TF-XLA for ResNet). This major overhead in ROLLER is caused by the less efficient convolution kernels compared to cuDNN as explained before. However, for the BERT-Large and LSTM models, ROLLER can outperform all other frameworks and compilers, e.g., by 1.07√ó and 1.55√ó faster than the state-ofthe-arts, i.e., TF for BERT-Large and TensorRT for LSTM. This mainly due to ROLLER's kernel construction favors large and regular operator shape, which are heavily used in the BERT-Large model. For both the BERT and LSTM models, since ROLLER can control to generate resource-efficient kernels by the scaling-up policy, it provides more opportunities for Rammer to co-schedule parallel kernels on the parallel SMs on GPUs. They together produce an efficient end-to-end program, which can even outperform TF-TRT by 1.55√ó for LSTM. Among all the implementations, Ansor can also produce very efficient programs for all the rest 3 models except for the BERT. However, it requires a long compilation time (29.3 hours on average). For the NASNet model, it reaches only 32% of the overall searching progress after tuning for 41.8 hours. In contrast, ROLLER only takes 422s on average to compile these models. This includes the graph-level optimization and the full-model compilation time in Rammer, which occupies about 41% of the total time on average.</p><p>Operator performance on K80 GPUs. We also evaluate ROLLER on the K80 GPUs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Other Accelerators.</head><p>Operator performance on AMD ROCm GPUs. We evaluate ROLLER on AMD ROCm GPUs by comparing it against ROCm libraries, TVM, and Ansor. Table <ref type="table" target="#tab_11">6</ref> shows the percentage of operators that ROLLER can produce better or comparable performance (e.g., within 5% and 10% differences) in our operator benchmarks. Compared to the ROCm libraries (e.g., rocBlas), 73.1% of the total operators ROLLER can produce better kernels. This percentage is much higher than that on CUDA GPUs (59.7% and 54.6% for V100 and K80 GPUs). This is mainly because the libraries on CUDA GPUs are more mature than the ROCm GPUs, where ROLLER can help significantly. Compared to TVM and Ansor, ROLLER can also produce 58.8% and 70.6% better kernels. Similar to CUDA GPUs, the kernels that are slower by more than 10% are mostly small operator and those with irregular tensor shapes: the average execution time of these kernels are only 1.69ms and 1.57ms for TVM and Ansor, respectively. Finally, the average compilation time for all operators is 0.85 (up to 4.2) hours for TVM and 0.99 (up to 3.4) hours for Ansor, respectively. In contrast, ROLLER's average compilation time is 0.24 (up to 0.63) seconds for top-1 kernel and 7.69 (up to 49.0) seconds for top-10 kernel.</p><p>Operator performance on Graphcore IPU. We evaluate ROLLER on Graphcore IPUs. Due to the limited on-chip memory capacity, we only evaluate a set of small MatMul and Conv2D operators with different configurations. Figure <ref type="figure" target="#fig_9">19</ref> shows the average kernel time of each operator in log-scale, comparing against the Poplar-sdk library (i.e. 6 Discussion and Future Work</p><p>Optimization space compared with loop-based compiler.</p><p>The abstraction of rTile and data processing pipeline allows ROLLER to construct an optimization space overlapped with, but different from, existing DNN compilers (e.g., Ansor) <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b32">35]</ref>. As mentioned previously, these compilers view tensor compilation as nested loop optimizations. For example, Ansor allows only divisible tiling sizes along a tensor dimension to partition a loop axis evenly. This makes it usually perform worse for tensor shapes with prime dimensions. ROLLER instead focuses on maximizing hardware efficiency from the data-processing-pipeline view, allowing more aggressive optimizations, e.g., exploring non-divisible but hardware-aligned tiling sizes with fused adjacent axis and padded tensor shapes. Driven by our observation that most DNN operators are memory-bound, ROLLER fundamentally differs from existing DNN compilers by first optimizing data-tile throughput, i.e., maximizing reuse score rewards and aligning with hardware features, and then for parallelism. Such a trade-off inherently leads to fast compilation and good performance for operators with sufficient parallelism.</p><p>Optimization trade-off. ROLLER's design philosophy is based on an observation: large and dense operators tend to be major contributors to the execution time. This leads to a design trade-off: optimizing data reuse (i.e., maximizing pipeline throughput) as the primary optimization goal, and turning other hardware related optimizations into alignment constraints. Such trade-off results in fast compilation and high kernel quality for a majority of operators in mainstream workloads. For small operators, ROLLER further employs some adaptive mechanisms to trade-off among different optimiza-tion goals, e.g., using a threshold to limit redundant work ( ¬ß3.1) when there are insufficient results, employing an adapting rTile shrinking process to increase parallelism ( ¬ß3.2), etc. Future work. ROLLER currently relies on high level device compiler, e.g., nvcc, to compile kernel code to executable. This sometimes introduces undesirable performance impacts and forces ROLLER to allocate registers conservatively. This is because the device compiler will implicitly allocate registers for intermediate values (e.g., loop variables). ROLLER cannot detect implicit register allocation beforehand, hence it is difficult to estimate and decide the precise register usage. One of our future work is to generate assembly (e.g., PTX for NVIDIA GPUs) code directly to avoid the side effects from the high level device compiler. Moreover, although the key hardware information that affects performance, including memory bandwidth, capacity, and transaction length, is often available in the hardware specification, there are still some devices (e.g., mobile GPUs) lacking such information. Another future work is to leverage some profiling techniques <ref type="bibr" target="#b22">[25]</ref> to disclose and quantify those hardware features.</p><p>ROLLER's HAL assumes hardware contains homogeneous computing units and symmetric memory accessing. However, we also observe that some devices have NUMA architecture. This makes it difficult for the micro-performance model to estimate rTile performance, as the same tile will perform differently at different locality under NUMA architecture. We leave this issue as future work.</p><p>Finally, the optimization for sparse kernel may also violate the assumption of homogeneous workload in a DNN kernel and make the micro-performance model inaccurate. Some tiles with a larger degree of sparsity may perform differently from dense tiles. ROLLER assumes a higher level, sparsityaware compiler (e.g., SparTA <ref type="bibr" target="#b31">[34]</ref>) will address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Most tensor compilers treat DNN operators as nested multilevel loop computation, which essentially defines a large space with a combinatorial complexity. TVM <ref type="bibr" target="#b12">[15]</ref> inherits the insight from Halide <ref type="bibr" target="#b24">[27]</ref> and describes DNN operators as loop optimization schedule primitives. Later, AutoTVM <ref type="bibr" target="#b13">[16]</ref> extends TVM to apply an ML-method to search for the best configurations from manually written code templates. FlexTensor <ref type="bibr" target="#b32">[35]</ref> proposes to automatically explore the space without manual templates. Ansor <ref type="bibr" target="#b30">[33]</ref> further advances such automation. It generates an even larger search space considering a hierarchical code structure and adopts an evolution algorithm to find performant kernels. Compilers like Tiramisu <ref type="bibr" target="#b11">[14]</ref>, AKG <ref type="bibr" target="#b29">[32]</ref>, and Tensor Comprehensions <ref type="bibr" target="#b26">[29]</ref> apply polyhedralbased techniques to loop optimization, which transforms the loop into an integer programming problem and finds a good configuration with a solver. All these approaches rely on a huge search space to provide good kernel, which leads to long compilation/solving time. ROLLER explores a different approach to construct rTiles that align with hardware features.</p><p>Tensor Processing Primitives (TPPs) <ref type="bibr" target="#b15">[18]</ref> define a set of 2D-tensor operators to compose complex operators on highdimensional tensors, providing limited expressiveness. In contrast, ROLLER does not limit the dimension of tile shape and can be applied to general tensor expressions. The OpenAI Triton <ref type="bibr" target="#b25">[28]</ref> is a programming framework and compiler for developing block-based GPU kernels. Triton relies on programmers to decide the block size and block scheduling, while this is the key problem ROLLER addressed by considering both hardware features and tensor shapes. MLIR <ref type="bibr" target="#b3">[5]</ref> and Tensor IR [10] plan to support block-level (i.e., tile) computation representation in their IRs. ROLLER's rTile abstraction and the rProgram construction are compatible with these initiatives.</p><p>Graph-level DNN compilers like XLA <ref type="bibr" target="#b8">[11]</ref>, TVM <ref type="bibr" target="#b12">[15]</ref>, and Rammer <ref type="bibr" target="#b23">[26]</ref> focus on cross-operator optimizations, e.g., operator fusion/co-scheduling. ROLLER's kernel generation is compatible with these compilers. ROLLER's rTile abstraction complements the rTask concept in Rammer <ref type="bibr" target="#b23">[26]</ref> as it provides an efficient way to construct an rTask.</p><p>Finally, some works focus on operator-specific optimizations. CUTLASS <ref type="bibr" target="#b5">[7]</ref> is a template for implementing matrixmultiplication. An analytical model <ref type="bibr" target="#b21">[24]</ref> is proposed to find the best loop-level optimization configuration only for convolution operators on multi-core CPUs. And DREW <ref type="bibr" target="#b27">[30]</ref> proposes a new way to optimize Winograd convolution using data compression <ref type="bibr" target="#b28">[31]</ref>. ROLLER's optimization approach is general for DNN operators on various devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>ROLLER takes an unconventional approach to deep learning compiler. Instead of relying on costly machine learning algorithms to find a good solution in a large search space, ROLLER generates efficient kernels using a recursive constructionbased algorithm that leverages the new rTile abstraction with much fewer shapes that align with multiple hardware features. The constructed program can be evaluated by a micro performance model, without running on a real device every time. As a result, ROLLER can compile high-performance kernels in seconds, even in less mature accelerators. More importantly, ROLLER offers a unique and significantly more efficient approach for new AI hardware vendors to build competent vendor-specific DNN libraries, bridging the ecosystem gap to market leaders and thereby facilitating innovations in AI accelerators.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Access pattern of different tile shape. Matrix multiplication, C m,n = A m,k √ó B k,n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System overview of ROLLER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The data structure of rTile. rTile.shape: [i, j, k]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The data tiles and computing tile inferred by an rTile for MatMul expression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of (a) transaction aligned memory load and (b) bank conflict-free padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The pseudo code of an rProgram on a device with a 3-layer memory hierarchy (Bottom-up: layer L2 to layer L0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ROLLER computation model. (a) An rTile program; (b) rTiles on matrix multiplication; (c) Execution of the rTile program on a hardware memory hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 T 13 if 17 if 23 SortedRTiles 25 T</head><label>213172325</label><figDesc>dev.MemLayer(0), rProg());5 Func EnlargeTile(T:rTile, mem:MemLayer, P:rProg): MemFootprint(T ‚Ä≤ ) &gt; mem.Capacity() 14 P.Add(mem, T ); 15 EnlargeTile(T , mem.Next(), P); 16 else MemPerf(T ‚Ä≤ ) &gt; MaxComputePerf(T ‚Ä≤ .expr) 18 P.Add(mem, T ‚Ä≤ ); 19 EnlargeTile(T ‚Ä≤ , mem.Next(), P); 20 EnlargeTile(T ‚Ä≤ , mem, P); 21 Func GetNextRTileShapes(T:rTile, mem:MemLayer) 22 alignedSizes = GetNextAlignedAxisSize(T , mem); ‚Ä≤ = T .Replace(d, alignedSizes[d]); 26 SortedRTiles.Insert({T ‚Ä≤ , DataReuseScore(T ‚Ä≤ )}); 27 Return SortedRTiles;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ROLLER's rProgram constructing algorithm for a single core (e.g., an SM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The interface of ROLLER's hardware abstraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Compilation time for each operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Kernel time for MatMul operator with different sizes of M in BERT-Large model, K=1024, N=4096.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Kernel time for Conv2d operator with different batch of N, where C=1024, H=14, F=2048, K=1, S=2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Compilation time for both MatMul and Conv2d operator with different batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :Figure 16 :</head><label>1516</label><figDesc>Figure 15: Matmul kernel time on TensorCore.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Performance for operators with irregular shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in both input and output tensors, ROLLER will transform it into the tensor // compute interface int Load(T* src , rTile st , T* dst , rTile dt ); int Store(T* dst , rTile dt , T* src , rTile st ); int Compute( TensorExpr e , rTile t , T ** args );</figDesc><table><row><cell>Spec GetDeviceSpec (); // Spec query interface</cell></row><row><cell>// interfaces of the micro-performance model</cell></row><row><cell>size_t MemFootprint ( rTile t );</cell></row><row><cell>size_t MemTraffic ( rTile t );</cell></row><row><cell>double MaxComputePerf ( TensorExpr expr );</cell></row><row><cell>double MemPerf ( rTile t );</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>A subset of operator configurations in our benchmark.</figDesc><table><row><cell>Operator</cell><cell>Configuration</cell><cell>Note</cell></row><row><cell>MatMul</cell><cell>M=65536,K=2,N=1024</cell><cell>M0</cell></row><row><cell>MatMul</cell><cell>M=128,K=4032,N=1000</cell><cell>M1</cell></row><row><cell>MatMul</cell><cell>M=65536,K=1024,N=4096</cell><cell>M2</cell></row><row><cell>Conv2D</cell><cell cols="2">D=(128,128,28,28), K=(128,128,3,3),S=1 C0</cell></row><row><cell>Conv2D</cell><cell cols="2">D=(128,128,58,58), K=(128,128,3,3),S=2 C1</cell></row><row><cell>Conv2D</cell><cell cols="2">D=(128,256,30,30), K=(256,256,3,3),S=2 C2</cell></row><row><cell cols="2">DepthwiseConv D=(128,84,83,83), K=(84,84,5,5),S=2</cell><cell>D0</cell></row><row><cell cols="2">DepthwiseConv D=(128,42,83,83), K=(42,42,5,5),S=1</cell><cell>D1</cell></row><row><cell cols="2">DepthwiseConv D=(128,84,21,21), K=(336,336,1,1),S=1</cell><cell>D2</cell></row><row><cell>Element(Relu)</cell><cell>I=(128,1008,42,42)</cell><cell>E0</cell></row><row><cell>Element(Relu)</cell><cell>I=(128,256,14,14)</cell><cell>E1</cell></row><row><cell>Element(Relu)</cell><cell>I=(128,1024,14,14)</cell><cell>E2</cell></row><row><cell>Avgpool</cell><cell>D=(128,168,83,83),K=1,S=2,VALID</cell><cell>P0</cell></row><row><cell>Avgpool</cell><cell>D=(128,617,21,21),K=3,S=2,SAME</cell><cell>P1</cell></row><row><cell>Avgpool</cell><cell>D=(128,42,83,83),K=3,S=1,SAME</cell><cell>P2</cell></row><row><cell>ReduceMean</cell><cell>I=(128, 512, 1024), axis=[2]</cell><cell>R0</cell></row><row><cell>ReduceMean</cell><cell>I=(65536, 1024),axis=[1]</cell><cell>R1</cell></row><row><cell>ReduceMean</cell><cell>I=(128, 4032, 11, 11), axis=[2,3]</cell><cell>R2</cell></row><row><cell cols="3">Experimental setup. ROLLER is evaluated on four types of</cell></row><row><cell cols="3">servers equipped with different accelerators. The CUDA GPU</cell></row><row><cell cols="3">evaluations use two types of servers: an Azure NC24s_v3 VM</cell></row><row><cell cols="3">equipped with Intel Xeon E5-2690v4 CPUs and 4 NVIDIA</cell></row><row><cell cols="3">Tesla V100 (16GB) GPUs and an Azure NC24_v1 VM with</cell></row><row><cell cols="3">24 Intel(R) Xeon(R) CPU E5-2690v3 CPUs and 4 NVIDIA</cell></row><row><cell cols="3">Tesla K80 GPUs. Both running on Ubuntu 16.04 with CUDA</cell></row><row><cell cols="3">10.2 and cuDNN 7.6.5. The AMD ROCm GPU evaluations</cell></row><row><cell cols="3">use a server equipped with Intel Xeon CPU E5-2640 v4 CPU</cell></row><row><cell cols="3">and 4 AMD Radeon Instinct MI50 (16GB) GPUs, installed</cell></row><row><cell cols="3">with Ubuntu 18.04 and ROCm 4.0.1 [1]. The IPU evalua-</cell></row><row><cell cols="3">tions use an Azure ND40s_v3 VM equipped with Intel Xeon</cell></row><row><cell cols="3">Platinum 8168 CPUs and 16 IPUs with Poplar-sdk 1.0.</cell></row></table><note>all other operators take only around 13 seconds to compile. 2) ROLLER matches the state-of-the-art performance of vendor libraries and other compilers on a wide range of operators. It even outperforms others for more than 50% of operators. 3) For operators with smaller sizes and irregular shapes, ROLLER's results are sub-optimal because of the difficulty in aligning with the hardware. However, their kernel execution time is usually small (around or below 1ms). 4) We have conducted the most extensive evaluations (119 ops in total) covering different operator types over different accelerators.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Kernel performance. We further study how close the performance of ROLLER generated kernels can approach the optimal. Since ROLLER's data pipeline model can naturally identify the bottleneck layer, e.g., DRAM, shared memory, orUSENIX Association16th USENIX Symposium on Operating Systems Design and Implementation 243</figDesc><table><row><cell cols="2">Resource utilization 60-70%</cell><cell cols="2">70-80% 80-90%</cell><cell>90-100%</cell></row><row><cell>Operator #</cell><cell>6</cell><cell>13</cell><cell>22</cell><cell>78</cell></row><row><cell>Percentage</cell><cell>5%</cell><cell>11%</cell><cell>18%</cell><cell>66%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>The distribution of resource utilization at the saturated layer for different kernels.</figDesc><table><row><cell cols="2">Baseline MemAlignn</cell><cell cols="2">EUAlign ShapeAlign</cell><cell>BankAlign</cell></row><row><cell>1.0x</cell><cell>1.42x</cell><cell>1.88x</cell><cell>1.92x</cell><cell>1.94x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Average accumulated performance improvement with different alignment optimization.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>End-to-end model execution time (in milliseconds) and compilation time on V100 GPUs.</figDesc><table><row><cell></cell><cell>TF(CudaLib)</cell><cell>TVM</cell><cell>Ansor</cell></row><row><cell>Better Performance</cell><cell>82.4%</cell><cell>65.5%</cell><cell>71.4%</cell></row><row><cell>Perf. within 5%</cell><cell>82.4%</cell><cell>67.2%</cell><cell>75.6%</cell></row><row><cell>Perf. within 10%</cell><cell>83.2%</cell><cell>73.1%</cell><cell>79.0%</cell></row><row><cell>Perf. within 50%</cell><cell>99.2%</cell><cell>93.3%</cell><cell>94.1%</cell></row><row><cell>Perf. within 90%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>100.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The percentage of better and comparable performant operators on NVIDIA K80 GPUs. a legitimate program for BERT-Large models. Thus, for this case, we use TVM to compile the model. Note that, the performance of TVM for BERT-Large is about 2.6√ó slower than Rammer+TVM, as the default layout of the dense operator in TVM (i.e., NT) is different from that in Rammer (i.e., NN).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows the percentage of</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The percentage of better and comparable performant operators on AMD ROCm MI50 GPUs.</figDesc><table><row><cell>Kernel time (ms)</cell><cell>0.01 0.1 1 10</cell><cell>PopART Ansor</cell><cell>Roller-top10</cell></row><row><cell></cell><cell></cell><cell cols="2">M0 M1 M2 M3 M4 C0</cell><cell>C1</cell><cell>C2</cell><cell>C3</cell></row><row><cell cols="5">Figure 19: Operator performance on Graphcore IPU (y-axis</cell></row><row><cell cols="3">in log-scale).</cell><cell></cell></row><row><cell cols="5">differences or 1.1√ó slow down) ROLLER generates for our</cell></row><row><cell cols="5">operator benchmarks. Compared to CUDA libraries, TVM,</cell></row><row><cell cols="5">and Ansor, ROLLER produces 82.4%, 65.5% and 71.4% bet-</cell></row><row><cell cols="5">ter kernels for the whole operator benchmark. The percent-</cell></row><row><cell cols="5">age is relatively low for TVM mainly because the manual-</cell></row><row><cell cols="5">crafted element-wise kernel templates in TVM are already</cell></row><row><cell cols="5">highly-optimized. Finally, the average compilation time for</cell></row><row><cell cols="5">all operators is 0.65 hours for TVM and 0.95 hours for Ansor</cell></row><row><cell cols="5">respectively. In contrast, ROLLER's average compilation time</cell></row><row><cell cols="5">is only 5.24 milliseconds for top-1 kernel and 12.3 seconds</cell></row><row><cell cols="3">for top-10 kernel.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>, PopART) provided by Graphcore and Ansor. Since TVM and Ansor do not have Graphcore backends, we use a modified version of Ansor in this experiment. As it shows, ROLLER can generate faster kernels than PopART for all operators, with an average of 3.1√ó and up to 9.2√ó speedup. Even comparing to Ansor, ROLLER can still construct comparable or even better kernels in most of operators, i.e., 2.9% average improvement. Note that Ansor still requires hours of tuning for each operator, as the device compiler on IPUs could take up to minutes to compile a program. However, ROLLER usually produce good kernels from the top-10 constructed candidates in several minutes. This time is mainly bottle-necked by the less-matured device compiler. It also brings more challenges to adopt the ML-based tensor compilers on these devices.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers and our shepherd, Prof.Yufei Ding, for their extensive suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="246">16th USENIX Symposium on Operating Systems Design and Implementation</head><p>USENIX Association</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://docs.nvidia.com/cuda/cublas/index.html" />
		<title level="m">CUDA Basic Linear Algebra Subroutine library</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nvcc</forename><surname>Cuda</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">'</forename><surname>Ipu Programmer</surname></persName>
		</author>
		<author>
			<persName><surname>Guide</surname></persName>
		</author>
		<ptr target="https://www.graphcore.ai/docs/ipu-programmers-guide" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://mlir.llvm.org/" />
		<title level="m">MLIR</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://developer.nvidia.com/cudnn" />
		<title level="m">NVIDIA cuDNN</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="https://github.com/NVIDIA/cutlass" />
		<title level="m">NVIDIA cutlass</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nvidia</forename><surname>Triton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inference</forename><surname>Server</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/nvidia-triton-inference-server" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://onnx.ai/" />
		<title level="m">ONNX</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://www.tensorflow.org/xla" />
		<title level="m">XLA</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<ptr target="https://www.amd.com/en/products/professional-graphics/instinct-mi50" />
		<title level="m">AMD Radeon Instinct‚Ñ¢ MI50 Accelerator</title>
				<imprint>
			<date type="published" when="2018-11">2018 Nov.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Mart√≠n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<title level="s">USENIX Association</title>
		<meeting><address><addrLine>GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Ben Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdurrahman</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</title>
				<meeting>the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TVM: An automated endto-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
				<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tensor processing primitives: A programming abstraction for efficiency and portability in deep learning workloads</title>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Georganas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dhiraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasikanth</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menachem</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Adelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhisek</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasimuddin</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanchit</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanarayan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barukh</forename><surname>Pabst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName><surname>Heinecke</surname></persName>
		</author>
		<idno>CoRR, abs/2104.05755</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">November 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Staiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><forename type="middle">P</forename><surname>Scarpazza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06826</idno>
		<title level="m">Dissecting the nvidia volta gpu architecture via microbenchmarking</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><forename type="middle">Paolo</forename><surname>Scarpazza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03413</idno>
		<title level="m">Dissecting the graphcore ipu architecture via microbenchmarking</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4013" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analytical characterization and design space exploration for optimization of cnns</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sukumaran-Rajam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2021</title>
				<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2021<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="928" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Romou: Rapidly generate high-performance tensor kernels for mobile gpus</title>
		<author>
			<persName><forename type="first">Rendong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jicheng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manni</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th Annual International Conference On Mobile Computing And Networking</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-02">MobiCom 2022. February 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rammer: Enabling holistic deep learning compiler optimizations with rtasks</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)</title>
				<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="881" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr√©do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;13</title>
				<meeting>the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="10" to="19" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno>CoRR, abs/1802.04730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Drew: Efficient winograd cnn inference with deep reuse</title>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022, WWW &apos;22</title>
				<meeting>the ACM Web Conference 2022, WWW &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1807" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Poclib: A high-performance framework for enabling near orthogonal processing on compression</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="459" to="475" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Akg: Automatic kernel generation for neural processing units using polyhedral transformations</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021</title>
				<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1233" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)</title>
				<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep-learning model sparsity via tensorwith-sparsity-attribute</title>
		<author>
			<persName><forename type="first">Ningxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanlu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation</title>
				<meeting>the 16th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
