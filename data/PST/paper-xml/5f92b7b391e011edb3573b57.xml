<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XOR QA: Cross-lingual Open-Retrieval Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
							<email>jkasai@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark ♦</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
							<email>kentonl@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
							<email>eunsol@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ron</forename><surname>Paul</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Randal</forename><surname>Paul</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington ♦ Google Research</orgName>
								<orgName type="institution" key="instit2">The University of Texas Austin ♠ Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">XOR QA: Cross-lingual Open-Retrieval Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multilingual question answering tasks typically assume answers exist in the same language as the question. Yet in practice, many languages face both information scarcity-where languages have few reference articles-and information asymmetry-where questions reference concepts from other cultures.</p><p>This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on questions from TYDI QA lacking samelanguage answers.</p><p>Our task formulation, called Cross-lingual Open Retrieval Question Answering (XOR QA), includes 40k information-seeking questions from across 7 diverse non-English languages. Based on this dataset, we introduce three new tasks that involve cross-lingual document retrieval using multi-lingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs. washington.edu/xorqa. 1. Question Translation 2. Answer Retrieval in English 3. Answer Translation ロン・ポールの学部時代 の専攻は？ What did Ron Paul major in during undergraduate? Paul went to Gettysburg College … He graduated with a B.S. degree in Biology in 1957. Q L → Q en Human translation en.wikipedia (Q en , P en )</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information-seeking questions-questions from people who are actually looking for an answerhave been increasingly studied in question answering (QA) research. Fulfilling these information needs has led the research community to look further for answers: beyond paragraphs and articles toward performing open retrieval 1 on large- 1 We use the term open-retrieval to refer to models that can access answer context from large document collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ロン・ポールの学部時代の専攻は？[Japanese]</head><p>(What did Ron Paul major in during undergraduate?)</p><formula xml:id="formula_0">生物学 (Biology)</formula><p>Paul went to Gettysburg College, where he was a member of the Lambda Chi Alpha fraternity. He graduated with a B.S. degree in Biology in 1957.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>高校卒業後はゲティスバーグ大学へ進学。</head><p>(After high school, he went to Gettysburg College.)</p><p>Multilingual document collections (Wikipedias)</p><p>ロン・ポール (ja.wikipedia)</p><p>Ron Paul (en.wikipedia)</p><p>Figure <ref type="figure">1</ref>: Overview of XOR QA. Given a question in L i , the model finds an answer in either English or L i Wikipedia and returns the answer in English or L i . L i is one of 7 typologically diverse languages.</p><p>scale document collections <ref type="bibr" target="#b9">(Chen and Yih, 2020</ref>).</p><p>Yet the bulk of this work has been exclusively on English. In this paper, we bring together for the first time information-seeking questions, openretrieval QA, and multilingual QA to create a multilingual open-retrieval QA dataset that enables cross-lingual answer retrieval. While multilingual open QA would benefit the many speakers of non-English languages, there are several pitfalls to consider in designing such a dataset. First, a multilingual QA dataset should include questions from non-English native speakers to represent real-world applications. Questions in most recent multilingual QA datasets <ref type="bibr" target="#b20">(Lewis et al., 2020b;</ref><ref type="bibr" target="#b1">Artetxe et al., 2020b;</ref><ref type="bibr" target="#b23">Longpre et al., 2020)</ref> are translated from English, which leads to English-centric questions-such as questions about American sports, cultures and politics. Second, it is important to support retriev-</p><p>While the term open-domain is sometimes used in the literature, we avoid this usage due to its double-meaning as "covering topics from many domains." arXiv:2010.11856v1 [cs.CL] 22 Oct 2020 ing answers in languages other than the original language due to information scarcity of lowresource languages <ref type="bibr">(Miniwatts Marketing Group, 2011)</ref>. Moreover, questions strongly related to entities from other cultures are less likely to have answer content in the questioner's language due to cultural bias (information asymmetry, Callahan and Herring, 2011). For example, Fig. <ref type="figure">1</ref> shows that the Japanese Wikipedia article of an American politician, Ron Paul, does not have information about his college degree perhaps because Japanese Wikipedia editors are less interested in the educational background of American politicians.</p><p>In this paper, we introduce the task of crosslingual open retrieval question answering (XOR QA) which aims at answering multilingual questions from non-English native speakers given multilingual resources. To support research in this domain, we construct a dataset (called XOR-TYDI QA) of 40k annotated questions and answers across 7 typologically diverse languages. Questions in our dataset are inherited from TYDI QA <ref type="bibr" target="#b10">(Clark et al., 2020)</ref> and answers are augmented with our annotation process, adding 20k new answer annotations. All questions are written by native speakers, but a large collection of questions are originally unanswerable due to the information scarcity and asymmetry issues.</p><p>2 XOR-TYDI QA is constructed with an annotation pipeline that allows for cross-lingual retrieval from large-scale Wikipedia corpora ( §2). Unanswerable questions in TYDI QA are first translated into English using a professional translation service. Then, annotators find answers to the English query given English Wikipedia using our new model-in-the-loop annotation framework designed to minimize crowdworker annotation errors. Finally, answers are verified and translated back to the target languages.</p><p>Building on the dataset, we introduce three new tasks in the order of increasing complexity ( §2): XOR-RETRIEVE, XOR-ENGLISHSPAN, and XOR-FULL. In XOR-RETRIEVE, a system retrieves paragraphs from English Wikipedia that contain information to answer the question posed in the target language. XOR-ENGLISHSPAN takes one step further and finds a minimal answer span from the retrieved English paragraphs. Finally, XOR-FULL expects a system to generate an answer end to end in the target language by consulting both English and the target language's Wikipedia.</p><p>We provide baselines that extend state-of-theart open-retrieval QA systems <ref type="bibr">(Asai et al., 2020;</ref><ref type="bibr" target="#b13">Karpukhin et al., 2020)</ref> to our multilingual retrieval setting. Our best baseline achieves 17.1 F1 on XOR-FULL. This result indicates that XOR-TYDI QA poses unique challenges which we need to tackle to build a real-world open-retrieval QA system for diverse languages. We further provide detailed analysis for several languages to guide future studies. To summarize, our contributions are:</p><p>• We introduce XOR QA, a new task framework that involves cross-lingual retrieval and question answering with three sub-tasks, to overcome the low answerability issues in multilingual open-retrieval QA. • We construct XOR-TYDI QA, a dataset with 40k newly annotated question-answer pairs across 7 languages built upon TYDI QA. • We introduce several strong baseline systems with experimental results, showing that there is much room for improvement in this area. • We demonstrate how XOR-TYDI QA serves to evaluate three crucial tasks: query and answer translation, multilingual machine reading, and cross-lingual retrieval.</p><p>2 The XOR-TYDI QA Dataset</p><p>Our XOR-TYDI QA dataset comprises questions inherited from TYDI QA <ref type="bibr" target="#b10">(Clark et al., 2020)</ref>, and answers augmented with our annotation process across 7 typologically diverse languages. Here we focus on cross-lingual retrieval from English Wikipedia because in our preliminary investigation we were able to find answers to a majority of the questions from resource-rich English Wikipedia, and experienced annotators were readily available via crowdsourcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">XOR-TYDI QA Collection</head><p>Our annotation pipeline consists of four steps: 1) collection of realistic questions that require cross-lingual references by annotating questions from TYDI QA without a same-language answer ( §2.1.1); 2) question translation from a target language to the pivot language of English where the missing information may exist ( §2.1.2); 3) answer span selection in the pivot language given a set of candidate documents ( §2.1.3); and 4) answer verification and translation from the pivot language back to the original language ( §2.1.4). Fig. <ref type="figure" target="#fig_0">2</ref> shows an overview of the annotation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Question Collection</head><p>We randomly sample 5000 questions without any passage answer annotations (unanswerable questions) from the TYDI QA train data, and split it into training (4,500) and development (500) sets.</p><p>We use the development data from TYDI QA as our test data, since the TYDI QA's original test data is not publicly available.</p><p>3</p><p>We chose 7 languages with varying amounts of Wikipedia data out of the 11 languages: Arabic, Bengali, Finnish, Japanese, Korean, Russian and Telugu. We remove Thai, Swahili, and Indonesian in TYDI QA based on the cost and availability of translators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Question Translation</head><p>We use a professional translation service, Gengo, 4 to translate all collected questions into English. Since named entities are crucial for QA, we instruct translators to carefully translate entities by searching for common English translations from English Wikipedia or other external sources. To assess the quality of these translations, we performed manual assessment by native speakers on 50 translation samples, finding that more than 95% are correct. Note that while these translations are a part of the annotation procedure (due to the inherently cross-lingual nature of this task), these trans-lations are not provided to models during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Answer Retrieval in English</head><p>We use Amazon Mechanical Turk<ref type="foot" target="#foot_3">5</ref> to retrieve answers to translated English questions given English Wikipedia articles. Annotators are instructed to select passage answers (gold paragraphs) and minimal answer spans.</p><p>Open-Retrieval Annotation Desiderata Openretrieval QA annotation comes with unique challenges. In article-oriented QA such as SQuAD <ref type="bibr">(Rajpurkar et al., 2016)</ref>, all labels are with regard to a single document and a single human can indeed read the whole document. In openretrieval QA, answers can be retrieved from millions of documents. Because exhaustively reading so much content is impossible for humans, the notion of "human performance" must be reconsidered in this context. This is why we only evaluate questions having answers in the openretrieval setting, discarding those where no answer was found-it is difficult to prove an answer does not exist in the millions of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limits of Traditional Annotation</head><p>In addition to fundamental problems of information scarcity and asymmetry in multilingual QA, questions can be labeled as unanswerable simply because of annotation errors. Annotation procedures for information-seeking QA data usually have each annotator read a single Wikipedia article retrieved by a search engine, labeling the correct answer span or labeling the question as not answered by the article <ref type="bibr" target="#b17">(Kwiatkowski et al., 2019;</ref><ref type="bibr" target="#b10">Clark et al., 2020)</ref>. In this procedure, the answer coverage is underestimated when the search engine fails to re-trieve relevant articles (retrieval miss) or the annotator overlooks answer content from the selected article (annotation miss) <ref type="bibr" target="#b2">(Asai and Choi, 2020)</ref>. Importantly, these two types of annotation errors present a tradeoff: if we retrieve many articles, retrieval misses will be reduced at the expense of annotation misses because annotators have to find answer context among many candidate articles. An annotation procedure that misses too many answers will lead to an artificially small dataset.</p><p>Collaborative model-in-the-loop. We introduce a collaborative model-in-the-loop framework that uses Google Search and a state-of-the-art paragraph ranker to find the middle ground. We first run Google search to retrieve as many as 10 Wikipedia articles, which translates to 387 paragraphs on average. We score them with Path Retriever <ref type="bibr">(Asai et al., 2020)</ref> and present the five highest scoring paragraphs. Annotators are asked to read these five paragraphs first; if they cannot find any answer content from the initial set, they are asked to skim the rest of the paragraphs, where the Wikipedia section headings help to quickly guide their reading. We found that about 70% of the answers from the 5 paragraphs and 30% from the rest of the paragraphs in the top 10 articles. This means that while our paragraph ranking was effective, the annotators did not fully rely on it, thereby mitigating the influence of the passage ranking model on the dataset. See Appendix §A.1 for annotation interface details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Answer Verification and Translation</head><p>Answer Verification We trained undergraduate students who are native English speakers to verify the annotated paragraphs and short answers. Only 8% of the answers were marked as incorrect through the verification phase and were later corrected by our pool of high-quality crowd workers who yielded less than 1% annotation error.</p><p>Answer Translation We again use Gengo to translate answers from English back to the original languages. We give translators further instructions to normalize answers such that they are consistent with answers in TYDI QA. For example, some languages use their own unique set of numerals rather than Arabic numerals to represent numeric answers (e.g., Bengali numerals, Chinese numerals in Japanese text). The details of the answer translation process are described in Appendix §A.3. Cross-lingual data comes from our re-annotated questions that did not originally have samelanguage answers in TYDI QA. In-language data are taken directly from answerable questions in TYDI QA. During evaluation, we exclude the questions for which we cannot find any minimal answer annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The XOR-TYDI QA corpus</head><p>Dataset Statistics Table <ref type="table" target="#tab_0">1</ref> shows the percentages of the questions annotated with short answers in the original TYDI QA and our XOR-TYDI QA, and Table <ref type="table" target="#tab_1">2</ref> shows statistics of our dataset. As shown in Table <ref type="table" target="#tab_0">1</ref>, cross-lingual retrieval significantly increases the answer coverage in all languages by up to 45% absolute (Korean), and consequently we found answers for more than 50% of the original information-seeking questions in 6 out of 7 languages,<ref type="foot" target="#foot_4">6</ref> confirming the effectiveness of searching multilingual document collections to improve the answer coverage of a multilingual QA system. Detailed statistics of the number of long Qualitative examples Table <ref type="table" target="#tab_2">3</ref> shows some example questions in our dataset to showcase that finding relevant articles from multilingual document collections is important to answer questions asked by users with diverse linguistic and cultural backgrounds. The first question is unanswerable in Korean Wikipedia, but there is a clear description about who was the prime minister of France at the time in the English Wikipedia. The second example shows how English Wikipedia contains rich information about a target language-specific topic (e.g., economy in Krasnodar, a city in Russia). Those examples show the effectiveness of searching for answers in another language with more abundant knowledge sources. The last question in Table <ref type="table" target="#tab_2">3</ref>, on the other hand, shows an example where only the Wikipedia of the target language can provide the answer. XOR QA allows for both retrieval paths. datasets such as MLQA <ref type="bibr" target="#b32">(Talmor and Berant, 2019)</ref> and MKQA <ref type="bibr" target="#b23">(Longpre et al., 2020)</ref>. Secondly, our dataset requires cross-lingual retrieval unlike other multilingual datasets such as TYDI QA or XQuAD <ref type="bibr" target="#b1">(Artetxe et al., 2020b)</ref>, which focus on same-language QA. Lastly, questions in XOR-TYDI QA require open retrieval from Wikipedia, whereas MLQA-R and XQuAD-R <ref type="bibr" target="#b28">(Roy et al., 2020)</ref> limit the search space to matching each question with one of the predetermined 2k and 1k answer sentences.</p><formula xml:id="formula_1">L Original Question Passage Answer Minimal Answer Final Answer Ko 1993ᄂ ᅧ ᆫ ᄑ ᅳᄅ ᅡ ᆼᄉ ᅳ ᄎ ᅩ ᆼᄅ ᅵᄂ ᅳ ᆫ ᄂ ᅮ ᄀ ᅮᄋ ᅵ ᆫᄀ ᅡᄋ ᅭ? (Who</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with other datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">XOR QA Tasks and Baselines</head><p>We introduce three new tasks (Fig. <ref type="figure" target="#fig_1">3</ref>): XOR-RETRIEVE, XOR-ENGLISHSPAN, and XOR-FULL with our newly collected XOR-TYDI QA  dataset. The first two tasks pose a novel challenge of large-scale cross-lingual retrieval and question answering for information-seeking questions. The final task adds several more challenges (e.g., finding evidence without knowing in advance in which languages we can find answers, presenting answers in the users' languages) that are crucial when we deploy a QA system in real-world applications. For each task, we construct strong baselines. We denote target languages as L i and their collection as</p><formula xml:id="formula_2">L = {L 1 , L 2 , . . . , L i , . . . , L N },</formula><p>where N is the number of the target languages.</p><p>We denote the English Wikipedia collection as W eng , the Wikipedia collection in each target language L i as W i , and the whole collection as W = {W 1 , W 2 , . . . , W i , . . . , W N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">XOR-RETRIEVE: Cross-lingual Paragraph Retrieval</head><p>Task Given a question in L i and English Wikipedia W eng , the task is to retrieve English paragraphs that can answer the question. Finding evidence paragraphs from large-scale document collections like Wikipedia is a challenging task, especially when a query and documents are in different languages, and systems cannot perform simple lexical matching.</p><p>Evaluation Different open-retrieval QA models use different units for retrieval: DPR retrieves 100-token segments while Path Retriever and BM25 select paragraphs. To make fair comparisons across various models, we measure the recall by computing the fraction of the questions for which the minimal answer is contained in the top n tokens selected. We evaluate with n = 2k, 5k: R@2kt and R@5kt (kilo-tokens).</p><p>Translate Baselines Here we first translate queries into English, and then paragraphs are retrieved in a monolingual way. For query translation, we train transformer machine translation models on publicly available corpora for easy replication. Additionally, we run Google's online machine translation service (GMT). Note that online translators are not completely reproducible as these production systems get constantly updated; Multilingual Baselines Alternatively, we can directly apply a multilingual pretrained model to retrieve paragraphs. We apply DPR to the multilingual setting by finetuning with multilingual BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">XOR-ENGLISHSPAN: L-to-English</head><p>Open-Retrieval QA Task Given a question in L i and English Wikipedia W eng , a system retrieves paragraphs from W eng and extracts an answer. This setting closely mirrors existing open-retrieval QA tasks <ref type="bibr" target="#b8">(Chen et al., 2017)</ref>, except that the query is not written in English. This task involves challenging cross-lingual retrieval and question answering on the target language L i query and En-glish evidence paragraphs.</p><p>Evaluation We use Exact Match (EM) and F1 over the annotated answer's token set as in SQuAD <ref type="bibr">(Rajpurkar et al., 2016)</ref>, following previous practice in English open-retrieval QA.</p><p>Baselines Our pipeline uses a machine reading model to find a minimal span that answers the question given paragraphs selected from the previous XOR-RETRIEVE step. In particular, for the translate baselines, we use an approach similar to state-of-the-art models <ref type="bibr">(Asai et al., 2020;</ref><ref type="bibr" target="#b13">Karpukhin et al., 2020)</ref> that jointly predict a span and a relevance score of each paragraph to the question. For the multilingual baseline where queries are not automatically translated during evaluation, we build a reader model with multilingual BERT and finetune it with data from Natural Questions and XOR-TYDI QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">XOR-FULL: Round Trip</head><p>Task Given a question q in target language L i and Wikipedia in both English and L i (W eng and W i ), a system is required to generate an answer in L i . In this setting, a system does not know a priori in which language we can find information that the user is seeking; this task is closest to realworld applications.</p><p>Evaluation We use F1 and EM scores as well as token-level BLEU <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref> scores over a ground-truth token set. The same tokenizer is applied to ground-truth and predicted answers to compute token-level F1 and BLEU.</p><p>7</p><p>Baselines We apply retrieval (BM25) and machine reading models (DPR) in the target language. We finally translate selected answer spans into the target language if they are extracted from English articles. Similarly to §3.1, we use our transformer models or GMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present results from the baselines discussed above. We find that the three XOR QA tasks present challenges even for the strong models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>For training, we first finetune the retrieval and machine reading models with Natural Questions 7 We use the Moses tokenizer <ref type="bibr" target="#b14">(Koehn et al., 2007)</ref> for all languages except we apply MeCab <ref type="bibr" target="#b15">(Kudo, 2006)</ref> to Japanese.</p><p>data <ref type="bibr" target="#b17">(Kwiatkowski et al., 2019)</ref> and then further finetune on our XOR-TYDI QA data, starting from the bert-base-uncased models. For the BM25 retrieval baseline, we use ElasticSearch<ref type="foot" target="#foot_5">8</ref> to store and search documents using BM25 similarities.</p><p>For both Path Retriever and DPR, we run the official open-source code. For our MT systems, we train base-sized autoregressive transformers <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> on parallel corpora from OPUS <ref type="bibr" target="#b33">(Tiedemann and Nygaard, 2004)</ref>, MultiUN <ref type="bibr" target="#b37">(Ziemski et al., 2016)</ref>, and WMT19 <ref type="bibr" target="#b6">(Barrault et al., 2019)</ref>. All data are encoded into subwords by BPE <ref type="bibr" target="#b30">(Sennrich et al., 2016)</ref> or SentencePiece <ref type="bibr" target="#b16">(Kudo and Richardson, 2018)</ref>. We use the fairseq library <ref type="bibr" target="#b25">(Ott et al., 2019)</ref> Additional experimental details and full lists of hyperparameteres are available in Appendix §B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">XOR-RETRIEVE Experiments</head><p>Table <ref type="table">5</ref> shows the R@5kt (as defined in §3.1) for different retrieval and query translation systems. 9</p><p>The table also reports the performance with the human English translations of the questions used during the dataset collection as an upper bound of translate baselines.</p><p>The best R@5kt macro-averaged over the 7 languages comes from running DPR on human translations: 70.3. Machine translation systems achieve averages of 65.5 (GMT) and 48.5 (our MT) again with DPR. The discrepancy between human and machine translation suggests that even state-ofthe-art translation systems struggle to translate questions precisely enough to retrieve an evidence paragraph. The translation-based approaches generally outperform the multilingual approach apart from Telugu. The Telugu translation model suffers from small training data (114k sentences), which implies that a multilingual approach performs better when we do not have sufficient translation training data.</p><p>The term-based retrieval of BM25 substantially underperforms both DPR and Path Retriever across the board. DPR generally achieves similar performance, if not better, compared to Path Retriever despite the fact that Path Retriever was used in our annotation ( §2.1.3). Since we found that these patterns continued in all the following  <ref type="table">5</ref>: R@5kt ( §3.1) on the test data in the XOR-RETRIEVE setting. PATH and BM denote Path Retriever and BM25 respectively. The rightmost column is a multilingual approach that bypasses the query translation step ( §3.1). experiments, we will only report results with DPR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">XOR-ENGLISHSPAN Experiments</head><p>Table <ref type="table">6</ref> shows the performance of the baseline models for XOR-ENGLISHSPAN. The average macro F1 score when the query is translated by a human translator is 31.0, substantially higher than that of machine translation-based models: 27.1 and 18.9 F1 points for GMT and our MT respectively. This suggests that errors in automatic query translation affect later layers in the pipeline. Nonetheless, the multilingual approach consistently underperforms translation-based methods, similarly to XOR-RETRIEVE. Similar to the original TYDI QA dataset, the performance on XOR-ENGLISHSPAN varies across languages, which can be partially explained by the differing sets of questions <ref type="bibr" target="#b10">(Clark et al., 2020)</ref>; 10 the best baseline achieves 31.3 in Arabic compared to 19.3 F1 points in Japanese. This gap may come from differences in question difficulty as well as how the multilingual model represent each language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">XOR-FULL Experiments</head><p>Results Table <ref type="table" target="#tab_7">7</ref> presents results on the XOR-FULL task. For this task, each model should output the answer in the original query language L i , and can use both English W eng and the target language Wikipedia W i corpora. We first describe 10 We found that the questions translated from Bengali and Arabic have high overlap with Google Translate results, possibly because human translators use it as a post-editing backbone, but native speakers confirmed the final translation results are still of high quality.</p><p>Human GMT Our MT Multi F1 EM F1 EM F1 EM F1 EM <ref type="bibr">Ar 34.8 25.5 31.3 23.3 24.0 19.7 15.2 11.1 Bn 38.3 31.3 38.8 30.5 22.0 15.2 12.3 9.2 Fi 30.3 21.8 24.2 17.9 24.3 17.9 13.7 9.7 Ja 24.9 19.1 19.3 14.4 14.3 10.2 11.0 8.9 Ko 30.1 22.3 27.3 20.0 17.1 12.2 10.0 6.7 Ru 28.2 20.8 24.1 17.4 14.3 10.6 11.8 8.7 Te 30.8 22.5 24.8 18.2 16.2 12.2 11.9 8.5</ref> Av. 31.0 23.3 27.1 20.2 18.9 14.0 12.3 9.0 Table 6: Performance on XOR-ENGLISHSPAN. The rightmost Multi. section is a multilingual approach without query translation ( §3.1).</p><p>the full set up which uses both document collections (first row block), and then describe single Wikipedia settings, which use only W eng or only W i as the document collection.</p><p>The pipelined model, which applies GMT for query and answer translation and Google Search (GS) and DPR retrieval from W eng and W i yields the best average performance: 17.1 F1, 10.1 EM, and 15.3 BLEU points. As an oracle experiment, this suggests that systems like GMT and Google Search, which are typically trained on large data, are very effective. However, we encourage the community to experiment on top of open systems such that all experimental details can be fully reported and understood. Replacing GMT with our MT (second row) results in a large performance drop in Bengali (8.0 vs. 16.7 F1 points) and Telugu (2.8 vs. 9.5). Further replacing GS with BM25 retrieval in the target languages (third row) causes a large performance drop in all languages (e.g., 9.4 vs. 19.1 in Korean). Consistent with the previous tasks, the multilingual approach shown in the forth row underperforms the translation-based counterpart (13.1 vs. 17.1 F1 points on average). These results illustrate the complex, multi-dimensional challenges posed by XOR-FULL task.</p><p>Single Language Wikipedia Ablations To assess our models' ability to benefit from multilingual collections, we explore English Wiki W eng only and target language Wiki W i only settings. The bottom section of Table <ref type="table" target="#tab_7">7</ref> shows the results. When we apply GMT for query and answer translation and DPR for retrieval, we observe a degraded performance compared to the best pipeline configuration that uses both W eng and W i : 15.6 vs. 17.1 F1 points on average. Similarly, the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we conduct a detailed analysis to understand what remains as challenges for stateof-the-art models, with a focus on the effect of translation models as well as a subset of the questions where retrieval is particularly hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of translation performance on overall QA results</head><p>Translating queries into the pivot language of English is a competitive approach for cross-lingual modeling, and a powerful machine translation system may boost the final performance. On the other hand, standard metrics such as BLEU may not always reflect an MT system's competitiveness in XOR QA. In Table <ref type="table" target="#tab_8">8</ref>, we compare the BLEU scores and the final QA F1 performance on XOR-ENGLISHSPAN of the translate-based baseline with three different MT systems: GMT, Our MT, and Helsinki (Tiedemann and Thottingal, 2020). It is noteworthy that high BLEU scores do not always lead to better QA performance. In Bengali, while Helsinki achieves considerably better BLEU scores than our MT (33.0 vs. 30.8), our MT shows 1.4 points better F1 scores. In Japanese, Helsinki and our MT show almost equivalent BLEU scores while our MT systems yields more than 2 points higher F1 scores. See Appendix §C.1 for an example of translation errors that caused final QA errors. Those results suggest that the BLEU score is not always indicative of the final XOR QA performance and that evaluating MT performance in the context of XOR QA would be important for further improvements of multilingual systems in the real world. This finding is consistent with recent work <ref type="bibr" target="#b31">(Sun et al., 2020)</ref> that found cross-lingual information retrieval with synthetic data is better correlated with human judgements than BLEU.</p><p>Per-difficulty retrieval performance In our collaborative annotation framework, we first show the paragraphs selected by the BERT retriever and annotators read additional paragraphs when those pre-selected paragraphs do not include sufficient information to answer given questions ( §2.1.3). We split our data by difficulty, i.e., whether or not a gold paragraph is selected by the BERT retriever used during annotation. As shown in Table <ref type="table" target="#tab_9">9</ref>, there is a large performance gap between the easy and hard subset in all models, suggesting that the questions from the hard subset are clearly more challenging than the ones from the easy subset. We found that in the hard subset, the gold paragraphs often require some reasoning and include limited lexical overlap with the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Multilingual QA Recently, much effort has been made to create non-English QA datasets to overcome the data scarcity in non-English languages. We already discussed primary differences between our XOR-TYDI QA and existing multilingual QA datasets in §2.2. MLQA <ref type="bibr" target="#b19">(Lewis et al., 2020a)</ref> and XQuAD <ref type="bibr" target="#b1">(Artetxe et al., 2020b)</ref> were created by translating questions from SQuAD <ref type="bibr">(Rajpurkar et al., 2016)</ref> to several languages to provide parallel evaluation datasets to estimate mod- Several other non-English reading comprehension datasets have been created in a similar way <ref type="bibr" target="#b21">(Lim et al., 2019;</ref><ref type="bibr" target="#b3">Asai et al., 2018)</ref>. Roy et al.</p><p>(2020) introduced a language-agnostic questionsentence matching task, LAReQA, and created two benchmarks from XQuAD and MLQA datasets (XQuAD-R, MLQA-R). <ref type="bibr" target="#b23">Longpre et al. (2020)</ref> introduced MKQA, an open-retrieval QA parallel evaluation dataset, by translating 10k questions from Natural Questions into diverse languages. However, as recent work <ref type="bibr" target="#b0">(Artetxe et al., 2020a;</ref><ref type="bibr" target="#b10">Clark et al., 2020)</ref> argues, those datasets contain translation artifacts, and the questions may diverge from native speakers' actual interests. <ref type="bibr" target="#b22">Liu et al. (2019)</ref> developed a template-based cloze task, leading to different data distributions from realistic questions with a great degree of lexical overlap between questions and reference paragraphs <ref type="bibr" target="#b18">(Lee et al., 2019)</ref>.</p><p>XOR-TYDI QA provides train and evaluation datasets to foster building a multilingual QA system that is robust to diverse linguistic and cultural backgrounds. All of our questions are information-seeking questions written by native speakers, thereby reducing exploitable artifacts. Further, none of the prior multilingual QA work that involves retrieval from large scale document collections (e.g., Wikipedia) addresses the challenges of cross-lingual open-retrieval. MKQA marks examples that cannot be answered by knowledge sources in a single language (e.g., English) as "unanswerable," while we find answers for those questions using another language's Wikipedia, demonstrating the benefit of largescale multilingual knowledge sources.</p><p>Cross-lingual Information Retrieval Crosslingual Information Retrieval (CLIR) is the task of retrieving relevant documents when the document collection is in a different language from the query language <ref type="bibr" target="#b12">(Hull and Grefenstette, 1996;</ref><ref type="bibr" target="#b5">Ballesteros and Croft, 1996)</ref>. The retrieval component in XOR QA is closely related to CLIR, but differs in several critical ways. Firstly, since the end goal of XOR QA is question answering, XOR QA queries always take question forms rather than key words for search. Further, while CLIR typically retrieves documents from a single (low-resource) language <ref type="bibr" target="#b36">(Zhang et al., 2019)</ref>, XOR QA considers documents from both English and the query language. In many real-world applications, we do not know a priori in which language we can find information that the user is seeking for. Lastly, our document collection is orders of magnitude bigger than typical CLIR benchmarks <ref type="bibr" target="#b29">(Sasaki et al., 2018;</ref><ref type="bibr" target="#b36">Zhang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented the task of XOR QA, in which a system retrieves and reads documents across languages to answer non-English informationseeking questions. We introduce a new largescale XOR QA dataset, XOR-TYDI QA, with 40k newly annotated open-retrieval questions that cover seven typologically diverse languages. Our experiments showed that XOR-TYDI QA is a challenging benchmark that can benefit from further effort in the research community. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Quality Control</head><p>Quality control for question translation We first ask Gengo translators to translate 20 sample questions from each language into English, and compare the translated results with the ones translated by bilingual speakers in the target language and English we recruited independently. Once we confirm that the translation results are generally consistent with the ones translated by our bilingual speakers, we request the full translation. In our preliminary experiments, we found that some translators heavily rely on open-source machine translation services, and failed to even correct errors from those tools. We closely collaborated with Gengo and filtered out those erroneous translators from our pool.</p><p>Quality control for QA annotation To control the QA annotation quality, we recruit workers with a high approval rate (≥ 96%) located in Englishspeaking countries and conducted a rigorous qualification procedure. In our qualification stage, we post small calibration batches and evaluate the workers' performance by expert judgements and agreement with other annotators. To keep the high quality of annotations, we randomly sample qualified workers weekly and manually monitor their annotations by comparing them with gold annotations by authors. We remove qualifications when we detect too many incorrect annotations (e.g., label a paragraph about a different person as a gold paragraph) and remove the annotations done by those disqualified annotators, which are later reannotated by a qualified worker. Over 200 annotators participated in our calibration tasks. About 40 workers are qualified with 24 actively working on the final dataset. Each HIT contains 5 questions with a reward ranging from 1.5 to 2.5 USD. A qualified annotators generally spends 1-2 min-utes to answer each question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Answer Translation Instructions</head><p>During answer translation, we asked annotators to follow the instructions listed below:</p><p>• Translators need to use metric units by default, instead of imperial units. • If the original answers are expressed in an imperial unit, translators are encouraged to convert them into a metric unit (e.g., Height 5'3" -&gt; Hauteur 160 cm). • When translating proper nouns, translators are asked to use an official translation if it is available in Wikidata; otherwise they are encouraged to transliterate them.</p><p>We also specify some language-specific instructions to make the translated answers format consistent with the ones in the original TYDI QA dataset..</p><p>• For Japanese and Korean, translators do not need to spell out the numbers (e.g., 1954 -&gt; 千九百五十四) as people usually use Arabic numerals. • For Bengali, we expect the numbers will be spelled out in Bengali numerals as Bengali speakers rarely use Arabic numerals. • For Japanese and Korean, translators use appropriate measure words (e.g., 1867ᄂ ᅧ ᆫ, 57歳) if those measure words are commonly added in those languages. • For the languages where the date needs to be expressed in some rigid format, translators need to translate the date following the format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Full Data Statistics</head><p>Seen in Table <ref type="table" target="#tab_10">10</ref> are full data statistics of crosslingual data of XOR-TYDI QA. Among the questions with "Long" answer annotations are some questions without any short answers as in Natural Questions or TYDI QA. We do not include those "Long answer only" examples in our XOR-TYDI QA evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training details</head><p>We describe the details in training our baselines to facilitate easy replication of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Machine Translation Models</head><p>Table <ref type="table" target="#tab_11">11</ref> lists hyperpameters for training our transformer machine translation models. We generally follow the hyperprameters for the base-sized transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref>. For each language direction, all data are encoded into subwords by Moses tokenization <ref type="bibr">(Koehn et al., 2007, for ar, fi, and ru)</ref> and BPE <ref type="bibr" target="#b30">(Sennrich et al., 2016)</ref> or Sen-tencePiece <ref type="bibr">(Kudo and Richardson, 2018, for bn, ja, ko, and te)</ref>. We train a base-sized transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017)</ref> with the fairseq library <ref type="bibr" target="#b25">(Ott et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Retrieval Models</head><p>Training DPR and Path Retriever To train an English DPR and Path Retriever, we first initialize the parameters of the models with the ones trained on Natural Questions Open data, available on their repository. During finetuning on XOR-TYDI QA, we use the human translated questions with the annotated gold paragraph data. The selection of positive and negative examples is crucial to train competitive neural retriever models <ref type="bibr" target="#b13">(Karpukhin et al., 2020)</ref>. To train DPR, we use the original gold paragraphs (long answers) annotated by MTurkers as positive passages, while we randomly sample one negative paragraph per question from the top 5 paragraphs pre-selected by our paragraph re-ranking model in §2.1.3. We also reuse the in-batch negative paragraphs as discussed in <ref type="bibr" target="#b13">(Karpukhin et al., 2020)</ref>. Regarding the training of Path Retriever, we randomly sample top 50 paragraphs from the top 10 articles retrieved for annotations and use them as negative paragraphs.</p><p>We also use the annotated long answers as positive paragraphs. We follow the hyperparameters used in the original papers <ref type="bibr" target="#b13">(Karpukhin et al., 2020;</ref><ref type="bibr">Asai et al., 2020)</ref>. ins 13 for Japanese and Korean respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details of BM25 Retrievers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Machine Reading Models</head><p>The choice of negative and positive examples For the Path Retriever and BM25 baselines' reader, we sample three negative paragraphs per annotated question-gold paragraph pair and train a model that jointly predicts an answer span and relevance score of each paragraph to the question, following <ref type="bibr">Asai et al. (2020)</ref>  <ref type="table" target="#tab_1">12</ref>: R@2kt ( §3.1) on the test data in the XOR-RETRIEVE setting. PATH and BM denote Path Retriever and BM25 respectively. The rightmost column is a multilingual approach that bypasses the query translation step ( §3.1). and we train the reader with 24 negative paragraphs and distant supervision <ref type="bibr" target="#b13">(Karpukhin et al., 2020)</ref>. We use human translated English questions to train English reader models, and use the original questions in L i to train a multilingual reader model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XOR-RETRIEVE results</head><p>We present the R@2kt scores of the retrieval baselines in Table <ref type="table" target="#tab_1">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XOR-FULL results</head><p>We present BLEU and EM scores for XOR-FULL in Tables <ref type="table" target="#tab_4">13 and 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Qualitative Analysis on Translation Errors</head><p>One primary challenge in question translation is precisely translating key words (e.g., entities, year); our MT correctly translates a Japanese</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the annotation process for XOR-TYDI QA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the tasks and baselines. Each dotted outline rectangle represents each sub-task and surround pipeline modules that are related to each sub-task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>nor do we know what model and training data they use. Because of this, we encourage the community to experiment with open MT systems such that system details can be fully understood. For retrieval, we explore three approaches: term-based retrieval (BM25, Robertson and Zaragoza 2009), term-based retrieval followed by neural paragraph ranking (Path Retriever, Asai et al. 2020), and fully end-to-end neural retrieval (DPR, Karpukhin et al. 2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Annotation interface (expanded). The blue highlighted paragraphs are ranked high by the BERT paragraph ranker, and the orange highlighted paragraph is the one clicked by an annotator.</figDesc><graphic url="image-10.png" coords="13,82.91,410.23,196.44,167.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Annotation interface (collapsed). Annotator can choose to read full articles or collapse articles.</figDesc><graphic url="image-11.png" coords="13,318.19,62.81,196.43,178.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Percentage of the questions with short answers (answerable questions) in the original TYDI QA dataset (dev) and XOR-TYDI QA. The third row (Improvement) represents how many of the questions become answerable by searching the English articles in addition to the target language.</figDesc><table><row><cell>%</cell><cell></cell><cell cols="5">Ar Bn Fi Ja Ko Ru Te All</cell></row><row><cell cols="2">TYDI QA</cell><cell cols="5">82 42 57 50 29 69 28 50</cell></row><row><cell cols="7">XOR-TYDI QA 92 82 83 77 68 83 44 72</cell></row><row><cell cols="2">Improvement</cell><cell cols="5">10 40 26 27 39 14 16 22</cell></row><row><cell></cell><cell cols="2">Cross-lingual</cell><cell></cell><cell cols="2">In-language</cell></row><row><cell></cell><cell cols="5">Train Dev Test Train Dev</cell><cell>Test</cell></row><row><cell>Ar</cell><cell cols="2">2574 351</cell><cell cols="4">137 15828 357 1133</cell></row><row><cell>Bn</cell><cell cols="2">2582 313</cell><cell>128</cell><cell cols="2">2433 115</cell><cell>138</cell></row><row><cell>Fi</cell><cell cols="2">2088 362</cell><cell>530</cell><cell cols="3">7680 253 1197</cell></row><row><cell>Ja</cell><cell cols="2">2288 296</cell><cell>449</cell><cell cols="2">5527 140</cell><cell>869</cell></row><row><cell>Ko</cell><cell cols="2">2469 300</cell><cell>647</cell><cell>1860</cell><cell>75</cell><cell>507</cell></row><row><cell>Ru</cell><cell cols="2">1941 255</cell><cell>235</cell><cell cols="3">7349 313 1125</cell></row><row><cell>Te</cell><cell cols="2">1308 238</cell><cell>375</cell><cell cols="2">5451 133</cell><cell>712</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Dataset size of the XOR-TYDI QA corpus.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>was the French Prime Minister in 1993?) Mayor of Neuilly-sur-Seine from 1983 to 2002, he was Minister of the Budget under Prime Minister Édouard Balladur(1993)(1994)(1995). Examples newly annotated for Korean (Ko) and Russian (Ru) questions. The bottom example is an answerable question from TYDI QA for which only Japanese Wikipedia includes the correct answer. answers, short answers, and unanswered questions are given in Appendix §A.4.</figDesc><table><row><cell>Édouard</cell><cell>ᄋ ᅦᄃ ᅮᄋ ᅡᄅ ᅳ</cell></row><row><cell>Balladur</cell><cell>ᄇ ᅡ ᆯᄅ ᅡᄃ ᅱᄅ ᅳ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>com-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with recent multilingual QA datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>en.wikipedia ja.wikipedia</head><label></label><figDesc></figDesc><table><row><cell>Retriever</cell><cell></cell><cell>Reader</cell><cell></cell></row><row><cell>What did Ron Paul major in</cell><cell>Paul went to Gettysburg College …</cell><cell>Eng. Reader (Q en , P en )</cell><cell></cell></row><row><cell>during</cell><cell>He</cell><cell></cell><cell></cell></row><row><cell>undergradu ate?</cell><cell>graduated with a B.S.</cell><cell>Multilingual</cell><cell>Translation</cell></row><row><cell>Multilingual Retriever</cell><cell>degree in Biology in 1957</cell><cell>Reader (Q L , P en )</cell><cell></cell></row><row><cell></cell><cell>高校卒業後 はゲティス</cell><cell>(Q L , P L )</cell><cell></cell></row><row><cell>Monolingual Retriever in L</cell><cell>バーグにある ゲティスバー</cell><cell></cell><cell></cell></row><row><cell></cell><cell>グ大学へ進</cell><cell></cell><cell></cell></row><row><cell></cell><cell>学。</cell><cell></cell><cell></cell></row></table><note>1. XOR-Retrieve 2. XOR-EnglishSpan 3. XOR-Full 生物学 Eng. Retriever Translation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance on XOR-FULL (test data F1 scores). "GS" denotes Google Search retrieval. The forth row is a multilingual method that avoids query translation for searching W eng . The bottom section shows results from the single Wikipedia baselines. ElasticSearch (BM25) does not support Telugu.</figDesc><table><row><cell>Wiki</cell><cell cols="2">Translation</cell><cell cols="2">Retrieval</cell><cell></cell><cell>Target Language L i</cell><cell>Macro Average</cell></row><row><cell cols="2">Corpus Query</cell><cell cols="2">Answer L i</cell><cell cols="2">Eng. Ar Bn</cell><cell>Fi</cell><cell>Ja Ko Ru Te F1 EM BLEU</cell></row><row><cell></cell><cell>GMT</cell><cell>GMT</cell><cell>GS</cell><cell cols="3">DPR 30.9 16.7 16.7 7.7 19.1 19.7 9.5 17.1 10.1 15.3</cell></row><row><cell>W i,eng</cell><cell cols="6">Our MT Our MT GS Our MT Our MT BM25 DPR 12.0 7.4 9.1 5.3 9.4 7.3 2.0 7.5 4.1 DPR 29.7 8.0 15.3 7.1 15.4 19.1 2.8 13.9 8.7</cell><cell>11.9 6.5</cell></row><row><cell></cell><cell>-</cell><cell>GMT</cell><cell>GS</cell><cell cols="3">DPR 29.7 5.5 12.8 6.8 16.0 18.9 3.6 13.3 8.6</cell><cell>12.0</cell></row><row><cell>W eng</cell><cell cols="3">GMT Our MT Our MT -GMT -</cell><cell cols="3">DPR 20.4 16.2 18.9 17.6 14.1 13.0 9.0 15.6 9.2 DPR 11.0 7.4 14.7 10.1 5.9 7.4 2.0 8.4 4.0</cell><cell>12.6 6.9</cell></row><row><cell>W i</cell><cell>--</cell><cell>--</cell><cell cols="2">GS BM25 --</cell><cell cols="2">28.9 0.9 9.4 6.1 13.7 18.3 0.8 11.2 7.4 12.0 16.8 9.1 5.3 9.4 7.3 ---</cell><cell>9.6 -</cell></row><row><cell cols="6">W i only setting generally underperforms the best</cell></row><row><cell cols="6">W i,eng pipeline. These results demonstrate the</cell></row><row><cell cols="6">importance of searching multilingual document</cell></row><row><cell cols="2">collections.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>F1 scores on XOR-ENGLISHSPAN and the BLEU scores of the target language to English on the dev set. All configurations use DPR for paragraph retrieval. Telugu is excluded since Helsinki does not support it as of October, 2020.</figDesc><table><row><cell>Query</cell><cell></cell><cell></cell><cell></cell><cell>MT BLEU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">XOR-ENGLISHSPAN F1</cell></row><row><cell cols="2">Translator Ar</cell><cell>Bn</cell><cell>Fi</cell><cell>Ja</cell><cell>Ko</cell><cell>Ru Avg</cell><cell>Ar</cell><cell>Bn</cell><cell>Fi</cell><cell>Ja</cell><cell>Ko</cell><cell>Ru Avg</cell></row><row><cell>GMT</cell><cell cols="12">53.9 86.9 30.2 38.2 44.7 52.9 51.8 30.1 34.7 28.9 25.6 29.6 27.0 29.3</cell></row><row><cell>Our MT</cell><cell cols="12">33.7 30.8 27.4 19.7 30.8 21.7 27.4 18.6 20.4 27.2 19.7 22.7 12.9 20.2</cell></row><row><cell>Helsinki</cell><cell cols="12">35.9 33.0 29.8 19.8 31.8 37.3 31.1 22.3 19.0 28.9 17.4 26.0 21.8 22.0</cell></row><row><cell>Query</cell><cell></cell><cell>Easy</cell><cell></cell><cell cols="2">Hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Translator</cell><cell cols="6">R@2kt R@5kt R@2kt R@5kt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human</cell><cell>65.3</cell><cell></cell><cell>72.5</cell><cell>59.9</cell><cell></cell><cell>68.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GMT</cell><cell>61.1</cell><cell></cell><cell>67.7</cell><cell>54.3</cell><cell></cell><cell>63.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our MT</cell><cell>41.9</cell><cell></cell><cell>49.9</cell><cell>37.7</cell><cell></cell><cell>44.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multilingual</cell><cell>34.3</cell><cell></cell><cell>44.3</cell><cell>36.1</cell><cell></cell><cell>40.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Macro-averaged retrieval recall on the development easy and hard subsets. All configurations use DPR for retrieval. The Multilingual model avoids query translation.</figDesc><table /><note>els' zero-shot cross-lingual transfer performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>To implement BM25-based retrievers for 7 langugages, we use ElasticSearch's python client (Python Elasticsearch Client).11 We apply the default tokenizers and analyzers for Arabic, Bengali, Finnish and Russian. Japanese and Korean are not supported by the default ElasticSearch language analyzers, so we use Kuromoji 12 and Nori plug-Total Long (%) Short (%) Total Long (%) Short (%) total Long (%) Short (%) Dataset statistics of the resulting XOR QA corpus (cross-lingual data). "Long" denotes the questions with paragraph answer annotations, and "Short" denotes the questions with short answer annotations. During evaluation, we disregard the questions without short answer annotations.</figDesc><table><row><cell>L i</cell><cell cols="2">Train (1 way)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dev (2 way)</cell><cell>Test (2 way)</cell></row><row><cell>Arabic</cell><cell cols="4">4500 2862 (63) 2574 (57)</cell><cell>500</cell><cell>357 (71)</cell><cell>351 (70)</cell><cell>235</cell><cell>144 (61)</cell><cell>137 (58)</cell></row><row><cell>Bengali</cell><cell cols="4">4500 2822 (63) 2582 (57)</cell><cell>500</cell><cell>330 (66)</cell><cell>313 (62)</cell><cell>185</cell><cell>131 (70)</cell><cell>128 (69)</cell></row><row><cell>Finnish</cell><cell cols="4">4500 2454 (55) 2088 (46)</cell><cell>500</cell><cell>372 (74)</cell><cell>362 (72)</cell><cell>800</cell><cell>556 (69)</cell><cell>530 (66)</cell></row><row><cell cols="5">Japanese 4500 2557 (57) 2288 (51)</cell><cell>500</cell><cell>320 (64)</cell><cell>296 (59)</cell><cell>779</cell><cell>477 (61)</cell><cell>449 (57)</cell></row><row><cell>Korean</cell><cell cols="4">4500 2674 (59) 2469 (54)</cell><cell>500</cell><cell>314 (63)</cell><cell cols="2">300 (60) 1177 684 (58)</cell><cell>647 (55)</cell></row><row><cell>Russian</cell><cell cols="4">4500 2178 (48) 1941 (43)</cell><cell>500</cell><cell>270 (34)</cell><cell>255 (51)</cell><cell>470</cell><cell>252 (53)</cell><cell>235 (50)</cell></row><row><cell>Telugu</cell><cell cols="4">4500 1515 (33) 1308 (29)</cell><cell>500</cell><cell>258 (52)</cell><cell cols="2">238 (47) 1752 394 (22)</cell><cell>375 (21)</cell></row><row><cell cols="2">Hyperparameter</cell><cell></cell><cell cols="2">Value</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">label smoothing</cell><cell></cell><cell cols="2">0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># max tokens</cell><cell></cell><cell cols="2">4096</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dropout rate</cell><cell cols="3">[0.1, 0.2, 0.3]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">encoder embedding dim</cell><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">encoder ffn dim</cell><cell></cell><cell cols="2">2048</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># encoder attn heads</cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">decoder embedding dim</cell><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">decoder ffn dim</cell><cell></cell><cell cols="2">2048</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># decoder attn heads</cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">max source positions</cell><cell></cell><cell cols="2">10000</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">max target positions</cell><cell></cell><cell cols="2">10000</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Adam lrate</cell><cell></cell><cell>5 × 10</cell><cell>−4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Adam β 1</cell><cell></cell><cell cols="2">0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Adam β 2</cell><cell></cell><cell cols="2">0.98</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">lr-scheduler</cell><cell cols="3">inverse square</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">warm-up lr</cell><cell></cell><cell>1 × 10</cell><cell>−7</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># warmup updates</cell><cell></cell><cell cols="2">4000</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># max updates</cell><cell></cell><cell cols="2">300K</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">length penalty</cell><cell></cell><cell cols="2">1.0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters for our transformer machine translation models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>. In DPR, the training examples are retrieved by the trained retriever, 13 https://www.elastic.co/guide/ en/elasticsearch/plugins/7.9/ analysis-nori.html Human GMT Our MT Multi. DPR PATH BM DPR PATH DPR PATH DPR Ar 62.5 65.0 41.6 60.0 59.1 44.1 45.0 35.8 Bn 74.2 78.1 57.7 57.2 58.2 50.8 60.9 33.7 Fi 65.5 68.0 43.7 57.3 60.3 56.2 56.6 36.9 Ja 60.1 59.0 38.8 51.3 50.0 40.9 36.7 25.7 Ko 63.1 60.0 43.8 56.1 50.3 38.8 33.8 30.4 Ru 53.6 59.9 35.2 49.7 54.1 38.6 34.7 32.8 Te 62.5 59.6 44.6 56.1 58.0 15.7 15.7 35.5 Av. 63.1 64.3 43.5 57.2 58.2 40.7 40.5 33.7</figDesc><table><row><cell>Table</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For example, TYDI QA labels 65% of the Bengali questions as unanswerable partially because Bengali has fewer than 100k Wikipedia articles as of fall 2020.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Furthermore, despite the benefits of hidden test sets, the resource-intensive nature of open-retrieval QA is not suitable to code-submission leaderboards. This further precluded the use of the original TYDI QA test sets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://gengo.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://www.mturk.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">We found in the Telugu data, certain types of questions are very frequent (e.g., what is the pin code of X mandal?). Those questions often ask some specific information of local administration districts, and are often unanswerable because (a) they are typically not described in English Wikipedia and (b) the overall coverage of Telugu Wikipedia is quite low. This results in overall low answer coverage in the Telugu set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">https://www.elastic.co/jp/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">We measure R@2kt as well (See Table12in Appendix §C), but the relative pattern did not change across languages and methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7">https://elasticsearch-py. readthedocs.io/en/master/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8">https://www.elastic.co/guide/ en/elasticsearch/plugins/7.9/ analysis-kuromoji.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_9">https://en.wikipedia.org/wiki/ Almond_Eye</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by gifts from Google, ONR N00014-18-1-2826, DARPA N66001-19-2-403, the NSF (IIS1252835, IIS-1562364), an Allen Distinguished Investigator Award, the Sloan Fellowship, and the Nakajima Foundation Fellowship. We thank Sewon Min, Kristina Toutanova, David Wadden, and the members of the UW NLP group for their insightful feedback on this paper, Nancy Li, Xun Cao, Hitesh Boinpally, Samek Mulepati, Casey Zhao, Vitaly Nikolaev, Soumyadip Sengupta, Bindita Chaudhuri, and Aditya Kusupati for their help on our annotations and dataset proofing, and Nelson Liu and Pradeep Dasigi for their suggestions on the annotation interface and Amazon Mechanical Turk crowdsourcing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Additional Details of Dataset Creation A.1 Annotation Interface</head><p>In this section, we describe the details of the annotation interface we used for answer annotation in English ( §2.1.3). The annotation interface can be seen in Figs. <ref type="figure">4 and 5</ref>. To maximize the answer coverage for open-retrieval questions, we first rank paragraphs from top articles retrieved by Google Search. During this paragraph ranking process, we only consider top 5 paragraphs and exclude the articles ranked from top 6 to 10. Increasing the number of the initial articles introduces more noise and confuses our paragraph ranking model, while human annotators sometimes found that those lowranked articles relevant and retrieve answers from them as discussed in §2.1.3. In the annotation interface, we first present those top 5 paragraphs first (the ones highlighted in light blue in Fig. <ref type="figure">4</ref>). When annotators do not find answers in the pre-selected top 5 paragraphs, they will explore more paragraphs and articles by expanding originally collapsed articles as in Fig. <ref type="figure">5</ref>. GMT GMT GS DPR 21.7 10.1 11.9 2.1 14.6 10.8 5.4 Our MT Our MT GS DPR 21.0 2.9 10.6 2.0 11.8 10.7 1.9 Our MT Our MT BM25 DPR 7.7 2.2 6.1 1.2 6.5 3.8 1.1 -GMT GS DPR 20.9 5.4 9.0 1.7 12.4 10.3 2.4</p><p>W eng GMT GMT -DPR 11.1 9.3 13.5 10.4 9.8 5.9 4.7 Our MT Our MT -DPR 5.0 2.2 8.9 4.8 3.3 2.5 1.4</p><p>.5 1.0 7.0 1.5 11.0 10.3 0.8 --BM25 -7.7 3.0 6.1 1.2 6.5 2.8 - </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translation artifacts in cross-lingual transfer learning</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Challenges in information seeking QA: Unanswerable questions and paragraph retrieval</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multilingual extractive reading comprehension by runtime machine translation</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dictionary methods for cross-lingual information retrieval</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1007/BFb0034731</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>In DEXA</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Findings of the 2019 conference on machine translation (WMT19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Santanu Pal, Matt Post, and Marcos Zampieri. In WMT</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ewa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">C</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><surname>Herring</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.21577</idno>
		<title level="m">Cultural bias in wikipedia content on famous persons</title>
				<imprint>
			<publisher>JASIST</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer open-domain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opendomain question answering: Tutorial abstracts</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-tutorials.8</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
	<note>Vitaly Nikolaev, and Jennimaria Palomaki</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Querying across languages: a dictionary-based approach to multilingual information retrieval</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL System Demonstrations</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MeCab: Yet another part-ofspeech and morphological analyzer</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentence-Piece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP System Demonstrations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<title level="m">Natural Questions: A benchmark for question answering research</title>
				<editor>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</editor>
		<imprint>
			<publisher>TACL</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">KorQuaAD1.0: Korean qa dataset for machine reading comprehension</title>
		<author>
			<persName><forename type="first">Seungyoung</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myungji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jooyoul</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">XQA: A cross-lingual open-domain question answering dataset</title>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MKQA: A linguistically diverse benchmark for multilingual open domain question answering</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m">Internet world stats: Usage and population statistics</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Miniwatts Marketing Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL System Demonstrations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang</title>
				<imprint>
			<date type="published" when="2002">2002. 2016</date>
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LAReQA: Language-agnostic answer retrieval from a multilingual pool</title>
		<author>
			<persName><forename type="first">Uma</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Crosslingual learning-to-rank with shared representations</title>
		<author>
			<persName><forename type="first">Shota</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CLIReval: Evaluating machine translation as a cross-lingual information retrieval task</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanna</forename><surname>Sia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL System Demonstrations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mul-tiQA: An empirical investigation of generalization and transfer in reading comprehension</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The OPUS corpus -parallel and free</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Nygaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">OPUS-MT -Building open translation services for the World</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Thottingal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In EAMT</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving low-resource cross-lingual document retrieval by reranking with deep bilingual representations</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caitlin</forename><surname>Westerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungrok</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrett</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The United Nations parallel corpus v1.0</title>
		<author>
			<persName><forename type="first">Michał</forename><surname>Ziemski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
