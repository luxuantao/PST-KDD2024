<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Future Generation Computer Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-13">13 September 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ahmed</forename><surname>Ghoneim</surname></persName>
							<email>ghoneim@ksu.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Software Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer and Information Sciences</orgName>
								<orgName type="institution">King Saud University</orgName>
								<address>
									<settlement>Riyadh</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ghulam</forename><surname>Muhammad</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer and Information Sciences</orgName>
								<orgName type="institution">King Saud University</orgName>
								<address>
									<settlement>Riyadh</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Shamim Hossain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Software Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer and Information Sciences</orgName>
								<orgName type="institution">King Saud University</orgName>
								<address>
									<settlement>Riyadh</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Future Generation Computer Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-13">13 September 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">9EA97C94241D85A9D80479CDFD31F4B2</idno>
					<idno type="DOI">10.1016/j.future.2019.09.015</idno>
					<note type="submission">Received 2 April 2019 Received in revised form 28 August 2019 Accepted 9 September 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cervical cancer is one of the main reasons of death from cancer in women. The complication of this cancer can be limited if it is diagnosed and treated at an early stage. In this paper, we propose a cervical cancer cell detection and classification system based on convolutional neural networks (CNNs). The cell images are fed into a CNNs model to extract deep-learned features. Then, an extreme learning machine (ELM)-based classifier classifies the input images. CNNs model is used via transfer learning and fine tuning. Alternatives to the ELM, multi-layer perceptron (MLP) and autoencoder (AE)-based classifiers are also investigated. Experiments are performed using the Herlev database. The proposed CNN-ELM-based system achieved 99.5% accuracy in the detection problem (2-class) and 91.2% in the classification problem (7-class).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The cervix of a human is covered by a thin layer of tissues consisting of cells. If a cell is changed into a malignant cell that can grow and divide rapidly and becomes a tumor, we call this situation the cervical cancer. This cancer can be treated if it is detected at an early stage. The diagnosis is normally done by a screening process and a biopsy. Image processing techniques can be applied to find the spread of the cancer. Cervical cancer is the fourth-most common cause of death from cancer in women <ref type="bibr" target="#b0">[1]</ref>.</p><p>Medical image processing and intelligent systems play a role in the analysis of the malignant cells. With the development of new techniques, they become cost-effective and less timeconsuming. They are now becoming popular over conventional methods such as Pap Smear, Colposcopy, and Cervicography. These techniques are unbiased to human experience; however, we want to stress that they cannot replace the subjective (expert doctor) evaluation, but can assist them to a great degree.</p><p>State-of-the-art machine learning techniques and wireless communication technologies have enabled us to develop a complete medical diagnosis system that can operate in real-time, accurately, and without human interaction. Yet, there are many issues that need to be solved, for example, packet loss during transmission, high bandwidth requirement for medical video data transfer, and a robust algorithm to deal with many variations in data. To address some of this issues, edge-based cloud computing was proposed for voice pathology detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, Internet of Things (IoTs) and cloud-based framework was realized in <ref type="bibr" target="#b3">[4]</ref>, deep learning for emotion recognition was proposed in <ref type="bibr" target="#b4">[5]</ref>, edge-based communication was introduced in <ref type="bibr" target="#b5">[6]</ref>, and a disease monitoring system was proposed in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Computer-aided systems in cancerous cell detection have been used in the literature for quite a few times. In breast cancer recognition, different feature extraction methods such as local binary pattern, histogram of gradient orientation, and Laplacian Gaussian filter were used <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Local texture analysis was used to diagnose pulmonary nodules in <ref type="bibr" target="#b10">[11]</ref>. To analyze dermoscopy images for skin cancer, directional filters and color component features were used in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. A method for voice pathology detection using different input modalities were proposed in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recently, deep learning has brought a big improvement in accuracy in many applications. Due to its high accuracies in many areas, it has become the state-of-the-art machine learning technique. A good survey on various cancerous cells detection using deep learning can be found in <ref type="bibr" target="#b15">[16]</ref>. Deep learning was successfully used in EEG pathology detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, environment classification <ref type="bibr" target="#b18">[19]</ref>, lung nodule detection <ref type="bibr" target="#b19">[20]</ref>, breast cancer detection <ref type="bibr" target="#b20">[21]</ref>, skin cancer detection <ref type="bibr" target="#b21">[22]</ref>, medical image analysis <ref type="bibr" target="#b22">[23]</ref>, audio-visual emotion recognition <ref type="bibr" target="#b23">[24]</ref> and diseases prediction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. With the increase in many types of sensors, processing of Big Data has added an extra dimension to the deep learning. Big Data has successfully handled in several medical related applications <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Due to the success of deep learning in many medical applications, in this paper, we propose a deep learning-based system to detect and classify cervical cancerous cells. In particular, we use convolutional neural networks (CNNs) followed by an extreme learning machine (ELM)-based classifier in the system. We investigate different models of CNNs via transfer learning. We also investigate various classifiers such as multi-layer perceptron (MLP), ELM, and autoencoder (AE). A public database, the Herlev database <ref type="bibr" target="#b28">[29]</ref>, has been used in the experiments. The contributions of the work are (i) introducing CNNs in cervical cancer cell detection and classification, (ii) introducing ELM-based classifier, and (iii) introducing AE-based classifier. The CNN extracts deeplearned features from the raw input. These features are fed into an ELM-based classifier or an AE-based classifier. To the best of our knowledge, this is the first attempt to investigate the use of the ELM and the AE after the CNN in the cervical cancer detection system.</p><p>The paper is organized as follows. Some related previous works are briefed in Section 2. The proposed system is described in Section 3. Experimental results and discussion are provided in Section 4. The conclusion of the work and some future directions are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous works</head><p>There are several works to classify and detect cervical cancer in the literature. In 2000's, most of the works involved handcrafted features and traditional classifiers such as support vector machine (SVM), artificial neural networks (ANNs), and kmeans neural networks (K-NNs). We describe some of the related previous works in the following paragraphs.</p><p>Zhang et al. proposed an SVM-based cervical cancer classification system in <ref type="bibr" target="#b29">[30]</ref>. They used a subset of Herlev database <ref type="bibr" target="#b28">[29]</ref>, and the subset contained 149 cell images of which 108 were normal and 41 were cancerous. The system achieved an accuracy of 98%. The subset was small, and therefore, we cannot reach to a final conclusion about the system's accuracy.</p><p>Segmentation of cervical cell images was performed using a hierarchical segmentation algorithm in <ref type="bibr" target="#b30">[31]</ref>. The segmentation accuracy was 96%; however, due to noisy texture, the nucleus could not be extracted from some cells. The authors used Herlev database. Another hierarchical segmentation algorithm was used in <ref type="bibr" target="#b31">[32]</ref>, where the classification was performed using a hierarchical clustering. There were problems of overlapping cytoplasm areas in some cells in the proposed algorithm.</p><p>Wang et al. proposed a segmentation and classification system for cervical cells in <ref type="bibr" target="#b32">[33]</ref>. For the classification they used shape and texture features together with Gabor features, and the SVM. The accuracy was more than 89% for normal and cancerous cells classification. The authors a private database. A genetic algorithm (GA)-based cervical cancer staging was proposed in <ref type="bibr" target="#b33">[34]</ref>. The database was private and the mutation of the GA was unguided resulting in too many generations.</p><p>In <ref type="bibr" target="#b34">[35]</ref>, the authors proposed a gray-level co-occurrence matrix (GLCM) and a probabilistic neural network based system, which achieved an accuracy of 92.8%. The images were MRI images, and the database was private. A similar system was proposed in <ref type="bibr" target="#b35">[36]</ref>, where the GLCM features and the SVM classifier were used. Results varied with different types of MRI images.</p><p>Several machine learning techniques were investigated in <ref type="bibr" target="#b36">[37]</ref> for cervical cancer prediction. The techniques included classification and regression tree, random forest tree, and random forest tree with k-means learning. The maximum accuracy was 96.7%; however, there were 61 features specially prepared for this study. A decision tree-based system was realized in <ref type="bibr" target="#b37">[38]</ref>, where 10 features from the cell images were obtained. An accuracy of 67.5% was achieved by the system. The database was small and private.</p><p>k-NNs and ANNs-based classification system was proposed in <ref type="bibr" target="#b38">[39]</ref>. The Herlev database was used in the experiments. The k-NNs-based system achieved 88% accuracy, while the ANNsbased system obtained 54% accuracy. Multiple backpropagation NN-based system was designed in <ref type="bibr" target="#b39">[40]</ref>. A private database was used, where the image quality was not good. 95.6% accuracy was obtained by the system. 20 shape and position features together with GA was used in <ref type="bibr" target="#b40">[41]</ref>. The Herlev database was used in the experiments. The classification was done using many folds. The best accuracy was 98% for 2-class problem (normal and abnormal) and 96.9% for 7class problem. Same 20 features and different machine learning techniques were used in <ref type="bibr" target="#b41">[42]</ref>. The reported accuracies varied between 94% and 97% depending on the techniques for the 2class problem, and 72% and 80% for the 7-class problem. The same features, and a second-order NN were investigated in <ref type="bibr" target="#b42">[43]</ref>. 98.4% accuracy was achieved using the Herlev database.</p><p>Recently, CNN-based systems were also proposed to detect cervical cancer cells. In <ref type="bibr" target="#b43">[44]</ref>, CNNs were used to develop a system called DeepPap. Using the Herlev database, the system obtained 98.6% accuracy. One of the problems was that the nucleus center was required to be identified before feeding the images to the CNNs. In <ref type="bibr" target="#b44">[45]</ref>, CNNs and some additional features were utilized. Based on the features, the accuracies varied between 90% and 95%.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the previous works in terms of methods, databases, and accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed system</head><p>The CNN models have been successful in many image processing applications including medical image analysis. Inspired by this, we propose a CNNs-based cervical cancer detection and classification system. The CNNs-based systems need huge data for training, and it is very difficult to get a large database of medical images. Therefore, transfer learning and fine tuning are popular when the database size is small. A deep CNN model can be trained using a large amount of data, and the trained model can be used as a pretrained model. The pretrained model parameters are finetuned using a training set of a targeted database. This fine-tuned model is used for the testing. A general block diagram of the proposed system is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In the proposed system, we investigate three CNN models, one of them has a shallow architecture and two others have a deep architecture. The architecture In the proposed system, we investigate two deep CNN models in the form of the VGG-16 Net <ref type="bibr" target="#b45">[46]</ref> and the CaffeNet <ref type="bibr" target="#b46">[47]</ref>. These two models were trained using million of images and won many competitions. They are also used in many applications including for Epilepsy Seizure Detection <ref type="bibr" target="#b47">[48]</ref>. The architectures of the VGG-16 Net and the CaffeNet are shown in Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>, respectively. These two models were fine-tuned by the same process as described in the previous paragraph. The models are followed by classifiers as shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>In the proposed system, we use an ELM-based classifier. The ELM is a shallow network and has many advantages such as fast learning, easy convergence, and less randomized <ref type="bibr" target="#b48">[49]</ref>. In the proposed system, there are two ELMs after the last fully connected layer of the CNN model (removing the softmax layer). The output of the first ELM is set to give normal or abnormal cells, while the output of the second ELM is set to give classes of normal and abnormal cases. Once the first ELM training is finished, the output is removed, and the hidden layer is fed to the second ELM. The number of neurons in the hidden layer is fixed to 2048, which means this ELM refers to a sparse representation. The SGD algorithm is used to optimize the weights of the ELM.</p><p>Suppose that there are N h neurons in the hidden layer of the ELM. The row-wise output vector of the layer has the size of 1 × N h and is denoted as h(x i ), where x i is the input vector. The output weight vector that connects the hidden layer to the output layer is denoted as α and has the size of N h × N o , where N o is the number of output classes. The output of the ELM is as follows:</p><formula xml:id="formula_0">f (x i ) = h(x i )α, i ∈ {1: N h }.</formula><p>The objective function is defined as</p><formula xml:id="formula_1">min α ∥α∥ 2 F + ρ N h ∑ i=1 ∥e i ∥ 2 ,</formula><p>where ∥α∥ F is the Frobenius norm of the weight vector, ρ is the penalty parameter, and e is the training error vector. Alternative to the ELM-based classifier, we investigate AEbased classifier. The AE has the power to remove noise and it is good for extracting generic features of a class. There is one hidden layer in the AE. The number of neurons in the hidden layer is empirically fixed to 128. The SGD algorithm is used to optimize the weights of the AE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We used the Herlev database in our experiments. The database was developed in Herlev University Hospital (Denmark), and is available at http://fuzzy.iau.dtu.dk/download/smear2005. There are total 917 cells and seven classes. Three classes belong to normal and four classes belong to abnormal. The number of images per class is shown in Table <ref type="table" target="#tab_1">2</ref>. There are 242 images for normal and 675 images for abnormal.</p><p>We used a 5-fold cross validation approach. For fine tuning, we used 80% of the data in iteration. The rest 20% was used for testing. After five iterations, all the data were tested. The final accuracy was obtained by averaging accuracies of five iterations. The Herlev dataset was used for both the training and the testing the shallow CNN model using the 5-fold cross-validation approach. There was no fine tuning the model.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows accuracies, false negative rates, and false positive rates of the proposed systems. All the three models have two fully connected layers and a softmax layer. The VGG-16 Net and the CaffeNet have equal performance, and the shallow model has comparable performance. The performance of the shallow model indicates that for a low size database a shallow CNN model can also perform well. If there is a constraint of time, we may use the shallow model without much loss in accuracy. Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref> show performances of the systems using the ELM-based classifier, and using the AE-based classifier, respectively. The performance using the ELM-based classifier is the best giving an accuracy of 99.7%. Figs. <ref type="figure" target="#fig_5">6</ref> and<ref type="figure" target="#fig_6">7</ref> show the confusion matrices of the system using the ELM and the AE, respectively.</p><p>Table <ref type="table" target="#tab_5">6</ref> shows the performance of the systems in a 7-class problem. The system with the ELM-based classifier achieved the highest accuracy. The deep CNN models performed better than the shallow CNN model. The AE-based classifier also had better performance than using only fully connected layer (MLP).     Fig. <ref type="figure" target="#fig_7">8</ref> shows the accuracy comparison between the systems. All the proposed systems achieved higher accuracy than all the previous systems. This high accuracy was obtained without using any hand-crafted features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>A cervical cancer detection and classification system using CNN was proposed. The ELM-based classifier or the AE-based   In a future work, the proposed system will be evaluated using other databases. The system can also be enhanced by incorporating some hand-crafted features. Recently, new deep architectures such as ResNet, Inception, and tree-based models <ref type="bibr" target="#b49">[50]</ref> are showing promising results in many applications. We will investigate these architectures in the cervical cancer detection system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of the proposed system.</figDesc><graphic coords="4,122.45,55.32,340.74,91.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of the shallow CNN model.</figDesc><graphic coords="4,65.66,180.82,454.15,135.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architecture of the VGG-16 Net.</figDesc><graphic coords="4,87.71,350.24,410.09,135.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of CaffeNet.</figDesc><graphic coords="4,60.71,519.48,464.27,104.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Proposed classifiers after the CNN model.</figDesc><graphic coords="5,114.17,55.32,376.86,107.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Confusion matrix of the proposed system with ELM.</figDesc><graphic coords="5,71.60,196.30,462.10,181.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Confusion matrix of the proposed system with AE. classifier after the CNN model was integrated into the system. Both shallow CNN model and deep CNN models were investigated. Using the Herlev database, the proposed system with the ELM-based classifier achieved 99.7% accuracy in the 2-class problem and 97.2% accuracy in the 7-class problem. These accuracies were better than any reported previous accuracies.</figDesc><graphic coords="5,71.60,411.98,462.10,181.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Accuracy comparison of the systems.</figDesc><graphic coords="6,172.73,55.32,240.12,172.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of the previous works.</figDesc><table><row><cell cols="2">Systems Method</cell><cell>Database</cell><cell>Accuracy (%)</cell></row><row><cell cols="3">Neural networks or deep neural networks based</cell><cell></cell></row><row><cell>[44]</cell><cell>CNN</cell><cell>Herlev</cell><cell>98.6</cell></row><row><cell>[40]</cell><cell>Backpropagation neural</cell><cell cols="2">North India, private 95.6</cell></row><row><cell></cell><cell>networks</cell><cell></cell><cell></cell></row><row><cell>[39]</cell><cell>k-NNs and ANNs</cell><cell>Herlev</cell><cell>k-NNs: 88;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ANNs: 54</cell></row><row><cell>[45]</cell><cell>CNN</cell><cell>Herlev</cell><cell>90∼95</cell></row><row><cell>[35]</cell><cell>GLCM, probabilistic</cell><cell cols="2">MRI images, private 92.8</cell></row><row><cell></cell><cell>neural networks</cell><cell></cell><cell></cell></row><row><cell cols="3">Hand-crafted features / traditional classifiers based</cell><cell></cell></row><row><cell>[30]</cell><cell>SVM</cell><cell>Subset of Herlev</cell><cell>98</cell></row><row><cell>[38]</cell><cell>Decision tree</cell><cell>Private</cell><cell>67.5</cell></row><row><cell>[36]</cell><cell>GLCM, SVM</cell><cell cols="2">MRI images, private 81∼83</cell></row><row><cell>[41]</cell><cell>20 features, GA</cell><cell>Herlev</cell><cell>98 (2-class),</cell></row><row><cell></cell><cell></cell><cell></cell><cell>96.9 (7-class)</cell></row><row><cell>[42]</cell><cell>20 features, C-means /</cell><cell>Herlev</cell><cell>94-96 (2-class),</cell></row><row><cell></cell><cell>Fuzzy clustering</cell><cell></cell><cell>72-80 (7-class)</cell></row><row><cell>[43]</cell><cell>20 features, Fuzzy</cell><cell>Herlev</cell><cell>98.4 (2-class)</cell></row><row><cell></cell><cell>C-means, 2nd order NN</cell><cell></cell><cell></cell></row><row><cell>[33]</cell><cell>Shape, texture, and</cell><cell>Private</cell><cell>89.8</cell></row><row><cell></cell><cell>Gabor; SVM</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Number of images per class in Herlev database. of size 224 × 224. There are 64 filters of size 5 × 5 in the first convolutional layer, and 128 filters of size 5 × 5 in the second convolutional layer. The stride of the filters is 2 pixels.The mask size of the max-pooling filter is 2 × 2. The rectified linear unit is used for the non-linear activation. After the second max-pooling layer, the features are flattened, and fed into a fully connected layer. There are two fully connected layers followed by a softmax (output) layer. Once the training was finished, it was fine-tuned by a training subset of the intended database. A minibatch Stochastic Gradient Descent (SGD) algorithm has been used to optimize the parameters of the model. The batch size is 20, the learning rate is 0.01, and the epoch size is 50.</figDesc><table><row><cell>Normal</cell><cell></cell><cell></cell><cell>Abnormal</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Superficial</cell><cell>Intermediate</cell><cell>Columnar</cell><cell>Mild squamous</cell><cell>Moderate squamous</cell><cell>Severe squamous</cell><cell>Squamous cell</cell></row><row><cell>squamous</cell><cell>squamous</cell><cell>epithelial</cell><cell>non-keratinizing</cell><cell>non-keratinizing</cell><cell>non-keratinizing</cell><cell>carcinoma in situ</cell></row><row><cell>epithelial</cell><cell>epithelial</cell><cell></cell><cell>dysplasia</cell><cell>dysplasia</cell><cell>dysplasia</cell><cell>intermediate</cell></row><row><cell>74</cell><cell>70</cell><cell>98</cell><cell>182</cell><cell>146</cell><cell>197</cell><cell>150</cell></row><row><cell></cell><cell>242</cell><cell></cell><cell></cell><cell cols="2">675</cell><cell></cell></row><row><cell cols="4">of the shallow CNN model is shown in Fig. 2. The model has only</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">two convolutional layers and two max-pooling layers. The input is</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the RGB image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Performance of the proposed systems in 2-class problem (without ELM or AE).</figDesc><table><row><cell>Model</cell><cell>Accuracy (%)</cell><cell>False negative (%)</cell><cell>False positive (%)</cell></row><row><cell>Shallow</cell><cell>98.4</cell><cell>1</cell><cell>0.6</cell></row><row><cell>VGG-16 Net</cell><cell>98.6</cell><cell>0.9</cell><cell>0.5</cell></row><row><cell>CaffeNet</cell><cell>98.6</cell><cell>0.9</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Performance of the proposed systems with ELMs in 2-class problem.</figDesc><table><row><cell>Model</cell><cell>Accuracy (%)</cell><cell>False negative (%)</cell><cell>False positive (%)</cell></row><row><cell>Shallow, ELM</cell><cell>99.5</cell><cell>0.6</cell><cell>0.4</cell></row><row><cell>VGG-16 Net, ELM</cell><cell>99.7</cell><cell>0.3</cell><cell>0.2</cell></row><row><cell>CaffeNet, ELM</cell><cell>99.7</cell><cell>0.3</cell><cell>0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Performance of the proposed systems with AEs in 2-class problem.</figDesc><table><row><cell>Model</cell><cell>Accuracy (%)</cell><cell>False negative (%)</cell><cell>False positive (%)</cell></row><row><cell>Shallow, AE</cell><cell>99.2</cell><cell>0.8</cell><cell>0.7</cell></row><row><cell>VGG-16 Net, AE</cell><cell>99.5</cell><cell>0.6</cell><cell>0.4</cell></row><row><cell>CaffeNet, AE</cell><cell>99.6</cell><cell>0.5</cell><cell>0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Accuracies (%) of the proposed systems in 7-class problem.</figDesc><table><row><cell>Model</cell><cell>Without ELM or AE</cell><cell>With ELM</cell><cell>With AE</cell></row><row><cell>Shallow</cell><cell>96.5</cell><cell>97.5</cell><cell>97.2</cell></row><row><cell>VGG-16 Net</cell><cell>97.1</cell><cell>98.1</cell><cell>97.7</cell></row><row><cell>CaffeNet</cell><cell>97.2</cell><cell>98.2</cell><cell>97.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors acknowledge funding from the Research and Development (R&amp;D) Program (Research Pooling Initiative), Ministry of Education, Riyadh, Saudi Arabia, (RPI-KSU).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">World Cancer Report</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotion recognition using secure edge and cloud computing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">504</biblScope>
			<biblScope unit="page" from="589" to="601" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An investigation of multidimensional voice program parameters in three different databases for voice pathology detection and classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Nasheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Voice</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>e9-113. e18</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smart health solution integrating IoT and cloud: a case study of voice pathology monitoring</title>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alelaiwi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotion recognition using deep learning approach from audio-visual emotional big data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="69" to="78" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Edge-CoCaCo: Toward Joint Optimization of Computation, Caching, and Communication on Edge Cloud</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cloud-based collaborative media service framework for HealthCare</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Distrib. Sens. Netw</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">858712</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computer-aided detection of breast masses on full field digital mammograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sahiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hadjiiski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Helvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Roubidoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2827" to="2838" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A concentric morphology model for the detection of masses in mammography</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Eltonsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Tourassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Elmaghraby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="880" to="889" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Medical image forgery detection for smart healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoneim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Texture feature analysis for computer-aided diagnosis on pulmonary nodules</title>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="99" to="115" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A system for the detection of pigment network in dermoscopy images using directional filters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="2744" to="2754" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detection and analysis of irregular streaks in dermoscopic images of skin lesions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Atkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="849" to="861" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smart healthcare monitoring: A voice pathology detection paradigm for smart cities</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00530-017-0561-x</idno>
		<ptr target="http://dx.doi.org/10.1007/s00530-017-0561-x" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Syst</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Voice pathology detection using interlaced derivative pattern on glottal source excitation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="156" to="164" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for image-based cancer detection and diagnosis -A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="134" to="149" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EEG pathology detection based on deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alhussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="27781" to="27788" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for EEG motor imagery classification based on multi-layer CNNs feature fusion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Amin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="542" to="554" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Environment classification for urban big data using deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="44" to="50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lung nodule classification using deep feature fusion in chest radiography</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elazab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aggnet: deep learning from crowds for mitosis detection in breast cancer histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Achilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1313" to="1321" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated melanoma recognition in dermoscopy images via very deep residual networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="994" to="1004" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep features learning for medical image analysis with convolutional autoencoder neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guizani</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBDATA.2017.2717439</idno>
		<ptr target="http://dx.doi.org/10.1109/TBDATA.2017.2717439" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Big Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Audio-visual emotion fusion (AVEF): A deep efficient weighted approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network based multimodal disease risk prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="page" from="76" to="83" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep feature learning for disease risk assessment based on convolutional neural network with intra-layer recurrent connection by using hospital big data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Usama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67927" to="67939" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotion-aware connected healthcare big data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2399" to="2406" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">User emotion recognition from a larger pool of social network data using active learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Alhamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10881" to="10892" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Analysis of Pap-Smear Data, NISIS 2006, Puerto de la Cruz</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jantzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dounias</surname></persName>
		</author>
		<ptr target="http://fuzzy.iau.dtu.dk/download/smear" />
		<imprint>
			<date type="published" when="2005">29 Nov. -1 2006. 2005</date>
			<pubPlace>Tenerife, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cervical cancer detection using SVM based feature screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segmentation of cervical cell images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation and classification of cervical cell images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Genctav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sevgen</forename><surname>Onder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4151" to="4168" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Utomatic cell nuclei segmentation and classification of cervical pap smear images</title>
		<author>
			<persName><forename type="first">Pin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Genetic algorithms for staging cervical cancer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Virk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Appl. Inf. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving cervical cancer classification on MR images using texture analysis and probabilistic neural network</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V R</forename><surname>Kumari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Sci. Eng. Technol. Res. (IJSETR)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cervical cancer detection and classification using texture analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Soumya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sneha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arunvinodh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Pharmacol. J</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prediction of cervical cancer using hybrid induction technique: A solution for human hereditary disease patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Nasira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian J. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">30</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cervical cancer stage prediction using decision tree approach of machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv. Res. Comput. Commun. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Machine learning technique for detection of cervical cancer using k-NN and artificial neural network</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Malli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Emerg. Trends Technol. Comput. Sci. (IJETTCS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Screening of cervical cancer by artificial intelligence based analysis of digitized papanicolaou-smear images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Contemp. Med. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pap smear diagnosis using a hybrid intelligent scheme focusing on genetic algorithm based feature selection and nearest neighbor classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Marinakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dounias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jantzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="78" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automated identification of cancerous smears using various competitive intelligent techniques</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dounias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oncol. Rep. J</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1001" to="1006" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pap-Smear Classification Using Efficient Second Order Neural Network Training Algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ampazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dounias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jantzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Artificial Intelligence (LNAI)</title>
		<imprint>
			<biblScope unit="volume">3025</biblScope>
			<biblScope unit="page" from="230" to="245" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08616v1[cs.CV</idno>
		<title level="m">DeepPap: Deep convolutional networks for cervical cell classification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pap smear image classification using convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Mahanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. Neural Inf. Process. Syst</title>
		<meeting>25th Int. Conf. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Multimedia (MM)</title>
		<meeting>ACM Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Applying deep learning for epilepsy seizure detection and brain mapping visualization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Al</forename><surname>Sulaiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl. (ACM TOMM)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1s</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extreme learning machine: Theory and applications</title>
		<author>
			<persName><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin-Yu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kheong</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep convolutional tree networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mathkour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="152" to="168" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
