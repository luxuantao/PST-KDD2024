<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A novel end-to-end brain tumor segmentation method using improved fully convolutional networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haichun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Centers for Biomedical Engineering</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<region>AH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Centers for Biomedical Engineering</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<region>AH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ao</forename><surname>Li</surname></persName>
							<email>aoli@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Centers for Biomedical Engineering</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<region>AH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Centers for Biomedical Engineering</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<region>AH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Centers for Biomedical Engineering</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<region>AH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Centers for Biomedical Engineering</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<region>AH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A novel end-to-end brain tumor segmentation method using improved fully convolutional networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">443F1ACA2E9FD9E75D2B84773A5A716B</idno>
					<idno type="DOI">10.1016/j.compbiomed.2019.03.014</idno>
					<note type="submission">Received Date: 30 August 2018 Revised Date: 13 March 2019 Accepted Date: 14 March 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Brain tumor segmentation</term>
					<term>Fully convolutional networks</term>
					<term>Deep learning</term>
					<term>Glioma</term>
					<term>Magnetic resonance imaging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate brain magnetic resonance imaging (MRI) tumor segmentation continues to be an active research topic in medical image analysis since it provides doctors with meaningful and reliable quantitative information in diagnosing and monitoring neurological diseases. Successful deep learning-based proposals have been designed, and most of them are built upon image patches. In this paper, a novel end-to-end brain tumor segmentation method is developed using an improved fully convolutional network by modifying the U-Net architecture. In our network, an innovative structure referred to as an up skip connection is first proposed between the encoding path and decoding path to enhance information flow. Moreover, an inception module is adopted in each block to help our network learn richer representations, and an efficient cascade training strategy is introduced to segment brain tumor subregions sequentially. In contrast to those patchwise methods, our model can automatically generate segmentation maps slice by slice. We have validated our proposal by using imaging data from the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2015 and BRATS 2016. Experimental results compared with U-Net suggest that our method is 2.6%, 3.9%, and 5.2% higher (by using the BRATS 2015 training dataset) as well as 2.8%, 3.7%, and 8.1% (by using the BRATS 2017 training dataset) higher in terms of complete, core and enhancing tumor regions, respectively. Quantitative and visual evaluation of our method has revealed the effectiveness of the proposed improvements and indicated that our end-to-end segmentation method can achieve a performance that can compete with state-of-the-art brain tumor segmentation approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the most prevalent primary brain tumors among adults, gliomas emanate from glial cells of the brain or the spine and are pathologically categorized into high-grade gliomas (HGGs) and low-grade gliomas (LGGs) according to the well-known World Health Organization (WHO) grading system <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Grade III and IV tumors are classified as HGGs and are highly invasive and commonly lead to a worse prognosis, while LGGs (comprised of grade I and II) tend to exhibit benign tendencies with better prognoses <ref type="bibr" target="#b2">[3]</ref>.</p><p>In clinical practice, brain magnetic resonance imaging (MRI) has been commonly used to diagnose gliomas and for follow-up evaluations of the brain due to its sensitivity and significant image contrast in soft tissues <ref type="bibr" target="#b3">[4]</ref>. However, the multiplicity and complexity of brain tumors under MRI often make tumor recognition and segmentation difficult for radiologists and other clinicians <ref type="bibr" target="#b4">[5]</ref>. Hence, automated segmentation of heterogeneous tumors is of great importance in clinical medicine by freeing doctors from the timeconsuming and laborious manual delineation of tumors <ref type="bibr" target="#b3">[4]</ref>. Moreover, excellent brain tumor segmentation algorithms with quantitative measurements of tumor delineation will significantly contribute to cancer diagnosis, treatment and prognosis.</p><p>In the past few decades, massive research efforts have been devoted to exploring accurate segmentation approaches in silico to generate reliable depictions of brain tumors, and many successful methods have been proposed in this field <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. These methods generally fall into two categories, namely, generative models and discriminative models. Generative models usually demand prior information generated from probabilistic atlases for image registration <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, while image registration is not reliable for very large tumors. On the other hand, discriminative models exploit various image features to segment target tumors by categorizing each image voxel <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>.</p><p>Discriminative models usually adopt discriminative learning approaches, including support vector machines (SVMs) and random forests. These discriminative methods are</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>4 usually costly and time consuming because they rely heavily on various handcrafted image features generated by experts, such as texture features <ref type="bibr" target="#b15">[16]</ref> and local histograms <ref type="bibr" target="#b8">[9]</ref>.</p><p>Recently, deep learning has become the favored approach for various complicated tasks due to the considerable capacity of the models to automatically learning taskadaptive image features, which often outperform manually designed features <ref type="bibr" target="#b16">[17]</ref>.</p><p>Implementing deep learning techniques in brain tumor segmentation has attracted increasing attention. For instance, Dvorak and Menze <ref type="bibr" target="#b17">[18]</ref> adopted three convolutional neural networks (CNNs) to solve binary segmentation subtasks in the multiclass brain tumor segmentation task. Pereira et al. <ref type="bibr" target="#b18">[19]</ref> modeled deep CNNs using 3×3 filters to achieve automatic brain tumor segmentation. Havaei et al. <ref type="bibr" target="#b19">[20]</ref> introduced a novel brain tumor segmentation method with a two-pathway structure to capture multiscale features and a cascaded architecture where the probability outputs of prior CNNs are used as extra inputs to subsequent CNNs. Although these deep learning-based methods efficiently adopt CNNs to attain promising performance, most of them are pointwise convolutions, which model the segmentation task as numerous classification tasks for each voxel by using local regions centered on the target voxel, namely, image patches.</p><p>The limitation of pointwise segmentation methods is that they normally suffer from high computational cost and the segmentation performance is sensitive to the size of image patches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Instead of modeling a powerful classification network to categorize numerous image patches, a more elegant architecture called fully convolutional networks (FCNs) <ref type="bibr" target="#b22">[23]</ref> was proposed to automatically generate the label map for whole input images, which solved the limitation of high computational cost and difficulty in determining the patch size for pointwise methods. Based on this architecture, a symmetric fully convolutional network named U-Net was proposed by Ronneberger et al. <ref type="bibr" target="#b20">[21]</ref>, which achieved convincing performance on medical images and has been extensively adopted in various tasks. Some researchers have employed U-Net in automated brain tumor segmentation <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. For example, Beers et al. <ref type="bibr" target="#b23">[24]</ref> proposed a 3D glioma segmentation method by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>adopting sequential U-Nets, and Dong et al. <ref type="bibr" target="#b24">[25]</ref> proposed a 2D fully automatic brain tumor segmentation network built upon the U-Net architecture. However, on account of the low memory capacity, U-Net presents limited capacity in effectively learning the feature information of the images in complicated tasks <ref type="bibr" target="#b26">[27]</ref>. This limitation leads to a need for optimizing the network structure to enlarge the parameter space, which allows the network to learn more representative features <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this paper, a novel end-to-end brain tumor segmentation method using an improved U-Net is developed for 2D brain MRI slices. We modify and improve the U-Net architecture to strengthen model capacity for yielding more accurate segmentations; see Fig. <ref type="figure" target="#fig_9">1</ref>. One important extension in our architecture is that we improve the network connectivity using an innovative structure referred to as an up skip connection. Different from previous fully convolutional architectures, the up skip connection further enhances information flow between the encoding portion and decoding portion to ensure more lowlevel features are used for optimizing the segmentation results. Moreover, the inception module <ref type="bibr" target="#b28">[29]</ref> is adopted in each block to increase the network's capacity for learning richer representations. Based on our improved fully convolutional network, a cascade of models is proposed to sequentially segment brain tumor subregions, and an effective cascaded training strategy is introduced to generate more precise segmentations for small tumor regions. Above all, in both the training and testing phases, we performed a simplified end-to-end brain tumor segmentation by using image slices. We evaluated the proposed method based on publicly available MR images obtained from the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) datasets from 2015 and 2017. The experimental results have indicated the efficiency of the proposed improvements and suggest that our approach could acquire competitive performance as state-of-the-art brain tumor segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Material and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Datasets</head><p>The proposed method is primarily validated on the image database from the The Cancer Imaging Archive (TCIA) of the NIH. There are 220 high-grade glioma (HGG) patients and 54 low-grade glioma (LGG) patients in the training dataset with professional segmentations provided as ground truth. The testing dataset consists of 110 unlabeled patients, which can be evaluated only via the challenge website. Each patient had different MRI pulses (i.e., T1-weighted (T1), T1 contrast-enhanced (T1c), T2-weighted (T2) and FLAIR), each of which is composed of 155 brain slices, for a total of 620 images per patient. All MRI images were skull stripped and coregistered to the identical anatomical template at 1 mm isotropic resolution <ref type="bibr" target="#b29">[30]</ref>. There are five labels for the imaging data in the BRATS 2015 dataset, including necrosis (labeled as 1), edema (labeled as 2), nonenhancing tumor (labeled as 3), enhancing tumor (labeled as 4) and everything else (labeled as 0). According to practical clinical applications, the evaluation of the multiclass brain tumor segmentation task is performed for different tumor subregions:</p><p>1) Complete tumor. Contains all four intratumor regions (i.e., label 1, label 2, label 3 and label 4).</p><p>2) Core tumor. Consists of three intratumor regions (i.e., label 1, label 3 and label 4).</p><p>3) Enhancing tumor. Includes an enhancing tumor (i.e., label 4)</p><p>The BRATS 2017 <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> training dataset, which contains 210 HGGs and 75 LGGs with manual segmentations, is also included to examine the performance of our proposed method. In contrast to the BRATS 2015 dataset, annotations of imaging data in BRATS 2017 comprise the GD-enhancing tumor (labeled as 4), the peritumoral edema (labeled as 2), the necrotic and nonenhancing tumor (labeled 1) and everything else (labeled as 0). Accordingly, the tumor subregions are defined as (1) complete tumor (i.e., labels 1, 2, and 4), (2) core tumor (i.e., 1, 4) and (3) enhancing tumor (i.e., 4).</p><p>In this study, each 2D brain slice is normalized to mitigate the large intensity variance among MR images by using the z-score defined as follows:</p><formula xml:id="formula_0">M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT = -<label>(1)</label></formula><p>where μ and σ represent the average value and the standard deviation of image , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Methods</head><p>In this study, the proposed end-to-end brain tumor segmentation model is built upon the U-Net architecture, which represents one of the most well-known fully convolutional network architectures for medical image segmentation <ref type="bibr" target="#b27">[28]</ref>. We extend and modify the U-Net structure by introducing a new cross-layer architecture, namely, an up skip connection, and incorporating inception modules (see Fig. <ref type="figure" target="#fig_9">1</ref>). In the training phase, we further propose a cascaded training strategy for sequentially segmenting complete, core and enhancing tumors (see Fig. <ref type="figure" target="#fig_10">2</ref>). The proposed method segments brain MR images slice by slice using all four sequences (i.e., T1, T2, T1c and FLAIR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Up skip connection</head><p>As depicted in Fig. <ref type="figure" target="#fig_9">1</ref> </p><formula xml:id="formula_1">= • + • + •<label>(2)</label></formula><p>where is the output filter of the weighted addition structure and , and represents the output feature maps of the skip connection, the up skip connection and the corresponding upsampling layer, respectively. Parameters , and are learned parameters, which denote the corresponding constraint coefficients of , and . The detailed implementation is given in Fig. <ref type="figure" target="#fig_9">s1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Inception module</head><p>To improve the representation capacity of the segmentation network and to optimize the segmentation performance, we modify the U-Net architecture with an inception module, which has been experimentally proven to enhance the capturing of more visual information under constrained computational complexity <ref type="bibr" target="#b28">[29]</ref>. As illustrated in Fig. <ref type="figure" target="#fig_9">1(c</ref>), the inception module implemented in our network is redesigned where a 7×7 convolution layer to enlarge the receptive field and the max pooling layer is replaced by a short path to directly incorporate the input filters. The output filters generated from the 1×1, 3×3 and 7×7 convolutional layers are concatenated with the input feature map to achieve feature fusion. At the same time, to speed up the convergence and further overcome the disadvantages of deep neural networks that are difficult to train, we adopt BN (batch normalization) operations to normalize the feature maps <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Cascaded training strategy</head><p>In this study, a cascade of binary segmentation networks was trained for three subtasks in segmenting brain tumor subregions (complete, core and enhancing tumor) sequentially. However, the separated process of the three tumor regions often leads to less adequate performance for core and enhancing tumor segmentation as the number</p><formula xml:id="formula_2">M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT 9</formula><p>of pixels for core and enhancing tumor is fairly small, which may impede the network to learn abundant feature representations. To partially solve this problem and to further improve the segmentation performance for the core and the enhancing tumor, we proposed an efficient cascaded training strategy inspired by the success of knowledge transfer in boosting performance <ref type="bibr" target="#b32">[33]</ref>. By the use of the proposed cascaded training strategy, the knowledge learned from the complete segmentation task is transferred to the core segmentation task, and the information learned from the core segmentation task is shared with the enhancing segmentation task. As depicted in Fig. <ref type="figure" target="#fig_10">2</ref>, the cascaded training strategy progressively optimizes the segmentation results for small tumor regions considering the inclusion relations of topologies between complete, core and enhancing tumors. In detail, the well-trained complete model is utilized as the pretrained model to initialize the core model, and the enhancing model is fine-tuned based on the well-trained core model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation metrics</head><p>In this work, we evaluate the performance of the proposed method for complete, core and enhancing tumor regions using three publicly available metrics: the Dice score, the positive predictive value (PPV) and the Sensitivity. Another widely adopted measure, the so-called Jaccard score <ref type="bibr" target="#b33">[34]</ref>, is also calculated to assess the results of our proposed method.</p><p>The Dice score measures the similarity of two segmented maps. It ranges from 0 to 1, and a higher Dice value indicates a better match. With regard to each tumor subregion,</p><p>given the binary segmentation map 0,1 obtained from predictions of segmentation models and the corresponding ground truth # 0,1 , the Dice score is defined as follows: while the Sensitivity represents a useful measure to evaluate the amount of true positives and false negatives. Accordingly, they are defined as Eq. ( <ref type="formula">4</ref>) and Eq. ( <ref type="formula" target="#formula_4">5</ref>):</p><formula xml:id="formula_3">$%&amp;' , # = 2 * | + ∩ # + | | + | + | # + |<label>(</label></formula><formula xml:id="formula_4">-'./%0%1%02 , # = | + ∩ # + | | # + | (4) 3 , # = | + ∩ # + | | + |<label>(5)</label></formula><p>The Jaccard score is one of the most popular evaluation measures in medical image segmentation <ref type="bibr" target="#b34">[35]</ref>, which can be calculated as follows:</p><formula xml:id="formula_5">45&amp;&amp;567 , # = | + ∩ # + | | + ∪ # + |<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and results</head><p>We validated our method based on the BRATS 2015 dataset by simultaneously using two GPUs and one CPU on a computing workstation equipped with 4 NVIDIA 1080Ti</p><p>GPUs and 2 Intel Xeon E5-2630 CPUs. Our proposed network was implemented using the Keras framework with a TensorFlow backend, and the detailed parameters are exhibited in Table <ref type="table" target="#tab_1">1</ref>. In this section, all models were evaluated on the testing set with 54 patients, and the quantitative results of our models along with the baseline U-Net are illustrated in Table <ref type="table" target="#tab_2">3</ref>, in which the Dice, PPV and Sensitivity metrics are presented as average values with standard deviations. These results are analyzed from different aspects:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experiments on</head><p>( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Evaluation of the proposed method</head><p>To investigate the impact of different network parameters, we trained the proposed method (the inception-based U-Net + up skip connection + cascaded training strategy) by using the BRATS 2015 training data with various batch sizes and learning rates. The segmentation results for different batch sizes (i.e., 8, 16, 32, and 64; see Table <ref type="table" target="#tab_3">4</ref>)</p><p>suggest that the batch size of 64 helps our method achieve the best segmentation performance with less time consumption. Analogously, the proposed model with a learning rate = 0.01 presents superior performance to that with a learning rate = 0.1 and a learning rate = 0.001 according to the comparison results illustrated in Table <ref type="table">5</ref>.</p><p>We present the results of six representative testing slices in Fig. Table <ref type="table" target="#tab_1">s1</ref> presents the learned parameters in the weighted addition structure associated with the up skip connection of well-trained models.</p><p>To further validate the efficiency of our method, we compared the proposed method with different deep learning-based segmentation networks, including SegNet <ref type="bibr" target="#b36">[37]</ref>, U-Net <ref type="bibr" target="#b20">[21]</ref>, PSPNet <ref type="bibr" target="#b38">[38]</ref> and 3D U-Net <ref type="bibr" target="#b39">[39]</ref>. Fig. <ref type="figure" target="#fig_21">8</ref> shows the boxplots for the different methods with various benchmarks based on the BRATS 2015 and 2017 datasets. It is evident from the results that our method achieves the best segmentation performance with regard to various tumor subregions and measurements. Table <ref type="table" target="#tab_4">6</ref> gives the mean Dice scores with standard deviations for the experiments in Fig. <ref type="figure" target="#fig_21">8</ref>, in which the statistical significance of differences was analyzed by using the Wilcoxon signed-rank test <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41]</ref>. In summary, the proposed method presents significant improvements compared with existing deep learning-based methods, giving p-values smaller than 0.01. The detailed segmentation performance for each case and their corresponding tumor size is provided in Table <ref type="table">s2</ref> and Table <ref type="table" target="#tab_2">s3</ref>. presented by <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b42">[42]</ref> on the BRATS 2015 testing data. To make full use of all 274 training cases, we implement a 5-fold cross validation procedure followed the method of Kamnitses et al. and calculate the segmentation results through the BRATS online system. To better evaluate the model efficiency, all the comparison results presented in enhancing tumors, while our method performs better in complete and core tumors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiments on Testing Data</head><p>Weighted against all evaluation metrics, the proposed method outperforms Kamnitses et al.'s method. In particular, Kamnitses et al.'s method is pointwise, while our method can automatically yield segmentation maps slice by slice. In summary, the proposed end-toend brain tumor segmentation method using improved fully convolutional networks can achieve a performance that can compete with existing brain tumor segmentation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>As an important component of diagnosing tumors, treatment planning and subsequent evaluations, accurate segmentation of gliomas have attracted enormous attention from medical doctors and researchers. Since manual depiction of tumor regions is laborious and time consuming, it is important to develop efficient computational methods for automatic brain tumor segmentation. Nevertheless, most existing deep learning methods proposed for brain tumor segmentation are voxelwise, which demands a high computational cost, and the segmentation performance is normally sensitive to the patch size <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Since fully convolutional networks have been proposed to overcome this problem by automatically generating segmentations for whole input images, incorporating this framework in brain tumor segmentation may solve the aforementioned limitations efficiently. Although efforts have been made to implement such a network architecture, especially U-Net, the segmentation performance remains to be improved due to low memory capacity <ref type="bibr" target="#b26">[27]</ref>.</p><p>Therefore, in this study, we propose a novel end-to-end brain tumor segmentation method built upon an improved U-Net for 2D brain MRI slices. Experiments were performed on the BRATS 2015 and 2017 datasets. The proposed method presents more accurate segmentation performance than most previous methods.</p><p>One important contribution of our work is that we improve the network connectivity using an innovative structure referred to as an up skip connection, which enhances the information flow between the encoding portion and the decoding portion with an associated weighted addition structure. We evaluated the segmentation performance of such architecture based on the BRATS 2015 dataset (see Table <ref type="table" target="#tab_2">3</ref>, Table <ref type="table" target="#tab_0">7</ref>, Fig. <ref type="figure" target="#fig_12">3</ref>, Fig. <ref type="figure" target="#fig_14">4</ref>, Fig. <ref type="figure" target="#fig_16">5</ref>).</p><p>Quantitative and visual results have demonstrated that the up skip connection could help the network extract more low-level features, optimizing brain tumor segmentation performance. Moreover, the inception module <ref type="bibr" target="#b28">[29]</ref> is adopted in each block to increase the network's capacity for learning richer representations. Experimental results (see Table <ref type="table" target="#tab_2">3</ref>, Fig. <ref type="figure" target="#fig_12">3</ref>, Fig. <ref type="figure" target="#fig_14">4</ref>) also indicate that our proposed inception-based U-Net could improve the proposed end-to-end method by utilizing efficient postprocessing steps is a topic of future research. Second, the proposed method with whole brain slices suffers from a data imbalance because the number of pixels for enhancing tumors is relatively small, which may worsen the segmentation performance of the trained network <ref type="bibr" target="#b42">[42]</ref>. However, pointwise segmentation methods are commonly trained on equally sampled image patches from each class, which may lead to a higher performance for enhancing tumors <ref type="bibr" target="#b16">[17]</ref>. Third, to effectively incorporate domain information in the axial, coronal and sagittal views, building a 3D fully convolutional network may further enhance the model capacity's to generate more precise segmentations. Finally, with more creative and effective architectures, such as capsule networks <ref type="bibr" target="#b43">[43]</ref> and nonlocal neural networks <ref type="bibr" target="#b44">[44]</ref>, which have been proposed and have been proven to be efficient in general image analysis tasks, a more robust brain tumor segmentation model could be constructed by incorporating these structures in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel end-to-end brain tumor segmentation method built upon an improved fully convolutional network by modifying the U-Net architecture. First, an innovative structure named the up skip connection is first proposed between the encoding portion and the decoding portion to enhance information flow. Then, an inception module is adopted in each block to help our network learn richer representations. In addition, an efficient cascaded training strategy is introduced to segment the brain tumor subregions sequentially. Different from existing patchwise methods, our model can automatically generate segmentation maps slice by slice, simplifying the experimental procedure. The proposed method was evaluated on the BRATS 2015 and the BRATS 2017 datasets.</p><p>Quantitative evaluation of our method has revealed that our end-to-end segmentation method can achieve performance that can complete with state-of-the-art brain tumor segmentation approaches, suggesting the potential of the proposed improvements in brain tumor segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>Tables:  Figures:               We implement a cascade of improved U-nets to sequentially segment brain tumor subregions. Then an efficient cascaded training strategy is introduced to improve segmentation performance for small tumor regions through transfer knowledge.</p><note type="other">Figure 1</note><p>The proposed method could achieve competitive performance as state-of-the-art brain tumor segmentation methods</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>6 Multimodal</head><label>6</label><figDesc>Brain Tumor Image Segmentation Challenge (BRATS) of 2015<ref type="bibr" target="#b29">[30]</ref>. The BRATS 2015 dataset comprises data from (1) BRATS 2012, (2) BRATS 2013, and<ref type="bibr" target="#b2">(3)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a), we enhance network connectivity through an innovative up skip connection between the downsampling path and the upsampling path to facilitate the model's capacity to learn multilevel features for accurate brain tumor segmentation. On the one hand, the up skip connection provides a new pathway between the encoding portion and the decoding portion during the forward propagation process, which allows the upsampling layers to extract more low-level features and thus helps recover spatial information lost during downsampling. On the other hand, the up skip connection improves the gradient flow in the backward propagation process, offering additional guidance for the learning of earlier layers and thus alleviating the difficulty of training the deep network. As shown in the figure, given the total number of layers N, a skip connection is typically employed to combine layer i and layer N-i+1 following conventional U-Net architectures, and our up skip connection further shares information between layer i and layer N-i+2 accordingly. A weighted addition structure is applied to automatically incorporate low-level feature maps in upsampling layers (see Fig.1(a)). To ensure that different feature maps input into the weighted addition structure possess the proposed up skip connection and the upsampling operation in the decoding path both adopt 3×3 deconvolution layers with 2×2 strides to acquire uniformsized outputs, while the skip connection utilizes a 1×1 convolution layer with a 1×1 stride to balance feature size. The weighted addition structure is graphically presented in Fig.1(b) and can be formally expressed as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10 The</head><label>10</label><figDesc>3) where | + ∩ # + | calculates the amount of elements found in both sets and| + | and | # + | are the cardinalities of the two sets (i.e., the number of pixels where = 1 and # = 1, respectively).M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT Sensitivity and the PPV are employed to evaluate the number of positive segmentations. The PPV measures the number of false positives and true positives,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Training Data 3 . 1 . 1</head><label>311</label><figDesc>Evaluating the effectiveness of the proposed improvementsDifferent experiments based on the BRATS 2015 training dataset were carried out to investigate how our proposed improvements influence tumor segmentation results with respect to the inception module, the up skip connection and the cascaded training strategy. In this study, we randomly split 274 labeled cases into a training set, validation set and testing set at a ratio of 6:2:2<ref type="bibr" target="#b35">[36]</ref>; that is, 165 samples were chosen as the training set, and 55 cases and 54 cases were chosen for the validation set and the testing set, respectively (see Table2). We also compare different implementations of our proposed network (the inception-based U-Net, inception-based U-Net + up skip connection and inception-based U-Net + up skip connection + cascaded training M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT 11 strategy) with the U-Net, which is the most relevant work in our approach and achieves state-of-the-art results for brain tumor segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 )( 2 )( 3 )</head><label>123</label><figDesc>The baseline U-Net and our inception-based U-Net. With respect to most measurements, our inception-based U-Net achieves superior segmentation performance compared to the baseline U-Net for the three tumor regions. In particular, the performance of our model in terms of the Dice score is 1.3% (complete tumor), 0.8% (core tumor) and 3.6% (enhancing tumor) higher than that of U-Net, which indicates that the proposed inception module can help improve segmentation performance. The inception-based U-Net and the inception-based U-Net + up skip connection. As we improve the inception-based U-Net with innovative up skip connection, the Dice value for the three tumors increased by 0.3%, 1.6% and 1.5%, respectively. The PPV of the inception-based U-Net with up skip connections also outperforms the inception-based U-Net. The comparison results suggest that enhancing information flow through up skip connections is efficient in generating more accurate segmentations. The inception-based U-Net + up skip connection and the inception-based U-Net + up skip connection + cascaded training strategy. The cascaded training strategy is proposed to further optimize segmentation performance for small tumor regions. The results showing that both the Dice score and the Sensitivity increased for core and enhancing tumors as we implemented a cascade training strategy demonstrate the efficiency of such a modification, suggesting the potential of a cascaded training strategy in brain tumor segmentation. As an overall analysis, Fig. 3 displays the boxplot of the calculated Dice score for all testing cases. The model tends to achieve better overall segmentation performance for the three tumor regions as we gradually implement the inception module, the up skip connection and the cascaded training strategy in U-Net. In summary, weighed against all and Sensitivity metrics, the comparison results demonstrate that the proposed improvements contribute to accurate brain tumor segmentation. The widely used Jaccard score is also used to assess and compare the segmentation results for the three tumor regions of our model and U-Net. Fig. 4 depicts bar plots of the average Jaccard score for the three tumor regions based on 54 cases in the testing set. These segmentation results further indicate that (1) both the up skip connection and the inception module implemented to build an improved U-Net architecture contribute to generating accurate brain tumor segmentations and that (2) the cascaded training strategy that learns from the transferred knowledge from the large tumor regions can help improve the performance for core and enhancing tumor. To exemplify the effect of the proposed up skip connection, segmentation results of two representative cases are plotted in Fig. 5. Segmentations generated from the inceptionbased U-Net, the inception-based U-Net with up skip connections and the ground truth are illustrated in the FLAIR images. Both our models display satisfactory conformity to the manual delineation performed by experts. However, some confusing areas are erroneously identified as target tumors by the inception-based U-Net. Implementing up skip connections mitigates these problems and improves segmentation performance, which corroborates the efficiency of the proposed up skip connection in our network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b5">6</ref>. Segmentations generated by the proposed method and the ground truth segmentations for complete, tumors are depicted in FLAIR, T2 and T1c, respectively. When the segmentation results of our model (green) are compared against the physician's manual segmentation results (red), it is evident that our improved U-Net with cascaded training strategy has the capacity to obtain accurate tumor segmentations from multimodal brain MR slices. The multiplicity and complexity of brain tumors make segmentation unusually challenging, yet our method still provides satisfactory segmentations, which demonstrates the efficiency of the proposed end-to-end method.The imaging data provided by the BRATS 2017 training dataset were further implemented to evaluate the performance of the proposed method. We randomly split the 285 labeled cases in the BRATS 2017 training dataset into a training set (171 cases), a validation set (57 cases) and a testing set (57 cases) with a ratio of 3:1:1<ref type="bibr" target="#b35">[36]</ref>. The learning curves of the proposed method based on the BRATS 2015 and 2017 imaging data are provided in Fig.7. The model that achieved the best performance on the validation set was used to evaluate segmentation performance on the testing dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>14 In</head><label>14</label><figDesc>this section, we compare the proposed models (the inception-based U-Net, inception-based U-Net + up skip connection and inception-based U-Net + up skip connection + cascaded training strategy) with existing brain tumor segmentation methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 1 (</head><label>1</label><figDesc>Fig. 1 (a) The network structure of our proposed network. This figure shows an illustration of the encoding path (left: top to bottom) and the decoding path (right: bottom to top). (b) Details of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed cascaded framework for brain tumor segmentation. The three models, from left to right, are proposed to hierarchically segment the complete tumor, the core tumor and the enhancing tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The boxplot of Dice scores based on the 54 patients in the testing set. For each tumor region, from left to right: U-Net, the inception-based U-Net, the inception-based U-Net + up skip connection and inception-based U-Net + up skip connection + cascaded training strategy. The circle represents the mean values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of Jaccard scores based on 54 patients in the testing set. The mean value is displayed as an unfilled circle. For each tumor region, from left to right: U-Net, inception-based U-Net, the inception-based U-Net + up skip connection and the inception-based U-Net + up skip connection + cascaded training strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>26 Fig. 5 .</head><label>265</label><figDesc>Fig. 5. Segmentation comparison between the inception-based U-Net (without up skip connection) and inception-based U-Net with up skip connection for two different cases: a) FLAIR slice, b) manual segmentation, where the blue region indicates an enhancing tumor, the red region indicates edema and the green region indicates necrosis and a nonenhancing tumor, c) segmentation through the inception-based U-Net, d) segmentation using the inception-based U-Net + up skip connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example of consensus expert annotation (red) and automatic segmentation of our method (green) applied to the testing set. Two cases are shown in each row. From left to right: segmentation of complete tumor (shown in FLAIR), core tumor (shown in T2) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The learning curves for our method based on the BRATS 2015 and the BRATS 2017 training datasets. (a) Complete tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 8 .Highlights</head><label>8</label><figDesc>Fig. 8. The boxplot of PSPNet (shown as 1), 3D U-Net (shown as 2), SegNet (shown as 3), U-Net (shown as 4) and the proposed method (shown as 5) based on the BRATS 2015 and the BRATS 2017 training datasets with Dice score, PPV and Sensitivity From top to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 7</head><label>7</label><figDesc>are generated without postprocessing steps. As shown in Table7, implementing the up skip connections in the inception-based U-Net increases the Dice scores by 0.6% (complete tumor), 1.7% (core tumor) and 0.9% (enhancing tumor), and further application of the cascaded training strategy helps our model achieve the best Dice score performances of 84.5%, 69.8% and 60.0% for the three tumor regions. In addition to the</figDesc><table><row><cell>achieves remarkable segmentation performance with Dice scores of 0.836 (complete</cell></row><row><cell>tumor), 0.674 (core tumor) and 0.629 (enhancing tumor), which shows advantages for</cell></row></table><note><p>Dice score, the comparison results based on the PPV and Sensitivity also show that the segmentation performance increases as we progressively implement the up skip connection and the cascaded training strategy. All these results are consistent with those in section 3.1 and further suggest that our proposed modifications and improvements are effective in generating accurate brain tumor segmentations. Quantitative results of state-of-the-art brain tumor segmentation approaches are also exhibited in this table. It is evident that the proposed method (the inception-based U-Net + up skip connection + cascaded training strategy) demonstrates superior performance on most benchmarks compared with Zhao et al.'s method. Kamnitses et al.'s method</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Hyperparameters in our proposed network.</figDesc><table><row><cell></cell><cell>Stage</cell><cell>Hyperparameter</cell><cell></cell><cell cols="2">Value</cell></row><row><cell cols="2">Initializer of</cell><cell>Bias</cell><cell></cell><cell cols="2">0</cell></row><row><cell cols="2">convolution layers</cell><cell>Weight</cell><cell></cell><cell cols="2">He_normal</cell></row><row><cell></cell><cell></cell><cell>β1</cell><cell></cell><cell cols="2">0.9</cell></row><row><cell cols="2">Adam optimizer</cell><cell>β2</cell><cell></cell><cell cols="2">0.99</cell></row><row><cell></cell><cell></cell><cell>epsilon</cell><cell></cell><cell>10</cell><cell>-8</cell></row><row><cell></cell><cell></cell><cell>learning rate</cell><cell></cell><cell cols="2">0.01</cell></row><row><cell></cell><cell></cell><cell>batch size</cell><cell></cell><cell cols="2">64</cell></row><row><cell></cell><cell>Training</cell><cell>epoch</cell><cell></cell><cell cols="2">100</cell></row><row><cell></cell><cell></cell><cell>early stopping</cell><cell></cell><cell cols="2">20</cell></row><row><cell></cell><cell></cell><cell>loss function</cell><cell>-</cell><cell cols="2">2  *  | + ∩ # + | | + | + | # + |</cell></row><row><cell cols="2">Table 2. Experiment data partitioning</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Training set Validation set Testing set</cell><cell>Total</cell></row><row><cell>HGG</cell><cell>132</cell><cell>44</cell><cell cols="2">44</cell><cell>220</cell></row><row><cell>LGG</cell><cell>33</cell><cell>11</cell><cell cols="2">10</cell><cell>54</cell></row><row><cell>Total</cell><cell>165</cell><cell>55</cell><cell cols="2">54</cell><cell>274</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The segmentation performance based on the Dice score, PPV and Sensitivity between</figDesc><table><row><cell>M A N U S C R I P T</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Experimental results for different batch sizes</figDesc><table><row><cell cols="2">Batch size</cell><cell>8</cell><cell></cell><cell>16</cell><cell></cell><cell>32</cell><cell>64</cell></row><row><cell></cell><cell cols="3">Complete 0.869(0.08)</cell><cell cols="2">0.876(0.06)</cell><cell>0.878(0.06)</cell><cell>0.890(0.06)</cell></row><row><cell>Dice score</cell><cell>Core</cell><cell cols="2">0.721(0.23)</cell><cell cols="2">0.717(0.21)</cell><cell>0.725(0.22)</cell><cell>0.733(0.22)</cell></row><row><cell></cell><cell cols="3">Enhancing 0.698(0.31)</cell><cell cols="2">0.709(0.31)</cell><cell>0.720(0.30)</cell><cell>0.726(0.31)</cell></row><row><cell cols="2">Time (s/epoch)</cell><cell>970</cell><cell></cell><cell>850</cell><cell></cell><cell>770</cell><cell>740</cell></row><row><cell cols="6">Table 5 Experimental results for different learning rates</cell></row><row><cell></cell><cell cols="2">Learning rate</cell><cell>0.1</cell><cell></cell><cell>0.01</cell><cell>0.001</cell></row><row><cell></cell><cell></cell><cell>Complete</cell><cell cols="2">0.875(0.07)</cell><cell cols="2">0.890(0.06)</cell><cell>0.890(0.06)</cell></row><row><cell></cell><cell>Dice score</cell><cell>Core</cell><cell cols="2">0.677(0.27)</cell><cell cols="2">0.733(0.22)</cell><cell>0.730(0.22)</cell></row><row><cell></cell><cell></cell><cell cols="3">Enhancing 0.682(0.34)</cell><cell cols="2">0.726(0.31)</cell><cell>0.707(0.30)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>The segmentation performance based on the Dice score between our method and previous methods using the BRATS 2015 and BRATS 2017 training datasets * Dice scores that are significantly different from our method (from the paired Wilcoxon signed-rank test with p&lt;0.01 significance level)</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell></cell><cell>Dice score</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell></row><row><cell></cell><cell>SegNet</cell><cell>0.856(0.08)**</cell><cell>0.670(0.22) **</cell><cell>0.612(0.31) **</cell></row><row><cell>BRATS 2015</cell><cell>U-Net</cell><cell>0.864(0.07) **</cell><cell>0.694(0.24) **</cell><cell>0.664(0.32) **</cell></row><row><cell></cell><cell>3D U-Net</cell><cell>0.856(0.11) **</cell><cell>0.618(0.29) **</cell><cell>0.655(0.31) **</cell></row><row><cell></cell><cell>PSPNet</cell><cell>0.820(0.07) **</cell><cell>0.639(0.24) **</cell><cell>0.575(0.27) **</cell></row><row><cell></cell><cell>Our method</cell><cell>0.890(0.06)</cell><cell>0.733(0.22)</cell><cell>0.726(0.31)</cell></row><row><cell></cell><cell>SegNet</cell><cell>0.833(0.12) **</cell><cell>0.703(0.15) **</cell><cell>0.496(0.28) **</cell></row><row><cell>BRATS 2017</cell><cell>U-Net</cell><cell>0.848(0.11) **</cell><cell>0.726(0.18) **</cell><cell>0.549(0.30) **</cell></row><row><cell></cell><cell>3D U-Net</cell><cell>0.835(0.10) **</cell><cell>0.655(0.18) **</cell><cell>0.551(0.31) **</cell></row><row><cell></cell><cell>PSPNet</cell><cell>0.809(0.11) **</cell><cell>0.701(0.15) **</cell><cell>0.554(0.26) **</cell></row><row><cell></cell><cell>Our method</cell><cell>0.876(0.09)</cell><cell>0.763(0.13)</cell><cell>0.642(0.28)</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Average performance of our system on the 110 test cases of BRATS 2015, as computed on the online evaluation platform.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Dice score</cell><cell></cell><cell></cell><cell>PPV</cell><cell></cell><cell></cell><cell>Sensitivity</cell><cell></cell></row><row><cell></cell><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell><cell>Complete</cell><cell>Core</cell><cell>Enhancing</cell></row><row><cell>Zhao et al. (2017)</cell><cell>0.80(-)</cell><cell>0.66(-)</cell><cell>0.57(-)</cell><cell>0.81(-)</cell><cell>0.79(-)</cell><cell>0.50(-)</cell><cell>0.83(-)</cell><cell>0.64(-)</cell><cell>0.72(-)</cell></row><row><cell>Kamnitsas et al. (2016)</cell><cell>0.836(-)</cell><cell>0.674(-)</cell><cell>0.629(-)</cell><cell>0.823(-)</cell><cell>0.846(-)</cell><cell>0.64(-)</cell><cell>0.885(-)</cell><cell>0.616(-)</cell><cell>0.656(-)</cell></row><row><cell>Inception-based U-Net</cell><cell cols="9">0.838(0.15) 0.676(0.23) 0.586(0.32) 0.838(0.17) 0.784(0.23) 0.611(0.29) 0.873(0.18) 0.667(0.24) 0.637(0.33)</cell></row><row><cell cols="10">Inception-based U-Net + up skip connection 0.844(0.14) 0.693(0.22) 0.595(0.32) 0.835(0.16) 0.767(0.22) 0.596(0.29) 0.884(0.17) 0.700(0.22) 0.661(0.32)</cell></row></table><note><p>Inception-based U-Net + up skip connection + cascaded training strategy 0.845(0.14) 0.698(0.21) 0.600(0.32) 0.837(0.17) 0.791(0.22) 0.608(0.29) 0.885(17) 0.690(0.19) 0.659(0.32)</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Natural Science Foundation of China through Grant No. 61571414, No. 61471331 and No. 31100955.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>16 segmentation performance in contrast to the baseline U-Net. The last contribution of this work is the cascaded training strategy, which effectively makes use of transferred knowledge among the three tumor regions to optimize the segmentation performance of core and enhancing tumors. Accordingly, the comparison results (see Table <ref type="table">3</ref>, Table <ref type="table">7</ref>, Fig. <ref type="figure">3</ref>, Fig. <ref type="figure">4</ref>) indicate the potential of utilizing the cascaded training strategy to enhance the segmentation performance for small tumor regions.</p><p>The proposed method, which implements all of these effective modifications and improvements, is further evaluated based on both the BRATS 2015 and 2017 datasets.</p><p>First, the proposed method was validated with various batch sizes and learning rates to investigate the impact of different parameters (see Table <ref type="table">4</ref>, Table <ref type="table">5</ref>). The comparison results indicate that (1) a larger batch size leads to better segmentation performance and</p><p>(2) the model with a learning rate = 0.01 achieves the best performance for our model. Second, the learning curves of the proposed method based on the BRATS 2015 and 2017 imaging data are provided in Fig. <ref type="figure">7</ref> to show the convergence progress. The learned parameters of the weighted addition structure in well-trained models are also depicted in Table <ref type="table">s1</ref>, reflecting the self-learning parameter optimization results for different blocks in the decoding portion. Third, we compared the proposed method with different deep learning-based segmentation networks, including SegNet <ref type="bibr" target="#b36">[37]</ref>, U-Net <ref type="bibr" target="#b20">[21]</ref>, PSPNet <ref type="bibr" target="#b38">[38]</ref> and 3D U-Net <ref type="bibr" target="#b39">[39]</ref> (see Table <ref type="table">6</ref> and Fig. <ref type="figure">8</ref>). The statistical test results further indicate that the proposed method presents significant improvements compared with existing deep learningbased methods. Finally, our method is evaluated by using the online system for the BRATS 2015 testing dataset (see Table <ref type="table">7</ref>). The proposed method achieves 0.845 (complete tumor), 0.698 (core tumor) and 0.600 (enhancing tumor) based on Dice scores, suggesting that our method can achieve a performance that can compete with existing brain tumor segmentation methods.</p><p>Despite the success of our approach, there also exist some limitations in our method.</p><p>First, the postprocessing steps <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">42]</ref>, such as conditional random fields (CRF), have been proven to be efficient in optimizing the segmenting results based on spatial consistency <ref type="bibr" target="#b42">[42]</ref>. Therefore, an improvement of in segmentation performance of the </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated brain tumour detection and segmentation using superpixel-based extremely randomized trees in FLAIR MRI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soltaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lambrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Barrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-016-1483-3</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="183" to="203" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervised learning based multimodal MRI brain tumour segmentation using texture features from supervoxels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soltaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lambrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Barrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2018.01.003</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of MRI-based medical image analysis for brain tumor studies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2013" to="2R97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic Semantic Segmentation of Brain Gliomas from MRI Images Using a Deep Cascaded Neural Network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Healthc. Eng</title>
		<imprint>
			<date type="published" when="2018">2018 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Molecular diagnostics of gliomas: the clinical perspective</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tabatabai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Bent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tonn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Neuropathol. Berl</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="585" to="592" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Atlas-based segmentation of pathological MR brain images using a model of lesion growth</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Cuadra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bardera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cuisenaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Villemure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1301" to="1314" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A brain tumor segmentation framework based on outlier detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A generative model for brain tumor segmentation in multi-modal images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Leemput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lashkari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput.-Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="151" to="159" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ilastik for multi-modal brain tumor segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI BraTS Brain Tumor Segmentation Chall</title>
		<meeting>MICCAI BraTS Brain Tumor Segmentation Chall</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully automatic segmentation of brain tumor images using support vector machine classification in combination with hierarchical conditional random field regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput.-Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="354" to="361" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Extremely randomized trees based brain tumor segmentation, Proceeding BRATS Chall</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bloecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stieltjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Meinzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno>MICCAI. 2014 006-011</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Appearance-and context-sensitive features for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI BRATS Chall</title>
		<meeting>MICCAI BRATS Chall</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="20" to="026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Segmenting brain tumors using alignment-based features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Levner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bistritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Int. Conf. On, IEEE</title>
		<meeting>Fourth Int. Conf. On, IEEE</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MRI Brain Tumor Segmentation and Patient Survival Prediction Using Random Forests and Fully Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soltaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lambrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brainlesion Glioma Mult. Scler. Stroke Trauma. Brain Inj</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="204" to="215" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Soltaninejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lambrou</surname></persName>
		</author>
		<title level="m">Brain Tumour Grading in Different MRI Protocols using SVM on Statistical Features</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic Gabor and MRF segmentation of brain tumours in MRI volumes</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Subbanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput.-Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="751" to="758" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F J</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2016.10.004</idno>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Structured prediction with convolutional neural networks for multimodal brain tumor segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dvorak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2016.2538465</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Med. Image Comput. Comput.-Assist. Interv</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial network with multi-scale l 1 loss for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Segan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequential 3D U-Nets for Biologically-Informed Brain Tumor Segmentation, ArXiv Prepr</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mammen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<idno>ArXiv170902967. 2017</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic brain tumor detection and segmentation using U-Net based fully convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Conf. Med. Image Underst. Anal</title>
		<imprint>
			<biblScope unit="page" from="506" to="517" />
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">3D Convolutional Neural Network for Brain Tumor Segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Erden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gamboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with improved U-net</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Comput. Intell. ICACI 2018 Tenth Int. Conf. On, IEEE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="402" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DRINet for Medical Image Segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Multimodal Brain Tumor Image Segmentation Benchmark BRATS</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lanczi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buendia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cordier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Durst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dojat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Festa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Geremia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hamamci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Iftekharuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lashkari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Mariz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Raviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Subbanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wintermark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Leemput</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2014.2377694</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2024">2015 1993-2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features, Sci</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2017.117</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">170117</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift, ArXiv Prepr</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>ArXiv150203167. 2015</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: Full training or fine tuning?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The distribution of the flora in the alpine zone. 1, New Phytol</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912">1912</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>ArXiv151100561</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cs</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.00561accessed" />
		<imprint>
			<date type="published" when="2015-01-07">2015. January 7, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1612" />
	</analytic>
	<monogr>
		<title level="m">ArXiv161201105 Cs</title>
		<imprint>
			<date type="published" when="1105-01-07">2016. 01105 accessed January 7, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.06650" />
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2016-02-09">160606650 Cs. 2016. February 9, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully Convolutional Network Ensembles for White Matter Hyperintensities Segmentation in MR Images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2018.07.005</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page" from="650" to="665" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automated segmentation and volumetric analysis of renal cortex, medulla, and pelvis based on non-contrast-enhanced T1-and T2-weighted MR images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martirosian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Würslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schick</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10334-014-0429-4</idno>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Mater. Phys. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="445" to="454" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A deep learning model integrating FCNNs and CRFs for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="98" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamic routing between capsules</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks, in: IEEE Conf. Comput. Vis. Pattern Recognit. CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
