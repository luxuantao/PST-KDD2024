<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comprehensive Multi-Modal Interactions for Referring Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kanishk</forename><surname>Jain</surname></persName>
							<email>kanishk5991@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology</orgName>
								<orgName type="institution" key="instit1">KCIS</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vineet</forename><surname>Gandhi</surname></persName>
							<email>vgandhi@iiit.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Visual Information Technology</orgName>
								<orgName type="institution" key="instit1">KCIS</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Comprehensive Multi-Modal Interactions for Referring Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the given natural language description. To solve RIS efficiently, we need to understand each word's relationship with other words, each region in the image to other regions, and cross-modal alignment between linguistic and visual domains. Recent methods model these three types of interactions sequentially. We argue that such a modular approach limits these methods' performance, and joint simultaneous reasoning can help resolve ambiguities. To this end, we propose a Joint Reasoning (JRM) module and a novel Cross-Modal Multi-Level Fusion (CMMLF) module for tackling this task. JRM effectively models the referent's multi-modal context by jointly reasoning over visual and linguistic modalities (performing word-word, image region-region, word-region interactions in a single module). CMMLF module further refines the segmentation masks by exchanging contextual information across visual hierarchy through linguistic features acting as a bridge. We present thorough ablation studies and validate our approach's performance on four benchmark datasets, and show that the proposed method outperforms the existing state-of-the-art methods on all four datasets by significant margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fundamental computer vision tasks related to localization, like detection and segmentation, aim to grant computers' visual abilities comparable to humans. Traditionally, these tasks have dealt with a pre-defined set of categories, making them difficult to scale and limit their practical use. Substituting the pre-defined categories with natural language expressions is a logical extension to counteract the above problems. Indeed, this is how humans interact with objects in their environment by referring them with linguistic queries. For example, the phrase "the kid running after "anywhere, not on the people" "store on left, next to hats, with blanket draped in front" Original Image CMPC Our Approach Ground Truth Figure <ref type="figure">1</ref>. Comparison of the proposed approach with CMPC <ref type="bibr" target="#b7">[8]</ref>.</p><p>In both the examples CMPC fails at the first stage itself, where it completely misses the actual referred entity. Our approach performs the exhaustive forms of interactions in a single step and identifies the correct referred entity. Best viewed in color and under zoom.</p><p>the butterfly" requires localizing only the child running after the butterfly and not the other kids. Formally, the task of localizing objects based on natural language expression is known as Visual Grounding. Existing works approach the grounding problem either by predicting a bounding box around the referred object or by predicting a segmentation mask corresponding to the referred object. In this paper, we focus on the latter approach, as a segmentation mask can effectively pinpoint the exact location and capture the actual shape of the referred object. The task is formally known as Referring Image Segmentation (RIS). RIS task requires understanding both visual and linguistic modalities at an individual level, specifically word-word and region-region interactions. Additionally, a joint understanding of both modalities is required to identify the referred object from the linguistic expression and localizing it in the image. For instance, to ground a sentence "whatever is on the truck", it is necessary to understand the relation-arXiv:2104.10412v1 [cs.CV] 21 Apr 2021 ship between words as grounding just the individual words will not work. Similarly, region to region interactions in visual modality help group semantically similar regions, ex: all regions belonging to the truck. Finally, to identify the referent regions, we need to transfer the distinctive information about the referent from the linguistic modality to the visual modality; this is taken care of by the cross-modal word-region interactions. The current state-of-the-art methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref> take a modular approach to the RIS task, where these interactions happen in parts, sequentially.</p><p>Different methods differ in how they model these interactions. Huang et al. <ref type="bibr" target="#b7">[8]</ref> first perform a region-word alignment (cross modal interaction). The second stage takes these region word alignments as input and selects the final relevant regions by reasoning over the entire linguistic expression. The reasoning step exploits the relationship and attributes corresponding to the referent in the textual expression. For example, for the sentence "The man holding a white Frisbee", the first stage will localize all instances of "man" and "Frisbee," and the second stage would select the correct instance of the "man" associated with a "white Frisbee". They use a Graph Convolutional Network for relational reasoning. Hui et al. <ref type="bibr" target="#b8">[9]</ref> uses the dependency tree structure of the referring expression for the reasoning stage instead. Hu et al. <ref type="bibr" target="#b6">[7]</ref> take a slightly different approach; instead of selecting the relevant region for each word, they select a relevant combination of words for each region. The second stage selects the relevant regions corresponding to referent based on the affinities with other regions. The problem with these approaches is that they model different forms of interactions in different stages. As a result, errors in the first stage of interaction limit the performance of subsequent ones (bottom row of Figure <ref type="figure">1</ref>). The sequential interactions are also limited by design, as some RIS instances ideally require to model these interactions simultaneously (top row of Figure <ref type="figure">1</ref>).</p><p>In this paper, we propose to perform all three forms of interactions simultaneously. We propose a Joint Reasoning Module (JRM) which jointly models inter-modal interactions and intra-modal interactions between the visual and linguistic modalities. Inter-modal interactions handle the cases for identifying the semantically similar words and regions in both modalities. Intra-modal interactions are used to transfer the contextual information between modalities to identify the referential context. Additionally, we propose a novel CMMLF module to exchange contextual information for referent across modalities and visual hierarchies and refine the referred object's segmentation mask.</p><p>We motivate the benefits of simultaneous interaction over the sequential interactions in Figure <ref type="figure">1</ref>. We use the best performing CMPC <ref type="bibr" target="#b7">[8]</ref> which first perceives all entities from the expression and individually aligns them in the visual domain. A reasoning step then follows. In both examples, CMPC fails at the first stage of entity perception. For the prediction in the top row (Figure <ref type="figure">1</ref>), the sentence is to be understood as a whole, since referred entity is not explicitly mentioned in the expression. CMPC identifies "people" as the only present entity and ends up giving a wrong prediction. Similarly, in the second row (Figure <ref type="figure">1</ref>), the expression is "store on left, next to hats with blanket draped in front". Both the scene and expression used are complex, as a lot of closely cluttered objects are there in the scene, and the language used to describe the referent uses complex relations between linguistic words. The first stage of CMPC predicts "hats" and "blankets" as entities and completely misses the actual referred object "store". In both cases, our approach with exhaustive interactions is able to understand the essence of the textual expression and reason about the referred object in the visual modality. Overall, our work makes the following contributions:-1. We propose a Joint Reasoning Module (JRM) to jointly reason over regions; words, and region-word features. Joint Reasoning allows each modality to focus on semantic information common to both modalities to identify the referred object. 2. We propose Cross-Modal Multi-level Fusion (CMMLF) module, which allows contextual information to be exchanged across visual hierarchies through linguistic features, enabling common semantic information for referent to be aggregated from different visual hierarchies and result in a refined segmentation mask. 3. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our approach and show notable performance gains against current state-of-the-art methods on four RIS benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Segmentation</head><p>In semantic segmentation, the goal is to predict a label for each pixel in the image. Introduction of Fully Convolution Networks <ref type="bibr" target="#b13">[14]</ref> led to a significant breakthrough in Semantic Segmentation. FCN replaces the fully connected layer in classification networks with convolutional layers and introduces skip connection for generating dense predictions for pixel-wise labels. DeepLab and its variants <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> introduce atrous Convolution to enlarge the receptive field of convolutional filters and aggregate multi-scale context using atrous spatial pyramid pooling. PSPNet <ref type="bibr" target="#b28">[29]</ref> performs region-based context aggregation through pyramid pooling to extract multi-scale context. DANet <ref type="bibr" target="#b3">[4]</ref> utilizes channel and position attention to adaptively integrate local features with their global dependencies. Recent works like ResNeSt <ref type="bibr" target="#b27">[28]</ref> and HRNet-OCR <ref type="bibr" target="#b22">[23]</ref> use attention-based approaches to combine information across feature map groups and to combine multi-scale predictions, respectively. The task of RIS is a more generalized and natural variant of semantic segmentation where natural language referring expressions replace the predefined set of object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Referring Expression Comprehension</head><p>Referring Expression Comprehension (REC) aims to localize the entities in the image referred to by the referring expression. In the REC task, the localization is performed using bounding box proposals. Existing approaches in REC can be categorized into two groups based on the model pipeline, (1) two-stage methods and (2) one-stage methods. In the two-stage methods, the first stage utilizes a pre-trained object detector to generate candidate bounding boxes for the given image, and the second stage selects the bounding box relevant to the object referred by the natural language expression. All the existing two-stage methods differ in their approaches for selecting the relevant bounding box proposal in the second stage. Earlier works like <ref type="bibr" target="#b5">[6]</ref> used a scoring function on candidate boxes based on text query, and <ref type="bibr" target="#b19">[20]</ref> use an attention mechanism for selecting the bounding box. Recent Works like <ref type="bibr" target="#b23">[24]</ref> use cross-modal attention to model relations between language and vision modalities, followed by Graph Convolutional Network to perform relational reasoning to select the correct bounding box. In contrast to two-stage methods, one-stage methods combine the proposal generation network with the proposal selection network to create an end-to-end trainable network. <ref type="bibr" target="#b24">[25]</ref> performs single-stage localization by augmenting the object detector with textual features. ZSGNet <ref type="bibr" target="#b20">[21]</ref> combines the detector network and the grounding network and predicts classification scores and regression parameters for the candidate bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Referring Image Segmentation</head><p>Bounding Box based methods in REC are limited in their capabilities to capture the inherent shape of the referred object and are known to struggle with multi-scale objects. Referring Image Segmentation (RIS) task was proposed to alleviate the problems associated with REC tasks. RIS task was first introduced in <ref type="bibr" target="#b4">[5]</ref>, where they generate the referent's segmentation mask by directly concatenating visual features from CNN with tiled language features from LSTM. Later works like <ref type="bibr" target="#b12">[13]</ref>, perform sequential reasoning over individual words and visual regions through a convolutional multi-modal LSTM. <ref type="bibr" target="#b10">[11]</ref> proposed Recurrent Refinement Networks (RRN) to generate refined segmentation masks by incorporate multi-scale semantic information from the image. Since each word in expression makes a different contribution to identify the desired object, <ref type="bibr" target="#b21">[22]</ref> model visual context for each word separately using query attention. <ref type="bibr" target="#b25">[26]</ref> uses a self-attention mechanism to capture long-range correlations between visual and textual modali-ties. Recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> utilize cross-modal attention to model multi-modal context, <ref type="bibr" target="#b8">[9]</ref> use dependency tree structure and <ref type="bibr" target="#b7">[8]</ref> use coarse labelling for each word in the expression for selective context modelling. Most of the existing works capture only a subset of multi-modal interactions to model the context for referent. In this work, we concurrently and comprehensively model the intra-modal and inter-modal interactions across visual and linguistic modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given an image and a natural language referring expression, the goal is to predict a pixel-level segmentation mask corresponding to the referred entity described by the expression. The overall architecture of the network is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>. Visual features for the image are extracted using a CNN backbone, and linguistic features for the referring expression are extracted using a LSTM. A Joint Reasoning Module (JRM) simultaneously aligns visual regions with textual words and jointly reasons about both modalities to identify the multi-modal context relevant to the referent. JRM is applied to hierarchical visual features extracted from CNN backbone since hierarchical features are better suited for segmentation tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>. A novel Cross-Modal Multi-Level Fusion (CMMLF) is applied to effectively fuse JRM's multi-level output and produce a refined segmentation mask for the referent. We describe the feature extraction process in the next section, and both JRM and CMMLF modules are described in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>Our network takes an image and a natural language expression as input. We extract hierarchical visual features for an image from a CNN backbone. All hierarchical visual features are transformed to the same spatial resolution and channel dimension through pooling and convolution operations. Final visual features for each level are of shape R Cv×H×W , with H, W and C v being the height, width and channel dimension of the visual features. Final visual features are denoted as {V 2 , V 3 , V 4 }, corresponding to layers 2, 3 and 4 of the CNN backbone. For ease of readability, we denote the visual features as V . We first initialize each word with a pre-trained word-embedding for the linguistic expression, which are then passed as input to the LSTM encoder. The hidden feature of LSTM at i th time step l i ∈ R C l , is used to denote the word feature for the i th word in the expression. The final linguistic feature of the expression is denoted as L = {l 1 , l 2 , ..., l T }, where T is the number of words in the referring expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Reasoning Module</head><p>In this section, we describe the Joint Reasoning Module (JRM). To successfully segment the referent, we need to identify the semantic information relevant to it in both the visual and linguistic modalities. This requires identifying region-region, word-word, and region-word pairs with similar contextual information. We model JRM as a multi-modal transformer encoder to capture the inter-modal and intra-modal interactions between visual and linguistic modalities. JRM is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>Hierarchical visual features V ∈ R Cv×H×W and linguistic word-level features L ∈ R C l ×T are passed as input to JRM, with C v = C l = C. We add separate positional embeddings to visual and linguistic features. For the visual features, we add spatially aware positional embedding S V of shape R C×H×W , and for linguistic features, we add length aware positional embeddings S l of shape R C×T .</p><formula xml:id="formula_0">V p = V + S v (1) L p = L + S l<label>(2)</label></formula><p>Here, V p and L p are the same shape as V and L, respectively. Following this, we flatten the spatial dimensions of visual features V p and perform a length-wise concatenation with the linguistic features L p to get a multi-modal feature M of shape R C×(HW +T ) . M is passed as input to the multi-modal transformer encoder. The self-attention mechanism in the encoder captures region-region and wordword interactions to identify similarly related regions and similarly related words. Further, region-word interactions help in reasoning about the referent by selecting regions and words with similar semantic context relevant to the referent. The output of JRM is a multi-modal feature X with cross-modal contextual information for the referent. X is the same shape as M . We compute X for all hierarchical visual features {V 2 , V 3 , V 4 }, resulting in hierarchical crossmodal output {X 2 , X 3 , X 4 }. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross Modal Multi-level Fusion</head><p>Since features from different hierarchies in the CNN capture different aspects of the image, the input to JRM will differ in the visual information, as a result visual contextual information captured in X i 's will be different. In order to predict a refined segmentation mask for the referent, we need to aggregate the relevant contextual information from all hierarchies effectively. We propose a novel cross-modal multi-level fusion (CMMLF) module to address this.</p><p>The input to CMMLF module are the multi-modal features X i s from JRM. Since each X i has shape of R C×(HW +T ) , they contain contextual information from both modalities. First, we separate the visual and linguistic context from X i s to get visual features with linguistic context X v i ∈ R C×HW , and linguistic features with visual context X l i ∈ R C×T . X l i is averaged along the length dimension to result in a global visually attended linguistic feature X i lavg . Because of the hierarchical visual features, the visual context captured by each X i lavg is different. We utilize this aspect to use these linguistic features as a bridge to exchange visual information with other hierarchies. We take visual features from one hierarchy and concatenate it with linguistic part attended at the other hierarchies. More specifically, we take i th layer output's visual part X v i and concatenate it with tiled textual part X j lavg of a different j th layer along channel dimension. The concatenation is done separately with each of the remaining two layers. The full procedure is described in Figure <ref type="figure" target="#fig_2">4</ref>.The visual contextual information is aggregated in the following way:-</p><formula xml:id="formula_1">Λ ij = σ(Conv([X i v ; X j lavg ]))<label>(3)</label></formula><formula xml:id="formula_2">Y i = X i v + j∈{2,3,4}/{i} Λ ij X i v<label>(4)</label></formula><p>Here Λ ij ∈ R C×H×W are similarity weights between the i th and j th level hierarchies, and Y i ∈ R C×H×W is a refined multi-modal feature with visual context from other hierarchies. Finally, Y i 's are fused by stacking them along new dimension, resulting in R 3×C×H×W dimensional vector, which is passed through 3-D Convolution to aggregate visual information from multiple levels to result in final refined multi-modal feature Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mask Generation</head><p>Finally, Y is passed through Atrous Spatial Pyramid Pooling (ASPP) decoder and Up-sampling convolution to predict final segmentation mask S. Pixel-level binary crossentropy loss is applied to predicted segmentation map S and the ground truth segmentation mask G to train the entire network end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Experimental Setup</head><p>We conduct experiments on four Referring Image Segmentation datasets: UNC <ref type="bibr" target="#b26">[27]</ref>, UNC+ <ref type="bibr" target="#b26">[27]</ref>, G-Ref <ref type="bibr" target="#b14">[15]</ref> and Referit <ref type="bibr" target="#b9">[10]</ref>. We describe each dataset separately.</p><p>UNC: The UNC dataset contains 19,994 images taken from MS-COCO <ref type="bibr" target="#b11">[12]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We adopt DeepLabv3+ <ref type="bibr" target="#b2">[3]</ref> with Resnet-101 as a backbone for image feature extraction. Like previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>, our CNN backbone is pre-trained on Pascal VOC, and its parameters are fixed during training. For multi-level features, we extract features from layers 2, 3 and 4 of the CNN backbone. We conduct experiments with images at spatial resolutions of 448 × 448 and 576 × 576. At 448 × 448 resolution, H = W = 14 and at 576 × 576 resolution, H = W = 18. We use GLoVe embeddings <ref type="bibr" target="#b16">[17]</ref> pre-trained on Common Crawl 840B tokens to initialize word embedding for words in the expressions. The maximum number of words in the linguistic expression is set to 25. We use LSTM for extracting textual features. The network is trained using Adam optimizer with weight decay (AdamW) with batch size set to 50; the initial learning rate is set to 2.5e −4 and weight decay of 5e −4 is used. The initial learning rate is gradually decreased using polynomial decay with a power of 0.5.</p><p>Evaluation Metrics: Following previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>, we evaluate the performance of our model using overall Intersection-over-Union (overall IoU) and Precision@X as metrics. Overall IoU metric calculates the ratio of the intersection and the union computed between the predicted segmentation mask and the ground truth mask over all test samples. Precision@X metric calculates the percentage of test samples having IoU greater than the threshold X, with X ∈ {0.5, 0.6, 0.7, 0.8, 0.9}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State of the Art</head><p>We evaluate our method's performance on four benchmark datasets and present the results in Table <ref type="table" target="#tab_0">1</ref>. Like previous works, we use the Overall IoU metric to compare the performance against other state-of-the-art methods. At 576 × 576 input resolution, we outperform the existing methods by significant margins and achieve stateof-the-art numbers on all four datasets. Our method also achieves superior performance on three of the dataset at the 448 × 448 resolution. Most previous methods present results after post-processing the segmentation maps through a Dense Conditional Random Field (Dense CRF). In contrast, the presented results of our approach are without any such post-processing.</p><p>The expressions in UNC+ avoid using positional words while referring to objects; instead, they are more descriptive about the object's attributes and relationships. Substantial performance gains on the UNC+ dataset at all splits showcases the effectiveness of utilizing comprehensive interactions simultaneously across visual and linguistic modalities. Similarly, our approach gains 1.46-1.88% over the next best performing method LSCM <ref type="bibr" target="#b8">[9]</ref> on the Referit dataset, reflecting its ability to ground unstructured regions (e.g., the sky, free space). We also achieve solid performance gains on the UNC dataset at both resolutions, indicating that our method is able to resolve among multiple instances of the same type of objects and effectively locate the referred one.</p><p>The performance gains on the G-Ref dataset are marginal, achieving an improvement of 0.33% over CMPC. G-Ref is a relatively complex dataset with longer and verbose referring expressions (the average sentence contains more than 8 words). The results suggest scope for better modeling of the longer sentences. We experimented with contextual embeddings like ELMo <ref type="bibr" target="#b17">[18]</ref> instead of the GLoVe; however, that did not improve the performance.</p><p>Our approach also achieves the highest gains on Precision@X metric on all datasets, specifically for X = 0.9. Our best performing model (with two encoder layers in JRM at resolution 576 × 576) gives 18.31% score in Precision@0.9 metric, compared to 12.89 of CMPC, achieving an improvement of 5.41%. More comprehensive evaluation results on Precision@X metric are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We perform ablation studies on the UNC dataset's validation split to validate the effectiveness of different modules in the proposed architecture. All methods are evaluated on Precision@X and Overall IoU metrics and the results are illustrated in Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref>. All ablations are performed at an input resolution of 448 × 448. The feature extraction process described in Section 3.1 is used for all ablation studies. ASPP + ConvUpsample decoder is also common to all the experiments.</p><p>The baseline model involves direct concatenation of visual features with the tiled textual feature to result in multimodal feature of shape R (Cv+Ct)×H×W . This multi-modal feature is passed as input to ASPP + ConvUpsample decoder. The baseline model achieves a better Overall IOU score than some of the older methods like DMN <ref type="bibr" target="#b15">[16]</ref> and ASGN <ref type="bibr" target="#b18">[19]</ref>.</p><p>CMMLF without JRM: "Only CMMLF" network differs with baseline method only on the fusion process of hierarchical multi-modal features. Introducing the CMMLF module over baseline results in 4.83 % improvement on the Method prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0. Overall IoU metric and an improvement of 2.5 % on the prec@0.9 metric (illustrated in Table <ref type="table" target="#tab_1">2</ref>), indicating that the CMMLF module results in refined segmentation masks.</p><p>JRM without CMMLF: Similarly, the "Only JRM" network differs from the baseline method in the way different types of visual-linguistic interactions are captured. We observe significant performance gains of 7.46 % over the baseline, validating our claim that joint reasoning helps identify the referent.</p><p>JRM + X: We replace CMMLF module with other multilevel fusion techniques like ConvLSTM and Conv3D. Com-paring the performance of JRM+ConvLSTM with JRNet (JRM+CMMLF), we observe that CMMLF is indeed effective at fusing hierarchical multi-modal features (Table <ref type="table" target="#tab_1">2</ref>). For JRM+Conv3D, we stack multi-level features along a new depth dimension resulting in 3D features, and perform 3D convolution on them. The same filter is applied to different level features that result in each level feature converging on a common region in the image. JRM+Conv3D achieves a similar performance as JRM+ConvLSTM while using fewer parameters. Using Conv3D achieves higher Precision@0.8 and Precision@0.9 than ConvLSTM, suggesting that it leads to more refined maps. It is worth noting that CMMLF also uses Conv3D at the end, and the additional gains of JRNet over JRM+Conv3D suggest the benefits of hierarchical information exchange in CMMLF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glove and Positional Embeddings:</head><p>We verify Glove embeddings' significance by replacing it with one hot embedding. We also validate the usefulness of Positional Embeddings (P.E.) by training a model without them. Both variants observe a drop in performance (Table <ref type="table" target="#tab_1">2</ref>), with the drop being more significant in the variant without Glove embeddings. These ablations suggest the importance of capturing word-level semantics and positional-aware features. # of encoder layers in JRM: In Table <ref type="table" target="#tab_2">3</ref>, we present ablations on the UNC dataset by varying the number of encoder layers in JRM. We use the full model (JRNet) and find that a two-layer encoder gives the best performance on all UNC dataset splits. Increasing the number of layers deteriorates the performance gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>Figure <ref type="figure">5</ref> presents qualitative results comparing the baseline model against JRNet. JRNet is able to localize heavily occluded objects (Figure <ref type="figure">5</ref> (a) and (b)) and reason on the overall essence of the highly ambiguous sentences (e.g. "person you cannot see", "right photo not left photo") and ground them. It is able to distinguish among multiple instances of the same type of object based on attributes and appearance cues (Figure <ref type="figure">5</ref> (b), (c), and (e)). In contrast, the baseline model struggles to segment the correct instance and confuses it with other similar objects (e.g., fails to distinguish among different animals, bowls, and the two refrigerators). Figure <ref type="figure">5</ref> (d) and (f) illustrate the ability of JRNet to localize unstructured non-explicit objects like "dark area" and "blue thing". The potential of JRNet to perform relative positional reasoning is highlighted in Figure <ref type="figure">5</ref> (b), (e), and (f).</p><p>To further highlight the contribution of both JRM and CMMLF modules, we present qualitative results with networks trained using "Only CMMLF", "Only JRM" and JR-Net in Figure <ref type="figure">6</ref>. "Only CMMLF" network does not involve any reasoning; however, it manages to predict the left sand-wich with refined boundaries. "Only JRM" network is able to understand the concept of "the right half of the sandwich" and leads to much better output; however, the output mask bleeds around the boundaries, and an extra small noisy segment is also seen. The full model benefits from the reasoning in "JRM," and when combined with CMMLF, it further facilitates information exchange across hierarchies and predicts a correct and refined mask as output.</p><p>In Figure <ref type="figure">7</ref>, we anchor an image and make predictions by varying the natural language expression. Our approach is able to correctly segment all of the instances, clearly highlighting the flexibility and adaptability of the proposed JR-Net model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we tackled the task of Referring Image Segmentation. We proposed to solve this problem by comprehensively capturing interactions between different words in the linguistic expression, different regions of the image, and cross-modal interactions between words and image regions, in a single step. Furthermore, we introduced a novel fusion module, CMMLF, that fuses hierarchical multi-modal features by effectively exchanging and aggregating the contextual information relevant to the referent. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our method. The proposed JRNet achieves substantial gains over the state-of-the-art on all the four commonly used RIS benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The proposed network architecture.</figDesc><graphic url="image-9.png" coords="4,50.11,72.00,495.01,161.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Joint Reasoning Module</figDesc><graphic url="image-10.png" coords="4,327.99,277.04,197.99,244.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Cross Modal Multi Level Fusion Module</figDesc><graphic url="image-11.png" coords="5,88.37,72.00,198.00,141.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>with 142,209 referring expressions corresponding to 50,000 objects. Referring Expressions for this dataset contain words indicating the location of the object. Two or more objects of the same object category appear in each image. UNC+: THE UNC+ dataset is also based on images from MS-COCO. It contains 19,992 images, with 141,564 referring expressions corresponding to 50,000 objects. Unlike UNC, this dataset does not contain words that indicate the object's location, and the expression describes the object based on their appearance and context within the scene. G-Ref: Like UNC and UNC+, G-Ref is also curated using images from MS-COCO. It contains 26,711 images, with 104,560 referring expressions for 50,000 objects. Each image contains 2 to 4 objects of the same category. G-Ref contains longer sentences with an average length of 8.4 words; compared to G-Ref, other datasets have an average sentence length of less than 4 words. Referit: Referit dataset comprises of 19,894 images collected from IAPR TC-12 dataset. It includes 130,525 expressions for 96,654 objects. The expressions are shorter compared to other datasets. The foreground regions consist of objects and stuff (e.g., sky, mountains, and ground).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>"Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. We present qualitative results corresponding to combinations of proposed modules. In (b) we show results when only CMMLF module is used, (c) result with only JRM module being used, (d) output mask when both JRM and CMMLF modules are used</figDesc><graphic url="image-18.png" coords="8,59.83,209.06,69.30,68.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-12.png" coords="7,50.11,221.92,495.01,212.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with State-Of-the-Arts on Overall IoU metric, * indicates results without using DenseCRF post processing</figDesc><table><row><cell>Method</cell><cell></cell><cell>UNC</cell><cell></cell><cell></cell><cell>UNC+</cell><cell></cell><cell cols="2">G-Ref Referit</cell></row><row><cell></cell><cell>val</cell><cell cols="2">testA testB</cell><cell>val</cell><cell cols="2">testA testB</cell><cell>val</cell><cell>test</cell></row><row><cell>LSTM-CNN [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28.14</cell><cell>48.03</cell></row><row><cell>KWAN [22]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.92</cell><cell>59.09</cell></row><row><cell>DMN [16]</cell><cell cols="7">49.78 54.83 45.13 38.88 44.22 32.29 36.76</cell><cell>52.81</cell></row><row><cell>ASGN [19]</cell><cell cols="7">50.46 51.20 49.27 38.41 39.79 35.97 41.36</cell><cell>60.31</cell></row><row><cell>RRN [11]</cell><cell cols="7">55.33 57.26 53.95 39.75 42.15 36.11 36.45</cell><cell>63.63</cell></row><row><cell>CMSA [26]</cell><cell cols="7">58.32 60.61 55.09 43.76 47.60 37.89 39.98</cell><cell>63.80</cell></row><row><cell>STEP [1]</cell><cell cols="7">60.04 63.46 57.97 48.19 52.33 40.41 46.40</cell><cell>64.13</cell></row><row><cell>BRIN [7]</cell><cell cols="7">61.35 63.37 59.57 48.57 52.87 42.13 48.04</cell><cell>63.46</cell></row><row><cell>LSCM [9]</cell><cell cols="7">61.47 64.99 59.55 49.34 53.12 43.50 48.05</cell><cell>66.57</cell></row><row><cell>CMPC [8]</cell><cell cols="7">61.36 64.53 59.64 49.56 53.44 43.23 49.05</cell><cell>65.53</cell></row><row><cell cols="9">JRNet* 448 × 448 64.31 68.13 60.48 52.00 56.44 43.96 48.72 68.03</cell></row><row><cell cols="8">JRNet* 576 × 576 65.76 69.33 60.93 53.97 60.06 45.49 49.49</cell><cell>68.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation Studies on Validation set of UNC, JRNet is the full architecture with both JRM and CMMLF modules. We use single encoder layer for all experiments using JRM. The input image resolution is 448 × 448 in each case. Figure 5. Qualitative results comparing baseline against JRNet.</figDesc><table><row><cell>9 Overall IoU</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on Overall IoU metric by varying the number of encoder layers in JRM on the UNC dataset.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Kanishk Jain and Vineet Gandhi Center for Visual Information Technology, KCIS, IIIT Hyderabad kanishk5991@gmail.com, vgandhi@iiit.ac.in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Detailed analysis on Precision@X</head><p>In this section, we present comprehensive results on Precision@X metric. We first compare against the existing approaches on Precision@X metric on UNC dataset and then present results of JRNet on Precision@X across all datasets.</p><p>In Table <ref type="table">1</ref>, we compare the performance of recent stateof-the-art methods on Precision@X metric. Other methods provide Precision@X metric results only on UNC dataset's validation split in their ablation studies and results on other datasets are not available. Hence the comparisons are limited to UNC's validation split. For other papers, we directly pick the results presented in their ablation section. As illustrated in Table <ref type="table">1</ref>, our approach achieves significantly higher Precision@X score for all values of X at both resolutions. At Precision@0.8 the performance improvement is 11.57% (30% relative improvement over the best performing CMPC). At Precision@0.9 our method achieves 6.16% improvement over CMPC (52% relative improvement, increasing to 19.05 from 12.89). The relative improvement over other methods increase with higher values of X in Precision@X, clearly illustrates the ability of our network to provide more refined segmentation maps, compared to the previous state-of-the-art methods.</p><p>In table <ref type="table">2</ref>, we present JRNet's performance on all four datasets on the Precision@X metric. High numbers at prec@0.5 metric indicate that our approach is able to localize the referent on a large number of cases (e.g. the correct referent is localized in more than 73% of cases across all splits of UNC dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Comparison at different Resolution</head><p>We understand that our input image resolution is higher than existing methods. For a fair comparison, we train the current best performing method CMPC <ref type="bibr" target="#b7">[8]</ref> at higher resolutions. In Table <ref type="table">3</ref> and Table <ref type="table">4</ref>, we compare our approach against CMPC trained at different image resolutions of 448 × 448 and 576 × 576, respectively. Our method consistently outperforms CMPC on both resolutions across all metrics by significant margins. Interestingly, our network trained at 448 × 448 resolution beats CMPC trained at 576 × 576 resolution at almost all metrics by good margins. This indicates our approach's capability to effectively utilize additional visual semantic information at higher resolution. When comparing on Precision@X metric, we observe that the performance gap between CMPC and JRNet increases with increase in X, while comparing the models trained at the same resolution.</p><p>We would like to point out that, despite higher resolution of input images, the feature map resolution of visual features in our approach is very low compared to other methods. Our approach utilizes feature maps that are downsampled by a factor of 32 from original image resolution, compared to other methods that down-sample the visual features only by factor of 8. We observed that using higher resolution feature map for the same image resolution results in increased training time with insignificant improvement in performance.</p><p>There is a small typo in Table1 of results section of our main paper. Lower values are reported for our method in the Overall IoU results for UNC's testB split. They change from 60.48% to 60.98% for JRNet at 448 × 448 resolution and from 60.93% to 61.93% for JRNet at 576 × 576 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Qualitative Results</head><p>In this section, we present additional qualitative results for JRNet model on variety of image-expression pairs.</p><p>In Figure <ref type="figure">S1</ref>, we present results where JRNet successfully grounded the referring expression in the image. JRNet is able to identify fine grained distinctive information about the referent from the referring expression, and utilize it to correctly localize the referent in complex visual scenes in (c), (d), (f) and (j). Specifically in (c), (d) and (j), JRNet is able to identify the correct person from large group of people based on the combination of person's attribute ("dark hair"), attributes of person's clothing ("green sleeves", "no shirt" etc) and its location with respect to other objects in the image ("by the wall"). Additionally, JRNet localizes objects Method prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9 Overall IoU CMSA <ref type="bibr" target="#b25">[26]</ref> 66 Dataset Split prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Split prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Split prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0. which are out of focus and are partially visible, ex: (b), (e), (g) and (h). We would like to point out that in these cases, rather than merely picking the most prominent objects, our network effectively incorporates the information from textual expression in visual domain to identify the less prominent correct object. In (a) and (i), the referring expressions refer to unstructured regions in image, our network predicts these regions with refined boundaries. In (k) and (l) of Figure <ref type="figure">S1</ref>, the referred objects occupy extremely small region in the image space and JRNet is able to accurately locate them.</p><p>In Figure <ref type="figure">S2</ref>, we present some failure cases of our approach. Our approach mostly fails in cases when either the referring expression or the visual scene is ambiguous in (a), (c) and (e), the visual scene is heavily cluttered in (b) and (d), or when common sense reasoning is required like (f). For example: the expression in (a), "chair at the end of table on the left" is itself ambiguous and non-specific, as there are two chairs at the end of table on left side. Similarly, in (b) their are multiple keyboards with a mouse on top and our method predicts one of the keyboards on the left with a partial black mouse on the top. In (d), the plant branch on the left is barely visible and also a lot of clutter is present. It is noteworthy, that in each case, JRNet predicts a well segmented and refined output and the class predictions are also correct (an umbrella, a chair, a bottle, a keyboard etc.).  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Natural language object retrieval. CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bi-directional relationship inferring network for referring image segmentation</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linguistic structure guided context modeling for referring image segmentation</title>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sansi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>ArXiv, abs/1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
				<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Referring image segmentation by generative adversarial learning</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1333" to="1344" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-shot grounding of objects from natural language queries</title>
		<author>
			<persName><forename type="first">Arka</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName><forename type="first">Hengcan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cross-modal relationship inference for grounding referring expressions</title>
		<author>
			<persName><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast and accurate onestage approach to visual grounding</title>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Resnest: Splitattention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
