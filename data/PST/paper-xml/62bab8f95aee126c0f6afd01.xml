<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Generation of Product-Image Sequence in E-commerce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-26">26 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaochuan</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Yang</surname></persName>
							<email>yangyong280@jd.com</email>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Shang</surname></persName>
							<email>shangyue1230@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xueying</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Xiao</surname></persName>
							<email>yun.xiao@yahoo.com</email>
						</author>
						<author>
							<persName><forename type="first">Lingfei</forename><forename type="middle">2022</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>He</surname></persName>
							<email>bjhezhen@jd.com</email>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Long</surname></persName>
							<email>bo.long@jd.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">JD.COM Research Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">JD.COM Research Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">JD.COM</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">JD.COM Research Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">JD.COM Research Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Zhen He JD.COM</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">JD.COM Research Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Bo Long JD.COM</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">JD.COM Research Mountain View</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<address>
									<addrLine>9 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Generation of Product-Image Sequence in E-commerce</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-26">26 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539149</idno>
					<idno type="arXiv">arXiv:2206.12994v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>product image selection</term>
					<term>image-sequence classifier</term>
					<term>multi-modality fusion</term>
					<term>e-commerce</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Product images are essential for providing desirable user experience in an e-commerce platform. For a platform with billions of products, it is extremely time-costly and labor-expensive to manually pick and organize qualified images. Furthermore, there are the numerous and complicated image rules that a product image needs to comply in order to be generated/selected. To address these challenges, in this paper, we present a new learning framework in order to achieve Automatic Generation of Product-Image Sequence (AGPIS) in e-commerce. To this end, we propose a Multi-modality Unified Image-sequence Classifier (MUIsC), which is able to simultaneously detect all categories of rule violations through learning. MUIsC leverages textual review feedback as the additional training target and utilizes product textual description to provide extra semantic information. Based on offline evaluations, we show that the proposed MUIsC significantly outperforms various baselines. Besides MUIsC, we also integrate some other important modules in the proposed framework, such as primary image selection, noncompliant content detection, and image deduplication. With all these modules, our framework works effectively and efficiently in JD.com recommendation platform. By Dec 2021, our AGPIS framework has generated high-standard images for about 1.5 million products and achieves 13.6% in reject rate. Code of this work is available at https://github.com/efan3000/muisc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Product images are essential for the success of online shopping <ref type="bibr" target="#b2">[3]</ref>[7] <ref type="bibr" target="#b37">[38]</ref>, as good product images enhance product description and help fill the gap between offline shopping and online shopping. Most of well-known e-commerce platforms have their own various complicated standards on product images, such as Amazon, Ebay, Alibaba and JD.com. "Haohuo" channel 1 (referring to "Discovery Goods"), an important traffic entrance on the top of JD.com (both website and App), is featured by its high standard of product presenting. A typical process for selecting product images are following this procedure: i) "HaoHuo" channel motivates human professions to submit high-quality product images; ii) human reviewers then review the submitted images according to a set of complicated image standards, which are largely designed based on in-house experts' experience and A/B tests. Figure <ref type="figure" target="#fig_0">1</ref> exhibits an example of comparison between product images from JD.com's main site and "Haohuo" channel. We can see that "Haohuo" channel presents the product in a concise and well-organized way. However, for an e-commence platform with large-scaled product catalogs, it is extremely time-costly and labor-expensive to manually pick product images. In this paper, we propose a novel learning framework for Automatic Generation of Product-Image Sequence (AGPIS), which automatically picks a sequence of product images from candidate images according to a set of numerous and complicated rules. There are some early works to address similar problems <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b8">[9]</ref>, which however are inadequate to address the aforementioned problems due to various unique challenges. Firstly, there are numerous and complicated rules and these rules could change over the time. This makes it unaffordable to develop rule-based specific methods or datasets for each rule due to the huge computation as well as the development and maintenance costs. Furthermore, the existing methods only focus on one or several rules <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b8">[9]</ref>. As a result, it is hard to work well with these systems since they are designed and customized with the fixed prior knowledge of rules. For example, detection of non-compliant content often involves logo detection <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b28">[29]</ref> and skin region detection <ref type="bibr" target="#b35">[36]</ref> <ref type="bibr" target="#b14">[15]</ref>.</p><p>Secondly, different rules may require different information. Product image rules can be generally divided into three categories as follows. First, Single-image rules. Typical single-image rules are about image quality, e.g. unnatural artifact or blurry image, and noncompliant content, e.g. logo, banner, and water-print. The detection of single-image rule violations is only relevant to an individual image. Second, Image-pair rules. Image-pair rules involve matching and comparison of two images in order to avoid redundant or wrong information. An example of image-pair rules states that two images should not present product appearance from similar viewing angles. The detection of this category of violations requires information from a pair of images. Third, Multi-image rules. These rules are usually designed for the layout of product images to make sure product information is adequate and released in a proper order. The detection of multi-image rule violations may require information from multiple images or even cross-modality product description. Most of the existing methods <ref type="bibr" target="#b8">[9]</ref>[14] <ref type="bibr" target="#b28">[29]</ref>[36] <ref type="bibr" target="#b14">[15]</ref> only focus on violation detection of single-image and image-pair rules, and ignore the exploration of relations between all the images in a sequence.</p><p>However, such relations are essential for automatic image selections since a product is presented by multiple images as a whole.</p><p>Last but not least, the rich information of image review feedback can be used for automatic image selection. The existing methods usually cast their problems into image classification problems. Then these product images are labeled as either qualified or not qualified according to the rules. However, textual feedback from human reviewers may contain rich semantic information which cannot be easily converted to classification labels. Taking "Haohuo" channel as an example, besides the violated rule name, a review feedback may also include extra information to explain these rejections, including which image in a sequence violates the rule, the location of noncompliant content in an image or what the non-compliant content is. Therefore, such rich semantic information is also helpful for improving automatic image selection.</p><p>To address these aforementioned challenges, in this paper, we present a novel learning framework in order to achieve Automatic Generation of Product-Image Sequence (AGPIS) in e-commerce. The core module of our framework is a Multi-modality Unified Image-sequence Classifier (MUIsC), which is able to simultaneously detect all categories of rule violations through learning. Firstly, to obtain adequate information for AGPIS, MUIsC takes as input an image sequence, rather than a single image or a pair of images, and extracts features via a hierarchical encoder. Then, along with the classification task, we use Natural Language Generation (NLG) of image review feedback as an auxiliary training task to fully exploit the rich semantic information of a review feedback. Different from some traditional tasks such as visual question answering <ref type="bibr" target="#b0">[1]</ref> and image captioning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>, which aim to generate text conditioned on a visual input, our NLG task functions as a guide for MUIsC to better understand complicated rules during training. In addition, the introduction of NLG task does not incur any additional burden in data processing since no manual labeling is needed. This is especially important for frequent model update in a real application. Lastly, textual product description is also fed to MUIsC as a input to assist image recognition. This requires our proposed MUIsC to be able to perform image-text interaction effectively. Interestingly, the resulting MUIsC has similar input and output with a human reviewer, which means that no prior knowledge or rule-based task is involved in our model.</p><p>In order to accumulate data for MUIsC training and make our framework work efficiently, we also integrate additional modules for single-image and image-pair recognition to detect unqualified images and build a sequence candidate for MUIsC. Figure <ref type="figure" target="#fig_1">2</ref> exhibits the overall pipeline of the proposed framework. Given a set of candidate images and textual product description, our framework outputs an image sequence and its probability of being qualified. If the probability is larger than a threshold, we submit the image sequence to a human reviewer and then "Haohuo" channel if approved. The red arrows in Figure <ref type="figure" target="#fig_0">1</ref> indicates an example of the correspondence between candidate images (from JD's main site) and the resulting image sequence of our framework for "Haohuo" channel.</p><p>Since its deployment in JD.com in Feb 2021, our AGPIS framework has generated high-standard images for about 1.5 million products and achieves 13.6% in reject rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In this section, we introduce works on similar topics and related domains.</p><p>E-commerce image selection. The significance of images in ecommerce has been well-studied <ref type="bibr" target="#b2">[3]</ref>[7] <ref type="bibr" target="#b37">[38]</ref>, but e-commerce retailers that offer marketplace platforms still struggle to control image quality. Gandhi et al. <ref type="bibr" target="#b8">[9]</ref> resolves the issue of non-compliant content by combining state-of-the-art image classification and object detection models, but non-compliant content is only part of numerous and complicated rules for automatic image selection. Chaudhuri et al. <ref type="bibr" target="#b1">[2]</ref> proposes a system that aggregates images from various suppliers to produce a image set, arranged in an order according to a set of manually-crafted templates. The template designer has to design different templates for different product categories. This costs huge human efforts and leads to low production efficiency. More importantly, this method limits the diversity of visual product presenting, i.e. the products in the same category are presented in a similar style. By contrast, our proposed framework learns to organize images from a large-scaled reviewed data, which is much more flexible. Besides, in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b1">[2]</ref>, systems consist of a sequence of modules designed for every specific rule, while our single MUIsC model is able to detect all categories of rule violations.</p><p>Image aesthetic and quality assessment has received considerable attention in recent years. This technique, which is similar to violation detection of single-image rules in automatic image selection, has been widely used in user album photo selection <ref type="bibr" target="#b18">[19]</ref> and image recommendation <ref type="bibr" target="#b36">[37]</ref>. For conventional image quality assessment methods, hand-craft features created from either photography practices or objective quality criteria are widely used <ref type="bibr" target="#b4">[5]</ref>[17] <ref type="bibr" target="#b24">[25]</ref>. More recently, learnt feature representations using deep neural network has surpassed the performance of hand-craft ones <ref type="bibr" target="#b15">[16]</ref>[13] <ref type="bibr" target="#b29">[30]</ref>. Most of the image assessment works are reported on datasets with assessment scores from peer reviewers, and cast the problem into image classification or ranking. Comparatively, textual product description and review feedback are in the form of natural language and have much richer semantic information than numeric scores. Besides, our framework confronts more complicated image rules and handles image sequences rather than single images.</p><p>Vision-and-language (VL) models, leveraging information from both modalities, has been a very active topic recently. Since transformer <ref type="bibr" target="#b32">[33]</ref> based models are adopted in computer vision, VL models achieved great success in tasks such as Visual Question Answering (VQA) <ref type="bibr" target="#b0">[1]</ref>, image captioning <ref type="bibr" target="#b33">[34]</ref> <ref type="bibr" target="#b21">[22]</ref>, and image-text matching <ref type="bibr" target="#b19">[20]</ref>, etc. Most of these works follow the model architecture in variants of visual backbones (ResNet <ref type="bibr" target="#b10">[11]</ref>, ViT <ref type="bibr" target="#b7">[8]</ref>, CLIP <ref type="bibr" target="#b25">[26]</ref>), text encoder/decoder (Bert <ref type="bibr" target="#b5">[6]</ref>, Roberta <ref type="bibr" target="#b22">[23]</ref>, GPT <ref type="bibr" target="#b26">[27]</ref>), modalityfusion scheme (single-stream <ref type="bibr" target="#b20">[21]</ref>, multi-stream <ref type="bibr" target="#b23">[24]</ref>[12]), and pretraining objectives (masked language modeling <ref type="bibr" target="#b5">[6]</ref>, masked image modeling <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b30">[31]</ref>, multimodal alignment <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>). Our MUIsC model also follows the transformer based text-image modeling framework. Specifically we use a transformer-based encoder for visual feature extraction and adopt an encoder-decoder (two-stream) architecture <ref type="bibr" target="#b32">[33]</ref> to fuse the visual and text information encoded by separate encoders. Pre-trained visual and language models are also used for model parameter initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Data and Notations</head><p>In this paper, we propose a framework for Automatic Generation of Product-Image Sequence (AGPIS), which picks a sequence of images from a set of candidate images according to a set of rules. Our work requires data from two different sources, JD.com's main site and JD.com's "HaoHuo" channel. From JD.com's main site, we collect product images as candidates and product title as a textual product description. From "HaoHuo" channel, we collect reviewed image sequences and corresponding textual feedback for model training and evaluation.</p><p>We adopt the following notations for our data. A set of ? ? candidate images are represented by</p><formula xml:id="formula_0">I ? = {i ? 1 , ..., i ? ? ? }, where i ? R ? ?? ?3</formula><p>denotes a RGB image, ? and ? are height and width of an image.</p><p>Similarly, an image-sequence that we aim to generate, called target image sequence, is represented by I ? = {i ? 1 , ...i ? ? ? }, where I ? ? I ? and ? ? ? ? ? is the number of images in I ? . Taking the dataset used in this work as an example, a product in JD.com's main site has about 7 images on average, i.e. ? ? ? 7, and a target sequence consists of 3 images, i.e. ? ? = 3. Note that our framework can be also applied to the problem with various values of ? ? and ? ? . Textual feedback and product title can both be represented by a sequence of words, W ? = {w   For the learning of primary-image selection model, we collect data from image sequences approved by human reviewers I ? * and their candidate images I ? * . We label an image, denoted as i ? I ? * ? I ? * , to ? ?????? ? as follows,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework Overview</head><formula xml:id="formula_1">? ?????? ? = 1 i = i ? * 1 0 otherwise<label>(1)</label></formula><p>, where i ? * 1 is the primary image of I ? * . We train the model on a binary classification task using a crossentropy loss function. Let the model's output be ? ?????? ? , which is the probability that i is a primary image. During inference, the image with the largest ? ?????? ? in I ? is selected as the primary image, and ? ?????? ? should be larger than a threshold. If no such image exists, the process of AGPIS is terminated and the framework outputs nothing.</p><p>Our model for non-compliant image detection has similar architecture and loss function with the primary-image selection model but uses a different dataset. From reviewed image sequences, we picked the ones which are rejected due to non-compliant content. Then the images in these sequences are manually labeled into two groups: compliance and non-compliance. Let the model's output for non-compliant image detection be ? ?? , which is the probability that an image contains non-compliant content. All images with ? ?? larger than a threshold are removed from I ? in the inference stage. If the number of remaining images is less than ? ? , the process of AGPIS is terminated and the framework outputs nothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Image-pair Recognition.</head><p>The module of image-pair recognition detects the violations of image-pair rules. In the real applications, this module is required to match and compare a pair of image patches rather than the whole images. Figure <ref type="figure" target="#fig_4">3</ref> shows a pair of images which violate one of our image-pair rules. We can see that these two images contain much duplicated visual information although they do not have exactly the same content. Therefore, a main difficulty in detection of this category of violations is the construction of a set of region proposals which contain meaningful product information. The representative region proposal algorithms include two-stage object detectors (e.g. Faster RCNN <ref type="bibr" target="#b27">[28]</ref>), Selective Search <ref type="bibr" target="#b31">[32]</ref> and EdgeBoxes <ref type="bibr" target="#b39">[40]</ref>. We empirically find that two-stage object detectors perform badly on e-commerce product images, and speculate that this is caused by different data distribution. In our method, we adopt EdgeBoxes to produce a certain number of region proposals for each image in I ? , and use a pretrained DNN to extract patch features. Then patches from two images are matched and compared based on k nearest neighbor algorithm. In Figure <ref type="figure" target="#fig_4">3</ref>, a pair of detected duplicated patches are outlined by green boxes. If a violation is detected in an image pair, the image with smaller ? ?? or the selected primary image is kept while the other image is removed. If the number of remaining images is less than ? ? , our framework outputs nothing.</p><p>Based on the output images of stage 1, a target sequence I ? is built. The selected primary image is the first image of I ? . Then, we randomly selected ? ? -1 images from the remaining images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stage 2: MUIsC</head><p>Our Multi-modality Unified Image-sequence Classifier (MUIsC) is designed following the transformer encoder-decoder architecture illustrated in Figure <ref type="figure" target="#fig_6">4</ref>. The encoder extracts features for a given image sequence using a hierarchical architecture, while the decoder performs vision-language fusion, and estimates textual feedback and classification probabilities. Then, we concatenate the features of all images in I ? and use extra ? ? stacked transformer encoder blocks <ref type="bibr" target="#b32">[33]</ref> to establish the relationship between features from different images. The output of these extra encoder blocks is sequence feature F ? ? R (? ?? +1)? ? ?? , which is also the output of MUIsC encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Decoder. The decoder is an autoregressive Natural Language</head><p>Generation (NLG) model with vision-language fusion and an additional classification head. Our decoder is made up of a stack of ? ? decoder blocks, and each decoder block consists of three layers: a masked MHSA, a Multi-Head Cross-Attention (MHCA) and a PFFN. Each of these three layers is followed by a residual connection and a layer normalization. Masked MHSA has a similar architecture with the MHSA in MUIsC encoder. But different from MHSA that is applied to all tokens (i.e. bi-directional attention mechanism), masked MHSA only collects information from the prior tokens (i.e. uni-directional attention mechanism). MHCA is also similar with MHSA except that MHCA takes two sequences of embeddings/features as inputs, and then computes the relationship between them. In our method, MHCA's two inputs include the output from MUIsC encoder, i.e. F ? and the embedding sequence from the previous masked MHSA. Note that MHCA may be absent in some decoder blocks to keep our model concise. The input of the masked MHSA of the first decoder block is a sequence of ? ? word embeddings, E ? = {e d 1 , , ..., e d ? ? }, which are generated from a sequence of words, denoted as W ? . In the training stage, W ? is constructed by concatenating product title W ? and textual feedback W ? separated by a special token ?, i.e. W ? = {w ? 1 , ..., w ? ? ? , s, w ? 1 , ..., w ? ? ? }, where W ? provides additional textual information for a product and W ? serves as the groundtruth in the training of autoregressive NLG model. In the inference stage, W ? only contains W ? and a following ?. We tokenize W ? into a sequence of tokens and encode the resulting tokens to word embeddings E ? via a word embedding layer. Then, position embeddings are added to E ? . Taking word embeddings E ? and image-sequence feature F ? as input, our decoder finally outputs a sequence of decoded hidden states H ? . Qualified Single-image Image-pair Multi-image Prop.</p><p>0.71 0.12 0.07 0.10 Table <ref type="table">1</ref>: Proportions of qualified samples, and samples rejected due to single-image, image-pair, and multi-image rule violations in AGPIS-data. Given an input word sequence which consists of product title W ? paired with review feedback W ? and a sequence of images I ? , our NLG task is to estimate the conditional probability for each token in W ? :</p><formula xml:id="formula_2">P ??? (W ? |I ? , W ? ) = ? ? ?=1 ? ??? (? ? ? |? ? &lt;? ; I ? ; W ? )<label>(2)</label></formula><p>where ? Given the whole training set N = (W ? , I ? , W ? ), our NLG task can be trained by optimizing the loss function as follows:</p><formula xml:id="formula_3">L ??? (N ) = - ?? (W ? ,I ? ,W ? ) ?N log P ??? (W ? |I ? , W ? ).<label>(3)</label></formula><p>If we set textual feedback of a qualified image-sequence to a constant word, e,g. "yes", i.e. ? ? = 1 and ? ? 1 = "yes", ? ??? (? ? 1 ) is actually the probability of I ? being qualified.</p><p>Sequence multi-class classification task classifies a sequence of images I ? into ? ? classes G ? {? 1 , ..., ? ? ? }, where ? 1 is the class of being qualified and {? 2 , ..., ? ? ? } represent the classes of being rejected due to various rule violations. A McC head is applied on the decoded hidden state of token ? which follows W ? , and thus our McC task is conditioned on W ? and I ? . We use a softmax classifier where the loss function is</p><formula xml:id="formula_4">L ??? (N ) = ?? (W ? ,I ? ,W ? ) ?N (- ? ? ?? ?=1 1(? (I ? ) = ?) log(? ??? ? (I ? )))</formula><p>, where 1(?) is the indicator function, ? (I ? ) is the class label of I ? , and ? ??? ? (I ? ) represents the probability of I ? belonging to class i. Especially, ? ??? 1 (I ? ) is the probability of I ? being qualified. Combining the natural language generation and multi-class classification, the loss function for MUIsC is</p><formula xml:id="formula_5">L (N ) = ? ??? L ??? (N ) + ? ??? L ??? (N )<label>(4)</label></formula><p>, where ? ??? , ? ??? &gt; 0 are factors that balance the two loss functions. In the inference stage, our framework takes ? ??? 1 (I ? ) as its output, i.e. ? ? = ? ??? 1 (I ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OFFLINE EVALUATION OF MUISC 4.1 Dataset and Metrics</head><p>The stage 1 of our proposed framework is firstly deployed to produce image sequences for JD.com's "Haohuo" channel. With months of data accumulation, we collect produced three-image sequences and corresponding textual feedback as our dataset for MUIsC training and evaluation. Besides, textual product title and candidate images are also collected from JD.com's main site for each product in the dataset. In this paper, we use a dataset collected within three months and call it as AGPIS-data. AGPIS-data contains over 700K samples from 39 product categories. Distribution details of this dataset can be found in Table <ref type="table">1</ref>. We can see that about 29% samples in our dataset are rejected ones. Note that, because part of image files are not valid anymore when we collect them and we only keep the samples with valid images, the proportion of qualified samples in our dataset is not equivalent to the acceptance rate in real production. We consider the rule name appeared in a textual feedback as the class label of a sample for the learning of multi-class classification task. If there are more than one rules in a textual feedback, we just choose the first one. In our dataset, only 4.7% rejected samples have more than one violated rules. There are mainly 43 image-relevant rules in our AGPIS-data, and our multi-class labels have 45 classes with one extra class for qualified samples and another class for the samples rejected by other image-relevant rules. We can also convert multi-class labels to binary-class ones by simply merging 44 classes of rejected samples into one class. AGPIS-data is randomly split into three subsets without any overlap in product SKUstraining (80%), validation(10%), and testing(10%). Besides, in order to evaluate the detection performance of different categories of rule violations, we also build datasets AGPIS-data-single, AGPIS-datapair, and AGPIS-data-multi for single-image rules, image-pair rules, and multi-image rules, respectively. Each of the above datasets contains the same number of randomly selected qualified samples and samples rejected by a specific category of rule violations.</p><p>ROC AUC (AUC) and Recall@Precision (R@P) are used to evaluate the performance of models. AUC is defined as the area under the ROC curve and R@P is the recall value at a given precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>MUIsC is trained in an end-to-end manner with the encoder initialized by a pretrained ViT model and the decoder initialized by a pretrained GPT2 <ref type="bibr" target="#b26">[27]</ref> model. We use a pretrained ViT-B16-224 (? ? = 12), the base version of the ViT with (? = 16) patch size and (? ?? = 224 ? 224) input image size. This model is pretrained on ImageNet-21k and finetuned on ImageNet 2012. For decoder, we use a GPT2 model pretrained on CLUECorpus <ref type="bibr" target="#b34">[35]</ref>, a large-scale Chinese Corpus. Our decoder only has ? ? = 3 blocks to keep the model concise and efficient, since our textual product title and review feedback are relatively simple. Embedding dimension in both encoder and decoder is ? = 768. The model is trained for 10 epochs using the AdamW optimizer, with batch size 64. We use 1.5e-4 as the initial learning rate in our experiments and real applications.</p><p>Other parameters that need to be set in the proposed method are ? Number of images in target image sequence is ? ? = 3, and number of candidate images is ? ? ? 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Backbone Fusion AUC R@P=0.8 R@P=0.85 R@P=0.9 AUC-single AUC-pair AUC- ? Number of extra encoder blocks for image-feature interaction is ? ? = 1. ? Balance factor in the loss function in Eq. 4 is ? ??? = 0.1 and ? ??? = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Methods</head><p>Our MUIsC aims to solve the problem of binary image-sequence classification, which can be considered as an extension of singleimage classification. Note that a specific issue in image-sequence classification is how to fuse the information of multiple images in a sequence. To validate MUIsC, we build baselines based on the classic single-image classification architecture, called single-tower, which consists of a visual backbone and a classification head. We experiment with two image-fusion methods (early fusion and late fusion) and 4 backbones (ResNet18 <ref type="bibr" target="#b10">[11]</ref>, ResNet50 <ref type="bibr" target="#b10">[11]</ref>, ResNetV2-101 <ref type="bibr" target="#b17">[18]</ref>, and the ViT <ref type="bibr" target="#b7">[8]</ref> used in MUIsC). The early fusion method concatenates all images of a sequence into a single image, while the late fusion method firstly extracts features for each image and then concatenates these features together. The classification head is a MLP classifier. Models are all trained on AGPIS-data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison</head><p>Table2 shows the performance of our method and baselines on different datasets, where AUC, AUC-single, AUC-pair, and AUCmulti represent the AUC on AGPIS-data, AGPIS-data-single, AGPISdata-pair, and AGPIS-data-multi, respectively. Besides the model with superscript *, all models are trained on a multi-classification task. We can observe that the proposed MUIsC outperforms other methods in AUC on AGPIS-data and achieves the best results in terms of all the other evaluation metrics and datasets except for AUC on AGPIS-data-pair. Among all visual backbones, ResNetV2-101 and ViT performs better than ResNet18 and ResNet50. This shows that powerful backbones are able to play some role in AGPIS. The comparison between early and late fusion modes indicates that each mode has its own strength and weakness, though late-fusion methods outperform early-fusion ones on the whole dataset. Early-fusion methods perform better on AGPIS-data-pair dataset. We think this is caused by the deep interaction between images. Late-fusion methods achieve better on AGPIS-data-single dataset since image features are extracted individually in the early stage. Our MUIsC adopts late-fusion considering the overall performance. But it is still an interesting topic to balance model performance between single-image rules and image-pair rules. Besides, early-fusion and late-fusion methods have similar performance on AGPIS-data-multi. We speculate the reason is that AGPIS-data-multi emphasizes both single-image features and interaction between images. We also show the performance of the model trained on binary classification task, and observe that binary-class classifier performs worse than its corresponding multi-class one. This indicates that more detailed information about rejection can help improve model performance. In MUIsC, we further use textual feedback to introduce more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>The structure of MUIsC is carefully ablated with the results listed in Table <ref type="table" target="#tab_1">3</ref>. Here, we use the same set of datasets and metrics with Section 4.4. The baseline for ablation study is a single-tower model trained on McC task, which either does not have decoder or uses any textual data. By introducing hierarchical image feature extraction method, a 1.4% AUC gain is achieved, which shows the extra ? ? transformer encoder blocks result in better interaction between images than baseline late fusion. After using an encoder-decoder architecture, the performance is slightly improved, and indicates that directly adding a decoder does not lead to a big performance gain. Then, we add the NLG task and formulate a multi-task learning model. It is observed that the performance is considerable improved, which indicates that the textual review feedback effectively guides the model to better understand rejected samples during training and results in better performance. By including textual product title as an extra input, we get a AUC gain of 0.8% and the best performance is achieved. Interestingly, two kinds of textual information play different roles in terms of performance improvement. The NLG task on textual feedback leads to a good AUC gain on AGPIS-data-single, while product-title input achieves a significant gain on AGPIS-datamulti. This verifies that the detections of different categories of rule violations require different information for AGPIS.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis</head><p>Four qualitative example results are illustrated in Figure <ref type="figure" target="#fig_9">5</ref>. Each example includes a "good" sequence and a "bad" one for a same product, according to the probability of being qualified ? ? estimated by our framework (shown above each sequence). We can observe that all the sequences with low ? ? (the second row) violates our rules. The sequence in the leftmost example contains non-compliant banner and logo in the primary image. For the second left example, the primary and the third images contain products with different colors, which violates an image-pair rule. Besides, the second right and rightmost examples have improper display orders since the primary image fails to provide a whole picture of the product and violates multi-image rules. Meanwhile, we can see that all good sequences (the first row) have compliant single images and present the product in a proper order. These examples show that the proposed framework is effective in detecting all categories of rule violations and generating qualified image sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ONLINE EVALUATION 5.1 Deployment and Online Evaluation</head><p>JD "Haohuo" Channel (Discovery Goods Channel) is an important traffic entrance for JD.com and also a good platform for users to discover their potential purchase interests, thus the product quality and presentation is of great influence for platform's income. Before the release of our AGPIS framework, image selection for products mainly depends on human labors, which may be expensive and perform lower efficient. Since Feb 2021, our AGPIS framework has been deployed on "Haohuo" Channel (both website and App), and has produced high-standard images for about 1.5 million featured products. The amount of production is equivalent to 1000+ human professions.</p><p>Our framework is deployed step by step. Stage 1 is developed and deployed firstly. Then, stage 2 is trained on the resulting data and is deployed following stage 1. Our MUIsC model is updated every 3 months and is trained on AGPIS-data collected within the past 3 months. We take reject rate by human reviewers as online evaluation metric and set the submission threshold ?? ? to 0.3. According to the statistical results, the reject rate is 19.3% for the period when there is only stage 1, and it is further reduced to 13.6% after stage 2 is added. Note that in order to avoid high reject rate in the period when stage 2 is not ready, stage 1 works under a strict setting, which may lead to high false-negative. In the future, we will gradually relax stage 1 and enable stage 2 to select among multiple candidates instead of just evaluating one candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">System A/B testing</head><p>In "Haohuo" Channel, an image-text system is built with our AGPIS framework and a product-copy generation framework <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b38">[39]</ref> to generate both images and textual description for a product. To evaluate the effectiveness of this system, we compare key online metrics before and after deploying our system on "Haohuo" Channel. The business of the platform is measured by Click-Through Rate (CTR) and Conversion Rate (CVR). The A/B testing is conducted on most of the main categories of products, covering clothes, electronics, computers, beauty &amp; health, groceries, etc. For users in baseline group, the platform shows them product images and text generated by human professions. For users in experiment group, the platform shows them images and text generated by our system. We find that images and text generated by our system outperform the ones submitted by human professions. Specifically, our system improves CTR by 3.2% and CVR by 3.6% over baseline. The increase of CTR indicates that the users prefer to click products with product images and text generated by our system. The improvement of CVR demonstrates that the images and text generated by our system is successful in convincing users to make purchase decisions.</p><p>Currently, the product image-text generation task in JD "Haohuo" Channel is jointly performed by human professions and our image-text system. We observe a trend that human professions sometimes prefer to use our system to assist their generation process. In addition, our system can also benefit for long-tail products, making the channel a healthier ecosystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we developed a framework for Automatic Generation of Product-Image Sequence (AGPIS) in e-commerce. To address the unique challenges in AGPIS, our framework is designed as a combination of rule-based specific methods (stage 1) and a Multimodality Unified Image-sequence Classifier (MUIsC) (stage 2) which is able to detect all categories of rule violations while considering multi-modality information. The experimental results show that our MUIsC outperforms various baselines. Our framework has been deployed on JD.com's "Haohuo" Channel, and a high volume of 1.5 million product image sequences have been generated. With the help of our AGPIS framework, our CTR is improved by 3.2% and CVR is improved by 3.6%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A pair of shoes are presented by images from (a) JD.com's main site and (b) JD.com's "Haohuo" Channel. (a) contains redundant and non-compliant content, while (b) presents all aspects of the product via three images.</figDesc><graphic url="image-1.png" coords="2,60.43,83.70,226.96,211.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pipeline of our AGPIS framework. Stage 1 detects non-compliant/redundant images and selects the primary image as shown by the red image border. Given a target image sequence built from stage 1, stage 2 outputs its probability of being qualified.</figDesc><graphic url="image-2.png" coords="3,53.80,83.69,504.36,168.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>? 1 ,</head><label>1</label><figDesc>..., w ? ? ? } and W ? = {w ? 1 , ..., w ? ? ? }, respectively. For accepted image sequences, we set their textual feedback to a constant word, e.g. "yes", i.e. ? ? = 1 and ? ? 1 = "yes".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>Figure 2 provides the overview of our proposed framework for automatic image selection. The framework consists of two stages. Stage 1 consists of a single-image recognition module and an image-pair recognition module. Given a set of candidate images I ? , the singleimage recognition module selects the primary (first) image for the target sequence and detects non-compliant content. Following the single-image recognition module, the image-pair recognition module detects the violations of image-pair rules. Then, a target image sequence I ? is built from the remaining images in I ? . In stage 2, I ? and its corresponding textual description W ? are fed into a model named Multi-modality Unified Image-sequence Classifier (MUIsC), which estimates the probability of I ? being qualified, denoted as ? ? . I ? and ? ? are the final output of our framework. If ? ? is larger than a threshold ?? ? , we send I ? to a human reviewer and then "Haohuo" channel if approved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 3</head><label>3</label><figDesc>Stage 1 3.3.1 Single-image Recognition. The single-image recognition module consists of two learning-based models. One selects the primary image for I ? since some rules are specifically designed for primary image, and the other model detects non-compliant images. Each of these two models works as a binary single-image classifier based on Deep Neural Networks (DNNs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example image pair that violates the rule "images with duplicated content". The green boxes show the duplicated patch-pair detected by our method.</figDesc><graphic url="image-3.png" coords="4,349.81,83.69,176.54,82.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of MUIsC</figDesc><graphic url="image-4.png" coords="5,56.32,83.70,499.30,270.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3. 4 . 3</head><label>43</label><figDesc>Multi-Task Learning. We combine two tasks in a single MUIsC model -NLG which generates a textual feedback, and sequence Multi-class Classification (McC) which classifies a sequence to a qualified one or a category of rule violations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>?</head><label></label><figDesc>&lt;? stands for all tokens prior to position ? (i.e. ? ? &lt;? = (? ? 1 , ..., ? ? ?-1 )). In the training stage, W ? is located prior to W ? , and thus the ? ??? (? ? ? ) is conditioned on the preceding tokens in W ? , I ? , and all tokes in W ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of qualitative AGPIS framework results.</figDesc><graphic url="image-5.png" coords="8,53.80,197.46,504.41,93.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison. Baselines use different visual backbones and image fusion methods. Superscript * indicates that the model is trained on a binary-class task and the other models are trained on a multi-class task.</figDesc><table><row><cell>multi</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Ablation of components in MUIsC.</figDesc><table><row><cell cols="5">Single Hier. Enc.-McC NLG</cell><cell>Text</cell><cell cols="5">AUC R@P=0.8 R@P=0.85 R@P=0.9 AUC-AUC-AUC-</cell></row><row><cell>tower</cell><cell></cell><cell>dec.</cell><cell cols="3">task task as input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>single</cell><cell>pair</cell><cell>multi</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>0.764</cell><cell>0.831</cell><cell>0.644</cell><cell>0.429</cell><cell>0.772 0.729 0.767</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>0.778</cell><cell>0.856</cell><cell>0.701</cell><cell>0.456</cell><cell>0.781 0.755 0.762</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>0.780</cell><cell>0.876</cell><cell>0.688</cell><cell>0.450</cell><cell>0.784 0.757 0.762</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>0.792</cell><cell>0.891</cell><cell>0.730</cell><cell>0.507</cell><cell>0.799 0.764 0.765</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.800</cell><cell>0.904</cell><cell>0.758</cell><cell>0.524</cell><cell>0.795 0.774 0.791</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A smart system for selection of optimal product images in e-commerce</title>
		<author>
			<persName><forename type="first">Abon</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samrat</forename><surname>Kokkula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinandan</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreyansh</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Magnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Kandaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1728" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive model of the effects of online store image on purchase intention in an e-commerce environment</title>
		<author>
			<persName><forename type="first">Ming-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-I</forename><surname>Teng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName><forename type="first">Ritendra</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is a picture really worth a thousand words? -on the role of images in e-commerce</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Bhardwaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="633" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Shreyansh</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samrat</forename><surname>Kokkula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abon</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Magnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theban</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Kandaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Ovenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02234</idno>
		<title level="m">Image matters: Detecting offensive and non-compliant content/logo in product images</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shugen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10613</idno>
		<title level="m">Intelligent Online Selling Point Extraction for E-Commerce Recommendation</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unit: Multimodal multitask learning with a unified transformer</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image aesthetic predictors based on weighted CNNs</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">V</forename><surname>Ortiz Segovia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2291" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM international conference on Multimedia</title>
		<meeting>the 17th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="581" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical color models with application to skin detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="81" to="96" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context in photo albums: Understanding and modeling user behavior in clustering and selection</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tania</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Cozot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Kervec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadi</forename><surname>Bouatouch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception (TAP)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Gang Hua, Houdong Hu, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longteng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10804</idno>
		<title level="m">Cptr: Full transformer network for image captioning</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comprehensive aesthetic quality assessment method for natural images using basic rules of photography</title>
		<author>
			<persName><forename type="first">Eftichia</forename><surname>Mavridaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Mezaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="887" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bundle min-hashing for logo recognition</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM conference on International conference on multimedia retrieval</title>
		<meeting>the 3rd ACM conference on International conference on multimedia retrieval</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting image aesthetic assessment via self-supervised feature learning</title>
		<author>
			<persName><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Gang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5709" to="5716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01355</idno>
		<title level="m">CLUECorpus2020: A large-scale Chinese corpus for pre-training language model</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Big skin regions detection for adult image identification</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Haiming Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 Workshop on Digital Media and Digital Content Management</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="242" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aesthetic-based clothing recommendation</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huidi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 world wide web conference</title>
		<meeting>the 2018 world wide web conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="649" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Zakrewsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamelia</forename><surname>Aryafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.03663</idno>
		<title level="m">Item popularity prediction in e-commerce using image quality feature vectors</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Xueying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hainan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11915</idno>
		<title level="m">Automatic Product Copywriting for E-Commerce</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
