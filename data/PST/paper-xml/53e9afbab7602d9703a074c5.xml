<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Deep Architectures for AI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yoshua</forename><forename type="middle">Bengio</forename><surname>Contents</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>yoshua.bengio@umontreal.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. IRO</orgName>
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<postCode>C.P. 6128, H3C 3J7</postCode>
									<settlement>Montreal</settlement>
									<region>Qc</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Deep Architectures for AI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD5496EFB007CD3726B331490987FD03</idno>
					<idno type="DOI">10.1561/2200000006</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2</term>
					<term>1 Computational Complexity 2</term>
					<term>2 Informal Arguments 3 Local vs Non-Local Generalization 3</term>
					<term>1 The Limits of Matching Local Templates 3</term>
					<term>2 Learning Distributed Representations 4 Neural Networks for Deep Architectures 4</term>
					<term>1 Multi-Layer Neural Networks 4</term>
					<term>2 The Challenge of Training Deep Neural Networks 4</term>
					<term>3 Unsupervised Learning for Deep Architectures 4</term>
					<term>4 Deep Generative Architectures 4</term>
					<term>5 Convolutional Neural Networks 4</term>
					<term>6 Auto-Encoders 5 Energy-Based Models and Boltzmann Machines 5</term>
					<term>1 Energy-Based Models and Products of Experts 5</term>
					<term>2 Boltzmann Machines 5</term>
					<term>3 Restricted Boltzmann Machines 5</term>
					<term>4 Contrastive Divergence 6 Greedy Layer-Wise Training of Deep Architectures 6</term>
					<term>1 Layer-Wise Training of Deep Belief Networks 6</term>
					<term>2 Training Stacked Auto-Encoders 6</term>
					<term>3 Semi-Supervised and Partially Supervised Training 7 Variants of RBMs and Auto-Encoders 7</term>
					<term>1 Sparse Representations in Auto-Encoders and RBMs 7</term>
					<term>2 Denoising Auto-Encoders 7</term>
					<term>3 Lateral Connections 7</term>
					<term>4 Conditional RBMs and Temporal RBMs 7</term>
					<term>5 Factored RBMs 7</term>
					<term>6 Generalizing RBMs and Contrastive Divergence 8 Stochastic Variational Bounds for Joint Optimization of DBN Layers 8</term>
					<term>1 Unfolding RBMs into Infinite Directed Belief Networks 8</term>
					<term>2 Variational Justification of Greedy Layer-wise Training 8</term>
					<term>3 Joint Unsupervised Training of All the Layers 9 Looking Forward 9</term>
					<term>1 Global Optimization Strategies 9</term>
					<term>2 Why Unsupervised Learning is Important 9</term>
					<term>3 Open Questions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the stateof-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">How do We Train Deep Architectures? 5</head><p>prior knowledge about the world to explain the observed variety of images, even for such an apparently simple abstraction as MAN, illustrated in Figure <ref type="figure">1</ref>.1. A high-level abstraction such as MAN has the property that it corresponds to a very large set of possible images, which might be very different from each other from the point of view of simple Euclidean distance in the space of pixel intensities. The set of images for which that label could be appropriate forms a highly convoluted region in pixel space that is not even necessarily a connected region. The MAN category can be seen as a high-level abstraction with respect to the space of images. What we call abstraction here can be a category (such as the MAN category) or a feature, a function of sensory data, which can be discrete (e.g., the input sentence is at the past tense) or continuous (e.g., the input video shows an object moving at 2 meter/second). Many lower-level and intermediate-level concepts (which we also call abstractions here) would be useful to construct a MAN-detector. Lower level abstractions are more directly tied to particular percepts, whereas higher level ones are what we call "more abstract" because their connection to actual percepts is more remote, and through other, intermediate-level abstractions.</p><p>In addition to the difficulty of coming up with the appropriate intermediate abstractions, the number of visual and semantic categories (such as MAN) that we would like an "intelligent" machine to capture is rather large. The focus of deep architecture learning is to automatically discover such abstractions, from the lowest level features to the highest level concepts. Ideally, we would like learning algorithms that enable this discovery with as little human effort as possible, i.e., without having to manually define all necessary abstractions or having to provide a huge set of relevant hand-labeled examples. If these algorithms could tap into the huge resource of text and images on the web, it would certainly help to transfer much of human knowledge into machine-interpretable form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">How do We Train Deep Architectures?</head><p>Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Allowing computers to model our world well enough to exhibit what we call intelligence has been the focus of more than half a century of research. To achieve this, it is clear that a large quantity of information about our world should somehow be stored, explicitly or implicitly, in the computer. Because it seems daunting to formalize manually all that information in a form that computers can use to answer questions and generalize to new contexts, many researchers have turned to learning algorithms to capture a large fraction of that information. Much progress has been made to understand and improve learning algorithms, but the challenge of artificial intelligence (AI) remains. Do we have algorithms that can understand scenes and describe them in natural language? Not really, except in very limited settings. Do we have algorithms that can infer enough semantic concepts to be able to interact with most humans using these concepts? No. If we consider image understanding, one of the best specified of the AI tasks, we realize that we do not yet have learning algorithms that can discover the many visual and semantic concepts that would seem to be necessary to interpret most images on the web. The situation is similar for other AI tasks.</p><p>Fig. <ref type="figure">1</ref>.1 We would like the raw input image to be transformed into gradually higher levels of representation, representing more and more abstract functions of the raw input, e.g., edges, local shapes, object parts, etc. In practice, we do not know in advance what the "right" representation should be for all these levels of abstractions, although linguistic concepts might help guessing what the higher levels should implicitly represent.</p><p>Consider for example the task of interpreting an input image such as the one in Figure <ref type="figure">1</ref>.1. When humans try to solve a particular AI task (such as machine vision or natural language processing), they often exploit their intuition about how to decompose the problem into subproblems and multiple levels of representation, e.g., in object parts and constellation models <ref type="bibr" target="#b137">[138,</ref><ref type="bibr" target="#b178">179,</ref><ref type="bibr" target="#b196">197]</ref> where models for parts can be re-used in different object instances. For example, the current stateof-the-art in machine vision involves a sequence of modules starting from pixels and ending in a linear or kernel classifier <ref type="bibr" target="#b133">[134,</ref><ref type="bibr" target="#b144">145]</ref>, with intermediate modules mixing engineered transformations and learning, e.g., first extracting low-level features that are invariant to small geometric variations (such as edge detectors from Gabor filters), transforming them gradually (e.g., to make them invariant to contrast changes and contrast inversion, sometimes by pooling and sub-sampling), and then detecting the most frequent patterns. A plausible and common way to extract useful information from a natural image involves transforming the raw pixel representation into gradually more abstract representations, e.g., starting from the presence of edges, the detection of more complex but local shapes, up to the identification of abstract categories associated with sub-objects and objects which are parts of the image, and putting all these together to capture enough understanding of the scene to answer questions about it.</p><p>Here, we assume that the computational machinery necessary to express complex behaviors (which one might label "intelligent") requires highly varying mathematical functions, i.e., mathematical functions that are highly non-linear in terms of raw sensory inputs, and display a very large number of variations (ups and downs) across the domain of interest. We view the raw input to the learning system as a high dimensional entity, made of many observed variables, which are related by unknown intricate statistical relationships. For example, using knowledge of the 3D geometry of solid objects and lighting, we can relate small variations in underlying physical and geometric factors (such as position, orientation, lighting of an object) with changes in pixel intensities for all the pixels in an image. We call these factors of variation because they are different aspects of the data that can vary separately and often independently. In this case, explicit knowledge of the physical factors involved allows one to get a picture of the mathematical form of these dependencies, and of the shape of the set of images (as points in a high-dimensional space of pixel intensities) associated with the same 3D object. If a machine captured the factors that explain the statistical variations in the data, and how they interact to generate the kind of data we observe, we would be able to say that the machine understands those aspects of the world covered by these factors of variation. Unfortunately, in general and for most factors of variation underlying natural images, we do not have an analytical understanding of these factors of variation. We do not have enough formalized lower level features. Automatically learning features at multiple levels of abstraction allow a system to learn complex functions mapping the input to the output directly from data, without depending completely on human-crafted features. This is especially important for higher-level abstractions, which humans often do not know how to specify explicitly in terms of raw sensory input. The ability to automatically learn powerful features will become increasingly important as the amount of data and range of applications to machine learning methods continues to grow.</p><p>Depth of architecture refers to the number of levels of composition of non-linear operations in the function learned. Whereas most current learning algorithms correspond to shallow architectures (1, 2 or 3 levels), the mammal brain is organized in a deep architecture <ref type="bibr" target="#b172">[173]</ref> with a given input percept represented at multiple levels of abstraction, each level corresponding to a different area of cortex. Humans often describe such concepts in hierarchical ways, with multiple levels of abstraction. The brain also appears to process information through multiple stages of transformation and representation. This is particularly clear in the primate visual system <ref type="bibr" target="#b172">[173]</ref>, with its sequence of processing stages: detection of edges, primitive shapes, and moving up to gradually more complex visual shapes.</p><p>Inspired by the architectural depth of the brain, neural network researchers had wanted for decades to train deep multi-layer neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b190">191]</ref>, but no successful attempts were reported before 2006 <ref type="foot" target="#foot_0">1</ref> : researchers reported positive experimental results with typically two or three levels (i.e., one or two hidden layers), but training deeper networks consistently yielded poorer results. Something that can be considered a breakthrough happened in 2006: Hinton et al. at University of Toronto introduced Deep Belief Networks (DBNs) <ref type="bibr" target="#b72">[73]</ref>, with a learning algorithm that greedily trains one layer at a time, exploiting an unsupervised learning algorithm for each layer, a Restricted Boltzmann Machine (RBM) <ref type="bibr" target="#b50">[51]</ref>. Shortly after, related algorithms based on auto-encoders were proposed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b152">153]</ref>, apparently exploiting the same principle: guiding the training of intermediate levels of representation using unsupervised learning, which can be performed locally at each level. Other algorithms for deep architectures were proposed more recently that exploit neither RBMs nor auto-encoders and that exploit the same principle <ref type="bibr" target="#b130">[131,</ref><ref type="bibr" target="#b201">202]</ref> (see <ref type="bibr">Section 4)</ref>.</p><p>Since 2006, deep networks have been applied with success not only in classification tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b194">195]</ref>, but also in regression <ref type="bibr" target="#b159">[160]</ref>, dimensionality reduction <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b157">158]</ref>, modeling textures <ref type="bibr" target="#b140">[141]</ref>, modeling motion <ref type="bibr" target="#b181">[182,</ref><ref type="bibr" target="#b182">183]</ref>, object segmentation <ref type="bibr" target="#b113">[114]</ref>, information retrieval <ref type="bibr" target="#b153">[154,</ref><ref type="bibr" target="#b158">159,</ref><ref type="bibr" target="#b189">190]</ref>, robotics <ref type="bibr" target="#b59">[60]</ref>, natural language processing <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b201">202]</ref>, and collaborative filtering <ref type="bibr" target="#b161">[162]</ref>. Although auto-encoders, RBMs and DBNs can be trained with unlabeled data, in many of the above applications, they have been successfully used to initialize deep supervised feedforward neural networks applied to a specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Intermediate Representations: Sharing Features and Abstractions Across Tasks</head><p>Since a deep architecture can be seen as the composition of a series of processing stages, the immediate question that deep architectures raise is: what kind of representation of the data should be found as the output of each stage (i.e., the input of another)? What kind of interface should there be between these stages? A hallmark of recent research on deep architectures is the focus on these intermediate representations: the success of deep architectures belongs to the representations learned in an unsupervised way by RBMs <ref type="bibr" target="#b72">[73]</ref>, ordinary auto-encoders <ref type="bibr" target="#b16">[17]</ref>, sparse auto-encoders <ref type="bibr" target="#b149">[150,</ref><ref type="bibr" target="#b152">153]</ref>, or denoising auto-encoders <ref type="bibr" target="#b194">[195]</ref>. These algorithms (described in more detail in Section 7.2) can be seen as learning to transform one representation (the output of the previous stage) into another, at each step maybe disentangling better the factors of variations underlying the data. As we discuss at length in Section 4, it has been observed again and again that once a good representation has been found at each level, it can be used to initialize and successfully train a deep neural network by supervised gradient-based optimization.</p><p>Each level of abstraction found in the brain consists of the "activation" (neural excitation) of a small subset of a large number of features that are, in general, not mutually exclusive. Because these features are not mutually exclusive, they form what is called a distributed representation <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b155">156]</ref>: the information is not localized in a particular neuron but distributed across many. In addition to being distributed, it appears that the brain uses a representation that is sparse: only a around 1-4% of the neurons are active together at a given time <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b112">113]</ref>. Section 3.2 introduces the notion of sparse distributed representation and Section 7.1 describes in more detail the machine learning approaches, some inspired by the observations of the sparse representations in the brain, that have been used to build deep architectures with sparse representations.</p><p>Whereas dense distributed representations are one extreme of a spectrum, and sparse representations are in the middle of that spectrum, purely local representations are the other extreme. Locality of representation is intimately connected with the notion of local generalization. Many existing machine learning methods are local in input space: to obtain a learned function that behaves differently in different regions of data-space, they require different tunable parameters for each of these regions (see more in Section 3.1). Even though statistical efficiency is not necessarily poor when the number of tunable parameters is large, good generalization can be obtained only when adding some form of prior (e.g., that smaller values of the parameters are preferred). When that prior is not task-specific, it is often one that forces the solution to be very smooth, as discussed at the end of Section 3.1. In contrast to learning methods based on local generalization, the total number of patterns that can be distinguished using a distributed representation scales possibly exponentially with the dimension of the representation (i.e., the number of learned features).</p><p>In many machine vision systems, learning algorithms have been limited to specific parts of such a processing chain. The rest of the design remains labor-intensive, which might limit the scale of such systems. On the other hand, a hallmark of what we would consider intelligent machines includes a large enough repertoire of concepts. Recognizing MAN is not enough. We need algorithms that can tackle a very large set of such tasks and concepts. It seems daunting to manually define that many tasks, and learning becomes essential in this context. Furthermore, it would seem foolish not to exploit the underlying commonalities between these tasks and between the concepts they require. This has been the focus of research on multi-task learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b185">186]</ref>. Architectures with multiple levels naturally provide such sharing and re-use of components: the low-level visual features (like edge detectors) and intermediate-level visual features (like object parts) that are useful to detect MAN are also useful for a large group of other visual tasks. Deep learning algorithms are based on learning intermediate representations which can be shared across tasks. Hence they can leverage unsupervised data and data from similar tasks <ref type="bibr" target="#b147">[148]</ref> to boost performance on large and challenging problems that routinely suffer from a poverty of labelled data, as has been shown by <ref type="bibr" target="#b36">[37]</ref>, beating the state-of-the-art in several natural language processing tasks. A similar multi-task approach for deep architectures was applied in vision tasks by <ref type="bibr" target="#b1">[2]</ref>. Consider a multi-task setting in which there are different outputs for different tasks, all obtained from a shared pool of highlevel features. The fact that many of these learned features are shared among m tasks provides sharing of statistical strength in proportion to m. Now consider that these learned high-level features can themselves be represented by combining lower-level intermediate features from a common pool. Again statistical strength can be gained in a similar way, and this strategy can be exploited for every level of a deep architecture.</p><p>In addition, learning about a large set of interrelated concepts might provide a key to the kind of broad generalizations that humans appear able to do, which we would not expect from separately trained object detectors, with one detector per visual category. If each high-level category is itself represented through a particular distributed configuration of abstract features from a common pool, generalization to unseen categories could follow naturally from new configurations of these features. Even though only some configurations of these features would present in the training examples, if they represent different aspects of the data, new examples could meaningfully be represented by new configurations of these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Desiderata for Learning AI</head><p>Summarizing some of the above issues, and trying to put them in the broader perspective of AI, we put forward a number of requirements we believe to be important for learning algorithms to approach AI, many of which motivate the research are described here:</p><p>• Ability to learn complex, highly-varying functions, i.e., with a number of variations much greater than the number of training examples. • Ability to learn with little human input the low-level, intermediate, and high-level abstractions that would be useful to represent the kind of complex functions needed for AI tasks. • Ability to learn from a very large set of examples: computation time for training should scale well with the number of examples, i.e., close to linearly. • Ability to learn from mostly unlabeled data, i.e., to work in the semi-supervised setting, where not all the examples come with complete and correct semantic labels. • Ability to exploit the synergies present across a large number of tasks, i.e., multi-task learning. These synergies exist because all the AI tasks provide different views on the same underlying reality. • Strong unsupervised learning (i.e., capturing most of the statistical structure in the observed data), which seems essential in the limit of a large number of tasks and when future tasks are not known ahead of time.</p><p>Other elements are equally important but are not directly connected to the material in this monograph. They include the ability to learn to represent context of varying length and structure <ref type="bibr" target="#b145">[146]</ref>, so as to allow machines to operate in a context-dependent stream of observations and produce a stream of actions, the ability to make decisions when actions influence the future observations and future rewards <ref type="bibr" target="#b180">[181]</ref>, and the ability to influence future observations so as to collect more relevant information about the world, i.e., a form of active learning <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Outline of the Paper</head><p>Section 2 reviews theoretical results (which can be skipped without hurting the understanding of the remainder) showing that an architecture with insufficient depth can require many more computational elements, potentially exponentially more (with respect to input size), than architectures whose depth is matched to the task. We claim that insufficient depth can be detrimental for learning. Indeed, if a solution to the task is represented with a very large but shallow architecture (with many computational elements), a lot of training examples might be needed to tune each of these elements and capture a highly varying function. Section 3.1 is also meant to motivate the reader, this time to highlight the limitations of local generalization and local estimation, which we expect to avoid using deep architectures with a distributed representation (Section 3.2).</p><p>In later sections, the monograph describes and analyzes some of the algorithms that have been proposed to train deep architectures. Section 4 introduces concepts from the neural networks literature relevant to the task of training deep architectures. We first consider the previous difficulties in training neural networks with many layers, and then introduce unsupervised learning algorithms that could be exploited to initialize deep neural networks. Many of these algorithms (including those for the RBM) are related to the auto-encoder: a simple unsupervised algorithm for learning a one-layer model that computes a distributed representation for its input <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b155">156]</ref>. To fully understand RBMs and many related unsupervised learning algorithms, Section 5 introduces the class of energy-based models, including those used to build generative models with hidden variables such as the Boltzmann Machine. Section 6 focuses on the greedy layer-wise training algorithms for Deep Belief Networks (DBNs) <ref type="bibr" target="#b72">[73]</ref> and Stacked Auto-Encoders <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b194">195]</ref>. Section 7 discusses variants of RBMs and auto-encoders that have been recently proposed to extend and improve them, including the use of sparsity, and the modeling of temporal dependencies. Section 8 discusses algorithms for jointly training all the layers of a Deep Belief Network using variational bounds. Finally, we consider in Section 9 forward looking questions such as the hypothesized difficult optimization problem involved in training deep architectures. In particular, we follow up on the hypothesis that part of the success of current learning strategies for deep architectures is connected to the optimization of lower layers. We discuss the principle of continuation methods, which minimize gradually less smooth versions of the desired cost function, to make a dent in the optimization of deep architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Advantages of Deep Architectures</head><p>In this section, we present a motivating argument for the study of learning algorithms for deep architectures, by way of theoretical results revealing potential limitations of architectures with insufficient depth. This part of the monograph (this section and the next) motivates the algorithms described in the later sections, and can be skipped without making the remainder difficult to follow.</p><p>The main point of this section is that some functions cannot be efficiently represented (in terms of number of tunable elements) by architectures that are too shallow. These results suggest that it would be worthwhile to explore learning algorithms for deep architectures, which might be able to represent some functions otherwise not efficiently representable. Where simpler and shallower architectures fail to efficiently represent (and hence to learn) a task of interest, we can hope for learning algorithms that could set the parameters of a deep architecture for this task.</p><p>We say that the expression of a function is compact when it has few computational elements, i.e., few degrees of freedom that need to be tuned by learning. So for a fixed number of training examples, and short of other sources of knowledge injected in the learning algorithm, we would expect that compact representations of the target function<ref type="foot" target="#foot_1">1</ref> would yield better generalization.</p><p>More precisely, functions that can be compactly represented by a depth k architecture might require an exponential number of computational elements to be represented by a depth k -1 architecture. Since the number of computational elements one can afford depends on the number of training examples available to tune or select them, the consequences are not only computational but also statistical: poor generalization may be expected when using an insufficiently deep architecture for representing some functions.</p><p>We consider the case of fixed-dimension inputs, where the computation performed by the machine can be represented by a directed acyclic graph where each node performs a computation that is the application of a function on its inputs, each of which is the output of another node in the graph or one of the external inputs to the graph. The whole graph can be viewed as a circuit that computes a function applied to the external inputs. When the set of functions allowed for the computation nodes is limited to logic gates, such as {AND, OR, NOT}, this is a Boolean circuit, or logic circuit.</p><p>To formalize the notion of depth of architecture, one must introduce the notion of a set of computational elements. An example of such a set is the set of computations that can be performed logic gates. Another is the set of computations that can be performed by an artificial neuron (depending on the values of its synaptic weights). A function can be expressed by the composition of computational elements from a given set. It is defined by a graph which formalizes this composition, with one node per computational element. Depth of architecture refers to the depth of that graph, i.e., the longest path from an input node to an output node. When the set of computational elements is the set of computations an artificial neuron can perform, depth corresponds to the number of layers in a neural network. <ref type="bibr">Let</ref>  and the sin operation, as illustrated in Figure <ref type="figure">2</ref>.1. In the example, there would be a different node for the multiplication a * x and for the final multiplication by x. Each node in the graph is associated with an output value obtained by applying some function on input values that are the outputs of other nodes of the graph. For example, in a logic circuit each node can compute a Boolean function taken from a small set of Boolean functions. The graph as a whole has input nodes and output nodes and computes a function from input to output. The depth of an architecture is the maximum length of a path from any input of the graph to any output of the graph, i.e., 4 in the case of x * sin(a * x + b) in Figure <ref type="figure">2</ref>.1.</p><p>• If we include affine operations and their possible composition with sigmoids in the set of computational elements, linear regression and logistic regression have depth 1, i.e., have a single level. • When we put a fixed kernel computation K(u, v) in the set of allowed operations, along with affine operations, kernel machines <ref type="bibr" target="#b165">[166]</ref> with a fixed kernel can be considered to have two levels. The first level has one element computing K(x, x i ) for each prototype x i (a selected representative training example) and matches the input vector x with the prototypes x i . The second level performs an affine combination b + i α i K(x, x i ) to associate the matching prototypes x i with the expected response. • When we put artificial neurons (affine transformation followed by a non-linearity) in our set of elements, we obtain ordinary multi-layer neural networks <ref type="bibr" target="#b155">[156]</ref>. With the most common choice of one hidden layer, they also have depth two (the hidden layer and the output layer). • Decision trees can also be seen as having two levels, as discussed in Section 3.1.</p><p>• Boosting <ref type="bibr" target="#b51">[52]</ref> usually adds one level to its base learners: that level computes a vote or linear combination of the outputs of the base learners. • Stacking <ref type="bibr" target="#b204">[205]</ref> is another meta-learning algorithm that adds one level. • Based on current knowledge of brain anatomy <ref type="bibr" target="#b172">[173]</ref>, it appears that the cortex can be seen as a deep architecture, with 5-10 levels just for the visual system.</p><p>Although depth depends on the choice of the set of allowed computations for each element, graphs associated with one set can often be converted to graphs associated with another by an graph transformation in a way that multiplies depth. Theoretical results suggest that it is not the absolute number of levels that matters, but the number of levels relative to how many are required to represent efficiently the target function (with some choice of set of computational elements).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Computational Complexity</head><p>The most formal arguments about the power of deep architectures come from investigations into computational complexity of circuits. The basic conclusion that these results suggest is that when a function can be compactly represented by a deep architecture, it might need a very large architecture to be represented by an insufficiently deep one.</p><p>A two-layer circuit of logic gates can represent any Boolean function <ref type="bibr" target="#b126">[127]</ref>. Any Boolean function can be written as a sum of products (disjunctive normal form: AND gates on the first layer with optional negation of inputs, and OR gate on the second layer) or a product of sums (conjunctive normal form: OR gates on the first layer with optional negation of inputs, and AND gate on the second layer). To understand the limitations of shallow architectures, the first result to consider is that with depth-two logical circuits, most Boolean functions require an exponential (with respect to input size) number of logic gates <ref type="bibr" target="#b197">[198]</ref> to be represented.</p><p>More interestingly, there are functions computable with a polynomial-size logic gates circuit of depth k that require exponential size when restricted to depth k -1 <ref type="bibr" target="#b61">[62]</ref>. The proof of this theorem relies on earlier results <ref type="bibr" target="#b207">[208]</ref> showing that d-bit parity circuits of depth 2 have exponential size. The d-bit parity function is defined as usual:</p><formula xml:id="formula_0">parity : (b 1 , . . . , b d ) ∈ {0, 1} d →      1, if d i=1 b i is even 0, otherwise.</formula><p>One might wonder whether these computational complexity results for Boolean circuits are relevant to machine learning. See <ref type="bibr" target="#b139">[140]</ref> for an early survey of theoretical results in computational complexity relevant to learning algorithms. Interestingly, many of the results for Boolean circuits can be generalized to architectures whose computational elements are linear threshold units (also known as artificial neurons <ref type="bibr" target="#b124">[125]</ref>), which compute</p><formula xml:id="formula_1">f (x) = 1 w x+b≥0 (2.1)</formula><p>with parameters w and b. The fan-in of a circuit is the maximum number of inputs of a particular element. Circuits are often organized in layers, like multi-layer neural networks, where elements in a layer only take their input from elements in the previous layer(s), and the first layer is the neural network input. The size of a circuit is the number of its computational elements (excluding input elements, which do not perform any computation).</p><p>Of particular interest is the following theorem, which applies to monotone weighted threshold circuits (i.e., multi-layer neural networks with linear threshold units and positive weights) when trying to represent a function compactly representable with a depth k circuit: Theorem 2.1. A monotone weighted threshold circuit of depth k -1 computing a function f k ∈ F k,N has size at least 2 cN for some constant c &gt; 0 and N &gt; N 0 <ref type="bibr" target="#b62">[63]</ref>.</p><p>The class of functions F k,N is defined as follows. It contains functions with N 2k-2 inputs, defined by a depth k circuit that is a tree. At the leaves of the tree there are unnegated input variables, and the function value is at the root. The ith level from the bottom consists of AND gates when i is even and OR gates when i is odd. The fan-in at the top and bottom level is N and at all other levels it is N 2 .</p><p>The above results do not prove that other classes of functions (such as those we want to learn to perform AI tasks) require deep architectures, nor that these demonstrated limitations apply to other types of circuits. However, these theoretical results beg the question: are the depth 1, 2 and 3 architectures (typically found in most machine learning algorithms) too shallow to represent efficiently more complicated functions of the kind needed for AI tasks? Results such as the above theorem also suggest that there might be no universally right depth: each function (i.e., each task) might require a particular minimum depth (for a given set of computational elements). We should therefore strive to develop learning algorithms that use the data to determine the depth of the final architecture. Note also that recursive computation defines a computation graph whose depth increases linearly with the number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Informal Arguments</head><p>Depth of architecture is connected to the notion of highly varying functions. We argue that, in general, deep architectures can compactly represent highly varying functions which would otherwise require a very large size to be represented with an inappropriate architecture. We say that a function is highly varying when a piecewise approximation (e.g., piecewise-constant or piecewise-linear) of that function would require a large number of pieces. A deep architecture is a composition of many operations, and it could in any case be represented by a possibly very large depth-2 architecture. The composition of computational units in a small but deep circuit can actually be seen as an efficient "factorization" of a large but shallow circuit. Reorganizing the way in which computational units are composed can have a drastic effect on the efficiency of representation size. For example, imagine a depth 2k representation of polynomials where odd layers implement products and even layers implement sums. This architecture can be seen as a particularly efficient factorization, which when expanded into a depth 2 architecture such as a sum of products, might require a huge number of terms in the sum: consider a level 1 product (like x 2 x 3 in Figure <ref type="figure">2</ref>.2) from the depth 2k architecture. It could occur many times as a factor in many terms of the depth 2 architecture. One can see in this example that deep architectures can be advantageous if some computations (e.g., at one level) can be shared (when considering the expanded depth 2 expression): in that case, the overall expression to be represented can be factored out, i.e., represented more compactly with a deep architecture. Further examples suggesting greater expressive power of deep architectures and their potential for AI and machine learning are also discussed by <ref type="bibr" target="#b18">[19]</ref>. An earlier discussion of the expected advantages of deeper architectures in a more cognitive perspective is found in <ref type="bibr" target="#b190">[191]</ref>. Note that connectionist cognitive psychologists have been studying for long time the idea of neural computation organized with a hierarchy of levels of representation corresponding to different levels of abstraction, with a distributed representation at each level <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b122">123,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b156">157]</ref>. The modern deep architecture approaches discussed here owe a lot to these early developments. These concepts were introduced in cognitive psychology (and then in computer science / AI) in order to explain phenomena that were not as naturally captured by earlier cognitive models, and also to connect the cognitive explanation with the computational characteristics of the neural substrate.</p><p>To conclude, a number of computational complexity results strongly suggest that functions that can be compactly represented with a depth k architecture could require a very large number of elements in order to be represented by a shallower architecture. Since each element of the architecture might have to be selected, i.e., learned, using examples, these results suggest that depth of architecture can be very important from the point of view of statistical efficiency. This notion is developed further in the next section, discussing a related weakness of many shallow architectures associated with non-parametric learning algorithms: locality in input space of the estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local vs Non-Local Generalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Limits of Matching Local Templates</head><p>How can a learning algorithm compactly represent a "complicated" function of the input, i.e., one that has many more variations than the number of available training examples? This question is both connected to the depth question and to the question of locality of estimators. We argue that local estimators are inappropriate to learn highly varying functions, even though they can potentially be represented efficiently with deep architectures. An estimator that is local in input space obtains good generalization for a new input x by mostly exploiting training examples in the neighborhood of x. For example, the k nearest neighbors of the test point x, among the training examples, vote for the prediction at x. Local estimators implicitly or explicitly partition the input space in regions (possibly in a soft rather than hard way) and require different parameters or degrees of freedom to account for the possible shape of the target function in each of the regions. When many regions are necessary because the function is highly varying, the number of required parameters will also be large, and thus the number of examples needed to achieve good generalization.</p><p>The local generalization issue is directly connected to the literature on the curse of dimensionality, but the results we cite show that what matters for generalization is not dimensionality, but instead the number of "variations" of the function we wish to obtain after learning. For example, if the function represented by the model is piecewise-constant (e.g., decision trees), then the question that matters is the number of pieces required to approximate properly the target function. There are connections between the number of variations and the input dimension: one can readily design families of target functions for which the number of variations is exponential in the input dimension, such as the parity function with d inputs.</p><p>Architectures based on matching local templates can be thought of as having two levels. The first level is made of a set of templates which can be matched to the input. A template unit will output a value that indicates the degree of matching. The second level combines these values, typically with a simple linear combination (an OR-like operation), in order to estimate the desired output. One can think of this linear combination as performing a kind of interpolation in order to produce an answer in the region of input space that is between the templates.</p><p>The prototypical example of architectures based on matching local templates is the kernel machine <ref type="bibr" target="#b165">[166]</ref> </p><formula xml:id="formula_2">f (x) = b + i α i K(x, x i ),<label>(3.1)</label></formula><p>where b and α i form the second level, while on the first level, the kernel function K(x, x i ) matches the input x to the training example x i (the sum runs over some or all of the input patterns in the training set).</p><p>In the above equation, f (x) could be for example, the discriminant function of a classifier, or the output of a regression predictor. A kernel is local when K(x, x i ) &gt; ρ is true only for x in some connected region around x i (for some threshold ρ). The size of that region can usually be controlled by a hyper-parameter of the kernel function. An example of local kernel is the Gaussian kernel K(x, x i ) = e -||x-x i || 2 /σ 2 , where σ controls the size of the region around x i . We can see the Gaussian kernel as computing a soft conjunction, because it can be written as a product of one-dimensional conditions: K(u, v) = j e -(u j -v j ) 2 /σ 2 . If |u jv j |/σ is small for all dimensions j, then the pattern matches and K(u, v) is large. If |u jv j |/σ is large for a single j, then there is no match and K(u, v) is small.</p><p>Well-known examples of kernel machines include not only Support Vector Machines (SVMs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref> and Gaussian processes <ref type="bibr" target="#b202">[203]</ref> <ref type="foot" target="#foot_2">1</ref> for classification and regression, but also classical non-parametric learning algorithms for classification, regression and density estimation, such as the k-nearest neighbor algorithm, Nadaraya-Watson or Parzen windows density, regression estimators, etc. Below, we discuss manifold learning algorithms such as Isomap and LLE that can also be seen as local kernel machines, as well as related semi-supervised learning algorithms also based on the construction of a neighborhood graph (with one node per example and arcs between neighboring examples).</p><p>Kernel machines with a local kernel yield generalization by exploiting what could be called the smoothness prior: the assumption that the target function is smooth or can be well approximated with a smooth function. For example, in supervised learning, if we have the training example (x i , y i ), then it makes sense to construct a predictor f (x) which will output something close to y i when x is close to x i . Note how this prior requires defining a notion of proximity in input space. This is a useful prior, but one of the claims made <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b18">[19]</ref> is that such a prior is often insufficient to generalize when the target function is highly varying in input space.</p><p>The limitations of a fixed generic kernel such as the Gaussian kernel have motivated a lot of research in designing kernels based on prior knowledge about the task <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b166">167]</ref>. However, if we lack sufficient prior knowledge for designing an appropriate kernel, can we learn it? This question also motivated much research <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b195">196]</ref>, and deep architectures can be viewed as a promising development in this direction. It has been shown that a Gaussian Process kernel machine can be improved using a Deep Belief Network to learn a feature space <ref type="bibr" target="#b159">[160]</ref>: after training the Deep Belief Network, its parameters are used to initialize a deterministic non-linear transformation (a multi-layer neural network) that computes a feature vector (a new feature space for the data), and that transformation can be tuned to minimize the prediction error made by the Gaussian process, using a gradient-based optimization. The feature space can be seen as a learned representation of the data. Good representations bring close to each other examples which share abstract characteristics that are relevant factors of variation of the data distribution. Learning algorithms for deep architectures can be seen as ways to learn a good feature space for kernel machines.</p><p>Consider one direction v in which a target function f (what the learner should ideally capture) goes up and down (i.e., as α increases, f (x + αv)b crosses 0, becomes positive, then negative, positive, then negative, etc.), in a series of "bumps". Following <ref type="bibr" target="#b164">[165]</ref>, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref> show that for kernel machines with a Gaussian kernel, the required number of examples grows linearly with the number of bumps in the target function to be learned. They also show that for a maximally varying function such as the parity function, the number of examples necessary to achieve some error rate with a Gaussian kernel machine is exponential in the input dimension. For a learner that only relies on the prior that the target function is locally smooth (e.g., Gaussian kernel machines), learning a function with many sign changes in one direction is fundamentally difficult (requiring a large VC-dimension, and a correspondingly large number of examples). However, learning could work with other classes of functions in which the pattern of variations is captured compactly (a trivial example is when the variations are periodic and the class of functions includes periodic functions that approximately match).</p><p>For complex tasks in high dimension, the complexity of the decision surface could quickly make learning impractical when using a local kernel method. It could also be argued that if the curve has many variations and these variations are not related to each other through an underlying regularity, then no learning algorithm will do much better than estimators that are local in input space. However, it might be worth looking for more compact representations of these variations, because if one could be found, it would be likely to lead to better generalization, especially for variations not seen in the training set.</p><p>Of course this could only happen if there were underlying regularities to be captured in the target function; we expect this property to hold in AI tasks.</p><p>Estimators that are local in input space are found not only in supervised learning algorithms such as those discussed above, but also in unsupervised and semi-supervised learning algorithms, e.g., Locally Linear Embedding <ref type="bibr" target="#b154">[155]</ref>, Isomap <ref type="bibr" target="#b184">[185]</ref>, kernel Principal Component Analysis <ref type="bibr" target="#b167">[168]</ref> (or kernel PCA) Laplacian Eigenmaps <ref type="bibr" target="#b9">[10]</ref>, Manifold Charting <ref type="bibr" target="#b25">[26]</ref>, spectral clustering algorithms <ref type="bibr" target="#b198">[199]</ref>, and kernel-based non-parametric semi-supervised algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b208">209,</ref><ref type="bibr" target="#b209">210]</ref>. Most of these unsupervised and semi-supervised algorithms rely on the neighborhood graph: a graph with one node per example and arcs between near neighbors. With these algorithms, one can get a geometric intuition of what they are doing, as well as how being local estimators can hinder them. This is illustrated with the example in Figure <ref type="figure">3</ref>.1 in the case of manifold learning. Here again, it was found that in order to Fig. <ref type="figure">3</ref>.1 The set of images associated with the same object class forms a manifold or a set of disjoint manifolds, i.e., regions of lower dimension than the original space of images. By rotating or shrinking, e.g., a digit 4, we get other images of the same class, i.e., on the same manifold. Since the manifold is locally smooth, it can in principle be approximated locally by linear patches, each being tangent to the manifold. Unfortunately, if the manifold is highly curved, the patches are required to be small, and exponentially many might be needed with respect to manifold dimension. Graph graciously provided by Pascal Vincent.</p><p>cover the many possible variations in the function to be learned, one needs a number of examples proportional to the number of variations to be covered <ref type="bibr" target="#b20">[21]</ref>.</p><p>Finally let us consider the case of semi-supervised learning algorithms based on the neighborhood graph <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b208">209,</ref><ref type="bibr" target="#b209">210]</ref>. These algorithms partition the neighborhood graph in regions of constant label. It can be shown that the number of regions with constant label cannot be greater than the number of labeled examples <ref type="bibr" target="#b12">[13]</ref>. Hence one needs at least as many labeled examples as there are variations of interest for the classification. This can be prohibitive if the decision surface of interest has a very large number of variations.</p><p>Decision trees <ref type="bibr" target="#b27">[28]</ref> are among the best studied learning algorithms. Because they can focus on specific subsets of input variables, at first blush they seem non-local. However, they are also local estimators in the sense of relying on a partition of the input space and using separate parameters for each region <ref type="bibr" target="#b13">[14]</ref>, with each region associated with a leaf of the decision tree. This means that they also suffer from the limitation discussed above for other non-parametric learning algorithms: they need at least as many training examples as there are variations of interest in the target function, and they cannot generalize to new variations not covered in the training set. Theoretical analysis <ref type="bibr" target="#b13">[14]</ref> shows specific classes of functions for which the number of training examples necessary to achieve a given error rate is exponential in the input dimension. This analysis is built along lines similar to ideas exploited previously in the computational complexity literature <ref type="bibr" target="#b40">[41]</ref>. These results are also in line with previous empirical results <ref type="bibr" target="#b142">[143,</ref><ref type="bibr" target="#b193">194]</ref> showing that the generalization performance of decision trees degrades when the number of variations in the target function increases.</p><p>Ensembles of trees (like boosted trees <ref type="bibr" target="#b51">[52]</ref>, and forests <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b26">27]</ref>) are more powerful than a single tree. They add a third level to the architecture which allows the model to discriminate among a number of regions exponential in the number of parameters <ref type="bibr" target="#b13">[14]</ref>. As illustrated in Figure <ref type="figure">3</ref>.2, they implicitly form a distributed representation (a notion discussed further in Section 3.2) with the output of all the trees in the forest. Each tree in an ensemble can be associated with a discrete symbol identifying the leaf/region in which the input example falls for Fig. <ref type="figure">3</ref>.2 Whereas a single decision tree (here just a two-way partition) can discriminate among a number of regions linear in the number of parameters (leaves), an ensemble of trees (left) can discriminate among a number of regions exponential in the number of trees, i.e., exponential in the total number of parameters (at least as long as the number of trees does not exceed the number of inputs, which is not quite the case here). Each distinguishable region is associated with one of the leaves of each tree (here there are three 2-way trees, each defining two regions, for a total of seven regions). This is equivalent to a multi-clustering, here three clusterings each associated with two regions. A binomial RBM with three hidden units (right) is a multi-clustering with 2 linearly separated regions per partition (each associated with one of the three binomial hidden units). A multi-clustering is therefore a distributed representation of the input pattern.</p><p>that tree. The identity of the leaf node in which the input pattern is associated for each tree forms a tuple that is a very rich description of the input pattern: it can represent a very large number of possible patterns, because the number of intersections of the leaf regions associated with the n trees can be exponential in n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Distributed Representations</head><p>In Section 1.2, we argued that deep architectures call for making choices about the kind of representation at the interface between levels of the system, and we introduced the basic notion of local representation (discussed further in the previous section), of distributed representation, and of sparse distributed representation. The idea of distributed representation is an old idea in machine learning and neural networks research <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b127">128,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b169">170]</ref>, and it may be of help in dealing with the curse of dimensionality and the limitations of local generalization. A cartoon local representation for integers i ∈ {1, 2, . . . , N} is a vector r(i) of N bits with a single 1 and N -1 zeros, i.e., with jth element r j (i) = 1 i=j , called the one-hot representation of i. A distributed representation for the same integer could be a vector of log 2 N bits, which is a much more compact way to represent i. For the same number of possible configurations, a distributed representation can potentially be exponentially more compact than a very local one. Introducing the notion of sparsity (e.g., encouraging many units to take the value 0) allows for representations that are in between being fully local (i.e., maximally sparse) and non-sparse (i.e., dense) distributed representations. Neurons in the cortex are believed to have a distributed and sparse representation <ref type="bibr" target="#b138">[139]</ref>, with around 1-4% of the neurons active at any one time <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b112">113]</ref>. In practice, we often take advantage of representations which are continuous-valued, which increases their expressive power. An example of continuous-valued local representation is one where the ith element varies according to some distance between the input and a prototype or region center, as with the Gaussian kernel discussed in Section 3.1. In a distributed representation the input pattern is represented by a set of features that are not mutually exclusive, and might even be statistically independent. For example, clustering algorithms do not build a distributed representation since the clusters are essentially mutually exclusive, whereas Independent Component Analysis (ICA) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b141">142]</ref> and Principal Component Analysis (PCA) <ref type="bibr" target="#b81">[82]</ref> build a distributed representation. Consider a discrete distributed representation r(x) for an input pattern x, where r i (x) ∈ {1, . . . M}, i ∈ {1, . . . , N}. Each r i (x) can be seen as a classification of x into M classes. As illustrated in Figure <ref type="figure">3</ref>.2 (with M = 2), each r i (x) partitions the x-space in M regions, but the different partitions can be combined to give rise to a potentially exponential number of possible intersection regions in x-space, corresponding to different configurations of r(x). Note that when representing a particular input distribution, some configurations may be impossible because they are incompatible. For example, in language modeling, a local representation of a word could directly encode its identity by an index in the vocabulary table, or equivalently a one-hot code with as many entries as the vocabulary size. On the other hand, a distributed representation could represent the word by concatenating in one vector indicators for syntactic features (e.g., distribution over parts of speech it can have), morphological features (which suffix or prefix does it have?), and semantic features (is it the name of a kind of animal? etc). Like in clustering, we construct discrete classes, but the potential number of combined classes is huge: we obtain what we call a multi-clustering and that is similar to the idea of overlapping clusters and partial memberships <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref> in the sense that cluster memberships are not mutually exclusive. Whereas clustering forms a single partition and generally involves a heavy loss of information about the input, a multi-clustering provides a set of separate partitions of the input space. Identifying which region of each partition the input example belongs to forms a description of the input pattern which might be very rich, possibly not losing any information. The tuple of symbols specifying which region of each partition the input belongs to can be seen as a transformation of the input into a new space, where the statistical structure of the data and the factors of variation in it could be disentangled. This corresponds to the kind of partition of x-space that an ensemble of trees can represent, as discussed in the previous section. This is also what we would like a deep architecture to capture, but with multiple levels of representation, the higher levels being more abstract and representing more complex regions of input space.</p><p>In the realm of supervised learning, multi-layer neural networks <ref type="bibr" target="#b156">[157,</ref><ref type="bibr" target="#b155">156]</ref> and in the realm of unsupervised learning, Boltzmann machines <ref type="bibr" target="#b0">[1]</ref> have been introduced with the goal of learning distributed internal representations in the hidden layers. Unlike in the linguistic example above, the objective is to let learning algorithms discover the features that compose the distributed representation. In a multi-layer neural network with more than one hidden layer, there are several representations, one at each layer. Learning multiple levels of distributed representations involves a challenging training problem, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-Layer Neural Networks</head><p>A typical set of equations for multi-layer neural networks <ref type="bibr" target="#b155">[156]</ref> is the following. As illustrated in Figure <ref type="figure">4</ref>.1, layer k computes an output vector h k using the output h k-1 of the previous layer, starting with the input x = h 0 ,</p><formula xml:id="formula_3">h k = tanh(b k + W k h k-1 ) (4.1)</formula><p>with parameters b k (a vector of offsets) and W k (a matrix of weights).</p><p>The tanh is applied element-wise and can be replaced by sigm(u) = 1/(1 + e -u ) = 1 2 (tanh(u) + 1) or other saturating non-linearities. The top layer output h is used for making a prediction and is combined with a supervised target y into a loss function L(h , y), typically convex in b + W h -1 . The output layer might have a non-linearity different from the one used in other layers, e.g., the softmax</p><formula xml:id="formula_4">h i = e b i +W i h -1 j e b j +W j h -1 (4.2)</formula><p>where W i is the ith row of W , h i is positive and i h i = 1. The softmax output h i can be used as estimator of P (Y = i|x), with the ... Fig. <ref type="figure">4</ref>.1 Multi-layer neural network, typically used in supervised learning to make a prediction or classification, through a series of layers, each of which combines an affine operation and a non-linearity. Deterministic transformations are computed in a feedforward way from the input x, through the hidden layers h k , to the network output h , which gets compared with a label y to obtain the loss L(h , y) to be minimized. interpretation that Y is the class associated with input pattern x. In this case one often uses the negative conditional log-likelihood L(h , y) =log P (Y = y|x) =log h y as a loss, whose expected value over (x, y) pairs is to be minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Challenge of Training Deep Neural Networks</head><p>After having motivated the need for deep architectures that are nonlocal estimators, we now turn to the difficult problem of training them. Experimental evidence suggests that training deep architectures is more difficult than training shallow architectures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Until 2006, deep architectures have not been discussed much in the machine learning literature, because of poor training and generalization errors generally obtained <ref type="bibr" target="#b16">[17]</ref> using the standard random initialization of the parameters. Note that deep convolutional neural networks <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b174">175,</ref><ref type="bibr" target="#b152">153]</ref> were found easier to train, as discussed in Section 4.5, for reasons that have yet to be really clarified.</p><p>Many unreported negative observations as well as the experimental results in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref> suggest that gradient-based training of deep supervised multi-layer neural networks (starting from random initialization) gets stuck in "apparent local minima or plateaus",<ref type="foot" target="#foot_3">1</ref> and that as the architecture gets deeper, it becomes more difficult to obtain good generalization. When starting from random initialization, the solutions obtained with deeper neural networks appear to correspond to poor solutions that perform worse than the solutions obtained for networks with 1 or 2 hidden layers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b97">98]</ref>. This happens even though k + 1layer nets can easily represent what a k-layer net can represent (without much added capacity), whereas the converse is not true. However, it was discovered <ref type="bibr" target="#b72">[73]</ref> that much better results could be achieved when pre-training each layer with an unsupervised learning algorithm, one layer after the other, starting with the first layer (that directly takes in input the observed x). The initial experiments used the RBM generative model for each layer <ref type="bibr" target="#b72">[73]</ref>, and were followed by experiments yielding similar results using variations of auto-encoders for training each layer <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b194">195]</ref>. Most of these papers exploit the idea of greedy layer-wise unsupervised learning (developed in more detail in the next section): first train the lower layer with an unsupervised learning algorithm (such as one for the RBM or some auto-encoder), giving rise to an initial set of parameter values for the first layer of a neural network. Then use the output of the first layer (a new representation for the raw input) as input for another layer, and similarly initialize that layer with an unsupervised learning algorithm. After having thus initialized a number of layers, the whole neural network can be fine-tuned with respect to a supervised training criterion as usual. The advantage of unsupervised pre-training versus random initialization was clearly demonstrated in several statistical comparisons <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b98">99]</ref>. What principles might explain the improvement in classification error observed in the literature when using unsupervised pre-training? One clue may help to identify the principles behind the success of some training algorithms for deep architectures, and it comes from algorithms that exploit neither RBMs nor auto-encoders <ref type="bibr" target="#b130">[131,</ref><ref type="bibr" target="#b201">202]</ref>. What these algorithms have in common with the training algorithms based on RBMs and auto-encoders is layer-local unsupervised criteria, i.e., the idea that injecting an unsupervised training signal at each layer may help to guide the parameters of that layer towards better regions in parameter space. In <ref type="bibr" target="#b201">[202]</ref>, the neural networks are trained using pairs of examples (x, x), which are either supposed to be "neighbors" (or of the same class) or not. Consider h k (x) the level-k representation of x in the model. A local training criterion is defined at each layer that pushes the intermediate representations h k (x) and h k (x) either towards each other or away from each other, according to whether x and x are supposed to be neighbors or not (e.g., k-nearest neighbors in input space). The same criterion had already been used successfully to learn a low-dimensional embedding with an unsupervised manifold learning algorithm <ref type="bibr" target="#b58">[59]</ref> but is here <ref type="bibr" target="#b201">[202]</ref> applied at one or more intermediate layer of the neural network. Following the idea of slow feature analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b203">204]</ref> exploit the temporal constancy of high-level abstraction to provide an unsupervised guide to intermediate layers: successive frames are likely to contain the same object.</p><p>Clearly, test errors can be significantly improved with these techniques, at least for the types of tasks studied, but why? One basic question to ask is whether the improvement is basically due to better optimization or to better regularization. As discussed below, the answer may not fit the usual definition of optimization and regularization.</p><p>In some experiments <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b97">98]</ref> it is clear that one can get training classification error down to zero even with a deep neural network that has no unsupervised pre-training, pointing more in the direction of a regularization effect than an optimization effect. Experiments in <ref type="bibr" target="#b49">[50]</ref> also give evidence in the same direction: for the same training error (at different points during training), test error is systematically lower with unsupervised pre-training. As discussed in <ref type="bibr" target="#b49">[50]</ref>, unsupervised pretraining can be seen as a form of regularizer (and prior): unsupervised pre-training amounts to a constraint on the region in parameter space where a solution is allowed. The constraint forces solutions "near" 2 ones that correspond to the unsupervised training, i.e., hopefully corresponding to solutions capturing significant statistical structure in the input. On the other hand, other experiments <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b97">98]</ref> suggest that poor tuning of the lower layers might be responsible for the worse results without pre-training: when the top hidden layer is constrained (forced to be small) the deep networks with random initialization (no unsupervised pre-training) do poorly on both training and test sets, and much worse than pre-trained networks. In the experiments mentioned earlier where training error goes to zero, it was always the case that the number of hidden units in each layer (a hyper-parameter) was allowed to be as large as necessary (to minimize error on a validation set). The explanatory hypothesis proposed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b97">98]</ref> is that when the top hidden layer is unconstrained, the top two layers (corresponding to a regular 1-hidden-layer neural net) are sufficient to fit the training set, using as input the representation computed by the lower layers, even if that representation is poor. On the other hand, with unsupervised pre-training, the lower layers are 'better optimized', and a smaller top layer suffices to get a low training error but also yields better generalization. Other experiments described in <ref type="bibr" target="#b49">[50]</ref> are also consistent with the explanation that with random parameter initialization, the lower layers (closer to the input layer) are poorly trained. These experiments show that the effect of unsupervised pre-training is most marked for the lower layers of a deep architecture.</p><p>We know from experience that a two-layer network (one hidden layer) can be well trained in general, and that from the point of view of the top two layers in a deep network, they form a shallow network whose input is the output of the lower layers. Optimizing the last layer of a deep neural network is a convex optimization problem for the training criteria commonly used. Optimizing the last two layers, although not convex, is known to be much easier than optimizing a deep network (in fact when the number of hidden units goes to infinity, the training criterion of a two-layer network can be cast as convex <ref type="bibr" target="#b17">[18]</ref>).</p><p>If there are enough hidden units (i.e., enough capacity) in the top hidden layer, training error can be brought very low even when the lower layers are not properly trained (as long as they preserve most of the information about the raw input), but this may bring worse generalization than shallow neural networks. When training error is low and test error is high, we usually call the phenomenon overfitting. Since unsupervised pre-training brings test error down, that would point to it as a kind of data-dependent regularizer. Other strong evidence has been presented suggesting that unsupervised pre-training acts like a regularizer <ref type="bibr" target="#b49">[50]</ref>: in particular, when there is not enough capacity, unsupervised pre-training tends to hurt generalization, and when the training set size is "small" (e.g., MNIST, with less than hundred thousand examples), although unsupervised pre-training brings improved test error, it tends to produce larger training error.</p><p>On the other hand, for much larger training sets, with better initialization of the lower hidden layers, both training and generalization error can be made significantly lower when using unsupervised pre-training (see Figure <ref type="figure">4</ref>.2 and discussion below). We hypothesize that in a welltrained deep neural network, the hidden layers form a "good" representation of the data, which helps to make good predictions. When the lower layers are poorly initialized, these deterministic and continuous representations generally keep most of the information about the input, but these representations might scramble the input and hurt rather than help the top layers to perform classifications that generalize well.</p><p>According to this hypothesis, although replacing the top two layers of a deep neural network by convex machinery such as a Gaussian process or an SVM can yield some improvements <ref type="bibr" target="#b18">[19]</ref>, especially on the training error, it would not help much in terms of generalization if the lower layers have not been sufficiently optimized, i.e., if a good representation of the raw input has not been discovered.</p><p>Hence, one hypothesis is that unsupervised pre-training helps generalization by allowing for a 'better' tuning of lower layers of a deep architecture. Although training error can be reduced either by exploiting only the top layers ability to fit the training examples, better generalization is achieved when all the layers are tuned appropriately. Another source of better generalization could come from a form of regularization: with unsupervised pre-training, the lower layers are constrained to capture regularities of the input distribution. Consider random inputoutput pairs (X, Y ). Such regularization is similar to the hypothesized effect of unlabeled examples in semi-supervised learning <ref type="bibr" target="#b99">[100]</ref> or the regularization effect achieved by maximizing the likelihood of P (X, Y ) (generative models) vs P (Y |X) (discriminant models) <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b136">137]</ref>. If the true P (X) and P (Y |X) are unrelated as functions of X (e.g., chosen independently, so that learning about one does not inform us of the other), then unsupervised learning of P (X) is not going to help learning P (Y |X). But if they are related, <ref type="foot" target="#foot_5">3</ref> and if the same parameters are involved in estimating P (X) and P (Y |X), <ref type="foot" target="#foot_6">4</ref> then each (X, Y ) pair brings information on P (Y |X) not only in the usual way but also through P (X). For example, in a Deep Belief Net, both distributions share essentially the same parameters, so the parameters involved in estimating P (Y |X) benefit from a form of data-dependent regularization: they have to agree to some extent with P (Y |X) as well as with P (X).</p><p>Let us return to the optimization versus regularization explanation of the better results obtained with unsupervised pre-training. Note how one should be careful when using the word 'optimization' here. We do not have an optimization difficulty in the usual sense of the word. Indeed, from the point of view of the whole network, there is no difficulty since one can drive training error very low, by relying mostly on the top two layers. However, if one considers the problem of tuning the lower layers (while keeping small either the number of hidden units of the penultimate layer (i.e., top hidden layer) or the magnitude of the weights of the top two layers), then one can maybe talk about an optimization difficulty. One way to reconcile the optimization and regularization viewpoints might be to consider the truly online setting (where examples come from an infinite stream and one does not cycle back through a training set). In that case, online gradient descent is performing a stochastic optimization of the generalization error. If the effect of unsupervised pre-training was purely one of regularization, one would expect that with a virtually infinite training set, online error with or without pre-training would converge to the same level. On the other hand, if the explanatory hypothesis presented here is correct, we would expect that unsupervised pre-training would bring clear benefits even in the online setting. To explore that question, we have used the 'infinite MNIST' dataset <ref type="bibr" target="#b119">[120]</ref>, i.e., a virtually infinite stream of MNIST-like digit images (obtained by random translations, rotations, scaling, etc. defined in <ref type="bibr" target="#b175">[176]</ref>). As illustrated in The figure strongly suggests that unsupervised pre-training converges to a lower error, i.e., that it acts not only as a regularizer but also to find better minima of the optimized criterion. In spite of appearances, this does not contradict the regularization hypothesis: because of local minima, the regularization effect persists even as the number of examples goes to infinity. The flip side of this interpretation is that once the dynamics are trapped near some apparent local minimum, more labeled examples do not provide a lot more new information.</p><p>To explain that lower layers would be more difficult to optimize, the above clues suggest that the gradient propagated backwards into the lower layer might not be sufficient to move the parameters into regions corresponding to good solutions. According to that hypothesis, the optimization with respect to the lower level parameters gets stuck in a poor apparent local minimum or plateau (i.e., small gradient). Since gradient-based training of the top layers works reasonably well, it would mean that the gradient becomes less informative about the required changes in the parameters as we move back towards the lower layers, or that the error function becomes too ill-conditioned for gradient descent to escape these apparent local minima. As argued in Section 4.5, this might be connected with the observation that deep convolutional neural networks are easier to train, maybe because they have a very special sparse connectivity in each layer. There might also be a link between this difficulty in exploiting the gradient in deep networks and the difficulty in training recurrent neural networks through long sequences, analyzed in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b118">119]</ref>. A recurrent neural network can be "unfolded in time" by considering the output of each neuron at different time steps as different variables, making the unfolded network over a long input sequence a very deep architecture. In recurrent neural networks, the training difficulty can be traced to a vanishing (or sometimes exploding) gradient propagated through many non-linearities. There is an additional difficulty in the case of recurrent neural networks, due to a mismatch between short-term (i.e., shorter paths in unfolded graph of computations) and long-term components of the gradient (associated with longer paths in that graph).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised Learning for Deep Architectures</head><p>As we have seen above, layer-wise unsupervised learning has been a crucial component of all the successful learning algorithms for deep architectures up to now. If gradients of a criterion defined at the output layer become less useful as they are propagated backwards to lower layers, it is reasonable to believe that an unsupervised learning criterion defined at the level of a single layer could be used to move its parameters in a favorable direction. It would be reasonable to expect this if the single-layer learning algorithm discovered a representation that captures statistical regularities of the layer's input. PCA and the standard variants of ICA requiring as many causes as signals seem inappropriate because they generally do not make sense in the so-called overcomplete case, where the number of outputs of the layer is greater than the number of its inputs. This suggests looking in the direction of extensions of ICA to deal with the overcomplete case <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b183">184]</ref>, as well as algorithms related to PCA and ICA, such as auto-encoders and RBMs, which can be applied in the overcomplete case. Indeed, experiments performed with these one-layer unsupervised learning algorithms in the context of a multi-layer system confirm this idea <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b152">153]</ref>. Furthermore, stacking linear projections (e.g., two layers of PCA) is still a linear transformation, i.e., not building deeper architectures.</p><p>In addition to the motivation that unsupervised learning could help reduce the dependency on the unreliable update direction given by the gradient of a supervised criterion, we have already introduced another motivation for using unsupervised learning at each level of a deep architecture. It could be a way to naturally decompose the problem into sub-problems associated with different levels of abstraction. We know that unsupervised learning algorithms can extract salient information about the input distribution. This information can be captured in a distributed representation, i.e., a set of features which encode the salient factors of variation in the input. A one-layer unsupervised learning algorithm could extract such salient features, but because of the limited capacity of that layer, the features extracted on the first level of the architecture can be seen as low-level features. It is conceivable that learning a second layer based on the same principle but taking as input the features learned with the first layer could extract slightly higherlevel features. In this way, one could imagine that higher-level abstractions that characterize the input could emerge. Note how in this process all learning could remain local to each layer, therefore side-stepping the issue of gradient diffusion that might be hurting gradient-based learning of deep neural networks, when we try to optimize a single global criterion. This motivates the next section, where we discuss deep generative architectures and introduce Deep Belief Networks formally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Deep Generative Architectures</head><p>Besides being useful for pre-training a supervised predictor, unsupervised learning in deep architectures can be of interest to learn a distribution and generate samples from it. Generative models can often be represented as graphical models <ref type="bibr" target="#b90">[91]</ref>: these are visualized as graphs in which nodes represent random variables and arcs say something about the type of dependency existing between the random variables. The joint distribution of all the variables can be written in terms of products involving only a node and its neighbors in the graph. With directed arcs (defining parenthood), a node is conditionally independent of its ancestors, given its parents. Some of the random variables in a graphical model can be observed, and others cannot (called hidden variables). Sigmoid belief networks are generative multi-layer neural networks that were proposed and studied before 2006, and trained using variational approximations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b163">164,</ref><ref type="bibr" target="#b188">189]</ref>. In a sigmoid belief network, the units (typically binary random variables) in each layer are independent given the values of the units in the layer above, as illustrated in Figure <ref type="figure">4</ref>.3. The typical parametrization of these conditional distributions (going downwards instead of upwards in ordinary neural nets) is similar to the neuron activation equation of Equation (4.1):</p><formula xml:id="formula_5">P (h k i = 1|h k+1 ) = sigm(b k i + j W k+1 i,j h k+1 j ) (4.3)</formula><p>where h k i is the binary activation of hidden node i in layer k, h k is the vector (h k 1 , h k 2 , . . .), and we denote the input vector x = h 0 . Note how the notation P (. . .) always represents a probability distribution associated with our model, whereas P is the training distribution (the </p><formula xml:id="formula_6">P (x, h 1 , . . . ,h ) = P (h ) -1 k=1 P (h k |h k+1 ) P (x|h 1 ) (4.4)</formula><p>and marginalization yields P (x), but this is intractable in practice except for tiny models. In a sigmoid belief network, the top level prior P (h ) is generally chosen to be factorized, i.e., very simple:</p><formula xml:id="formula_7">P (h ) = i P (h i )</formula><p>, and a single Bernoulli parameter is required for each P (h i = 1) in the case of binary units. Deep Belief Networks are similar to sigmoid belief networks, but with a slightly different parametrization for the top two layers, as illustrated in Figure <ref type="figure">4</ref>.4: There are no links between units of the same layer, only between input (or visible) units x j and hidden units h i , making the conditionals P (h|x) and P (x|h) factorize conveniently.</p><formula xml:id="formula_8">P (x, h 1 , . . . ,h ) = P (h -1 , h )</formula><p>The joint distribution of the top two layers is a Restricted Boltzmann Machine (RBM), </p><formula xml:id="formula_9">P (h -1 , h ) ∝ e b h -1 +c h +h W h -1 (4.6) illustrated in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Convolutional Neural Networks</head><p>Although deep supervised neural networks were generally found too difficult to train before the use of unsupervised pre-training, there is one notable exception: convolutional neural networks. Convolutional nets were inspired by the visual system's structure, and in particular by the models of it proposed by <ref type="bibr" target="#b82">[83]</ref>. The first computational models based on these local connectivities between neurons and on hierarchically organized transformations of the image are found in Fukushima's Neocognitron <ref type="bibr" target="#b53">[54]</ref>. As he recognized, when neurons with the same parameters are applied on patches of the previous layer at different locations, a form of translational invariance is obtained. Later, LeCun and collaborators, following up on this idea, designed and trained convolutional networks using the error gradient, obtaining state-of-the-art performance <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b103">104]</ref> on several pattern recognition tasks. Modern understanding of the physiology of the visual system is consistent with the processing style found in convolutional networks <ref type="bibr" target="#b172">[173]</ref>, at least for the quick recognition of objects, i.e., without the benefit of attention and top-down feedback connections. To this day, pattern recognition systems based on convolutional neural networks are among the best performing systems. This has been shown clearly for handwritten character recognition <ref type="bibr" target="#b100">[101]</ref>, which has served as a machine learning benchmark for many years. <ref type="foot" target="#foot_7">5</ref>Concerning our discussion of training deep architectures, the example of convolutional neural networks <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b174">175]</ref> is interesting because they typically have five, six or seven layers, a number of layers which makes fully connected neural networks almost impossible to train properly when initialized randomly. What is particular in their architecture that might explain their good generalization performance in vision tasks?</p><p>LeCun's convolutional neural networks are organized in layers of two types: convolutional layers and subsampling layers. Each layer has a topographic structure, i.e., each neuron is associated with a fixed twodimensional position that corresponds to a location in the input image, along with a receptive field (the region of the input image that influences the response of the neuron). At each location of each layer, there are a number of different neurons, each with its set of input weights, associated with neurons in a rectangular patch in the previous layer. The same set of weights, but a different input rectangular patch, are associated with neurons at different locations.</p><p>One untested hypothesis is that the small fan-in of these neurons (few inputs per neuron) helps gradients to propagate through so many layers without diffusing so much as to become useless. Note that this alone would not suffice to explain the success of convolutional networks, since random sparse connectivity is not enough to yield good results in deep neural networks. However, an effect of the fan-in would be consistent with the idea that gradients propagated through many paths gradually become too diffuse, i.e., the credit or blame for the output error is distributed too widely and thinly. Another hypothesis (which does not necessarily exclude the first) is that the hierarchical local connectivity structure is a very strong prior that is particularly appropriate for vision tasks, and sets the parameters of the whole network in a favorable region (with all non-connections corresponding to zero weight) from which gradient-based optimization works well. The fact is that even with random weights in the first layers, a convolutional neural network performs well <ref type="bibr" target="#b150">[151]</ref>, i.e., better than a trained fully connected neural network but worse than a fully optimized convolutional neural network.</p><p>Very recently, the convolutional structure has been imported into RBMs <ref type="bibr" target="#b44">[45]</ref> and DBNs <ref type="bibr" target="#b110">[111]</ref>. An important innovation in <ref type="bibr" target="#b110">[111]</ref> is the design of a generative version of the pooling / subsampling units, which worked beautifully in the experiments reported, yielding state-of-theart results not only on MNIST digits but also on the Caltech-101 object classification benchmark. In addition, visualizing the features obtained at each level (the patterns most liked by hidden units) clearly confirms the notion of multiple levels of composition which motivated deep architectures in the first place, moving up from edges to object parts to objects in a natural way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Auto-Encoders</head><p>Some of the deep architectures discussed below (Deep Belief Nets and Stacked Auto-Encoders) exploit as component or monitoring device a particular type of neural network: the auto-encoder, also called autoassociator, or Diabolo network <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b155">156,</ref><ref type="bibr" target="#b171">172]</ref>. There are also connections between the auto-encoder and RBMs discussed in Section 5.4.3, showing that auto-encoder training approximates RBM training by Contrastive Divergence. Because training an auto-encoder seems easier than training an RBM, they have been used as building blocks to train deep networks, where each level is associated with an auto-encoder that can be trained separately <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b194">195</ref>].</p><p>An auto-encoder is trained to encode the input x into some representation c(x) so that the input can be reconstructed from that representation. Hence the target output of the auto-encoder is the auto-encoder input itself. If there is one linear hidden layer and the mean squared error criterion is used to train the network, then the k hidden units learn to project the input in the span of the first k principal components of the data <ref type="bibr" target="#b24">[25]</ref>. If the hidden layer is non-linear, the auto-encoder behaves differently from PCA, with the ability to capture multi-modal aspects of the input distribution <ref type="bibr" target="#b89">[90]</ref>. The formulation that we prefer generalizes the mean squared error criterion to the minimization of the negative log-likelihood of the reconstruction, given the encoding c(x):</p><formula xml:id="formula_10">RE = -log P (x|c(x)). (4.7)</formula><p>If x|c(x) is Gaussian, we recover the familiar squared error. If the inputs x i are either binary or considered to be binomial probabilities, then the loss function would be</p><formula xml:id="formula_11">-log P (x|c(x)) = - i x i log f i (c(x)) + (1 -x i ) log(1 -f i (c(x))) (4.8)</formula><p>where f (•) is called the decoder, and f (c(x)) is the reconstruction produced by the network, and in this case should be a vector of numbers in (0, 1), e.g., obtained with a sigmoid. The hope is that the code c(x) is a distributed representation that captures the main factors of variation in the data: because c(x) is viewed as a lossy compression of x, it cannot be a good compression (with small loss) for all x, so learning drives it to be one that is a good compression in particular for training examples, and hopefully for others as well (and that is the sense in which an auto-encoder generalizes), but not for arbitrary inputs.</p><p>One serious issue with this approach is that if there is no other constraint, then an auto-encoder with n-dimensional input and an encoding of dimension at least n could potentially just learn the identity function, for which many encodings would be useless (e.g., just copying the input). Surprisingly, experiments reported in <ref type="bibr" target="#b16">[17]</ref> suggest that in practice, when trained with stochastic gradient descent, non-linear auto-encoders with more hidden units than inputs (called overcomplete) yield useful representations (in the sense of classification error measured on a network taking this representation in input). A simple explanation is based on the observation that stochastic gradient descent with early stopping is similar to an 2 regularization of the parameters <ref type="bibr" target="#b210">[211,</ref><ref type="bibr" target="#b35">36]</ref>. To achieve perfect reconstruction of continuous inputs, a one-hidden layer auto-encoder with non-linear hidden units needs very small weights in the first layer (to bring the non-linearity of the hidden units in their linear regime) and very large weights in the second layer. With binary inputs, very large weights are also needed to completely minimize the reconstruction error. Since the implicit or explicit regularization makes it difficult to reach large-weight solutions, the optimization algorithm finds encodings which only work well for examples similar to those in the training set, which is what we want. It means that the representation is exploiting statistical regularities present in the training set, rather than learning to replicate the identity function.</p><p>There are different ways that an auto-encoder with more hidden units than inputs could be prevented from learning the identity, and still capture something useful about the input in its hidden representation. Instead or in addition to constraining the encoder by explicit or implicit regularization of the weights, one strategy is to add noise in the encoding. This is essentially what RBMs do, as we will see later. Another strategy, which was found very successful <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b138">139,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b152">153]</ref>, is based on a sparsity constraint on the code. Interestingly, these approaches give rise to weight vectors that match well qualitatively the observed receptive fields of neurons in V1 and V2 <ref type="bibr" target="#b109">[110]</ref>, major areas of the mammal visual system. The question of sparsity is discussed further in Section 7.1.</p><p>Whereas sparsity and regularization reduce representational capacity in order to avoid learning the identity, RBMs can have a very large capacity and still not learn the identity, because they are not (only) trying to encode the input but also to capture the statistical structure in the input, by approximately maximizing the likelihood of a generative model. There is a variant of auto-encoder which shares that property with RBMs, called denoising auto-encoder <ref type="bibr" target="#b194">[195]</ref>. The denoising auto-encoder minimizes the error in reconstructing the input from a stochastically corrupted transformation of the input. It can be shown that it maximizes a lower bound on the log-likelihood of a generative model. See Section 7.2 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Energy-Based Models and Boltzmann Machines</head><p>Because Deep Belief Networks (DBNs) are based on Restricted Boltzmann Machines (RBMs), which are particular energy-based models, we introduce here the main mathematical concepts helpful to understand them, including Contrastive Divergence (CD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Energy-Based Models and Products of Experts</head><p>Energy-based models associate a scalar energy to each configuration of the variables of interest <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b148">149]</ref>. Learning corresponds to modifying that energy function so that its shape has desirable properties. For example, we would like plausible or desirable configurations to have low energy. Energy-based probabilistic models may define a probability distribution through an energy function, as follows:</p><formula xml:id="formula_12">P (x) = e -Energy(x) Z , (<label>5.1)</label></formula><p>i.e., energies operate in the log-probability domain. Th above generalizes exponential family models <ref type="bibr" target="#b28">[29]</ref>, for which the energy function Energy(x) has the form η(θ) • φ(x). We will see below that the conditional distribution of one layer given another, in the RBM, can be taken from any of the exponential family distributions <ref type="bibr" target="#b199">[200]</ref>. Whereas any probability distribution can be cast as an energy-based models, many more specialized distribution families, such as the exponential family, can benefit from particular inference and learning procedures. Some instead have explored rather general-purpose approaches to learning in energy-based models <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b148">149]</ref>.</p><p>The normalizing factor Z is called the partition function by analogy with physical systems,</p><formula xml:id="formula_13">Z = x e -Energy(x) (5.2)</formula><p>with a sum running over the input space, or an appropriate integral when x is continuous. Some energy-based models can be defined even when the sum or integral for Z does not exist (see Section 5.1.2).</p><p>In the product of experts formulation <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70]</ref>, the energy function is a sum of terms, each one associated with an "expert" f i :</p><formula xml:id="formula_14">Energy(x) = i f i (x),<label>(5.3)</label></formula><p>i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P (x)</head><formula xml:id="formula_15">∝ i P i (x) ∝ i e -f i (x) .</formula><p>(5.4)</p><p>Each expert P i (x) can thus be seen as a detector of implausible configurations of x, or equivalently, as enforcing constraints on x. This is clearer if we consider the special case where f i (x) can only take two values, one (small) corresponding to the case where the constraint is satisfied, and one (large) corresponding to the case where it is not.</p><p>[69] explains the advantages of a product of experts by opposition to a mixture of experts where the product of probabilities is replaced by a weighted sum of probabilities. To simplify, assume that each expert corresponds to a constraint that can either be satisfied or not. In a mixture model, the constraint associated with an expert is an indication of belonging to a region which excludes the other regions. One advantage of the product of experts formulation is therefore that the set of f i (x) forms a distributed representation: instead of trying to partition the space with one region per expert as in mixture models, they partition the space according to all the possible configurations (where each expert can have its constraint violated or not). <ref type="bibr" target="#b68">[69]</ref> proposed an algorithm for estimating the gradient of log P (x) in Equation (5.4) with respect to parameters associated with each expert, using the first instantiation <ref type="bibr" target="#b69">[70]</ref> of the Contrastive Divergence algorithm (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Introducing Hidden Variables</head><p>In many cases of interest, x has many component variables x i , and we do not observe of these components simultaneously, or we want to introduce some non-observed variables to increase the expressive power of the model. So we consider an observed part (still denoted x here) and a hidden part h</p><formula xml:id="formula_16">P (x, h) = e -Energy(x,h) Z (5.5)</formula><p>and because only x is observed, we care about the marginal</p><formula xml:id="formula_17">P (x) = h e -Energy(x,h) Z . (<label>5.6)</label></formula><p>In such cases, to map this formulation to one similar to Equation (5.1), we introduce the notation (inspired from physics) of free energy, defined as follows:</p><formula xml:id="formula_18">P (x) = e -FreeEnergy(x) Z , (<label>5.7)</label></formula><p>with Z = x e -FreeEnergy(x) , i.e., FreeEnergy(x) =log h e -Energy(x,h) .</p><p>(5.8)</p><p>So the free energy is just a marginalization of energies in the logdomain. The data log-likelihood gradient then has a particularly interesting form. Let us introduce θ to represent parameters of the model. Starting from Equation (5.7), we obtain</p><formula xml:id="formula_19">∂ log P (x) ∂θ = - ∂FreeEnergy(x) ∂θ + 1 Z x e -FreeEnergy(x) ∂FreeEnergy(x) ∂θ = - ∂FreeEnergy(x) ∂θ + x P (x)</formula><p>∂FreeEnergy(x) ∂θ .</p><p>(5.9)</p><p>Hence the average log-likelihood gradient over the training set is</p><formula xml:id="formula_20">E P ∂ log P (x) ∂θ = -E P ∂FreeEnergy(x) ∂θ + E P ∂FreeEnergy(x) ∂θ</formula><p>(5.10) where expectations are over x, with P the training set empirical distribution and E P the expectation under the model's distribution P . Therefore, if we could sample from P and compute the free energy tractably, we would have a Monte-Carlo method to obtain a stochastic estimator of the log-likelihood gradient.</p><p>If the energy can be written as a sum of terms associated with at most one hidden unit</p><formula xml:id="formula_21">Energy(x, h) = -β(x) + i γ i (x, h i ),<label>(5.11)</label></formula><p>a condition satisfied in the case of the RBM, then the free energy and numerator of the likelihood can be computed tractably (even though it involves a sum with an exponential number of terms):</p><formula xml:id="formula_22">P (x) = 1 Z e -FreeEnergy(x) = 1 Z h e -Energy(x,h) = 1 Z h 1 h 2 • • • h k e β(x)-i γ i (x,h i ) = 1 Z h 1 h 2 • • • h k e β(x) i e -γ i (x,h i ) = e β(x) Z h 1 e -γ 1 (x,h 1 ) h 2 e -γ 2 (x,h 2 ) • • • h k e -γ k (x,h k ) = e β(x) Z i h i e -γ i (x,h i ) (5.12)</formula><p>In the above, h i is a sum over all the values that h i can take (e.g., two values in the usual binomial units case); note how that sum is much easier to carry out than the sum h over all values of h. Note that all sums can be replaced by integrals if h is continuous, and the same principles apply. In many cases of interest, the sum or integral (over a single hidden unit's values) is easy to compute. The numerator of the likelihood (i.e., also the free energy) can be computed exactly in the above case, where Energy(x, h) = -β(x) + i γ i (x, h i ), and we have</p><formula xml:id="formula_23">FreeEnergy(x) = -log P (x) -log Z = -β(x) - i log h i e -γ i (x,h i ) .</formula><p>(5.13)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Conditional Energy-Based Models</head><p>Whereas computing the partition function is difficult in general, if our ultimate goal is to make a decision concerning a variable y given a variable x, instead of considering all configurations (x, y), it is enough to consider the configurations of y for each given x. A common case is one where y can only take values in a small discrete set, i.e., P (y|x) = e -Energy(x,y) y e -Energy(x,y) .</p><p>(</p><p>In this case the gradient of the conditional log-likelihood with respect to parameters of the energy function can be computed efficiently. This formulation applies to a discriminant variant of the RBM called Discriminative RBM <ref type="bibr" target="#b96">[97]</ref>. Such conditional energy-based models have also been exploited in a series of probabilistic language models based on neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b168">169,</ref><ref type="bibr" target="#b169">170,</ref><ref type="bibr" target="#b170">171,</ref><ref type="bibr" target="#b206">207]</ref>. That formulation (or generally when it is easy to sum or maximize over the set of values of the terms of the partition function) has been explored at length <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b152">153]</ref>. An important and interesting element in the latter work is that it shows that such energy-based models can be optimized not just with respect to log-likelihood but with respect to more general criteria whose gradient has the property of making the energy of "correct" responses decrease while making the energy of competing responses increase. These energy functions do not necessarily give rise to a probabilistic model (because the exponential of the negated energy function is not required to be integrable), but they may nonetheless give rise to a function that can be used to choose y given x, which is often the ultimate goal in applications. Indeed when y takes a finite number of values, P (y|x) can always be computed since the energy function needs to be normalized only over the possible values of y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Boltzmann Machines</head><p>The Boltzmann machine is a particular type of energy-based model with hidden variables, and RBMs are special forms of Boltzmann machines in which P (h|x) and P (x|h) are both tractable because they factorize. In a Boltzmann machine <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref>, the energy function is a general second-order polynomial:</p><formula xml:id="formula_25">Energy(x, h) = -b x -c h -h W x -x U x -h V h. (5.15)</formula><p>There are two types of parameters, which we collectively denote by θ: the offsets b i and c i (each associated with a single element of the vector x or of the vector h), and the weights W ij , U ij and V ij (each associated with a pair of units). Matrices U and V are assumed to be symmetric, <ref type="foot" target="#foot_8">1</ref>and in most models with zeros in the diagonal. Non-zeros in the diagonal can be used to obtain other variants, e.g., with Gaussian instead of binomial units <ref type="bibr" target="#b199">[200]</ref>.</p><p>Because of the quadratic interaction terms in h, the trick to analytically compute the free energy (Equation (5.12)) cannot be applied here. However, an MCMC (Monte Carlo Markov Chain <ref type="bibr" target="#b3">[4]</ref>) sampling procedure can be applied in order to obtain a stochastic estimator of the gradient. The gradient of the log-likelihood can be written as follows, starting from Equation (5.6):</p><formula xml:id="formula_26">∂ log P (x) ∂θ = ∂ log h e -Energy(x,h) ∂θ - ∂ log x,h e -Energy(x,h) ∂θ = - 1 h e -Energy(x,h) h e -Energy(x,h) ∂Energy(x, h) ∂θ + 1</formula><p>x,h e -Energy(x,h)</p><p>x,h e -Energy(x,h) ∂Energy(x, h) ∂θ</p><formula xml:id="formula_27">= - h P (h|x) ∂Energy(x, h) ∂θ + x,h P (x, h) ∂Energy(x, h) ∂θ .</formula><p>(5.16)</p><p>Note that (∂Energy(x, h)/∂θ) is easy to compute. Hence if we have a procedure to sample from P (h|x) and one to sample from P (x, h), we can obtain an unbiased stochastic estimator of the log-likelihood gradient. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref> introduced the following terminology: in the positive phase, x is clamped to the observed input vector, and we sample h given x; in the negative phase both x and h are sampled, ideally from the model itself. In general, only approximate sampling can be achieved tractably, e.g., using an iterative procedure that constructs an MCMC. The MCMC sampling approach introduced in [1, <ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref> is based on Gibbs sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57]</ref>. Gibbs sampling of the joint of N random variables S = (S 1 , . . . , S N ) is done through a sequence of N sampling sub-steps of the form</p><formula xml:id="formula_28">S i ∼ P (S i |S -i = s -i ) (5.17)</formula><p>where S -i contains the N -1 other random variables in S, excluding S i . After these N samples have been obtained, a step of the chain is completed, yielding a sample of S whose distribution converges to P (S) as the number of steps goes to ∞, under some conditions. A sufficient condition for convergence of a finite-state Markov Chain is that it is aperiodic<ref type="foot" target="#foot_10">2</ref> and irreducible. <ref type="foot" target="#foot_11">3</ref>How can we perform Gibbs sampling in a Boltzmann machine? Let s = (x, h) denote all the units in the Boltzmann machine, and s -i the set of values associated with all units except the ith one. The Boltzmann machine energy function can be rewritten by putting all the parameters in a vector d and a symmetric matrix A, Energy(s) = -d ss As.</p><p>(5.18)</p><p>Let d -i denote the vector d without the element d i , A -i the matrix A without the ith row and column, and a -i the vector that is the ith row (or column) of A, without the ith element. Using this notation, we obtain that P (s i |s -i ) can be computed and sampled from easily in a Boltzmann machine. For example, if s i ∈ {0, 1} and the diagonal of A is null:</p><formula xml:id="formula_29">P (s i = 1|s -i ) = exp(d i + d -i s -i + 2a -i s -i + s -i A -i s -i ) exp(d i + d -i s -i + 2a -i s -i + s -i A -i s -i ) + exp(d -i s -i + s -i A -i s -i ) = exp(d i + 2a -i s -i ) exp(d i + 2a -i s -i ) + 1 = 1 1 + exp(-d i -2a -i s -i ) = sigm(d i + 2a -i s -i ) (5.19)</formula><p>which is essentially the usual equation for computing a neuron's output in terms of other neurons s -i , in artificial neural networks. Since two MCMC chains (one for the positive phase and one for the negative phase) are needed for each example x, the computation of the gradient can be very expensive, and training time very long. This is essentially why the Boltzmann machine was replaced in the late 1980's by the back-propagation algorithm for multi-layer neural networks as the dominant learning approach. However, recent work has shown that short chains can sometimes be used successfully, and this is the principle of Contrastive Divergence, discussed in Section 5.4 to train RBMs. Note also that the negative phase chain does not have to be restarted for each new example x (since it does not depend on the training data), and this observation has been exploited in persistent MCMC estimators <ref type="bibr" target="#b160">[161,</ref><ref type="bibr" target="#b186">187]</ref> discussed in Section 5.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Restricted Boltzmann Machines</head><p>The Restricted Boltzmann Machine (RBM) is the building block of a Deep Belief Network (DBN) because it shares parametrization with individual layers of a DBN, and because efficient learning algorithms were found to train it. The undirected graphical model of an RBM is illustrated in Figure <ref type="figure">4</ref>.5, showing that the h i are independent of each other when conditioning on x and the x j are independent of each other when conditioning on h. In an RBM, U = 0 and V = 0 in Equation (5.15), i.e., the only interaction terms are between a hidden unit and a visible unit, but not between units of the same layer. This form of model was first introduced under the name of Harmonium <ref type="bibr" target="#b177">[178]</ref>, and learning algorithms (beyond the ones for Boltzmann Machines) were discussed in <ref type="bibr" target="#b50">[51]</ref>. Empirically demonstrated and efficient learning algorithms and variants were proposed more recently <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b199">200]</ref>. As a consequence of the lack of input-input and hidden-hidden interactions, the energy function is bilinear,</p><formula xml:id="formula_30">Energy(x, h) = -b x -c h -h W x (5.20)</formula><p>and the factorization of the free energy of the input, introduced with Equations (5.11) and (5.13) can be applied with β(x) = b x and</p><formula xml:id="formula_31">γ i (x, h i ) = -h i (c i + W i x)</formula><p>, where W i is the row vector corresponding to the ith row of W . Therefore the free energy of the input (i.e., its unnormalized log-probability) can be computed efficiently:</p><formula xml:id="formula_32">FreeEnergy(x) = -b x - i log h i e h i (c i +W i x) .</formula><p>(5.21)</p><p>Using the same factorization trick (in Equation (5.12)) due to the affine form of Energy(x, h) with respect to h, we readily obtain a tractable expression for the conditional probability P (h|x):</p><formula xml:id="formula_33">P (h|x) = exp(b x + c h + h W x) h exp(b x + c h + h W x) = i exp(c i h i + h i W i x) i hi exp(c i hi + hi W i x) = i exp(h i (c i + W i x)) hi exp( hi (c i + W i x)) = i P (h i |x).</formula><p>In the commonly studied case where h i ∈ {0, 1}, we obtain the usual neuron equation for a neuron's output given its input:</p><formula xml:id="formula_34">P (h i = 1|x) = e c i +W i x 1 + e c i +W i x = sigm(c i + W i x).</formula><p>(5.22)</p><p>Since x and h play a symmetric role in the energy function, a similar derivation allows to efficiently compute and sample P (x|h):</p><formula xml:id="formula_35">P (x|h) = i P (x i |h) (5.23)</formula><p>and in the binary case</p><formula xml:id="formula_36">P (x j = 1|h) = sigm(b j + W •j h) (5.24)</formula><p>where W •j is the jth column of W .</p><p>In <ref type="bibr" target="#b72">[73]</ref>, binomial input units are used to encode pixel gray levels in input images as if they were the probability of a binary event. In the case of handwritten character images this approximation works well, but in other cases it does not. Experiments showing the advantage of using Gaussian input units rather than binomial units when the inputs are continuous-valued are described in <ref type="bibr" target="#b16">[17]</ref>. See <ref type="bibr" target="#b199">[200]</ref> for a general formulation where x and h (given the other) can be in any of the exponential family distributions (discrete and continuous).</p><p>Although RBMs might not be able to represent efficiently some distributions that could be represented compactly with an unrestricted Boltzmann machine, RBMs can represent any discrete distribution <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b101">102]</ref>, if enough hidden units are used. In addition, it can be shown that unless the RBM already perfectly models the training distribution, adding a hidden unit (and properly choosing its weights and offset) can always improve the log-likelihood <ref type="bibr" target="#b101">[102]</ref>.</p><p>An RBM can also be seen as forming a multi-clustering (see Section 3.2), as illustrated in Figure <ref type="figure">3</ref>.2. Each hidden unit creates a tworegion partition of the input space (with a linear separation). When we consider the configurations of say three hidden units, there are eight corresponding possible intersections of three half-planes (by choosing each half-plane among the two half-planes associated with the linear separation performed by a hidden unit). Each of these eight intersections corresponds to a region in input space associated with the same hidden configuration (i.e., code). The binary setting of the hidden units thus identifies one region in input space. For all x in one of these regions, P (h|x) is maximal for the corresponding h configuration. Note that not all configurations of the hidden units correspond to a non-empty region in input space. As illustrated in Figure <ref type="figure">3</ref>.2, this representation is similar to what an ensemble of two-leaf trees would create.</p><p>The sum over the exponential number of possible hidden-layer configurations of an RBM can also be seen as a particularly interesting form of mixture, with an exponential number of components (with respect to the number of hidden units and of parameters):</p><formula xml:id="formula_37">P (x) = h P (x|h)P (h) (5.25)</formula><p>where P (x|h) is the model associated with the component indexed by configuration h. For example, if P (x|h) is chosen to be Gaussian (see <ref type="bibr" target="#b199">[200,</ref><ref type="bibr" target="#b16">17]</ref>), this is a Gaussian mixture with 2 n components when h has n bits. Of course, these 2 n components cannot be tuned independently because they depend on shared parameters (the RBM parameters), and that is also the strength of the model, since it can generalize to configurations (regions of input space) for which no training example was seen. We can see that the Gaussian mean (in the Gaussian case) associated with component h is obtained as a linear combination b + W h, i.e., each hidden unit bit h i contributes (or not) a vector W i in the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Gibbs Sampling in RBMs</head><p>Sampling from an RBM is useful for several reasons. First of all it is useful in learning algorithms, to obtain an estimator of the log-likelihood gradient. Second, inspection of examples generated from the model is useful to get an idea of what the model has captured or not captured about the data distribution. Since the joint distribution of the top two layers of a DBN is an RBM, sampling from an RBM enables us to sample from a DBN, as elaborated in Section 6.1. Gibbs sampling in fully connected Boltzmann Machines is slow because there are as many sub-steps in the Gibbs chain as there are units in the network. On the other hand, the factorization enjoyed by RBMs brings two benefits: first we do not need to sample in the positive phase because the free energy (and therefore its gradient) is computed analytically; second, the set of variables in (x, h) can be sampled in two sub-steps in each step of the Gibbs chain. First we sample h given x, and then a new x given h. In general product of experts models, an alternative to Gibbs sampling is hybrid Monte-Carlo <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b135">136]</ref>, an MCMC method involving a number of free-energy gradient computation sub-steps for each step of the Markov chain. The RBM structure is therefore a special case of product of experts model: the ith term log h i e (c i +W i x)h i in Equation (5.21) corresponds to an expert, i.e., there is one expert per hidden neuron and one for the input offset. With that special structure, a very efficient Gibbs sampling can be performed. For k Gibbs steps, starting from a training example (i.e., sampling from P ):</p><formula xml:id="formula_38">x 1 ∼ P (x) h 1 ∼ P (h|x 1 ) x 2 ∼ P (x|h 1 ) h 2 ∼ P (h|x 2 )</formula><p>. . .</p><formula xml:id="formula_39">x k+1 ∼ P (x|h k ).</formula><p>(5.26)</p><p>It makes sense to start the chain from a training example because as the model becomes better at capturing the structure in the training data, the model distribution P and the training distribution P become more similar (having similar statistics). Note that if we started the chain from P itself, it would have converged in one step, so starting from P is a good way to ensure that only a few steps are necessary for convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Contrastive Divergence</head><p>Contrastive Divergence is an approximation of the log-likelihood gradient that has been found to be a successful update rule for training RBMs <ref type="bibr" target="#b30">[31]</ref>. A pseudo-code is shown in Algorithm 1, with the particular equations for the conditional distributions for the case of binary input and hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Justifying Contrastive Divergence</head><p>To obtain this algorithm, the first approximation we are going to make is replace the average over all possible inputs (in the second term of Equation (5.10)) by a single sample. Since we update the parameters often (e.g., with stochastic or mini-batch gradient updates after one or a few training examples), there is already some averaging going on across Algorithm 1 RBMupdate(x 1 , , W, b, c) This is the RBM update procedure for binomial units. It can easily adapted to other types of units.</p><p>x 1 is a sample from the training distribution for the RBM is a learning rate for the stochastic gradient descent in Contrastive Divergence W is the RBM weight matrix, of dimension (number of hidden units, number of inputs) b is the RBM offset vector for input units c is the RBM offset vector for hidden units Notation:</p><formula xml:id="formula_40">Q(h 2• = 1|x 2 ) is the vector with elements Q(h 2i = 1|x 2 )</formula><p>for all hidden units i do</p><formula xml:id="formula_41">• compute Q(h 1i = 1|x 1 ) (for binomial units, sigm(c i + j W ij x 1j )) • sample h 1i ∈ {0, 1} from Q(h 1i |x 1 )</formula><p>end for for all visible units j do • compute P (x 2j = 1|h 1 ) (for binomial units, sigm(b j + i W ij h 1i ))</p><p>• sample x 2j ∈ {0, 1} from P (x 2j = 1|h 1 ) end for for all hidden units i do</p><formula xml:id="formula_42">• compute Q(h 2i = 1|x 2 ) (for binomial units, sigm(c i + j W ij x 2j )) end for • W ← W + (h 1 x 1 -Q(h 2• = 1|x 2 )x 2 ) • b ← b + (x 1 -x 2 ) • c ← c + (h 1 -Q(h 2• = 1|x 2 ))</formula><p>updates (which we know to work well <ref type="bibr" target="#b104">[105]</ref>), and the extra variance introduced by taking one or a few MCMC samples instead of doing the complete sum might be partially canceled in the process of online gradient updates, over consecutive parameter updates. We introduce additional variance with this approximation of the gradient, but it does not hurt much if it is comparable or smaller than the variance due to online gradient descent.</p><p>Running a long MCMC chain is still very expensive. The idea of k-step Contrastive Divergence (CD-k) <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70]</ref> is simple, and involves a second approximation, which introduces some bias in the gradient: run the MCMC chain x 1 , x 2 , . . . ,x k+1 for only k steps starting from the observed example x 1 = x. The CD-k update (i.e., not the log-likelihood gradient) after seeing example x is, therefore, ∆θ ∝ ∂FreeEnergy(x) ∂θ -∂FreeEnergy(x) ∂θ <ref type="bibr">(5.27)</ref> where x = x k+1 is the last sample from our Markov chain, obtained after k steps. We know that when k → ∞, the bias goes away. We also know that when the model distribution is very close to the empirical distribution, i.e., P ≈ P , then when we start the chain from x (a sample from P ) the MCMC has already converged, and we need only one step to obtain an unbiased sample from P (although it would still be correlated with x).</p><p>The surprising empirical result is that even k = 1 (CD-1) often gives good results. An extensive numerical comparison of training with CD-k versus exact log-likelihood gradient has been presented in <ref type="bibr" target="#b30">[31]</ref>. In these experiments, taking k larger than 1 gives more precise results, although very good approximations of the solution can be obtained even with k = 1. Theoretical results <ref type="bibr" target="#b11">[12]</ref> discussed in Section 5.4.3 help to understand why small values of k can work: CD-k corresponds to keeping the first k terms of a series that converges to the log-likelihood gradient.</p><p>One way to interpret Contrastive Divergence is that it is approximating the log-likelihood gradient locally around the training point x 1 . The stochastic reconstruction x = x k+1 (for CD-k) has a distribution (given x 1 ) which is in some sense centered around x 1 and becomes more spread out around it as k increases, until it becomes the model distribution. The CD-k update will decrease the free energy of the training point x 1 (which would increase its likelihood if all the other free energies were kept constant), and increase the free energy of x, which is in the neighborhood of x 1 . Note that x is in the neighborhood of x 1 , but at the same time more likely to be in regions of high probability under the model (especially for k larger). As argued by <ref type="bibr" target="#b105">[106]</ref>, what is mostly needed from the training algorithm for an energy-based model is that it makes the energy (free energy, here, to marginalize hidden variables) of observed inputs smaller, shoveling "energy" elsewhere, and most importantly in areas of low energy. The Contrastive Divergence algorithm is fueled by the contrast between the statistics collected when the input is a real training example and when the input is a chain sample. As further argued in the next section, one can think of the unsupervised learning problem as discovering a decision surface that can roughly separate the regions of high probability (where there are many observed training examples) from the rest. Therefore, we want to penalize the model when it generates examples on the wrong side of that divide, and a good way to identify where that divide should be moved is to compare training examples with samples from the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Alternatives to Contrastive Divergence</head><p>An exciting recent development in the research on learning algorithms for RBMs is use of a so-called persistent MCMC for the negative phase <ref type="bibr" target="#b160">[161,</ref><ref type="bibr" target="#b186">187]</ref>, following an approach already introduced in <ref type="bibr" target="#b134">[135]</ref>. The idea is simple: keep a background MCMC chain . . . x t → h t → x t+1 → h t+1 . . . to obtain the negative phase samples (which should be from the model). Instead of running a short chain as in CD-k, the approximation made is that we ignore the fact that parameters are changing as we move along the chain, i.e., we do not run a separate chain for each value of the parameters (as in the traditional Boltzmann Machine learning algorithm). Maybe because the parameters move slowly, the approximation works very well, usually giving rise to better log-likelihood than CD-k (experiments were against k = 1 and k = 10). The trade-off with CD-1 is that the variance is larger but the bias is smaller. Something interesting also happens <ref type="bibr" target="#b187">[188]</ref>: the model systematically moves away from the samples obtained in the negative phase, and this interacts with the chain itself, preventing it from staying in the same region very long, substantially improving the mixing rate of the chain. This is a very desirable and unforeseen effect, which helps to explore more quickly the space of RBM configurations.</p><p>Another alternative to Contrastive Divergence is Score Matching <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86]</ref>, a general approach to train energy-based models in which the energy can be computed tractably, but not the normalization constant Z. The score function of a density p(x) = q(x)/Z is ψ = (∂ log p(x))/∂x, and we exploit the fact that the score function of our model does not depend on its normalization constant, i.e., ψ = (∂ log q(x))/∂x. The basic idea is to match the score function of the model with the score function of the empirical density. The average (under the empirical density) of the squared norm of the difference between the two score functions can be written in terms of squares of the model score function and second derivatives (∂ 2 log q(x))/∂x 2 . Score matching has been shown to be locally consistent <ref type="bibr" target="#b83">[84]</ref>, i.e., converging if the model family matches the data generating process, and it has been used for unsupervised models of image and audio data <ref type="bibr" target="#b93">[94]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Truncations of the Log-Likelihood Gradient in Gibbs-Chain Models</head><p>Here, we approach the Contrastive Divergence update rule from a different perspective, which gives rise to possible generalizations of it and links it to the reconstruction error often used to monitor its performance and that is used to optimize auto-encoders (Equation (4.7)).</p><p>The inspiration for this derivation comes from <ref type="bibr" target="#b72">[73]</ref>: first from the idea (explained in Section 8.1) that the Gibbs chain can be associated with an infinite directed graphical model (which here we associate with an expansion of the log-likelihood gradient), and second that the convergence of the chain justifies Contrastive Divergence (since the expected value of Equation (5.27) becomes equivalent to Equation (5.9) when the chain sample x comes from the model). In particular, we are interested in clarifying and understanding the bias in the Contrastive Divergence update rule, compared to using the true (intractable) gradient of the log-likelihood. Consider a converging Markov chain x t ⇒ h t ⇒ x t+1 ⇒ • • • defined by conditional distributions P (h t |x t ) and P (x t+1 |h t ), with x 1 sampled from the training data empirical distribution. The following theorem, demonstrated by <ref type="bibr" target="#b11">[12]</ref>, shows how one can expand the log-likelihood gradient for any t ≥ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5.1. Consider the converging Gibbs chain x</head><formula xml:id="formula_43">1 ⇒ h 1 ⇒ x 2 ⇒ h 2 • • • starting at data point x 1 .</formula><p>The log-likelihood gradient can be written</p><formula xml:id="formula_44">∂ log P (x 1 ) ∂θ = - ∂FreeEnergy(x 1 ) ∂θ + E ∂FreeEnergy(x t ) ∂θ + E ∂ log P (x t ) ∂θ (5.28)</formula><p>and the final term converges to zero as t goes to infinity.</p><p>Since the final term becomes small as t increases, that justifies truncating the chain to k steps in the Markov chain, using the approximation</p><formula xml:id="formula_45">∂ log P (x 1 ) ∂θ - ∂FreeEnergy(x 1 ) ∂θ + E ∂FreeEnergy(x k+1 ) ∂θ</formula><p>which is exactly the CD-k update (Equation (5.27)) when we replace the expectation with a single sample x = x k+1 . This tells us that the bias of CD-k is E[(∂ log P (x k+1 ))/∂θ]. Experiments and theory support the idea that CD-k yields better and faster convergence (in terms of number of iterations) than CD-(k -1), due to smaller bias (though the computational overhead might not always be worth it). However, although experiments show that the CD-k bias can indeed be large when k is small, empirically the update rule of CD-k still mostly moves the model's parameters in the same quadrant as log-likelihood gradient <ref type="bibr" target="#b11">[12]</ref>. This is in agreement with the good results can be obtained even with k = 1. An intuitive picture that may help to understand the phenomenon is the following: when the input example x 1 is used to initialize the chain, even the first Markov chain step (to x 2 ) tends to be in the right direction compared to x 1 , i.e., roughly going down the energy landscape from x 1 . Since the gradient depends on the change between x 2 and x 1 , we tend to get the direction of the gradient right.</p><p>So CD-1 corresponds to truncating the chain after two samples (one from h 1 |x 1 , and one from x 2 |h 1 ). What about stopping after the first one (i.e., h 1 |x 1 )? It can be analyzed from the following log-likelihood gradient expansion <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_46">∂ log P (x 1 ) ∂θ = E ∂ log P (x 1 |h 1 ) ∂θ -E ∂ log P (h 1 ) ∂θ . (5.29)</formula><p>Let us consider a mean-field approximation of the first expectation, in which instead of the average over all h 1 configurations according to P (h 1 |x 1 ) one replaces h 1 by its average configuration</p><formula xml:id="formula_47">h 1 = E[h 1 |x 1 ],</formula><p>yielding:</p><formula xml:id="formula_48">E ∂ log P (x 1 |h 1 ) ∂θ ∂ log P (x 1 | h 1 ) ∂θ .</formula><p>(5.30)</p><p>If, as in CD, we then ignore the second expectation in Equation (5.29) (incurring an additional bias in the estimation of the log-likelihood gradient), we then obtain the right-hand side of Equation (5.30) as an update direction, which is minus the gradient of the reconstruction error,</p><formula xml:id="formula_49">-log P (x 1 | h 1 )</formula><p>typically used to train auto-encoders (see Equation (4.7) with c(x) = E[h|x]). <ref type="foot" target="#foot_12">4</ref>So we have found that the truncation of the Gibbs chain gives rise to first approximation (one sample) to roughly reconstruction error (through a biased mean-field approximation), with slightly better approximation (two samples) to CD-1 (approximating the expectation by a sample), and with more terms to CD-k (still approximating expectations by samples). Note that reconstruction error is deterministically computed and is correlated with log-likelihood, which is why it has been used to track progress when training RBMs with CD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Model Samples Are Negative Examples</head><p>Here, we argue that training an energy-based model can be achieved by solving a series of classification problems in which one tries to discriminate training examples from samples generated by the model. In the Boltzmann machine learning algorithms, as well as in Contrastive Divergence, an important element is the ability to sample from the model, maybe approximately. An elegant way to understand the value of these samples in improving the log-likelihood was introduced in <ref type="bibr" target="#b200">[201]</ref>, using a connection with boosting. We start by explaining the idea informally and then formalize it where there are more samples from the model). Mathematically, consider the gradient of the log-likelihood with respect to the parameters of the FreeEnergy(x) (or Energy(x) if we do not introduce explicit hidden variables), given in Equation (5.10). Now consider a highly regularized two-class probabilistic classifier that will attempt to separate training samples of P (x) from model samples of P (x), and which is only able to produce an output probability q(x) = P (y = 1|x) barely different from 1  2 (hopefully on the right side more often than not). Let q(x) = sigm(-a(x)), i.e., -a(x) is the discriminant function or an unnormalized conditional log-probability, just like the free energy. Let P denote the empirical distribution over (x, y) pairs, and Pi the distribution over x when y = i. Assume that P (y = 1) = P (y = 0) = 1/2, so that ∀f,</p><formula xml:id="formula_50">E P [f (x, y)] = E P1 [f (x, 1)] P (y = 1) + E P0 [f (x, 0)] P (y = 0) = 1 2 (E P1 [f (x, 1)] + E P0 [f (x, 0)]).</formula><p>Using this, the average conditional log-likelihood gradient for this probabilistic classifier is written</p><formula xml:id="formula_51">E P ∂ log P (y|x) ∂θ = E P ∂(y log q(x) + (1 -y) log(1 -q(x))) ∂θ = 1 2 E P1 (q(x) -1) ∂a(x) ∂θ + E P0 q(x) ∂a(x) ∂θ ≈ 1 4 -E P1 ∂a(x) ∂θ + E P0 ∂a(x) ∂θ (5.31)</formula><p>where the last equality is when the classifier is highly regularized: when the output weights are small, a(x) is close to 0 and q(x) ≈ 1/2, so that (1q(x)) ≈ q(x). This expression for the log-likelihood gradient corresponds exactly to the one obtained for energy-based models where the likelihood is expressed in terms of a free energy (Equation (5.10)), when we interpret training examples from P1 as positive examples (y = 1) (i.e., P1 = P ) and model samples as negative examples (y = 0, i.e., P0 = P ). The gradient is also similar in structure to the Contrastive Divergence gradient estimator (Equation (5.27)). One way to interpret this result is that if we could improve a classifier that separated training samples from model samples, we could improve the log-likelihood of the model, by putting more probability mass on the side of training samples. Practically, this could be achieved with a classifier whose discriminant function was defined as the free energy of a generative model (up to a multiplicative factor), and assuming one could obtain samples (possibly approximate) from the model. A particular variant of this idea has been used to justify a boosting-like incremental algorithm for adding experts in products of experts <ref type="bibr" target="#b200">[201]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy Layer-Wise Training of Deep Architectures 6.1 Layer-Wise Training of Deep Belief Networks</head><p>A Deep Belief Network <ref type="bibr" target="#b72">[73]</ref> with layers models the joint distribution between observed vector x and hidden layers h k as follows:</p><formula xml:id="formula_52">P (x, h 1 , . . . ,h ) = -2 k=0 P (h k |h k+1 ) P (h -1 , h ),<label>(6.1)</label></formula><p>where x = h 0 , P (h k-1 |h k ) is a visible-given-hidden conditional distribution in an RBM associated with level k of the DBN, and P (h -1 , h ) is the joint distribution in the top-level RBM. This is illustrated in Figure <ref type="figure">6</ref>.1.</p><p>The conditional distributions P (h k |h k+1 ) and the top-level joint (an RBM) P (h -1 , h ) define the generative model. In the following we introduce the letter Q for exact or approximate posteriors of that model, which are used for inference and training. The Q posteriors are all approximate except for the top level Q(h |h -1 ) which is equal to the true P (h |h -1 ) because (h , h -1 ) form an RBM, where exact inference is possible.</p><p>When we train the DBN in a greedy layer-wise fashion, as illustrated with the pseudo-code of Algorithm 2, each layer is initialized 2 ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>... P(h | h )</head><formula xml:id="formula_53">1 2 P(h , h ) ~ RBM 2 3 x h 1 3 ... h h 2 ... P(x | h ) 1 Q(h | x ) 1 Q(h | h ) 1</formula><p>Fig. <ref type="figure">6</ref>.1 Deep Belief Network as a generative model (generative path with P distributions, full arcs) and a means to extract multiple levels of representation of the input (recognition path with Q distributions, dashed arcs). The top two layers h 2 and h 3 form an RBM (for their joint distribution). The lower layers form a directed graphical model (sigmoid belief net h 2 ⇒ h 1 ⇒ x) and the prior for the penultimate layer h 2 is provided by the top-level RBM. Q(h k+1 |h k ) approximates P (h k+1 |h k ) but can be computed easily.</p><p>as an RBM, and we denote Q(h k , h k-1 ) the k-th RBM trained in this way, whereas P (. . .) denotes probabilities according to the DBN. We will use Q(h k |h k-1 ) as an approximation of P (h k |h k-1 ), because it is easy to compute and sample from Q(h k |h k-1 ) (which factorizes), and not from P (h k |h k-1 ) (which does not). These Q(h k |h k-1 ) can also be used to construct a representation of the input vector x. To obtain an approximate posterior or representation for all the levels, we use the following procedure. First sample h 1 ∼ Q(h 1 |x) from the first-level RBM, or alternatively with a mean-field approach use h 1 = E[h 1 |x] instead of a sample of h 1 , where the expectation is over the RBM distribution Q(h 1 |x). This is just the vector of output probabilities of the hidden units, in the common case where they are binomial units:</p><formula xml:id="formula_54">h 1 i = sigm(b 1 + W 1 i x).</formula><p>Taking either the mean-field vector h 1 or the sample h 1 as input for the second-level RBM, compute h 2 or a sample h 2 , etc. until the last layer. Once a DBN is trained as per Algorithm 2, the parameters W i (RBM weights) and c i (RBM hidden unit offsets) for each layer can be used to initialize a deep multi-layer neural network. These parameters can then be fine-tuned with respect to another criterion (typically a supervised learning criterion).</p><p>A sample of the DBN generative model for x can be obtained as follows:</p><p>1. Sample a visible vector h -1 from the top-level RBM. This can be achieved approximately by running a Gibbs chain in that RBM alternating between h ∼ P (h |h -1 ) and h -1 ∼ P (h -1 |h ), as outlined in Section 5.3.1. By starting the chain from a representation h -1 obtained from a training set example (through the Q s as above), fewer Gibbs steps might be required. 2. For k = -1 down to 1, sample h k-1 given h k according to the level-k hidden-to-visible conditional distribution P (h k-1 |h k ). 3. x = h 0 is the DBN sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training Stacked Auto-Encoders</head><p>Auto-Encoders have been used as building blocks to build and initialize a deep multi-layer neural network <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b194">195]</ref>. The training procedure is similar to the one for Deep Belief Networks:</p><p>1. Train the first layer as an auto-encoder to minimize some form of reconstruction error of the raw input. This is purely unsupervised. 2. The hidden units' outputs (i.e., the codes) of the autoencoder are now used as input for another layer, also trained to be an auto-encoder. Again, we only need unlabeled examples. 3. Iterate as in step (2) to initialize the desired number of additional layers. 4. Take the last hidden layer output as input to a supervised layer and initialize its parameters (either randomly or by supervised training, keeping the rest of the network fixed). 5. Fine-tune all the parameters of this deep architecture with respect to the supervised criterion. Alternately, unfold all the auto-encoders into a very deep auto-encoder and finetune the global reconstruction error, as in <ref type="bibr" target="#b74">[75]</ref>.</p><p>The hope is that the unsupervised pre-training in this greedy layerwise fashion has put the parameters of all the layers in a region of parameter space from which a good 1 local optimum can be reached by local descent. This indeed appears to happen in a number of tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b194">195]</ref>.</p><p>The principle is exactly the same as the one previously proposed for training DBNs, but using auto-encoders instead of RBMs. Comparative experimental results suggest that Deep Belief Networks typically have an edge over Stacked Auto-Encoders <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b194">195]</ref>. This may be because CD-k is closer to the log-likelihood gradient than the reconstruction error gradient. However, since the reconstruction error gradient has less variance than CD-k (because no sampling is involved), it might be interesting to combine the two criteria, at least in the initial phases of learning. Note also that the DBN advantage disappeared in experiments where the ordinary auto-encoder was replaced by a denoising autoencoder <ref type="bibr" target="#b194">[195]</ref>, which is stochastic (see Section 7.2).</p><p>An advantage of using auto-encoders instead of RBMs as the unsupervised building block of a deep architecture is that almost any parametrization of the layers is possible, as long as the training criterion is continuous in the parameters. On the other hand, the class of probabilistic models for which CD or other known tractable estimators of the log-likelihood gradient can be applied is currently more limited. A disadvantage of Stacked Auto-Encoders is that they do not correspond to a generative model: with generative models such as RBMs and DBNs, samples can be drawn to check qualitatively what has been learned, e.g., by visualizing the images or word sequences that the model sees as plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Semi-Supervised and Partially Supervised Training</head><p>With DBNs and Stacked Auto-Encoders two kinds of training signals are available, and can be combined: the local layer-wise unsupervised training signal (from the RBM or auto-encoder associated with the layer), and a global supervised training signal (from the 1 Good at least in the sense of generalization. One possibility is to combine both signals during training, and this is called partially supervised training in <ref type="bibr" target="#b16">[17]</ref>. It has been found useful <ref type="bibr" target="#b16">[17]</ref> when the true input distribution P (X) is believed to be not strongly related to P (Y |X). To make sure that an RBM preserves information relevant to Y in its hidden representation, the CD update is combined with the classification log-probability gradient, and for some distributions better predictions are thus obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Semi-Supervised and</head><p>An appealing generalization of semi-supervised learning, especially in the context of deep architectures, is self-taught learning <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b147">148]</ref>, in which the unlabeled examples potentially come from classes other than the labeled classes. This is more realistic than the standard semisupervised setting, e.g., even if we are only interested in some specific object classes, one can much more easily obtain unlabeled examples of arbitrary objects from the web (whereas it would be expensive to select only those pertaining to those selected classes of interest).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants of RBMs and Auto-Encoders</head><p>We review here some of the variations that have been proposed on the basic RBM and auto-encoder models to extend and improve them.</p><p>We have already mentioned that it is straightforward to generalize the conditional distributions associated with visible or hidden units in RBMs, e.g., to any member of the exponential family <ref type="bibr" target="#b199">[200]</ref>. Gaussian units and exponential or truncated exponential units have been proposed or used in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b200">201]</ref>. With respect to the analysis presented here, the equations can be easily adapted by simply changing the domain of the sum (or integral) for the h i and x i . Diagonal quadratic terms (e.g., to yield Gaussian or truncated Gaussian distributions) can also be added in the energy function without losing the property that the free energy factorizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Sparse Representations in Auto-Encoders and RBMs</head><p>Sparsity has become a concept of great interest recently, not only in machine learning but also in statistics and signal processing, in particular with the work on compressed sensing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b46">47]</ref>, but it was introduced earlier in computational neuroscience in the context of sparse coding in the visual system <ref type="bibr" target="#b138">[139]</ref>, and has been a key element deep convolutional networks exploiting of a variant of auto-encoders <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b152">153]</ref> with a sparse distributed representation, and has become a key ingredient in Deep Belief Networks <ref type="bibr" target="#b109">[110]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Why a Sparse Representation?</head><p>We argue here that if one is going to have fixed-size representations, then sparse representations are more efficient (than non-sparse ones) in an information-theoretic sense, allowing for varying the effective number of bits per example. According to learning theory <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b192">193]</ref>, to obtain good generalization it is enough that the total number of bits needed to encode the whole training set be small, compared to the size of the training set. In many domains of interest different examples require different number of bits when compressed.</p><p>On the other hand, dimensionality reduction algorithms, whether linear such as PCA and ICA, or non-linear such as LLE and Isomap, map each example to the same low-dimensional space. In light of the above argument, it would be more efficient to map each example to a variable-length representation. To simplify the argument, assume this representation is a binary vector. If we are required to map each example to a fixed-length representation, a good solution would be to choose that representation to have enough degrees of freedom to represent the vast majority of the examples, while at the same allowing to compress that fixed-length bit vector to a smaller variable-size code for most of the examples. We now have two representations: the fixedlength one, which we might use as input to make predictions and make decisions, and a smaller, variable-size one, which can in principle be obtained from the fixed-length one through a compression step. For example, if the bits in our fixed-length representation vector have a high probability of being 0 (i.e., a sparsity condition), then for most examples it is easy to compress the fixed-length vector (in average by the amount of sparsity). For a given level of sparsity, the number of configurations of sparse vectors is much smaller than when less sparsity (or none at all) is imposed, so the entropy of sparser codes is smaller.</p><p>Another argument in favor of sparsity is that the fixed-length representation is going to be used as input for further processing, so that it should be easy to interpret. A highly compressed encoding is usually highly entangled, so that no subset of bits in the code can really be interpreted unless all the other bits are taken into account. Instead, we would like our fixed-length sparse representation to have the property that individual bits or small subsets of these bits can be interpreted, i.e., correspond to meaningful aspects of the input, and capture factors of variation in the data. For example, with a speech signal as input, if some bits encode the speaker characteristics and other bits encode generic features of the phoneme being pronounced, we have disentangled some of the factors of variation in the data, and some subset of the factors might be sufficient for some particular prediction tasks.</p><p>Another way to justify sparsity of the representation was proposed in <ref type="bibr" target="#b149">[150]</ref>, in the context of models based on auto-encoders. This view actually explains how one might get good models even though the partition function is not explicitly minimized, or only minimized approximately, as long as other constraints (such as sparsity) are used on the learned representation. Suppose that the representation learned by an auto-encoder is sparse, then the auto-encoder cannot reconstruct well every possible input pattern, because the number of sparse configurations is necessarily smaller than the number of dense configurations. To minimize the average reconstruction error on the training set, the auto-encoder then has to find a representation which captures statistical regularities of the data distribution. First of all, <ref type="bibr" target="#b149">[150]</ref> connect the free energy with a form of reconstruction error (when one replaces summing over hidden unit configurations by maximizing over them). Minimizing reconstruction error on the training set therefore amounts to minimizing free energy, i.e., maximizing the numerator of an energybased model likelihood (Equation (5.7)). Since the denominator (the partition function) is just a sum of the numerator over all possible input configurations, maximizing likelihood roughly amounts to making reconstruction error high for most possible input configurations, while making it low for those in the training set. This can be achieved if the encoder (which maps an input to its representation) is constrained in such a way that it cannot represent well most of the possible input patterns (i.e., the reconstruction error must be high for most of the possible input configurations). Note how this is already achieved when the code is much smaller than the input. Another approach is to impose a sparsity penalty on the representation <ref type="bibr" target="#b149">[150]</ref>, which can be incorporated in the training criterion. In this way, the term of the log-likelihood gradient associated with the partition function is completely avoided, and replaced by a sparsity penalty on the hidden unit code. Interestingly, this idea could potentially be used to improve CD-k RBM training, which only uses an approximate estimator of the gradient of the log of the partition function. If we add a sparsity penalty to the hidden representation, we may compensate for the weaknesses of that approximation, by making sure we increase the free energy of most possible input configurations, and not only of the reconstructed neighbors of the input example that are obtained in the negative phase of Contrastive Divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Sparse Auto-Encoders and Sparse Coding</head><p>There are many ways to enforce some form of sparsity on the hidden layer representation. The first successful deep architectures exploiting sparsity of representation involved auto-encoders <ref type="bibr" target="#b152">[153]</ref>. Sparsity was achieved with a so-called sparsifying logistic, by which the codes are obtained with a nearly saturating logistic whose offset is adapted to maintain a low average number of times the code is significantly nonzero. One year later the same group introduced a somewhat simpler variant <ref type="bibr" target="#b149">[150]</ref> based on a Student-t prior on the codes. The Student-t prior has been used in the past to obtain sparsity of the MAP estimates of the codes generating an input <ref type="bibr" target="#b138">[139]</ref> in computational neuroscience models of the V1 visual cortex area. Another approach also connected to computational neuroscience involves two levels of sparse RBMs <ref type="bibr" target="#b109">[110]</ref>. Sparsity is achieved with a regularization term that penalizes a deviation of the expected activation of the hidden units from a fixed low level. Whereas <ref type="bibr" target="#b138">[139]</ref> had already shown that one level of sparse coding of images led to filters very similar to those seen in V1, <ref type="bibr" target="#b109">[110]</ref> find that when training a sparse Deep Belief Network (i.e., two sparse RBMs on top of each other), the second level appears to learn to detect visual features similar to those observed in area V2 of visual cortex (i.e., the area that follows area V1 in the main chain of processing of the visual cortex of primates).</p><p>In the compressed sensing literature sparsity is achieved with the 1 penalty on the codes, i.e., given bases in matrix W (each column of W is a basis) we typically look for codes h such that the input signal x is reconstructed with low 2 reconstruction error while h is sparse:</p><formula xml:id="formula_55">min h ||x -W h|| 2 2 + λ||h|| 1 ,<label>(7.1)</label></formula><p>where</p><formula xml:id="formula_56">||h|| 1 = i |h i |.</formula><p>The actual number of non-zero components of h would be given by the 0 norm, but minimizing with it is combinatorially difficult, and the 1 norm is the closest p-norm that is also convex, making the overall minimization in Equation (7.1) convex. As is now well understood <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b46">47]</ref>, the 1 norm is a very good proxy for the 0 norm and naturally induces sparse results, and it can even be shown to recover exactly the true sparse code (if there is one), under mild conditions. Note that the 1 penalty corresponds to a Laplace prior, and that the posterior does not have a point mass at 0, but because of the above properties, the mode of the posterior (which is recovered when minimizing Equation (7.1)) is often at 0. Although minimizing Equation (7.1) is convex, minimizing jointly the codes and the decoder bases W is not convex, but has been done successfully with many different algorithms <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b138">139,</ref><ref type="bibr" target="#b147">148]</ref>. Like directed graphical models (such as the sigmoid belief networks discussed in Section 4.4), sparse coding performs a kind of explaining away: it chooses one configuration (among many) of the hidden codes that could explain the input. These different configurations compete, and when one is selected, the others are completely turned off. This can be seen both as an advantage and as a disadvantage. The advantage is that if a cause is much more probable than the other, than it is the one that we want to highlight. The disadvantage is that it makes the resulting codes somewhat unstable, in the sense that small perturbations of the input x could give rise to very different values of the optimal code h. This instability could spell trouble for higher levels of learned transformations or a trained classifier that would take h as input. Indeed it could make generalization more difficult if very similar inputs can end up being represented very differently in the sparse code layer. There is also a computational weakness of these approaches that some authors have tried to address. Even though optimizing Equation (7.1) is efficient it can be hundreds of time slower than the kind of computation involved in computing the codes in ordinary auto-encoders or RBMs, making both training and recognition very slow. Another issue connected to the stability question is the joint optimization of the bases W with higher levels of a deep architecture. This is particularly important in view of the objective of fine-tuning the encoding so that it focuses on the most discriminant aspects of the signal. As discussed in Section 9.1.2, significant classification error improvements were obtained when fine-tuning all the levels of a deep architecture with respect to a discriminant criterion of interest. In principle one can compute gradients through the optimization of the codes, but if the result of the optimization is unstable, the gradient may not exist or be numerically unreliable. To address both the stability issue and the above fine-tuning issue, <ref type="bibr" target="#b5">[6]</ref> propose to replace the 1 penalty by a softer approximation which only gives rise to approximately sparse coefficients (i.e., many very small coefficients, without actually converging to 0).</p><p>Keep in mind that sparse auto-encoders and sparse RBMs do not suffer from any of these sparse coding issues: computational complexity (of inferring the codes), stability of the inferred codes, and numerical stability and computational cost of computing gradients on the first layer in the context of global fine-tuning of a deep architecture. Sparse coding systems only parametrize the decoder: the encoder is defined implicitly as the solution of an optimization. Instead, an ordinary autoencoder or an RBM has an encoder part (computing P (h|x)) and a decoder part (computing P (x|h)). A middle ground between ordinary auto-encoders and sparse coding is proposed in a series of papers on sparse auto-encoders <ref type="bibr" target="#b149">[150,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b152">153]</ref> applied in pattern recognition and machine vision tasks. They propose to let the codes h be free (as in sparse coding algorithms), but include a parametric encoder (as in ordinary auto-encoders and RBMs) and a penalty for the difference between the free non-parametric codes h and the outputs of the parametric encoder. In this way, the optimized codes h try to satisfy two objectives: reconstruct well the input (like in sparse coding), while not being too far from the output of the encoder (which is stable by construction, because of the simple parametrization of the encoder). In the experiments performed, the encoder is just an affine transformation followed by a non-linearity like the sigmoid, and the decoder is linear as in sparse coding. Experiments show that the resulting codes work very well in the context of a deep architecture (with supervised fine-tuning) <ref type="bibr" target="#b149">[150]</ref>, and are more stable (e.g., with respect to slight perturbations of input images) than codes obtained by sparse coding <ref type="bibr" target="#b91">[92]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Denoising Auto-Encoders</head><p>The denoising auto-encoder <ref type="bibr" target="#b194">[195]</ref> is a stochastic version of the autoencoder where the input is stochastically corrupted, but the uncorrupted input is still used as target for the reconstruction. Intuitively, a denoising auto-encoder does two things: try to encode the input (preserve the information about the input), and try to undo the effect of a corruption process stochastically applied to the input of the autoencoder. The latter can only be done by capturing the statistical dependencies between the inputs. In fact, in <ref type="bibr" target="#b194">[195]</ref>, the stochastic corruption process consists in randomly setting some of the inputs (as many as half of them) to zero. Hence the denoising auto-encoder is trying to predict the missing values from the non-missing values, for randomly selected subsets of missing patterns. The training criterion for denoising auto-encoders is expressed as a reconstruction log-likelihood,</p><formula xml:id="formula_57">-log P (x|c(x)),<label>(7.2)</label></formula><p>where x is the uncorrupted input, x is the stochastically corrupted input, and c(x) is the code obtained from x. Hence the output of the decoder is viewed as the parameter for the above distribution (over the uncorrupted input). In the experiments performed <ref type="bibr" target="#b194">[195]</ref>, this distribution is factorized and binomial (one bit per pixel), and input pixel intensities are interpreted as probabilities. Note that a recurrent version of the denoising auto-encoder had been proposed earlier by <ref type="bibr" target="#b173">[174]</ref>, with corruption also corresponding to a form of occlusion (setting a rectangular region of the input image to 0). Using auto-encoders for denoising was actually introduced much earlier <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b54">55]</ref>. The main innovation in <ref type="bibr" target="#b194">[195]</ref> is therefore to show how this strategy is highly successful as unsupervised pre-training for a deep architecture, and to link the denoising auto-encoder to a generative model. Consider a random d-dimensional vector X, S a set of k indices, X S = (X S 1 , . . . , X S k ) the sub-elements selected by S, and let X -S all the sub-elements except those in S. Note that the set of conditional distributions P (X S |X -S ) for some choices of S fully characterize the joint distribution P (X), and this is exploited, for example, in Gibbs sampling. Note that bad things can happen when |S| = 1 and some pairs of input are perfectly correlated: the predictions can be perfect even though the joint has not really been captured, and this would correspond to a Gibbs chain that does not mix, i.e., does not converge. By considering random-size subsets and also insisting on reconstructing everything (like ordinary auto-encoders), this type of problem may be avoided in denoising auto-encoders.</p><p>Interestingly, in a series of experimental comparisons over 8 vision tasks, stacking denoising auto-encoders into a deep architecture fine-tuned with respect to a supervised criterion yielded generalization performance that was systematically better than stacking ordinary auto-encoders, and comparable or superior to Deep Belief Networks <ref type="bibr" target="#b194">[195]</ref>.</p><p>An interesting property of the denoising auto-encoder is that it can be shown to correspond to a generative model. Its training criterion is a bound on the log-likelihood of that generative model. Several possible generative models are discussed in <ref type="bibr" target="#b194">[195]</ref>. A simple generative model is semi-parametric: sample a training example, corrupt it stochastically, apply the encoder function to obtain the hidden representation, apply the decoder function to it (obtaining parameters for a distribution over inputs), and sample an input. This is not very satisfying because it requires to keep the training set around (like non-parametric density models). Other possible generative models are explored in <ref type="bibr" target="#b194">[195]</ref>.</p><p>Another interesting property of the denoising auto-encoder is that it naturally lends itself to data with missing values or multi-modal data (when a subset of the modalities may be available for any particular example). This is because it is trained with inputs that have "missing" parts (when corruption consists in randomly hiding some of the input values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Lateral Connections</head><p>The RBM can be made slightly less restricted by introducing interaction terms or "lateral connections" between visible units. Sampling h from P (h|x) is still easy but sampling x from P (x|h) is now generally more difficult, and amounts to sampling from a Markov Random Field which is also a fully observed Boltzmann machine, in which the offsets are dependent on the value of h. <ref type="bibr" target="#b140">[141]</ref> propose such a model for capturing image statistics and their results suggest that Deep Belief Nets (DBNs) based on such modules generate more realistic image patches than DBNs based on ordinary RBMs. Their results also show that the resulting distribution has marginal and pairwise statistics for pixel intensities that are similar to those observed on real image patches.</p><p>These lateral connections capture pairwise dependencies that can be more easily captured this way than using hidden units, saving the hidden units for capturing higher-order dependencies. In the case of the first layer, it can be seen that this amounts to a form of whitening, which has been found useful as a preprocessing step in image processing systems <ref type="bibr" target="#b138">[139]</ref>. The idea proposed by <ref type="bibr" target="#b140">[141]</ref> is to use lateral connections at all levels of a DBN (which can now be seen as a hierarchy of Markov random fields). The generic advantage of this type of approach would be that the higher level factors represented by the hidden units do not have to encode all the local "details" that the lateral connections at the levels below can capture. For example, when generating an image of a face, the approximate locations of the mouth and nose might be specified at a high level whereas their precise location could be selected in order to satisfy the pairwise preferences encoded in the lateral connections at a lower level. This appears to yield generated images with sharper edges and generally more accuracy in the relative locations of parts, without having to expand a large number of higher-level units.</p><p>In order to sample from P (x|h), we can start a Markov chain at the current example (which presumably already has pixel co-dependencies similar to those represented by the model, so that convergence should be quick) and only run a short chain on the x's (keeping h fixed). Denote U the square matrix of visible-to-visible connections, as per the general Boltzmann Machine energy function in Equation (5.15).</p><p>To reduce sampling variance in CD for this model, <ref type="bibr" target="#b140">[141]</ref> used five damped mean-field steps instead of an ordinary Gibbs chain on the x's: x t = αx t-1 + (1α)sigm(b + U x t-1 + W h), with α ∈ (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Conditional RBMs and Temporal RBMs</head><p>A Conditional RBM is an RBM where some of the parameters are not free but are instead parametrized functions of a conditioning random variable. For example, consider an RBM for the joint distribution P (x, h) between observed vector x and hidden vector h, with parameters (b, c, W ) as per Equation (5.15), respectively for input offsets b, hidden units offsets c, and the weight matrix W . This idea has been introduced by <ref type="bibr" target="#b181">[182,</ref><ref type="bibr" target="#b182">183]</ref> for context-dependent RBMs in which the hidden units offsets c are affine functions of a context variable z. Hence the RBM represents P (x, h|z) or, marginalizing over h, P (x|z). In general the parameters θ = (b, c, W ) of an RBM can be written as a parametrized function θ = f (z; ω), i.e., the actual free parameters of the conditional RBM with conditioning variable z are denoted ω. Generalizing RBMs to conditional RBMs allows building deep architectures in which the hidden variables at each level can be conditioned on the value of other variables (typically representing some form of context).</p><p>The Contrastive Divergence algorithm for RBMs can be easily generalized to the case of Conditional RBMs. The CD gradient estimator ∆θ on a parameter θ can be simply back-propagated to obtain a gradient estimator on ω:</p><formula xml:id="formula_58">∆ω = ∆θ ∂θ ∂ω . (<label>7.3)</label></formula><p>In the affine case c = β + M z (with c, β and z column vectors and M a matrix) studied by <ref type="bibr" target="#b182">[183]</ref>, the CD update on the conditional parameters is simply ∆β = ∆c, ∆M = ∆c z , <ref type="bibr">(7.4)</ref> where the last multiplication is an outer product (applying the chain rule on derivatives), and ∆c is the update given by CD-k on hidden units offsets.</p><p>This idea has been successfully applied to model conditional distributions P (x t |x t-1 , x t-2 , x t-3 ) in sequential data of human motion <ref type="bibr" target="#b182">[183]</ref>, where x t is a vector of joint angles and other geometric features computed from motion capture data of human movements such as walking and running. Interestingly, this allows generating realistic human motion sequences, by successively sampling the t-th frame given the previously sampled k frames, i.e., approximating</p><formula xml:id="formula_59">P (x 1 , x 2 , . . . ,x T ) ≈ P (x 1 , . . . ,x k ) T t=k+1 P (x t |x t-1 , . . . ,x t-k ).</formula><p>(7.5)</p><p>The initial frames can be generated by using special null values as context or using a separate model for P (x 1 , . . . ,x k ). As demonstrated by <ref type="bibr" target="#b125">[126]</ref>, it can be useful to make not just the offsets but also the weights conditional on a context variable. In that, case, we greatly increase the number of degrees of freedom, introducing the capability to model three-way interactions between an input unit x i , a hidden unit h j , and a context unit z k through interaction parameters ζ ijk . This approach has been used with x an image and z the previous image in a video, and the model learns to capture flow fields <ref type="bibr" target="#b125">[126]</ref>.</p><p>Probabilistic models of sequential data with hidden variables h t (called state) can gain a lot by capturing the temporal dependencies between the hidden states at different times t in the sequence. This is what allows Hidden Markov Models (HMMs) <ref type="bibr" target="#b146">[147]</ref> to capture dependencies in a long observed sequence x 1 , x 2 , . . . even if the model only considers the hidden state sequence h 1 , h 2 , . . . to be a Markov chain of order 1 (where the direct dependence is only between h t and h t+1 ). Whereas the hidden state representation h t in HMMs is local (all the possible values of h t are enumerated and specific parameters associated with each of these values), Temporal RBMs have been proposed <ref type="bibr" target="#b179">[180]</ref> to construct a distributed representation of the state. The idea is an extension of the Conditional RBM presented above, but where the context includes not only past inputs but also past values of the state, e.g., we build a model of where the context is z t = (h t-1 , x t-1 , . . . ,h t-k , x t-k ), as illustrated in Figure <ref type="figure">7</ref>.1. Although sampling of sequences generated by Temporal RBMs can be done as in Conditional RBMs (with the same MCMC approximation used to sample from RBMs, at each time step), exact inference of the hidden state sequence given an input sequence is no longer tractable. Instead, <ref type="bibr" target="#b179">[180]</ref> propose to use a mean-field filtering approximation of the hidden sequence posterior.</p><formula xml:id="formula_60">P (h t , x t |h t-1 , x t-1 , . . . ,h t-k , x t-k ),<label>(7.6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Factored RBMs</head><p>In several probabilistic language models, it has been proposed to learn a distributed representation of each word <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b127">128,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b168">169,</ref><ref type="bibr" target="#b169">170,</ref><ref type="bibr" target="#b170">171,</ref><ref type="bibr" target="#b206">207]</ref>. For an RBM that models a sequence of words, it would be convenient to have a parametrization that leads to automatically learning a distributed representation for each word in the vocabulary. This is essentially what <ref type="bibr" target="#b128">[129]</ref> proposed. Consider an RBM input x that is the concatenation of one-hot vectors v t for each word w t in a fixedsize sequence (w 1 , w 2 , . . . , w k ), i.e., v t contains all 0's except for a 1 at position w t , and x = (v 1 , v 2 , . . . ,v k ) . <ref type="bibr" target="#b128">[129]</ref> use a factorization of the RBM weight matrix W into two factors, one that depends on the location t in the input subsequence, and one that does not. Consider the computation of the hidden units' probabilities given the input subsequence (v 1 , v 2 , . . . ,v k ). Instead of applying directly a matrix W to x, do the following. First, each word symbol w t is mapped through a matrix R to a d-dimensional vector R .,wt = Rv t , for t ∈ {1 . . . k}; second, the concatenated vectors (R .,w 1 , R .,w 2 , . . . , R .,w k ) are multiplied by a matrix B.</p><p>Hence W = BDiag(R), where Diag(R) is a block-diagonal matrix filled with R on the diagonal. This model has produced n-grams with better log-likelihood <ref type="bibr" target="#b128">[129,</ref><ref type="bibr" target="#b129">130]</ref>, with further improvements in generalization performance when averaging predictions with state-of-the-art n-gram models <ref type="bibr" target="#b128">[129]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Generalizing RBMs and Contrastive Divergence</head><p>Let us try to generalize the definition of RBM so as to include a large class of parametrizations for which essentially the same ideas and learning algorithms (such as Contrastive Divergence) that we have discussed above can be applied in a straightforward way. We generalize RBMs as follows: a Generalized RBM is an energy-based probabilistic model with input vector x and hidden vector h whose energy function is such that P (h|x) and P (x|h) both factorize. This definition can be formalized in terms of the parametrization of the energy function, which is also proposed by <ref type="bibr" target="#b72">[73]</ref>:</p><p>Proposition 7.1. The energy function associated with a model of the form of Equation (5.5) such that P (h|x) = i P (h i |x) and P (x|h) = j P (x j |h) must have the form</p><formula xml:id="formula_61">Energy(x, h) = j φ j (x j ) + i ξ i (h i ) + i,j η i,j (h i , x j ). (7.7)</formula><p>This is a direct application of the Hammersley-Clifford theorem <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b60">61]</ref>. <ref type="bibr" target="#b72">[73]</ref> also showed that the above form is a necessary and sufficient condition to obtain complementary priors. Complementary priors allow the posterior distribution P (h|x) to factorize by a proper choice of P (h).</p><p>In the case where the hidden and input values are binary, this new formulation does not actually bring any additional power of representation. Indeed, η i,j (h i , x j ), which can take at most four different values according to the 2 × 2 configurations of (h i , x j ) could always be rewritten as a second order polynomial in (h i , x j ): a + bx j + ch i + dh i x j .</p><p>However, b and c can be folded into the offset terms and a into a global additive constant which does not matter (because it gets cancelled by the partition function).</p><p>On the other hand, when x or h are real vectors, one could imagine higher-capacity modeling of the (h i , x j ) interaction, possibly nonparametric, e.g., gradually adding terms to η i,j so as to better model the interaction. Furthermore, sampling from the conditional densities P (x j |h) or P (h i |x) would be tractable even if the η i,j are complicated functions, simply because these are one-dimensional densities from which efficient approximate sampling and numerical integration are easy to compute (e.g., by computing cumulative sums of the density over nested sub-intervals or bins).</p><p>This analysis also highlights the basic limitation of RBMs, which is that its parametrization only considers pairwise interactions between variables. It is because the h are hidden and because we can choose the number of hidden units, that we still have full expressive power over possible marginal distributions in x (in fact, we can represent any discrete distribution <ref type="bibr" target="#b101">[102]</ref>). Other variants of RBMs discussed in Section 7.4 allow three-way interactions <ref type="bibr" target="#b125">[126]</ref>.</p><p>What would be a Contrastive Divergence update in this generalized RBM formulation? To simplify notations we note that the φ j 's and ξ i 's in Equation (7.7) can be incorporated within the η i,j 's, so we ignore them in the following. Theorem 5.1 can still be applied with</p><formula xml:id="formula_62">FreeEnergy(x) = -log h exp   - i,j η i,j (h i , x j )   .</formula><p>The gradient of the free energy of a sample x is thus</p><formula xml:id="formula_63">∂FreeEnergy(x) ∂θ = h exp -i,j η i,j (h i , x j ) h exp -i,j η i,j ( hi , x j ) i,j ∂η i,j (h i , x j ) ∂θ = h P (h|x) i,j ∂η i,j (h i , x j ) ∂θ = E h   i,j ∂η i,j (h i , x j ) ∂θ x   .</formula><p>Thanks to Proposition 7.1, a Gibbs chain can still be run easily. Truncating the log-likelihood gradient expansion (Equation (5.28)) after k steps of the Gibbs chain, and approximating expectations with samples from this chain, one obtains an approximation of the log-likelihood gradient at training point x 1 that depends only on Gibbs samples h 1 , h k+1 and x k+1 :</p><formula xml:id="formula_64">∂ log P (x 1 ) ∂θ - ∂FreeEnergy(x 1 ) ∂θ + ∂FreeEnergy(x k+1 ) ∂θ ,   - i,j ∂η i,j (h 1,i , x 1,j ) ∂θ + i,j ∂η i,j (h k+1,i , x k+1,j ) ∂θ   ∝ ∆θ,</formula><p>with ∆θ the update rule for parameters θ of the model, corresponding to CD-k in such a generalized RBM. Note that in most parametrizations we would have a particular element of θ depend on η i,j 's in such a way that no explicit sum is needed. For instance (taking expectation over h k+1 instead of sampling) we recover Algorithm 1 when</p><formula xml:id="formula_65">η i,j (h i , x j ) = -W ij h i x j - b j x j n h - c i h i n x ,</formula><p>where n h and n x are, respectively, the numbers of hidden and visible units, and we also recover the other variants described by <ref type="bibr" target="#b199">[200,</ref><ref type="bibr" target="#b16">17]</ref> for different forms of the energy and allowed set of values for hidden and input units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stochastic Variational Bounds for Joint Optimization of DBN Layers</head><p>In this section, we discuss mathematical underpinnings of algorithms for training a DBN as a whole. The log-likelihood of a DBN can be lower bounded using Jensen's inequality, and as we discuss below, this can justify the greedy layer-wise training strategy introduced in <ref type="bibr" target="#b72">[73]</ref> and described in Section 6.1. We will use Equation (6.1) for a DBN joint distribution, writing h for h 1 (the first level hidden vector) to lighten notation, and introducing an arbitrary conditional distribution Q(h|x). First multiply log P (x) by 1 = h Q(h|x), then use P (x) = P (x, h)/P (h|x), and multiply by 1 = Q(h|x)/Q(h|x) and expand the terms:</p><formula xml:id="formula_66">log P (x) = h Q(h|x) log P (x) = h Q(h|x) log P (x, h) P (h|x) = h Q(h|x) log P (x, h) P (h|x) Q(h|x) Q(h|x) = H Q(h|x) + h Q(h|x) log P (x, h) + h Q(h|x) log Q(h|x) P (h|x) = KL(Q(h|x)||P (h|x)) + H Q(h|x) + h Q(h|x) (log P (h) + log P (x|h)) , (<label>8.1)</label></formula><p>where H Q(h|x) is the entropy of the distribution Q(h|x). Non-negativity of the KL divergence gives the inequality</p><formula xml:id="formula_67">log P (x) ≥ H Q(h|x) + h Q(h|x) (log P (h) + log P (x|h)) , (<label>8.2)</label></formula><p>which becomes an equality when P and Q are identical, e.g., in the single-layer case (i.e., an RBM). Whereas we have chosen to use P to denote probabilities under the DBN, we use Q to denote probabilities under an RBM (the first level RBM), and in the equations choose Q(h|x) to be the hidden-given-visible conditional distribution of that first level RBM. We define that first level RBM such that Q(x|h) = P (x|h). In general P (h|x) = Q(h|x). This is because although the marginal P (h) on the first layer hidden vector h 1 = h is determined by the upper layers in the DBN, the RBM marginal Q(h) only depends on the parameters of the RBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Unfolding RBMs into Infinite Directed Belief Networks</head><p>Before using the above decomposition of the likelihood to justify the greedy training procedure for DBNs, we need to establish a connection between P (h 1 ) in a DBN and the corresponding marginal Q(h 1 ) given by the first level RBM. The interesting observation is that there exists a DBN whose h 1 marginal equals the first RBM's h 1 marginal, i.e., P (h 1 ) = Q(h 1 ), as long the dimension of h 2 equals the dimension of h 0 = x. To see this, consider a second-level RBM whose weight matrix is the transpose of the first-level RBM (that is why we need the matching dimensions). Hence, by symmetry of the roles of visible and hidden in an RBM joint distribution (when transposing the weight matrix), the marginal distribution over the visible vector of the second RBM is equal to the marginal distribution Q(h 1 ) of the hidden vector of the first RBM.</p><p>Another interesting way to see this is given by <ref type="bibr" target="#b72">[73]</ref>: consider the infinite Gibbs sampling Markov chain starting at t = -∞ and terminating at t = 0, alternating between x and h 1 for the first RBM, with visible vectors sampled on even t and hidden vectors on odd t. This chain can be seen as an infinite directed belief network with tied parameters (all even steps use weight matrix W while all odd ones use weight matrix W ). Alternatively, we can summarize any sub-chain from t = -∞ to t = τ by an RBM with weight matrix W or W according to the parity of τ , and obtain a DBN with 1τ layers (not counting the input layer), as illustrated in Figure <ref type="figure">8</ref>.1. This argument also shows that a two-layer DBN in which the second level has weights equal to the transpose of the first level weights is equivalent to a single RBM.  <ref type="figure">8</ref>.1 An RBM can be unfolded as an infinite directed belief network with tied weights (see text). Left, the weight matrix W or its transpose are used depending on the parity of the layer index. This sequence of random variables corresponds to a Gibbs Markov chain to generate xt (for t large). Right, the top-level RBM in a DBN can also be unfolded in the same way, showing that a DBN is an infinite directed graphical model in which some of the layers are tied (all except the bottom few ones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Variational Justification of Greedy Layer-wise Training</head><p>Here, we discuss the argument made by <ref type="bibr" target="#b72">[73]</ref> that adding one RBM layer improves the likelihood of a DBN. Let us suppose we have trained an RBM to model x, which provides us with a model Q(x) expressed through two conditionals Q(h 1 |x) and Q(x|h 1 ). Exploiting the argument in the previous subsection, let us now initialize an equivalent two-layer DBN, i.e., generating P (x) = Q(x), by taking P (x|h 1 ) = Q(x|h 1 ) and P (h 1 , h 2 ) given by a second-level RBM whose weights are the transpose of the first-level RBM. Now let us come back to Equation (8.1) above, and the objective of improving the DBN likelihood by changing P (h 1 ), i.e., keeping P (x|h 1 ) and Q(h 1 |x) fixed but allowing the second level RBM to change. Interestingly, increasing the KL divergence term increases the likelihood. Starting from P (h 1 |x) = Q(h 1 |x), the KL term is zero (i.e., can only increase) and the entropy term in Equation (8.1) does not depend on the DBN P (h 1 ), so small improvements to the term with P (h 1 ) guarantee an increase in log P (x). We are also guaranteed that further improvements of the P (h 1 ) term (i.e., further training of the second RBM, detailed below) cannot bring the log-likelihood lower than it was before the second RBM was added. This is simply because of the positivity of the KL and entropy terms: further training of the second RBM increases a lower bound on the log-likelihood (Equation (8.2)), as argued by <ref type="bibr" target="#b72">[73]</ref>. This justifies training the second RBM to maximize the second term, i.e., the expectation over the training set of h 1 Q(h 1 |x) log P (h 1 ).</p><p>The second-level RBM is thus trained to maximize</p><p>x,h 1</p><formula xml:id="formula_68">P (x)Q(h 1 |x) log P (h 1 ),<label>(8.3)</label></formula><p>with respect to P (h 1 ). This is the maximum-likelihood criterion for a model that sees examples h 1 obtained as marginal samples from the joint distribution P (x)Q(h 1 |x). If we keep the first-level RBM fixed, then the second-level RBM could therefore be trained as follows: sample x from the training set, then sample h 1 ∼ Q(h 1 |x), and consider that h 1 as a training sample for the second-level RBM (i.e., as an observation for its 'visible' vector). If there was no constraint on P (h 1 ), the maximizer of the above training criterion would be its "empirical" or target distribution</p><formula xml:id="formula_69">P * (h 1 ) = x P (x)Q(h 1 |x). (8.4)</formula><p>The same argument can be made to justify adding a third layer, etc. We obtain the greedy layer-wise training procedure outlined in Section 6.1. In practice the requirement that layer sizes alternate is not satisfied, and consequently neither is it common practice to initialize the newly added RBM with the transpose of the weights at the previous layer <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b16">17]</ref>, although it would be interesting to verify experimentally (in the case where the size constraint is imposed) whether the initialization with the transpose of the previous layer helps to speed up training.</p><p>Note that as we continue training the top part of the model (and this includes adding extra layers), there is no guarantee that log P (x) (in average over the training set) will monotonically increase. As our lower bound continues to increase, the actual log-likelihood could start decreasing. Let us examine more closely how this could happen. It would require the KL(Q(h 1 |x)||P (h 1 |x)) term to decrease as the second RBM continues to be trained. However, this is unlikely in general: as the DBN's P (h 1 ) deviates more and more from the first RBM's marginal Q(h 1 ) on h 1 , it is likely that the posteriors P (h 1 |x) (from the DBN) and Q(h 1 |x) (from the RBM) deviate more and more (since P (h 1 |x) ∝ Q(x|h 1 )P (h 1 ) and Q(h 1 |x) ∝ Q(x|h 1 )Q(h 1 )), making the KL term in Equation (8.1) increase. As the training likelihood for the second RBM increases, P (h 1 ) moves smoothly from Q(h 1 ) towards P * (h 1 ). Consequently, it seems very plausible that continued training of the second RBM is going to increase the DBN's likelihood (not just initially) and by transitivity, adding more layers will also likely increase the DBN's likelihood. However, it is not true that increasing the training likelihood for the second RBM starting from any parameter configuration guarantees that the DBN likelihood will increase, since at least one pathological counter-example can be found (I. Sutskever, personal communication). Consider the case where the first RBM has very large hidden biases, so that Q(h 1 |x) = Q(h 1 ) = 1 h 1 = h = P * (h 1 ), but large weights and small visible offsets so that P (x i |h) = 1 x i =h i , i.e., the hidden vector is copied to the visible units. When initializing the second RBM with the transpose of the weights of the first RBM, the training likelihood of the second RBM cannot be improved, nor can the DBN likelihood. However, if the second RBM was started from a "worse" configuration (worse in the sense of its training likelihood, and also worse in the sense of the DBN likelihood), then P (h 1 ) would move towards P * (h 1 ) = Q(h 1 ), making the second RBM likelihood improve while the KL term would decrease and the DBN likelihood would decrease. These conditions could not happen when initializing the second RBM properly (with a copy of the first RBM). So it remains an open question whether we can find conditions (excluding the above) which guarantee that while the likelihood of the second RBM increases, the DBN likelihood also increases.</p><p>Another argument to explain why the greedy procedure works is the following (Hinton, NIPS'2007 tutorial). The training distribution for the second RBM (samples h 1 from P * (h 1 )) looks more like data generated by an RBM than the original training distribution P (x). This is because P * (h 1 ) was obtained by applying one sub-step of an RBM Gibbs chain on examples from P (x), and we know that applying many Gibbs steps would yield data from that RBM.</p><p>Unfortunately, when we train within this greedy layer-wise procedure an RBM that will not be the top-level level of a DBN, we are not taking into account the fact that more capacity will be added later to improve the prior on the hidden units. <ref type="bibr" target="#b101">[102]</ref> have proposed considering alternatives to Contrastive Divergence for training RBMs destined to initialize intermediate layers of a DBN. The idea is to consider that P (h) will be modeled with a very high capacity model (the higher levels of the DBN). In the limit case of infinite capacity, one can write down what that optimal P (h) will be: it is simply the stochastic transformation of the empirical distribution through the stochastic mapping Q(h|x) of the first RBM (or previous RBMs), i.e., P * of Equation <ref type="bibr">(8.4)</ref> in the case of the second level. Plugging this back into the expression for log P (x), one finds that a good criterion for training the first RBM is the KL divergence between the data distribution and the distribution of the stochastic reconstruction vectors after one step of the Gibbs chain. Experiments <ref type="bibr" target="#b101">[102]</ref> confirm that this criterion yields better optimization of the DBN (initialized with this RBM). Unfortunately, this criterion is not tractable since it involves summing over all configurations of the hidden vector h. Tractable approximations of it might be considered, since this criterion looks like a form of reconstruction error on a stochastic auto-encoder (with a generative model similar to one proposed for denoising auto-encoders <ref type="bibr" target="#b194">[195]</ref>). Another interesting alternative, explored in the next section, is to directly work on joint optimization of all the layers of a DBN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Joint Unsupervised Training of All the Layers</head><p>We discuss here how one could train a whole deep architecture such as a DBN in an unsupervised way, i.e., to represent well the input distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">The Wake-Sleep Algorithm</head><p>The Wake-Sleep algorithm <ref type="bibr" target="#b71">[72]</ref> was introduced to train sigmoidal belief networks (i.e., where the distribution of the top layer units factorizes). It is based on a "recognition" model Q(h|x) (along with Q(x) set to be the training set distribution) that acts as a variational approximation to the generative model P (h, x). Here, we denote with h all the hidden layers together. In a DBN, Q(h|x) is as defined above (Section 6.1), obtained by stochastically propagating samples upward (from input to higher layers) at each layer. In the Wake-Sleep algorithm, we decouple the recognition parameters (upward weights, used to compute Q(h|x)) from the generative parameters (downward weights, used to compute P (x|h)). The basic idea of the algorithm is simple: </p><formula xml:id="formula_70">(x) = KL(Q(h|x)||P (h|x)) + H Q(h|x) + h Q(h|x) (log P (h) + log P (x|h)) , (<label>8.7)</label></formula><p>shows that the log-likelihood can be bounded from below by the opposite of the Helmholtz free energy <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b52">53]</ref> </p><formula xml:id="formula_71">F : log P (x) = KL(Q(h|x)||P (h|x)) -F (x) ≥ -F (x),<label>(8.8)</label></formula><p>where</p><formula xml:id="formula_72">F (x) = -H Q(h|x) - h Q(h|x) (log P (h) + log P (x|h)) , (<label>8.9)</label></formula><p>and the inequality is tight when Q = P . The variational approach is based on maximizing the lower bound -F while trying to make the bound tight, i.e., minimizing KL(Q(h|x)||P (h|x)). When the bound is tight, an increase of -F (x) is more likely to yield an increase of log P (x). Since we decouple the parameters of Q and of P , we can now see what the two phases are doing. In the Wake phase we consider Q fixed and do a stochastic gradient step towards maximizing the expected value of F (x) over samples x of the training set, with respect to parameters of P (i.e., we do not care about the entropy of Q). In the Sleep phase we would ideally like to make Q as close to P as possible in the sense of minimizing KL(Q(h|x)||P (h|x)) (i.e., taking Q as the reference), but instead we minimize KL(P (h, x)||Q(h, x)), taking P as the reference, because KL(Q(h|x)||P (h|x)) is intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2">Transforming the DBN into a Boltzmann Machine</head><p>Another approach was recently proposed, yielding in the evaluated cases results superior to the use of the Wake-Sleep algorithm <ref type="bibr" target="#b160">[161]</ref>.</p><p>After initializing each layer as an RBM as already discussed in Section 6.1, the DBN is transformed into a corresponding deep Boltzmann machine. Because in a Boltzmann machine each unit receives input from above as well as from below, it is proposed to halve the RBM weights when initializing the deep Boltzmann machine from the layerwise RBMs. It is very interesting to note that the RBM initialization of the deep Boltzmann machine was crucial to obtain the good results reported. The authors then propose approximations for the positive phase and negative phase gradients of the Boltzmann machine (see Section 5.2 and Equation (5.16)). For the positive phase (which in principle requires holding x fixed and sampling from P (h|x)), they propose a variational approximation corresponding to a mean-field relaxation (propagating probabilities associated with each unit given the others, rather than samples, and iterating a few dozen times to let them settle). For the negative phase (which in principle requires sampling from the joint P (h, x)) they propose to use the idea of a persistent MCMC chain already discussed in Section 5.4.1 and introduced in <ref type="bibr" target="#b186">[187]</ref>. The idea is to keep a set of (h, x) states (or particles) that are updated by one Gibbs step according to the current model (i.e., sample each unit according to its probability given all the others at the previous step). Even though the parameters keep changing (very slowly), we continue the same Markov chain instead of starting a new one (as in the old Boltzmann machine algorithm <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b75">76]</ref>). This strategy seems to work very well, and <ref type="bibr" target="#b160">[161]</ref> report an improvement over DBNs on the MNIST dataset, both in terms of data log-likelihood (estimated using annealed importance sampling <ref type="bibr" target="#b162">[163]</ref>) and in terms of classification error (after supervised fine-tuning), bringing down the error rate from 1.2% to 0.95%. More recently, <ref type="bibr" target="#b110">[111]</ref> also transform the trained DBN into a deep Boltzmann machine in order to generate samples from it, and here the DBN has a convolutional structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Looking Forward 9.1 Global Optimization Strategies</head><p>As discussed Section 4.2, part of the explanation for the better generalization observed with layer-local unsupervised pre-training in deep architectures could well be that they help to better optimize the lower layers (near the input), by initializing supervised training in regions of parameter space associated with better unsupervised models. Similarly, initializing each layer of a deep Boltzmann machine as an RBM was important to achieve the good results reported <ref type="bibr" target="#b160">[161]</ref>. In both settings, we optimize a proxy criterion that is layer-local before fine-tuning with respect to the whole deep architecture.</p><p>Here, we draw connections between existing work and approaches that could help to deal with difficult optimization problems, based on the principle of continuation methods <ref type="bibr" target="#b2">[3]</ref>. Although they provide no guarantee to obtain the global optimum, these methods have been particularly useful in computational chemistry to find approximate solutions to difficult optimization problems involving the configurations of molecules <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b131">132,</ref><ref type="bibr" target="#b205">206]</ref>. The basic idea is to first solve an easier and smoothed version of the problem and gradually consider less smoothing, with the intuition that a smooth version of the problem reveals the global picture, just like with simulated annealing <ref type="bibr" target="#b92">[93]</ref>. One defines a single-parameter family of cost functions C λ (θ) such that C 0 can be optimized more easily (maybe convex in θ), while C 1 is the criterion that we actually wish to minimize. One first minimizes C 0 (θ) and then gradually increases λ while keeping θ at a local minimum of C λ (θ). Typically C 0 is a highly smoothed version of C 1 , so that θ gradually moves into the basin of attraction of a dominant (if not global) minimum of C 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.1">Greedy Layer-wise Training of DBNs as a Continuation Method</head><p>The greedy layer-wise training algorithm for DBNs described in Section 6.1 can be viewed as an approximate continuation method, as follows. First of all recall (Section 8.1) that the top-level RBM of a DBN can be unfolded into an infinite directed graphical model with tied parameters. At each step of the greedy layer-wise procedure, we untie the parameters of the top-level RBM from the parameters of the penultimate level. So one can view the layer-wise procedure as follows. The model structure remains the same, an infinite chain of sigmoid belief layers, but we change the constraint on the parameters at each step of the layer-wise procedure. Initially all the layers are tied.</p><p>After training the first RBM (i.e., optimizing under this constraint), we untie the first level parameters from the rest. After training the second RBM (i.e., optimizing under this slightly relaxed constraint), we untie the second level parameters from the rest, etc. Instead of a continuum of training criteria, we have a discrete sequence of (presumably) gradually more difficult optimization problems. By making the process greedy we fix the parameters of the first k levels after they have been trained and only optimize the (k + 1)th, i.e., train an RBM. For this analogy to be strict we would need to initialize the weights of the newly added RBM with the transpose of the previous one. Note also that instead of optimizing all the parameters, the greedy layer-wise approach only optimizes the new ones. But even with these approximations, this analysis suggests an explanation for the good performance of the layer-wise training approach in terms of reaching better solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.2">Unsupervised to Supervised Transition</head><p>The experiments reported in many papers clearly show that an unsupervised pre-training followed by a supervised fine-tuning works very well for deep architectures. Whereas previous work on combining supervised and unsupervised criteria <ref type="bibr" target="#b99">[100]</ref> focus on the regularization effect of an unsupervised criterion (and unlabeled examples, in semi-supervised learning), the discussion of Section 4.2 suggests that part of the gain observed with unsupervised pre-training of deep networks may arise out of better optimization of the lower layers of the deep architecture.</p><p>Much recent work has focused on starting from an unsupervised representation learning algorithm (such as sparse coding) and fine-tuning the representation with a discriminant criterion or combining the discriminant and unsupervised criteria <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr" target="#b120">121]</ref>.</p><p>In <ref type="bibr" target="#b96">[97]</ref>, an RBM is trained with a two-part visible vector that includes both the input x and the target class y. Such an RBM can either be trained to model the joint P (x, y) (e.g., by Contrastive Divergence) or to model the conditional P (y|x) (the exact gradient of the conditional log-likelihood is tractable). The best results reported <ref type="bibr" target="#b96">[97]</ref> combine both criteria, but the model is initialized using the nondiscriminant criterion.</p><p>In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b120">121]</ref> the task of training the decoder bases in a sparse coding system is coupled with a task of training a classifier on to of the sparse codes. After initializing the decoder bases using non-discriminant learning, they can be fine-tuned using a discriminant criterion that is applied jointly on the representation parameters (i.e., the first layer bases, that gives rise to the sparse codes) and a set of classifier parameters (e.g., a linear classifier that takes the representation codes as input).</p><p>According to <ref type="bibr" target="#b120">[121]</ref>, trying to directly optimize the supervised criterion without first initializing with non-discriminant training yielded very poor results. In fact, they propose a smooth transition from the nondiscriminant criterion to the discriminant one, hence performing a kind of continuation method to optimize the discriminant criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.3">Controlling Temperature</head><p>Even optimizing the log-likelihood of a single RBM might be a difficult optimization problem. It turns out that the use of stochastic gradient (such as the one obtained from CD-k) and small initial weights is again close to a continuation method, and could easily be turned into one. Consider the family of optimization problems corresponding to the regularization path <ref type="bibr" target="#b63">[64]</ref> for an RBM, e.g., with 2 regularization of the parameters, the family of training criteria parametrized by λ ∈ (0, 1]:</p><formula xml:id="formula_73">C λ (θ) = - i log P θ (x i ) -||θ|| 2 log λ. (9.1)</formula><p>When λ → 0, we have θ → 0, and it can be shown that the RBM loglikelihood becomes convex in θ. When λ → 1, there is no regularization (note that some intermediate value of λ might be better in terms of generalization, if the training set is small). Controlling the magnitude of the offsets and weights in an RBM is equivalent to controlling the temperature in a Boltzmann machine (a scaling coefficient for the energy function). High temperature corresponds to a highly stochastic system, and at the limit a factorial and uniform distribution over the input. Low temperature corresponds to a more deterministic system where only a small subset of possible configurations are plausible. Interestingly, one observes routinely that stochastic gradient descent starting from small weights gradually allows the weights to increase in magnitude, thus approximately following the regularization path. Early stopping is a well-known and efficient capacity control technique based on monitoring performance on a validation set during training and keeping the best parameters in terms of validation set error. The mathematical connection between early stopping and 2 regularization (along with margin) has already been established <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b210">211]</ref>: starting from small parameters and doing gradient descent yields gradually larger parameters, corresponding to a gradually less regularized training criterion. However, with ordinary stochastic gradient descent (with no explicit regularization term), there is no guarantee that we would be tracking the sequence of local minima associated with a sequence of values of λ in Equation (9.1). It might be possible to slightly change the stochastic gradient algorithm to make it track better the regularization path, (i.e., make it closer to a continuation method), by controlling λ explicitly, gradually increasing λ when the optimization is near enough a local minimum for the current value of λ. Note that the same technique might be extended for other difficult non-linear optimization problems found in machine learning, such as training a deep supervised neural network. We want to start from a globally optimal solution and gradually track local minima, starting from heavy regularization and moving slowly to little or none.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.4">Shaping: Training with a Curriculum</head><p>Another continuation method may be obtained by gradually transforming the training task, from an easy one (maybe convex) where examples illustrate the simpler concepts, to the target one (with more difficult examples). Humans need about two decades to be trained as fully functional adults of our society. That training is highly organized, based on an education system and a curriculum which introduces different concepts at different times, exploiting previously learned concepts to ease the learning of new abstractions. The idea of training a learning machine with a curriculum can be traced back at least to <ref type="bibr" target="#b48">[49]</ref>. The basic idea is to start small, learn easier aspects of the task or easier sub-tasks, and then gradually increase the difficulty level. From the point of view of building representations, advocated here, the idea is to learn representations that capture low-level abstractions first, and then exploit them and compose them to learn slightly higher-level abstractions necessary to explain more complex structure in the data. By choosing which examples to present and in which order to present them to the learning system, one can guide training and remarkably increase the speed at which learning can occur. This idea is routinely exploited in animal training and is called shaping <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b143">144,</ref><ref type="bibr" target="#b176">177]</ref>.</p><p>Shaping and the use of a curriculum can also be seen as continuation methods. For this purpose, consider the learning problem of modeling the data coming from a training distribution P . The idea is to reweigh the probability of sampling the examples from the training distribution, according to a given schedule that starts from the "easiest" examples and moves gradually towards examples illustrating more abstract concepts. At point t in the schedule, we train from distribution Pt , with P1 = P and P0 chosen to be easy to learn. Like in any continuation method, we move along the schedule when the learner has reached a local minimum at the current point t in the schedule, i.e., when it has sufficiently mastered the previously presented examples (sampled from Pt ). Making small changes in t corresponds to smooth changes in the probability of sampling examples in the training distribution, so we can construct a continuous path starting from an easy learning problem and ending in the desired training distribution. This idea is developed further in <ref type="bibr" target="#b19">[20]</ref>, with experiments showing better generalization obtained when training with a curriculum leading to a target distribution, compared to training only with the target distribution, on both vision and language tasks.</p><p>There is a connection between the shaping/curriculum idea and the greedy layer-wise idea. In both cases we want to exploit the notion that a high level abstraction can more conveniently be learned once appropriate lower-level abstractions have been learned. In the case of the layer-wise approach, this is achieved by gradually adding more capacity in a way that builds upon previously learned concepts. In the case of the curriculum, we control the training examples so as to make sure that the simpler concepts have actually been learned before showing many examples of the more advanced concepts. Showing complicated illustrations of the more advanced concepts is likely to be generally a waste of time, as suggested by the difficulty for humans to grasp a new idea if they do not first understand the concepts necessary to express that new idea compactly.</p><p>With the curriculum idea we introduce a teacher, in addition to the learner and the training distribution or environment. The teacher can use two sources of information to decide on the schedule: (a) prior knowledge about a sequence of concepts that can more easily be learned when presented in that order, and (b) monitoring of the learner's progress to decide when to move on to new material from the curriculum. The teacher has to select a level of difficulty for new examples which is a compromise between "too easy" (the learner will not need to change its model to account for these examples) and "too hard" (the learner cannot make an incremental change that can account for these examples so they will most likely be treated as outliers or special cases, i.e., not helping generalization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Why Unsupervised Learning is Important</head><p>One of the claims of this monograph is that powerful unsupervised or semi-supervised (or self-taught) learning is a crucial component in building successful learning algorithms for deep architectures aimed at approaching AI. We briefly cover the arguments in favor of this hypothesis here:</p><p>• Scarcity of labeled examples and availability of many unlabeled examples (possibly not only of the classes of interest, as in self-taught learning <ref type="bibr" target="#b147">[148]</ref>). • Unknown future tasks: if a learning agent does not know what future learning tasks it will have to deal with in the future, but it knows that the task will be defined with respect to a world (i.e., random variables) that it can observe now, it would appear very rational to collect and integrate as much information as possible about this world so as to learn what makes it tick. • Once a good high-level representation is learned, other learning tasks (e.g., supervised or reinforcement learning) could be much easier. We know for example that kernel machines can be very powerful if using an appropriate kernel, i.e., an appropriate feature space. Similarly, we know powerful reinforcement learning algorithms which have guarantees in the case where the actions are essentially obtained through linear combination of appropriate features. We do not know what the appropriate representation should be, but one would be reassured if it captured the salient factors of variation in the input data, and disentangled them. • Layer-wise unsupervised learning: this was argued in Section 4.3. Much of the learning could be done using information available locally in one layer or sub-layer of the architecture, thus avoiding the hypothesized problems with supervised gradients propagating through long chains with large fan-in elements. • Connected to the two previous points is the idea that unsupervised learning could put the parameters of a supervised or reinforcement learning machine in a region from which gradient descent (local optimization) would yield good solutions. This has been verified empirically in several settings, in particular in the experiment of Figure <ref type="figure">4</ref>.2 and in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b49">50]</ref>. • The extra constraints imposed on the optimization by requiring the model to capture not only the input-to-target dependency but also the statistical regularities of the input distribution might be helpful in avoiding some poorly generalizing apparent local minima (those that do not correspond to good modeling of the input distribution). Note that in general extra constraints may also create more local minima, but we observe experimentally <ref type="bibr" target="#b16">[17]</ref> that both training and test error can be reduced by unsupervised pre-training, suggesting that the unsupervised pre-training moves the parameters in a region of space closer to local minima corresponding to learning better representations (in the lower layers). It has been argued <ref type="bibr" target="#b70">[71]</ref> (but is debatable) that unsupervised learning is less prone to overfitting than supervised learning. Deep architectures have typically been used to construct a supervised classifier, and in that case the unsupervised learning component can clearly be seen as a regularizer or a prior <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b49">50]</ref> that forces the resulting parameters to make sense not only to model classes given inputs but also to capture the structure of the input distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Open Questions</head><p>Research on deep architectures is still young and many questions remain unanswered. The following are potentially interesting.</p><p>1. Can the results pertaining to the role of computational depth in circuits be generalized beyond logic gates and linear threshold units?</p><p>2. Is there a depth that is mostly sufficient for the computations necessary to approach human-level performance of AI tasks? 3. How can the theoretical results on depth of circuits with a fixed size input be generalized to dynamical circuits operating in time, with context and the possibility of recursive computation? 4. Why is gradient-based training of deep neural networks from random initialization often unsuccessful? 5. Are RBMs trained by CD doing a good job of preserving the information in their input (since they are not trained as auto-encoders they might lose information about the input that may turn out to be important later), and if not how can that be fixed? 6. Is the supervised training criterion for deep architectures (and maybe the log-likelihood in deep Boltzmann machines and DBNs) really fraught with actual poor local minima or is it just that the criterion is too intricate for the optimization algorithms tried (such as gradient descent and conjugate gradients)? 7. Is the presence of local minima an important issue in training RBMs? 8. Could we replace RBMs and auto-encoders by algorithms that would be proficient at extracting good representations but involving an easier optimization problem, perhaps even a convex one? 9. Current training algorithms for deep architectures involves many phases (one per layer, plus a global fine-tuning). This is not very practical in the purely online setting since once we have moved into fine-tuning, we might be trapped in an apparent local minimum. Is it possible to come up with a completely online procedure for training deep architectures that preserves an unsupervised component all along? Note that <ref type="bibr" target="#b201">[202]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This monograph started with a number of motivations: first to use learning to approach AI, then on the intuitive plausibility of decomposing a problem into multiple levels of computation and representation, followed by theoretical results showing that a computational architecture that does not have enough of these levels can require a huge number of computational elements, and the observation that a learning algorithm that relies only on local generalization is unlikely to generalize well when trying to learn highly varying functions. Turning to architectures and algorithms, we first motivated distributed representations of the data, in which a huge number of possible configurations of abstract features of the input are possible, allowing a system to compactly represent each example, while opening the door to a rich form of generalization. The discussion then focused on the difficulty of successfully training deep architectures for learning multiple levels of distributed representations. Although the reasons for the failure of standard gradient-based methods in this case remain to be clarified, several algorithms have been introduced in recent years that demonstrate much better performance than was previously possible with simple gradient-based optimization, and we have tried to focus on the underlying principles behind their success.</p><p>Although much of this monograph has focused on deep neural net and deep graphical model architectures, the idea of exploring learning algorithms for deep architectures should be explored beyond the neural net framework. For example, it would be interesting to consider extensions of decision tree and boosting algorithms to multiple levels.</p><p>Kernel-learning algorithms suggest another path which should be explored, since a feature space that captures the abstractions relevant to the distribution of interest would be just the right space in which to apply the kernel machinery. Research in this direction should consider ways in which the learned kernel would have the ability to generalize non-locally, to avoid the curse of dimensionality issues raised in Section 3.1 when trying to learn a highly varying function.</p><p>The monograph focused on a particular family of algorithms, the Deep Belief Networks, and their component elements, the Restricted Boltzmann Machine, and very near neighbors: different kinds of autoencoders, which can also be stacked successfully to form a deep architecture. We studied and connected together estimators of the loglikelihood gradient in Restricted Boltzmann machines, helping to justify the use of the Contrastive Divergence update for training Restricted Boltzmann Machines. We highlighted an optimization principle that has worked well for Deep Belief Networks and related algorithms such as Stacked Auto-Encoders, based on a greedy, layer-wise, unsupervised initialization of each level of the model. We found that this optimization principle is actually an approximation of a more general optimization principle, exploited in so-called continuation methods, in which a series of gradually more difficult optimization problems are solved. This suggested new avenues for optimizing deep architectures, either by tracking solutions along a regularization path, or by presenting the system with a sequence of selected examples illustrating gradually more complicated concepts, in a way analogous to the way students or animals are trained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 . 1</head><label>21</label><figDesc>Fig. 2.1 Examples of functions represented by a graph of computations, where each node is taken in some "element set" of allowed computations. Left, the elements are { * , +, -, sin} ∪ R. The architecture computes x * sin(a * x + b) and has depth 4. Right, the elements are artificial neurons computing f (x) = tanh(b + w x); each element in the set has a different (w, b) parameter. The architecture is a multi-layer neural network of depth 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 . 2</head><label>22</label><figDesc>Fig.2.2 Example of polynomial circuit (with products on odd layers and sums on even ones) illustrating the factorization enjoyed by a deep architecture. For example the level-1 product x 2 x 3 would occur many times (exponential in depth) in a depth 2 (sum of product) expansion of the above polynomial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 . 2</head><label>42</label><figDesc>Fig. 4.2 Deep architecture trained online with 10 million examples of digit images, either with pre-training (triangles) or without (circles). The classification error shown (vertical axis, log-scale) is computed online on the next 1000 examples, plotted against the number of examples seen from the beginning. The first 2.5 million examples are used for unsupervised pre-training (of a stack of denoising auto-encoders). The oscillations near the end are because the error rate is too close to 0, making the sampling variations appear large on the log-scale. Whereas with a very large training set regularization effects should dissipate, one can see that without pre-training, training converges to a poorer apparent local minimum: unsupervised pre-training helps to find a better minimum of the online error. Experiments were performed by Dumitru Erhan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 . 2 , a 3 -</head><label>423</label><figDesc>hidden layer neural network trained online converges to significantly lower error when it is pre-trained (as a Stacked Denoising Auto-Encoder, see Section 7.2). The figure shows progress with the online error (on the next 1000 examples), an unbiased Monte-Carlo estimate of generalization error. The first 2.5 million updates are used for unsupervised pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>- 2 k=1PFig. 4 . 4 Fig. 4 . 5</head><label>24445</label><figDesc>Fig.4.4 Graphical model of a Deep Belief Network with observed vector x and hidden layers h 1 , h 2 and h 3 . Notation is as in Figure4.3. The structure is similar to a sigmoid belief network, except for the top two layers. Instead of having a factorized prior for P (h 3 ), the joint of the top two layers, P (h 2 , h 3 ), is a Restricted Boltzmann Machine. The model is mixed, with double arrows on the arcs between the top two layers because an RBM is an undirected graphical model rather than a directed one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 . 5 ,</head><label>45</label><figDesc>and whose inference and training algorithms are described in more detail in Sections 5.3 and 5.4, respectively. This apparently slight change from sigmoidal belief networks to DBNs comes with a different learning algorithm, which exploits the notion of training greedily one layer at a time, building up gradually more abstract representations of the raw input into the posteriors P (h k |x). A detailed description of RBMs and of the greedy layer-wise training algorithms for deep architectures follows in Sections 5 and 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, justifying algorithms based on training the generative model with a classification criterion separating model samples from training examples. The maximum likelihood criterion wants the likelihood to be high on the training examples and low elsewhere. If we already have a model and we want to increase its likelihood, the contrast between where the model puts high probability (represented by samples) and where the training examples are indicates how to change the model. If we were able to approximately separate training examples from model samples with a decision surface, we could increase likelihood by reducing the value of the energy function on one side of the decision surface (the side where there are more training examples) and increasing it on the other side (the side</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Partially Supervised Training 73 deep multi-layer network sharing the same parameters as the DBN or Stacked Auto-Encoder). In the algorithms presented above, the two training signals are used in sequence: first an unsupervised training phase, and second a supervised fine-tuning phase. Other combinations are possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 . 1</head><label>71</label><figDesc>Fig. 7.1 Example of Temporal RBM for modeling sequential data, including dependencies between the hidden states. The double-arrow full arcs indicate an undirected connection, i.e., an RBM. The single-arrow dotted arcs indicate conditional dependency: the (xt, ht) RBM is conditioned by the values of the past inputs and past hidden state vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig.8.1 An RBM can be unfolded as an infinite directed belief network with tied weights (see text). Left, the weight matrix W or its transpose are used depending on the parity of the layer index. This sequence of random variables corresponds to a Gibbs Markov chain to generate xt (for t large). Right, the top-level RBM in a DBN can also be unfolded in the same way, showing that a DBN is an infinite directed graphical model in which some of the layers are tied (all except the bottom few ones).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1 .</head><label>1</label><figDesc>Wake phase: sample x from the training set, generate h ∼ Sleep algorithm has been used for DBNs in<ref type="bibr" target="#b72">[73]</ref>, after the weights associated with each layer have been trained as RBMs as discussed earlier. For a DBN with layers (h 1 , . . . ,h ), the Wake phase updates for the weights of the top RBM (between h -1 and h ) is done by considering the h -1 sample (obtained from Q(h|x)) as training data for the top RBM.A variational approximation can be used to justify the Wake-Sleep algorithm. The log-likelihood decomposition in Equation (8.1) log P</figDesc><table><row><cell cols="2">2. Sleep phase: sample (h, x) from the model P (x, h), and</cell></row><row><cell cols="2">use that pair as fully observed data for training Q(h|x).</cell></row><row><cell cols="2">This corresponds to doing one stochastic gradient step with</cell></row><row><cell>respect to</cell><cell></cell></row><row><cell>P (h, x) log Q(h|x).</cell><cell>(8.6)</cell></row><row><cell>h,x</cell><cell></cell></row><row><cell>The Wake-</cell><cell></cell></row><row><cell cols="2">Q(h|x) and use this (h, x) as fully observed data for training</cell></row><row><cell cols="2">P (x|h) and P (h). This corresponds to doing one stochastic</cell></row><row><cell>gradient step with respect to</cell><cell></cell></row><row><cell>Q(h|x) log P (x, h).</cell><cell>(8.5)</cell></row><row><cell>h</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>is appealing for this reason. 10. Should the number of Gibbs steps in Contrastive Divergence be adjusted during training? 11. Can we significantly improve upon Contrastive Divergence, taking computation time into account? New alternatives have recently been proposed which deserve further investigation [187, 188]. 12. Besides reconstruction error, are there other more appropriate ways to monitor progress during training of RBMs and DBNs? Equivalently, are there tractable approximations of the partition function in RBMs and DBNs? Recent work in this direction [163, 133] using annealed importance sampling is encouraging. 13. Could RBMs and auto-encoders be improved by imposing some form of sparsity penalty on the representations they learn, and what are the best ways to do so? 14. Without increasing the number of hidden units, can the capacity of an RBM be increased using non-parametric forms of its energy function? 15. Since we only have a generative model for single denoising auto-encoders, is there a probabilistic interpretation to models learned in Stacked Auto-Encoders or Stacked Denoising Auto-Encoders? 16. How efficient is the greedy layer-wise algorithm for training Deep Belief Networks (in terms of maximizing the training data likelihood)? Is it too greedy? 17. Can we obtain low variance and low bias estimators of the log-likelihood gradient in Deep Belief Networks and related deep generative models, i.e., can we jointly train all the layers (with respect to the unsupervised objective)? 18. Unsupervised layer-level training procedures discussed here help training deep architectures, but experiments suggest that training still gets stuck in apparent local minima and cannot exploit all the information in very large datasets. Is it true? Can we go beyond these limitations by developing more powerful optimization strategies for deep architectures? 19. Can optimization strategies based on continuation methods deliver significantly improved training of deep architectures? 20. Are there other efficiently trainable deep architectures besides Deep Belief Networks, Stacked Auto-Encoders, and deep Boltzmann machines? 21. Is a curriculum needed to learn the kinds of high-level abstractions that humans take years or decades to learn? 22. Can the principles discovered to train deep architectures be applied or generalized to train recurrent networks or dynamical belief networks, which learn to represent context and long-term dependencies? 23. How can deep architectures be generalized to represent information that, by its nature, might seem not easily representable by vectors, because of its variable size and structure (e.g., trees, graphs)? 24. Although Deep Belief Networks are in principle well suited for the semi-supervised and self-taught learning settings, what are the best ways to adapt the current deep learning algorithms to these setting and how would they fare compared to existing semi-supervised algorithms? 25. When labeled examples are available, how should supervised and unsupervised criteria be combined to learn the model's representations of the input? 26. Can we find analogs of the computations necessary for Contrastive Divergence and Deep Belief Net learning in the brain? 27. The cortex is not at all like a feedforward neural network in that there are significant feedback connections (e.g., going back from later stages of visual processing to earlier ones) and these may serve a role not only in learning (as in RBMs) but also in integrating contextual priors with visual evidence [112]. What kind of models can give rise to such interactions in deep architectures, and learn properly with such interactions?</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Except for neural networks with a special structure called convolutional networks, discussed in Section 4.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The target function is the function that we would like the learner to discover.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>In the Gaussian Process case, as in kernel regression, f (x) in Equation (3.1) is the conditional expectation of the target variable Y to predict, given the input x.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p>We call them apparent local minima in the sense that the gradient descent learning trajectory is stuck there, which does not completely rule out that more powerful optimizers could not find significantly better solutions far from these.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4"><p>In the same basin of attraction of the gradient descent procedure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_5"><p>For example, the MNIST digit images form rather well-separated clusters, especially when learning good representations, even unsupervised<ref type="bibr" target="#b191">[192]</ref>, so that the decision surfaces can be guessed reasonably well even before seeing any label.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_6"><p>For example, all the lower layers of a multi-layer neural net estimating P (Y |X) can be initialized with the parameters from a Deep Belief Net estimating P (X).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_7"><p>Maybe too many years? It is good that the field is moving towards more ambitious benchmarks, such as those introduced by<ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b98">99]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_8"><p>For example, if U was not symmetric, the extra degrees of freedom would be wasted sincex i U ij x j + x j U ji x i can be rewritten x i (U ij + U ji )x j = 1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_9"><p>x i (U ij + U ji )x j + 1 2 x j (U ij + U ji )x i , i.e., in a symmetric-matrix form.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_10"><p>Aperiodic: no state is periodic with period k &gt; 1; a state has period k if one can only return to it at times t + k, t + 2k, etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_11"><p>Irreducible: one can reach any state from any state in finite time with non-zero probability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_12"><p>It is debatable whether or not one would take into account the fact that h 1 depends on θ when computing the gradient in the mean-field approximation of Equation (5.30), but it must be the case to draw a direct link with auto-encoders.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author is particularly grateful for the inspiration from and constructive input from Yann LeCun, Aaron Courville, Olivier Delalleau, Dumitru Erhan, Pascal Vincent, Geoffrey Hinton, Joseph Turian, Hugo Larochelle, Nicolas Le Roux, Jérôme Louradour, Pascal Lamblin, James Bergstra, Pierre-Antoine Manzagol and Xavier Glorot. This research was performed thanks to the funding from NSERC, MITACS, and the Canada Research Chairs.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2</head><p>TrainUnsupervisedDBN( P , , , W, b, c, mean field computation) Train a DBN in a purely unsupervised way, with the greedy layer-wise procedure in which each added layer is trained as an RBM (e.g., by Contrastive Divergence). P is the input training distribution for the network is a learning rate for the RBM training is the number of layers to train W k is the weight matrix for level k, for k from 1 to b k is the visible units offset vector for RBM at level k, for k from 1 to c k is the hidden units offset vector for RBM at level k, for k from 1 to mean field computation is a Boolean that is true iff training data at each additional level is obtained by a mean-field approximation instead of stochastic sampling</p><p>) for future use} end while end for</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning algorithm for boltzmann machines</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training hierarchical feed-forward visual recognition models using transfer learning from pseudo tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Computer Vision (ECCV&apos;08)</title>
		<meeting>the 10th European Conference on Computer Vision (ECCV&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="69" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Numerical Continuation Methods. An Introduction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Allgower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Georg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Springer Series in Computational Mathematics</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="1980">1980</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An introduction to MCMC for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="5" to="43" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An energy budget for signaling in the grey matter of the brain</title>
		<author>
			<persName><forename type="first">D</forename><surname>Attwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Laughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cerebral Blood Flow And Metabolism</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1133" to="1145" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differentiable sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS&apos;08)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>NIPS Foundation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning internal representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Computational Learning Theory (COLT&apos;95)</title>
		<meeting>the 8th International Conference on Computational Learning Theory (COLT&apos;95)<address><addrLine>Santa Cruz, California</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Bayesian/information theoretic model of learning via multiple task sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="7" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regularization and semi-supervised learning on large graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Computational Learning Theory (COLT&apos;04)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<meeting>the 17th International Conference on Computational Learning Theory (COLT&apos;04)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="624" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using manifold structure for partially labeled classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15 (NIPS&apos;02)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An information maximisation approach to blind separation and blind deconvolution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Justifying and generalizing contrastive divergence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1601" to="1621" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Curse of highly variable functions for local kernel machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18 (NIPS&apos;05)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decision trees do not generalize to new variations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13 (NIPS&apos;00)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="933" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19 (NIPS&apos;06)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convex neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marcotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18 (NIPS&apos;05)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling learning algorithms towards AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large Scale Kernel Machines</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-sixth InternationalConference onMachine Learning (ICML09</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</editor>
		<meeting>the Twenty-sixth InternationalConference onMachine Learning (ICML09<address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-local estimation of manifold structure</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monperrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2509" to="2528" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Slow, decorrelated features for pretraining complex cell-like networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22 (NIPS&apos;09)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting><address><addrLine>Pittsburgh</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Charting a manifold</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15 (NIPS&apos;02)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth International Group</publisher>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fundamentals of Statistical Exponential Families</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Inst. of Math. Statist. Lecture Notes Monograph Series</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On contrastive divergence learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpiñan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTATS&apos;05)</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics (AISTATS&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multitask connectionist learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 Connectionist Models Summer School</title>
		<meeting>the 1993 Connectionist Models Summer School</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="372" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Markov random fields in statistics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Disorder in Physical Systems: A Volume in Honour of John M. Hammersley</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Grimmett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Welsh</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="19" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 7 (NIPS&apos;94)</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="705" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Parallel continuation-based global optimization for molecular conformation and protein folding</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Cornell University, Dept. of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Links between perceptrons, MLPs and SVMs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-first International Conference on Machine Learning (ICML&apos;04)</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</editor>
		<meeting>the Twenty-first International Conference on Machine Learning (ICML&apos;04)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rational kernels: Theory and algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1035" to="1062" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On kerneltarget alignment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14 (NIPS&apos;01)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="367" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Complexity lower bounds for approximation algebraic computation trees</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grigoriev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Helmholtz machine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient non-parametric function induction in semi-supervised learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005-01">January 2005</date>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Empirical evaluation of convolutional RBMs for vision</title>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1327</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Département d&apos;Informatique et de Recherche Opérationnelle, Université de Montréal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A theoretical analysis of robust coding over noisy overcomplete channels</title>
		<author>
			<persName><forename type="first">E</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18 (NIPS&apos;05)</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="307" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hybrid Monte Carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roweth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Lett. B</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning and development in neural networks: The importance of starting small</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="781" to="799" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pretraining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;09)</title>
		<meeting>The Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised learning of distributions on binary vectors using two layer networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<idno>UCSC-CRL-94-25</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>University of California</publisher>
			<pubPlace>Santa Cruz</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of Thirteenth International Conference</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Does the wake-sleep algorithm learn good density estimators?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 8 (NIPS&apos;95)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hasselmo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Memoires associatives distribuees</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fogelman-Soulie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COGNITIVA 87</title>
		<meeting>COGNITIVA 87<address><addrLine>Paris; La Villette</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A survey of kernels for structured data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the Bayesian restoration of images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984-11">November 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shift-invariant sparse coding for audio classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-third Conference on Uncertainty in Artificial Intelligence (UAI&apos;07)</title>
		<meeting>the Twenty-third Conference on Uncertainty in Artificial Intelligence (UAI&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR&apos;06)</title>
		<meeting>the Computer Vision and Pattern Recognition Conference (CVPR&apos;06)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep belief net learning in a long-range vision system for autonomous offroad driving</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scoffier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intelligent Robots and Systems (IROS&apos;08)</title>
		<meeting>Intelligent Robots and Systems (IROS&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="628" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Markov field on finite graphs and lattices</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hammersley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clifford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note>Unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Almost optimal lower bounds for small depth circuits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Håstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th annual ACM Symposium on Theory of Computing</title>
		<meeting>the 18th annual ACM Symposium on Theory of Computing<address><addrLine>Berkeley, California</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="6" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">On the power of small-depth threshold circuits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Håstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="113" to="129" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The entire regularization path for the support vector machine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1391" to="1415" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A nonparametric bayesian approach to modeling overlapping clusters</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)</title>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)<address><addrLine>San Juan, Porto Rico</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Statistical models for partial membership</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="392" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">Parallel Models of Associative Memory</title>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Assoc</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual Conference of the Cognitive Science Society</title>
		<meeting>the Eighth Annual Conference of the Cognitive Science Society<address><addrLine>Amherst; Hillsdale</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Products of experts</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>the Ninth International Conference on Artificial Neural Networks (ICANN)<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>IEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">To recognize shapes, first learn to generate images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>UTML TR 2006-003</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The wake-sleep algorithm for unsupervised neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="1558" to="1161" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning and relearning in Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<editor>
			<persName><forename type="first">(</forename><forename type="middle">D E</forename><surname>Foundations</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Boltzmann machines: Constraint satisfaction networks that learn</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<idno>TR-CMU-CS-84-119</idno>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University, Dept. of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A new view of ICA</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd International Conference on Independent Component Analysis and Blind Signal Separation (ICA&apos;01)</title>
		<meeting>3rd International Conference on Independent Component Analysis and Blind Signal Separation (ICA&apos;01)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length, and Helmholtz free energy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 6 (NIPS&apos;93)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Alspector</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann Publishers, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Random decision forest</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Document Analysis and Recognition (ICDAR&apos;95)</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen Netzen</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>Institut für Informatik, Lehrstuhl Prof. Brauer, Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma thesis</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="498" to="520" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction, and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology (London)</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models using score matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1529" to="1531" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Some extensions of score matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2499" to="2512" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<title level="m">Independent Component Analysis</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">How to make a low-dimensional representation suitable for diverse tasks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Intrator</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science, Special issue on Transfer in Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="205" to="224" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<ptr target="http://www.cse.ucsc.edu/haussler/pubs.html" />
	</analytic>
	<monogr>
		<title level="m">A shorter version is in Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>Dept.of Computer Science, Univ. of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Nonlinear autoassociation is not equivalent to PCA</title>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gluck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="531" to="545" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Learning in Graphical Models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Dordrecht, Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Fast inference in sparse coding algorithms with applications to object recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CBLL-TR-2008-12-01</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Computational and Biological Learning Lab, Courant Institute, NYU,</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D G</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A two-layer ICA-like model estimated by score matching</title>
		<author>
			<persName><forename type="first">U</forename><surname>Köster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Artificial Neural Networks (ICANN&apos;2007)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="798" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Flexible shaping: How learning in small steps helps</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="380" to="394" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semi-definite programming</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">El</forename><surname>Gahoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning (ICML&apos;02)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Sammut</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hoffmann</surname></persName>
		</editor>
		<meeting>the Nineteenth International Conference on Machine Learning (ICML&apos;02)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="323" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Classification using discriminative restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Principled hybrids of generative and discriminative models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lasserre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR&apos;06)</title>
		<meeting>the Computer Vision and Pattern Recognition Conference (CVPR&apos;06)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Representational power of restricted boltzmann machines and deep belief networks</title>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1631" to="1649" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Modèles connexionistes de l&apos;apprentissage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>Université de Paris VI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Efficient BackProp</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting Structured Data</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bakir</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hofman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="191" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Loss functions for discriminative training of energy-based models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTATS&apos;05)</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics (AISTATS&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR&apos;04)</title>
		<meeting>the Computer Vision and Pattern Recognition Conference (CVPR&apos;04)<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19 (NIPS&apos;06)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Sparse deep belief net model for visual area V2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS&apos;07)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-sixth International Conference on Machine Learning (ICML&apos;09)</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</editor>
		<meeting>the Twenty-sixth International Conference on Machine Learning (ICML&apos;09)<address><addrLine>Montreal (Qc), Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian inference in the visual cortex</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optical Society of America</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1434" to="1448" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>A</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">The cost of cortical computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lennie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="493" to="497" />
			<date type="published" when="2003-03-18">Mar 18 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Data Driven Object Segmentation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Levner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Alberta</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Learning nonlinear overcomplete representations for efficient coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 10 (NIPS&apos;97)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">An Introduction to Kolmogorov Complexity and Its Applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vitanyi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Second ed.</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="584" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies is not as difficult with NARX recurrent neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<idno>UMICAS-TR-95-78</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Institute for Advanced Computer Studies, University of Mariland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Training invariant support vector machines using selective sampling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loosli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large Scale Kernel Machines</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS&apos;08)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
	<note>NIPS Foundation</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">An interactive activation model of context effects in letter perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="page" from="375" to="407" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<title level="m">Explorations in parallel distributed processing</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><surname>Research Group</surname></persName>
		</author>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A logical calculus of ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of Mathematical Biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Unsupervised learning of image transformations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</title>
		<meeting>the Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Mendelson</surname></persName>
		</author>
		<title level="m">Introduction to Mathematical Logic</title>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="1997">4th ed. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Natural language processing with modular PDP networks and distributed lexicon</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="343" to="399" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS&apos;08)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Deep Learning from temporal coherence in video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</editor>
		<meeting>the 26th International Conference on Machine Learning<address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Smoothing techniques for macromolecular global optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear Optimization and Applications</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Pillo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Giannessi</surname></persName>
		</editor>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Evaluating probabilities under highdimensional latent variable models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS&apos;08)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Object class recognition and localization using sparse features with limited receptive fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Connectionist learning of belief networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="71" to="113" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">On Discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14 (NIPS&apos;01)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A hierarchical model of shape and appearance for human action classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</title>
		<meeting>the Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: a strategy employed by V1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997-12">December 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Computational complexity of neural networks: a survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Orponen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nordic Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="110" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Modeling image patches with a directed hierarchy of Markov random field</title>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS&apos;07)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1121" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A context-sensitive generalization of ICA</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Neural Information Processing</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</editor>
		<meeting><address><addrLine>Hong-Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Learning despite concept variation by finding structure in attribute-based data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rendell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Machine Learning (ICML&apos;96)</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Saitta</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Machine Learning (ICML&apos;96)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="391" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A day of great illumination: B. F. Skinner&apos;s discovery of shaping</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Experimental Analysis of Behavior</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Establishing good benchmarks and baselines for face recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008 Faces in &apos;Real-Life&apos; Images Workshop</title>
		<meeting><address><addrLine>Marseille France</addrLine></address></meeting>
		<imprint>
			<publisher>Erik Learned-Miller and Andras Ferencz and Frédéric Jurie</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">An Introduction to hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="page" from="257" to="285" />
			<date type="published" when="1986-01">january 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A unified energy-based framework for unsupervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)</title>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)<address><addrLine>San Juan, Porto Rico</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS&apos;07)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</title>
		<meeting>the Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">A sparse and locally shift invariant feature extractor applied to document images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR&apos;07)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1213" to="1217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19 (NIPS&apos;06)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of compact document representations with deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Research Group</surname></persName>
		</author>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)</title>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)<address><addrLine>San Juan, Porto Rico</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Workshop on Information Retrieval and applications of Graphical Models (SIGIR 2007)</title>
		<meeting>the 2007 Workshop on Information Retrieval and applications of Graphical Models (SIGIR 2007)<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Using deep belief nets to learn covariance kernels for Gaussian processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20 (NIPS&apos;07)</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;09)</title>
		<meeting>The Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>the Twenty-fourth International Conference on Machine Learning (ICML&apos;07)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Mean field theory for sigmoid belief networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="61" to="76" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Descartes&apos; rule of signs for radial basis function neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2997" to="3011" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Input space versus feature space in kernel-based methods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1000" to="1017" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Efficient training of large neural networks for language modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="3050" to="3064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Connectionist language modeling for large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Orlando, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main">Building continuous space language models for transcribing European languages</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="737" to="740" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Transformation invariant autoassociation with application to handwritten character recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milgram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 7 (NIPS&apos;94)</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Leen</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">A quantitative theory of immediate visual recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kouh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Knoblich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Neuroscience: Theoretical Insights into Brain Function</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="33" to="56" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Progress in Brain Research</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Learning continuous attractors in recurrent networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 10 (NIPS&apos;97)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="654" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition (ICDAR&apos;03)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">958</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Efficient pattern recognition using a new transformation distance</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 5 (NIPS&apos;92)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Giles</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Cowan</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Reinforcement today</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Skinner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="94" to="99" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Information processing in dynamical systems: Foundations of harmony theory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="194" to="281" />
		</imprint>
	</monogr>
	<note>ch. 6</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Describing visual scenes using transformed objects and parts</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="291" to="330" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Learning multilevel distributed representations for high-dimensional sequences</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)</title>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS&apos;07)<address><addrLine>San Juan, Porto Rico</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Factored conditional restricted Boltzmann machines for modeling motion style</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML&apos;09)</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</editor>
		<meeting>the 26th International Conference on Machine Learning (ICML&apos;09)</meeting>
		<imprint>
			<publisher>Montreal: Omnipress</publisher>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="1025" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19 (NIPS&apos;06)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Energy-based models for sparse overcomplete representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1235" to="1260" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 8 (NIPS&apos;95)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hasselmo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Training restricted Boltzmann machines using approximations to the likelihood gradient</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1064" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Using fast weights to improve persistent contrastive divergence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-sixth International Conference on Machine Learning (ICML&apos;09)</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</editor>
		<meeting>the Twenty-sixth International Conference on Machine Learning (ICML&apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Constituent parsing with incremental sigmoid belief networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 45th Meeting of Association for Computational Linguistics (ACL&apos;07)</title>
		<meeting>45th Meeting of Association for Computational Linguistics (ACL&apos;07)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="632" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Small codes and large databases for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR&apos;08)</title>
		<meeting>the Computer Vision and Pattern Recognition Conference (CVPR&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Many-layered learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Utgoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Stracuzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2497" to="2539" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Visualizing data using t-Sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">November 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Global data analysis and the fragmentation problem in decision tree induction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rendell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European Conference on Machine Learning (ECML&apos;97)</title>
		<meeting>the 9th European Conference on Machine Learning (ECML&apos;97)</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="312" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Learning kernel parameters by using class separability measure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">conjunction with Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>6th kernel machines workshop</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Unsupervised learning of models for recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Europ. Conf. Comp. Vis., ECCV2000</title>
		<meeting>6th Europ. Conf. Comp. Vis., ECCV2000<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main">The Complexity of Boolean Functions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wegener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Segmentation using eigenvectors: a unifying view</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE International Conference on Computer Vision (ICCV&apos;99)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Exponential family harmoniums with an application to information retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17 (NIPS&apos;04)</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1481" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Self-supervised boosting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15 (NIPS&apos;02)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="665" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Gaussian processes for regression</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems 8 (NIPS&apos;95)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hasselmo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="514" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="249" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Global continuation for distance geometry problems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Optimization</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="814" to="836" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Training connectionist models for the structured language model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;2003)</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Separating the polynomial-time hierarchy by oracles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the 26th Annual IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Navin Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 16 (NIPS&apos;03)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty International Conference on Machine Learning (ICML&apos;03)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</editor>
		<meeting>the Twenty International Conference on Machine Learning (ICML&apos;03)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty International Conference on Machine Learning (ICML&apos;03)</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</editor>
		<meeting>the Twenty International Conference on Machine Learning (ICML&apos;03)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
