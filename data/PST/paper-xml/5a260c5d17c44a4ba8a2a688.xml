<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A c c e p t e d M a n u s c r i p t A novel hybridization strategy for krill herd algorithm applied to clustering techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-06-02">June 2, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Laith</forename><forename type="middle">Mohammad</forename><surname>Abualigah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Sciences</orgName>
								<orgName type="institution">Universiti Sains Malaysia</orgName>
								<address>
									<postCode>11800</postCode>
									<settlement>Pinang</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahamad</forename><forename type="middle">Tajudin</forename><surname>Khader</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Sciences</orgName>
								<orgName type="institution">Universiti Sains Malaysia</orgName>
								<address>
									<postCode>11800</postCode>
									<settlement>Pinang</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammed</forename><forename type="middle">Azmi</forename><surname>Al-Betar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution" key="instit1">Al-Huson University College</orgName>
								<orgName type="institution" key="instit2">Al-Balqa Applied University</orgName>
								<address>
									<addrLine>Al-Huson</addrLine>
									<postBox>P.O. Box 50</postBox>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hossein</forename><surname>Amir</surname></persName>
						</author>
						<author>
							<persName><surname>Gandomi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">Stevens Institute of Technology</orgName>
								<address>
									<postCode>07030</postCode>
									<settlement>Hoboken</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A c c e p t e d M a n u s c r i p t A novel hybridization strategy for krill herd algorithm applied to clustering techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-06-02">June 2, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">877B83D9A07E44890B17237BC0FC5A3E</idno>
					<idno type="DOI">10.1016/j.asoc.2017.06.059</idno>
					<note type="submission">Preprint submitted to Journal of L A T E X Templates</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Krill herd algorithm</term>
					<term>Hybridization</term>
					<term>Global exploration</term>
					<term>Data clustering</term>
					<term>Text clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Krill herd (KH) is a stochastic nature-inspired optimization algorithm that has been successfully used to solve numerous complex optimization problems. This paper proposed a novel hybrid of KH algorithm with harmony search (HS) algorithm, namely, H-KHA, to improve the global (diversification) search ability. The enhancement includes adding global search operator (Improvise a new solution) of the HS algorithm to the KH algorithm for improving the exploration search ability by a new probability factor, namely, Distance factor, thereby moving krill individuals toward the best global solution. The effectiveness of the proposed H-KHA is tested on seven standard datasets from the UCI Machine Learning Repository that are commonly used in the domain of data clustering, also six common text datasets that are used in the domain of text document clustering. The experiments reveal that the proposed hybrid KHA with HS algorithm (H-KHA) enhanced the results in terms of accurate clusters and high convergence rate. Mostly, the performance of H-KHA is superior or at least highly competitive with the original KH algorithm, well-known clustering techniques and other comparative optimization algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A c c e p t e d M a n u s c r i p t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering is an important unsupervised learning mode that is used in numerous analysis applications to group a set of objects into a subset of coherent groups <ref type="bibr" target="#b0">[1]</ref>. Clustering algorithms are labeled into two classes: hierarchical clustering algorithms and partitional clustering algorithms <ref type="bibr" target="#b1">[2]</ref>. In the hierarchical 5 algorithms, each object can simultaneously belong to two clusters; each object distributes data objects within a series of partitions either from one cluster to another cluster, including all objects or vice versa. Hierarchical can be either agglomerative or divisive algorithms. Agglomerative algorithms begin with each object as a freelance cluster, merging them in large clusters respectively. By 10 contrast, divisive algorithms begin with all dataset objects and proceed to partition these objects into analogous clusters, respectively. Partition clustering procedures are the primary concern of this paper. This clustering technique attempts to partition a set of data objects into a subset of similar clusters by minimizing the objective function without the hierarchical structure <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr" target="#b14">15</ref> Data clustering is a common technique that is used for clustering similar or dissimilar data in a provided dataset based on the distance between data objects, thereby indicating that similar objects are partitioned into the same cluster and the dissimilar objects are partitioned into different clusters <ref type="bibr" target="#b2">[3]</ref>. The aim of the data clustering technique is to maximize the intra-cluster similarity and 20 minimize the inter-cluster similarity. Data clustering techniques have been used in many areas such as decision-making, data compression, machine learning, data mining, text document retrieval, text categorization, image segmentation, pattern classification, and large transactions for the customer such as marketing analysis, sales management, and document management <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. <ref type="bibr" target="#b24">25</ref> Text clustering techniques are used to partition a set of considerably large text documents into a subset of document-related clusters; such a process implies that documents within the same cluster share similar text features whereas the documents within different clusters share dissimilar text features <ref type="bibr" target="#b5">[6]</ref>. Subse-Page 3 of 43 A c c e p t e d M a n u s c r i p t quently, clustering technique is an important automatic task in text mining do-30 mains <ref type="bibr" target="#b6">[7]</ref>. In addition, the clustering technique is also an unsupervised learning technique because it deals with a set of documents without any foreknowledge of the class label <ref type="bibr" target="#b7">[8]</ref>. This technique is extensively used in numerous areas, such as text classification, information retrieval, search engine, image clustering, and others <ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr" target="#b34">35</ref> In the last few years, several metaheuristic algorithms have been proposed to play a pivotal role in solving several complex optimization problems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Natural metaheuristic algorithms have been used in numerous areas, such as text clustering, data mining, agriculture, computer vision, forecasting, medicine and biology, as well as economics and engineering <ref type="bibr" target="#b11">[12]</ref>. Recently, numerous meta-40 heuristic algorithms have been used to improve the data clustering technique, such as k-mean, artificial bee colony (ABC), bee colony optimization (BCO), tabu search approach (TS), particle swarm optimization (PSO), bat algorithm (BA), and harmony search (HS) algorithm <ref type="bibr" target="#b4">[5]</ref>.</p><p>Krill herd algorithm (KHA), which is inspired by the individual krill behav-45 iors of herding, is a novel evolutionary algorithm based on swarm intelligence and bacterial foraging algorithms. This algorithm was introduced by Gandomi and Alavi in 2012 <ref type="bibr" target="#b12">[13]</ref>. The algorithm has attracted researchers and has been extensively used in solving numerous optimization problems because of its simple idea and concept, easy implementation, and suitable behavior for clustering 50 techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>In this paper, a hybrid strategy is proposed to increase the diversity of the KH population by adding HS operator based on a new factor, namely, Distance factor, to make a fine-tune for each position before the updating. The aim of this hybrid strategy is increase its performance and convergence speed. KHA 55 was tested and was not good enough and that is the reason for hybridization <ref type="bibr" target="#b15">[16]</ref>. The HS serving (improvise operator) is used to produce a new solution iteratively. Eventually, according to the principle of KH and HS, we merge these two approaches to propose a new hybrid KHA for enhancing the search ability in order to obtain the optimal fitness function value. The proposed method is The overall structure of this paper is constructed as follows. Section 2 introduces additional related works in the domain of clustering technique using metaheuristic algorithms. Section 3 describes the proposed H-KHA. Section </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Additional related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>75</head><p>In recent years, many scientists who work on data clustering proposed several methods such as metaheuristic algorithms, which they frequently use for solving complex optimization problems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Data clustering is a common data analysis technique that is necessary for many areas <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The HS algorithm is proposed as a novel approach to address the problem of data clustering. The 80 proposed algorithm depends on two steps. The first steps explores the available search space of the HS algorithm to identify the optimal clusters of the centroid.</p><p>The optimal cluster of the centroid is after that evaluated using reformulated c-means as an objective function. In the second steps, the optimal clusters centers obtained are used the initial cluster centers for the c-means algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>85</head><p>The experiments were conducted using standard datasets. The results reveal that the proposed HS algorithm reduced the difficulty of selecting the initial clusters of the centroid <ref type="bibr" target="#b20">[21]</ref>. Four versions of hybridization ABC and PSO are used, which includes se-100 quence, parallel, sequence with an enlarged pheromone-particle table, and global best exchange approaches, was proposed to improve the data clustering technique. These hybrid versions were investigated by the data clustering problem.</p><p>The experimental results showed that the performances of the proposed methods are superior compared with the other standalone algorithms. These experiments 105 were conducted using standard datasets from the UCI Machine Learning Repository. Among the versions of hybridization, the sequence approach is superior to all other approaches because the growth diversifies during the generation of new solutions, thereby preventing being stuck into the local optimum <ref type="bibr" target="#b3">[4]</ref>.</p><p>A hybrid ABC algorithm is proposed to improve the data clustering tech-110 nique. The main goal of the hybrid ABC algorithm is to enhance the social learning between bees by adding the crossover operator of the genetic algorithm to an artificial bee colony. Ten benchmark functions and six datasets were used to investigate the data clustering technique from the machine learning repository (UCI). The results show that the proposed algorithm is better compared with 115 other algorithms and obtained better results in the data clustering technique <ref type="bibr" target="#b18">[19]</ref>.</p><p>One of the main difficulties in data clustering algorithms is the sensitivity of tuning the initial clusters centroid, which has long elicited the attention of clustering researchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. The author proposed a novel evolutionary Page 6 of 43 A c c e p t e d M a n u s c r i p t algorithm, namely, HS algorithm, as a new method aimed at solving the data clustering problem <ref type="bibr" target="#b20">[21]</ref>. The proposed algorithm comprises two stages. In the first stage, the HS investigates the search space of the provided dataset to show the optimal clusters centroid. The center of clusters obtained by the harmony search is evaluated using an objective function of the c-means algorithm. In the 125 second stage, the obtained optimal cluster centers are used as the initial cluster centers for the c-means. Experiments were conducted using standard benchmark data from the UCI Machine Learning Repository; the data showed that the proposed harmony search algorithm had reduced the challenges of choosing an initialization cluster centroid for the c-means clustering <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>130</head><p>An efficient hybrid optimization algorithm, which is called Tabu-KM, is proposed for solving data clustering problem. This algorithm concurrently gathers the optimization characteristic of tabu search and the local search ability of the k-mean algorithm. The proposed algorithm creates tabu space to escape the trap of the local optimal solution and find optimal solutions. The proposed 135 algorithm is also tested on several standard datasets and its performance is compared to popular algorithms in the domain of text clustering. The experimental results showed that the robustness and efficiency of the proposed algorithm are suitable for enhancing the data clustering problem <ref type="bibr" target="#b2">[3]</ref>.</p><p>A hybrid approach based on particle swarm optimization (PSO) and K-har-140 monic mean is proposed for the data clustering technique; the approach fully uses the merits of both algorithms. The proposed hybridization algorithm not only helps the K-harmonic mean clustering escape from local optima solution but also overcomes its limitations by tuning the convergence speed of the particle swarm optimization algorithm. The performance of the proposed hybridization 145 algorithm is investigated by using seven datasets that are used for the data clustering technique from the UCI machine learning repository and compared with swarm optimization and K-harmonic mean clustering standalone. Experimental results show the superiority of the hybridization technique <ref type="bibr" target="#b21">[22]</ref>.  A new hybrid strategy, namely, cuckoo search and krill herd (CSKH), is proposed to make KH more efficient <ref type="bibr" target="#b23">[24]</ref>. The CSKH includes krill updating (KU) and abandoning (KA) operator introduced from CS through the process when the krill position was updating. Thus, as to significantly improve its performance and reliability dealing with function optimization problems. The KU operator 165 encourages the exploitation search and allows the krill individuals perform a careful search, while KA operator is applied to improve the exploration search of the CSKH further in place of the poor krill by the end of each iteration. The performance of this strategy is tested by 14 standard optimization functions, and the results reveal that the proposed hybrid strategy of CSKH algorithms 170 is more powerful and efficient than the basic KH and the other comparative methods.</p><p>Over the past few years, a large proportion of researchers applied metaheuristic algorithms to solve several optimization problems. However, a major drawback of these algorithms is that it provides a good exploration of the search 175 space at the cost of exploitation <ref type="bibr" target="#b24">[25]</ref>. The best results are obtained to solve these problems were by applying the hybrid strategies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>. However, these reasons can be justified by the no free lunch theorem.</p><p>A c c e p t e d M a n u s c r i p t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed hybrid krill herd algorithm</head><p>KH is a recent metaheuristic population-based algorithm. The inspiration 180 of the algorithm is the herding behavior of krill individuals when looking for the nearest food; krill herd with high density based on communication with each other <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref>. In this paper, the H-KHA is used for enhancing the clustering problem by testing GA, PSO, k-mean, k-mean++, HS, KH and others.</p><p>The researchers proposed novel H-KHA to solve the problem of the basic KH 185 algorithm. The proposed H-KHA algorithm has three stages as follows:</p><p>1. Motion calculation.</p><p>2. Genetic operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Improvising a new solution. 2. Foraging action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion calculation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Physical diffusion.</head><p>195</p><formula xml:id="formula_0">dx i dt = N i + F i + D i ,<label>(1)</label></formula><p>where for krill i, N i is first part, which indicates to the motion induced by other krill individuals, F i indicates the forging motion, and D i indicates the physical diffusion of the i th krill individual <ref type="bibr" target="#b12">[13]</ref>. The H-KHA factors discussed below. a target effect of the density of the individuals, and a repulsive effect of the individuals <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref>. Eq. ( <ref type="formula">2</ref>) is used to calculate the motion induced by other 205 krill individuals.</p><formula xml:id="formula_1">N new i = N max α i + ω n N old i (2)</formula><p>where,</p><formula xml:id="formula_2">α i = α local i + α target i (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>N max is the parameter used to tune the part of the induced motion, ω n is the array of random values in the range [0, 1] and N old is the current motion induced. For More details refer <ref type="bibr" target="#b14">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Foraging motion</head><p>This factor has two affected parameters: the first is the food location and the second is the old food location. This action can be expressed for the i th krill individual by Eq. (4).</p><formula xml:id="formula_4">F i = V f β i + ω f F old i (4)</formula><p>where,</p><formula xml:id="formula_5">215 β i = β f ood i + β best i ,<label>(5)</label></formula><p>V f is the forging speed, ω f is the intra weight used to balance the local exploitation and global exploration for each individual, β f ood i is the food attraction, β best i is the best food attraction so far. For More details refer <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_6">x f ood = N i=1 1 Ki x i N i=1 1 Ki<label>(6)</label></formula><p>Page 10 of 43</p><p>A c c e p t e d M a n u s c r i p t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Physical diffusion</head><p>In this factor, the krill individual is estimated as the random process which 220 used two terms to express the physical diffusion: the first is the maximum diffusion speed and the second is the random directional vector <ref type="bibr" target="#b12">[13]</ref>. The physical diffusion is determined by Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_7">D i = D max 1 - I I max δ<label>(7)</label></formula><p>where D max is the maximum diffusion speed and δ is the random values of the vector which has arrays containing random values between [-1, 1]. This 225 action decreased the speed value of the krill individual <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Motion process of the KH algorithm</head><p>The motion-inducing and foraging motion contained two local and two global strategies <ref type="bibr" target="#b28">[29]</ref>. These strategies work in parallel to obtain a powerful algorithm.</p><p>The physical diffusion generates random vectors <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. KH algorithm param-230 eters are effective during the algorithm acts. The positions of krill individuals are updated in each iteration using the Langranging model by Eq. ( <ref type="formula" target="#formula_8">8</ref>).</p><formula xml:id="formula_8">x i (I + 1) = x i (I) + ∆t dx i dt<label>(8)</label></formula><p>where,</p><formula xml:id="formula_9">∆t = C t N j=1 (U B j -LB j )<label>(9)</label></formula><p>x i is the position i in the search space, (I + 1) is the next iteration, ∆t is an important and more sensitive constant computed by Eq. ( <ref type="formula" target="#formula_9">9</ref>); N represents the 235 total number of variables, the lower bounds LB j , and the upper bounds U B j of the i th variables (J = 1, 2, ...., N ), respectively. C t is a constant value between This algorithm has been successfully applied to solve numerous optimization problems, such as numerical function optimization <ref type="bibr" target="#b13">[14]</ref>, text clustering <ref type="bibr" target="#b30">[31]</ref>, and data clustering problems <ref type="bibr" target="#b31">[32]</ref>.</p><p>HS operator is based on the probability of the Distance factor (Def). It is 250 determined by using the distance between each position with the best-achieved fitness function value. Def performs a fine-tune for each position towards the best global solution based on their objective functions using Eq. ( <ref type="formula">10</ref>) to enhance the exploration search ability. If the rand less than probability value of (Def) that means the current position could be improved using Algorithm <ref type="bibr" target="#b0">(1)</ref>. This</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>255</head><p>factor is developed as an operator improvement in order to adjust the solutions to prevent the premature convergence during the global (diversification) search.</p><p>Note, Def value is between (0, 1) because the left part value multiplied by two values, each one less than one and the right part is the objective function of the current position which is between (0, 1).</p><p>260</p><formula xml:id="formula_10">Def (i, j) =   X best * ( 1 S S j=1 |X j -X best |)   + K j (10)</formula><p>Where, X best is the fitness function of the best solution, S is the number of all solutions, X j is the solution number j and K i,j is the objective function of the position number j. if rand ≤ Def i,j then 4:</p><p>if rand [0,1] ≤ HM CR then 5:</p><formula xml:id="formula_11">x i,j =KHM [i][j] where i ∼ U (1, 2, ...., S) 6:</formula><p>if rand [0,1] ≤ P AR then 7:</p><p>x i,j = x i,j ± rand× bw, where rε U(0,1) and bw 8: 9:</p><formula xml:id="formula_12">else x i,j = LBj + rand × (U Bj -LBj) 10:</formula><p>end if</p><formula xml:id="formula_13">11:</formula><p>end if</p><formula xml:id="formula_14">12:</formula><p>end if 13: end for fitness is evaluated. The selection technique is employed according to Def values for obtaining an optimal krill positions. Thus, after computing the Def value of j th krill position. This value is used to assist the decision making either to update the j th krill position by HS operator or let this position for the KH motion calculation. We conclude that if the objective function of j th krill 270 position is high the Def value is increased, then the HS operator is applied.</p><p>Otherwise, the old position is preserved in order to keep the current best position in the solution to the next iteration. Finally, HS operator encourages the good solutions (Solutions that have high fitness values) that made by KH motion calculation in order to improve the convergence behavior by preventing the 275 premature convergence.</p><p>One of the main motivations to contribute in hybrid KH with HS operator is the HS algorithm one of the most powerful algorithms successfully utilized to solve the text clustering technique <ref type="bibr" target="#b9">[10]</ref>. Hence, HS works depend on the HS operator, which is the main operator used to improve the solutions by ex-  <ref type="formula" target="#formula_15">11</ref>) and ( <ref type="formula" target="#formula_16">12</ref>). This operator is applied in krill herd memory (KHM) in order to improve the exploration search ability of the H-KHA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P AR(I)</head><formula xml:id="formula_15">= P AR min + P AR max -P AR min I max * I,<label>(11)</label></formula><p>where,</p><formula xml:id="formula_16">290 bw(I) = bw max exp   In bwmin bwmax I max   * I,<label>(12)</label></formula><p>Where, bw I is the bandwidth for iteration I and bw min and bw max are minimum and maximum adjusting memory consideration, respectively, P AR(I) is the pitch adjusting rate for for iteration I, P AR min and P AR max are minimum and maximum pitch adjustment, respectively, I is the number of the current iteration, and I max is the max number of iterations Algorithm 2 Hybrid-krill herd algorithm (H-KHA)</p><formula xml:id="formula_17">1: Initialization of krill parameters: N max , D max , V f , ω n etc. 2: for i =1 to S do 3:</formula><p>for j =1 to n do 4:</p><formula xml:id="formula_18">x i,j = LB i + (U B i -LB i ) * U (0, 1)</formula><p>Initialization of krill memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>end for 6:</p><p>Evaluate the krill (i) 7: end for 8: Sort the krill and find x best , where best ∈ (1, 2, ..., S) 9: while I = I M ax do 10:</p><formula xml:id="formula_19">for i =1 to S do 11:</formula><p>Perform the three motion calculation using Eq. (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>x i (I + 1) = x i (I) + δt dxi dt 13:</p><p>Fine-tune x i + 1 by using KH operators: Crossover and mutation. if rand [0,1] ≤ HM CR then 21:</p><p>x i,j =KHM [i][j] where i ∼ U (1, 2, ...., S)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>if rand [0,1] ≤ P AR then 23:</p><p>x i,j = x i,j ± rand× bw, where rε U(0,1) and bw 24:</p><p>25:  </p><formula xml:id="formula_20">else x i,j = LB j + rand × (U B j -LB j )</formula><formula xml:id="formula_21">I = I + 1 32: end while 33: Return X best</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Data clustering using H-KHA</head><p>Data clustering is a popular technique that is used to partition a set of data objects and statistical data analysis <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>, in which a cluster of data objects is distributed in such a way that the data objects within the same clusters are similar and the data objects in different clusters are dissimilar <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>. The 330 similarity between each data object with the cluster centroid is determined by a distance metric <ref type="bibr" target="#b13">[14]</ref>.</p><p>Page 16 of 43 number i and t is the length of each object <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. The aim of clustering algorithm is to find a subset of K, where each cluster belongs to one centroid 340 as C = c 1 , c 2 , ..., c k , ..., c K and c k = Ø. The measurement of these similarities are evaluated by certain optimization criterions, particularly distance measure and squared error function <ref type="bibr" target="#b13">[14]</ref>, which have been calculated by Eqs. ( <ref type="formula" target="#formula_22">13</ref>) and <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_22">F F = n i=1 K j=1 min(Des(d i , c j )),<label>(13)</label></formula><p>where c j represents a j th cluster center, d j represents a j th data object, and</p><formula xml:id="formula_23">345</formula><p>Des is the distance measure between the object d i and the cluster center c j .</p><p>This criterion is used as the objective function value to evaluate the algorithm solution. Different distance measurements have been used in the domain of data clustering techniques, such as Euclidean, Manhattan, Minkowski, Cosine, Pearson correlation coefficient, and Jaccard coefficient measures <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b13">14]</ref>. In this 350 paper, Euclidean distance is used as distance measure from numerous distance metrics used in the literature, which is defined by Eq. <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_24">Des(d i , c j ) = t j=1 (d 1j , c 2j ) 2 , (<label>14</label></formula><formula xml:id="formula_25">)</formula><p>where Des(d i , c j ) is the distance measure between the document i and the cluster j, d 1j represents the term j in document 1, c 2j represents the term j in cluster centroid 2, and c 1 the cluster center of cluster 1 <ref type="bibr" target="#b13">[14]</ref>, which is calculated Page 17 of 43</p><p>A c c e p t e d M a n u s c r i p t by Eq. <ref type="bibr" target="#b17">(18)</ref>.</p><formula xml:id="formula_26">c j = 1 n j di cj d i ,<label>(15)</label></formula><p>where d i represents the object i, c j represents the cluster centroid of cluster j, and n j is the total number of objects in cluster j. The centroid of each cluster is repeatedly computed based on data objects assigned to the cluster.</p><p>This process proceed continuously until the maximum number of iterations is 360 reached (stopping criteria) <ref type="bibr" target="#b1">[2]</ref>. Note, each cluster has one centroid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental results of H-KHA for data clustering</head><p>This section applies seven standard data benchmark datasets to validate the proposed H-KHA method in comparison with K-mean <ref type="bibr" target="#b13">[14]</ref>, K-mean++ <ref type="bibr" target="#b13">[14]</ref>, Spectral clustering <ref type="bibr" target="#b38">[38]</ref>, Agglomerative clustering <ref type="bibr" target="#b39">[39]</ref>, DBSCAN <ref type="bibr" target="#b40">[40]</ref>, GA 365 <ref type="bibr" target="#b41">[41]</ref>, HS <ref type="bibr" target="#b20">[21]</ref>, PSO <ref type="bibr" target="#b42">[42]</ref>, CS <ref type="bibr" target="#b2">[3]</ref>, ABC <ref type="bibr" target="#b18">[19]</ref> and other hybrid algorithms <ref type="bibr" target="#b13">[14]</ref>.</p><p>These comparative methods are programmed by the authors as discussed in their publications. A more detailed explanation is presented as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Benchmark Datasets</head><p>In this paper, we conducted the results using seven datasets provided by the 370 Machine Learning Repository (UCI) of the University of California N max , HM CR, and etc <ref type="bibr" target="#b12">[13]</ref>. These parameters are recorded in Table <ref type="table" target="#tab_2">2</ref>.  pared with other popular algorithms using two criteria include <ref type="bibr" target="#b0">(1)</ref> The sum of the intra-cluster distances is considered as an internal quality measure. The distance value between each object and the cluster centroid of the corresponding cluster is computed, as defined in Eq. ( <ref type="formula" target="#formula_22">13</ref>). A higher quality data clustering 390 provides a small fitness function <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">44]</ref>. (2) Error Rate (ER) value is an external quality measure. The percentage of misplaced objects on the overall number of objects <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">45]</ref>, as shown in Eq. ( <ref type="formula" target="#formula_27">16</ref>).</p><formula xml:id="formula_27">ER = numberof misplacedobjects sizeof testdataset * 100, (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>ER measure is assigned a class label and compared with the desired class label. The pattern is distributed as incorrect partitioning if these measures are 395 dissimilar. The measure is calculated for all data objects in the provided dataset and the total incorrect number of partitioning pattern is a percentage of the size of all data objects in the dataset. A summary of the ER obtained by the data clustering algorithms is provided in Table <ref type="table" target="#tab_4">3</ref>. The values reported are worst, average, and best solutions over 20 independent runs <ref type="bibr" target="#b13">[14]</ref>. The experimental 400 results are provided in Table <ref type="table" target="#tab_4">3</ref>, which shows that the proposed hybrid H-KHA algorithm obtains near optimal solutions compared with the comparative algorithms. The proposed H-KHA achieves better results for almost all datasets with small final rankings. Note, the statistical ranking of each comparative algorithm is based on the average ER among the seven datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>405</head><p>Table <ref type="table" target="#tab_4">3</ref> shows the ER of using five clustering techniques and seven meta-       This section explains the experimental results to empirically investigate the validation of the proposed H-KHA for data clustering problem and compare with the successful comparative algorithms. We concluded from Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_8">475</ref> 4 that the proposed H-KHA obtained the best performance according to error </p><formula xml:id="formula_29">heuristic</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Text clustering solution representation</head><p>The text clustering solution is represented by a vector X= (x 1 , x 2 , ..., x i , ..., x n ),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>505</head><p>where X represents one solution and x i is the value of the position i, this value represents that the document number i belong to which cluster. The available range for each document is [1, 2, ..., K], where K is the number of all clusters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">51]</ref>.  defined below <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b53">53]</ref>.</p><formula xml:id="formula_30">Cos(d 1 , c 2 ) = t j=1 w(t j , d 1 ) × w(t j , c 2 ) t j=1 w(t j , d 1 ) 2 t j=1 j , c 2 ) 2 ,<label>(17)</label></formula><p>where w(t j , d 1 ) is the weight of the term j in document 1, and w(t j , c 2 ) is 520 the weight of the term j in the centroid of cluster 2. This measure returns to one if the document and the centroid are conformable and returns to zero if the document and the centroid are different.  <ref type="formula" target="#formula_31">18</ref>) <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Fitness function</head><formula xml:id="formula_31">c kj = n i=1 (A kj )d j r i ,<label>(18)</label></formula><p>where d i is the document number i that belongs to centroid c j of the cluster j;</p><p>A kj represents that document k, which belongs to cluster j and r i , is the number of documents in each cluster. The cosine similarity is used as an objective function to evaluate each solution position. The fitness function for each solution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>535</head><p>A c c e p t e d M a n u s c r i p t in the KH memory is determined by the average similarity of documents to the cluster centroid (ASDC) as represented by Eq. <ref type="bibr" target="#b18">(19)</ref>.</p><formula xml:id="formula_32">ASDC = k i=1 ( r i j=1 Cos(c i ,d j ) r i ) K ,<label>(19)</label></formula><p>.</p><p>where K is the number of all clusters, r i is the number of documents in cluster number i, and Cos(c i , d i ) is the similarity measure between document 540 number i and cluster centroid number i. Table <ref type="table" target="#tab_8">7</ref> shows six standard benchmark text datasets that are used to analyze and compare the performance of the proposed hybrid algorithm <ref type="bibr" target="#b54">[54]</ref>. Text clustering standard datasets<ref type="foot" target="#foot_3">3</ref> are available at Laboratory of Computational Intelligence (LABIC) by numerical form after the terms extraction <ref type="bibr" target="#b7">[8]</ref>. More than 20 experimental runs were performed for statistical comparisons. This num-  The main characteristics of these text document datasets are as follows. The first dataset (DS1), Classic4, contains randomly 500 documents on two topics: Precision and recall measures 585</p><p>The precision and recall measurements are used together to calculate the F-measure score for cluster j and class i <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">57,</ref><ref type="bibr" target="#b49">49]</ref>. Eq. ( <ref type="formula" target="#formula_33">20</ref>) and ( <ref type="formula" target="#formula_34">21</ref>) are used to calculate the precision and recall measures, respectively.</p><formula xml:id="formula_33">P (i, j) = n i,j n j ,<label>(20)</label></formula><formula xml:id="formula_34">R(i, j) = n i,j n i ,<label>(21)</label></formula><p>where n ij is the number of true documents of class i in cluster j, n j is the number of all documents of cluster j, and n i is the number of all documents of 590 class i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F-measure evaluation</head><p>F-measure is a common external measurement that is used in particular on the text clustering domain. F-measure calculates the percentage of the matched clusters depending on Precision and Recall measurements by Eq. ( <ref type="formula" target="#formula_36">23</ref>) <ref type="bibr">[57]</ref>.</p><p>595</p><formula xml:id="formula_35">F (i, j) = 2 × P (i, j) × R(i, j) P (i, j) + R(i, j) ,<label>(22)</label></formula><p>where P (i, j) is the precision of the true documents of class i in cluster j, R(i, j) is the recall of the class i in cluster j, and F-measure for all clusters is A c c e p t e d M a n u s c r i p t calculated by Eq. ( <ref type="formula" target="#formula_36">23</ref>) according to the number of all documents n.</p><formula xml:id="formula_36">F = j n j n max i {n(i, j)},<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy evaluation</head><p>The accuracy is one of the popular external measurements that is used to 600 precisely compute the percentage of true assigned text documents to each cluster by using Eq. ( <ref type="formula" target="#formula_37">24</ref>) <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_37">AC = 1 n K i=1 n i,i<label>(24)</label></formula><p>Where, n i,j is the number of documents of the class i in cluster i, n is the number of all documents and K is the number of all clusters are given in the dataset. Table <ref type="table" target="#tab_9">8</ref> shows that the performance of the H-KHA is better than the original KH algorithm and the other comparative algorithms with regard to the five clustering measures include ASDC, Accuracy, Precision, Recall and F-measure.</p><p>The proposed H-KHA realizes the best performance as regards the average F-610 measure. In particular, The performance of H-KHA according to the F-measure is better than the other comparative algorithms in all text datasets. The proposed algorithm also performs better than the original KH algorithm over the whole datasets.</p><p>Table <ref type="table" target="#tab_9">8</ref> shows the performance of the H-KHA based on the quality of clusters        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>A</head><label></label><figDesc>c c e p t e d M a n u s c r i p t evaluated on several common standard benchmark datasets which are used in domains of data and text clustering. The results are compared with the other comparative techniques and algorithms. Experimental results illustrate that the hybrid KHA (H-KHA) performs more accurately and efficiently than well-known clustering techniques (i.e., K-mean, k-mean++, Spectral clustering, agglomer-65 ative clustering, and DBSCAN), original and hybrid optimization algorithms (i.e., GA, HS, PSO, ABC and CS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>70 4 explains</head><label>4</label><figDesc>the hybridization architecture of KH algorithm and HS algorithm. Sections 5 and 6 explain the data clustering and text clustering techniques, respectively, along with their results. Finally, Section 7 provides the conclusion and future work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>An efficient method is introduced for improving the data clustering technique Page 5 of 43 A c c e p t e d M a n u s c r i p t based on cuckoo optimization algorithm (COA) and fuzzy cuckoo optimization 90 algorithm (FCOA). The COA is inspired by the natural life of a cuckoo bird to solve optimization problems. The authors have used COA to cluster a set of data objects into a subset of clusters. The fuzzy logic technique obtained optimal results. This algorithm generates a random solution to the cuckoo population that calculates the cost function for each solution. Finally, fuzzy 95 logic aims to obtain the optimal solution. The performance of the proposed algorithms is evaluated and compared with other algorithms, thereby showing that the proposed algorithm has improved the performance of data clustering [17].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A</head><label></label><figDesc>new hybrid algorithm, namely, differential evolution KH (DEKH), are Page 7 of 43 A c c e p t e d M a n u s c r i p t of the KH algorithm [23]. This improvement has been made by appending a hybrid differential evolution (HDE) into the KH to deal with complex optimization problems more efficiently. HDA inspires the intensification and encourages the krill to perform the intensification search within the defined region. Experi-155 ments were conducted on 26 optimization functions. The results clearly revealed that the proposed DEKH are adequate to find the accurate solution than the KH and the other comparative methods. As well, the robustness of the DEKH method and the control of the initial population volume on convergence and effectiveness are tested by a set of experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>160</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>190H-</head><label></label><figDesc>KH algorithm follows the Lagrangian model for efficacious search, which is calculated by Eq. (1) based on three factors as follows [27, 23]: 1. Movement motivated other individuals krill.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3. 1 . 1 .</head><label>11</label><figDesc>Movement induced by other krill individuals 200 Based on certain theoretical arguments, each krill individual attempts to maintain a high density and closeness to the nearest food. The direction of the induced motion is derived from the local effect of each solution density, A c c e p t e d M a n u s c r i p t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>210</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3 . 2 .</head><label>32</label><figDesc>Reproduction procedures are incorporated into H-KHA algorithm to im-240 prove its performance. Crossover and mutation operators are inspired from the classical differential evolutionary algorithm. For More details refer<ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. Improvise a new solution of H-KHA Improvising a new solution is the most influential part in HS algorithm, it is used to generate a new solution by global exploration strategy. HS is a stochas-245 tic population-based metaheuristic approach that was introduced in 2001<ref type="bibr" target="#b29">[30]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Hof 43 A c c e p t e d M a n u s c r i p t Algorithm 1 a new solution 1 :</head><label>4311</label><figDesc>-KHA is going to make logical decisions according to the objective function of each krill individual. After obtaining the new position of the krill, solution Page 12 Improvising Improvise a new solution 2: for each j ∈ [1, n] do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>280</head><label></label><figDesc>ploration search. In terms of the KH, it is the unique metaheuristic algorithm owns behavior similar to the clustering technique, so we hybrid these two factors to obtain an effective hybrid algorithm in comparison with other original algorithms, well-known clustering techniques and the other comparative algorithms. Improvise a new solution: Each krill position is updated based on three 285 Page 13 of 43 A c c e p t e d M a n u s c r i p t rules after got permission by Def . These rules are: memory consideration, pitch adjustment, and random selection using Eqs. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>295 4 .</head><label>4</label><figDesc>H-KHAThe early sections represent the introduction of H-KHA. In this section, we explain the integration of two approaches to made the proposed hybrid krill herd with harmony search algorithm (H-KHA) as shown in Algorithm 2. It modified the KH solutions with poor fitness function to increase the diversity of the KH 300 solutions for improving the search capability and speed up the convergence to the global optimal. Sometimes, KH gets tripped in the local search or premature convergence in the global search because of the poor exploration. A novel algorithm, namely, H-KHA, which combines a KHA and HS operator is proposed for solve the 305 KH weakness. The main improvement in H-KHA is to add HS operator (i.e., Improvise a new solution). Global search strategy in the HS algorithm (i.e., improvising a new solution) is combined into KH algorithm in order to enable the proposed H-KHA to reach the promising search region. H-KHA is performed Page 14 of 43 A c c e p t e d M a n u s c r i p t to change each krill position into a new proper position by global strategy. After 310 the local search stage, global search is executed with the improvised new solution to obtain the global best solution, then add this solution to the proposed H-KH memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Replace the worst krill with the best krill 17: Improvise a new solution 18: for each j ∈ [1, n] do 19:if rand ≤ Def i,j then 20:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A flowchart of the hybrid krill herd algorithm (H-KHA)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>A c c e p t e d M a n u s c r i p t 5.1. Data clustering formulation Data clustering technique is the process of partitioning the set of data objects into a subset of K clusters based on certain distance measure [2, 36]. Let 335 D be a set of n data objects D = d 1 , d 2 , ..., d i , ..., d n to be distributed over K clusters and each data object d i , i = 1....n is represented as vector d i = d i1 , d i2 , ..., d ij , ..., d it , where d ij represents j th dimension value of the data object</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>A c c e p t e d M a n u s c r i p t</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>A c c e p t e d M a n u s c r i p t</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>A c c e p t e d M a n u s c r i p t</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>. 6 . 6 . 1 .</head><label>661</label><figDesc>rate and objective function measures compared with the other comparative algorithms. The H-KHA performed better on all seven data clustering datasets compared with the clustering techniques and the comparative algorithms. A proper balance between exploitation and exploration improves the performance 480 of the proposed hybrid KH algorithms. It passes by obtaining the best results on almost all dataset and in comparison with all the other comparative algorithms to prove that the proposed hybrid strategy (H-KHA) is very effective to solve complex optimization problem by Adding new operators (HS operator) to the main structure of the KHA using a new probability value that reproduction by 485 Def Text clustering using H-KHA The text clustering technique aims to generate optimal text document clusters that contain relevant (similar) documents. This technique is based on partitioning a set of text documents into a subset of related clusters, in which Page 25 of 43 A c c e p t e d M a n u s c r i p t each cluster contains a set of similar text documents, whereas different clusters contain dissimilar text documents [46, 47, 18]. Text clustering problem descriptions and formulations A combination of text documents D is partitioned into K clusters, where D refers to a vector of documents D = (d 1 , d 2 , d i , ....., d n ), d 1 is the document 495 number 1, i is the document number i, and n is the number of all documents in D<ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49]</ref>. Each cluster has a cluster centroid c k , which stands as a vector of terms weight c k = (c k1 , c k2 , ..., c kj , ..., c kt ), where c k is the k th cluster centroid, c k1 is the value of position 1 in the centroid of cluster k, and t is the number of all unique centroid terms (length). Note that as previously mentioned, each 500 cluster centroid represents as a vector such as any document with the same dimension. The similarity measure is used to assign each text document to the similar cluster centroid<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">50]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Fig 2  shows the representation of the document-clustering solutions in H-KHA. In this case, cluster 1 contains three documents (i.e., 3, 4, and 8), clus-510 ter 2 contains two documents (i.e., 6 and 9), cluster 4 contains four documents (i.e., 1, 2, 5, and 10), and cluster 4 contains one document (i.e., 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 2 :Page 26 of 43 A c c e p t e d M a n u s c r i p t 6 . 3 .</head><label>24363</label><figDesc>Figure 2: Representation of the text clustering solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fitness</head><label></label><figDesc>function (FF) is calculated for evaluating each solution based on 525 its current position using Eq. 19, sorting it in an ascending order. KH memory contains several solutions to solve the text clustering problem. Each solution in the KH memory illustrates the candidate solution to solve the problem of documents clustering. Each solution has a set of K centroid C = (c 1 , c 2 , ...., c k , c K ), where c k is the centroid of cluster k, which represents a vector 530 c k = (c k1 , c k2 , ..., c kj , ..., c kt ) and is computed by Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>6. 5 .</head><label>5</label><figDesc>Experimental results of H-KHA for text document clustering Also, the experiments of this section are implemented in a Windows 7 environment using MATLAB (7.10.0) computer programming with different CPU and RAM capabilities using different standard benchmark text datasets in the 545 text clustering domain. The proceeding subsections explain datasets in detail, illustrate the evaluation criteria, and present the results of experiments and discussion. Note, clustering techniques 2 are available at Scikit-Learn Machine Learning in Python. 6.5.1. Standard text document datasets 550</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>555 ber, which was selected based on the literature, can sufficiently validate the proposed method. Local-based algorithms for clustering technique run 100 iterations in each run time. Experimentally, 100 iterations are adequate for the convergence of intensification search algorithm and 1000 iterations are adequate A c c e p t e d M a n u s c r i p t for the convergence of diversification search algorithm for clustering techniques 560 [55].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Page 29 of 43 A c c e p t e d M a n u s c r i p t 6 . 5 . 2 .</head><label>43652</label><figDesc>cacm and cran. The second dataset (DS2), Classic4, contains randomly 2000 documents on four topics: cacm, cisi, med and cran. The third dataset (DS3),565Reuters21578, contains randomly 3000 documents on seven topics: gold, tin, sunseed, nkr, lei, hog and citruspulp. The fourth dataset (DS4), 20Newsgroup, contains randomly 5000 documents on nnine topics: comp-windows-x, rec-autos, talk-politics-misc, comp-sys-mac-hardware, talk-religion-misc, misc-forsale, scicrypt, sci-med and rec-motorcycles. The fifth dataset (DS5), Reuters21578, con-570 tains randomly 2000 documents on ten topics: gold , tin, sunseed , nkr, lei, hog, citruspulp, gas, peseta andnzdlr. The sixth dataset (DS6), 20Newsgroup, contains randomly 2000 documents on twenty topics: comp-windows-x, rec-autos, talk-politics-misc, comp-sys-mac-hardware, talk-religion-misc, misc-forsale, scicrypt, sci-med, rec-motorcycles, alt-atheism, sci-electronics, soc-religion-christian, 575 rec-sport-baseball, talk-politics-mideast, rec-sport-hockey, sci-space, comp-sysibm-pc-hardware, comp-os-ms-windows-misc, comp-graphics and talk-politicguns. Evaluation measures Four external evaluation measures were conducted for comparative evalu-580 ations: Accuracy measure (Ac), Precision measure (P ), Recall measure (R), and F-measure (F ). These measures are popular evaluation criteria used to accurately identify the clusters and compare the different clustering methods [55, 56, 51].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>605 6 . 5 . 3 .</head><label>653</label><figDesc>Text clustering results and discussion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>615</head><label></label><figDesc>using six benchmark standard text datasets. The proposed algorithm apparently performed very well and exceeded the popular algorithms. The proposed algorithm also scored a better performance based on Accuracy Precision, Recall, and F-measure as an external measurement in two dataset (i.e., DS1, and DS2) and almost it obtained near optimal clusters in the other datasets (i.e., Page 31 of 43 A c c e p t e d M a n u s c r i p t improved in comparison with the original KH algorithm. Notably, all experiments show that the proposed H-KHA obtained better results in comparison with the comparative algorithms over all datasets, which can be attributed to the high quality of clusters. The performance measures (i.e., precision, recall, 625 accuracy and F-measure) are obtained by using the proposed H-KHA on all the datasets and comparable with that obtained from the comparative algorithms in all datasets. A novel hybrid algorithm, namely, H-KHA with improved global search ability, which combines the extended search ability of HS operator and uses the 630 search ability of the hybrid computation algorithm and the global increase capacity of the KH algorithm, is proposed. Besides, a new position update approach is proposed to enhance the ability of the search space. In conclusion, H-KHA obtained the best results for all the tested datasets. The statistical analysis is done based on F-measure evaluation. The average 635 rankings of the clustering algorithms are reported in Table 8. The proposed H-KHA is ranked the highest one, which is followed by agglomerative clustering, H-CS, H-ABC, H-PSO, Spectral, KHA, DBSCAN, PSO, GA, CS, HS, k-mean, H-GA, k-mean++ and ABC, among the datasets. This section demonstrates the experimental results to empirically investigate 640 the effectiveness of the proposed H-KHA and compare them with the successful comparative clustering algorithms. We concluded that the proposed H-KHA obtained the best performance according to F-measure evaluation compared with the other comparative algorithms and the clustering techniques. The H-KHA performed better on all seven data clustering datasets, also on all six text 645 clustering datasets compared with the comparative algorithms consistent with the all evaluation measures. A proper balance between exploitation (intensification) and exploration (diversification) search improves the performance of the proposed hybrid KH algorithms. It balances these basic components in H-KHA owing to the combination that added the serving of the HS algorithm after the Page 32 of 43 A c c e p t e d M a n u s c r i p t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Page 33 of 43 A c c e p t e d M a n u s c r i p t 6 . 5 . 4 .</head><label>43654</label><figDesc>lowest ranked algorithm is the best one. Convergence analysis One of the strong criteria for evaluating metaheuristic algorithms is the convergence rate to an optimal solution. The criterion for evaluating the clustering algorithm is their convergence rate to find the optimal solution accourding to 655 the ASDC measure. Fig.3shows the convergence behaviors of H-KHA and the comparative optimization algorithms (i.e., GA, HS, PSO, CS, ABC, KHA, H-GA, H-PSO, H-CS, H-ABC and H-KHA) on the text document dataset. The researchers conducted 20 independent runs for each dataset with randomly generated initializations. Thereafter, the average value is calculated based on the 660 convergence behavior of each algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Fig. 3</head><label>3</label><figDesc>Fig.3clearly shows that the convergence of the original KH algorithm compared with H-KHA is faster because the KH algorithm may be stuck in the local optima and got premature convergence. However, H-KHA is more efficient than the original KH algorithm with regard to the algorithm performance and exe-665</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convergence behaviour of text clustering optimization algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>A c c e p t e d M a n u s c r i p t 7 .</head><label>7</label><figDesc>ConclusionMetaheuristic optimization algorithms have been a prominent focus of re-680 search in solving complex optimization problems. Krill herd (KH) is a new optimization algorithm for solving many difficult global optimization problems. In this paper, a hybrid krill herd algorithm is proposed to solve the clustering problems. The original krill herd was quickly saturated and subsequently trapped in the local optimum. An enhanced krill herd was invented by introduc-685 ing the global exploration operator of harmony search to relieve the premature convergence of krill herd. Using this hybridization, H-KHA quickly converges to optimal solutions under the harmony control. The main contributions of this paper are hybrid krill herd with serving harmony search algorithm based on a new probability value (Def ) in order to control the harmony search operator to 690 explore the exploration search effectively, as well as an analysis of the proposed method. An improved version of H-KHA aims to deal with the global search problems. To evaluate the proposed H-KHA, five evaluation measures are adopted to text clustering technique such as ASDC, precision, recall, accuracy and F-695 measure, and two evaluation measures are adopted to data clustering technique such as Error rate and objective function. These measures are the most popular evaluation criteria in data and text mining domain to evaluate the newly proposed clustering method. The proposed H-KHA can produce the best-recorded results for all benchmark datasets used compared with other versions along 700 with the several successful clustering methods and techniques from the literature. Thus, KH algorithm with serving of the harmony search is an effective method for clustering techniques and expects a huge number of upcoming successful stories in the domain of data and text clustering. The results show that the proposed hybridization is active and efficient for tackling the clustering 705 problems. The experimental results are compared with the other comparative algorithms, which showed that the proposed hybridization of KHA (H-KHA) is suitable for solving the clustering problems as regards data and text. Page 36 of 43 A c c e p t e d M a n u s c r i p t H-KHA is a suitable addition for clustering domain. Other clustering problem can be used in the future to ensure the capability of this algorithm in 710 this domain. Moreover, other powerful local search can be hybridized to further improve the exploitation capability of KH algorithm. Future research can investigate the proposed algorithm on benchmark function datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>A c c e p t e d M a n u s c r i p t Page 38 of 43 A</head><label>43</label><figDesc>c c e p t e d M a n u s c r i p t Applications &amp; Industrial Electronics (ISCAIE), 2016 IEEE Symposium on, IEEE, 2016, pp. 67-72.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Sort the krill and find X best , where best ∈ (1, 2, ..., S)</figDesc><table><row><cell>26:</cell><cell>end if</cell></row><row><cell>27:</cell><cell>end if</cell></row><row><cell>28:</cell><cell>end if</cell></row><row><cell>29:</cell><cell>end for</cell></row><row><cell>30:</cell><cell></cell></row><row><cell>31:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Characteristic of the data clustering datasets The related information is summarized in Table2. The KH algorithm parameters are chosen accounting to the standard values of V f , D max ,</figDesc><table><row><cell>375</cell><cell>5.2.2. Data preprocessing and parameter sitting</cell></row><row><cell></cell><cell>The features in each dataset have variant value ranges. Accurate clusters</cell></row><row><cell></cell><cell>cannot be obtained if these values are quite different [43]. Determining the pa-</cell></row><row><cell></cell><cell>rameter values are inevitable for the proposed H-KHA and other comparative</cell></row></table><note><p><p><p><p>1 </p>, namely, (CMC, Iris, Vowel, Seeds, Cancer, Glass, and Wine). Table</p>1</p>presents the related information that given in each dataset, including the number of datasets, dataset name, number of features, and number of clusters in advance.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Characteristics of H-KHA</figDesc><table><row><cell>H-KHA</cell><cell>value</cell></row><row><cell>Number of solution</cell><cell>20</cell></row><row><cell cols="2">Number of generation 1000</cell></row><row><cell>V f</cell><cell>0.02</cell></row><row><cell>D max</cell><cell>0.002</cell></row><row><cell>N max</cell><cell>0.01</cell></row><row><cell>N max</cell><cell>0.05</cell></row><row><cell>HMCR</cell><cell>0.90</cell></row><row><cell>PARmin</cell><cell>0.45</cell></row><row><cell>PARmax</cell><cell>0.90</cell></row><row><cell>bwmin</cell><cell>0.10</cell></row><row><cell>bwmax</cell><cell>1.00</cell></row><row><cell>5.2.3. Data clustering results and discussion</cell><cell></cell></row><row><cell cols="2">In this paper, the experiments of data clustering problem are implemented</cell></row><row><cell cols="2">in a Windows 7 environment using MATLAB (7.10.0) computer programming</cell></row><row><cell cols="2">with different CPU and RAM capabilities using different standard benchmark</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>optimization algorithm to enhance the data clustering technique. For the CMC dataset, H-KHA obtains the best value over (average ER), whereas k-means++ obtains the best value over (best ER) and spectral clustering obtains the best value over (worst ER). For the Iris dataset, H-PSO obtains the best</figDesc><table><row><cell>410</cell></row><row><cell>value over (average ER), whereas H-KHA obtains the best value over (best ER)</cell></row><row><cell>and DBSCAN clustering obtains the best value over (worst ER). For the Vowel</cell></row><row><cell>dataset, H-KHA obtains the best value for the overall statistic measures (aver-</cell></row><row><cell>age, best, and worst ER). For the Seeds dataset, H-KHA obtains the best value</cell></row><row><cell>Page 20 of 43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Error rate results for seven datasets</figDesc><table><row><cell>Optimization algorithms</cell><cell>GA PSO HS KHA H-GA H-PSO H-KHA</cell><cell>54.656 54.101 55.430 53.936 53.124 53.201 52.213</cell><cell>56.697 55.899 56.001 56.056 55.142 54.204 53.656</cell><cell>57.296 56.486 57.906 56.999 56.214 55.333 54.333</cell><cell>12 7 8 9 5 2 1</cell><cell>10.666 10.667 10.509 8.430 9.765 9.666 9.000</cell><cell>21.652 15.867 21.054 22.658 21.100 15.800 19.866</cell><cell>43.333 43.447 44.286 42.548 44.667 44.333 43.333</cell><cell>11 2 8 12 9 1 6</cell><cell>14.164 15.555 15.485 14.514 15.945 15.099 14.010</cell><cell>15.298 15.957 16.000 16.501 16.987 15.800 14.666</cell><cell>16.254 19.547 18.562 18.659 19.245 17.568 17.154</cell><cell>2 4 5 8 11 3 1</cell><cell>13.810 8.571 12.290 13.595 9.565 9.047 8.623</cell><cell>21.000 15.262 13.015 13.595 13.458 13.881 11.666</cell><cell>25.714 36.190 16.021 20.321 19.525 19.238 14.523</cell><cell>12 11 6 9 8 10 3</cell><cell>39.510 40.775 40.111 39.256 40.254 39.775 38.670</cell><cell>44.270 43.051 42.054 42.543 41.214 39.125 39.012</cell><cell>47.753 45.455 45.640 47.191 46.214 46.758 44.154</cell><cell>12 11 7 10 5 2 1</cell><cell>42.991 43.925 41.162 38.318 35.249 41.589 32.242</cell><cell>51.028 46.262 42.054 43.925 44.219 47.617 42.219</cell><cell>56.075 52.804 46.255 50.476 51.985 56.075 51.420</cell><cell>12 9 1 4 5 11 2</cell><cell>29.310 29.775 29.865 29.213 29.654 29.775 29.650</cell><cell>34.270 32.051 32.568 32.303 30.989 30.871 33.000</cell><cell>47.753 44.449 44.467 47.191 44.001 43.888 42.134</cell><cell>12 4 7 5 2 1 8</cell><cell>10.42 6.85 6.00 8.14 6.42 4.28 3.14</cell><cell>12 8 4 11 6 2 1</cell><cell></cell></row><row><cell>Clustering techniques</cell><cell>Dataset Statistics K-mean K-mean++ Spectral Agglomerative DBSCAN</cell><cell>53.541 52.391 54.280 CMC Best 54.660 52.003</cell><cell>55.120 54.944 56.544 Average 55.470 56.258</cell><cell>54.044 57.487 56.654 Worst 56.667 57.001</cell><cell>4 3 11 Rank 6 10</cell><cell>10.547 9.874 9.987 Iris Best 10.660 10.101</cell><cell>17.458 18.544 16.311 Average 21.467 20.983</cell><cell>55.541 48.397 43.111 Worst 56.667 54.274</cell><cell>4 5 3 Rank 10 7</cell><cell>15.645 15.114 15.645 Vowel Best 16.245 15.364</cell><cell>17.540 16.541 16.477 Average 16.547 16.177</cell><cell>18.006 17.450 17.499 Worst 18.551 17.560</cell><cell>12 9 7 Rank 10 6</cell><cell>9.454 10.254 12.322 Seeds Best 12.643 11.254</cell><cell>11.297 13.021 12.214 Average 12.643 11.200</cell><cell>18.451 20.214 19.397 Worst 12.643 14.111</cell><cell>2 7 4 Rank 5 1</cell><cell>38.111 39.148 39.654 Cancer Best 39.865 39.500</cell><cell>40.154 41.645 42.199 Average 42.388 40.145</cell><cell>44.685 46.699 44.021 Worst 45.970 44.965</cell><cell>4 6 8 Rank 9 3</cell><cell>38.541 32.001 33.717 Glass Best 42.262 45.123</cell><cell>46.614 43.222 44.984 Average 46.154 44.566</cell><cell>51.991 52.140 51.123 Worst 46.215 45.250</cell><cell>10 3 7 Rank 8 6</cell><cell>29.189 30.665 30.140 Wine Best 29.775 30.546</cell><cell>33.585 34.154 33.487 Average 32.388 31.841</cell><cell>43.137 42.688 42.009 Worst 43.820 43.534</cell><cell>10 11 9 Rank 6 3</cell><cell>6.57 6.28 7.00 Mean rank 7.71 5.14</cell><cell>7 5 9 Final rank 10 3</cell><cell>Note: The lowest ranked algorithm is the best one.</cell></row></table><note><p><p><p>H-KHA failed to reach the best value in all runs; however, it obtained the number one rank among all comparative algorithms. As regards Vowel data sets, H-KHA achieved the best optimum value of 14.666 overall runs and for Cancer data sets, H-KHA achieved the best optimum value of 39.012 overall 430 runs and so on. Thus, H-KHA algorithm obtained the near best value in all runs. The statistical analysis is done based on average ER. The average rankings of the clustering algorithms are reported in Table</p>3</p>. The proposed H-KHA is ranked the highest one, which is followed by H-KHA, H-PSO, k-mean++, HS, agglomerative clustering, H-GA, spectral clustering, PSO, DBSCAN, k-mean, 435 KHA, and GA among the datasets. spectively, which is much better than the worst solutions that were obtained</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The objective function values obtained by algorithms for different datasets</figDesc><table><row><cell>Dataset Statistics GA PSO K-mean KHA H-GA H-PSO H-KHA K-mean++ HS</cell><cell>5698.564 5688.854 5745.548 5699.964 5586.532 CMC Best 5641.556 5609.369 5609.254 5710.915</cell><cell>5781.854 5795.546 5714.287 5706.654 5601.681 Average 5790.448 5760.294 5689.299 5770.648</cell><cell>5814.861 5836.266 5847.214 5880.565 5666.943 Worst 5896.101 5790.120 5879.119 5994.545</cell><cell>7 9 4 3 1 Rank 8 5 2 6</cell><cell>98.648 96.744 96.989 96.590 96.154 Iris Best 113.986 96.148 97.325 97.325</cell><cell>98.447 96.914 96.752 97.235 96.524 Average 125.197 97.997 104.576 99.569</cell><cell>99.144 97.909 97.154 97.985 96.989 Worst 139.778 98.287 123.969 110.650</cell><cell>6 4 3 2 1 Rank 9 5 8 7</cell><cell>156.155 155.684 150.145 149.348 149.123 Vowel Best 152.648 149.540 149.499 161.154</cell><cell>156.489 156.244 152.145 149.954 149.565 Average 153.697 150.154 149.990 161.990</cell><cell>157.548 157.369 155.124 150.023 149.999 Worst 153.993 150.861 150.346 162.146</cell><cell>8 7 5 2 1 Rank 6 4 3 9</cell><cell>4412.525 4490.815 4421.325 4395.445 4350.140 Seeds Best 4523.125 4532.142 4480.162 4471.654</cell><cell>4450.848 4499.840 4521.648 4390.900 4391.546 Average 4550.910 4607.564 4488.542 4483.709</cell><cell>4460.464 4510.988 4596.145 4419.410 4399.254 Worst 4621.021 4660.216 4499.649 4493.197</cell><cell>3 4 7 1 2 Rank 8 9 6 5</cell><cell>2988.856 3012.648 2998.259 2988.654 2975.191 Cancer Best 3010.325 3002.450 2989.258 3064.694</cell><cell>2990.654 3045.542 2999.489 2990.263 2982.437 Average 3050.365 3012.394 2992.640 3086.159</cell><cell>2998.286 3089.654 3075.295 2993.372 2990.493 Worst 3085.259 3090.910 3001.193 3094.369</cell><cell>3 6 5 2 1 Rank 8 6 4 9</cell><cell>243.157 273.258 215.975 213.162 213.105 Glass Best 285.654 279.767 216.198 216.447</cell><cell>246.254 276.441 217.659 216.658 215.665 Average 290.159 282.734 220.214 218.845</cell><cell>251.556 279.982 225.154 218.851 217.355 Worst 294.565 285.299 222.015 221.125</cell><cell>6 7 3 2 1 Rank 9 8 5 4</cell><cell>16,759.449 16,954.901 16,765.456 16,980.297 16,350.154 Wine Best 16,854.654 16,764.901 16,642.141 17,142.158</cell><cell>16,945.698 17,015.148 16,765.456 17,587.145 16,410.147 Average 17,900.154 17,964.390 16,976.691 17,556.149</cell><cell>16,989.931 17,152.697 16,765.456 18,564.154 16,961.147 Worst 18,397.490 19,297.145 20,452.147 17,947.154</cell><cell>6 3 2 1 4 5 7 Rank 8 9</cell><cell>6.14 2.28 2.00 1.14 5.28 4.71 6.71 Mean rank 8.00 6.57</cell><cell>6 3 2 1 5 4 8 Final rank 9 7</cell><cell>Note: The lowest ranked algorithm is the best one.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Identifying the clusters centroid for the Iris dataset</figDesc><table><row><cell>Centroids</cell><cell cols="2">calculated centroid</cell></row><row><cell></cell><cell>Att.1</cell><cell>Att.2</cell><cell>Att.3 Att.4</cell></row><row><cell cols="2">Centroid1 5.624</cell><cell>3.102</cell><cell>3.210 1.355</cell></row><row><cell cols="2">Centroid2 5.325</cell><cell>3.001</cell><cell>3.749 1.296</cell></row><row><cell cols="2">Centroid3 5.623</cell><cell>2.998</cell><cell>3.447 1.191</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Identifying the clusters centroid for the Vowel dataset update approach is proposed by H-KHA to enhance the ability of the search space. Subsequently, we conclude based on all of these results that the H-KHA obtained the best results almost on all the tested datasets.The statistical analysis is done based on two measures include error rate 465 and objective function. The average rankings of the clustering algorithms are reported according to error rate measure in Table3. The proposed H-KHA is ranked the highest one among the datasets. Also, in Table4, the average rankings of the clustering algorithms are reported according to objective function measure. Again, the proposed H-KHA is ranked the highest one, which is fol-</figDesc><table><row><cell></cell><cell>Centroids</cell><cell cols="2">calculated centroid</cell></row><row><cell></cell><cell></cell><cell>Att.1</cell><cell>Att.2</cell><cell>Att.3</cell></row><row><cell></cell><cell cols="2">Centroid1 461.4</cell><cell>1430.5</cell><cell>2651.5</cell></row><row><cell></cell><cell cols="2">Centroid2 432.1</cell><cell>1490.3</cell><cell>2690.4</cell></row><row><cell></cell><cell cols="2">Centroid3 471.2</cell><cell>1502.9</cell><cell>2669.4</cell></row><row><cell></cell><cell cols="2">Centroid4 448.5</cell><cell>1490.5</cell><cell>2544.6</cell></row><row><cell></cell><cell cols="2">Centroid5 482.4</cell><cell>1477.6</cell><cell>2539.1</cell></row><row><cell></cell><cell cols="2">Centroid6 475.6</cell><cell>1409.8</cell><cell>2691.3</cell></row><row><cell></cell><cell cols="3">The proposed hybrid KHA and PSO have been demonstrated to be superior</cell></row><row><cell></cell><cell cols="3">compared to an original standalone KH algorithm in this paper. In addition ,</cell></row><row><cell></cell><cell cols="3">in Table 4, H-KHA might lead to achieving superior solutions after a specific</cell></row><row><cell>455</cell><cell cols="3">number of iterations. As an example, Tables 5 and 6 describe the calculated</cell></row><row><cell></cell><cell cols="3">cluster centers for two datasets (Iris and wine) to identify the best solution. The</cell></row><row><cell></cell><cell cols="3">best cluster centroid is displayed to validate the sum of the objective function</cell></row><row><cell></cell><cell cols="3">(intra-cluster distances) values in Table 4. Each data object is assigned to the</cell></row><row><cell></cell><cell cols="3">closest centroid in Tables 6 to reach the best values. For example, all of the</cell></row></table><note><p><p>470</p>lowed by H-PSO, H-GA, k-mean, HS, KHA, PSO, k-mean++ and GA, among the datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Text document datasets characteristics</figDesc><table><row><cell>Datasets</cell><cell>Number of</cell><cell>Number of</cell><cell>Number of</cell></row><row><cell></cell><cell cols="3">Documents (d) Terms (t) Clusters (K)</cell></row><row><cell>Classic4</cell><cell>500</cell><cell>1800</cell><cell>2</cell></row><row><cell>Classic4</cell><cell>2000</cell><cell>6500</cell><cell>4</cell></row><row><cell>Reuters21578</cell><cell>3000</cell><cell>10150</cell><cell>7</cell></row><row><cell>20Newsgroup</cell><cell>5000</cell><cell>20140</cell><cell>9</cell></row><row><cell>Reuters21578</cell><cell>2000</cell><cell>7481</cell><cell>10</cell></row><row><cell>20Newsgroup</cell><cell>2000</cell><cell>9560</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Algorithm performance based on clusters quality</figDesc><table><row><cell>Optimization algorithms</cell><cell>GA HS PSO CS ABC KHA H-GA H-PSO H-CS H-ABC H-KHA</cell><cell>0.221 0.244 0.248 0.235 0.222 0.215 0.227 0.280 0.278 0.279 0.292</cell><cell>0.214 0.249 0.236 0.233 0.225 0.205 0.234 0.255 0.256 0.272 0.286</cell><cell>0.225 0.238 0.231 0.228 0.229 0.218 0.233 0.247 0.261 0.265 0.284</cell><cell>0.221 0.242 0.233 0.230 0.222 0.216 0.231 0.253 0.251 0.270 0.285</cell><cell>15 8 10 13 14 16 12 6 7 3 1</cell><cell>0.332 0.335 0.321 0.339 0.322 0.335 0.334 0.345 0.339 0.349 0.347</cell><cell>0.323 0.334 0.312 0.334 0.326 0.342 0.331 0.333 0.341 0.334 0.349</cell><cell>0.331 0.345 0.322 0.329 0.329 0.338 0.233 0.336 0.339 0.332 0.354</cell><cell>0.329 0.339 0.320 0.331 0.325 0.340 0.331 0.331 0.343 0.333 0.348</cell><cell>7 5 16 10 14 4 10 10 3 8 1</cell><cell>0.476 0.466 0.461 0.465 0.456 0.473 0.455 0.469 0.473 0.475 0.492</cell><cell>0.468 0.464 0.459 0.461 0.460 0.468 0.451 0.465 0.467 0.470 0.488</cell><cell>0.458 0.466 0.456 0.466 0.450 0.464 0.439 0.468 0.479 0.468 0.485</cell><cell>0.466 0.465 0.467 0.465 0.456 0.467 0.449 0.465 0.467 0.465 0.487</cell><cell>5 7 3 7 15 3 14 7 3 7 2</cell><cell>0.365 0.366 0.361 0.370 0.367 0.373 0.356 0.381 0.381 0.387 0.396</cell><cell>0.363 0.369 0.366 0.371 0.362 0.362 0.359 0.373 0.379 0.386 0.389</cell><cell>0.367 0.351 0.368 0.365 0.365 0.366 0.356 0.381 0.377 0.380 0.380</cell><cell>0.365 0.355 0.367 0.366 0.360 0.363 0.355 0.374 0.378 0.383 0.384</cell><cell>9 14 7 8 12 11 14 6 5 3 2</cell><cell>0.366 0.367 0.369 0.365 0.356 0.373 0.368 0.372 0.373 0.374 0.481</cell><cell>0.368 0.366 0.370 0.363 0.364 0.369 0.367 0.373 0.374 0.372 0.482</cell><cell>0.368 0.365 0.369 0.366 0.353 0.364 0.366 0.369 0.379 0.368 0.479</cell><cell>0.366 0.364 0.370 0.365 0.356 0.366 0.365 0.371 0.368 0.369 0.480</cell><cell>9 13 6 11 16 9 11 5 8 7 2</cell><cell>0.343 0.336 0.341 0.325 0.346 0.362 0.355 0.368 0.371 0.373 0.394</cell><cell>0.341 0.334 0.348 0.321 0.340 0.366 0.352 0.367 0.369 0.370 0.391</cell><cell>0.339 0.335 0.346 0.326 0.341 0.364 0.354 0.362 0.371 0.368 0.394</cell><cell>0.341 0.334 0.347 0.325 0.339 0.364 0.353 0.363 0.369 0.370 0.393</cell><cell>10 13 9 15 12 5 8 6 4 3 2</cell><cell>9.16 10.00 8.50 10.66 13.83 8.00 11.50 6.66 5.00 5.16 1.66</cell></row><row><cell>Clustering techniques</cell><cell>K-mean Kmean++ Spectral Agglomerative DBSCAN</cell><cell>0.234 0.254 0.245 0.299 0.244</cell><cell>0.234 0.239 0.255 0.288 0.255</cell><cell>0.229 0.233 0.260 0.280 0.259</cell><cell>0.233 0.235 0.256 0.284 0.258</cell><cell>10 9 5 2 4</cell><cell>0.335 0.335 0.325 0.345 0.326</cell><cell>0.325 0.334 0.322 0.341 0.342</cell><cell>0.329 0.332 0.326 0.349 0.335</cell><cell>0.328 0.333 0.324 0.346 0.339</cell><cell>13 8 15 2 5</cell><cell>0.450 0.462 0.415 0.495 0.465</cell><cell>0.452 0.466 0.422 0.487 0.460</cell><cell>0.450 0.462 0.421 0.495 0.465</cell><cell>0.450 0.463 0.421 0.493 0.463</cell><cell>13 11 16 1 11</cell><cell>0.366 0.367 0.398 0.355 0.381</cell><cell>0.361 0.358 0.395 0.354 0.382</cell><cell>0.366 0.356 0.380 0.352 0.379</cell><cell>0.365 0.356 0.386 0.353 0.381</cell><cell>9 13 1 16 4</cell><cell>0.370 0.362 0.482 0.476 0.461</cell><cell>0.375 0.366 0.475 0.479 0.466</cell><cell>0.369 0.361 0.471 0.485 0.461</cell><cell>0.372 0.363 0.473 0.483 0.462</cell><cell>4 14 3 1 15</cell><cell>0.320 0.331 0.354 0.389 0.346</cell><cell>0.322 0.329 0.355 0.398 0.341</cell><cell>0.326 0.330 0.359 0.389 0.342</cell><cell>0.323 0.329 0.357 0.395 0.341</cell><cell>16 14 7 1 10</cell><cell>10.83 11.50 7.83 3.83 8.16</cell></row><row><cell></cell><cell>Measure</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell>Rank</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell>Rank</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell>Rank</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell>Rank</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell>Rank</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell><cell>Rank</cell><cell></cell></row><row><cell></cell><cell>Dataset</cell><cell>DS1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DS6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean rank</cell><cell>Final rank</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://archive.ics.uci.edu/ml/datasets.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="460" xml:id="foot_1"><p>the six cluster centers, which is presented in Table6. Moreover, a new position</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>http://scikit-learn.org/stable/modules/clustering.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>http://sites.labic.icmc.usp.br/text collections/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>DS3, DS4, DS5, and DS6). Subsequently, the proposed H-KHA significantly</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-715 objectives-based text clustering technique using k-mean algorithm</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ai-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-J</forename><surname>Ai-Huson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data clustering using cuckoo search algorithm (csa)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manikandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Selvarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Soft Computing for Problem Solving</title>
		<meeting>the Second International Conference on Soft Computing for Problem Solving</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012-12-28">SocProS 2012. December 28-30, 2012. 2014</date>
			<biblScope unit="page" from="1275" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A new algorithm for data clustering based on cuckoo search optimization</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Saida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nadjet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Omar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Black hole: A new heuristic optimization approach for data clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamlou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information sciences</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on nature inspired metaheuristic algorithms for partitional clustering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm and Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Text document clustering on the basis of inter passage approach by using k-means</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing, Communication 730 &amp; Automation (ICCCA), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="110" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Web document clustering by using pso-based cuckoo search clustering algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Zaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Mon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Swarm Intelligence and Evolutionary Computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="263" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of text document clustering approach based on particle swarm optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mangat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Open Computer Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="69" to="90" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised feature selection technique based on genetic algorithm for improving the text clus-740 tering</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science and Information Technology (CSIT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient stochastic algorithms for document clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shamsfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Meybodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="269" to="291" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Applying genetic algorithms to information retrieval using vector space model</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M Q</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Hanandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science, Engineering and Applications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data clustering using bee colony optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mizooji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haghighat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forsati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Multi-Conference on Computing in the 750 Global IT</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Krill herd: a new bio-inspired optimization algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Nonlinear Science and Numerical Simulation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4831" to="4845" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An improved krill herd algorithm with global ex-755 ploration capability for solving numerical function optimization problems and its application to data clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jensi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Jiji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="230" to="245" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bolaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A comprehensive review: Krill herd algorithm (kh) and its 760 applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A krill herd algorithm for efficient text documents clustering</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Awadallah</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient protocol for data clustering by fuzzy cuckoo optimization algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahmoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised text feature selection technique based on hybrid particle swarm optimization algorithm with genetic operators for the text clustering</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">770</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel clustering approach: Artificial bee colony (abc) algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ozturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied soft computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="652" to="657" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new hybridization strategy for krill herd algorithm and harmony search algorithm 775 applied to improve the data clustering</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Hanandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">management</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data clustering using harmony search algorithm</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mohd Alia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Swarm, Evolutionary, and Memetic Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An efficient hybrid data clustering method 780 based on k-harmonic means and particle swarm optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="9847" to="9852" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid krill herd algorithm with differential evolution for global numerical optimization</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="308" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new hybrid method based on krill herd and cuckoo search for global optimization tasks</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Bio-Inspired Computation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chaotic gradient artificial bee colony for text clustering</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1113" to="1126" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opposition-based krill herd algorithm with cauchy mutation and position clamping</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="147" to="157" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incorporating mutation scheme into krill herd algorithm for global numerical optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural 795 Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="853" to="871" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hybrid method based on krill herd and quantum-behaved particle swarm optimization</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="989" to="1006" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A discrete krill herd method with mul-800 tilayer coding strategy for flexible job-shop scheduling problem</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Thampi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent systems technologies and applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="201" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new heuristic optimization algorithm: harmony search</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Geem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simulation</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="68" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Harmony k-means algorithm for document 805 clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abolhassani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="391" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data clustering using harmony search algorithm</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mohd Alia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Swarm, Evolutionary, and Memetic Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A hybridized approach to data clustering</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-W</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">810</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1754" to="1762" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new clustering approach based on k-means and krill herd algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nikbakht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mirvaziri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electrical Engineering (ICEE), 2015 23rd Iranian Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="662" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clustering 815 based on the krill herd algorithm with selected validity measures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charytanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kulczycki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science and Information Systems (FedCSIS), 2016 Federated Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data clustering using particle swarm optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Der Merwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Engelbrecht</surname></persName>
		</author>
		<idno>CEC&apos;03. The</idno>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">820 Congress on</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="215" to="220" />
			<date type="published" when="2003">2003</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mba-lf: A new data clustering method using modified bat algorithm and levy flight</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jensi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Jiji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICTACT Journal on Soft Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Study on multi-center fuzzy c-means algorithm based on transitive closure and spectral clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">825</biblScope>
			<biblScope unit="page" from="89" to="101" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Agglomerative hierarchical clustering with constraints: Theoretical and empirical results</title>
		<author>
			<persName><forename type="first">I</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Principles of Data Mining and Knowledge Discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fuzzy extensions of the dbscan clustering algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bordogna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Genetic algorithm-based clustering technique</title>
		<author>
			<persName><forename type="first">U</forename><surname>Maulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1455" to="1465" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A review on particle swarm optimization 835 algorithms and their applications to data clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jasola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of clustering data mining techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Berkhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grouping multidimensional data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="25" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<title level="m">Data mining: concepts and techniques</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">840</biblScope>
			<biblScope unit="page">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clustering using an improved krill herd algorithm</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A hybrid evolutionary computation approach with its application for optimizing text document clustering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">845</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2517" to="2524" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Document clustering using particle swarm optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Potok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2005 IEEE Swarm Intelligence Symposium, 2005. SIS 2005</title>
		<meeting>2005 IEEE Swarm Intelligence Symposium, 2005. SIS 2005</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="185" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improved particle swarm optimization 850 based k-means clustering</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Prabha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Visalakshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Computing Applications (ICICA), 2014 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Text feature selection with a robust weight scheme and dynamic dimension reduction to text document clustering</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Alomari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="page">855</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A similarity assessment technique for effective grouping of documents</title>
		<author>
			<persName><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unsupervised text feature selection technique based on particle swarm optimization algorithm for improving the text clustering</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Abualigah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Albetar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Hanandeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Clustering of web search results based on the cuckoo search algorithm and balanced bayesian information criterion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cobos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muñoz-Collazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urbano-Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>León</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herrera-Viedma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page" from="248" to="264" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A text clustering approach of chinese 865 news based on neural network language model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="206" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey of multiobjective evolutionary clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Maulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
