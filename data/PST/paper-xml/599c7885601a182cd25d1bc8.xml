<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CURE-SMOTE algorithm and hybrid algorithm for feature selection and parameter optimization based on random forests</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Jinan University</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suohai</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Jinan University</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CURE-SMOTE algorithm and hybrid algorithm for feature selection and parameter optimization based on random forests</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">40077F81B3B048F450EFB7535190FF43</idno>
					<idno type="DOI">10.1186/s12859-017-1578-z</idno>
					<note type="submission">Received: 25 August 2016 Accepted: 3 March 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Random forests</term>
					<term>Imbalance data</term>
					<term>Intelligence algorithm</term>
					<term>Feature selection</term>
					<term>Parameter optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background:</head><p>The random forests algorithm is a type of classifier with prominent universality, a wide application range, and robustness for avoiding overfitting. But there are still some drawbacks to random forests. Therefore, to improve the performance of random forests, this paper seeks to improve imbalanced data processing, feature selection and parameter optimization. Results: We propose the CURE-SMOTE algorithm for the imbalanced data classification problem. Experiments on imbalanced UCI data reveal that the combination of Clustering Using Representatives (CURE) enhances the original synthetic minority oversampling technique (SMOTE) algorithms effectively compared with the classification results on the original data using random sampling, Borderline-SMOTE1, safe-level SMOTE, C-SMOTE, and k-means-SMOTE. Additionally, the hybrid RF (random forests) algorithm has been proposed for feature selection and parameter optimization, which uses the minimum out of bag (OOB) data error as its objective function. Simulation results on binary and higher-dimensional data indicate that the proposed hybrid RF algorithms, hybrid genetic-random forests algorithm, hybrid particle swarm-random forests algorithm and hybrid fish swarm-random forests algorithm can achieve the minimum OOB error and show the best generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion:</head><p>The training set produced from the proposed CURE-SMOTE algorithm is closer to the original data distribution because it contains minimal noise. Thus, better classification results are produced from this feasible and effective algorithm. Moreover, the hybrid algorithm's F-value, G-mean, AUC and OOB scores demonstrate that they surpass the performance of the original RF algorithm. Hence, this hybrid algorithm provides a new way to perform feature selection and parameter optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Tin Kam Ho proposed the random forests (RF) concept <ref type="bibr" target="#b0">[1]</ref> and the Random Subspace algorithm <ref type="bibr" target="#b1">[2]</ref> in 1995 and 1998, respectively. Breiman <ref type="bibr" target="#b2">[3]</ref> proposed a novel ensemble learning classification, random forests, by combining bagging ensemble learning and Tin Kam Ho's concept in 2001. The feature of random forests that allows for avoiding over-fitting makes it suitable for use as a data dimension reduction method for processing data with missing values, noise and outliers. Although random forests have been applied to many other fields such as biological prediction <ref type="bibr" target="#b3">[4]</ref>, fault detection <ref type="bibr" target="#b4">[5]</ref>, and network attacks <ref type="bibr" target="#b5">[6]</ref>, studies seeking to improve the algorithm itself are lacking. The RF algorithm still has some shortcomings; for example, it performs poorly for classification on imbalanced data, fails to control the model during specific operations, and is sensitive to parameter adjustment and random data attempts. Usually, there are two ways to improve RF: increase the accuracy of each individual classifier or reduce the correlation between classifiers.</p><p>First, it is possible to increase the classification accuracy in minor class samples of RF for imbalanced training sets through data preprocessing. Several types of methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> based on both data and algorithms exist for imbalanced data. Chen <ref type="bibr" target="#b10">[11]</ref> found that undersampling provides results closer to the original samples than does oversampling for large-scale data. A novel sampling approach <ref type="bibr" target="#b11">[12]</ref> based on sub-modularity subset selection was employed to balance the data and select a more representative data subset for predicting local protein properties. Similarly, an algorithm combining RF and a Support Vector Machine (SVM) with stratified sampling <ref type="bibr" target="#b12">[13]</ref> yielded a better performance than did other traditional algorithms for imbalanced-text categorization, including RF, SVM, SVM with undersampling and SVM with oversampling. A novel hybrid algorithm <ref type="bibr" target="#b13">[14]</ref> using a radial basis function neural network (RBFNN) integrated with RF was proposed to improve the ability to classify the minor class of imbalanced datasets. In addition, imbalanced data for bioinformatics is a well-known problem and widely found in biomedical fields. Applying RF with SMOTE to the CHOM, CHOA and Vero (A) datasets <ref type="bibr" target="#b14">[15]</ref> is considered a remarkable improvement that is helpful in the field of functional and structural proteomics as well as in drug discovery. Ali S <ref type="bibr" target="#b15">[16]</ref> processed imbalanced breast cancer data using the CSL technique, which imposes a higher cost on misclassified examples and develops an effective Cost-Sensitive Classifier with a GentleBoost Ensemble (Can-CSC-GBE). The Mega-Trend-Diffusion (MTD) technique <ref type="bibr" target="#b16">[17]</ref> was developed to obtain the best results on breast and colon cancer datasets by increasing the samples of the minority class when building the prediction model.</p><p>Second, it is possible to improve algorithm construction. Because the decision trees in the original algorithm have the same weights, a weighted RF was proposed that used different weights that affected the similarity <ref type="bibr" target="#b17">[18]</ref> between trees, out-of-bag error <ref type="bibr" target="#b18">[19]</ref>, and so on. Weighted RF has been shown to be better than the original RF algorithm <ref type="bibr" target="#b19">[20]</ref>. Ma <ref type="bibr" target="#b20">[21]</ref> combined Adaboost with RF and adaptive weights to obtain a better performance. The weight of attributes reduces the similarity among trees and improves RF <ref type="bibr" target="#b21">[22]</ref>. Moreover, the nearest K-neighbour <ref type="bibr" target="#b22">[23]</ref> and pruning mechanism can help achieve a better result when using margin as the evaluation criterion <ref type="bibr" target="#b23">[24]</ref>.</p><p>In this paper, the main work is divided into two parts: first, the CURE-SMOTE algorithm is combined with RF to solve the shortcomings of using SMOTE alone. Compared with results on the original data, random oversampling, SMOTE, Borderline SMOTE1, safe-level-SMOTE, C-SMOTE, and the k-means-SMOTE algorithm, CURE-SMOTE's effectiveness when classifying imbalanced data is verified. Then, to simultaneously optimize feature selection, tree size, and the number of sub-features, we propose a hybrid algorithm that includes a genetic-random forests algorithm (GA-RF), a particle swarm-random forests algorithm (PSO-RF) and an artificial fish swarm-random forests algorithm (AFSA-RF). Simulation experiments show that the hybrid algorithm obtains better features, selects better parameter values and achieves a higher performance than traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random forests algorithm review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm principle</head><p>RF is a combination of Bagging and Random Subspace, consisting of many binary or multi-way decision trees h 1 (x), h 2 (x), … h nTree (x), as shown in Fig. <ref type="figure">1</ref>. The final decision is made by majority voting to aggregate the predictions of all the decision trees. The original dataset</p><formula xml:id="formula_0">T = {(x i1 , x i2 , …, x iM , y i )} i = 1 N</formula><p>contains N samples, the vector x i1 , x i2 , …, x iM denotes the M-dimension attributes or features, Y = {y i } i N denotes classification labels, and a sample is deduced as label c by y i = c.</p><p>There are two random procedures in RF. First, training sets are constructed by using a bootstrap <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> mechanism randomly with replacement [Fig. <ref type="figure">2 (I)</ref>]. Second, random features are selected with non-replacement from the total features when the nodes of the trees are split. The size κ of the feature subset is usually far less than the size of the total features, M. The first step is to select κ features randomly, calculate the information gain of κ split and select the best features. Thus, the size of candidate features becomes M -κ . Then, continue as shown in Fig. <ref type="figure">2</ref> (II).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification rules and algorithmic procedure</head><p>The best attribute can be computed by three methods: information gain, information gain rate and Gini coefficient, which correspond to ID3, C4.5 <ref type="bibr" target="#b26">[27]</ref> and CART <ref type="bibr" target="#b27">[28]</ref>, respectively. When the attribute value is continuous, the best split point must be selected. We use the CART method in this paper; hence, a smaller Gini coefficient indicates a better classification result. Let P i represent the proportion of sample i in the total sample size. Assume that sample T is divided into k parts after splitting by attribute A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gini</head><formula xml:id="formula_1">T ð Þ ¼ 1- X c i P 2 i ð1Þ Gini T ; A ð Þ¼ X k j¼1 T j T j j Gini T j À Á<label>ð2Þ</label></formula><p>There are several ways by which the termination criteria for RF can be met. For example, termination occurs when the decision tree reaches maximum depth, the impurity of the end node reaches the threshold, the number of final samples reaches a set point, and the candidate attribute is used up. The RF classification algorithm procedure is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CURE-SMOTE algorithm Definition and impact of imbalanced data</head><p>In recent years, the problem of classifying imbalanced data <ref type="bibr" target="#b28">[29]</ref> has attracted increasing attention. Imbalanced data sets generally refer to data that is distributed unevenly among different categories where the data in the smaller category is far less prevalent than data in the larger category. The Imbalance Ratio (IR) is defined as the ratio of the number of minor class samples to the number of major class samples. Therefore, imbalanced data causes the training set for each decision tree to be imbalanced during the first "random" procedure. The classification performance of traditional RF on imbalanced data sets <ref type="bibr" target="#b29">[30]</ref> is even worse than that of SVMs <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMOTE algorithm</head><p>Several methods exist for processing imbalanced data, including sample-based and algorithmic techniques, the combination of sampling and algorithm techniques, and feature selection. In particular, a type of synthesis resampling technique algorithm called the synthetic minority oversampling technique (SMOTE) <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>, has a positive effect on the imbalanced data problem. The specific idea is implemented as follows: obtain the k -nearest neighbours of sample X in the minor class, select n samples randomly and record them as X i . Finally, the new sample X new is defined by interpolation as follows:</p><formula xml:id="formula_2">X new ¼ X origin þ rand Â X i -X origin À Á ; i ¼ 1; 2; …; n;<label>ð3Þ</label></formula><p>where rand is a random number uniformly distributed within the range (0,1), and the ratio for generating new samples approximates [1/IR] -1. However, some flaws exist in the SMOTE algorithm. First, the selection of a value for k is not informed by the nearest neighbours selection. Second, it is impossible to completely reflect the distribution of original data because the artificial samples generated by the minor class samples at the edges may lead to problems such as repeatability and noisy, fuzzy boundaries between the positive and negative classes.</p><p>Therefore, researchers have sought to improve the SMOTE algorithm. The Borderline-SMOTE1 algorithm <ref type="bibr" target="#b34">[35]</ref> causes new samples to be more effective using interpolation along the border areas, but it fails to find all the boundary points. Definitions for this algorithm are shown in Table <ref type="table" target="#tab_0">1</ref>: m is the number of nearest-neighbour samples in the minor class, and k is the number of samples in the major class.</p><p>Motivated by Borderline-SMOTE 1, safe-level-SMOTE <ref type="bibr" target="#b35">[36]</ref> advocates calculating the safe level of minor class samples, but it can easily fall into overfitting. Cluster-SMOTE <ref type="bibr" target="#b36">[37]</ref> obtains a satisfactory classification effect for imbalanced datasets by using K-means to find clusters of minor class samples and then applying SMOTE. In addition, spatial structures have been studied such as N-SMOTE <ref type="bibr" target="#b37">[38]</ref> and nuclear SMOTE <ref type="bibr" target="#b38">[39]</ref>. The authors of <ref type="bibr" target="#b39">[40]</ref> proposed an interpolation algorithm based on cluster centres. SMOTE was combined with a fuzzy nearestneighbour algorithm in <ref type="bibr" target="#b40">[41]</ref>. In <ref type="bibr" target="#b41">[42]</ref>, a preferable classification effect promoted by hierarchical clustering sampling was shown. Recently, a SMOTE noise-filtering algorithm <ref type="bibr" target="#b42">[43]</ref> and MDO algorithms with Markov distance <ref type="bibr" target="#b43">[44]</ref> have been proposed. In general, many improved versions of the SMOTE algorithm have been proposed, but none of these improvements seem perfect. This paper seeks to solve the shortcomings of SMOTE.</p><p>The K-means algorithm is effective only for spherical datasets and its application requires a certain amount of time. The CURE <ref type="bibr" target="#b44">[45]</ref> hierarchical clustering algorithm is efficient for large datasets and suitable datasets of any shape dataset. Moreover, it is not sensitive to outlier and can recognize abnormal points. Consequently, CURE is better than the BIRCH, CLARANS and DBSCAN algorithms <ref type="bibr" target="#b45">[46]</ref>. In the CURE algorithm, each sample point is assumed to be a cluster. These points are merged using local clustering until the end of the algorithm. Thus, the CURE algorithm is appropriate for distributed extensions. In this paper, inspired by C-SMOTE <ref type="bibr" target="#b39">[40]</ref> and the hierarchical clustering sampling adaptive semi-unsupervised weighted oversampling (A-SUWO) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design and analysis of CURE-SMOTE</head><p>The general idea of the CURE-SMOTE algorithm is as follows: cluster the samples of the minor class using CURE, remove the noise and outliers from the original samples, and, then, generate artificial samples randomly between representative points and the centre point. The implementation steps of the CURE-SMOTE algorithm are as follows:</p><p>Step 1. Normalize the dataset, extract the minor class samples, X, and calculate the distance dist among them. Each point is initially considered as a cluster. For each cluster U, Ur and Uc represent the representative set and the centre point, respectively. For two data items p and q, the distance between the two clusters U and V is:</p><formula xml:id="formula_3">dist U; V ð Þ¼ min p∈Ur;q∈V r dist p; q ð Þ:<label>ð4Þ</label></formula><p>Step 2. Set the clustering number, c, and update the centre and representative points after clustering and merging based on the smallest distance of the two clusters,</p><formula xml:id="formula_4">Uc← U j j⋅UcþjV j⋅V c U þ j jV j j<label>ð5Þ</label></formula><formula xml:id="formula_5">Ur← p þ α⋅ Uc-p ð Þjp∈Ur f g ;<label>ð6Þ</label></formula><p>where |U| is the number of data items for class U, and the shrinkage factor α is generally 0.5. The class with slowest growth speed is judged to contain abnormal points and will be deleted. If the number of representative points is larger than required, select the data point farthest from the clustering centre as the first representative point. Then, the next representative point is the one farthest from the former. When the number of clustering centres reaches a predefined setting, the algorithm terminates, and clusters containing only a few samples are removed.</p><p>Step 3. Generate a new sample according to the interpolation formula. X represents the samples after clustering by the CURE algorithm.</p><formula xml:id="formula_6">X n new ¼ X þ rand 0; 1 ð ÞÂ Ur-X À Á :<label>ð7Þ</label></formula><p>Step 4. Calculate IR, and return to Step 3 if IR ≤ IR 0 .</p><p>Step 5. Finally, classify the new dataset as</p><formula xml:id="formula_7">X new ¼ X ∪ X n new È É</formula><p>and add samples of the major class by RF. The distance is measured using Euclidean distance.</p><p>For example, the distance between sample X 1 = (X 11 , X 12 …, X 1M ) and sample</p><formula xml:id="formula_8">X 2 = (X 21 , X 22 …, X 2M ) is d 12 ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X M j¼1 X 1j -X 2j À Á 2 v u u t .</formula><p>During the clustering process of the CURE-SMOTE algorithm, noisy points must be removed because they are far away from the normal points, and they hinder the merge speed in the corresponding class. When clustering is complete, the clusters containing only a few samples are also deemed to be noisy points. For the sample points after clustering, the interpolation can effectively prevent generalization and preserve the original distribution attributes of the data set. In the interpolation formula, X i is replaced by the representative points; consequently, the samples are generated only between the representative samples and the samples in the original minor class, which effectively avoids the influence of boundary points. The combination of the clustering and merge operations serves to eliminate the noise points at the end of the process and reduce the complexity because there is no need to eliminate the farthest generated artificial samples after the SMOTE algorithm runs. Moreover, all the termination criteria such as reaching the pre-set number of clusters, the number of representative samples, or the distance threshold, avoid setting the k value of the original SMOTE algorithm and, thus, reduce the instability of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research concerning feature selection and parameter optimization</head><p>Classification <ref type="bibr" target="#b46">[47]</ref> and feature selection <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref> are widely applied in bioinformatics applications such as gene selection <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> and gene expression <ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref>. Chinnaswamy A <ref type="bibr" target="#b55">[56]</ref> proposed a hybrid feature selection using correlation coefficients and particle swarm optimization on microarray gene expression data. The goal of feature selection is to choose a feature subset that retains most of the information of the original dataset, especially for high-dimensional data <ref type="bibr" target="#b56">[57]</ref>. The authors of <ref type="bibr" target="#b57">[58]</ref> showed that machine-learning algorithms achieve better results after feature selection. Kausar N.</p><p>[59] proposed a scheme-based RF in which useful features were extracted from both the spatial and transform domains for medical image fusion. During the second "random" time of RF, a number of attributes were selected randomly to reduce the correlation between trees, but this operation promotes redundant features that may affect the generalization ability to some degree. Thus, new types of evaluation mechanisms were proposed based on the importance of the attributes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, using weighted features as well as costsensitivity features <ref type="bibr" target="#b61">[62]</ref>, and so on; however, their calculations are comparatively complicated. Recently, researchers have combined the RF algorithm with intelligent algorithms. Such combinations have achieved good results in a variety of fields. In <ref type="bibr" target="#b4">[5]</ref>, an improved feature selection method based on GA and RF was proposed for fault detection that significantly reduces the OOB error. The results of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> indicate that a type of hybrid PSO-RF feature selection algorithm is widely applied in certain fields. However, the works mentioned above do not involve parameter optimization.</p><p>Three main parameters influence the efficiency and performance of RF: nTree-the size of the tree, MinLeaf-the minimum sample number of leaf nodes, and κ -the attribute subset size. Previous studies have shown that the classification performance of RF is less sensitive to MinLeaf <ref type="bibr" target="#b62">[63]</ref>. A larger nTree increases the number of trees in the classifier, helps ensure the diversity of individual classifiers and, thus, improves performance. However, a larger nTree also increases the time cost and may lead to less interpretable results, while a small nTree results in increased classification errors and poor performance. Usually, κ is far less than the number of total attributes <ref type="bibr" target="#b63">[64]</ref>. When all the similar attributes are used for splitting the tree nodes in the Bagging algorithm, the effect of the tree model worsens due to the higher similarity degree among trees <ref type="bibr" target="#b64">[65]</ref>; when κ is smaller, the stronger effects of randomness lower the classification accuracy. The hyper parameter κ behaves differently for different issues <ref type="bibr" target="#b65">[66]</ref>; hence, an appropriate value can cause the algorithm to have excellent performance for a specific problem. Breiman pointed out that selecting the proper κ value has a great influence on the performance of the algorithm <ref type="bibr" target="#b2">[3]</ref> and suggested that the value should be 1,</p><formula xml:id="formula_9">ffiffiffiffiffi M p , 1 2 ffiffiffiffiffi M p , 2 ffiffiffiffiffi M p and ⌊ log 2 (M) + 1⌋. Generally, κ is fixed as ffiffiffiffiffi M p</formula><p>, but that value does not guarantee obtaining the best classifier. Therefore, the authors of <ref type="bibr" target="#b66">[67]</ref> suggested that the minimum OOB error be used to obtain the approximate value to overcome the shortcomings of the orthogonal validation method. Moreover, OOB data has been used to estimate the optimal training sample proportion to construct the Bagging classifier <ref type="bibr" target="#b67">[68]</ref>. To sum up, it is difficult for traditional parameter values to achieve an optimal performance. In terms of the search for the optimal parameter, typical approaches have incorporated exhaustive search, grid search, and orthogonal selection, but these methods have a high time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review of intelligent algorithms</head><p>Because intelligent algorithms are superior for solving NP-hard problems and for optimizing parameters, they have been the subject of many relevant and successful studies <ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref>.</p><p>The main idea behind the genetic algorithm (GA) is to encode unknown variables into chromosomes and change the objective function into fitness functions. The fitness value drives the main operations-selection, crossover and mutation-to search for the best potential individuals iteratively. Eventually the algorithm converges, and the optimal or a suboptimal solution of the problem is obtained. GA has the advantage of searching in parallel, and it is suitable for a variety of complex scenarios.</p><p>The particle swarm optimization (PSO) algorithm is theoretically simpler and more efficient than the GA <ref type="bibr" target="#b72">[73]</ref>. The main idea behind PSO is to simulate the predation behaviour of birds. Each particle represents a candidate solution and has a position, speed and a fitness value. Historical information on the optimal solution instructs the particle to fly toward a better position.</p><p>The artificial fish swarm algorithm (AFSA) <ref type="bibr" target="#b73">[74]</ref> is a novel algorithm with high potential. The main idea behind AFSA is to imitate the way that fish prey, swarm, follow and adopt random behaviours. The candidate solution is translated into the individual positions of the fish, while the objective function is converted to food concentration.</p><p>Diagrams for GA, PSO and AFSA are shown in Fig. <ref type="figure">3</ref>.</p><p>There is little research on optimizing the hyper parameter κ of random forests. In <ref type="bibr" target="#b66">[67]</ref>, the size of the decision tree is fixed at 500, but this approach achieves the optimal parameter on only half the dataset. Worse, it requires considerable time and is suitable for single parameter optimization only. This paper proposes combining a new hybrid algorithm for feature selection and parameter optimization with RF is proposed based on <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The proposed hybrid algorithm for feature selection and parameter optimization</head><p>We propose the hybrid GA-RF, PSO-RF or AFSA-RF algorithm for feature selection, parameter optimization and classification. The algorithm seeks to remove redundant features and attain the optimal feature subset and, finally, to explore the relation between performance and nTree, as well as the hyper parameter κ.</p><p>Generally, p -fold cross validation is used to traverse the parameter and to estimate the algorithm in the experiment, but time complexity is high. In this paper, OOB error replaces the cross-validation algorithm for binary classification, while the full misclassification error is used for multi-classification. Hence, the time complexity is reduced to 1/p. During the process, cross validation is required for classification.</p><p>Objective function:</p><formula xml:id="formula_10">f nTree Ã ; κ Ã ; Attribute i ji ¼ 1; 2…; M f g ð Þ ¼ arg min avgOOB error ð Þ<label>ð8Þ</label></formula><p>Studies have shown that the larger nTree is, the more stable the classification accuracy will be. We set nTree and κ in the range [0, 500] and <ref type="bibr">[1, M]</ref>, respectively, by considering both the time and space complexities.</p><p>Optimization variables: nTree, κ, {Attribute i |i = 1, 2 …, M} Binary encoding involves two tangent points and three steps. Let nTree and κ be numbers in the binary system. A value of 0 in {Attribute i |i = 1, 2 …, M} represents an unselected feature in the corresponding position, while a 1 represents the selected features.</p><p>The constraint condition is κ≤</p><formula xml:id="formula_11">X M i¼1 Attribute i .</formula><p>Then, an nTree is generated randomly between [0, 500]. Because 2 9 = 512, a 9-bit length ensures a full set of variables. The bits used for κ and the bits used for the attributes are different for different data sets. The bits of κ are the binary representation of M, while the number of bits of the attributes are M (Fig. <ref type="figure">4</ref>) . The initialization continues until a valid variable is generated.</p><p>The diagram for a hybrid algorithm based on RF and an artificial algorithm for feature selection and parameter optimization is shown in Fig. <ref type="figure" target="#fig_2">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid GA-RF</head><p>Step 1. Initialize the population: Perform binary encoding. The population size is set to popsize, the max iteration time is set to maxgen, the crossover probability is P c , and the mutation probability is P m .</p><p>Step 2. Combine the GA with RF classification and calculate the fitness function, F = max(1/f ), gen = 1.</p><p>Step 3. Perform the selection operation with the roulette method: the probability of selecting an individual is dependent on the proportion of the overall fitness value that the individual represents:</p><formula xml:id="formula_12">p i ¼ F i = X popsize i¼1 F i :<label>ð9Þ</label></formula><p>Step 4. Conduct the crossover operation with the single-point method: two selected individuals cross at a random position with different values. The offspring generation will be regenerated until it turns out to be legal. The process is shown in Fig. <ref type="figure" target="#fig_3">6</ref>.</p><p>Step 5. Mutation operation: select an individual and a position j randomly to mutate by switching 0 and 1. When a feasible solution is achieved, calculate the fitness value and update the optimal solution. The mutation operation is shown in Fig. <ref type="figure">7</ref> Step 6. When gen &gt; maxgen, the algorithm will terminate; otherwise, return to Step 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid PSO-RF</head><p>Step 1. Initialize the population. The population size is set to popsize, the max iteration time is set to maxgen, the position of the binary particle is X k = {Z k,1 , Z k,2 , …}, k = 1, 2, … popsize, the velocity is V, the learning factors are c 1 , c 2 , and the weight is w.</p><p>Step 2. Combine the PSO with RF classification and calculate the fitness function F = max(1/f ), gen = 1.</p><p>Step 3. Update the velocities V k + 1 and positions X k + 1 of particles. Let P k be the optimal position of an individual particle, Pg k be the optimal position of all particles, and rand be a random number uniformly distributed in the range (0,1):</p><formula xml:id="formula_13">V kþ1 ¼ wV k þ c 1 r 1 P k -X k À Á þ c 2 r 2 Pg k -X k À Á ; r 1 ; r 2 ∈ 0; 1 ½<label>ð10Þ</label></formula><formula xml:id="formula_14">sigmoid V kþ1 À Á ¼ 1 1 þ e -V kþ1<label>ð11Þ</label></formula><formula xml:id="formula_15">Z kþ1;j ¼ 0; 1; &amp; rand &gt; sigmoid V kþ1 À Á rand≤sigmoid V kþ1 À Á randeU 0; 1 ð Þ:<label>ð12Þ</label></formula><p>Step 4. If gen &gt; maxgen, the algorithm will terminate; otherwise, return to Step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid AFSA-RF</head><p>Step 1. Initialize the population. The population size is set to popsize, the maximum number of iterations is set to maxgen, the fish positions are X k = {Z k,1 , Z k,2 , …}, k = 1, 2, … popsize, the visual distance is visual, the crowding degree factor is delta, and the maximum number of behaviours to try is try_number.</p><p>Step 2. Combine with RF classification and calculate the food concentration F = max(1/f );</p><p>Step 3. Swarm and follow at the same time. a) Swarm behaviour: The current state of a fish is X i , the number of partners in view is nf, and the centre position is X c . When F c nf &gt; delta⋅ Fitness i , move to the centre position according  to the following formula; otherwise, conduct the prey behaviour.</p><formula xml:id="formula_16">Z kþ1;i ¼ Z k;i Z k;i ¼ Z c;i 0 Z k;i ≠Z c;i ; rand &gt; 0:5 1 Z k;i ≠Z c;i ; rand≤0:5: 8 &gt; &gt; &lt; &gt; &gt; :<label>ð13Þ</label></formula><p>b) Follow behaviour: Find the fish X max with the maximum food concentration value, F max . If F max nf &gt; delta⋅F i , move to X max and calculate the food concentration value. Then, update the food concentration value by comparing it with the value of the swarm behaviour; otherwise, conduct the prey behaviour.</p><formula xml:id="formula_17">Z kþ1;i ¼ Z k;i Z k;i ¼ Z max;i 0 Z k;i ≠Z max;i ; rand &gt; 0:5 1 Z k;i ≠Z max;i ; rand≤0:5: 8 &gt; &lt; &gt; :<label>ð14Þ</label></formula><p>c) Prey behaviour: The current state is X k = {Z k,i }, and the random selection state is X j = {Z j,i } around the vision range with d ij = visual . When F k &gt; F j ,restart to generate the next state, X k + 1 , and calculate the food concentration until try_number is reached; otherwise, terminate the prey behaviour according to the following function:</p><formula xml:id="formula_18">Z kþ1;i ¼ Z k;i Z k;i ¼ Z j;i 0 Z k;i ≠Z j;i ; rand &gt; 0:5 1 Z k;i ≠Z j;i ; rand≤0:5: 8 &gt; &lt; &gt; :<label>ð15Þ</label></formula><p>Step 4. Update the state of the optimal fish. When gen &gt; maxgen, the algorithm will terminate; otherwise, return to Step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head><p>The experiments in this paper are divided into two parts. Experiment 1 explores the validity of the CURE-SMOTE algorithm. Experiment 2 investigates the effectiveness of the hybrid algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance evaluation criteria</head><p>Referring to the evaluation used in <ref type="bibr" target="#b74">[75]</ref>, the measures of the quality of binary classification are built using a confusion matrix, where TP and FN are the numbers of correctly and incorrectly classified compounds of the actual positive class, respectively. Similarly, TN and FP denote the numbers of correctly and incorrectly classified compounds of the actual negative class.</p><p>The measures accuracy, sensitivity, specificity and precision are defined as follows.</p><formula xml:id="formula_19">Accurcacy ¼ TP þ TN ð Þ = TP þ TN þ FP þ FN ð Þ ¼ TP þ TN ð Þ =N<label>ð16Þ</label></formula><formula xml:id="formula_20">Sensitivity or Recall ¼ TP= TP þ FN ð Þ ð<label>17Þ</label></formula><formula xml:id="formula_21">Specificity ¼ TN= FP þ TN ð Þ ð<label>18Þ</label></formula><formula xml:id="formula_22">Precision ¼ TP= TP þ FP ð Þ ð<label>19Þ</label></formula><p>The classifiers may have a high overall accuracy with 100% accuracy in the majority class while achieving only a 0-10% accuracy in the minority class because the overall accuracy is biased towards the majority class. Hence, the accuracy measure is not a proper evaluation metric for the imbalanced class problem. Instead, we suggest using F-value, Geometric Mean (G-mean) and AUC for imbalanced data evaluations.</p><p>The F-value measure is defined following <ref type="bibr" target="#b25">[26]</ref>. A larger F-value indicates a better F-value is a performance metric that links both precision and recall:</p><formula xml:id="formula_23">F ¼ 2 1=Precision þ 1=Recall :<label>ð20Þ</label></formula><p>The G-mean <ref type="bibr" target="#b75">[76]</ref> attempts to maximize the accuracy across the two classes with a good balance and is defined as follows. Only when both sensitivity and specificity are high can the G-mean attain its maximum, which indicates a better classifier:</p><formula xml:id="formula_24">G-mean ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi SensitivityÁSpecificity p :<label>ð21Þ</label></formula><p>AUC is the area under the receiver operating characteristics (ROC) curve. AUC has been shown to be a reliable performance measure for imbalanced and cost-   sensitive problems. An AUC-based permutation variable is presented in <ref type="bibr" target="#b76">[77]</ref>; this approach is more efficient than the approach based on the OOB error.</p><p>The training set is obtained by using the bootstrap method. Because of repeated extraction, it contains only 63% of the original data; the 37% of the data that never appear are called "out-of--bag" (OOB) data <ref type="bibr" target="#b77">[78]</ref>. OOB estimation is an unbiased estimate of the RF algorithm and can be used to measure the classifier's generalization ability. A smaller OOB error indicates a better classification performance. OOB error is defined as follows:</p><formula xml:id="formula_25">OOB error ¼ X nTree i OOB error i =nTree:<label>ð22Þ</label></formula><p>Margin is a new evaluation criterion that has been applied to the classification of remote sensing data <ref type="bibr" target="#b78">[79]</ref>. The larger the margin is, the higher the classifier's credibility is:</p><formula xml:id="formula_26">margin ¼ X nTree i margin i =nTree:<label>ð23Þ</label></formula><p>Experiment 1 and parameter settings</p><p>The experiments were implemented using Matlab 2012a on a workstation with a 64-bit operating system, 2 GB of RAM and a 2.53 GHz CPU. Artificial Data Circle and UCI imbalanced datasets were selected for the experiments. More detailed information about five datasets is listed in Table <ref type="table" target="#tab_1">2</ref>. To simulate the actual situation appropriately and preserve the degree of imbalance of the original data, the training set and testing set were divided using stratified random sampling at a ratio of 3:1, except for SPECT. The SPECT.test dataset incorporates 187 samples, and the proportions of the classes labelled 1 and 0 are 84:103, respectively. The tree size is 100 and the depth is 20.</p><p>To verify the effectiveness of the CURE-SMOTE algorithm it was compared with the original data, random oversampling, SMOTE, Borderline-SMOTE1, safe-level SMOTE, C-SMOTE (using mean value as the centre) and k-means-SMOTE (shown in Table <ref type="table" target="#tab_2">3</ref>) algorithms. To evaluate the performance of the different algorithms, F-value, G-mean, AUC and OOB error are used as performance measures. The results of each experiment were averaged over 100 runs to eliminate random effects.</p><p>To facilitate the comparisons, m and k were set to 20 and 5, respectively, in SMOTE, Borderline-SMOTE1 and  From the classification results obtained by the different sampling algorithms discussed in Table <ref type="table" target="#tab_3">4</ref>, the best F-value, G-mean and AUC were achieved on the Circle dataset by CURE-SMOTE, and its OOB error is second-best, behind only random sampling. The overall classification result on the bloodtransfusion dataset is poorer, but the CURE-SMOTE algorithm achieves the best F-value, G-mean and AUC, while its OOB error is inferior to the original data. On the Haberman's survival dataset, the F-value, G-mean and AUC achieved by CURE-SMOTE are superior to the other sampling algorithms.</p><p>For the breast-cancer-wisconsin dataset, CURE-SMOTE achieves the best F-value, but its G-mean and AUC are slightly lower, although they are little different from the other sampling algorithms. On the SPECT dataset, CURE-SMOTE surpasses the other sampling algorithms with regard to F-value, G-mean, AUC and OOB error The best value of every performance evaluation criteria obtained by the algorithms are marked in boldface safe-level-SMOTE. The number of clusters in C-SMOTE and k-means-SMOTE were set to five. Following the suggested setting for the CURE algorithm, the cluster results are better when the constriction factor is in the range [0.2, 0.7] and when the number of representative points is greater than 10. Thus, the constriction factor was set to 0.5 and the number of representative points was set to 15. The number of clusters was set to two in the circle, while the others were all five. Samples were removed when the number of representative points did not increase for ten iterations or when the sample size of the cluster class was less than 1/(10c) of the total sample size when clustering was complete. In the experiments in this paper, IR 0 was fixed at 0.7. The CURE-SMOTE algorithm diagram is depicted in Fig. <ref type="figure">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion of CURE -SMOTE algorithm</head><p>Figure <ref type="figure">9</ref> shows the results of the original data, random sampling, SMOTE sampling, Borderline-SMOTE1 sampling, safe-level SMOTE sampling, C-SMOTE sampling, K-means SMOTE sampling and CURE-SMOTE sampling, as well as the CURE clustering result. The black circles and the red star represent the major class sample and minor class sample, respectively, in the original data, and the blue squares represent the artificial samples generated by different methods. Figure <ref type="figure" target="#fig_5">10</ref> shows the CURE clustering results of the minor class sample. The clustering centre is two, the stars show the centres, and the blue diamonds indicate the representative points.</p><p>Figure <ref type="figure">9</ref> shows that a large number of data are obtained repeatedly by random sampling, and some data are not selected at all. The SMOTE algorithm also produces repeated data and generates mixed data in other classes as well as noise. Borderline-SMOTE1 picks out the boundary point of minor class by calculating and comparing the samples of the major class around the minor class; consequently, the generated data are concentrated primarily at the edges of the class. Safe-level SMOTE follows the original distribution, but still generates repeated points and distinguishes the boundary incorrectly. Although C-SMOTE can erase the noise, the generated data are too close to the centre to accurately identify other centres. K-means-SMOTE can identify the area of the small class and slightly improves on the SMOTE effect. The proposed CURE-SMOTE algorithm generates data both near the centre and the representative points; overall, it follows the original distribution. Moreover, the representative points help to avoid noise being treated as a constraining boundary during the generating process. Detailed results are listed in Table <ref type="table" target="#tab_3">4</ref>.</p><p>In conclusion, the classification results of the CURE-SMOTE algorithm as measured by the F-value, Gmeans, and AUC are substantially enhanced, whereas the results using SMOTE alone are not particularly stable. Meanwhile, Borderline-SMOTE1, C-SMOTE, and the k-means-SMOTE algorithm are even worse than random sampling on some datasets. Thus, the CURE-SMOTE algorithm combined with RF has a substantial effect on classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2 and parameter settings</head><p>In this section, to test the effectiveness of the hybrid algorithm for feature selection and parameter optimization, we selected the representative binary classification and multi-classification imbalanced datasets shown in Table <ref type="table" target="#tab_4">5</ref>. These data are randomly stratified by sampling them into four parts with a training set to testing set ratio of 3:1. In this procedure, 4-fold stratified cross validation is used for classification. The parameter settings are listed in Table <ref type="table" target="#tab_5">6</ref>. The depth is set to 20 for experiment 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion of the hybrid algorithm</head><p>According to the proposed settings in previous works, the parameters for all of the methods were set as follows: nTree = 100, κ =1, ffiffiffiffiffi M p , ⌊ log 2 (M) + 1⌋ and M. Accuracy, OOB error and margin were selected as the evaluation criteria. The detailed results are listed in Table <ref type="table" target="#tab_6">7</ref> and Table <ref type="table" target="#tab_8">8</ref>.  From the Connectionist Bench results, we find that the AFSA-RF achieves the minimum OOB error and the maximum margin. The best parameter combination is (151,4), and κ is the same as the traditional value, ffiffiffiffiffi M p . The features selected by AFSA-RF were [1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1], meaning that the 7th and 10th features were removed. PSO-RF obtained the best F-value, G-mean and AUC. On the wine dataset, PSO-RF achieved the minimum OOB error and the maximum G-mean and AUC scores. The best parameter combination is (354,1), and κ is the same as the traditional value, 1. There are 15 features selected in total. Moreover, GA-RF achieved the best F-value and AFSA-RF achieved the best margin. For Ionosphere, we find that GA-RF achieved the best OOB error, F-value and margin. The best parameter combination is (339,9), but the value of κ is considerably different from the classic value. There are 29 total features selected. The best G-mean and AUC scores were obtained by AFSA-RF. For breast-cancerwisconsin, we GA-RF achieved the best performance for OOB error and margin. The best parameter combination is (319,3), and κ is the same as the traditional value, ffiffiffiffiffi M p . There are nine features selected in total. PSO-RF achieved the maximum F-value, G-mean and AUC.</p><p>The multi-classification results show that the hybrid GA-RF, PSO-RF and AFSA-RF almost always discover better features and select better parameter values than the traditional value. There, are some differences between the best κ and the traditional value. The more features there are originally, the greater the number of redundant features that are removed.</p><p>Figure <ref type="figure" target="#fig_6">11</ref> demonstrates that, overall, the OOB error values for all the hybrid algorithms are lower than the traditional value with fixed parameters for the six datasets. Although the traditional value is reasonable for some datasets, it fails to achieve good performance over the entire problem set. In conclusion, the hybrid algorithm effectively eliminates redundant features and The best value of every performance evaluation criteria obtained by the algorithms are marked in boldface obtains a suitable combination of parameters. Therefore, it enhances the classification performance of RF on imbalanced high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>To improve the performance of the random forests algorithm, the CURE-SMOTE algorithm is proposed for imbalanced data classification. The experiments show that the proposed algorithm effectively resolves the shortcomings of the original SMOTE algorithm for typical datasets and that various adaptive clustering techniques can be added to further improve the algorithm. We plan to continue to study the influence of feature selection and parameter settings on RF. The proposed hybrids of RF with intelligent algorithms are used to optimize RF for feature selection and parameter optimization. Simulation results show that the hybrid algorithms achieve the minimum OOB error, the best generalization ability and that their F-value, G-mean and AUC scores are generally better than those obtained using traditional values. The hybrid algorithm provides new effective guidance for feature selection and parameter optimization. The time and data dimensions of the experiments can be increased to further verify the algorithm's effectiveness. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 Fig. 2</head><label>12</label><figDesc>Fig. 1 Random forests algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 Fig. 3</head><label>43</label><figDesc>Fig. 4 Binary coding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig.5The diagram of a hybrid algorithm based on RF and an artificial algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Crossover operation Fig. 7 Mutation operation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 Fig. 9</head><label>89</label><figDesc>Fig. 8 CURE-SMOTE algorithm diagram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10</head><label>10</label><figDesc>Fig.10The CURE clustering result</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11</head><label>11</label><figDesc>Fig. 11 Comparison of OOB errors among different methods and datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Definitions in Borderline-SMOTE 1</figDesc><table><row><cell>Point</cell><cell>Definition</cell></row><row><cell>Noisy point</cell><cell>m = k</cell></row><row><cell>Boundary point/dangerous point</cell><cell>m/2 ≤ k &lt; m</cell></row><row><cell>Safe point</cell><cell>0 ≤ k &lt; m/2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 Dataset</head><label>2</label><figDesc></figDesc><table><row><cell>Id Dataset</cell><cell>N</cell><cell>M Positive</cell><cell>Negative</cell><cell>IR</cell><cell>Label</cell></row><row><cell></cell><cell></cell><cell>class</cell><cell>class</cell><cell></cell><cell></cell></row><row><cell>1 Circle</cell><cell cols="2">1362 2 229</cell><cell>1133</cell><cell cols="2">0.2021:1 1:0</cell></row><row><cell>2 Blood-transfusion</cell><cell cols="2">748 4 178</cell><cell>570</cell><cell cols="2">0.3123:1 4:2</cell></row><row><cell cols="3">3 Haberman's survival 306 3 81</cell><cell>225</cell><cell>0.36:1</cell><cell>2:1</cell></row><row><cell>4 Breast-cancer-</cell><cell cols="2">702 10 243</cell><cell>459</cell><cell cols="2">0.5249:1 1:0</cell></row><row><cell>wisconsin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 SPECT.train</cell><cell>80</cell><cell>23 26</cell><cell>54</cell><cell cols="2">0.4815 1:0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparison of algorithms and references</figDesc><table><row><cell>Algorithm</cell><cell>Reference</cell><cell>Algorithm</cell><cell>Reference</cell></row><row><cell>SMOTE</cell><cell>[32]</cell><cell>Safe-level SMOTE</cell><cell>[36]</cell></row><row><cell>Borderline-SMOTE 1</cell><cell>[35]</cell><cell>C-SMOTE</cell><cell>[36]</cell></row><row><cell>k-means-SMOTE</cell><cell>[37]</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>The classification results of different sampling algorithms</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>F</cell><cell>G-Mean</cell><cell>AUC</cell><cell>OOB error</cell></row><row><cell>1. Circle</cell><cell>Original data</cell><cell>0.9081</cell><cell>0.9339</cell><cell>0.9389</cell><cell>0.0296</cell></row><row><cell></cell><cell>Random oversampling</cell><cell>0.9249</cell><cell>0.9553</cell><cell>0.9567</cell><cell>0.0163</cell></row><row><cell></cell><cell>SMOTE</cell><cell>0.9086</cell><cell>0.9535</cell><cell>0.9579</cell><cell>0.0384</cell></row><row><cell></cell><cell>Borderline-SMOTE1</cell><cell>0.9110</cell><cell>0.9534</cell><cell>0.9619</cell><cell>0.0438</cell></row><row><cell></cell><cell>Safe-level-SMOTE</cell><cell>0.9146</cell><cell>0.9595</cell><cell>0.9559</cell><cell>0.0431</cell></row><row><cell></cell><cell>C-SMOTE</cell><cell>0.9302</cell><cell>0.9713</cell><cell>0.9813</cell><cell>0.0702</cell></row><row><cell></cell><cell>k-means-SMOTE</cell><cell>0.9262</cell><cell>0.9589</cell><cell>0.9602</cell><cell>0.0323</cell></row><row><cell></cell><cell>CURE-SMOTE</cell><cell>0.9431</cell><cell>0.9808</cell><cell>0.9855</cell><cell>0.0323</cell></row><row><cell>2. Blood-transfusion</cell><cell>Original data</cell><cell>0.3509</cell><cell>0.5094</cell><cell>0.5083</cell><cell>0.2548</cell></row><row><cell></cell><cell>Random oversampling</cell><cell>0.3903</cell><cell>0.5490</cell><cell>0.5449</cell><cell>0.2250</cell></row><row><cell></cell><cell>SMOTE</cell><cell>0.4118</cell><cell>0.5798</cell><cell>0.5537</cell><cell>0.2152</cell></row><row><cell></cell><cell>Borderline-SMOTE1</cell><cell>0.4185</cell><cell>0.5832</cell><cell>0.5424</cell><cell>0.1630</cell></row><row><cell></cell><cell>Safe-level-SMOTE</cell><cell>0.4494</cell><cell>0.6174</cell><cell>0.5549</cell><cell>0.2479</cell></row><row><cell></cell><cell>C-SMOTE</cell><cell>0.4006</cell><cell>0.5549</cell><cell>0.5531</cell><cell>0.2418</cell></row><row><cell></cell><cell>k-means-SMOTE</cell><cell>0.4157</cell><cell>0.5941</cell><cell>0.5433</cell><cell>0.1872</cell></row><row><cell></cell><cell>CURE-SMOTE</cell><cell>0.5393</cell><cell>0.6719</cell><cell>0.6533</cell><cell>0.2531</cell></row><row><cell>3. Haberman's survival</cell><cell>Original data</cell><cell>0.3279</cell><cell>0.5018</cell><cell>0.6063</cell><cell>0.3149</cell></row><row><cell></cell><cell>Random oversampling</cell><cell>0.3504</cell><cell>0.5178</cell><cell>0.5959</cell><cell>0.1534</cell></row><row><cell></cell><cell>SMOTE</cell><cell>0.4350</cell><cell>0.5971</cell><cell>0.6259</cell><cell>0.1728</cell></row><row><cell></cell><cell>Borderline-SMOTE1</cell><cell>0.4523</cell><cell>0.6119</cell><cell>0.6298</cell><cell>0.2589</cell></row><row><cell></cell><cell>Safe-level-SMOTE</cell><cell>0.4762</cell><cell>0.6008</cell><cell>0.6030</cell><cell>0.3077</cell></row><row><cell></cell><cell>C-SMOTE</cell><cell>0.4528</cell><cell>0.5487</cell><cell>0.5656</cell><cell>0.2780</cell></row><row><cell></cell><cell>k-means-SMOTE</cell><cell>0.4685</cell><cell>0.6249</cell><cell>0.6328</cell><cell>0.1828</cell></row><row><cell></cell><cell>CURE-SMOTE</cell><cell>0.5000</cell><cell>0.6282</cell><cell>0.6940</cell><cell>0.2717</cell></row><row><cell>4. Breast-cancer-wisconsin</cell><cell>Original data</cell><cell>0.9486</cell><cell>0.9619</cell><cell>0.9491</cell><cell>0.0446</cell></row><row><cell></cell><cell>Random oversampling</cell><cell>0.9451</cell><cell>0.9623</cell><cell>0.9620</cell><cell>0.0301</cell></row><row><cell></cell><cell>SMOTE</cell><cell>0.9502</cell><cell>0.9666</cell><cell>0.9627</cell><cell>0.0341</cell></row><row><cell></cell><cell>Borderline-SMOTE1</cell><cell>0.9506</cell><cell>0.9661</cell><cell>0.9635</cell><cell>0.0379</cell></row><row><cell></cell><cell>Safe-level-SMOTE</cell><cell>0.9509</cell><cell>0.9671</cell><cell>0.9638</cell><cell>0.0404</cell></row><row><cell></cell><cell>C-SMOTE</cell><cell>0.9491</cell><cell>0.9636</cell><cell>0.9561</cell><cell>0.0380</cell></row><row><cell></cell><cell>k-means-SMOTE</cell><cell>0.9449</cell><cell>0.9616</cell><cell>0.9562</cell><cell>0.0373</cell></row><row><cell></cell><cell>CURE-SMOTE</cell><cell>0.9511</cell><cell>0.9664</cell><cell>0.9621</cell><cell>0.0427</cell></row><row><cell>5. SPECT.train</cell><cell>Original data</cell><cell>0.6348</cell><cell>0.6764</cell><cell>0.6579</cell><cell>0.3634</cell></row><row><cell></cell><cell>Random oversampling</cell><cell>0.6539</cell><cell>0.6924</cell><cell>0.6753</cell><cell>0.3468</cell></row><row><cell></cell><cell>SMOTE</cell><cell>0.6618</cell><cell>0.6990</cell><cell>0.6825</cell><cell>0.3688</cell></row><row><cell></cell><cell>Borderline-SMOTE1</cell><cell>0.6710</cell><cell>0.6926</cell><cell>0.6746</cell><cell>0.3489</cell></row><row><cell></cell><cell>Safe-level-SMOTE</cell><cell>0.6770</cell><cell>0.7074</cell><cell>0.6913</cell><cell>0.3160</cell></row><row><cell></cell><cell>C-SMOTE</cell><cell>0.6564</cell><cell>0.6936</cell><cell>0.6764</cell><cell>0.3448</cell></row><row><cell></cell><cell>k-means-SMOTE</cell><cell>0.6796</cell><cell>0.6941</cell><cell>0.6846</cell><cell>0.3599</cell></row><row><cell></cell><cell>CURE-SMOTE</cell><cell>0.6855</cell><cell>0.7155</cell><cell>0.6951</cell><cell>0.1108</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 Dataset</head><label>5</label><figDesc></figDesc><table><row><cell>id Dataset</cell><cell>N</cell><cell>M Positive</cell><cell>Negative</cell><cell>IR</cell><cell>Label</cell></row><row><cell></cell><cell></cell><cell>class</cell><cell>class</cell><cell></cell><cell></cell></row><row><cell>1 Connectionist</cell><cell cols="2">208 17 97</cell><cell>111</cell><cell cols="2">0.8739 R:M</cell></row><row><cell>Bench</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 Wine</cell><cell cols="2">130 13 59</cell><cell>71</cell><cell cols="2">0.831 1:2</cell></row><row><cell>3 Ionosphere</cell><cell cols="2">351 34 126</cell><cell>225</cell><cell>0.56</cell><cell>b:g</cell></row><row><cell>4 Breast-cancer-</cell><cell cols="2">702 10 243</cell><cell>459</cell><cell cols="2">0.5249 1:0</cell></row><row><cell>wisconsin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">5 Steel Plates Faults 1,941 27 -</cell><cell>-</cell><cell>-</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>labels</cell></row><row><cell>6 Libras Movement</cell><cell cols="2">360 90 -</cell><cell>-</cell><cell>-</cell><cell>15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>labels</cell></row><row><cell>7 mfeat-factors</cell><cell cols="2">2,000 216 -</cell><cell>-</cell><cell>-</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>labels</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Parameter settings</figDesc><table><row><cell>Hybrid GA-RF</cell><cell>popsize :5</cell><cell>maxgen :20</cell><cell>Pc: 0.6</cell><cell>Pm:0.1</cell><cell></cell><cell></cell></row><row><cell>Hybrid PSO-RF</cell><cell>popsize :5</cell><cell>maxgen :20</cell><cell>c 1 :1.5</cell><cell>r 1 ,r 2 ∈[0,1]</cell><cell>Vmin:Vmax = -0.5:0.5</cell><cell>w:0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>c 2 :1.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hybrid AFSA-RF</cell><cell>popsize: 5</cell><cell>maxgen: 20</cell><cell>visual: 3</cell><cell cols="2">try_number: 5, delta: 0.618</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>The binary classification results</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell>ffiffiffiffi M p</cell><cell>⌊ log 2 (M) + 1⌋</cell><cell>M</cell><cell>GA-RF</cell><cell>PSO-RF</cell><cell>AFSA-RF</cell></row><row><cell>Connectionist Bench</cell><cell>Accuracy</cell><cell>0.6442</cell><cell>0.6442</cell><cell>0.6058</cell><cell>0.6635</cell><cell>0.6538</cell><cell>0.7308</cell><cell>0.6827</cell></row><row><cell></cell><cell>Sensitive</cell><cell>0.5882</cell><cell>0.6122</cell><cell>0.6500</cell><cell>0.7556</cell><cell>0.5741</cell><cell>0.6744</cell><cell>0.5870</cell></row><row><cell></cell><cell>Precision</cell><cell>0.6522</cell><cell>0.6250</cell><cell>0.4906</cell><cell>0.5862</cell><cell>0.7045</cell><cell>0.6744</cell><cell>0.6585</cell></row><row><cell></cell><cell>Specificity</cell><cell>0.6981</cell><cell>0.6727</cell><cell>0.5781</cell><cell>0.5932</cell><cell>0.7400</cell><cell>0.7705</cell><cell>0.7586</cell></row><row><cell></cell><cell>F</cell><cell>0.6186</cell><cell>0.6186</cell><cell>0.5591</cell><cell>0.6602</cell><cell>0.6327</cell><cell>0.6744</cell><cell>0.6207</cell></row><row><cell></cell><cell>G-mean</cell><cell>0.6408</cell><cell>0.6418</cell><cell>0.6130</cell><cell>0.6695</cell><cell>0.6518</cell><cell>0.7209</cell><cell>0.6673</cell></row><row><cell></cell><cell>AUC</cell><cell>0.4107</cell><cell>0.4119</cell><cell>0.3758</cell><cell>0.4482</cell><cell>0.4248</cell><cell>0.5196</cell><cell>0.4453</cell></row><row><cell></cell><cell>OOB</cell><cell>0.3808</cell><cell>0.3889</cell><cell>0.3344</cell><cell>0.3391</cell><cell>0.3314</cell><cell>0.3085</cell><cell>0.2932</cell></row><row><cell></cell><cell>margin</cell><cell>0.1078</cell><cell>0.1632</cell><cell>0.1991</cell><cell>0.2084</cell><cell>0.2056</cell><cell>0.1468</cell><cell>0.2418</cell></row><row><cell></cell><cell>nTree</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>315</cell><cell>193</cell><cell>151</cell></row><row><cell></cell><cell>κ</cell><cell>1</cell><cell>4</cell><cell>5</cell><cell>1 7</cell><cell>6</cell><cell>8</cell><cell>4</cell></row><row><cell></cell><cell>num (Attribute)</cell><cell>17</cell><cell>17</cell><cell>17</cell><cell>17</cell><cell>13</cell><cell>16</cell><cell>15</cell></row><row><cell>Wine</cell><cell>Accuracy</cell><cell>0.9846</cell><cell>0.9692</cell><cell>0.9846</cell><cell>0.9692</cell><cell>0.9846</cell><cell>0.9846</cell><cell>0.9692</cell></row><row><cell></cell><cell>Sensitive</cell><cell>1.0000</cell><cell>0.9286</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>Precision</cell><cell>0.9655</cell><cell>1.0000</cell><cell>0.9677</cell><cell>0.9333</cell><cell>0.9706</cell><cell>0.9643</cell><cell>0.9355</cell></row><row><cell></cell><cell>Specificity</cell><cell>0.9730</cell><cell>1.0000</cell><cell>0.9714</cell><cell>0.9459</cell><cell>0.9688</cell><cell>0.9737</cell><cell>0.9444</cell></row><row><cell></cell><cell>F</cell><cell>0.9825</cell><cell>0.9630</cell><cell>0.9836</cell><cell>0.9655</cell><cell>0.9851</cell><cell>0.9818</cell><cell>0.9667</cell></row><row><cell></cell><cell>G-mean</cell><cell>0.9864</cell><cell>0.9636</cell><cell>0.9856</cell><cell>0.9726</cell><cell>0.9843</cell><cell>0.9868</cell><cell>0.9718</cell></row><row><cell></cell><cell>AUC</cell><cell>0.9730</cell><cell>0.9286</cell><cell>0.9714</cell><cell>0.9459</cell><cell>0.9688</cell><cell>0.9737</cell><cell>0.9444</cell></row><row><cell></cell><cell>OOB</cell><cell>0.0442</cell><cell>0.0502</cell><cell>0.0288</cell><cell>0.0748</cell><cell>0.0246</cell><cell>0.0156</cell><cell>0.0238</cell></row><row><cell></cell><cell>margin</cell><cell>0.6951</cell><cell>0.7553</cell><cell>0.8149</cell><cell>0.7995</cell><cell>0.7863</cell><cell>0.7890</cell><cell>0.8345</cell></row><row><cell></cell><cell>nTree</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>349</cell><cell>354</cell><cell>90</cell></row><row><cell></cell><cell>κ</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>1 3</cell><cell>5</cell><cell>1</cell><cell>5</cell></row><row><cell></cell><cell>num (Attribute)</cell><cell>13</cell><cell>13</cell><cell>13</cell><cell>13</cell><cell>12</cell><cell>11</cell><cell>12</cell></row><row><cell>Ionosphere</cell><cell>Accuracy</cell><cell>0.9200</cell><cell>0.9314</cell><cell>0.9371</cell><cell>0.9257</cell><cell>0.9371</cell><cell>0.9257</cell><cell>0.9314</cell></row><row><cell></cell><cell>Sensitive</cell><cell>0.9107</cell><cell>0.8475</cell><cell>0.8889</cell><cell>0.8824</cell><cell>0.8333</cell><cell>0.9032</cell><cell>0.9107</cell></row><row><cell></cell><cell>Precision</cell><cell>0.8500</cell><cell>0.9434</cell><cell>0.9057</cell><cell>0.9231</cell><cell>0.9804</cell><cell>0.8889</cell><cell>0.8793</cell></row><row><cell></cell><cell>Specificity</cell><cell>0.9244</cell><cell>0.9741</cell><cell>0.9587</cell><cell>0.9533</cell><cell>0.9913</cell><cell>0.9381</cell><cell>0.9412</cell></row><row><cell></cell><cell>F</cell><cell>0.8793</cell><cell>0.8929</cell><cell>0.8972</cell><cell>0.9003</cell><cell>0.9009</cell><cell>0.8960</cell><cell>0.8947</cell></row><row><cell></cell><cell>G-mean</cell><cell>0.9175</cell><cell>0.9086</cell><cell>0.9231</cell><cell>0.9171</cell><cell>0.9089</cell><cell>0.9205</cell><cell>0.9258</cell></row><row><cell></cell><cell>AUC</cell><cell>0.8956</cell><cell>0.8651</cell><cell>0.9002</cell><cell>0.8975</cell><cell>0.8548</cell><cell>0.8835</cell><cell>0.9029</cell></row><row><cell></cell><cell>OOB</cell><cell>0.1096</cell><cell>0.0860</cell><cell>0.1132</cell><cell>0.0884</cell><cell>0.0668</cell><cell>0.0831</cell><cell>0.0825</cell></row><row><cell></cell><cell>margin</cell><cell>0.5696</cell><cell>0.6918</cell><cell>0.6511</cell><cell>0.7041</cell><cell>0.7349</cell><cell>0.6934</cell><cell>0.6351</cell></row><row><cell></cell><cell>nTree</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>339</cell><cell>321</cell><cell>350</cell></row><row><cell></cell><cell>κ</cell><cell>1</cell><cell>5</cell><cell>6</cell><cell>3 4</cell><cell>9</cell><cell>15</cell><cell>2</cell></row><row><cell></cell><cell>num (Attribute)</cell><cell>34</cell><cell>34</cell><cell>34</cell><cell>34</cell><cell>29</cell><cell>30</cell><cell>28</cell></row><row><cell>Breast -cancer -wisconsin</cell><cell>Accuracy</cell><cell>0.9801</cell><cell>0.9658</cell><cell>0.9715</cell><cell>0.9573</cell><cell>0.9544</cell><cell>0.9801</cell><cell>0.9658</cell></row><row><cell></cell><cell>Sensitive</cell><cell>0.9914</cell><cell>0.9474</cell><cell>0.9583</cell><cell>0.9748</cell><cell>0.9919</cell><cell>1.0000</cell><cell>0.9474</cell></row><row><cell></cell><cell>Precision</cell><cell>0.9504</cell><cell>0.9474</cell><cell>0.9583</cell><cell>0.9063</cell><cell>0.8905</cell><cell>0.9421</cell><cell>0.9474</cell></row><row><cell></cell><cell>Specificity</cell><cell>0.9745</cell><cell>0.9747</cell><cell>0.9784</cell><cell>0.9483</cell><cell>0.9342</cell><cell>0.9705</cell><cell>0.9747</cell></row><row><cell></cell><cell>F</cell><cell>0.9701</cell><cell>0.9474</cell><cell>0.9583</cell><cell>0.9393</cell><cell>0.9385</cell><cell>0.9702</cell><cell>0.9474</cell></row><row><cell></cell><cell>G-mean</cell><cell>0.9829</cell><cell>0.9609</cell><cell>0.9683</cell><cell>0.9614</cell><cell>0.9626</cell><cell>0.9851</cell><cell>0.9609</cell></row><row><cell></cell><cell>AUC</cell><cell>0.9844</cell><cell>0.9555</cell><cell>0.9595</cell><cell>0.9547</cell><cell>0.9601</cell><cell>0.9850</cell><cell>0.9474</cell></row><row><cell></cell><cell>OOB</cell><cell>0.0422</cell><cell>0.0399</cell><cell>0.0433</cell><cell>0.0467</cell><cell>0.0304</cell><cell>0.0411</cell><cell>0.0372</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>The binary classification results (Continued)</figDesc><table><row><cell>margin</cell><cell>0.8247</cell><cell>0.8569</cell><cell>0.8509</cell><cell>0.8652</cell><cell>0.8842</cell><cell>0.8179</cell><cell>0.8616</cell></row><row><cell>nTree</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>319</cell><cell>420</cell><cell>351</cell></row><row><cell>κ</cell><cell>1</cell><cell>3</cell><cell>4</cell><cell>1 0</cell><cell>3</cell><cell>1</cell><cell>3</cell></row><row><cell>num (Attribute)</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>9</cell><cell>9</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>The multi-classification results</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell>ffiffiffiffi M p</cell><cell>⌊ log 2 (M) + 1⌋</cell><cell>M</cell><cell>GA-RF</cell><cell>PSO-RF</cell><cell>AFSA-RF</cell></row><row><cell>Steel Plates Faults</cell><cell>Accuracy</cell><cell>0.7464</cell><cell>0.7485</cell><cell>0.7598</cell><cell>0.7814</cell><cell>0.7881</cell><cell>0.7998</cell><cell>0.7914</cell></row><row><cell></cell><cell>OOB</cell><cell>0.3152</cell><cell>0.2819</cell><cell>0.2746</cell><cell>0.2640</cell><cell>0.2437</cell><cell>0.2276</cell><cell>0.2115</cell></row><row><cell></cell><cell>margin</cell><cell>0.2456</cell><cell>0.3384</cell><cell>0.3484</cell><cell>0.3789</cell><cell>0.3803</cell><cell>0.3812</cell><cell>0.3810</cell></row><row><cell></cell><cell>nTree</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>397</cell><cell>283</cell><cell>400</cell></row><row><cell></cell><cell>κ</cell><cell>1</cell><cell>5</cell><cell>5</cell><cell>2 7</cell><cell>8</cell><cell>6</cell><cell>6</cell></row><row><cell></cell><cell>num (Attribute)</cell><cell>27</cell><cell>27</cell><cell>27</cell><cell>27</cell><cell>23</cell><cell>22</cell><cell>22</cell></row><row><cell>Libras Movement</cell><cell>Accuracy</cell><cell>0.7167</cell><cell>0.7556</cell><cell>0.6889</cell><cell>0.6444</cell><cell>0.7606</cell><cell>0.7767</cell><cell>0.7928</cell></row><row><cell></cell><cell>OOB</cell><cell>0.3546</cell><cell>0.3397</cell><cell>0.3480</cell><cell>0.3163</cell><cell>0.3030</cell><cell>0.3323</cell><cell>0.3116</cell></row><row><cell></cell><cell>margin</cell><cell>0.1464</cell><cell>0.1798</cell><cell>0.1990</cell><cell>0.2180</cell><cell>0.2443</cell><cell>0.2677</cell><cell>0.2910</cell></row><row><cell></cell><cell>nTree</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>258</cell><cell>348</cell><cell>135</cell></row><row><cell></cell><cell>κ</cell><cell>1</cell><cell>9</cell><cell>7</cell><cell>9 0</cell><cell>12</cell><cell>8</cell><cell>9</cell></row><row><cell></cell><cell>num (Attribute)</cell><cell>90</cell><cell>90</cell><cell>90</cell><cell>90</cell><cell>56</cell><cell>76</cell><cell>49</cell></row><row><cell>mfeat-fac</cell><cell>Accuracy</cell><cell>0.4280</cell><cell>0.9030</cell><cell>0.8010</cell><cell>0.9620</cell><cell>0.9673</cell><cell>0.9600</cell><cell>0.9611</cell></row><row><cell></cell><cell>OOB</cell><cell>0.6949</cell><cell>0.1823</cell><cell>0.3192</cell><cell>0.0486</cell><cell>0.0416</cell><cell>0.0410</cell><cell>0.0361</cell></row><row><cell></cell><cell>margin</cell><cell>-0.0987</cell><cell>0.4561</cell><cell>0.2361</cell><cell>0.8708</cell><cell>0.8749</cell><cell>0.8615</cell><cell>0.8698</cell></row><row><cell></cell><cell>nTree</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>377</cell><cell>270</cell><cell>196</cell></row><row><cell></cell><cell>κ</cell><cell>1</cell><cell>15</cell><cell>8</cell><cell>215</cell><cell>14</cell><cell>18</cell><cell>11</cell></row><row><cell></cell><cell>num (Attribute)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="215" xml:id="foot_0"><p>215 215 145 112</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>The best value of every performance evaluation criteria obtained by the algorithms are marked in boldface</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the editor and the anonymous reviewers for their helpful suggestions and comments which provide a great contribution to the research of this paper, and Wenxing Ye for linguistic improvements of the paper.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>All data generated or analysed during this study are included in this published article. The datasets used and/or analysed during the current study available from the corresponding author on reasonable request.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work is supported in part by the National Natural Science Foundation of China (Grant No. 61572233) and the National Social Science Foundation of China (Grant No. 16BTJ032).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>AFSA: Artificial fish swarm algorithm; AFSA-RF: Artificial fish swarmrandom forests algorithm; AUC: Area under the ROC curve; Can-CSC-GBE: Cost-Sensitive Classifier with a GentleBoost Ensemble; CURE: Clustering using representatives; GA: Genetic algorithm; GA-RF: Genetic-random forests; G-mean: Geometric mean; IR: Imbalance ratio; MTD: Mega-trend-diffusion; OOB: Out of bag; PSO: Particle swarm optimization; PSO-RF: Particle swarm-random forests; RBFNN: Radial basis function neural network; RF: Random forests; ROC: Receiver operating characteristics; SMOTE: Enhances the original synthetic minority oversampling technique; SVM: Support vector machine Authors' contributions LM wrote the paper and conducted all analyses. SHF developed the paper. Both authors contributed to the design of the analyses and substantially edited the manuscript. Both authors read and approved he final manuscript</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication Not applicable.</head><p>Ethics approval and consent to participate Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random decision forests [C]//Document Analysis and Recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on IEEE</title>
		<meeting>the Third International Conference on IEEE</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prediction of O-glycosylation sites using random forest and GA-tuned PSO technique</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Abdelhalim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform Biol Insights</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fault diagnosis in spur gears based on genetic algorithm and random forest</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cerrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cabrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech Syst Signal Process</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="87" to="103" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Network intrusion detection using hybrid binary PSO and random forests algorithm. Security and Communication Networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shahzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2646" to="2660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics</title>
		<author>
			<persName><forename type="first">V</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Sci</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page" from="113" to="141" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification of imbalanced data: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akc</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Pattern Recognit Artif Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="687" to="719" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of learning from imbalanced data using random forest</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hulse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tools with Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="310" to="317" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A study of the behavior of several methods for balancing machine learning training data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Monard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification ensembles for imbalanced class sizes in predictive toxicology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAR QSAR Environ Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="517" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting protein-RNA interaction amino acids using random forest based on submodularity subset selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Chem</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="324" to="330" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ForesTexter: an efficient random forest algorithm for imbalanced text categorization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="105" to="116" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid algorithm for classification of unbalanced datasets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Theory &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1485" to="1489" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Subcellular localization using fluorescence imagery: Utilizing ensemble classification with diverse feature extraction strategies and data balancing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tahir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4231" to="4243" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can-CSC-GBE: Developing Cost-sensitive Classifier with Gentleboost Ensemble for breast cancer classification using protein amino acids and imbalanced data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Javed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="38" to="46" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prediction of human breast and colon cancers from imbalanced data using nearest neighbor and support vector machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Majid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iqbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="792" to="808" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving random forests</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robnik-Sikonja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2004</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Trees Weighting Random Forests Method for Classifying High-Dimensional Noisy Data</title>
		<author>
			<persName><forename type="first">H B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">/2010 IEEE 7th International Conference on IEEE e-Business Engineering (ICEBE)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="160" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Setting of class weights in random forest for smallsample data</title>
		<author>
			<persName><forename type="first">Jian-Geng L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z-K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Eng Appl</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="131" to="134" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quasi-adaptive random forest for classification</title>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>-Z</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B-C</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Application of Statistics and Management</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="805" to="811" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional variable importance for random forests</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Boulesteix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kneib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><forename type="middle">T</forename><surname>Zeileis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random KNN feature selection-a fast and stable alternative to Random Forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Harner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Margin optimization based pruning for random forest</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuro computing</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An introduction to the boostrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">]</forename><surname>Newyork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinaln</surname></persName>
		</author>
		<author>
			<persName><surname>C4</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan kuafmann</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Class prediction for high-dimensional class-imbalanced data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lusa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">523</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparison on classification performance between random forests and support vector machine</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-X</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="107" to="110" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority oversampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving prediction of the minority class in Boosting</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2003)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2003)</meeting>
		<imprint>
			<publisher>Berlin</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2838</biblScope>
			<biblScope unit="page" from="107" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SMOTE for high-dimensional class-imbalanced data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blagus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusa</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">W Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIC 2005, Part I</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3644</biblScope>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Safe-level-SMOTE: Safelevel-synthetic minority over-sampling technique for handling the class imbalanced problem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bunkhumpornpat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sinapiromsaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lursinsap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<title level="s">Lecture Notes on Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag: Berlin</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5476</biblScope>
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Combating imbalance in network intrusion datasets</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cieslak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Striegel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="732" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the use of surrounding neighbors for synthetic over-sampling of the minority class</title>
		<author>
			<persName><forename type="first">V</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Mollineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th conference on Simulation, modeling and optimization</title>
		<meeting>the 8th conference on Simulation, modeling and optimization</meeting>
		<imprint>
			<publisher>World Scientific and Engineering Academy and Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="389" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A classification method for imbalance data Set based on hybrid strategy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>-L, Yuan-Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Electron Sin</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2161" to="2165" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Zheng-Feng C</forename></persName>
		</author>
		<title level="m">Study on optimization of random forests algorithm</title>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>Capital University of Economics and Business</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Classification Method for Imbalanced Data Based on SMOTE and Fuzzy Rough Nearest Neighbor Algorithm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rough Sets, Fuzzy Sets, Data Mining, and Granular Computing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag: Berlin</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9437</biblScope>
			<biblScope unit="page" from="340" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive semi-unsupervised weighted oversampling (A-SUWO) for imbalanced datasets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nekooeimehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai-Yuen</forename><surname>Sk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="405" to="416" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SMOTE-IPF: Addressing the noisy and borderline examples problem in imbalanced classification by a resampling method with filtering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sáez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stefanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform Sci</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="page" from="184" to="203" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">To combat multi-class imbalanced problems by means of over-sampling techniques</title>
		<author>
			<persName><forename type="first">L</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="238" to="251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CURE: an efficient clustering algorithm for large databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="84" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection method based on improved CURE clustering algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ya-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Guo L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="18" to="23" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gene functional classification from heterogeneous data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Annual International Conference on Computational Molecular Biology</title>
		<meeting>the fifth Annual International Conference on Computational Molecular Biology</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="249" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Null space based feature selection method for gene expression data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Mach Learn Cybern</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Structured feature selection using coordinate descent optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Ghalwash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stojkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A review of feature selection techniques in bioinformatics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Inza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larrañaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2507" to="2517" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A centroid-based gene selection method for microarray data classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Theor Biol</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A hybrid gene selection approach for microarray data classification using cellular learning automata and ant colony optimization</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Sharbaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mosafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Moattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="231" to="238" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Support vector machine classification and validation of cancer tissue samples using microarray expression data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="906" to="914" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A top-r feature selection algorithm for microarray gene expression data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="754" to="764" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hybrid Feature Selection Using Correlation Coefficient and Particle Swarm Optimization on Microarray Gene Expression Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chinnaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Bio-Inspired Computing and Applications. Advances in Intelligent Systems and Computing</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Snášel</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing Switzerland</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">424</biblScope>
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Destrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mosci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Mol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Manag Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="40" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feature selection for gene expression using model-based entropy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans Comput Biol Bioinform</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Random forest-based scheme using feature and decision levels information for multi-focus image fusion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kausar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal Applic</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="236" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A comparison of random forest and its Gini importance with standard chemometric methods for the feature selection and classification of spectral data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">213</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bias in random forest variable importance measures: Illustrations, sources and a solution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cost-sensitive feature selection using random forest: Selecting low-cost subsets of informative features</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gene selection and classification of microarray data using random forest</title>
		<author>
			<persName><forename type="first">R</forename><surname>Díaz-Uriarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Predicting customer retention and profitability by using random forests and regression forests techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lariviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Den Poel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="472" to="484" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An assessment of the effectiveness of a random forest classifier for landcover classification</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Rodriguez-Galiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghimire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chica-Olmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rigol-</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J Photogramm Remote Sens</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Influence of Hyper parameters on Random Forest Accuracy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International workshop on multiple classifier systems</title>
		<meeting>the 8th International workshop on multiple classifier systems<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Estimation of the hyper-parameter in random forest based on out-of-bag sample</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Xia Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Syst Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="566" to="572" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Out-of-bag estimation of the optimal sample size in bagging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Martinez-Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="152" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Feature selection and parameter optimization for SVM based on genetic algorithm with feature chromosomes</title>
		<author>
			<persName><forename type="first">Ming-Yuan Z</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Tian Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control and Decision</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1133" to="1138" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Question of SVM kernel parameter optimization with particle swarm algorithm based on neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shijie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Eng Appl</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="162" to="164" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Application of artificial fish-swarm algorithm in SVM parameter optimization selection</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Leifu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Shijie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Eng Appl</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="86" to="90" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Parameters selection and application of support vector machines based on particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">Xin-Guang</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui-Zhong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Theory &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="740" to="744" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Particle Swarm Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="1942" to="1948" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">An optimizing method based on autonomous animals: Fish-swarm Algorithm. Systems Engineering-Theory &amp; Practice</title>
		<author>
			<persName><forename type="first">Xiao-Lei L</forename><surname>Zhi-Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Xin Q</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">In silico prediction of toxic action mechanisms of phenols for imbalanced data with Random Forest learner</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mol Graph Model</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On extending f-measure and g-mean metrics to multi-class problems</title>
		<author>
			<persName><forename type="first">N F F</forename><surname>Espíndola R P, Ebecken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth international conference on data mining, text mining and their business applications</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
		<respStmt>
			<orgName>Wessex Institute of Technology, UK.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">An AUC-based permutation variable importance measure for random forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Janitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Boulesteix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Out-of-bag Estimation [R]</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Statistics Department, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Exploring issues of training data imbalance and mislabeling on random forest performance for large area land cover classification using the ensemble margin</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boukir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haywood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J Photogramm Remote Sens</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="155" to="168" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
