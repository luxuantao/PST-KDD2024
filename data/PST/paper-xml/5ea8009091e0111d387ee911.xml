<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ColBERT: E icient and E ective Passage Search via Contextualized Late Interaction over BERT</title>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_fEwrdjV">
					<orgName type="full">CAREER</orgName>
				</funder>
				<funder>
					<orgName type="full">NEC</orgName>
				</funder>
				<funder>
					<orgName type="full">Eltoukhy Family Graduate Fellowship at the Stanford School of Engineering</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Omar</forename><surname>Kha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University okha</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ColBERT: E icient and E ective Passage Search via Contextualized Late Interaction over BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3397271.3401075</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to ne-tuning deep language models (LMs) for document ranking.</p><p>While remarkably e ective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for e cient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their ne-grained similarity. By delaying and yet retaining this negranular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations o ine, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's e ectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ery Latency (log-scale) for a number of representative ranking models on MS MARCO Ranking <ref type="bibr" target="#b23">[24]</ref>. e gure also shows ColBERT. Neural re-rankers run on top of the o cial BM25 top-1000 results and use a Tesla V100 GPU. Methodology and detailed results are in ?4.</p><p>to prior learning-to-rank methods that rely on hand-cra ed features, these models employ embedding-based representations of queries and documents and directly model local interactions (i.e., ne-granular relationships) between their contents. Among them, a recent approach has emerged that ne-tunes deep pre-trained language models (LMs) like ELMo <ref type="bibr" target="#b28">[29]</ref> and BERT <ref type="bibr" target="#b4">[5]</ref> for estimating relevance. By computing deeply-contextualized semantic representations of query-document pairs, these LMs help bridge the pervasive vocabulary mismatch <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref> between documents and queries <ref type="bibr" target="#b29">[30]</ref>. Indeed, in the span of just a few months, a number of ranking models based on BERT have achieved state-of-the-art results on various retrieval benchmarks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref> and have been proprietarily adapted for deployment by Google<ref type="foot" target="#foot_0">1</ref> and Bing <ref type="foot" target="#foot_1">2</ref> .</p><p>However, the remarkable gains delivered by these LMs come at a steep increase in computational cost. Hofst? er et al. <ref type="bibr" target="#b8">[9]</ref> and MacAvaney et al. <ref type="bibr" target="#b17">[18]</ref> observe that BERT-based models in the literature are 100-1000? more computationally expensive than prior models-some of which are arguably not inexpensive to begin with <ref type="bibr" target="#b12">[13]</ref>. is quality-cost tradeo is summarized by Figure <ref type="figure" target="#fig_0">1</ref>, which compares two BERT-based rankers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> against a representative set of ranking models. e gure uses MS MARCO Ranking <ref type="bibr" target="#b23">[24]</ref>, a recent collection of 9M passages and 1M queries from Bing's logs. It reports retrieval e ectiveness (MRR@10) on the o cial validation set as well as average query latency (log-scale) using a high-end server that dedicates one Tesla V100 GPU per query for neural re-rankers. Following the re-ranking setup of MS MARCO, ColBERT (re-rank), the Neural Matching Models, and the Deep LMs re-rank the MS MARCO's o cial top-1000 documents per query. Other methods, including ColBERT (full retrieval), directly retrieve the top-1000 results from the entire collection.</p><p>As the gure shows, BERT considerably improves search precision, raising MRR@10 by almost 7% against the best previous methods; simultaneously, it increases latency by up to tens of thousands of milliseconds even with a high-end GPU. is poses a challenging tradeo since raising query response times by as li le as 100ms is known to impact user experience and even measurably diminish revenue <ref type="bibr" target="#b16">[17]</ref>. To tackle this problem, recent work has started exploring using Natural Language Understanding (NLU) techniques to augment traditional retrieval models like BM25 <ref type="bibr" target="#b31">[32]</ref>. For example, Nogueira et al. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> expand documents with NLU-generated queries before indexing with BM25 scores and Dai &amp; Callan <ref type="bibr" target="#b1">[2]</ref> replace BM25's term frequency with NLU-estimated term importance. Despite successfully reducing latency, these approaches generally reduce precision substantially relative to BERT.</p><p>To reconcile e ciency and contextualization in IR, we propose ColBERT, a ranking model based on contextualized late interaction over BERT. As the name suggests, ColBERT proposes a novel late interaction paradigm for estimating relevance between a query q and a document d. Under late interaction, q and d are separately encoded into two sets of contextual embeddings, and relevance is evaluated using cheap and pruning-friendly computations between both sets-that is, fast computations that enable ranking without exhaustively evaluating every possible candidate.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> contrasts our proposed late interaction approach with existing neural matching paradigms. On the le , Figure <ref type="figure" target="#fig_1">2</ref> (a) illustrates representation-focused rankers, which independently compute an embedding for q and another for d and estimate relevance as a single similarity score between two vectors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41]</ref>. Moving to the right, Figure <ref type="figure" target="#fig_1">2</ref> (b) visualizes typical interaction-focused rankers. Instead of summarizing q and d into individual embeddings, these rankers model word-and phrase-level relationships across q and d and match them using a deep neural network (e.g., with CNNs/MLPs <ref type="bibr" target="#b21">[22]</ref> or kernels <ref type="bibr" target="#b35">[36]</ref>). In the simplest case, they feed the neural network an interaction matrix that re ects the similiarity between every pair of words across q and d. Further right, Figure <ref type="figure" target="#fig_1">2</ref> (c) illustrates a more powerful interaction-based paradigm, which models the interactions between words within as well as across q and d at the same time, as in BERT's transformer architecture <ref type="bibr" target="#b24">[25]</ref>.</p><p>ese increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure <ref type="figure" target="#fig_1">2</ref> (b) and (c)) tend to be superior for IR tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, a representation-focused model-by isolating the computations among q and d-makes it possible to precompute document representations o ine <ref type="bibr" target="#b40">[41]</ref>, greatly reducing the computational load per query. In this work, we observe that the ne-grained matching of interaction-based models and the precomputation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query-document interaction. Figure <ref type="figure" target="#fig_1">2</ref> (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. is paradigm allows ColBERT to exploit deep LM-based representations while shi ing the cost of encoding documents ofine and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval.</p><p>As Figure <ref type="figure" target="#fig_0">1</ref> illustrates, ColBERT can serve queries in tens or few hundreds of milliseconds. For instance, when used for reranking as in "ColBERT (re-rank)", it delivers over 170? speedup (and requires 14,000? fewer FLOPs) relative to existing BERT-based models, while being more e ective than every non-BERT baseline ( ?4.2 &amp; 4.3). ColBERT's indexing-the only time it needs to feed documents through BERT-is also practical: it can index the MS MARCO collection of 9M passages in about 3 hours using a single server with four GPUs ( ?4.5), retaining its e ectiveness with a space footprint of as li le as few tens of GiBs. Our extensive ablation study ( ?4.4) shows that late interaction, its implementation via MaxSim operations, and crucial design choices within our BERTbased encoders are all essential to ColBERT's e ectiveness.</p><p>Our main contributions are as follows.</p><p>(1) We propose late interaction ( ?3.1) as a paradigm for e cient and e ective neural ranking. (2) We present ColBERT ( ?3.2 &amp; 3.3), a highly-e ective model that employs novel BERT-based query and document encoders within the late interaction paradigm.</p><p>(3) We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model ( ?3.5) and for searching a full collection using vector similarity indexes ( ?3.6). ( <ref type="formula">4</ref>) We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Neural Matching Models. Over the past few years, IR researchers have introduced numerous neural architectures for ranking. In this work, we compare against KNRM <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, Duet <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, Con-vKNRM <ref type="bibr" target="#b3">[4]</ref>, and fastText+ConvKNRM <ref type="bibr" target="#b9">[10]</ref>. KNRM proposes a di erentiable kernel-pooling technique for extracting matching signals from an interaction matrix, while Duet combines signals from exact-match-based as well as embedding-based similarities for ranking. Introduced in 2018, ConvKNRM learns to match ngrams in the query and the document. Lastly, fastText+ConvKNRM (abbreviated fT+ConvKNRM) tackles the absence of rare words from typical word embeddings lists by adopting sub-word token embeddings.</p><p>In 2018, Zamani et al. <ref type="bibr" target="#b40">[41]</ref> introduced SNRM, a representationfocused IR model that encodes each query and each document as a single, sparse high-dimensional vector of "latent terms". By producing a sparse-vector representation for each document, SNRM is able to use a traditional IR inverted index for representing documents, allowing fast end-to-end retrieval. Despite highly promising results and insights, SNRM's e ectiveness is substantially outperformed by the state of the art on the datasets with which it was evaluated (e.g., see <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38]</ref>). While SNRM employs sparsity to allow using inverted indexes, we relax this assumption and compare a (dense) BERT-based representation-focused model against our late-interaction ColBERT in our ablation experiments in ?4.4. For a detailed overview of existing neural ranking models, we refer the readers to two recent surveys of the literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Language Model Pretraining for IR. Recent work in NLU emphasizes the importance pre-training language representation models in an unsupervised fashion before subsequently ne-tuning them on downstream tasks. A notable example is BERT <ref type="bibr" target="#b4">[5]</ref>, a bidirectional transformer-based language model whose ne-tuning advanced the state of the art on various NLU benchmarks. Nogueira et al. <ref type="bibr" target="#b24">[25]</ref>, MacAvaney et al. <ref type="bibr" target="#b17">[18]</ref>, and Dai &amp; Callan <ref type="bibr" target="#b2">[3]</ref> investigate incorporating such LMs (mainly BERT, but also ELMo <ref type="bibr" target="#b28">[29]</ref>) on different ranking datasets. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref> (c), the common approach (and the one adopted by Nogueira et al. on MS MARCO and TREC CAR) is to feed the query-document pair through BERT and use an MLP on top of BERT's [CLS] output token to produce a relevance score. Subsequent work by Nogueira et al. <ref type="bibr" target="#b26">[27]</ref> introduced duoBERT, which ne-tunes BERT to compare the relevance of a pair of documents given a query. Relative to their single-document BERT, this gives duoBERT a 1% MRR@10 advantage on MS MARCO while increasing the cost by at least 1.4?.</p><p>BERT Optimizations. As discussed in ?1, these LM-based rankers can be highly expensive in practice. While ongoing efforts in the NLU literature for distilling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>, compressing <ref type="bibr" target="#b39">[40]</ref>, and pruning <ref type="bibr" target="#b18">[19]</ref> BERT can be instrumental in narrowing this gap,  is includes doc2query <ref type="bibr" target="#b27">[28]</ref> and DeepCT <ref type="bibr" target="#b1">[2]</ref>. e doc2query model expands each document with a pre-de ned number of synthetic queries queries generated by a seq2seq transformer model that is trained to generate queries given a document. It then relies on a BM25 index for retrieval from the (expanded) documents. DeepCT uses BERT to produce the term frequency component of BM25 in a contextaware manner, essentially representing a feasible realization of the term-independence assumption with neural networks <ref type="bibr" target="#b22">[23]</ref>. Lastly, docTTTTTquery <ref type="bibr" target="#b25">[26]</ref> is identical to doc2query except that it netunes a pre-trained model (namely, T5 <ref type="bibr" target="#b30">[31]</ref>) for generating the predicted queries.</p><p>Concurrently with our dra ing of this paper, Hofst? er et al. <ref type="bibr" target="#b10">[11]</ref> published their Transformer-Kernel (TK) model. At a high level, TK improves the KNRM architecture described earlier: while KNRM employs kernel pooling on top of word-embedding-based interaction, TK uses a Transformer <ref type="bibr" target="#b33">[34]</ref> component for contextually encoding queries and documents before kernel pooling. TK establishes a new state-of-the-art for non-BERT models on MS MARCO (Dev); however, the best non-ensemble MRR@10 it achieves is 31% while ColBERT reaches up to 36%. Moreover, due to indexing document representations o ine and employing a MaxSim-based late interaction mechanism, ColBERT is much more scalable, enabling end-to-end retrieval which is not supported by TK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COLBERT</head><p>ColBERT prescribes a simple framework for balancing the quality and cost of neural IR, particularly deep language models like BERT. As introduced earlier, delaying the query-document interaction can facilitate cheap neural re-ranking (i.e., through pre-computation) and even support practical end-to-end neural retrieval (i.e., through pruning via vector-similarity search). ColBERT addresses how to do so while still preserving the e ectiveness of state-of-the-art models, which condition the bulk of their computations on the joint query-document pair.</p><p>Even though ColBERT's late-interaction framework can be applied to a wide variety of architectures (e.g., CNNs, RNNs, transformers, etc.), we choose to focus this work on bi-directional transformerbased encoders (i.e., BERT) owing to their state-of-the-art e ectiveness yet very high computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Figure <ref type="figure" target="#fig_2">3</ref> depicts the general architecture of ColBERT, which comprises: (a) a query encoder f Q , (b) a document encoder f D , and (c) the late interaction mechanism. Given a query q and document d, f Q encodes q into a bag of xed-size embeddings E q while f D encodes d into another bag E d . Crucially, each embeddings in E q and E d is contextualized based on the other terms in q or d, respectively. We describe our BERT-based encoders in ?3. <ref type="bibr" target="#b1">2</ref>.</p><p>Using E q and E d , ColBERT computes the relevance score between q and d via late interaction, which we de ne as a summation of maximum similarity (MaxSim) operators. In particular, we nd the maximum cosine similarity of each ? E q with vectors in E d , and combine the outputs via summation. Besides cosine, we also evaluate squared L2 distance as a measure of vector similarity. Intuitively, this interaction mechanism so ly searches for each query term t q -in a manner that re ects its context in the query-against the document's embeddings, quantifying the strength of the "match" via the largest similarity score between t q and a document term t d . Given these term scores, it then estimates the document relevance by summing the matching evidence across all query terms. While more sophisticated matching is possible with other choices such as deep convolution and a ention layers (i.e., as in typical interaction-focused models), a summation of maximum similarity computations has two distinctive characteristics. First, it stands out as a particularly cheap interaction mechanism, as we examine its FLOPs in ?4.2. Second, and more importantly, it is amenable to highly-e cient pruning for top-k retrieval, as we evaluate in ?4.3. is enables using vector-similarity algorithms for skipping documents without materializing the full interaction matrix or even considering each document in isolation. Other cheap choices (e.g., a summation of average similarity scores, instead of maximum) are possible; however, many are less amenable to pruning. In ?4.4, we conduct an extensive ablation study that empirically veri es the advantage of our MaxSim-based late interaction against alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ery &amp; Document Encoders</head><p>Prior to late interaction, ColBERT encodes each query or document into a bag of embeddings, employing BERT-based encoders. We share a single BERT model among our query and document encoders but distinguish input sequences that correspond to queries and documents by prepending a special token [Q] to queries and another token [D] to documents. ery Encoder. Given a textual query q, we tokenize it into its BERT-based WordPiece <ref type="bibr" target="#b34">[35]</ref> tokens q 1 q 2 ...q l . We prepend the token [Q] to the query. We place this token right a er BERT's sequencestart token <ref type="bibr">[CLS]</ref>. If the query has fewer than a pre-de ned number of tokens N q , we pad it with BERT's special [mask] tokens up to length N q (otherwise, we truncate it to the rst N q tokens).</p><p>is padded sequence of input tokens is then passed into BERT's deep transformer architecture, which computes a contextualized representation of each token. We denote the padding with masked tokens as query augmentation, a step that allows BERT to produce query-based embeddings at the positions corresponding to these masks. ery augmentation is intended to serve as a so , di erentiable mechanism for learning to expand queries with new terms or to re-weigh existing terms based on their importance for matching the query. As we show in ?4.4, this operation is essential for ColBERT's e ectiveness.</p><p>Given BERT's representation of each token, our encoder passes the contextualized output representations through a linear layer with no activations.</p><p>is layer serves to control the dimension of ColBERT's embeddings, producing m-dimensional embeddings for the layer's output size m. As we discuss later in more detail, we typically x m to be much smaller than BERT's xed hidden dimension.</p><p>While ColBERT's embedding dimension has limited impact on the e ciency of query encoding, this step is crucial for controlling the space footprint of documents, as we show in ?4. <ref type="bibr" target="#b4">5</ref>. In addition, it can have a signi cant impact on query execution time, particularly the time taken for transferring the document representations onto the GPU from system memory (where they reside before processing a query). In fact, as we show in ?4.2, gathering, stacking, and transferring the embeddings from CPU to GPU can be the most expensive step in re-ranking with ColBERT. Finally, the output embeddings are normalized so each has L2 norm equal to one.</p><p>e result is that the dot-product of any two embeddings becomes equivalent to their cosine similarity, falling in the [-1, 1] range. Document Encoder. Our document encoder has a very similar architecture. We rst segment a document d into its constituent tokens d 1 d 2 ...d m , to which we prepend BERT's start token [CLS] followed by our special token [D] that indicates a document sequence. Unlike queries, we do not append [mask] tokens to documents. After passing this input sequence through BERT and the subsequent linear layer, the document encoder lters out the embeddings corresponding to punctuation symbols, determined via a pre-de ned list. is ltering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings of punctuation are unnecessary for e ectiveness.</p><p>In summary, given q = q 0 q 1 ...q l and d = d 0 d 1 ...d n , we compute the bags of embeddings E q and E d in the following manner, where # refers to the [mask] tokens:</p><formula xml:id="formula_0">E q := Normalize( CNN( BERT("[Q]q 0 q 1 ...q l ##...#") ) )<label>(1)</label></formula><formula xml:id="formula_1">E d := Filter( Normalize( CNN( BERT("[D]d 0 d 1 ...d n ") ) ) ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Late Interaction</head><p>Given the representation of a query q and a document d, the relevance score of d to q, denoted as S q,d , is estimated via late interaction between their bags of contextualized embeddings. As mentioned before, this is conducted as a sum of maximum similarity computations, namely cosine similarity (implemented as dot-products due to the embedding normalization) or squared L2 distance.</p><p>S q,d :=</p><formula xml:id="formula_2">i ?[ |E q |] max j ?[ |E d |] E q i ? E T d j<label>(3)</label></formula><p>ColBERT is di erentiable end-to-end. We ne-tune the BERT encoders and train from scratch the additional parameters (i.e., the linear layer and the [Q] and [D] markers' embeddings) using the Adam <ref type="bibr" target="#b15">[16]</ref> optimizer. Notice that our interaction mechanism has no trainable parameters. Given a triple q, d + , d -with query q, positive document d + and negative document d -, ColBERT is used to produce a score for each document individually and is optimized via pairwise so max cross-entropy loss over the computed scores of d + and d -.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">O line Indexing: Computing &amp; Storing Document Embeddings</head><p>By design, ColBERT isolates almost all of the computations between queries and documents, largely to enable pre-computing document representations o ine. At a high level, our indexing procedure is straight-forward: we proceed over the documents in the collection in batches, running our document encoder f D on each batch and storing the output embeddings per document. Although indexing a set of documents is an o ine process, we incorporate a few simple optimizations for enhancing the throughput of indexing. As we show in ?4.5, these optimizations can considerably reduce the o ine cost of indexing.</p><p>To begin with, we exploit multiple GPUs, if available, for faster encoding of batches of documents in parallel. When batching, we pad all documents to the maximum length of a document within the batch. 3 To make capping the sequence length on a per-batch basis more e ective, our indexer proceeds through documents in groups of B (e.g., B = 100,000) documents. It sorts these documents by length and then feeds batches of b (e.g., b = 128) documents of comparable length through our encoder. is length-based bucketing is sometimes refered to as a BucketIterator in some libraries (e.g., allenNLP). Lastly, while most computations occur on the GPU, we found that a non-trivial portion of the indexing time is spent on pre-processing the text sequences, primarily BERT's WordPiece tokenization. Exploiting that these operations are independent across documents in a batch, we parallelize the pre-processing across the available CPU cores.</p><p>Once the document representations are produced, they are saved to disk using 32-bit or 16-bit values to represent each dimension. As we describe in ?3.5 and 3.6, these representations are either simply loaded from disk for ranking or are subsequently indexed for vector-similarity search, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Top-k Re-ranking with ColBERT</head><p>Recall that ColBERT can be used for re-ranking the output of another retrieval model, typically a term-based model, or directly for end-to-end retrieval from a document collection. In this section, we discuss how we use ColBERT for ranking a small set of k (e.g., k = 1000) documents given a query q. Since k is small, we rely on batch computations to exhaustively score each document 3 e public BERT implementations we saw simply pad to a pre-de ned length.</p><p>(unlike our approach in ?3.6). To begin with, our query serving subsystem loads the indexed documents representations into memory, representing each document as a matrix of embeddings.</p><p>Given a query q, we compute its bag of contextualized embeddings E q (Equation <ref type="formula" target="#formula_0">1</ref>) and, concurrently, gather the document representations into a 3-dimensional tensor D consisting of k document matrices. We pad the k documents to their maximum length to facilitate batched operations, and move the tensor D to the GPU's memory. On the GPU, we compute a batch dot-product of E q and D, possibly over multiple mini-batches. e output materializes a 3-dimensional tensor that is a collection of cross-match matrices between q and each document. To compute the score of each document, we reduce its matrix across document terms via a max-pool (i.e., representing an exhaustive implementation of our MaxSim computation) and reduce across query terms via a summation. Finally, we sort the k documents by their total scores.</p><p>Relative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in fact, its cost is dominated by the cost of gathering and transferring the pre-computed embeddings. To illustrate, ranking k documents via typical BERT rankers requires feeding BERT k di erent inputs each of length l = |q| + |d i | for query q and documents d i , where a ention has quadratic cost in the length of the sequence. In contrast, ColBERT feeds BERT only a single, much shorter sequence of length l = |q|. Consequently, ColBERT is not only cheaper, it also scales much be er with k as we examine in ?4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">End-to-end Top-k Retrieval with ColBERT</head><p>As mentioned before, ColBERT's late-interaction operator is specically designed to enable end-to-end retrieval from a large collection, largely to improve recall relative to term-based retrieval approaches.</p><p>is section is concerned with cases where the number of documents to be ranked is too large for exhaustive evaluation of each possible candidate document, particularly when we are only interested in the highest scoring ones. Concretely, we focus here on retrieving the top-k results directly from a large document collection with N (e.g., N = 10, 000, 000) documents, where k N . To do so, we leverage the pruning-friendly nature of the MaxSim operations at the backbone of late interaction. Instead of applying MaxSim between one of the query embeddings and all of one document's embeddings, we can use fast vector-similarity data structures to e ciently conduct this search between the query embedding and all document embeddings across the full collection. For this, we employ an o -the-shelf library for large-scale vector-similarity search, namely faiss <ref type="bibr" target="#b14">[15]</ref> from Facebook. <ref type="foot" target="#foot_2">4</ref> In particular, at the end of o ine indexing ( ?3.4), we maintain a mapping from each embedding to its document of origin and then index all document embeddings into faiss.</p><p>Subsequently, when serving queries, we use a two-stage procedure to retrieve the top-k documents from the entire collection. Both stages rely on ColBERT's scoring: the rst is an approximate stage aimed at ltering while the second is a re nement stage. For the rst stage, we concurrently issue N q vector-similarity queries (corresponding to each of the embeddings in E q ) onto our faiss index. is retrieves the top-k (e.g., k = k/2) matches for that vector over all document embeddings. We map each of those to its document of origin, producing N q ? k document IDs, only K ? N q ? k of which are unique. ese K documents likely contain one or more embeddings that are highly similar to the query embeddings. For the second stage, we re ne this set by exhaustively re-ranking only those K documents in the usual manner described in ?3. <ref type="bibr" target="#b4">5</ref>.</p><p>In our faiss-based implementation, we use an IVFPQ index ("inverted le with product quantization"). is index partitions the embedding space into P (e.g., P = 1000) cells based on k-means clustering and then assigns each document embedding to its nearest cell based on the selected vector-similarity metric. For serving queries, when searching for the top-k matches for a single query embedding, only the nearest p (e.g., p = 10) partitions are searched. To improve memory e ciency, every embedding is divided into s (e.g., s = 16) sub-vectors, each represented using one byte. Moreover, the index conducts the similarity computations in this compressed domain, leading to cheaper computations and thus faster search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>We now turn our a ention to empirically testing ColBERT, addressing the following research questions.</p><p>RQ 1 : In a typical re-ranking setup, how well can ColBERT bridge the existing gap (highlighted in ?1) between highly-e cient and highly-e ective neural models? ( ?4.2) RQ 2 : Beyond re-ranking, can ColBERT e ectively support endto-end retrieval directly from a large collection? ( ?4.3) RQ 3 : What does each component of ColBERT (e.g., late interaction, query augmentation) contribute to its quality? ( ?4.4) RQ 4 : What are ColBERT's indexing-related costs in terms of o ine computation and memory overhead? ( ?4.5) we conduct our experiments on the MS MARCO Ranking <ref type="bibr" target="#b23">[24]</ref> (henceforth, MS MARCO) and TREC Complex Answer Retrieval (TREC-CAR) <ref type="bibr" target="#b5">[6]</ref> datasets. Both of these recent datasets provide large training data of the scale that facilitates training and evaluating deep neural networks. We describe both in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methodology</head><p>MS MARCO. MS MARCO is a dataset (and a corresponding competition) introduced by Microso in 2016 for reading comprehension and adapted in 2018 for retrieval. It is a collection of 8.8M passages from Web pages, which were gathered from Bing's results to 1M real-world queries. Each query is associated with sparse relevance judgements of one (or very few) documents marked as relevant and no documents explicitly indicated as irrelevant. Per the o cial evaluation, we use MRR@10 to measure e ectiveness.</p><p>We use three sets of queries for evaluation. e o cial development and evaluation sets contain roughly 7k queries. However, the relevance judgements of the evaluation set are held-out by Microso and e ectiveness results can only be obtained by submi ing to the competition's organizers. We submi ed our main re-ranking ColBERT model for the results in ?4.2. In addition, the collection includes roughly 55k queries (with labels) that are provided as additional validation data. We re-purpose a random sample of 5k queries among those (i.e., ones not in our development or training sets) as a "local" evaluation set. Along with the o cial development set, we use this held-out set for testing our models as well as baselines in ?4.3. We do so to avoid submi ing multiple variants of the same model at once, as the organizers discourage too many submissions by the same team.</p><p>TREC CAR. Introduced by Dietz <ref type="bibr" target="#b5">[6]</ref> et al. in 2017, TREC CAR is a synthetic dataset based on Wikipedia that consists of about 29M passages. Similar to related work <ref type="bibr" target="#b24">[25]</ref>, we use the rst four of ve pre-de ned folds for training and the h for validation. is amounts to roughly 3M queries generated by concatenating the title of a Wikipedia page with the heading of one of its sections. at section's passages are marked as relevant to the corresponding query. Our evaluation is conducted on the test set used in TREC 2017 CAR, which contains 2,254 queries.</p><p>4.1.2 Implementation. Our ColBERT models are implemented using Python 3 and PyTorch 1. We use the popular transformers<ref type="foot" target="#foot_3">5</ref> library for the pre-trained BERT model. Similar to <ref type="bibr" target="#b24">[25]</ref>, we ne-tune all ColBERT models with learning rate 3 ? 10 -6 with a batch size 32. We x the number of embeddings per query at N q = 32. We set our ColBERT embedding dimension m to be 128; ?4.5 demonstrates ColBERT's robustness to a wide range of embedding dimensions.</p><p>For MS MARCO, we initialize the BERT components of the Col-BERT query and document encoders using Google's o cial pretrained BERT base model. Further, we train all models for 200k iterations. For TREC CAR, we follow related work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref> and use a different pre-trained model to the o cial ones. To explain, the o cial BERT models were pre-trained on Wikipedia, which is the source of TREC CAR's training and test sets. To avoid leaking test data into train, Nogueira and Cho's <ref type="bibr" target="#b24">[25]</ref> pre-train a randomly-initialized BERT model on the Wiki pages corresponding to training subset of TREC CAR. ey release their BERT large pre-trained model, which we ne-tune for ColBERT's experiments on TREC CAR. Since netuning this model is signi cantly slower than BERT base , we train on TREC CAR for only 125k iterations.</p><p>In our re-ranking results, unless stated otherwise, we use 4 bytes per dimension in our embeddings and employ cosine as our vectorsimilarity function. For end-to-end ranking, we use (squared) L2 distance, as we found our faiss index was faster at L2-based retrieval. For our faiss index, we set the number of partitions to P =2,000, and search the nearest p = 10 to each query embedding to retrieve k = k = 1000 document vectors per query embedding. We divide each embedding into s = 16 sub-vectors, each encoded using one byte. To represent the index used for the second stage of our end-to-end retrieval procedure, we use 16-bit values per dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Hardware &amp; Time Measurements.</head><p>To evaluate the latency of neural re-ranking models in ?4.2, we use a single Tesla V100 GPU that has 32 GiBs of memory on a server with two Intel Xeon Gold 6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469 GiBs of RAM. For the mostly CPU-based retrieval experiments in ?4.3 and the indexing experiments in ?4.5, we use another server with the same CPU and system memory speci cations but which has four Titan V GPUs a ached, each with 12 GiBs of memory. Across all experiments, only one GPU is dedicated per query for  <ref type="bibr" target="#b24">[25]</ref> 34.7 -10,700 97T (13,900?) BERT base (our training) 36.0 -10,700 97T (13,900?) BERT large <ref type="bibr" target="#b24">[25]</ref> 36.5 35.9 32,900 340T (48,600?)</p><p>ColBERT (over BERT base ) 34.9 34.9 61 7B (1?)</p><p>Table <ref type="table">1</ref>: "Re-ranking" results on MS MARCO. Each neural model re-ranks the o cial top-1000 results produced by BM25.</p><p>Latency is reported for re-ranking only. To obtain the end-to-end latency in Figure <ref type="figure" target="#fig_0">1</ref>, we add the BM25 latency from Table <ref type="table">2</ref>.</p><p>Method MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000 Table <ref type="table">2</ref>: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query directly from the entire 8.8M document collection.</p><formula xml:id="formula_3">BM25 (o</formula><p>retrieval (i.e., for methods with neural computations) but we use up to all four GPUs during indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ality-Cost Tradeo : Top-k Re-ranking</head><p>In this section, we examine ColBERT's e ciency and e ectiveness at re-ranking the top-k results extracted by a bag-of-words retrieval model, which is the most typical se ing for testing and deploying neural ranking models. We begin with the MS MARCO dataset. We compare against KNRM, Duet, and fastText+ConvKNRM, a representative set of neural matching models that have been previously tested on MS MARCO. In addition, we compare against the natural adaptation of BERT for ranking by Nogueira and Cho <ref type="bibr" target="#b24">[25]</ref>, in particular, BERT base and its deeper counterpart BERT large . We also report results for "BERT base (our training)", which is based on Nogueira and Cho's base model (including hyperparameters) but is trained with the same loss function as ColBERT ( ?3.3) for 200k iterations, allowing for a more direct comparison of the results. We report the competition's o cial metric, namely MRR@10, on the validation set (Dev) and the evaluation set (Eval). We also report the re-ranking latency, which we measure using a single Tesla V100 GPU, and the FLOPs per query for each neural ranking model. For ColBERT, our reported latency subsumes the entire computation from gathering the document representations, moving them to the GPU, tokenizing then encoding the query, and applying late interaction to compute document scores. For the baselines, we measure the scoring computations on the GPU and exclude the CPU-based text preprocessing (similar to <ref type="bibr" target="#b8">[9]</ref>). In principle, the baselines can pre-compute the majority of this preprocessing (e.g., document tokenization) o ine and parallelize the rest across documents online, leaving only a negligible cost. We estimate the FLOPs per query of each model using the torchpro le<ref type="foot" target="#foot_4">6</ref> library.</p><p>We now proceed to study the results, which are reported in Table 1. To begin with, we notice the fast progress from KNRM in 2017 to the BERT-based models in 2019, manifesting itself in over 16% increase in MRR@10. As described in ?1, the simultaneous increase in computational cost is di cult to miss. Judging by their rather monotonic pa ern of increasingly larger cost and higher effectiveness, these results appear to paint a picture where expensive models are necessary for high-quality ranking.</p><p>In contrast with this trend, ColBERT (which employs late interaction over BERT base ) performs no worse than the original adaptation of BERT base for ranking by Nogueira and Cho <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> and is only marginally less e ective than BERT large and our training of BERT base (described above). While highly competitive in e ectiveness, ColBERT is orders of magnitude cheaper than BERT base , in particular, by over 170? in latency and 13,900? in FLOPs. is highlights the expressiveness of our proposed late interaction mechanism, particularly when coupled with a powerful pre-trained LM like BERT. While ColBERT's re-ranking latency is slightly higher than the non-BERT re-ranking models shown (i.e., by 10s of milliseconds), this di erence is explained by the time it takes to gather, stack, and transfer the document embeddings to the GPU. In particular, the query encoding and interaction in ColBERT consume only 13 milliseconds of its total execution time. We note that ColBERT's latency and FLOPs can be considerably reduced by padding queries to a shorter length, using smaller vector dimensions (the MRR@10 of which is tested in ?4.5), employing quantization of the document vectors, and storing the embeddings on GPU if su cient memory exists. We leave these directions for future work. Diving deeper into the quality-cost tradeo between BERT and ColBERT, Figure <ref type="figure" target="#fig_4">4</ref> demonstrates the relationships between FLOPs and e ectiveness (MRR@10) as a function of the re-ranking depth k when re-ranking the top-k results by BM25, comparing ColBERT and BERT base (our training). We conduct this experiment on MS MARCO (Dev). We note here that as the o cial top-1000 ranking does not provide the BM25 order (and also lacks documents beyond the top-1000 per query), the models in this experiment re-rank the Anserini <ref type="bibr" target="#b36">[37]</ref> toolkit's BM25 output. Consequently, both MRR@10 values at k = 1000 are slightly higher from those reported in Table <ref type="table">1</ref>.</p><p>Studying the results in Figure <ref type="figure" target="#fig_4">4</ref>, we notice that not only is Col-BERT much cheaper than BERT for the same model size (i.e., 12layer "base" transformer encoder), it also scales be er with the number of ranked documents. In part, this is because ColBERT only needs to process the query once, irrespective of the number of documents evaluated. For instance, at k = 10, BERT requires nearly 180? more FLOPs than ColBERT; at k = 1000, BERT's overhead jumps to 13,900?. It then reaches 23,000? at k = 2000. In fact, our informal experimentation shows that this orders-of-magnitude gap in FLOPs makes it practical to run ColBERT entirely on the CPU, although CPU-based re-ranking lies outside our scope. Table <ref type="table">3</ref>: Results on TREC CAR.</p><p>Having studied our results on MS MARCO, we now consider TREC CAR, whose o cial metric is MAP. Results are summarized in Table <ref type="table">3</ref>, which includes a number of important baselines (BM25, doc2query, and DeepCT) in addition to re-ranking baselines that have been tested on this dataset. ese results directly mirror those with MS MARCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">End-to-end Top-k Retrieval</head><p>Beyond cheap re-ranking, ColBERT is amenable to top-k retrieval directly from a full collection. Table <ref type="table">2</ref> considers full retrieval, wherein each model retrieves the top-1000 documents directly from MS MARCO's 8.8M documents per query. In addition to MRR@10 and latency in milliseconds, the table reports Recall@50, Recall@200, and Recall@1000, important metrics for a full-retrieval model that essentially lters down a large collection on a per-query basis.</p><p>We compare against BM25, in particular MS MARCO's o cial BM25 ranking as well as a well-tuned baseline based on the Anserini toolkit. <ref type="foot" target="#foot_5">7</ref> While many other traditional models exist, we are not aware of any that substantially outperform Anserini's BM25 implementation (e.g., see RM3 in <ref type="bibr" target="#b27">[28]</ref>, LMDir in <ref type="bibr" target="#b1">[2]</ref>, or Microso 's proprietary feature-based RankSVM on the leaderboard).</p><p>We also compare against doc2query, DeepCT, and docTTTT-Tquery. All three rely on a traditional bag-of-words model (primarily BM25) for retrieval. Crucially, however, they re-weigh the frequency of terms per document and/or expand the set of terms in each document before building the BM25 index. In particular, doc2query expands each document with a pre-de ned number of synthetic queries generated by a seq2seq transformer model (which docTTTTquery replaced with a pre-trained language model, T5 <ref type="bibr" target="#b30">[31]</ref>). In contrast, DeepCT uses BERT to produce the term frequency component of BM25 in a context-aware manner.</p><p>For the latency of Anserini's BM25, doc2query, and docTTTTquery, we use the authors' <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> Anserini-based implementation. While this implementation supports multi-threading, it only utilizes parallelism across di erent queries. We thus report single-threaded latency for these models, noting that simply parallelizing their computation over shards of the index can substantially decrease their already-low latency. For DeepCT, we only estimate its latency using that of BM25 (as denoted by (est.) in the table), since DeepCT re-weighs BM25's term frequency without modifying the index otherwise. <ref type="foot" target="#foot_6">8</ref> As discussed in ?4.1, we use ColBERT L2 for end-toend retrieval, which employs negative squared L2 distance as its vector-similarity function. For its latency, we measure the time for faiss-based candidate ltering and the subsequent re-ranking. In this experiment, faiss uses all available CPU cores.</p><p>Looking at Table <ref type="table">2</ref>, we rst see Anserini's BM25 baseline at 18.7 MRR@10, noticing its very low latency as implemented in Anserini (which extends the well-known Lucene system), owing to both very cheap operations and decades of bag-of-words top-k retrieval optimizations. e three subsequent baselines, namely doc2query, DeepCT, and docTTTTquery, each brings a decisive enhancement to e ectiveness. ese improvements come at negligible overheads in latency, since these baselines ultimately rely on BM25-based retrieval. e most e ective among these three, docTTTTquery, demonstrates a massive 9% gain over vanilla BM25 by ne-tuning the recent language model T5.</p><p>Shi ing our a ention to ColBERT's end-to-end retrieval e ectiveness, we see its major gains in MRR@10 over all of these end-toend models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due to the improved recall. Moving beyond MRR@10, we also see large gains in Recall@k for k equals to 50, 200, and 1000. For instance, its Recall@50 actually exceeds the o cial BM25's Recall@1000 and even all but docTTTTTquery's Recall@200, emphasizing the value of end-to-end retrieval (instead of just re-ranking) with ColBERT.  e results from ?4.2 indicate that ColBERT is highly e ective despite the low cost and simplicity of its late interaction mechanism. To be er understand the source of this e ectiveness, we examine a number of important details in ColBERT's interaction and encoder architecture. For this ablation, we report MRR@10 on the validation set of MS MARCO in Figure <ref type="figure" target="#fig_7">5</ref>, which shows our main re-ranking ColBERT model [E], with MRR@10 of 34.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Due to the cost of training all models, we train a copy of our main model that retains only the rst 5 layers of BERT out of 12 (i.e., model <ref type="bibr">[D]</ref>) and similarly train all our ablation models for 200k iterations with ve BERT layers. To begin with, we ask if the negranular interaction in late interaction is necessary. Model <ref type="bibr">[A]</ref> tackles this question: it uses BERT to produce a single embedding vector for the query and another for the document, extracted from BERT's [CLS] contextualized embedding and expanded through a linear layer to dimension 4096 (which equals N q ? 128 = 32 ? 128). Relevance is estimated as the inner product of the query's and the document's embeddings, which we found to perform be er than cosine similarity for single-vector re-ranking. As the results show, this model is considerably less e ective than ColBERT, reinforcing the importance of late interaction.</p><p>Subsequently, we ask if our MaxSim-based late interaction is better than other simple alternatives. We test a model [B] that replaces ColBERT's maximum similarity with average similarity. e results suggest the importance of individual terms in the query paying special a ention to particular terms in the document. Similarly, the gure emphasizes the importance of our query augmentation mechanism: without query augmentation [C], ColBERT has a noticeably lower MRR@10. Lastly, we see the impact of end-to-end retrieval not only on recall but also on MRR@10. By retrieving directly from the full collection, ColBERT is able to retrieve to the top-10 documents missed entirely from BM25's top-1000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Indexing roughput &amp; Footprint</head><p>Lastly, we examine the indexing throughput and space footprint of ColBERT. Figure <ref type="figure" target="#fig_8">6</ref>  Table <ref type="table">4</ref>: Space Footprint vs MRR@10 (Dev) on MS MARCO.</p><p>Table <ref type="table">4</ref> reports the space footprint of ColBERT under various se ings as we reduce the embeddings dimension and/or the bytes per dimension. Interestingly, the most space-e cient se ing, that is, re-ranking with cosine similarity with 24-dimensional vectors stored as 2-byte oats, is only 1% worse in MRR@10 than the most space-consuming one, while the former requires only 27 GiBs to represent the MS MARCO collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we introduced ColBERT, a novel ranking model that employs contextualized late interaction over deep LMs (in particular, BERT) for e cient retrieval. By independently encoding queries and documents into ne-grained representations that interact via cheap and pruning-friendly computations, ColBERT can leverage the expressiveness of deep LMs while greatly speeding up query processing. In addition, doing so allows using ColBERT for end-toend neural retrieval directly from a large document collection. Our results show that ColBERT is more than 170? faster and requires 14,000? fewer FLOPs/query than existing BERT-based models, all while only minimally impacting quality and while outperforming every non-BERT baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: E ectiveness (MRR@10) versus Meanery Latency (log-scale) for a number of representative ranking models on MS MARCO Ranking<ref type="bibr" target="#b23">[24]</ref>. e gure also shows ColBERT. Neural re-rankers run on top of the o cial BM25 top-1000 results and use a Tesla V100 GPU. Methodology and detailed results are in ?4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic diagrams illustrating query-document matching paradigms in neural IR. e gure contrasts existing approaches (sub-gures (a), (b), and (c)) with the proposed late interaction paradigm (sub-gure (d)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: e general architecture of ColBERT given a query q and a document d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 1 . 1</head><label>11</label><figDesc>Datasets &amp; Metrics. Similar to related work<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: FLOPs (in millions) and MRR@10 as functions of the re-ranking depth k. Since the o cial BM25 ranking is not ordered, the initial top-k retrieval is conducted with Anserini's BM25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 BERT</head><label>10</label><figDesc>0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 MRR@</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation results on MS MARCO (Dev). Between brackets is the number of BERT layers used in each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: E ect of ColBERT's indexing optimizations on the o line indexing throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>reports indexing throughput on MS MARCO documents with ColBERT and four other ablation se ings, which individually enable optimizations described in ?3.4 on top of basic batched indexing. Based on these throughputs, ColBERT can index MS MARCO in about three hours. Note that any BERT-based model must incur the computational cost of processing each document at least once. While ColBERT encodes each document with BERT exactly once, existing BERT-based rankers would repeat similar computations on possibly hundreds of documents for each query.</figDesc><table><row><cell>Se ing</cell><cell cols="4">Dimension(m) Bytes/Dim Space(GiBs) MRR@10</cell></row><row><cell>Re-rank Cosine</cell><cell>128</cell><cell>4</cell><cell>286</cell><cell>34.9</cell></row><row><cell>End-to-end L2</cell><cell>128</cell><cell>2</cell><cell>154</cell><cell>36.0</cell></row><row><cell>Re-rank L2</cell><cell>128</cell><cell>2</cell><cell>143</cell><cell>34.8</cell></row><row><cell>Re-rank Cosine</cell><cell>48</cell><cell>4</cell><cell>54</cell><cell>34.4</cell></row><row><cell>Re-rank Cosine</cell><cell>24</cell><cell>2</cell><cell>27</cell><cell>33.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>h ps://blog.google/products/search/search-language-understanding-bert/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>h ps://azure.microso .com/en-us/blog/bing-delivers-its-largest-improvementin-search-experience-using-azure-gpus/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>h ps://github.com/facebookresearch/faiss</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>h ps://github.com/huggingface/transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>h ps://github.com/mit-han-lab/torchpro le</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>h p://anserini.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>In practice, a myriad of reasons could still cause DeepCT's latency to di er slightly from BM25's. For instance, the top-k pruning strategy employed, if any, could interact di erently with a changed distribution of scores.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. OK was supported by the <rs type="funder">Eltoukhy Family Graduate Fellowship at the Stanford School of Engineering</rs>. is research was supported in part by a liate members and other supporters of the <rs type="institution">Stanford DAWN project-Ant Financial, Facebook, Google, Infosys</rs>, <rs type="funder">NEC</rs>, and VMware-as well as Cisco, SAP, and the <rs type="funder">NSF</rs> under <rs type="funder">CAREER</rs> grant <rs type="grantNumber">CNS-1651570</rs>. Any opinions, ndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect the views of the <rs type="funder">National Science Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fEwrdjV">
					<idno type="grant-number">CNS-1651570</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">To Index or Not to Index: Optimizing Exact Maximum Inner Product Search</title>
		<author>
			<persName><forename type="first">Firas</forename><surname>Abuzaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geet</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 35th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1250" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10687</idno>
		<title level="m">Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09217</idno>
		<title level="m">Deeper Text Understanding for IR with Contextual Neural Language Modeling</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for so -matching n-grams in ad-hoc search</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TREC Complex Answer Retrieval Overview</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Filip Radlinski, and Nick Craswell</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In TREC</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cro</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bruce Cro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06902</idno>
		<title level="m">A deep look into neural ranking models for information retrieval</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Let&apos;s measure run time! Extending the IR replicability infrastructure to include performance aspects</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hofst?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Er</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04614</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the e ect of low-frequency terms on neural-IR models</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hofst?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Er</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Navid</forename><surname>Rekabsaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Eickho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1137" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hofst?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Er</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01385</idno>
		<title level="m">TU Wien@ TREC Deep Learning&apos;19-Simple Contextualization for Re-ranking</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">E cient Interaction-based Neural Ranking with Locality Sensitive Hashing</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2858" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">Tinybert: Distilling bert for natural language understanding</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Je</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename><surname>Hijs Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<title level="m">Billion-scale similarity search with GPUs</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online controlled experiments at large scale</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Pohlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cedr: Contextualized embeddings for document ranking</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are Sixteen Heads Really Be er than One?</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07666</idno>
		<title level="m">An Updated Duet Model for Passage Re-ranking</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An introduction to neural information retrieval</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="126" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi ee</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi ee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1291" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Corby</forename><surname>Bhaskar Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emine</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><surname>Yilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03693</idno>
		<title level="m">Incorporating query term independence assumption for e cient retrieval and ranking using deep neural networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage Re-ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">From doc2query to docTTTTTquery</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Epistemic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14424</idno>
		<title level="m">Multi-Stage Document Ranking with BERT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08375</idno>
		<title level="m">Document Expansion by ery Prediction</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Ma Hew E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Ze Lemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07531</idno>
		<title level="m">Understanding the Behaviors of BERT in Ranking</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a uni ed text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin Ra</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Okapi at TREC-3</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gatford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<publisher>NIST Special Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Distilling task-speci c knowledge from BERT into simple neural networks</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12136</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A ention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Anserini: Reproducible ranking baselines using Lucene</title>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data and Information ality (JDIQ)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Critically Examining the&quot; Neural Hype&quot; Weak Baselines and the Additivity of E ectiveness Gains from Neural Ranking Models</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1129" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-domain modeling of sentence-level evidence for document retrieval</title>
		<author>
			<persName><forename type="first">Akkalyoncu</forename><surname>Zeynep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3481" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Guy</forename><surname>O R Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><surname>Wasserblat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06188</idno>
		<title level="m">Q8bert: antized 8bit bert</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bruce Cro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Modeling and solving term mismatch for full-text retrieval</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
