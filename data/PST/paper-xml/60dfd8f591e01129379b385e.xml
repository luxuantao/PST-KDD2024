<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Filter Networks for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="laboratory">Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="institution">Tsinghua University State Key</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Global Filter Networks for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The transformer architecture, originally designed for the natural language processing (NLP) tasks <ref type="bibr" target="#b41">[42]</ref>, has shown promising performance on various vision problems recently <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5]</ref>. Different from convolutional neural networks (CNNs), vision transformer models use self-attention layers to capture long-term dependencies, which are able to learn more diverse interactions between spatial locations. The pure multi-layer perceptrons (MLP) models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> further simplify the vision transformers by replacing the self-attention layers with MLPs that are applied across spatial locations. Since fewer inductive biases are introduced, these two kinds of models have the potential to learn more generic and flexible interactions among spatial locations from raw data. One primary challenge of applying self-attention and pure MLP models to vision tasks is the considerable computational complexity that grows quadratically as the number of tokens increases. Therefore, typical vision transformer style models usually consider a relatively small resolution for the intermediate features (e.g. 14 × 14 tokens are extracted from the input images in both ViT <ref type="bibr" target="#b9">[10]</ref> and MLP-Mixer <ref type="bibr" target="#b37">[38]</ref>). This design may limit the applications of downstream dense prediction tasks like detection and segmentation. A possible solution is to replace the global self-attention with several local self-attention like Swin transformer <ref type="bibr" target="#b26">[27]</ref>. Despite the effectiveness in practice, local self-attention brings quite a few hand-made choices (e.g., window size, padding strategy, etc.) and limits the receptive field of each layer. In this paper, we present a new conceptually simple yet computationally efficient architecture called Global Filter Network (GFNet), which follows the trend of removing inductive biases from vision models while enjoying the log-linear complexity in computation. The basic idea behind our architecture is to learn the interactions among spatial locations in the frequency domain. Different from the self-attention mechanism in vision transformers and the fully connected layers in MLP models, the interactions among tokens are modeled as a set of learnable global filters that are applied to the spectrum of the input features. Since the global filters are able to cover all the frequencies, our model can capture both long-term and short-term interactions. The filters are directly learned from the raw data without introducing human priors. Our architecture is largely based on the vision transformers only with some minimal modifications. We replace the self-attention sub-layer in vision transformers with three key operations: a 2D discrete Fourier transform to convert the input spatial features to the frequency domain, an element-wise multiplication between frequency-domain features and the global filters, and a 2D inverse Fourier transform to map the features back to the spatial domain. Since the Fourier transform is used to mix the information of different tokens, the global filter is much more efficient compared to the self-attention and MLP thanks to the O(L log L) complexity of the fast Fourier transform algorithm (FFT) <ref type="bibr" target="#b6">[7]</ref>. Benefiting from this, the proposed global filter layer is less sensitive to the token length L and thus is compatible with larger feature maps and CNN-style hierarchical architectures without modifications. The overall architecture of GFNet is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. We also compare our global filter with prevalent operations in deep vision models in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Our experiments on ImageNet verify the effectiveness of GFNet. With a similar architecture, our model outperform the recent vision transformer and MLP models including DeiT <ref type="bibr" target="#b39">[40]</ref>, ResMLP <ref type="bibr" target="#b38">[39]</ref> and gMLP <ref type="bibr" target="#b25">[26]</ref>. When using the hierarchical architecture, GFNet can further enlarge the gap. GFNet also works well on downstream transfer learning and semantic segmentation tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Vision transformers. Since Dosovitskiy et al. <ref type="bibr" target="#b9">[10]</ref> introduce transformers to the image classification and achieve a competitive performance compared to CNNs, transformers begin to exhibit their</p><formula xml:id="formula_0">Complexity (FLOPs) # Parameters Depthwise Convolution O(k 2 HW D) k 2 D Self-Attention O(HW D 2 + H 2 W 2 D) 4D 2 Spatial MLP O(H 2 W 2 D) H 2 W 2</formula><p>Global Filter O (HW D log 2 (HW ) + HW D) HW D potential in various vision tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b48">49]</ref>. Recently, there are a large number of works which aim to improve the transformers <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">48]</ref>. These works either seek for better training strategies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b10">11]</ref> or design better architectures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref> or both <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11]</ref>. However, most of the architecture modification of the transformers <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref> introduces additional inductive biases similar to CNNs. In this work, we only focus on the standard transformer architecture <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> and our goal is to replace the heavy self-attention layer (O(L 2 )) to an more efficient operation which can still model the interactions among different spatial locations without introducing the inductive biases associated with CNNs.</p><p>MLP-like models. More recently, there are several works that question the importance of selfattention in the vision transformers and propose to use MLP to replace the self-attention layer in the transformers <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26]</ref>. The MLP-Mixer <ref type="bibr" target="#b37">[38]</ref> employs MLPs to perform token mixing and channel mixing alternatively in each block. ResMLP <ref type="bibr" target="#b38">[39]</ref> adopts a similar idea but substitutes the Layer Normalization with an Affine transformation for acceleration. The recently proposed gMLP <ref type="bibr" target="#b25">[26]</ref> uses a spatial gating unit to re-weight tokens in the spatial dimension. However, all of the above models include MLPs to mix the tokens spatially, which brings two drawbacks: (1) like the self-attention in the transformers, the spatial MLP still requires computational complexity quadratic to the length of tokens. (2) unlike transformers, MLP models are hard to scale up to higher resolution since the weights of the spatial MLPs have fixed sizes. Our work follows this trend and successfully resolves the above issues in MLP-like models. The proposed GFNet enjoys log-linear complexity and can be easily scaled up to any resolution.</p><p>Applications of Fourier transform in vision. Fourier transform has been an important tool in digital image processing for decades <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1]</ref>. With the breakthroughs of CNNs in vision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref>, there are a variety of works that start to incorporate Fourier transform in some deep learning method <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6]</ref> for vision tasks. Some of these works employ discrete Fourier transform to convert the images to the frequency domain and leverage the frequency information to improve the performance in certain tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">46]</ref>, while others utilize the convolution theorem to accelerate the CNNs via fast Fourier transform (FFT) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>. FFC <ref type="bibr" target="#b5">[6]</ref> replaces the convolution in CNNs with an Local Fourier Unit and perform convolutions in the frequency domain. Very recent works also try to leverage Fourier transform to develop deep learning models to solve partial differential equations <ref type="bibr" target="#b24">[25]</ref> and NLP tasks <ref type="bibr" target="#b22">[23]</ref>. In this work, we propose to use learnable filters to interchange information globally among the tokens in the Fourier domain, inspired by the frequency filters in the digital image processing <ref type="bibr" target="#b31">[32]</ref>. We also take advantage of some properties of FFT to reduce the computational costs and the number of parameters.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries: discrete Fourier transform</head><p>We start by introducing the discrete Fourier transform (DFT), which plays an important role in the area of digital signal processing and is a crucial component in our GFNet. For clarity, We first consider the 1D DFT. Given a sequence of N complex numbers x[n], 0 ≤ n ≤ N − 1, the 1D DFT converts the sequence into the frequency domain by:</p><formula xml:id="formula_1">X[k] = N −1 n=0 x[n]e −j(2π/N )kn := N −1 n=0 x[n]W kn N (3.1)</formula><p>Algorithm 1 Pseudocode of Global Filter Layer.</p><p># x: the token features, B x H x W x D (where N = H * W) # K: the frequency-domain filter, H x W_hat x D (where W_hat = W // 2 + 1, see Section 3.2 for details)</p><formula xml:id="formula_2">X = rfft2(x, dim=(1, 2)) X_tilde = X * K x = irfft2(X_tilde, dim=(1, 2))</formula><p>rfft2/irfft2: 2D FFT/IFFT for real signal where j is the imaginary unit and W N = e −j(2π/N ) . The formulation of DFT in Equation (3.1) can be derived from the Fourier transform for continuous signal by sampling in both the time domain and the frequency domain (see Appendix A for details). Since X[k] repeats on intervals of length N , it is suffice to take the value of</p><formula xml:id="formula_3">X[k] at N consecutive points k = 0, 1, . . . , N − 1. Specifically, X[k]</formula><p>represents to the spectrum of the sequence x[n] at the frequency</p><formula xml:id="formula_4">ω k = 2πk/N .</formula><p>It is also worth noting that DFT is a one-to-one transformation. Given the DFT X[k], we can recover the original signal x[n] by the inverse DFT (IDFT):</p><formula xml:id="formula_5">x[n] = 1 N N −1 k=0 X[k]e j(2π/N )kn . (3.2) For real input x[n], it can be proved that (see Appendix A) its DFT is conjugate symmetric, i.e., X[N − k] = X * [k].</formula><p>The reverse is true as well: if we perform IDFT to X[k] which is conjugate symmetric, a real discrete signal can be recovered. This property implies that the half of the DFT</p><formula xml:id="formula_6">{X[k] : 0 ≤ k ≤ N/2 } contains the full information about the frequency characteristics of x[n].</formula><p>DFT is widely used in modern signal processing algorithms for mainly two reasons: (1) the input and output of DFT are both discrete thus can be easily processed by computers; (2) there exist efficient algorithms for computing the DFT. The fast Fourier transform (FFT) algorithms take advantage of the symmetry and periodicity properties of W kn N and reduce the complexity to compute DFT from O(N 2 ) to O(N log N ). The inverse DFT (3.2), which has a similar form to the DFT, can also be computed efficiently using the inverse fast Fourier transform (IFFT).</p><p>The DFT described above can be extend to 2D signals.</p><formula xml:id="formula_7">Given the 2D signal X[m, n], 0 ≤ m ≤ M − 1, 0 ≤ n ≤ N − 1, the 2D DFT of x[m, n] is given by: X[u, v] = M −1 m=0 N −1 n=0 x[m, n]e −j2π( um M + vn N ) .<label>(3.3)</label></formula><p>The 2D DFT can be viewed as performing 1D DFT on the two dimensions alternatively. Similar to 1D DFT, 2D DFT of real input x[m, n] satisfied the conjugate symmetry property</p><formula xml:id="formula_8">X[M − u, N − v] = X * [u, v].</formula><p>The FFT algorithms can also be applied to 2D DFT to improve computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Filter Networks</head><p>Overall architecture. Recent advances in vision transformers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> demonstrate that models based on self-attention can achieve competitive performance even without the inductive biases associated with the convolutions. Henceforth, there are several works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref> that exploit approaches (e.g., MLPs) other than self-attention to mix the information among the tokens. The proposed Global Filter Networks (GFNet) follows this line of work and aims to replace the heavy self-attention layer (O(N 2 )) with a simpler and more efficient one.</p><p>The overall architecture of our model is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Our model takes as an input H × W non-overlapping patches and projects the flattened patches into L = HW tokens with dimension D. The basic building block of GFNet consists of: 1) a global filter layer that can exchange spatial information efficiently (O(L log L)); 2) a feedforward network (FFN) as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>. The output tokens of the last block are fed into a global average pooling layer followed by a linear classifier.</p><p>Global filter layer. We propose global filter layer as an alternative to the self-attention layer which can mix tokens representing different spatial locations. Given the tokens x ∈ R H×W ×D , we first perform 2D FFT (see Section 3.1) along the spatial dimensions to convert x to the frequency domain:</p><formula xml:id="formula_9">X = F[x] ∈ C H×W ×D ,<label>(3.4)</label></formula><p>where F[•] denotes the 2D FFT. Note that X is a complex tensor and represents the spectrum of x.</p><p>We can then modulate the spectrum by multiplying a learnable filter K ∈ C H×W ×D to the X:</p><formula xml:id="formula_10">X = K X,<label>(3.5)</label></formula><p>where is the element-wise multiplication (also known as the Hadamard product). The filter K is called the global filter since it has the same dimension with X, which can represent an arbitrary filter in the frequency domain. Finally, we adopt the inverse FFT to transform the modulated spectrum X back to the spatial domain and update the tokens:</p><formula xml:id="formula_11">x ← F −1 [ X]. (3.6)</formula><p>The formulation of the global filter layer is motivated by the frequency filters in the digital image processing <ref type="bibr" target="#b31">[32]</ref>, where the global filter K can be regarded as a set of learnable frequency filters for different hidden dimensions. It can be proved (see Appendix A) that the global filter layer is equivalent to a depthwise global circular convolution with the filter size H × W . Therefore, the global filter layer is different from the standard convolutional layer which adopts a relatively small filter size to enforce the inductive biases of the locality. We also find although the proposed global filter can also be interpreted as a spatial domain operation, the filters learned in our networks exhibit more clear patterns in the frequency domain than the spatial domain, which indicates our models tend to capture relation in the frequency domain instead of spatial domain (see Figure <ref type="figure" target="#fig_3">4</ref>). Note that the global filter implemented in the frequency domain is also much more efficient compared to the spatial domain, which enjoys a complexity of O(DL log L) while the vanilla depthwise global circular convolution in the spatial domain has O(DL 2 ) complexity. We will also show that the global filter layer is better than its local convolution counterparts in the experiments.</p><p>It is also worth noting that in the implementation, we make use of the property of DFT to reduce the redundant computation. Since x is a real tensor, its DFT X is conjugate symmetric, i.e. X[H − u, W − v, :] = X * [H, W, :]. Therefore, we can take only the half of the values in the X but preserve the full information at the same time:</p><formula xml:id="formula_12">X r = X[:, 0 : W ] := F r [x], W = W/2 ,<label>(3.7)</label></formula><p>where F r denotes the 2D FFT for real input. In this way, we can implement the global filter as K r ∈ C H× W ×D , which can reduce half the parameters. This can also ensure F −1 r [K r X r ] is a real tensor, thus it can be added directly to the input x. The global filter layer can be easily in modern deep learning frameworks (e.g., PyTorch <ref type="bibr" target="#b30">[31]</ref>), as is shown in Algorithm 1. The FFT and IFFT are well supported by GPU and CPU thanks to the acceleration libraries like cuFFT and mkl-fft, which makes our models perform well on hardware.</p><p>Relationship to other transformer-style models. The GFNet follows the line of research about the exploration of approaches to mix the tokens. Compared to existing architectures like vision transformers and pure MLP models, we exhibit that GFNet has several favorable properties: 1) GFNet is more efficient. The complexity of both the vision transformers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> and the MLP models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> is O(L 2 ). Different from them, global filter layer only consists an FFT (O(L log L)), an element-wise multiplication (O(L)) and an IFFT (O(L log L)), which means the total computational complexity is O(L log L). 2) Although pure MLP models are simpler compared to transformers, it is hard to fine-tune them on higher resolution (e.g., from 224 × 224 resolution to 384 × 384 resolution) since they can only process a fixed number of tokens. As opposed to pure MLP models, we will show that our GFNet can be easily scaled up to higher resolution. Our model is more flexible since both the FFT and the IFFT have no learnable parameters and can process sequences with arbitrary length. We can simply interpolate the global filter K to K ∈ C H ×W ×D for different inputs, where H × W is the target size. The interpolation is reasonable due to the property of DFT. Each element of the global filter K[u, v] corresponds to the spectrum of the filter at ω u = 2πu/H, ω v = 2πv/W and thus, the global filter K can be viewed as a sampling of a continuous spectrum K(ω u , ω v ), where ω u , ω v ∈ [0, 2π]. Hence, changing the resolution is equivalent to changing the sampling interval of K(ω u , ω v ). Therefore, we only need to perform interpolation to shift from one resolution to another. We also notice recently a concurrent work FNet <ref type="bibr" target="#b22">[23]</ref> leverages Fourier transform to mix tokens. Our work is distinct from FNet in three aspects: (1) FNet performs FFT to the input and directly adds the real part of the spectrum to the input tokens, which blends the information from different domains (spatial/frequency) together. On the other hand, GFNet draws motivation from the frequency filters, which is more reasonable. (2) FNet only keeps the real part of the spectrum. Note that the spectrum of real input is conjugate symmetric, which means the real part is exactly symmetric and thus contains redundant information. Our GFNet, however, utilizes this property to simplify the computation. (3) FNet is designed for NLP tasks, while our GFNet focuses on vision tasks. In our experiments, we also implement the FNet and show that our model outperforms it.</p><p>Architecture variants. Due to the limitation from the quadratic complexity in the self-attention, vision transformers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> are usually designed to process a relatively small feature map (e.g., 14 × 14). However, our GFNet, which enjoys log-linear complexity, avoids that problem. Since in our GFNet the computational costs do not grow such significantly when the feature map size increases, we can adopt a hierarchical architecture inspired by the success of CNNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14]</ref>. Generally speaking, we can start from a large feature map (e.g., 56 × 56) and gradually perform downsampling after a few blocks. In this paper, we mainly investigate two kinds of variants of GFNet: transformer-style models with a fixed number of tokens in each block and CNN-style hierarchical models with gradually downsampled tokens. For transformer-style models, we begin with a 12-layer model (GFNet-XS) with a similar architecture with DeiT-S and ResMLP-12. Then, we obtain 3 variants of the model (GFNet-Ti, GFNet-S and GFNet-B) by simply adjusting the depth and embedding dimension, which have similar computational costs with ResNet-18, 50 and 101 <ref type="bibr" target="#b13">[14]</ref>. For hierarchical models, we also design three models (GFNet-H-Ti, GFNet-H-S and GFNet-H-B) that have these three levels of complexity following the design of PVT <ref type="bibr" target="#b42">[43]</ref>. We use 4 × 4 patch embedding to form the input tokens and use a non-overlapping convolution layer to downsample tokens following <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b26">27]</ref>. Unlike PVT <ref type="bibr" target="#b42">[43]</ref> and Swin <ref type="bibr" target="#b26">[27]</ref>, we directly apply our building block on different stages without any modifications. The detailed architectures are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments to verify the effectiveness of our GFNet. We present the main results on ImageNet <ref type="bibr" target="#b7">[8]</ref> and compare them with various architectures. We also test our models on the downstream transfer learning datasets including CIFAR-10/100 <ref type="bibr" target="#b19">[20]</ref>, Stanford Cars <ref type="bibr" target="#b18">[19]</ref> and Flowers-102 <ref type="bibr" target="#b29">[30]</ref>. Lastly, we investigate the efficiency and robustness of the proposed models and provide visualization to have an intuitive understanding of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ImageNet Classification</head><p>Setups. We conduct our main experiments on ImageNet <ref type="bibr" target="#b7">[8]</ref>, which is a widely used large-scale benchmark for image classification. ImageNet contains roughly 1.2M images from 1,000 categories. Following common practice <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40]</ref>, we train our models on the training set of ImageNet and report the single-crop top-1 accuracy on 50,000 validation images. To fairly compare with previous works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref>, we follow the most training details for our models and do not add extra regularization methods like <ref type="bibr" target="#b17">[18]</ref>. Different from <ref type="bibr" target="#b39">[40]</ref>, we does not use EMA model <ref type="bibr" target="#b32">[33]</ref>, RandomEarse <ref type="bibr" target="#b49">[50]</ref> and repeated augmentation <ref type="bibr" target="#b16">[17]</ref>, which are important to train DeiT while sightly hurting the performance of our models. We set the gradient clipping norm to 1 for all of our models. During finetuning at the higher resolution, we use the hyper-parameters suggested by the implementation of <ref type="bibr" target="#b39">[40]</ref> and train the model for 30 epochs. All of our models are trained on a single machine with 8 GPUs. More details can be found in Appendix B.</p><p>Comparisons with transformer-style architectures. The results are presented in Table <ref type="table" target="#tab_2">3</ref>. We compare our method with different transformer-style architectures for image classification including vision transformers (DeiT <ref type="bibr" target="#b39">[40]</ref>) and MLP-like models (ResMLP <ref type="bibr" target="#b38">[39]</ref> and gMLP <ref type="bibr" target="#b25">[26]</ref>) that have similar complexity and number of parameters. We see that our method can clearly outperform recent MLP-like models such as ResMLP <ref type="bibr" target="#b38">[39]</ref> and gMLP <ref type="bibr" target="#b25">[26]</ref>, and show similar performance to DeiT. Specifically, GFNet-XS outperforms ResMLP-12 by 2.0% while having slightly fewer FLOPs. GFNet-S also achieves better top-1 accuracy compared to gMLP-S and DeiT-S. Our tiny model is significantly better compared to both DeiT-Ti (+2.4%) and gMLP-Ti (+2.6%) with the similar level of complexity.</p><p>Comparisons with hierarchical architectures. We compare different kinds of hierarchical models in Figure <ref type="figure" target="#fig_3">4</ref>. ResNet <ref type="bibr" target="#b13">[14]</ref> is the most widely used convolutional model while RegNet <ref type="bibr" target="#b33">[34]</ref> is a family of carefully designed CNN models. We also compare with recent hierarchical vision transformers PVT <ref type="bibr" target="#b42">[43]</ref> and Swin <ref type="bibr" target="#b26">[27]</ref>. Benefiting from the log-linear complexity, GFNet-H models show significantly better performance than ResNet, RegNet and PVT and achieve similar performance with Swin while having a much simpler and more generic design.</p><p>Fine-tuning at higher resolution. One prominent problem of MLP-like models is that the feature resolution is not adjustable. On the contrary, the proposed global filter is more flexible. We demonstrate the advantage of GFNet by finetuning the model trained at 224 × 224 resolution to higher resolution following the practice in vision transformers <ref type="bibr" target="#b39">[40]</ref>. As shown in Table <ref type="table" target="#tab_2">3</ref>, our model can easily adapt to higher resolution with only 30 epoch finetuning and achieve better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transfer learning</head><p>To test the generality of our architecture and the learned representation, we evaluate GFNet on a set of commonly used transfer learning benchmark datasets including CIFAR-10 <ref type="bibr" target="#b19">[20]</ref>, CIFAR-100 <ref type="bibr" target="#b19">[20]</ref>,</p><p>Stanford Cars <ref type="bibr" target="#b18">[19]</ref> and Flowers-102 <ref type="bibr" target="#b29">[30]</ref>. We follow the setting of previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>, where the model is initialized by the ImageNet pre-trained weights and finetuned on the new datasets. We evaluate the transfer learning performance of our basic model and best model. The results are presented in Table <ref type="table" target="#tab_4">5</ref>. The proposed models generally work well on downstream datasets. GFNet models outperform ResMLP models by a large margin and achieve very competitive performance with state-of-the-art EfficientNet-B7. Our models also show competitive performance compared to state-of-the-art CNNs and vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis and visualization</head><p>Efficiency of GFNet. We demonstrate the efficiency of our GFNet in Figure <ref type="figure">2</ref>, where the models are compared in theoretical FLOPs, actual latency and peak memory usage on GPU. We test a single building block of each model (including one token mixing layer and one FFN) with respect to the different numbers of tokens and set the feature dimension and batch size to 384 and 32 respectively. The self-attention model quickly runs out of memory when feature resolution exceeds 56 2 , which is also the feature resolution of our hierarchical model. The advantage of the proposed architecture becomes larger as the resolution increases, which strongly shows the potential of our model in vision tasks requiring high-resolution feature maps.   Complexity/accuracy trade-offs. We show the computational complexity and accuracy trade-offs of various transformer-style architectures in Figure <ref type="figure">3</ref>. It is clear that GFNet achieves the best trade-off among all kinds of models.</p><p>Ablation study on the global filter. To more clearly show the effectiveness of the proposed global filters, we compare GFNet-XS with several baseline models that are equipped with different token mixing operations. The results are presented in Table <ref type="table" target="#tab_6">6</ref>. All models have a similar building block ( token mixing layer + FFN ) and the same feature dimension of D = 384. We also implement the recent FNet <ref type="bibr" target="#b22">[23]</ref> for comparison, where a 1D FFT on feature dimension and a 2D FFT on spatial dimensions are used to mix tokens. As shown in Table <ref type="table" target="#tab_6">6</ref>, our method outperforms all baseline methods except DeiT-S that has 64% higher FLOPs.</p><p>Robustness    Visualization. The core operation in GFNet is the element-wise multiplication between frequencydomain features and the global filter. Therefore, it is easy to visualize and interpret. We visualize the frequency domain filters as well as their corresponding spatial domain filters in Figure <ref type="figure" target="#fig_3">4</ref>. The learned global filters have more clear patterns in the frequency domain, where different layers have different characteristics. Interestingly, the filters in the last layer particularly focus on the low-frequency component. The corresponding filters in the spatial domain are less interpretable for humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented the Global Filter Network (GFNet), which is a conceptually simple yet computationally efficient architecture for image classification. Our model replaces the self-attention sub-layer in vision transformer with 2D FFT/IFFT and a set of learnable global filters in the frequency domain.</p><p>Benefiting from the token mixing operation with log-linear complexity, our architecture is highly efficient. Our experimental results demonstrated that GFNet can be a very competitive alternative to vision transformers, MLP-like models and CNNs in accuracy/complexity trade-offs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall architecture of the Global Filter Network. Our architecture is based on Vision Transformer (ViT) models with some minimal modifications. We replace the self-attention sub-layer with the proposed global filter layer, which consists of three key operations: a 2D discrete Fourier transform to convert the input spatial features to the frequency domain, an element-wise multiplication between frequency-domain features and the global filters, and a 2D inverse Fourier transform to map the features back to the spatial domain. The efficient fast Fourier transform (FFT) enables us to learn arbitrary interactions among spatial locations with log-linear complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Comparisons among GFNet, ViT<ref type="bibr" target="#b9">[10]</ref> and ResMLP<ref type="bibr" target="#b38">[39]</ref> in (a) FLOPs (b) latency and (c) GPU memory with respect to the number of tokens (feature resolution). The dotted lines indicate the estimated values when the GPU memory has run out. The latency and GPU memory is measured using a single NVIDIA RTX 3090 GPU with batch size 32 and feature dimension 384.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the learned global filters in GFNet-XS. We visualize the original frequency domain global filters in (a) and show the corresponding spatial domain filters for the first 6 columns in (b). There are more clear patterns in the frequency domain than the spatial domain.</figDesc><graphic url="image-12.png" coords="10,135.09,234.20,288.44,144.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( c )</head><label>c</label><figDesc>Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?[No]  We follow the common practice used in our baseline methods, where no error bars are reported. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See our implementation details in the experiment part. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of the proposed Global Filter with prevalent operations in deep vision models. H, W and D are the height, width and the number of channels of the feature maps. k is the kernel size of the convolution operation. The proposed global filter is much more efficient than self-attention and spatial MLP.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed configurations of different variants of GFNet. For hierarchical models, we provide the number of channels and blocks in 4 stages. The FLOPs are calculated with 224 × 224 input.</figDesc><table><row><cell>Model</cell><cell>#Blocks</cell><cell>#Channels</cell><cell cols="2">Params (M) FLOPs (G)</cell></row><row><cell>GFNet-Ti</cell><cell>12</cell><cell>256</cell><cell>7</cell><cell>1.3</cell></row><row><cell>GFNet-XS</cell><cell>12</cell><cell>384</cell><cell>16</cell><cell>2.9</cell></row><row><cell>GFNet-S</cell><cell>19</cell><cell>384</cell><cell>25</cell><cell>4.5</cell></row><row><cell>GFNet-B</cell><cell>19</cell><cell>512</cell><cell>43</cell><cell>7.9</cell></row><row><cell>GFNet-H-Ti</cell><cell>[3, 3, 10, 3]</cell><cell>[64, 128, 256, 512]</cell><cell>15</cell><cell>2.1</cell></row><row><cell>GFNet-H-S</cell><cell>[3, 3, 10, 3]</cell><cell>[96, 192, 384, 768]</cell><cell>32</cell><cell>4.6</cell></row><row><cell>GFNet-H-B</cell><cell>[3, 3, 27, 3]</cell><cell>[96, 192, 384, 768]</cell><cell>54</cell><cell>8.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with transformer-style architectures on ImageNet. We compare different transformer-style architectures for image classification including vision transformers<ref type="bibr" target="#b39">[40]</ref>, MLP-like models<ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b25">26]</ref> and our models that have comparable FLOPs and the number of parameters. We report the top-1 accuracy on the validation set of ImageNet as well as the number of parameters and FLOPs. All of our models are trained with 224 × 224 images. We use "↑384" to represent models finetuned on 384 × 384 images for 30 epochs.</figDesc><table><row><cell>Model</cell><cell cols="5">Params (M) FLOPs (G) Resolution Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>DeiT-Ti [40]</cell><cell>5</cell><cell>1.2</cell><cell>224</cell><cell>72.2</cell><cell>91.1</cell></row><row><cell>gMLP-Ti [26]</cell><cell>6</cell><cell>1.4</cell><cell>224</cell><cell>72.0</cell><cell>-</cell></row><row><cell>GFNet-Ti</cell><cell>7</cell><cell>1.3</cell><cell>224</cell><cell>74.6</cell><cell>92.2</cell></row><row><cell>ResMLP-12 [39]</cell><cell>15</cell><cell>3.0</cell><cell>224</cell><cell>76.6</cell><cell>-</cell></row><row><cell>GFNet-XS</cell><cell>16</cell><cell>2.9</cell><cell>224</cell><cell>78.6</cell><cell>94.2</cell></row><row><cell>DeiT-S [40]</cell><cell>22</cell><cell>4.6</cell><cell>224</cell><cell>79.8</cell><cell>95.0</cell></row><row><cell>gMLP-S [26]</cell><cell>20</cell><cell>4.5</cell><cell>224</cell><cell>79.4</cell><cell>-</cell></row><row><cell>GFNet-S</cell><cell>25</cell><cell>4.5</cell><cell>224</cell><cell>80.0</cell><cell>94.9</cell></row><row><cell>ResMLP-36 [39]</cell><cell>45</cell><cell>8.9</cell><cell>224</cell><cell>79.7</cell><cell>-</cell></row><row><cell>GFNet-B</cell><cell>43</cell><cell>7.9</cell><cell>224</cell><cell>80.7</cell><cell>95.1</cell></row><row><cell>GFNet-XS↑384</cell><cell>18</cell><cell>8.4</cell><cell>384</cell><cell>80.6</cell><cell>95.4</cell></row><row><cell>DeiT-B [40]</cell><cell>86</cell><cell>17.5</cell><cell>224</cell><cell>81.8</cell><cell>95.6</cell></row><row><cell>gMLP-B [26]</cell><cell>73</cell><cell>15.8</cell><cell>224</cell><cell>81.6</cell><cell>-</cell></row><row><cell>GFNet-S↑384</cell><cell>28</cell><cell>13.2</cell><cell>384</cell><cell>81.7</cell><cell>95.8</cell></row><row><cell>GFNet-B↑384</cell><cell>47</cell><cell>23.3</cell><cell>384</cell><cell>82.1</cell><cell>95.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with hierarchical architectures on ImageNet. We compare different hierarchical architectures for image classification including convolutional neural networks<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, hierarchical vision transformers<ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b26">27]</ref> and our hierarchical models that have comparable FLOPs and number of parameters. We report the top-1 accuracy on the validation set of ImageNet as well as the number of parameters and FLOPs. All models are trained and tested with 224 × 224 images.</figDesc><table><row><cell>Model</cell><cell cols="4">Params (M) FLOPs (G) Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>ResNet-18 [14]</cell><cell>12</cell><cell>1.8</cell><cell>69.8</cell><cell>89.1</cell></row><row><cell>RegNetY-1.6GF [34]</cell><cell>11</cell><cell>1.6</cell><cell>78.0</cell><cell>-</cell></row><row><cell>PVT-Ti [26]</cell><cell>13</cell><cell>1.9</cell><cell>75.1</cell><cell>-</cell></row><row><cell>GFNet-H-Ti</cell><cell>15</cell><cell>2.1</cell><cell>80.1</cell><cell>95.1</cell></row><row><cell>ResNet-50 [40]</cell><cell>26</cell><cell>4.1</cell><cell>76.1</cell><cell>92.9</cell></row><row><cell>RegNetY-4.0GF [34]</cell><cell>21</cell><cell>4.0</cell><cell>80.0</cell><cell>-</cell></row><row><cell>PVT-S [26]</cell><cell>25</cell><cell>3.8</cell><cell>79.8</cell><cell>-</cell></row><row><cell>Swin-Ti [27]</cell><cell>29</cell><cell>4.5</cell><cell>81.3</cell><cell>-</cell></row><row><cell>GFNet-H-S</cell><cell>32</cell><cell>4.6</cell><cell>81.5</cell><cell>95.6</cell></row><row><cell>ResNet-101 [40]</cell><cell>45</cell><cell>7.9</cell><cell>77.4</cell><cell>93.5</cell></row><row><cell>RegNetY-8.0GF [34]</cell><cell>39</cell><cell>8.0</cell><cell>81.7</cell><cell>-</cell></row><row><cell>PVT-M [26]</cell><cell>44</cell><cell>6.7</cell><cell>81.2</cell><cell>-</cell></row><row><cell>Swin-S [27]</cell><cell>50</cell><cell>8.7</cell><cell>83.0</cell><cell>-</cell></row><row><cell>GFNet-H-B</cell><cell>54</cell><cell>8.6</cell><cell>82.9</cell><cell>96.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on transfer learning datasets. We report the top-1 accuracy on the four datasets as well as the number of parameters and FLOPs.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPs Params</cell><cell>CIFAR-10</cell><cell cols="2">CIFAR-100 Flowers-102</cell><cell>Cars-196</cell></row><row><cell>ResNet50 [14]</cell><cell>4.1G</cell><cell>26M</cell><cell>-</cell><cell>-</cell><cell>96.2</cell><cell>90.0</cell></row><row><cell>EfficientNet-B7 [37]</cell><cell>37G</cell><cell>66M</cell><cell>98.9</cell><cell>91.7</cell><cell>98.8</cell><cell>94.7</cell></row><row><cell>ViT-B/16 [10]</cell><cell>55.4G</cell><cell>86M</cell><cell>98.1</cell><cell>87.1</cell><cell>89.5</cell><cell>-</cell></row><row><cell>ViT-L/16 [10]</cell><cell>190.7G</cell><cell>307M</cell><cell>97.9</cell><cell>86.4</cell><cell>89.7</cell><cell>-</cell></row><row><cell>Deit-B/16 [40]</cell><cell>17.5G</cell><cell>86M</cell><cell>99.1</cell><cell>90.8</cell><cell>98.4</cell><cell>92.1</cell></row><row><cell>ResMLP-12 [39]</cell><cell>3.0G</cell><cell>15M</cell><cell>98.1</cell><cell>87.0</cell><cell>97.4</cell><cell>84.6</cell></row><row><cell>ResMLP-24 [39]</cell><cell>6.0G</cell><cell>30M</cell><cell>98.7</cell><cell>89.5</cell><cell>97.9</cell><cell>89.5</cell></row><row><cell>GFNet-XS</cell><cell>2.9G</cell><cell>16M</cell><cell>98.6</cell><cell>89.1</cell><cell>98.1</cell><cell>92.8</cell></row><row><cell>GFNet-H-B</cell><cell>8.6G</cell><cell>54M</cell><cell>99.0</cell><cell>90.3</cell><cell>98.8</cell><cell>93.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparisons among the GFNet and other variants based on the transformer-like architecture on ImageNet. We show that GFNet outperforms the ResMLP<ref type="bibr" target="#b38">[39]</ref>, FNet<ref type="bibr" target="#b22">[23]</ref> and models with local depthwise convolutions. We also report the number of parameters and theoretical complexity in FLOPs.</figDesc><table><row><cell>Model</cell><cell cols="3">Acc Param FLOPs (%) (M) (G)</cell></row><row><cell>DeiT-S [40]</cell><cell>79.8</cell><cell>22</cell><cell>4.6</cell></row><row><cell cols="2">Local Conv (3 × 3) 77.7</cell><cell>15</cell><cell>2.8</cell></row><row><cell cols="2">Local Conv (5 × 5) 78.1</cell><cell>15</cell><cell>2.9</cell></row><row><cell cols="2">Local Conv (7 × 7) 78.2</cell><cell>15</cell><cell>2.9</cell></row><row><cell>ResMLP [39]</cell><cell>76.6</cell><cell>15</cell><cell>3.0</cell></row><row><cell>FNet [23]</cell><cell>71.2</cell><cell>15</cell><cell>2.9</cell></row><row><cell>GFNet-XS</cell><cell>78.6</cell><cell>16</cell><cell>2.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>&amp; generalization ability. Inspired by the<ref type="bibr" target="#b28">[29]</ref>, we further conduct experiments to evaluate the robustness and the generalization ability of the GFNet. For robustness, we consider</figDesc><table><row><cell>ImageNet-A, ImageNet-C, FGSM and PGD. ImageNet-A [16] (IN-A) is a challenging dataset that</cell></row><row><cell>contains natural adversarial examples. ImageNet-C [15] (IN-C) is used to validate the robustness of</cell></row><row><cell>the model under various types of corruption. We use the mean corruption error (mCE, lower is better)</cell></row><row><cell>on ImageNet-C as the evaluation metric. FGSM [12] and PGD [28] are two widely used algorithms</cell></row><row><cell>that are targeted to evaluate the adversarial robustness of the model by single-step attack and multi-</cell></row><row><cell>step attack, respectively. For generalization ability, we adopt two variants of ImageNet validation</cell></row></table><note>set: ImageNet-V2<ref type="bibr" target="#b35">[36]</ref> (IN-V2) and ImageNet-Real<ref type="bibr" target="#b1">[2]</ref> (IN-Real). ImageNet-V2 is a re-collected version of ImageNet validation set following the same data collection procedure of ImageNet, while ImageNet-Real contains the same images as ImageNet validation set but has reassessed labels. We compare GFNet-S with various baselines in Table7including CNNs, Transformers and MLP-like architectures and find the GFNet enjoys both favorable robustness and generalization ability.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Evaluation of robustness and generalization ability. We measure the robustness from different aspects, including the adversarial robustness by adopting adversarial attack algorithms including FGSM and PGD and the performance on corrupted/out-of-distribution datasets including ImageNet-A<ref type="bibr" target="#b15">[16]</ref> (top-1 accuracy) and ImageNet-C<ref type="bibr" target="#b14">[15]</ref> (mCE, lower is better). The generalization ability is evaluated on ImageNet-V2<ref type="bibr" target="#b35">[36]</ref> and ImageNet-Real<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPs Params</cell><cell cols="2">ImageNet</cell><cell cols="2">Generalization</cell><cell></cell><cell cols="2">Robustness</cell><cell></cell></row><row><cell></cell><cell>(G)</cell><cell>(M)</cell><cell cols="8">Top-1↑ Top-5↑ IN-V2↑ IN-Real↑ FGSM↑ PGD↑ IN-C↓ IN-A↑</cell></row><row><cell>ResNet-50 [14]</cell><cell>4.1</cell><cell>26</cell><cell>76.1</cell><cell>92.9</cell><cell>67.4</cell><cell>85.8</cell><cell>12.2</cell><cell>0.9</cell><cell>76.7</cell><cell>0.0</cell></row><row><cell>ResNeXt50-32x4d [45]</cell><cell>4.3</cell><cell>25</cell><cell>79.8</cell><cell>94.6</cell><cell>68.2</cell><cell>85.2</cell><cell>34.7</cell><cell>13.5</cell><cell>64.7</cell><cell>10.7</cell></row><row><cell>DeiT-S [40]</cell><cell>4.6</cell><cell>22</cell><cell>79.8</cell><cell>95.0</cell><cell>68.4</cell><cell>85.6</cell><cell>40.7</cell><cell>16.7</cell><cell>54.6</cell><cell>18.9</cell></row><row><cell>ResMLP-12 [39]</cell><cell>3.0</cell><cell>15</cell><cell>76.6</cell><cell>93.2</cell><cell>64.4</cell><cell>83.3</cell><cell>23.9</cell><cell>8.5</cell><cell>66.0</cell><cell>7.1</cell></row><row><cell>GFNet-S</cell><cell>4.5</cell><cell>25</cell><cell>80.1</cell><cell>94.9</cell><cell>68.5</cell><cell>85.8</cell><cell>42.6</cell><cell>21.0</cell><cell>53.8</cell><cell>14.3</cell></row><row><cell>layer 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>layer 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 62125603, Grant 61822603, Grant U1813218, Grant U1713214, in part by Beijing Academy of Artificial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua University.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Digital image processing: principles and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><surname>Baxes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet?</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast fourier convolution</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex fourier series</title>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Circnn: accelerating and compressing deep neural networks using block-circulant weight matrices</title>
		<author>
			<persName><forename type="first">Caiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="395" to="408" />
			<pubPlace>MICRO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2009">2020. 1, 2, 3, 4, 5, 6, 8, 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improve vision transformers training by suppressing over-smoothing</title>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12753</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2016. 3, 6, 7, 8</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<title level="m">Benchmarking neural network robustness to common corruptions and perturbations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhyeok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Rae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fnet: Mixing tokens with fourier transforms</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Falcon: A fourier transform based approach for fast and secure convolutional neural network predictions</title>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiping</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenkai</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8705" to="8714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fourier neural operator for parametric partial differential equations</title>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burigede</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>ICLR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
				<imprint>
			<date type="published" when="2008">2021. 2, 3, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2008">2021. 1, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking the design principles of robust vision transformer</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gege</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07926</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Digital image processing algorithms and applications</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Pitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoli</forename><forename type="middle">B</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2005">2021. 1, 3, 4, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
				<imprint>
			<date type="published" when="2009">2021. 1, 2, 3, 4, 5, 6, 7, 8, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2009">2020. 1, 2, 3, 4, 5, 6, 7, 8, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pointr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName><forename type="first">Xumin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><surname>Philip Hs Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
