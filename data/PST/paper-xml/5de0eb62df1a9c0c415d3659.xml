<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xingda</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Human Factors and Ergonomics</orgName>
								<orgName type="department" key="dep2">College of Mechatronics and Control Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Human Factors and Ergonomics</orgName>
								<orgName type="department" key="dep2">College of Mechatronics and Control Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Human Factors and Ergonomics</orgName>
								<orgName type="department" key="dep2">College of Mechatronics and Control Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Human Factors and Ergonomics</orgName>
								<orgName type="department" key="dep2">College of Mechatronics and Control Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F9C9C05CF4ABBBE65D32793B8AF557E5</idno>
					<idno type="DOI">10.1109/TIE.2019.2945295</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2019.2945295, IEEE Transactions on Industrial Electronics received April 10, 2019; revised August 23, 2019; accepted September 23, 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Driver assistance systems</term>
					<term>driving safety</term>
					<term>pedestrian detection</term>
					<term>deep learning</term>
					<term>hazy weather</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effectively detecting pedestrians in various environments would significantly improve driving safety for autonomous vehicles. However, the degraded visibility and blurred outline and appearance of pedestrian images captured during hazy weather strongly limit the effectiveness of current pedestrian detection methods. To solve this problem, this paper presents three novel deep learning approaches based on Yolo. The depthwise separable convolution and linear bottleneck skills were used to reduce the computational cost and number of parameters, rendering our network more efficient. We also innovatively developed a weighted combination layer in one of the approaches by combining multi-scale feature maps and a squeeze and excitation block. Collected pedestrian images in hazy weather were augmented using six strategies to enrich the database. Experimental results show that our proposed methods can effectively detect pedestrians in hazy weather, significantly outperforming state-of-the-art methods in both accuracy and speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>detection methods based on deep learning architectures have also been widely used <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>.</p><p>However, most of the pedestrian detection models were tested only under clear-day conditions. Cai et al. used multi bounding box regressors with different IoU (intersection over union) thresholds to build a cascade architecture (Cascade R-CNN) for detection <ref type="bibr" target="#b14">[15]</ref>. RefineDet, a single-shot network, used 2 bounding regressors for detection, one for anchor box adjustment and the other for regression of the final bounding box <ref type="bibr" target="#b15">[16]</ref>. Recent improvements on the anchor-free detection network <ref type="bibr" target="#b16">[17]</ref> and generalized IoU loss definition <ref type="bibr" target="#b17">[18]</ref> were also conducted in detection tasks. These methods do not have the ability to detect pedestrians under poor lighting conditions (e.g., in hazy weather). The diminished contrast during hazy weather makes it more difficult for drivers to clearly see signs and pedestrians, and thus increases the likelihood of accident occurrence. In fact, it was well reported that the number of traffic accidents in hazy weather was higher than that in clear days <ref type="bibr" target="#b18">[19]</ref>. Generally, the denser the haze, the greater the accident rate <ref type="bibr" target="#b19">[20]</ref>. Therefore, detecting pedestrians in haze is particularly challenging because of the degraded visibility, undesirable color cast, and blurred outline and appearance of pedestrians, making it difficult to distinguish them from the background.</p><p>To solve the object detection problem in hazy weather, many de-haze algorithms were proposed to restore and increase the contrast of images in previous studies. Huang et al. proposed a Laplacian-based visibility restoration approach to effectively solve inadequate haze thickness estimation problems <ref type="bibr" target="#b20">[21]</ref>. Tan developed a de-haze algorithm based on his hypothesis of image contrast and atmospheric light <ref type="bibr" target="#b21">[22]</ref>. He et al. developed a de-haze algorithm based on the dark channel priori <ref type="bibr" target="#b22">[23]</ref>. Chen et al. showed a high-speed refinement method based on gain intervention to remove haze in images by combining the dark channel prior strategy <ref type="bibr" target="#b23">[24]</ref>. These algorithms are mainly applied to daytime scenes under the assumption that the light distribution is uniform. However, most haze conditions occur under dim light, which makes the existing de-haze algorithms unsuitable.</p><p>To alleviate this, we developed three pedestrian detection models for use during hazy conditions based on Yolo (You Lock Only Once) <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b25">[26]</ref>. The proposed models are different from the traditional Yolo network. In particular, the loss function and the label definition were modified for application in single target detection. Moreover, we changed the network architecture to reduce the computational cost, making the network more efficient while maintaining high accuracy. The main contributions of this study include: (1) We constructed a new pedestrian data set with pedestrian images in hazy weather. It will be useful for future development of pedestrian detection technologies.</p><p>(2) We proposed an effective and efficient Yolo based network for detection tasks. The proposed network can automatically select features from different channels to model the relationship between labeled objects and the original images with less information loss and faster running speed.</p><p>Based on the developed models, we proposed an application framework in Fig. <ref type="figure" target="#fig_0">1</ref>. Our efforts can be used in the development of advanced driver assistance systems and the design of autonomous vehicles to help drivers avoid traffic crashes with pedestrians in hazy weather, which would significantly improve both driving safety for drivers and traffic safety for vulnerable traffic participants <ref type="bibr" target="#b26">[27]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND LITERATURE REVIEW</head><p>The recently developed object detection algorithms are mainly divided into two categories: the one-stage method <ref type="bibr" target="#b24">[25]</ref>[26] <ref type="bibr" target="#b27">[28]</ref> and the two-stage method <ref type="bibr" target="#b28">[29]</ref>[30] <ref type="bibr" target="#b30">[31]</ref>. The former directly classifies the object and renders a regression of the object location from the raw image. The latter divides the detection problem into two steps: 1) it extracts the region of interest from an image where the objects may be, and 2) it then further corrects and recognizes the candidate region. The one-stage method runs faster, but it has a relatively lower accuracy compared with the two-stage method. To provide the desired speed and accuracy, we propose three deep learning approaches inspired by the one-stage Yolo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Yolo</head><p>The baseline Yolo architecture has 24 convolutional layers and 2 fully connected layers, which is inspired by GoogleLeNet. See <ref type="bibr" target="#b24">[25]</ref> for more details about the Yolo architecture. The concept of Yolo is to divide a picture into SÃ—S grid cells. Each grid cell produces B bounding boxes. The grid cell in which the object center falls is responsible for detecting this object. Each bounding box describes five attributes: the coordinates of the box center (x, y), the size of the box (h, w), and a ğ‘™ indicating whether there is an object in the bounding box. See Fig. <ref type="figure" target="#fig_7">2</ref>. Additionally, Yolo requires K more attributes to determine the object type. We set S and K as 7 and 20, respectively. There are at most two pedestrians in the same 7Ã—7 grid in 62.6% of the images in our data set. Hence, we set B=2. The network output is 7Ã—7Ã—30. The label of each grid cell is defined as:    are the weights that balance each loss.</p><p>In the method proposed herein, we used the Yolo concept to detect pedestrians, but our network differed from the traditional Yolo. We used depthwise and pointwise convolution <ref type="bibr" target="#b31">[32]</ref> in place of standard convolution to reduce the computational cost. We also used priori boxes <ref type="bibr" target="#b27">[28]</ref> to make our network more precise. Additionally, we proposed a method called weighted combination layer to improve the performance of our network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depthwise Separable Convolution</head><p>Depthwise separable convolution is a combination of depthwise and pointwise convolution. The former separately applies a filter to each input channel, and the latter then applies a 1Ã—1 convolution to combine the outputs of the depthwise convolution. Fig. <ref type="figure" target="#fig_4">3</ref> shows the difference between the standard and depthwise separable convolution. Generally, the computational cost of depthwise separable convolution is 8 to 9 times less than the standard convolution but with reduction in accuracy <ref type="bibr" target="#b31">[32]</ref>. To alleviate this, researchers proposed a new architecture based on depthwise separable convolution, called linear bottlenecks and inverted residuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Linear Bottlenecks and Inverted Residuals</head><p>In <ref type="bibr" target="#b32">[33]</ref>, Chollet found that the performance of depthwise convolution followed by a non-linear activation function was poorer than the depthwise convolution followed by the linear activation function. This could possibly be due to a lack of information. In another study <ref type="bibr" target="#b33">[34]</ref>, researchers proved that with low-dimensional manifolds of interest via Relu function, the output would lose information. The lower the dimension of the manifolds of interest, the more information would be lost. To alleviate this, <ref type="bibr" target="#b33">[34]</ref> proposed the linear bottlenecks and inverted residuals. Its architecture is shown in Fig. <ref type="figure" target="#fig_5">4</ref>. It follows a process of conv1Ã—1 (expansion)â†’Dwise3Ã—3 (reduce computational cost)â†’conv1Ã—1. The purpose of the expansion layer is to reduce the lack of information because the use of nonlinear functions is inevitable. The linear activation function is used to minimize the information loss after dimension reduction. Shortcuts directly between the bottlenecks are used, which is different from ResNet <ref type="bibr" target="#b34">[35]</ref>.</p><formula xml:id="formula_0">Block Stride = 1 Block Stride = 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Squeeze and Excitation</head><p>The squeeze and excitation block improves the network by recalibrating the feature map through learning weights in each channel of any feature map <ref type="bibr" target="#b35">[36]</ref>. The weights can be regarded as the importance of the corresponding channel on the detection results. See Fig. <ref type="figure" target="#fig_6">5</ref> for how it works. For more details, see <ref type="bibr" target="#b35">[36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Batch Normalization</head><p>To alleviate the gradient disappearance and explosion during training, batch normalization is a well-accepted method <ref type="bibr" target="#b36">[37]</ref>. By applying batch normalization, the network not only alleviates gradient explosions and disappearances, but also allows higher learning rates and diverse initializations. This concept was applied in our method to avoid training problems and to accelerate network convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACHES</head><p>In this section, we present the three proposed pedestrian detection models for use during haze based on deep learning architectures. The first is inspired by YoloV1 <ref type="bibr" target="#b24">[25]</ref>, but the architecture, label definition, and loss function differ from YoloV1. We call this model Simple-Yolo. The other two models use the idea of priori boxes <ref type="bibr" target="#b27">[28]</ref> to make the bounding boxes more accurate. One of them is based on Vgg16 <ref type="bibr" target="#b37">[38]</ref>, called VggPrioriBoxes-Yolo. The other model, MNPrioriBoxes-Yolo, uses separable depthwise convolution and bottlenecks to reduce computational cost. We proposed a weighted combination layer method to improve the accuracy of this network. A comparison between the proposed methods and the traditional Yolo methods is shown in Table <ref type="table" target="#tab_3">I</ref>. As observed, the number of parameters in the proposed method, especially in MNPrioriBoxes-Yolo, is much less than the others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simple-Yolo</head><p>The Simple-Yolo's backbone was inspired by Vgg16. We added batch normalization to each layer of the Vgg16 model. Additionally, we added another seven convolutional layers, the activation function of which was Leaky-Relu. The whole architecture is shown in Table <ref type="table" target="#tab_5">II</ref>. Each line describes the information of one or more layers. Totally, the Simple-Yolo has 23 convolution layers. The total number of parameters is approximately 16 million, which is much less than the traditional Yolo. We reduced the amount of parameters by using 3Ã—3, 1Ã—1 convolution and full convolutional layers.</p><p>The label of Simple-Yolo's bounding boxes is defined as: </p><formula xml:id="formula_1">H W 1 M N</formula><p>where b and p denote the scores belonging to background and pedestrians, respectively. In our model, we set S, B as 7, 2. We used the value relative to the global image instead of the local cell to parameterize the bounding box. The reason is that the last layer output had the full receptive field for the input image, so this label definition sufficed. We defined the box responsible for ground-truth as 6 and the box for background was defined as [ ? ? ? ? 1 0] âˆˆ â„ 6 , where the '?' indicates that the value number does not matter here. </p><formula xml:id="formula_3">[ ğ‘¥ ğ‘ ğ¼ğ¼ ğ‘¤ ğ‘¦ ğ‘ ğ¼ğ¼ â„ â„ ğ‘” â„ ğ‘¤ ğ‘” ğ‘¤ 0 1 ] âˆˆ â„</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outputï¼š7Ã—7Ã—12</head><p>Simple-Yolo's loss function is similar to YoloV1. We slightly modified it to make it meet the requirement of our pedestrian detection system. We used cross entropy to replace the MSE (mean square error) loss for classification. The loss function consists of three parts: 1) the target center, 2) the size of the bounding boxes, and 3) the classification. </p><p>where ğ¿ ğ‘ and ğ¿ ğ‘  are the total loss of bounding boxes, ğ¿ ğ‘ğ‘™ğ‘ğ‘ ğ‘  is the loss of classification, ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ‘–,ğ‘—,ğ‘˜ , ğ‘ğ‘™ğ‘ğ‘ ğ‘  Ì‚ğ‘–,ğ‘—,ğ‘˜ represent the prediction and ground truth of class vectors, ğ‘¥ ğ‘–,ğ‘—,ğ‘˜ , ğ‘¦ ğ‘–,ğ‘—,ğ‘˜ , ğ‘¤ ğ‘–,ğ‘—,ğ‘˜ , â„ ğ‘–,ğ‘—,ğ‘˜ are the prediction of bounding box, ğ‘¥ Ì‚ğ‘–,ğ‘—,ğ‘˜ , ğ‘¦ Ì‚ğ‘–,ğ‘—,ğ‘˜ , ğ‘¤ Ì‚ğ‘–,ğ‘—,ğ‘˜ , â„ Ì‚ğ‘–,ğ‘—,ğ‘˜ are the ground truth of bounding box, ğœ† ğ‘ğ‘œğ‘¥ and ğœ† ğ‘ğ‘™ğ‘ğ‘ ğ‘  are used to balance the different losses.</p><p>As Simple-Yolo marks pedestrians from scratch which can make this method not precise enough, we proposed the following two methods using priori boxes for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PrioriBoxes-Yolo</head><p>The priori box <ref type="bibr" target="#b27">[28]</ref> is also called an anchor box <ref type="bibr" target="#b25">[26]</ref>[29] <ref type="bibr" target="#b30">[31]</ref>. It has been widely used in target detection models. It includes a series of differently sized boxes. We used k-means <ref type="bibr" target="#b38">[39]</ref> to classify pedestrian size and set the size of priori boxes. The cluster number was set as 2. Additionally, we made the center of the priori box coincide with the center of the grid cell. Because we divided the image into 7Ã—7 grid cells, we had 98 priori boxes. Fig. <ref type="figure" target="#fig_8">6</ref> shows the priori box distribution. The red boxes are the priori boxes. The thick red boxes are examples of priori boxes in ğ¶ğ‘’ğ‘™ğ‘™ 3,3 .</p><p>Because we pre-defined priori boxes for detection, the network did not need to learn how to mark pedestrians from scratch. Therefore, the truth-value represents the conversion from a priori box to the ground-truth. More specifically, it requires position translation and shape transformation. We used a matching strategy to ensure that each priori box was responsible only for one ground-truth, and the label corresponding to the priori box and ground-truth was defined as 6 . The box for background was defined as [? ? ? ? 1 0 ] âˆˆ â„ 6 . See Fig. <ref type="figure" target="#fig_9">7</ref>.  Based on the presented priori box and label definition improvements, two models were proposed with different architectures. The architecture of VggPrioriBoxes-Yolo is the same as Simple-Yolo, but the output meaning is different. To further reduce the computational cost, we applied depthwise separable convolution and bottleneck structure in the MNPrioriBoxes-Yolo. Additionally, we developed a weighted combination layer that combined multi-scale feature maps and an attention module (i.e., the squeeze and excitation block) to improve the model performance. See Fig. <ref type="figure" target="#fig_10">8</ref> for how the weighted combination layer work. The architecture of MNPrioriBoxes-Yolo is shown in Fig. <ref type="figure" target="#fig_10">8</ref> and Table <ref type="table" target="#tab_6">III</ref>. The output of PrioriBoxes-Yolo cannot be used directly to mark pedestrians, because it requires a process to decode the output. The decoding algorithm is described in Algorithm 1. </p><formula xml:id="formula_5">[ ğ‘¥ ğ‘” -ğ‘¥ ğ‘ ğ‘¤ ğ‘ ğ‘¦ ğ‘” -ğ‘¦ ğ‘ â„ ğ‘ ğ‘™ğ‘œğ‘” ( â„ ğ‘” â„ ğ‘ ) ğ‘™ğ‘œğ‘” ( ğ‘¤ ğ‘” ğ‘¤ ğ‘ ) 0 1 ] âˆˆ â„</formula><formula xml:id="formula_6">- 7Ã—7Ã—320 Conv2d 1Ã—1 - 1280 1 1 - 7Ã—7Ã—1280 Conv2d 3x3 - - 1 1 âˆš Weighted Combination Layer 7Ã—7Ã—768 Conv2d - 512 1 1 - 7Ã—7Ã—512 Conv2d - 256 1 1 - 7Ã—7Ã—256 Conv2d - 128 1 1 - 7Ã—7Ã—128 Conv2d - 64 1 1 - 7Ã—7Ã—64 Conv2d - 32 1 1 - 7Ã—7Ã—32 Conv2d - 12 1 1 - Output:7Ã—7Ã—12</formula><p>The VggPrioriBoxes-Yolo and MNPrioriBoxes-Yolo share the same loss function, which includes the loss of classification and the loss of box. The loss of box is similar with other detection models <ref type="bibr" target="#b27">[28]</ref>[29] <ref type="bibr" target="#b30">[31]</ref>. However, we used cross entropy to replace the MSE loss for classification. The loss function is defined as:  where ğœ† ğ‘ğ‘™ğ‘ğ‘ ğ‘  , ğœ† ğ‘ğ‘œğ‘¥ are two pre-defined parameters that make a trade-off between the losses. The ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ ğ¿1 function has been widely used in previous detection models <ref type="bibr" target="#b27">[28]</ref>[32] <ref type="bibr" target="#b35">[36]</ref>. ğ‘¡ ğ‘ ğ‘–,ğ‘—,ğ‘˜ and ğ‘¡ ğ‘” ğ‘–,ğ‘—,ğ‘˜ are the prediction and ground truth of the conversion from a priori box to the ground-truth, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Detection Framework</head><p>Based on the PrioriBoxes-Yolo methods, we developed a pedestrian detection framework. One point that should be emphasized is that our proposed framework is suitable not only for detecting pedestrians in haze, but also for almost all single-target detection tasks. Fig. <ref type="figure" target="#fig_13">9</ref> shows a schematic diagram of the detection framework. Its corresponding pseudo code is presented in Algorithm 2 and Algorithm 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>To validate our method, we collected 1195 pedestrian images in hazy weather from the internet, of which 1052 were used as the training set and 143 as the test set. Please find our HazePerson data set via: https://github.com/YoungYoung619/pedestrian-detection-in-ha zy-weather/tree/master/dataset. The input size of images was 224Ã—224Ã—3. The source code of our proposed methods can be found in the same link above. All the images were processed on Intel-i7 6700K(4.0Ghz) with GTX 1080.</p><p>As the image number in our data set was not sufficient enough, we used six image augmentation skills to expand our data sets including src, fliplr, contrast, crop, multiply, and affine. Fig. <ref type="figure" target="#fig_14">10</ref> shows examples of these augmented images. Besides, we randomly added Gaussian noise and Gaussian blur.</p><p>During training, we randomly extracted three kinds of augmentation methods to augment the image, and then sent the augmented image for training. Actually, our data set can reach an infinite number of images after augmentation, which greatly increases the generalization ability of the models. The following strategies were also applied in the network training: a) Match strategy: During training, the network needs to determine which priori boxes correspond to the ground truth detection. For each ground truth, we selected a priori box with the maximum IoU value for prediction which greatly simplified the loss of bounding boxes in the detection task.</p><p>b) Hard-negative mining: During training, the number of negative boxes is much greater than the number of positive boxes. For those negative boxes that are easy to classify, they would not contribute much in training so that the easy-negative boxes would be deleted. The left negative boxes (hard-negative) and the positive boxes were used for training.</p><p>We compared the performance of different algorithms on our test set. The comparison indices include average precision (AP), precision (P), recall(R), and frame per second (FPS). To further describe the performance of different algorithms, we used two additional indices: all true (AT) and all miss (AM). AT is the proportion of images in which all the pedestrians are detected correctly. AM is the proportion of images in which all the pedestrians are not detected.</p><p>We used the criteria defined in the well-known PASCAL VOC 2012 competition for evaluation. When there are more than one pedestrians in one prediction box, the prediction will be assigned to the ground truth with the maximum IoU with the prediction. Otherwise, when multiple predictions frame the same pedestrian, the predictions would be sorted by decreasing confidences. All the predictions are compared with the ground-truth for IoU calculation. A "match" will be conducted when more than one predictions frame the same pedestrian and all their IoU&gt;=0.5. To avoid multiple detections of the same pedestrian, this match only considers the prediction with higher confidence as the true positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS V. RESULTS AND DISCUSSION</head><p>To examine the effectiveness of our proposed methods, three commonly accepted methods in previous studies, i.e., HOG+SVM <ref type="bibr" target="#b4">[5]</ref>, Haar+Adaboost <ref type="bibr" target="#b9">[10]</ref>, and YoloV3 <ref type="bibr" target="#b39">[40]</ref>, were adopted for comparison. See Fig. <ref type="figure" target="#fig_15">11</ref> for the illustrated detection results of all the examined methods.</p><p>Table <ref type="table" target="#tab_7">IV</ref> shows the comparison of statistical results for all the examined methods. The corresponding precision-recall (PR) curves are illustrated in Fig. <ref type="figure" target="#fig_16">12</ref>. Results show that the detection performances of the methods based on convolutional neural network are much better than the traditional HOG+SVM and Haar+Adaboost methods. Among all the examined methods, MNPrioriBoxes-Yolo gives the best performance with respect to the detection accuracy and running speed. Compared with other deep learning approaches with the similar size (e.g., YoloV3-Tiny <ref type="bibr" target="#b39">[40]</ref>), the running time of MNPrioriBoxes-Yolo is more excellent because of the used depthwise seperable convolution.</p><p>Normally, the running time of depthwise separable convolution is 8 to 9 times less than the standard convolution <ref type="bibr" target="#b31">[32]</ref>.  The advances of our proposed method can be attributed to the following reasons: (1) We innovatively proposed a weighted combination layer to combine the multi-scale features with the SE block <ref type="bibr" target="#b35">[36]</ref> to automatically search important features from different channels. ( <ref type="formula" target="#formula_2">2</ref>) Training techniques including image augmentation and hard-negative mining were employed from <ref type="bibr" target="#b27">[28]</ref> to generate a more comprehensive and balanced data set.</p><p>(3) We used the priori boxes method proposed by <ref type="bibr" target="#b28">[29]</ref> for more precise and faster detections. Differently, we used k-means to get the initial size of priori boxes while <ref type="bibr" target="#b28">[29]</ref> set the size of priori boxes based on empirical experience. (4) We used the bottleneck technique proposed by <ref type="bibr" target="#b33">[34]</ref> to deal with information loss problems. <ref type="bibr" target="#b4">(5)</ref> The used depthwise separable convolution <ref type="bibr" target="#b31">[32]</ref> in our method is more time efficient. To examine how much the above techniques contributed to the detection performance, we conducted an analysis to examine the detection performance when using different skills. Results are shown in Table <ref type="table" target="#tab_9">V</ref>. Without using the weighted combination layer, the MNPrioiriBoxes-Yolo only had an AP of 77.3%. The SE block in the weighted combination layer contributed to a 4.6% increase on the AP than without using this skill.  It is worth noting that the number of bounding boxes may affect the detection performance. We conducted an additional experiment to examine this issue using the proposed MNPrioiriBoxes-Yolo. The compared detection performance when using different number of bounding boxes was shown in Table <ref type="table" target="#tab_10">VI</ref>. The results show that the AP, P, and R were the highest when the number of bounding boxes was two, indicating that our proposed method achieved the best performance with two bounding boxes. More bounding boxes probably increased the proportion of negative cases, making it more difficult for the network to distinguish pedestrians from background. To further examine the adaptiveness of our proposed methods, we conducted an experiment on the INRIA Person data set <ref type="bibr" target="#b4">[5]</ref> which is a popular data set for pedestrian detection. Table <ref type="table" target="#tab_12">VII</ref> shows the statistical results of the examined methods. The corresponding PR curves are illustrated in Fig. <ref type="figure" target="#fig_4">13</ref>. The illustrated detection performance are shown in Fig. <ref type="figure" target="#fig_5">14</ref>. Results show that the MNPrioriBoxes-Yolo generally outperforms the other compared methods. Compared with the detection results on our HazePerson data set, the statistics in Table VII are not as satisfactory as the numbers in Table <ref type="table" target="#tab_7">IV</ref>. This may be attributed to the following reasons: (1) The weighted combination layer in our proposed method may be more adaptive to pedestrian detection in hazy weather. <ref type="bibr" target="#b1">(2)</ref> The pedestrians in the images of INRIA were small. To date, detection of small pedestrians is still a challenging task all over the world. A state-of-the-art method <ref type="bibr" target="#b16">[17]</ref> reported a 54.4% miss rate on Caltech, one of the data sets with many small pedestrians. Small pedestrians were also included in INRIA. Therefore, the detection performance on INRIA is not as satisfactory as the performance on our HazePerson data set.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. CONCLUSION</head><p>In this paper, we proposed three deep learning approaches for pedestrian detection in hazy weather based on Yolo. A novel weighted combination layer was proposed in our presented method to improve the detection precision performance. Depthwise separable convolution and linear bottleneck were used to reduce the computational cost and number of parameters for a faster running speed. Augmented strategies were used to enrich our data set. Compared with the examined pedestrian detection algorithms, the results from the present study show that the methods proposed herein were more efficient with higher accuracy. However, the detection based on priori boxes is partially dependent on the initial setting of the priori boxes. Researchers have recently introduced the idea of key point detection (e.g., CornerNet <ref type="bibr" target="#b40">[41]</ref>, ExtremeNet <ref type="bibr" target="#b41">[42]</ref>) into target detection with satisfactory performance. Therefore, as a next step, we will try to integrate our detection method with key point detection to see whether we can further improve the detection ability of our network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Application framework of the proposed detection methods.</figDesc><graphic coords="2,72.00,277.51,201.87,129.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig.2shows how Yolo defines the label. The green cell is responsible for detection. The boxes responsible for detecting the ground truth and background are defined as [ ? ? ğ‘™ ] âˆˆ â„5 , respectively, where the '?' indicates that the value number does not matter here. The ğ‘™ describes whether the box frames a pedestrian (ğ‘™ =1) or the background (ğ‘™ =0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 . 2 +</head><label>22</label><figDesc>Fig. 2. Yolo label definition. The Yolo loss function is defined as: ğ¿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ = ğœ† ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘‘ âˆ‘ âˆ‘ âˆ‘ 1 ğ‘–,ğ‘—,ğ‘˜ ğ‘œğ‘ğ‘— [(ğ‘¥ ğ‘–,ğ‘—,ğ‘˜ -ğ‘¥ Ì‚ğ‘–,ğ‘—,ğ‘˜ ) 2 + (ğ‘¦ ğ‘–,ğ‘—,ğ‘˜ -ğ‘¦ Ì‚ğ‘–,ğ‘—,ğ‘˜ ) 2 ] ğµ</figDesc><graphic coords="2,445.50,269.31,106.62,112.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2019.2945295, IEEE Transactions on Industrial Electronics IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Difference between standard and depthwise separable convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The linear bottlenecks and inverted residuals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Squeeze and excitation block. For any given input, we can construct this block to perform feature recalibration.</figDesc><graphic coords="3,313.20,53.14,257.35,59.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 +</head><label>2</label><figDesc>ğ¿ ğ‘ = âˆ‘ âˆ‘ âˆ‘ 1 ğ‘–,ğ‘—,ğ‘˜ ğ‘œğ‘ğ‘— [(ğ‘¥ ğ‘–,ğ‘—,ğ‘˜ -ğ‘¥ Ì‚ğ‘–,ğ‘—,ğ‘˜ ) 2 + (ğ‘¦ ğ‘–,ğ‘—,ğ‘˜ -ğ‘¦ Ì‚ğ‘–,ğ‘—,ğ‘˜ ) 2 ] âˆ‘ âˆ‘ âˆ‘ 1 ğ‘–,ğ‘—,ğ‘˜ ğ‘œğ‘ğ‘— [(âˆšğ‘¤ ğ‘–,ğ‘—,ğ‘˜ -âˆšğ‘¤ Ì‚ğ‘–,ğ‘—,ğ‘˜ ) (âˆšâ„ ğ‘–,ğ‘—,ğ‘˜ -âˆšâ„ Ì‚ğ‘–,ğ‘—,ğ‘˜ ) âˆ‘ âˆ‘ âˆ‘ ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ‘–,ğ‘—,ğ‘˜ , ğ‘ğ‘™ğ‘ğ‘ ğ‘  Ì‚ğ‘–,ğ‘—,ğ‘˜ ) ğœ† ğ‘ğ‘œğ‘¥ (ğ¿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ + ğ¿ ğ‘ ğ‘–ğ‘§ğ‘’ ) + ğœ† ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ¿ ğ‘ğ‘™ğ‘ğ‘ ğ‘ </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Priori box distribution.</figDesc><graphic coords="4,395.55,302.86,77.90,77.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. PrioriBoxes-Yolo labels.</figDesc><graphic coords="4,441.00,399.56,111.15,116.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. MNPrioriBoxes-Yolo architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>ğ¿ ğ‘ğ‘™ğ‘ğ‘ ğ‘  = âˆ‘ âˆ‘ âˆ‘ ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦(ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ‘–,ğ‘—,ğ‘˜ , ğ‘ğ‘™ğ‘ğ‘ ğ‘  Ì‚ğ‘–,ğ‘—,ğ‘˜ ) âˆ‘ âˆ‘ âˆ‘ 1 ğ‘–,ğ‘—,ğ‘˜ ğ‘ğ‘œğ‘  [ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ ğ¿1 (ğ‘¡ ğ‘” ğ‘–,ğ‘—,ğ‘˜ -ğ‘¡ ğ‘ ğ‘–,ğ‘—,ğ‘˜ )] ğœ† ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ¿ ğ‘ğ‘™ğ‘ğ‘ ğ‘  + ğœ† ğ‘ğ‘œğ‘¥ ğ¿ ğ‘ğ‘œğ‘¥ ğ‘ ğ‘šğ‘œğ‘œğ‘¡â„ ğ¿1 (ğ‘¥) = { 0.5ğ‘¥ 2 , ğ‘–ğ‘“|ğ‘¥| &lt; 1 |ğ‘¥| -0.5, ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The detection system framework.</figDesc><graphic coords="5,313.67,316.44,250.46,77.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Examples of augmented images.</figDesc><graphic coords="6,47.25,53.90,256.50,303.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>11 .</head><label>11</label><figDesc>Illustrated detection examples of the examined methods on our HazePerson data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. PR curves of the examined methods on our HazePerson data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>MNPrioriBoxes-Yolo Fig. 13. PR curves of the examined methods on the INRIA data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>0278-0046 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2019.2945295, IEEE Transactions on Industrial Electronics IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS (d) Simple-Yolo (e) VggPrioriBoxes-Yolo (f) MNPrioriBoxes-Yolo Fig. 14. Detection examples of the examined methods on the INRIA Person data set. Only images of pedestrians in hazy or clear weather were included in our HazePerson or the INRIA data sets, which may cause overfitting problems. Therefore, we combined our data set and the INRIA Person data set to generate a broader data set with balanced hazy and clear images (1:1) for further examination. The detection results of the three advanced methods are shown in Table VIII. Comparatively, our MNPrioriBoxes-Yolo method outperforms the other methods with better generalization performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Guofa Li, Member, IEEE, Yifan Yang, Xingda Qu This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2019.2945295, IEEE Transactions on Industrial Electronics</figDesc><table><row><cell>Deep Learning Approaches on Pedestrian</cell></row><row><cell>Detection in Hazy Weather</cell></row></table><note><p><p>W 0278-0046 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p>IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ğ‘— are the center position, height and width of the kth bounding box in ğ¶ğ‘’ğ‘™ğ‘™ ğ‘–,ğ‘— , ğ‘™ ğ‘˜ ğ‘–,ğ‘— indicates whether objects exist in the bounding box, ğ¶ğ‘™ğ‘ğ‘ ğ‘  ğ‘˜ ğ‘–,ğ‘— is the class score vector and ğ‘ ğ‘–,ğ‘—,ğ‘˜ is the probability score corresponding to a class.</figDesc><table><row><cell>ğµğ‘œğ‘¥ ğ‘˜ ğ‘–,ğ‘— = [ğ‘¥ ğ‘˜ ğ‘–,ğ‘— ğ‘¦ ğ‘˜ ğ‘–,ğ‘— â„ ğ‘˜ ğ‘–,ğ‘— ğ‘¤ ğ‘˜ ğ‘–,ğ‘— ğ‘™ ğ‘˜ ğ‘–,ğ‘— ] âˆˆ â„ 5</cell><cell></cell></row><row><cell>ğ¶ğ‘™ğ‘ğ‘ ğ‘  ğ‘˜ ğ‘–,ğ‘— = [ğ‘ 1 ğ‘–,ğ‘—,ğ‘˜ ğ‘ 2 ğ‘–,ğ‘—,ğ‘˜ â€¦ ğ‘ 20 ğ‘–,ğ‘—,ğ‘˜ ] âˆˆ â„ 20 ğ¶ğ‘’ğ‘™ğ‘™ ğ‘–,ğ‘— = [ğµğ‘œğ‘¥ 1 ğ‘–,ğ‘— ğµğ‘œğ‘¥ 2 ğ‘–,ğ‘— ğ¶ğ‘™ğ‘ğ‘ ğ‘  ğ‘–,ğ‘— ] âˆˆ â„ 30</cell><cell>(1)</cell></row><row><cell>ğ‘–, ğ‘— âˆˆ ğ‘†, ğ‘˜ âˆˆ ğµ</cell><cell></cell></row><row><cell>where ğ‘¥ ğ‘˜ ğ‘–,ğ‘— ğ‘¦ ğ‘˜ ğ‘–,ğ‘— â„ ğ‘˜ ğ‘–,ğ‘— ğ‘¤ ğ‘˜ ğ‘–,</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ğ‘–,ğ‘—,ğ‘˜ ğ‘œğ‘ğ‘— [(ğ‘ ğ‘–,ğ‘—,ğ‘˜ -ğ‘Ì‚ğ‘– ,ğ‘—,ğ‘˜ ) 2 ] ğ¿ ğ‘ ğ‘–ğ‘§ğ‘’ and ğ¿ ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ are the total loss of bounding boxes, ğ¿ ğ‘ğ‘™ğ‘ğ‘ ğ‘  is the loss of classification, ğ‘™ ğ‘–,ğ‘—,ğ‘˜ , ğ‘™ Ì‚ğ‘–,ğ‘—,ğ‘˜ are the prediction and ground truth of whether there is an object, ğ‘¥ ğ‘–,ğ‘—,ğ‘˜ , ğ‘¦ ğ‘–,ğ‘—,ğ‘˜ , ğ‘¤ ğ‘–,ğ‘—,ğ‘˜ , â„ ğ‘–,ğ‘—,ğ‘˜ are the predictions of bounding boxes, ğ‘¥ Ì‚ğ‘–,ğ‘—,ğ‘˜ , ğ‘¦ Ì‚ğ‘–,ğ‘—,ğ‘˜ , ğ‘¤ Ì‚ğ‘–,ğ‘—,ğ‘˜ , â„ Ì‚ğ‘–,ğ‘—,ğ‘˜ are the ground truths of bounding boxes, ğ‘ ğ‘–,ğ‘—,ğ‘˜ and ğ‘Ã® ,ğ‘—,ğ‘˜ are the prediction and ground-truth of class score,</figDesc><table><row><cell>ğ‘†</cell><cell>ğ‘†</cell><cell>ğµ</cell></row><row><cell>ğ‘–=0</cell><cell>ğ‘—=0</cell><cell>ğ‘˜=0</cell></row><row><cell cols="3">ğ¿ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ = ğ¿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ + ğ¿ ğ‘ ğ‘–ğ‘§ğ‘’ + ğ¿ ğ‘ ğ‘œğ‘ğ‘Ÿğ‘’ + ğ¿ ğ‘ğ‘™ğ‘ğ‘ ğ‘ </cell></row><row><cell cols="3">where ğ¿ ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ , 1 ğ‘–,ğ‘—,ğ‘˜ ğ‘œğ‘ğ‘— denotes the ğµğ‘œğ‘¥ ğ‘˜ ğ‘–,ğ‘— responsible for detection, 1 ğ‘–,ğ‘—,ğ‘˜ nooğ‘ğ‘—</cell></row><row><cell cols="3">denotes the ğµğ‘œğ‘¥ ğ‘˜ ğ‘–,ğ‘— responsible for background, ğœ† ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘‘ , ğœ† ğ‘›ğ‘œğ‘œğ‘ğ‘—</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table I .</head><label>I</label><figDesc>Comparison between different methods.</figDesc><table><row><cell>Model</cell><cell>BB</cell><cell cols="5">BN PB Conv Method WCL IRS</cell><cell>NumP</cell></row><row><cell cols="3">YoloV1 GoogleLeNet Ã—</cell><cell>Ã—</cell><cell>Std-Conv</cell><cell>Ã—</cell><cell cols="2">Ã— 93970430</cell></row><row><cell>YoloV2</cell><cell>DarkNet</cell><cell>âˆš</cell><cell>âˆš</cell><cell>Std-Conv</cell><cell>Ã—</cell><cell cols="2">Ã— 50962889</cell></row><row><cell>YoloV3</cell><cell>DarkNet</cell><cell>âˆš</cell><cell>âˆš</cell><cell>Std-Conv</cell><cell>Ã—</cell><cell cols="2">Ã— 61523734</cell></row><row><cell>SY</cell><cell>Vgg16</cell><cell>âˆš</cell><cell>âˆš</cell><cell>Std-Conv</cell><cell>Ã—</cell><cell cols="2">Ã— 15945606</cell></row><row><cell>VPBY</cell><cell>Vgg16</cell><cell>âˆš</cell><cell>âˆš</cell><cell>Std-Conv</cell><cell>Ã—</cell><cell cols="2">Ã— 15945606</cell></row><row><cell cols="3">MNPBY MobileNetV2 âˆš</cell><cell>âˆš</cell><cell>Dwise-Conv</cell><cell>âˆš</cell><cell>âˆš</cell><cell>6620213</cell></row><row><cell>SY:</cell><cell>Simple-Yolo.</cell><cell></cell><cell>VPBY:</cell><cell cols="3">VggPrioriBoxes-Yolo.</cell></row></table><note><p>MNPBY: MNPrioriBoxes-Yolo. BB: backbone. BN: batch normalization. PB: priori box. WCL: weighted combination layer. IRS: inverted residuals. Dwise-Conv: depthwise separable convolution. NumP: number of parameters.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2019.2945295, IEEE Transactions on Industrial Electronics</figDesc><table><row><cell cols="5">IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS</cell></row><row><cell cols="4">ğ¶ğ‘™ğ‘ğ‘ ğ‘  ğ‘˜ ğ‘–,ğ‘— = [ğ‘ ğ‘˜ ğ‘–,ğ‘— ğ‘ ğ‘˜ ğ‘–,ğ‘— ] âˆˆ â„ 2</cell></row><row><cell cols="5">ğµğ‘œğ‘¥ ğ‘˜ ğ‘–,ğ‘— = [ğ‘¥ ğ‘˜ ğ‘–,ğ‘— ğ‘¦ ğ‘˜ ğ‘–,ğ‘— â„ ğ‘˜ ğ‘–,ğ‘— ğ‘¤ ğ‘˜ ğ‘–,ğ‘— ğ‘ ğ‘˜ ğ‘–,ğ‘— ğ‘ ğ‘˜ ğ‘–,ğ‘— ] âˆˆ â„ 6</cell></row><row><cell cols="4">ğ¶ğ‘’ğ‘™ğ‘™ ğ‘–,ğ‘— = [ğµğ‘œğ‘¥ 0 ğ‘–,ğ‘— ğµğ‘œğ‘¥ 1 ğ‘–,ğ‘— ] âˆˆ â„ 12</cell></row><row><cell cols="3">ğ‘–, ğ‘— âˆˆ ğ‘†, ğ‘˜ âˆˆ ğµ</cell><cell></cell></row><row><cell></cell><cell></cell><cell>N</cell><cell></cell></row><row><cell></cell><cell>M</cell><cell></cell><cell></cell><cell>M</cell></row><row><cell>H</cell><cell>W</cell><cell>Std Conv Filters</cell><cell cols="2">1 Depthwise Conv Filters 1</cell></row><row><cell>Input</cell><cell></cell><cell cols="2">Conv 1x1, Relu6</cell><cell>Dwise 3x3, Relu6</cell></row><row><cell></cell><cell></cell><cell>Output</cell><cell>ADD</cell><cell>Conv 1x1, Linear</cell></row><row><cell cols="2">Input</cell><cell cols="2">Conv 1x1, Relu6</cell><cell>Dwise 3x3, Relu6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stride = 2</cell></row><row><cell></cell><cell></cell><cell cols="2">Conv 1x1, Linear</cell></row></table><note><p>Output 0278-0046 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table II .</head><label>II</label><figDesc>Simple-Yolo's architecture. (The Conv2d is a standard convolution, n is the number of repeats, c is the number of output channels, and s is the number of stride.)</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>c</cell><cell>n</cell><cell>s</cell></row><row><cell>224Ã—224Ã—3</cell><cell>Conv2d 3Ã—3</cell><cell>64</cell><cell>2</cell><cell>1</cell></row><row><cell>224Ã—224Ã—64</cell><cell>Max pooling 2Ã—2</cell><cell>64</cell><cell>1</cell><cell>2</cell></row><row><cell>112Ã—112Ã—64</cell><cell>Conv2d 3Ã—3</cell><cell>128</cell><cell>2</cell><cell>1</cell></row><row><cell>112Ã—112Ã—128</cell><cell>Max pooling 2Ã—2</cell><cell>128</cell><cell>1</cell><cell>2</cell></row><row><cell>56Ã—56Ã—128</cell><cell>Conv2d 3Ã—3</cell><cell>256</cell><cell>3</cell><cell>1</cell></row><row><cell>56Ã—56Ã—256</cell><cell>Max pooling 2Ã—2</cell><cell>256</cell><cell>1</cell><cell>2</cell></row><row><cell>28Ã—28Ã—256</cell><cell>Conv2d 3Ã—3</cell><cell>512</cell><cell>3</cell><cell>1</cell></row><row><cell>28Ã—28Ã—512</cell><cell>Max pooling 2Ã—2</cell><cell>512</cell><cell>1</cell><cell>2</cell></row><row><cell>14Ã—14Ã—512</cell><cell>Conv2d 3Ã—3</cell><cell>512</cell><cell>3</cell><cell>1</cell></row><row><cell>14Ã—14Ã—512</cell><cell>Max pooling 2Ã—2</cell><cell>512</cell><cell>1</cell><cell>2</cell></row><row><cell>7Ã—7Ã—512</cell><cell>Conv2d 3Ã—3</cell><cell>180</cell><cell>2</cell><cell>1</cell></row><row><cell>7Ã—7Ã—180</cell><cell>Conv2d 1Ã—1+3Ã—3</cell><cell>90</cell><cell>2</cell><cell>1</cell></row><row><cell>7Ã—7Ã—90</cell><cell>Conv2d 1Ã—1+3Ã—3</cell><cell>30</cell><cell>2</cell><cell>1</cell></row><row><cell>7Ã—7Ã—30</cell><cell>Conv2d 1Ã—1+3Ã—3</cell><cell>12</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table III .</head><label>III</label><figDesc>MNPrioriBoxes-Yolo architecture. (t is the expansion factor used in the bottleneck architecture. The âˆš in column f indicates the extracted feature maps.)</figDesc><table><row><cell>Input</cell><cell>Operator</cell><cell>t</cell><cell>c</cell><cell>n</cell><cell>s</cell><cell>f</cell></row><row><cell>224Ã—224Ã—3</cell><cell>Conv2d 3x3</cell><cell>-</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>-</cell></row><row><cell>112Ã—112Ã—32</cell><cell>Bottleneck</cell><cell>1</cell><cell>16</cell><cell>1</cell><cell>1</cell><cell>-</cell></row><row><cell>112Ã—112Ã—16</cell><cell>Bottleneck</cell><cell>6</cell><cell>24</cell><cell>2</cell><cell>2</cell><cell>-</cell></row><row><cell>56Ã—56Ã—24</cell><cell>Bottleneck</cell><cell>6</cell><cell>32</cell><cell>3</cell><cell>2</cell><cell>-</cell></row><row><cell>28Ã—28Ã—32</cell><cell>Bottleneck</cell><cell>6</cell><cell>64</cell><cell>4</cell><cell>2</cell><cell>âˆš</cell></row><row><cell>14Ã—14Ã—64</cell><cell>Bottleneck</cell><cell>6</cell><cell>96</cell><cell>3</cell><cell>1</cell><cell>-</cell></row><row><cell>14Ã—14Ã—96</cell><cell>Bottleneck</cell><cell>6</cell><cell>160</cell><cell>3</cell><cell>2</cell><cell>âˆš</cell></row><row><cell>7Ã—7Ã—160</cell><cell>Bottleneck</cell><cell>6</cell><cell>320</cell><cell>3</cell><cell>1</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table IV .</head><label>IV</label><figDesc>Detection results on our HazePerson data set.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>P</cell><cell>R</cell><cell>AT</cell><cell>AM</cell><cell>FPS</cell></row><row><cell>HOG+SVM</cell><cell cols="4">43.5 71.3 54.7 39.9</cell><cell>9.1</cell><cell>-</cell></row><row><cell>Haar+Adaboost</cell><cell cols="5">35.6 64.7 50.2 44.8 27.9</cell><cell>-</cell></row><row><cell>YoloV3</cell><cell cols="4">81.6 87.2 83.3 83.9</cell><cell>7.7</cell><cell>22.2</cell></row><row><cell>Simple-Yolo</cell><cell cols="4">62.7 76.8 70.2 64.3</cell><cell>2.1</cell><cell>80.1</cell></row><row><cell cols="5">VggPrioriBoxes-Yolo 80.8 85.1 84.1 81.8</cell><cell>2.8</cell><cell>81.7</cell></row><row><cell>MNPrioriBoxes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>-Yolo 86.6 88.0 89.3 86.7 0.7 151.9</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table V .</head><label>V</label><figDesc>Detection results using different skills in MNPrioiriBoxes-Yolo. DSC: depthwise separable convolution. LB: linear bottleneck. MSF: multi-scale feature. SE: squeeze and excitation.</figDesc><table><row><cell>DSC</cell><cell>LB</cell><cell>MSF</cell><cell>SE</cell><cell>P</cell><cell>R</cell><cell>AP</cell></row><row><cell>âˆš</cell><cell>âˆš</cell><cell>Ã—</cell><cell>Ã—</cell><cell>82.4</cell><cell>80.4</cell><cell>77.3</cell></row><row><cell>âˆš</cell><cell>âˆš</cell><cell>âˆš</cell><cell>Ã—</cell><cell>84.9</cell><cell>85.3</cell><cell>82.0</cell></row><row><cell>âˆš</cell><cell>âˆš</cell><cell>âˆš</cell><cell>âˆš</cell><cell>88.0</cell><cell>89.3</cell><cell>86.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table VI .</head><label>VI</label><figDesc>Detection results when using different number of bounding boxes in MNPrioiriBoxes-Yolo on our HazePerson data set.</figDesc><table><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell>5</cell><cell></cell></row><row><cell>AP</cell><cell>P</cell><cell>R</cell><cell>AP</cell><cell>P</cell><cell>R</cell><cell>AP</cell><cell>P</cell><cell>R</cell><cell>AP P</cell><cell>R</cell></row><row><cell>86.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>88.8 89.3 80</head><label></label><figDesc>.8 83.2 84.9 86.6 83.0 89.2 81.9 86.7 84.9</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table VII .</head><label>VII</label><figDesc>Detection results on the INRIA Person dataset.</figDesc><table><row><cell>Method HOG+SVM</cell><cell>AP 20.YoloV3 P R AT AM FPS Simple-Yolo VggPrioriBoxes-Yolo 77.8 94.9 78.2 82.3 76.1 94.9 77.2 79.1 36.1 59.0 49.4 40.8 MNPrioriBoxes-Yolo</cell><cell>3.5 4.8 4.2</cell><cell>22.2 80.1 81.7</cell></row></table><note><p>7 40.0 48.4 20.8 22.6 -Haar+Adaboost 18.4 50.8 35.5 35.7 23.4 -</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>79.2 96.2 79.9 81.9 2.1 151.9</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table VIII .</head><label>VIII</label><figDesc>Detection results of the examined methods on the combined data set.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>P</cell><cell>R</cell><cell>AT</cell><cell>AM</cell><cell>FPS</cell></row><row><cell>YoloV3</cell><cell>78.0</cell><cell>89.4</cell><cell>79.9</cell><cell>76.0</cell><cell>1.7</cell><cell>22.2</cell></row><row><cell>VggPrioriBoxes-Yolo</cell><cell>80.5</cell><cell>90.8</cell><cell>82.4</cell><cell>83.3</cell><cell>2.1</cell><cell>81.7</cell></row><row><cell>MNPrioriBoxes-Yolo</cell><cell>80.5</cell><cell>89.3</cell><cell>83.0</cell><cell>84.3</cell><cell>1.0</cell><cell>151.9</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the NSF China with 51805332, Natural Science Foundation of Guangdong Province with 2018A030310532, the Young Elite Scientists Sponsorship Program by China Society of Automotive Engineers, and the State Key Laboratory of Automotive Safety and Energy under Project No. KF1801.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Correlation between Euro NCAP pedestrian test results and injury severity in injury crashes with pedestrians and bicyclists in Sweden</title>
		<author>
			<persName><forename type="first">J</forename><surname>Strandroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sternlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tingvall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kullgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fredriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stapp Car Crash J</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Driving behaviors: models and challenges for non-lane based mixed traffic</title>
		<author>
			<persName><forename type="first">G</forename><surname>Asaithambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kanagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. in Dev. Econ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2016-10">Oct. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detection of road traffic participants using cost-effective arrayed ultrasonic sensors in low-speed traffic situations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="535" to="545" />
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kalman filter-based tracking of moving objects using linear ultrasonic sensor array for road vehicles</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="173" to="189" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian Detection: An Evaluation of the State of the Art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human detection using partial least squares analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pedestrian detection via classification on riemannian manifolds</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jointly learning deep features, deformable parts, occlusion and classification for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1874" to="1887" />
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE WACV</title>
		<meeting>IEEE WACV</meeting>
		<imprint>
			<date type="published" when="2017-03">Mar. 2017</date>
			<biblScope unit="page" from="953" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scale-aware fast R-CNN for pedestrian detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Traffic accidents in Iraq: An analytical study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Ambak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Syamsunur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Adv. Res. Civ. Environ. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1&amp;2</biblScope>
			<biblScope unit="page" from="10" to="22" />
			<date type="published" when="2018-07">Jul. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of wrong-way driving fatal crashes in the United States</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baratian-Ghorghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITE J.-Inst. Transp. Eng</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An advanced single-image visibility restoration algorithm for real-world hazy scenes</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2962" to="2972" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A high-efficiency and high-speed gain intervention refinement filter for haze removal</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Disp. Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="753" to="759" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2017-06">Jun. 2017</date>
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimation of driving style in naturalistic highway traffic using maneuver transition probabilities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transp. Res. Pt. C-Emerg. Technol</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="113" to="125" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">SSD: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2014-06">Jun. 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2017-06">Jun. 2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Algorithm AS 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat.-J. R. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">YOLOv3: An Incremental Improvement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018-09">Sep. 2018</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08043</idno>
		<imprint>
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
