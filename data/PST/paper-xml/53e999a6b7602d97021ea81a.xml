<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental subspace learning via non-negative matrix factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Serhat</forename><forename type="middle">S</forename><surname>Bucak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical-Electronics Engineering Faculty</orgName>
								<orgName type="department" key="dep2">Department of Electronics and Communications Engineering</orgName>
								<orgName type="laboratory">Multimedia Signal Processing and Pattern Recognition Laboratory</orgName>
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<postCode>34469</postCode>
									<settlement>Maslak, Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bilge</forename><surname>Gunsel</surname></persName>
							<email>gunselb@itu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical-Electronics Engineering Faculty</orgName>
								<orgName type="department" key="dep2">Department of Electronics and Communications Engineering</orgName>
								<orgName type="laboratory">Multimedia Signal Processing and Pattern Recognition Laboratory</orgName>
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<postCode>34469</postCode>
									<settlement>Maslak, Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incremental subspace learning via non-negative matrix factorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C31F32B2E57E5B0E096A281A5C29FFFA</idno>
					<idno type="DOI">10.1016/j.patcog.2008.09.002</idno>
					<note type="submission">Received 21 August 2007 Received in revised form 30 July 2008 Accepted 2 September 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Incremental subspace learning Non-negative matrix factorization Statistical background modeling Video content representation Clustering</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we introduce an incremental non-negative matrix factorization (INMF) scheme in order to overcome the difficulties that conventional NMF has in online processing of large data sets. The proposed scheme enables incrementally updating its factors by reflecting the influence of each observation on the factorization appropriately. This is achieved via a weighted cost function which also allows controlling the memorylessness of the factorization. Unlike conventional NMF, with its incremental nature and weighted cost function the INMF scheme successfully utilizes adaptability to dynamic data content changes with a lower computational complexity. Test results reported for two video applications, namely background modeling in video surveillance and clustering, demonstrate that INMF is capable of online representing data content while reducing dimension significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Eliminating redundancies and extracting vital components in multidimensional data are among the major steps of content analysis tasks. Several decomposition methods such as principal component analysis (PCA) <ref type="bibr" target="#b0">[1]</ref> aim to achieve this goal as they decrease dimension of data significantly and reveal patterns of interest. Non-negative matrix factorization (NMF) <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, which is offered to extract key features of data by operating an iterative matrix factorization, is one of the recent decomposition tools for multivariate data. It is shown that NMF offers dimension reduction and produces useful representations by converting a data matrix to multiplication of two smaller matrices. These factor matrices are expected to contain latent and vital components of original data. Unlike similar methods, NMF puts a non-negativity constraint which enables it to form intuitive and parts based representations of data on its factors.</p><p>Although there are earlier approaches to the problem of solving NMF <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, it was 1999 when NMF started to draw attentions of researchers after Lee and Seung proposed their multiplicative update rules for NMF and showed NMF's success as a machine learning tool <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Afterwards new cost functions were offered to increase sparseness of NMF representations for addressing requirements of specific applications. In general these cost functions include 0031-3203/$ -see front matter Â© 2008 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2008.09.002 additional penalty components which would add new constraints on the optimization problem and force sparseness on factors <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</p><p>To speed up convergence of the optimization process, new projected gradient descent methods are proposed for NMF <ref type="bibr" target="#b10">[11]</ref>. Alternatively, methods that employ Newton-type numerical approaches are also presented to derive exact solutions <ref type="bibr" target="#b11">[12]</ref>.</p><p>As a tool that offers dimension reduction and forms intuitive representations, NMF has been used in different research areas. Some examples of these fields are face recognition <ref type="bibr" target="#b12">[13]</ref>, biomedical applications <ref type="bibr" target="#b6">[7]</ref>, music transcription <ref type="bibr" target="#b13">[14]</ref>, bioinformatics <ref type="bibr" target="#b14">[15]</ref>, blind source separation <ref type="bibr" target="#b7">[8]</ref>, and image hashing <ref type="bibr" target="#b15">[16]</ref>. One of the reasons that make NMF suitable for these applications is the nature of data they use. Mostly, data used in these applications consist of non-negative elements. Sparseness, which provides parts-based representations and localization, is another utility of NMF that has been reported and proven to be useful. Moreover, it has been shown in Ref. <ref type="bibr" target="#b16">[17]</ref> that NMF is equivalent to fuzzy k-means clustering and produces successful results in clustering tasks.</p><p>In this paper we propose an incremental non-negative matrix factorization (INMF) method in order to overcome the difficulties that conventional NMF confronts in online processing of large scale data. Generally in an online analysis updating former representations according to new samples is required. Thus, an effective online factorization scheme is expected to update its factors without causing much computational effort. However, conventional NMF's computational complexity is proportional to the number of samples; therefore, it is not practical to perform the batch NMF processing whenever a new sample arrives. Additionally, in some applications contribution of old observations to factorization should be excluded whereas participations of new and significant samples should be emphasized. This is important in modeling dynamic content changes and can be achieved by imposing memorylessness. On the other hand, since adding a new sample into existing data would require recomputation of the whole basis set, batch algorithms are not generally suitable to online applications. Moreover, because such a recomputation requires using all of the previous samples, a huge memory requirement appears.</p><p>The introduced INMF scheme first extracts subspace representations for each new sample and then updates the former representation in an efficient way. It is capable of adaptively controlling contribution of each sample to the representation. This is achieved by assigning different weighting coefficients to each sample of the cost function. INMF also reduces significance of optimal rank selection by its weighted cost function.</p><p>The proposed INMF scheme is inspired by the technique introduced in Ref. <ref type="bibr" target="#b17">[18]</ref> which is called as incremental principal component analysis (IPCA). However, unlike the incremental PCA scheme that is presented in Ref. <ref type="bibr" target="#b17">[18]</ref>, INMF brings a new approach to incremental matrix factorization by including the non-negativity constraint. Preliminary versions of this work were published in Refs. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>This paper is organized as follows: After giving mathematical background of conventional NMF, its limitations for large data sets are discussed in Section 2. Section 3 presents the introduced INMF scheme. Prior to the final remarks given in Section 5, tests results are reported in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Conventional NMF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Mathematical definitions</head><p>Assume that an observation is represented by an n dimensional vector and the number of observations is equal to m. Therefore, V, the data matrix, will be an n by m matrix where V ij refers to an entity in V (i = 1, ... ,n, j = 1, ... ,m). As it is shown by Eq. (1), conventional NMF approximately factorizes the data matrix (V â R nÃm ) into two matrices: W â R nÃr , the mixing matrix, and H â R rÃm which is called as the encoding matrix <ref type="bibr" target="#b3">[4]</ref>. r is a pre-defined parameter which is named as rank of the factorization and determines the level of dimension reduction. Smaller values of r yield higher reconstruction errors that result in a higher loss in representation of original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V â WH</head><p>V consists of column vectors each of which corresponds to a different sample. Each column vector in V has a corresponding representation vector in the encoding matrix H. For the matrices  <ref type="formula" target="#formula_2">2</ref>) shows that the elements in a column of H determine how to combine the columns of W in constructing the corresponding column of V.</p><formula xml:id="formula_1">V = [v 1 v 2 v 3 ... v m ] and H=[h 1 h 2 h 3 ... h m ] v c</formula><formula xml:id="formula_2">v c = Wh c , c = 1, 2, . . . , m<label>(2)</label></formula><p>Conventionally the NMF scheme minimizes a cost function described in terms of the reconstruction error defined by Eq. (3). Although different cost functions are defined in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, the mean squared error given by Eq. ( <ref type="formula" target="#formula_3">3</ref>) is generally preferred by many researchers <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> due to its simplicity and efficiency.</p><formula xml:id="formula_3">F = 1 2 V -WH 2 = 1 2 n i=1 m j=1 (V ij -(WH) ij ) 2<label>(3)</label></formula><p>It is shown that the error function (F) defined by Eq. ( <ref type="formula" target="#formula_3">3</ref>) is a convex function of W and H separately. Therefore, multiplicative and alternating update rules for H and W are derived via gradient decent optimization <ref type="bibr" target="#b4">[5]</ref>. The update rule for the elements of matrix H is given by Eq. ( <ref type="formula">4</ref>), where j = 1, ... ,m, and a = 1, ... ,r.</p><formula xml:id="formula_4">H aj â H aj (W T V) aj (W T WH) aj (4)</formula><p>The update procedure which is derived for the elements of the mixing matrix W is shown by Eq. ( <ref type="formula">5</ref>) where i = 1, ... ,n, and a = 1, ... ,r.</p><formula xml:id="formula_5">W ia â W ia (VH T ) ia (VHH T ) ia (5)</formula><p>The update rules given by Eqs. ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>) are executed alternatively which means that at each iteration the elements of W will be updated after the update process of the encoding matrix is completed.</p><p>Step size is a crucial parameter of a gradient descent algorithm. A small step size may lead to a late convergence whereas a large one can cause divergence. It is proven by Lee and Seung <ref type="bibr" target="#b4">[5]</ref> that convergence is guaranteed when step size is chosen as aj for the update procedure of entity H aj and chosen as ia for W ia . aj and ia are given by Eq. ( <ref type="formula">6</ref>) where i = 1, ... ,n, and a = 1, ... ,r.</p><formula xml:id="formula_6">aj = H aj (W T WH) aj , ia = W ia (WHH T ) ia (6)</formula><p>Note that multiplicative update formulas shown by Eqs. ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>) do not require a predefined fixed learning step size, instead the step size is automatically updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Drawbacks of conventional NMF in online processing</head><p>In an online process once a factorization is obtained for a group of samples, that representation should be appropriately updated with respect to subsequent observations with a small computational cost. Because of its batch nature conventional NMF suffers some problems in this aspect.</p><p>Since each column of data matrix V corresponds to a different sample, arrival of each new sample leads to an increase in dimension of V. NMF's computational complexity is O(mnr) per iteration, and this implies that computational load linearly increases by dimension of data. Therefore, re-running the whole batch NMF process repetitiously whenever a new sample is received is impractical. Moreover, an increase in data dimension necessitates a higher value of rank which increases computational load to preserve the same representation quality.</p><p>NMF's significant storage demand is also a problem for online data processing. As Eqs. ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>) indicate, the multiplicative updating formulas use matrices W, H and V at each iteration thus require storing all the previous samples. Therefore, a huge amount of memory needs to be assigned for storing matrices V and H which makes the factorization impractical in an online application.</p><p>Another drawback of conventional NMF is encountered in integrating contribution of the latest samples to the representation. Conventionally, each sample has an equal contribution to the factors of NMF. However, some degree of memorylessness is desired for most applications. In such applications, systems need to exclude participations of old samples from their representations and need to increase influence of the latest samples. For example in a video surveillance application, a background model that can reflect content changes is required. For instance, if a mobile foreground object becomes stationary, then it has to be added into the background model immediately. Oppositely, a background object becomes a foreground object when it starts to move; therefore, it has to be excluded from the background representation. In order to achieve this, influences of new samples on the representation should be higher while effects of old ones need to be diminished. Because of its batch processing nature, conventional NMF lacks the ability to adaptively control contributions of individual samples to its representations and fails to impose memorylessness.</p><p>Next section presents INMF which is introduced to overcome these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Incremental non-negative matrix factorization</head><p>The aim of INMF is to update factor matrices W and H by adding effects of subsequent samples without requiring much computational effort. A new column should be added to both V and H, and mixing matrix W needs to be updated for each new sample. This yields an online updating scheme. Following subsections describe the introduced incremental form of the cost function and updating rules derived for INMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulating the cost function of INMF</head><p>Let W k and H k denote the optimized factor matrices of the initial k samples where k 2r. In Eq. ( <ref type="formula" target="#formula_7">7</ref>), F k refers to the cost function corresponding to NMF representation of the first k samples.</p><formula xml:id="formula_7">F k = V -W k H k 2 = 1 2 n i=1 k j=1 (V ij -(W k H k ) ij ) 2<label>(7)</label></formula><p>When the (k+1)th sample, v k+1 , arrives the reconstruction error is formulated by Eq. ( <ref type="formula" target="#formula_8">8</ref>). Note that this error function is expressed in terms of W k+1 and H k+1 .</p><formula xml:id="formula_8">F k+1 = V -W k+1 H k+1 2 = 1 2 n i=1 k+1 j=1 (V ij -(W k+1 H k+1 ) ij ) 2<label>(8)</label></formula><p>Conventional NMF updating rules given by Eqs. ( <ref type="formula">4</ref>) and ( <ref type="formula">5</ref>) can also be used to obtain optimal W k+1 and H k+1 matrices. However, this is not plausible in an online scheme because of high computational load.</p><p>Columns of matrix W can be thought as the building blocks of the data. Each entity of vector h c , where the subscript indicates the sample number, determines how these building blocks participate in the corresponding observation vector v c <ref type="bibr" target="#b5">[6]</ref>. This is why mixing matrix W is important in representing whole data. Moreover, as the number of observed samples increases, effects of new samples on the representation decrease. Hence, INMF does not need to update the encoding vectors of the old samples, because the new sample would not be able to significantly affect W's optimality for previous samples. By assuming that the first k columns of H k+1 would be approximately equal to H k , it is adequate to update only the last column of matrix H k+1 in addition to updating mixing matrix W. Thus, when the number of observed samples is equal to k+1, the reconstruction error of the first k samples (F k ) can be formulated as follows:</p><formula xml:id="formula_9">F k = 1 2 n i=1 k j=1 (V ij -(W k+1 H k+1 ) ij ) 2 1 2 n i=1 k j=1 (V ij -(W k+1 H k ) ij ) 2<label>(9)</label></formula><p>Consequently, reconstruction error of all samples can approximately be expressed as the sum of F k and f k+1 . In Eq. ( <ref type="formula" target="#formula_10">10</ref>) f k+1 refers to the residual error corresponding to the (k+1)th sample. In this formulation h k+1 is the encoding vector corresponding to the last sample.</p><formula xml:id="formula_10">F k+1 = 1 2 n i=1 k+1 j=1 (V ij -(W k+1 H k+1 ) ij ) 2 F k + 1 2 n i=1 ((v k+1 ) i -(W k+1 h k+1 ) i ) 2 F k + f k+1<label>(10)</label></formula><p>It can be seen from Eq. ( <ref type="formula" target="#formula_10">10</ref>) that contribution of each sample to the cost function is equal. Theoretically this is reasonable when samples are independent. However, in some applications, i.e. video processing, a number of successive samples with similar content may become dominant in the factorization. This may cause a problem in reflecting content changes which are brought to the factorization by a new frame, especially when the number of existing samples is high. Therefore, one might want to control contribution of each sample to the representation. In fact, it's usually desirable to make the latest samples have more effect on the representation. For example in background modeling, just as it is indicated in Section 2.2, if latest samples have more influence on the factorization, then adaptability of the factorization to dynamic content changes would increase <ref type="bibr" target="#b18">[19]</ref>.</p><p>In order to control contribution of each sample on the representation and add a degree of memorylessness to the INMF model, weighting coefficients S o ( ) and S f ( ), which are two functions of constant , are included to the cost function F k+1 as it is formulated by Eq. <ref type="bibr" target="#b10">(11)</ref>. Function S o ( ) determines contribution of older samples whereas S f ( ) is used to control that of the last one.</p><formula xml:id="formula_11">F k+1 = S o ( )F k + 1 2 S f ( ) n i=1 ((v k+1 ) i -(W k+1 h k+1 ) i ) 2<label>(11)</label></formula><p>According to Eq. ( <ref type="formula" target="#formula_11">11</ref>), error terms corresponding to previous samples are repetitiously multiplied by S o ( ), and the error component of a new sample is weighted by S f ( ). This forms a power series of S o ( ) in the weighting coefficients. Consequently, the cost function given by Eq. ( <ref type="formula" target="#formula_11">11</ref>) can be rewritten as it is formulated by Eq. ( <ref type="formula" target="#formula_12">12</ref>) where S j ( ) indicates the weight of jth sample.</p><formula xml:id="formula_12">F k+1 = 1 2 k+1 j=1 S j ( ) n i=1 (V ij -(W k+1 H k+1 ) ij ) 2<label>(12)</label></formula><p>where</p><formula xml:id="formula_13">S j ( ) = S k+1-2Ãr o ( ), j 2 Ã r S j ( ) = S k+1-j o ( )S f ( ), 2 Ã r &lt; j k + 1</formula><p>Under the constraint that S o ( ) is smaller than 1, contribution of older frames that are weighted by S k+1-2Ãr o ( ) and S k+1-j o ( ) will always decrease and will converge to zero as k goes infinity. This means that memorylessness is achieved when such a weighting is used. Note that S o ( ) controls how fast participations of early samples wane. In other words, S o ( ) supervises the subspace learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Derivation of multiplicative update rules of INMF</head><p>After constructing the cost function given by Eq. ( <ref type="formula" target="#formula_12">12</ref>), gradient descent optimization that yields INMF is performed. Whenever a new sample is acquired, mixing matrix W and the corresponding new column of H are updated. The update rule of h k+1 can be formulated within the framework of gradient descent algorithm as it is shown by Eq. ( <ref type="formula">13</ref>) where a = 1, ... ,r.</p><p>(h k+1 ) a â (h k+1 ) aa jF k+1 j(h k+1 ) a <ref type="bibr" target="#b12">(13)</ref> The partial derivative in Eq. ( <ref type="formula">13</ref>) can be calculated as follows:</p><formula xml:id="formula_14">jF k+1 j(h k+1 ) a = j j(h k+1 ) a â¡ â£ 1 2 k+1 j=1 S j ( ) n i=1 (V ij -(W k+1 H k+1 ) ij ) 2 â¤ â¦ = 1 2 k+1 j=1 S j ( ) j j(h k+1 ) a Ã â¡ â¢ â£ n i=1 â â (v j ) i - r k=1 (W k+1 ) ik (h j ) k â â  2 â¤ â¥ â¦ = S k+1 ( ) n i=1 (((v k+1 ) i -(W k+1 h k+1 ) i )(-W k+1 ) ia ) = S k+1 ( )[-(W T k+1 v k+1 ) a + (W T k+1 W k+1 h k+1 ) a ]<label>(14)</label></formula><p>When the expression given by Eq. ( <ref type="formula" target="#formula_15">15</ref>) is selected as the step size, the update rule given by Eq. ( <ref type="formula" target="#formula_16">16</ref>) is derived.</p><formula xml:id="formula_15">a = (h k+1 ) a S j ( )(W T k+1 W k+1 h k+1 ) a (<label>15</label></formula><formula xml:id="formula_16">) (h k+1 ) a â (h k+1 ) a (W T k+1 v k+1 ) a (W T k+1 W k+1 h k+1 ) a<label>(16)</label></formula><p>The update equation for the iath component of mixing matrix W k+1 is the one given by Eq. ( <ref type="formula">17</ref>) where i = 1,2, . . . ,n and a = 1,2, . . . ,r. W k will be used as initial condition for the factorization process that will be done when (k+1)th sample is received.</p><formula xml:id="formula_17">(W k+1 ) ia â (W k+1 ) ia -ia jF k+1 j(W k+1 ) ia (17)</formula><p>The partial derivative shown in Eq. ( <ref type="formula">17</ref>) can be expressed by Eq. <ref type="bibr" target="#b17">(18)</ref>.</p><formula xml:id="formula_18">jF k+1 j(W k+1 ) ia = j j(W k+1 ) ia â¡ â£ 1 2 k+1 j=1 S j ( ) n i=1 (V ij -(W k+1 H k+1 ) ij ) 2 â¤ â¦ = 1 2 k+1 j=1 S j ( ) j j(W k+1 ) ia â¡ â£ n i=1 (V ij -(W k+1 H k+1 ) ij ) 2 â¤ â¦ = k+1 j=1 S j ( )(V ij -(W k+1 H k+1 ) ij )(-H k+1 ) aj = k+1 j=1 S j ( )(-V ij (H k+1 ) aj + (H k+1 ) aj (W k+1 H k+1 ) ij ) (<label>18</label></formula><formula xml:id="formula_19">)</formula><p>ia , which is given by Eq. ( <ref type="formula">19</ref>) is chosen as the step size. By defining a matrix that contains elements such that T ij = S( )H ij and using it in Eqs. ( <ref type="formula" target="#formula_12">12</ref>) and ( <ref type="formula">19</ref>), it can be seen that this formulation is basically equivalent to the case of conventional NMF. As a consequence, the proof of convergence given in Ref. <ref type="bibr" target="#b4">[5]</ref> will also be valid for this situation.</p><formula xml:id="formula_20">ia = (W k+1 ) ia k+1 j=1 S j ( )(W k+1 H k+1 ) ij (H T k+1 ) ja (19)</formula><p>By using Eqs. ( <ref type="formula">17</ref>)-( <ref type="formula">19</ref>), multiplicative update rule for elements of W k+1 can be shown to be equal to the one given by Eq. ( <ref type="formula" target="#formula_21">20</ref>) where h j denotes jth column of H k+1 , v j denotes jth column of V and i = 1, ... ,n, a = 1, ... ,r.</p><formula xml:id="formula_21">(W k+1 ) ia â (W k+1 ) ia k+1 j=1 S j ( )(v j h T j ) ia k+1 j=1 S j ( )(W k+1 h k+1 h T k+1 ) ia (<label>20</label></formula><formula xml:id="formula_22">)</formula><p>Since the first k columns of H k+1 is not updated when the (k+1)th sample arrives, the update rule given by Eq. ( <ref type="formula" target="#formula_21">20</ref>) can be rewritten as in the form of Eq. ( <ref type="formula" target="#formula_23">21</ref>).</p><formula xml:id="formula_23">(W k+1 ) ia â (W k+1 ) ia (S o ( )V k H T k + S f ( )v k+1 h T k+1 ) ia (S o ( )W k+1 H k H T k + S f ( )W k+1 h k+1 h T k+1 ) ia (<label>21</label></formula><formula xml:id="formula_24">)</formula><p>Although the new form given by Eq. ( <ref type="formula" target="#formula_23">21</ref>) is equivalent to Eq. ( <ref type="formula" target="#formula_21">20</ref>), the expression shown in Eq. ( <ref type="formula" target="#formula_23">21</ref>) is helpful in reducing the complexity. This is because both V k and H k do not change throughout the process; thus, instead of storing V k and H k , whose dimensions increase as new samples arrive, the multiplications V k H T k and H k H T k can be stored. Two benefits appear here one of which is about size of required storage memory. Since dimensions of the matrices obtained by these multiplications remain constant, required storage memory will be the same regardless the sizes of V k and H k . Secondly, the number of matrix multiplications which are the main reason of computational complexity of conventional NMF will be reduced. When using these matrix multiplications in the update rule given by Eq. ( <ref type="formula" target="#formula_23">21</ref>), instead of implementing them repetitiously we can make use of the updated versions given by</p><formula xml:id="formula_25">V k+1 H T k+1 = S o ( )V k H T k + S f ( )v k+1 h T k+1 (<label>22</label></formula><formula xml:id="formula_26">)</formula><formula xml:id="formula_27">H k+1 H T k+1 = S o ( )H k H T k + S f ( )h k+1 h T k+1 (<label>23</label></formula><formula xml:id="formula_28">)</formula><p>V k+1 H T k+1 can be expressed as sum of the multiplications of the corresponding columns of V k+1 and H k+1 . Since only the last column of H k+1 is updated, there will be no need to recalculate V k H T k which would require multiplying each k column pair of V k and H k and then adding these ( k i=1 v i h T i ). In addition, multiplying the right hand side components by S o ( ) and S f ( ), respectively, whenever a new sample is processed will produce the weighing coefficients which are given by Eq. ( <ref type="formula" target="#formula_12">12</ref>). Hence, computational complexity of executing these multiplications will be O(nr) instead of O(nmr). Considering this, overall computational complexity of INMF becomes O(nr 2 ) per iteration. Note that it is independent of the number of samples, m.</p><p>A comparison between complexities of INMF and NMF can be done via Fig. <ref type="figure" target="#fig_1">1</ref>. The time required running the NMF algorithm increases linearly by the number of samples. On the other hand, INMF's computational complexity is independent of the number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Selection of weighting coefficients/functions</head><p>This subsection discusses weighting scheme of INMF's cost function and its effect on controlling memorylessness of the factorization. As it is described in the previous subsection, S o ( ) and S f ( ), which are introduced in Eq. <ref type="bibr" target="#b10">(11)</ref>, are independent of the elements of V, W and H. These functions are included into the cost function for the aim of controlling contributions of the subsequent observations to INMF representations, and there are no restrictions for these functions except their non-negativity. Non-negativity constraint is necessary in order to preserve additive nature of INMF. A wise approach is choosing S o ( ) and S f ( ) in a way that the prior samples' weights decay while effects of the latest samples are emphasized. With taking a value between 0 and 1, a suitable function pair for S o ( ) and S f ( ) would be (1-) and , respectively. A similar approach is also used for the incremental PCA scheme presented in Ref. <ref type="bibr" target="#b17">[18]</ref> for statistical background modeling problem.</p><p>As indicated in Section 3.1, consecutive execution of this weighting process will form a power series of S o ( ) = 1-on the weighting coefficients. Let f 2r+k shown in Eq. ( <ref type="formula" target="#formula_29">24</ref>) be used to indicate the residual error caused by the (2r+k)th sample. As it can be seen in Eq. ( <ref type="formula" target="#formula_29">24</ref>), since S o ( ) is chosen as a positive number which is smaller than 1, effects of earlier samples are smaller than those of new ones on the cost function.</p><formula xml:id="formula_29">F 2r+k = (1 -) k F 2r + (1 -) k-1 f 2r+1 + (1 -) k-2 f 2r+2 + â¢ â¢ â¢ + f 2r+k (<label>24</label></formula><formula xml:id="formula_30">)</formula><p>One consequence of selecting weighting functions as in Eq. ( <ref type="formula" target="#formula_29">24</ref>) is an increase in the speed of adaptability to dynamic data changes. Although this was the main reason of choosing the weighting functions as (1-) and , it is not the only significant advantage that Eq. ( <ref type="formula" target="#formula_29">24</ref>) provides. For conventional NMF there is a tradeoff between rank and factorization accuracy. In contrast, the introduced weighted cost function eliminates importance of optimal rank selection. This is because the weighting scheme makes a windowing effect on the factorization process, since the effects of the earlier samples continuously wane. Thus, even if the number of samples increases significantly the actual dimension of the problem would not change due to memorylessness property. Note that should be selected experimentally depending on requirements of the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In order to evaluate performance of the introduced INMF method in large scale online data processing, several tests are performed on PETS <ref type="bibr" target="#b20">[21]</ref> and TRECVID <ref type="bibr" target="#b23">[24]</ref> benchmarking video data sequences. In order to demonstrate use of INMF, results of general experiments on two applications, which are background modeling and clustering, are reported in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Background representation capability of INMF</head><p>Automatic visual tracking module included in most video surveillance systems requires a proper background model that needs to be adapted to dynamic content changes. We have performed background modeling tests on video samples taken from PETS 2000 and 2001 surveillance video data sets <ref type="bibr" target="#b20">[21]</ref>. Outdoor scene videos in PETS database are captured under severe illumination changes and include several foreground objects moving on a busy background. Thus, PETS database is a good test case to evaluate memorylessness and subspace learning capability of our incremental factorization scheme.</p><p>Since INMF aims to minimize the mean squared error formulated by Eq. ( <ref type="formula" target="#formula_10">10</ref>), it is convenient to evaluate the factorization performance by examining the residual (reconstruction) error encountered for each video frame. Therefore, in this paper factorization performance is reported in terms of residual error obtained for each new frame. Note that in Eq. ( <ref type="formula" target="#formula_10">10</ref>) residual error for frame (k+1) is defined as</p><formula xml:id="formula_31">f k+1 = 1 2 n i=1 ((v k+1 ) i -(W k+1 h k+1 ) i ) 2</formula><p>Here subscription k denotes the number of video frames processed up to the last observation. Initially, the first 150 frames of `dataset1_cam1' surveillance video sequence from PETS 2001 database is used to constitute data matrix V. An (144Ã192) dc image <ref type="bibr" target="#b21">[22]</ref> of each video frame is considered as a sample observation and stored as a column of V, thus V is an ((144Ã192)Ã150) matrix. Since all of these 150 frames are background frames, it is assumed that content of data samples remains stationary throughout the observations. Note that conventional NMF applies a batch processing on all of these 150 frames. Therefore, residual error corresponding to the (k+1)th frame is equal to</p><formula xml:id="formula_32">f k+1 = 1 2 n i=1 (V i,k+1 -(WH) i,k+1 ) 2</formula><p>for NMF. Fig. <ref type="figure" target="#fig_2">2</ref> illustrates distribution of reconstruction error versus frame number for four different tests where rank is chosen as r = 2. Since the observed 150 background frames are very similar to each other, the reconstruction error obtained both by unweighted INMF and conventional NMF are very small (blue and red lines plotted in Fig. <ref type="figure" target="#fig_2">2</ref>). In fact, an error value about 0.5 is negligible when compared to the upper bound of error which is equal to 255 2 /2. This indicates that both INMF and NMF are capable of forming an efficient initial background representation. In this test, the number of iterations for  <ref type="formula" target="#formula_11">11</ref>), is (1-) and , respectively. Although both results shown in Fig. <ref type="figure" target="#fig_2">2</ref> are acceptable and sensible, reconstruction error of weighted INMF is smaller for = 0.3 compared to = 0.2 (green and cyan line plots in Fig. <ref type="figure" target="#fig_2">2</ref>). This is because higher values increase contribution of the last observation on the factorization. Note that unlike conventional NMF, INMF allows controlling contribution of each sample to representation which results in a better optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adaptability to dynamic content changes</head><p>In video surveillance applications generally a background model is constructed at first. If there is a moving foreground object in the scene, for instance if an object enters the scene or an initially stable object starts moving, some deviations from background model appear. On the other hand, a mobile object may exit the scene or stops and turns into a background object. A powerful online and incremental factorization scheme should be capable of adapting existing background representation according to these changes which are referred as dynamic content changes. These changes can be considered as local variations from current model, because the observed samples (new video frames) do not contain a global change but only a local region of data experiences variations. So, it is common to evaluate dynamic content representation capability of a factorization by first projecting the samples (frames) onto the background model and then measuring the deviations from the background model. Numerical value corresponding to these deviations is referred as residual error within the text.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref>(a) and (b) illustrate two frames from a PETS2000 test sequence which are used in examining online representation capability of INMF. In the first frame (Fig. <ref type="figure" target="#fig_3">3(a)</ref>) there are two moving objects: a car is parking while a man is walking. In the second frame (Fig. <ref type="figure" target="#fig_3">3(b)</ref>) the car becomes stationary after parking. On the other hand, two new moving objects, a man who is getting off from that car and a new car, enter the scene. Figs. <ref type="figure" target="#fig_3">3(c</ref>) and (d) illustrate the residue images obtained by taking the difference between the reconstructed background and the observed frame. It is shown that the proposed INMF scheme is capable of efficiently representing the background, thus all foreground objects are observable in the residue images. Furthermore, the moving car is successfully included into the background model after parking. For this experiment the INMF algorithm is executed with S o ( ) = 0.8 and S f ( ) = 0.2, and it is observed that adaptation of the background model takes 7 frames after parking.</p><p>Performances of different methods can numerically be examined by evaluating the variations of residual error. For this test a PETS2001 video sequence is used and Fig. <ref type="figure" target="#fig_4">4</ref> plots distribution of the residual error versus frame number for 700 frames. Three different methods are compared: INMF, NMF and the incremental PCA algorithm that is presented in Ref. <ref type="bibr" target="#b17">[18]</ref>.</p><p>As it can be seen from Fig. <ref type="figure" target="#fig_4">4</ref>, since there is no foreground object in the scene from frame 800 to 940, the residual errors for all of the methods are small. Note that the error of conventional NMF is slightly higher compared to other two methods. This is because of batch processing scheme of NMF which assigns equal weighting factors to each frame.</p><p>It can be concluded from Fig. <ref type="figure" target="#fig_4">4</ref> that a car enters the scene at frame 980 and moves till frame 1120. Thus, the residual error increases, since the existence of a foreground object results in high variations from the background model. The important point here is that the residual error of INMF drops at 1120, just after the car stops and becomes a background object. This demonstrates the fact that deviations from the background model are diminished when the car is successfully included into the background model. Notice that unlike INMF, NMF fails to achieve this adaptability (Fig. <ref type="figure" target="#fig_4">4</ref> black line). On the other hand, the residual error increases again when a new walking man enters the scene, and one of the parking cars starts moving after frame 1200.</p><p>By examining Fig. <ref type="figure" target="#fig_4">4</ref> it can also be concluded that unlike conventional NMF, INMF has the ability of adapting its factors with respect to content changes and is capable of imposing memorylessness to its representations. Moreover, background modeling and adaptability performances of INMF and incremental PCA method of Ref. <ref type="bibr" target="#b17">[18]</ref> are very close. In this experiment S o ( ) and S f ( ) are selected as 0.8 and 0.2, respectively. As it is discussed in Section 3.3, choosing S o ( ) = 0.8 enforces memorylessness by lowering contributions of older frames. In order to prevent dominance of the last frame on the representation, it is appropriate to choose S f ( ) = 0.2. It should be noted that the choice of weighting functions are problem specific. Taking both of them as equal to 1 yields an unweighted INMF scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness to illumination changes</head><p>We have also evaluated INMF's performance under severe illumination changes that shift intensity values of majority of pixels and thus significantly change the mean intensity. Intense illumination changes are difficult to handle because of significant deviations they cause on the original data.</p><p>For this test, samples are taken from another PETS2001 sequence where severe illumination changes are experienced. A comparison between INMF and incremental PCA of Ref. <ref type="bibr" target="#b17">[18]</ref> has been performed for the weighting functions (1-) and where = 0.05. Fig. <ref type="figure" target="#fig_5">5</ref> plots distribution of the residual error versus frame number through the frames taken from a stationary scene in which there are no foreground objects or motions but only illumination changes. Fig. <ref type="figure" target="#fig_5">5</ref> leads to the conclusion that INMF is more robust to global data changes compared to incremental PCA that is described in Ref. <ref type="bibr" target="#b17">[18]</ref>. This can also be observed from Fig. <ref type="figure" target="#fig_6">6(c)</ref> and<ref type="figure">(d</ref>) which illustrate the residue images (variations from the background model) corresponding to the original frames that are shown in Fig. <ref type="figure" target="#fig_6">6</ref>(a) and (b), respectively. Since there are no foreground objects in the scene, the residue images are expected to be blank. However, because the illumination variations change the pixel intensities significantly, pixels which belong to shiny regions of the scene are slightly visible in the residue images. Although the residue images obtained by both methods  are distorted, the level of distortion is very small for INMF (see Fig. <ref type="figure" target="#fig_6">6(d)</ref>) which reflects the error distribution shown in Fig. <ref type="figure" target="#fig_5">5</ref>. This is because theoretically INMF does not impose a Gaussian distribution assumption on the transformed data in contrast to PCA. Therefore, INMF representations are more robust to illumination variations which significantly change mean value of the observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">INMF as a clustering tool</head><p>In this subsection we have examined INMF as a clustering tool to demonstrate another application of the introduced incremental factorization scheme. We have used clustering capability of INMF for online video scene change detection which constitutes the first step in content-based video abstraction <ref type="bibr" target="#b22">[23]</ref>.</p><p>Recently the relationship between NMF and fuzzy k-means clustering has been evaluated, and it is shown that both methods optimize the same objective function <ref type="bibr" target="#b16">[17]</ref>. Inspired by this approach, it can be shown that INMF can also be considered as a clustering tool, because-as it is derived in Section 3.1-the cost function that INMF optimizes (Eq. ( <ref type="formula" target="#formula_10">10</ref>)) is in the same form as conventional NMF (Eq. ( <ref type="formula" target="#formula_8">8</ref>)), except that INMF performs the optimization incrementally.</p><p>To demonstrate the use of INMF as a clustering tool, we have shown its performance in video scene change detection, thus a video sequence taken from TRECVID data set <ref type="bibr" target="#b23">[24]</ref> which contains two different scenes is processed. A video scene is a sequence of consecutive frames taken within the same camera shot and generally considered as the main structural element of video that contain temporal knowledge about the content. If we define scene change detection as a pattern classification problem, these two scenes correspond to two different clusters, and the motivation behind solving this problem   can be simply defined as finding an appropriate clustering scheme that provides cluster centroids as well as the members. Fig. <ref type="figure" target="#fig_7">7</ref>(a) and (b) illustrate two video frames (samples) which are taken from each of these two scenes (classes). For simplicity, test results obtained on a short video sequence which consists of 180 frames are reported, and the number of samples taken from each class is very close. Since the number of clusters in a scene change detection problem is fixed to 2, it is appropriate to choose factorization rank as equal to 2 (r = 2). The introduced INMF scheme is performed on 180 frames, and the analogy between factorization and clustering has been clearly observed by examining the basis images (columns of mixing matrix W) and the reconstruction error for each new sample (frame). Fig. <ref type="figure" target="#fig_8">8</ref> illustrates how the first and second basis images (the first and second columns of W) evolve during the factorization. Note that rather than a batch process, an incremental and online clustering scheme has been performed by INMF. Since the first 80 observations (frames) come from the first cluster, the basis images that are obtained after factorizing these samples are visually similar to the frames of the first class (Fig. <ref type="figure" target="#fig_8">8</ref>(a), (b), (e) and (f)). This is not surprising since INMF's objective function that minimizes the reconstruction error forces mixing matrix W to contain as much information about data as possible. After the 80th frame samples of the second class are processed, and content of basis images start to change. Throughout the incremental factorization process the second basis image incrementally converges to the cluster centroid of the second cluster (Fig. <ref type="figure" target="#fig_8">8</ref>, second row). On the other hand, the first basis image converges to centroid of the first cluster (Fig. <ref type="figure" target="#fig_8">8</ref>, first row). Note that each of the basis images contains a little information about the other cluster as well. This is due to fuzzy clustering nature of INMF.</p><p>In an online video scene change detection task, the aim is to detect scene boundaries in an online manner. This requires online determination of cluster boundaries or the location where the old scene ends and the new one begins. We take advantage of examining the factorization residual error to achieve this. Therefore, whenever a new frame (sample) is processed, we also determine whether that frame is the last sample of the current cluster or not by examining its residual error. As before, the residual error is obtained by calculating</p><formula xml:id="formula_33">f k+1 = 1 2 n i=1 (V i,k+1 -(WH) i,k+1 ) 2 .</formula><p>If it is higher than a prespecified threshold, this implies that the new frame does not belong to the current cluster. So, the residual error can be used as a decision criterion to locate scene changes (cluster boundaries). Fig. <ref type="figure" target="#fig_9">9</ref> plots the residual error versus frame number. Up to frame 80 all frames are from the first scene, thus the basis images contain information only about the first scene (Fig. <ref type="figure" target="#fig_8">8</ref>(a), (b), (e), (f)). When a frame from the new scene arrives, a rise in the error is experienced. This is due to the fact that it will take some time until the basis images retrieve enough information about the new scene to secure a successful reconstruction performance. On the other hand, as new frames from the second scene continue to arrive, their content is included into the basis images (Fig. <ref type="figure" target="#fig_8">8(c</ref>), (d), (g), (h)) gradually. This results in a better reconstruction performance and a drop in the residual error. Fig. <ref type="figure" target="#fig_7">7(c</ref>) and (d) gives the reconstructed images of frames 75 and 175, respectively. The reconstructions of both frames are successful. A peak in the residual error observed at frame 80 indicates that a scene change occurs. Considering this, we can conclude that the first 80 frames belong to the first cluster (scene) while the rest belongs to the second cluster (scene 2).</p><p>Regarding the clustering issue, since each column of the encoding matrix H determines how the individual basis images contribute to reconstruction of corresponding samples, these columns can be considered as cluster indicators. For instance, let the first column of matrix H be [0.8 0.2] T . This illustrates that contribution of the first mixing vector (cluster centroid) is higher than the second one. Consequently, it can be concluded that the corresponding sample belongs to the first cluster. Hence, the cluster indicator of each sample constitutes a 2D feature vector and shows to which cluster that sample belongs. Consequently, samples that belong to the same cluster have similar cluster indicators. Fig. <ref type="figure" target="#fig_10">10</ref> plots columns of encoding matrix H obtained by the test described above. It can be seen from Fig. <ref type="figure" target="#fig_10">10</ref> that cluster indicators of frames coming from different scenes are linearly separable.</p><p>The use of NMF's clustering ability for detecting scene changes can be further expanded, and scene-based video frame classification can be achieved by INMF representations. In this case, in contrast to scene change detection application, we do not seek to detect whether the latest sample belongs to a new pattern, but aim to determine to which of the classes (clusters) it belongs. Additionally, the number of classes is not restricted to two. As an example, 250 video frames of three scenes are taken from TRECVID database. Two of these scenes have some frames that contain partial information about the other scene as it is shown in the last column of the first and second rows of Fig. <ref type="figure" target="#fig_11">11</ref>. As a result of a tilt type camera motion (vertical motion of camera), content variation of the third scene shown at the last row of Fig. <ref type="figure" target="#fig_11">11</ref> is higher than the other two. Three samples from each of these three scenes are given in Fig. <ref type="figure" target="#fig_11">11</ref>.</p><p>When INMF (r = 3) is executed on these 250 frames at random order, three different clusters, each of which is plotted in a different color in Fig. <ref type="figure" target="#fig_12">12</ref>, are obtained. Note that this process is an unsupervised one and Fig. <ref type="figure" target="#fig_12">12</ref> plots 3D cluster indicators acquired by the encoding matrix.</p><p>The use of INMF in this application is quite straightforward. NMF has been used in this kind of classification/clustering problems recently <ref type="bibr" target="#b16">[17]</ref>, and what INMF does is achieving the same goal in an incremental manner. For comparison purposes, Fig. <ref type="figure" target="#fig_13">13</ref> is obtained when NMF (r = 3) is executed on the same data. Observe that classification performances of both factorizations are high but shapes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper an incremental non-negative matrix factorization (INMF) scheme is introduced for online processing of large scale non-stationary data. It is shown that with its incremental nature the introduced factorization achieves the goal of forming subspace representations which can reflect dynamic data content with a low computational complexity. Our algorithm, which is implemented with an application that is developed in Visual Studio on Intel Core 2 Quad CPU with 2.40 GHz processor, works real-time on the test video sequences. Unlike conventional NMF, memorylessness property which is crucial in modeling online data is another important feature of INMF. Furthermore, INMF's weighted cost function that results in an adaptive control of memorylessness reduces significance of optimum rank selection which is an important problem in factorization.</p><p>In order to demonstrate INMF's efficiency, general experiments on two applications, which are background modeling and clustering, are performed. Background modeling in video surveillance illustrates our algorithm's robustness and success in tracking the content changes while the other group of experiments aims to show how INMF's clustering ability can be used in video scene change detection and scene-based video frame classification problems.</p><p>We are currently investigating new weighting functions to improve the subspace learning capability of incremental NMF. By using appropriate weighting functions it could also be possible to improve the adaption rate of our algorithm. We are also exploring the clustering capability of incremental NMF and how NMF's clustering performance could be improved by imposing sparseness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and h c indicate the column vectors of V and H, respectively, where c = 1, ... ,m. The mixing matrix is defined as W = [w 1 w 2 ... w r ]. Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Time required by one iteration versus number of samples.</figDesc><graphic coords="5,40.11,71.07,236.52,188.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of residual errors versus frame number.</figDesc><graphic coords="5,309.10,71.16,236.52,195.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Original frame 614. (b) Original frame 837. Residue image showing the deviations from the background model (c) at frame 614 and (d) at frame 837 (pixel intensities are increased for illustration purpose).</figDesc><graphic coords="6,50.15,71.19,236.52,208.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Distribution of the residual error versus frame number for three methods: the introduced INMF with So = 0.8 and Sf = 0.2, the conventional NMF, and IPCA.</figDesc><graphic coords="6,319.14,70.86,236.52,197.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distribution of the residual error versus frame number for the INMF and IPCA.</figDesc><graphic coords="7,40.11,410.23,236.52,198.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Original frame 2635. (b) Original frame 2874. Variations from the background model for frame 2874 obtained (c) by IPCA and (d) by INMF.</figDesc><graphic coords="7,112.60,656.18,360.00,80.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Original frame 75; (b) original frame 175; (c) frame 75 reconstructed by INMF; and (d) frame 175 reconstructed by INMF.</figDesc><graphic coords="7,309.10,405.97,236.52,195.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Incrementally evolved basis images. The first column of: (a) W12, (b) W63, (c) W84, (d) W170 and the second column of: (e) W12, (f) W63, (g) W84, (h) W170, where the subscripts shows the number of frames processed incrementally up to that time instant.</figDesc><graphic coords="8,122.65,70.92,360.00,154.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Distribution of the reconstruction error of INMF versus frame number.</figDesc><graphic coords="8,50.15,278.54,236.52,198.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Cluster indicators which are obtained by the INMF.</figDesc><graphic coords="8,319.64,278.09,235.44,175.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Nine different video frames taken from three scenes. Each row contains three frames from one of the three scenes.</figDesc><graphic coords="9,112.60,70.92,360.00,244.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Cluster indicators obtained by INMF for three clusters.</figDesc><graphic coords="9,310.10,360.27,234.00,154.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Cluster indicators obtained by NMF for three clusters.</figDesc><graphic coords="10,51.15,71.35,234.72,153.65" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is partially supported by the Scientific and Technological Research Council of Turkey.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A receptor model using a specific non-negative transformation technique for ambient aerosol</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Israel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmos. Environ</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2289" to="2298" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Positive matrix factorization-a nonnegative factor model with optimal utilization of error estimates of data values</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paatero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Systems</title>
		<meeting>Neural Information Systems</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="942" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonsmooth nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pascual-Montano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Carazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kochi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Pascual-Marqui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="403" to="415" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">New algorithms for non-negative matrix factorization in applications to blind source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="621" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonnegative sparse coding</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop Neural Networks for Signal Processing</title>
		<meeting><address><addrLine>Martigny, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatially localized partsbased representations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Projected gradient methods for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast Newton-type methods for the least squares nonnegative matrix approximation problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth SIAM Conference on Data Mining</title>
		<meeting><address><addrLine>Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for face recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guillamet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Catalonian Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for polyphonic music transcription</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop Applications of Signal Processing to Audio and Acoustics</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferential robust non-negative matrix factorization analysis of microarray data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ledirac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="44" to="49" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust image hashing via non-negative matrix factorizations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Mihcak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The relationships among various nonnegative matrix factorization methods for clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth IEEE International Conference on Data Mining</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On incremental and robust subspace learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1509" to="1518" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video content representation by incremental non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Bucak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gunsel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>San Antonio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Incremental non-negative matrix factorization for dynamic background modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Bucak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gunsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gursoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICEIS International Workshop on Pattern Recognition in Information Systems</title>
		<meeting><address><addrLine>Funchal, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="http://ftp.pets.rdg.ac.uk/" />
		<title level="m">PETS Video Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid scene analysis on compressed videos</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Systems Video Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="533" to="544" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature extraction and a database strategy for video fingerprinting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oostveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Recent Advances in Visual Information Systems</title>
		<meeting><address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">He is currently a graduate student and Research Assistant at Istanbul Technical University. About the Author-BILGE GUNSEL received M.S. and Ph.D. degrees in Electronics and Communication Engineering from Istanbul Technical University, Istanbul, Turkey in 1988 and 1993, respectively. She is currently working as a Professor in the Electrical and Electronics Engineering Department at Istanbul Technical University. She has been a Senior Researcher at the Scientific and Technical Research Council of Turkey</title>
	</analytic>
	<monogr>
		<title level="m">Assistant Professor in the Electrical and Electronics Engineering Department at Istanbul Technical University</title>
		<meeting><address><addrLine>USA; Rochester, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="1994">2006. 1999-2002. 1998. 1995-1997. 1994</date>
		</imprint>
		<respStmt>
			<orgName>About the Author-SERHAT SELCUK BUCAK received his B.Sc. in Telecommunication Engineering from Istanbul Technical University ; Marmara Research Center, Information Technologies Research Institute ; Research Associate in the Electrical Engineering Department at University of Rochester</orgName>
		</respStmt>
	</monogr>
	<note>Her research interests include audio watermarking, video/image content analysis and retrieval, stochastic image models and multimedia information systems. She has been serving as an Editorial Board member. She holds one US patent on content based access to video</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
