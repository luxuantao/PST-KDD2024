<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yujia</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zeng Microsoft</surname></persName>
						</author>
						<title level="a" type="main">Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>sweater vest Location: [123</term>
					<term>176</term>
					<term>334</term>
					<term>461 ] Tags: apple Caption: N/A Attribute: gala apple Location: [52</term>
					<term>208</term>
					<term>101</term>
					<term>254]</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People say, "A picture is worth a thousand words". Then how can we get the rich information out of the image? We argue that by using visual clues to bridge large pretrained vision foundation models and language models, we can do so without any extra cross-modal training. Thanks to the strong zero-shot capability of foundation models, we start by constructing a rich semantic representation of the image (e.g., image tags, object attributes / locations, captions) as a structured textual prompt, called visual clues, using a vision foundation model. Based on visual clues, we use large language model to produce a series of comprehensive descriptions for the visual content, which is then verified by the vision model again to select the candidate that aligns best with the image. We evaluate the quality of generated descriptions by quantitative and qualitative measurement. The results demonstrate the effectiveness of such a structured semantic representation. Tags: argyle sweater vest, nerd ? Caption: a man holding a red apple Objects in this image: a man wearing a vest. vest, is at lower middle of the image and is moderate in the image. Attribute: sweater vest apple, is at left of the image and is small in the image. Attribute: gala apple ?? Caption: a man holding a red apple Tags: This image is about argyle sweater vest, nerd, geek charming, argyle sweater, sweater vest.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This image is of a family celebrating Christmas. They are all gathered around a dinner table, with a turkey and other food on it. The family is smiling and seems to be enjoying themselves. There is a Christmas tree in the background and some Christmas lights on the walls. This image features a horse and buggy travelling down a road in the town of Holguin, Cuba. The horse is harnessed to the buggy and is pulling it along, while a farmworker rides behind. This image captures the everyday life of Cubans, with their traditional horse-drawn carts still in use. "Vision is a process that produces from images of the external world a description that is useful to the viewer and not cluttered with irrelevant information."</p><p>-David Marr, Vision, p31</p><p>What makes a good "description" for vision? Over the past several decades, computer vision pioneers drew inspiration from neural science, cognitive science, and psychophysics <ref type="bibr">(Marr, 2010)</ref>, pointing us to the North Stars <ref type="bibr" target="#b9">(Fei-Fei and Krishna, 2022)</ref>, some among them being image classification and object detection. Despite the tremendous progress that has been made, much of these object-centric works remain a proxy for an eventual task or application that requires a holistic view of the visual content, involving concepts beyond objects: actions, attributes, and relations, to name a few.</p><p>In our work, we argue that textual representation suffices such "description". It brings forth a more holistic visual representation than categorical labels. It allows machines to interpret visual signals through descriptive captions <ref type="bibr" target="#b28">(Zhou et al., 2020;</ref><ref type="bibr" target="#b18">Li et al., 2022)</ref>, and perform more language-heavy tasks such as question-answers <ref type="bibr">(Rajpurkar et al., 2016)</ref>, or multi-round dialogues <ref type="bibr" target="#b19">(Li et al., 2017)</ref>.</p><p>On the other hand, the access to abundant web multimodal language data (e.g., image alt-text, video subtitles) provides us with the fuel for powering neural visual representations from contrastive language-image pre-training (CLIP, <ref type="bibr" target="#b26">Yuan et al. (2021)</ref>; <ref type="bibr">Radford et al. (2021)</ref>). The marriage of the two renders a new computer vision system that is faithful, generic, and versatile.</p><p>We name this new computer vision system BEST, for Bridging with Explicit Structured Textural clues. We start by constructing a semantic representation of the image. This semantic representation, which we referred to as visual clues, comprises rich semantic components, from object and attribute tags to localized detection regions and region captions. Powered by the recent advances in vision foundation model Florence, the visual clues are rich in open-vocabulary expressions, marking a major difference compared to existing symbolic approaches (e.g., scene graphs <ref type="bibr" target="#b17">Krishna et al. (2017)</ref>) with closed-set vocabularies.</p><p>The visual clues are interpretable, not only for humans, but also for machines. Take the generative language model GPT-3 <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>. The visual clues could be digested by GPT-3, which in return produces crisp language descriptions that are sensible to the viewer while not cluttered with irrelevant information from the visual clues. Whereas this open-loop process could potentially suffer from object hallucination issues as the outputs from GPT-3 are not governed by any means, we further deploy a closed-loop verification procedure that grounds descriptions back to the original image.</p><p>To evaluate the quality of the language descriptions, we resort to an existing task named Image Paragraph Captioning (IPC), but with a twist. IPC aims to address the demand for generating long, coherent, and informative descriptions of the whole visual content of an image <ref type="bibr" target="#b16">(Krause et al., 2017)</ref>, which can eventually be used for many applications including poetry composition <ref type="bibr">(Liu et al., 2018)</ref>, automatic recipe generation <ref type="bibr">(Salvador et al., 2019)</ref>, visual storytelling <ref type="bibr" target="#b11">(Huang et al., 2016)</ref>, advertisement generation, or help blind or visually-impaired people see better. The existing metrics for IPC such as BLEU <ref type="bibr">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b8">(Denkowski and</ref><ref type="bibr" target="#b8">Lavie, 2014), and</ref><ref type="bibr">CIDEr (Vedantam et al., 2015)</ref> encourage exact matching between semantics in generated captions and those in the reference. However, they over penalize visual details that are not annotated thus compromising their qualifications for measuring overall representation quality. Inspired by <ref type="bibr" target="#b1">Anderson et al. (2016)</ref>; <ref type="bibr" target="#b17">Krishna et al. (2017)</ref>, we propose to measure the accuracy on scene graphs extracted from generated text against human-annotated graphs, which, as suggested by <ref type="bibr" target="#b1">Anderson et al. (2016)</ref>, co-relates better with human judgment.</p><p>The contributions are twofold. First, we propose a general framework for semantic visual representation and showcase its application to image paragraph captioning. The framework is simple yet highly extendable, allowing new components to be plug-in and supporting other use scenarios that require a holistic view of the visual content. Second, we benchmark the effectiveness of the proposed model on its capacity for representing visual concepts (e.g., scene graphs) and set new state-of-the-art results.</p><p>Notations. We denote  <ref type="formula">2018</ref>) add a repeat penalty to the optimization, to prevent the appearance of repeated sequences. <ref type="bibr" target="#b20">Wang et al. (2019)</ref> use convolutional auto-encoder for topic modeling based on region-level image features. Along this line, many other works have been done <ref type="bibr" target="#b6">(Dai et al., 2017;</ref><ref type="bibr">Luo et al., 2019;</ref><ref type="bibr">Mao et al., 2018;</ref><ref type="bibr" target="#b23">Xu et al., 2020;</ref><ref type="bibr" target="#b10">Guo et al., 2021;</ref><ref type="bibr">Shi et al., 2021)</ref>. Most of the proposed models, however, are trained on Stanford image-paragraph dataset <ref type="bibr" target="#b16">(Krause et al., 2017)</ref>, which only contains 14 thousand of training paragraphs for its expensive nature to collect. Due to lack of data, the generated paragraphs usually lack coherence both locally and globally. Therefore, many of the above works aim to make the best use of data to improve the coherence. Yet nowadays, large language models can generate long coherent paragraphs by default. Our work, leveraging recent progress of large pretrained models, focuses more on how to guide and constrain the generated text instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constrained text generation</head><p>In recent years, rapid progress has been made in vision-language pretraining (VLP). CLIP <ref type="bibr">(Radford et al., 2021)</ref>, ALIGN <ref type="bibr" target="#b12">(Jia et al., 2021)</ref>, and Florence <ref type="bibr" target="#b26">(Yuan et al., 2021)</ref> are proposed to encode vision and language into a joint representation space for crossmodal alignment tasks, e.g., zero-shot image classification. Another line of research, e.g., SimVLM <ref type="bibr" target="#b22">(Wang et al., 2021</ref><ref type="bibr">), FLAVA (Singh et al., 2021)</ref>, BLIP <ref type="bibr" target="#b18">(Li et al., 2022)</ref>, CoCa <ref type="bibr" target="#b25">(Yu et al., 2022)</ref> and many others <ref type="bibr" target="#b5">(Cho et al., 2021;</ref><ref type="bibr" target="#b21">Wang et al., 2022;</ref><ref type="bibr" target="#b29">Zhu et al., 2021;</ref><ref type="bibr" target="#b0">Alayrac et al., 2022)</ref> adopt encoderdecoder models trained with generative losses. Those models are capable of performing image captioning in a zero-shot manner. A concurrent work, Socratic Models (SM, <ref type="bibr" target="#b27">Zeng et al. (2022)</ref>), also use textual data to bridge the domain gap between vision-language models and language models. The model, however, is stronger in retrieval tasks than captioning tasks as we will show later. There are also other works leveraging large language models to solve vision tasks, e.g., PICa <ref type="bibr" target="#b24">(Yang et al., 2021)</ref> uses <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> to extract commonsense knowledge for visual question answering tasks, MAGIC <ref type="bibr">(Su et al., 2022)</ref> uses a CLIP-induced score to regularize the language generation so that it is semantically related to the given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework</head><p>Given an image I, our goal is to generate long and coherent descriptive text based on image inputs, leveraging only the existing pretrained models. Our framework can be divided into three stages:</p><p>1. Represent I with visual clues S, which contain the rich visual information; 2. Feed the visual clues into a language model to generate K candidate paragraphs {T i } K i=1 ; 3. Select the best paragraph T * from the candidates</p><formula xml:id="formula_0">{T i } K i=1 .</formula><p>The overall framework is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We will then elaborate on each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Clue Extraction</head><p>We leverage three state-of-the-art models with the open-vocabulary capability to extract the visual information.</p><p>The first one is contrastively trained vision-language models, e.g., <ref type="bibr">CLIP (Radford et al., 2021)</ref>, Florence <ref type="bibr" target="#b26">(Yuan et al., 2021)</ref>. Such models are pretrained on image-text pairs {x i , y i }. Denote the image encoder as f v (?) and the text encoder as f t (?). Given a minibatch B, the models are optimized by contrastive loss</p><formula xml:id="formula_1">L = - 1 |B| xi,yi?B exp( f v (x i ), f t (y i ) /? ) yj ?B,j =i exp( f v (x i ), f t (y j ) /? ) + exp( f v (x i ), f t (y i ) /? ) xj ?B,j =i exp( f v (x j ), f t (y i ) /? )</formula><p>, where ? is the temperature. This loss explicitly uses inner product ?, ? to measure the similarity between the encoded image f v (x i ) and encoded text f t (y j ), and higher similarities are encouraged if the images and texts are paired. Therefore, such a pretrained model is capable of selecting the tags that describe the image I from a set of customized tags by computing the similarities. Given a set of tags {t i } N i=1 , we compute the similarities between the input image I and the tags, and adopt the tags with top-M similarities,</p><formula xml:id="formula_2">T = {t * j } M j=1 = arg top-M ti,i=1,??? ,N f v (I), f t (t i ) .<label>(1)</label></formula><p>The second model is a caption model c(?). We use it to generate an overall image description c(I).</p><p>The third one is an object detection model. We adopt a well-trained object detector, to provide us with the locations of the possible objects in the format of bounding boxes. The bounding boxes are processed with the non-maximum suppression technique to filter out repetitions. Denote the object proposals as {b j } R j=1 and image regions cropped from corresponding boxes as {p j } R j=1 . We first select the indices of the bounding boxes with objects that can be named by our customized tag set,</p><formula xml:id="formula_3">P = { k } Q k=1 = {j| f v (p j ), f t (t i ) &gt; ?, i = 1, ? ? ? , N, j = 1, ? ? ? , R}.<label>(2)</label></formula><p>Here, ? is a threshold certifying whether t i is aligned with p j . Given a set of customized attribute {a i } V i=1 , each selected proposal k from P is then assigned to an attribute</p><formula xml:id="formula_4">a * k = argmax ai,i=1,??? ,V f v (p k ), f t (a i ) ,<label>(3)</label></formula><p>and the corresponding tags</p><formula xml:id="formula_5">O k = {t i | f v (p k ), f t (t i ) &gt; ?, i = 1, ? ? ? , N }.<label>(4)</label></formula><p>In addition to the tags and attributes to the bounding boxes, we also use the caption model c(?) to provide some more descriptive texts {c(p k )} Q k=1 . In summary, we collect a tag set T and a caption c(I) as global descriptions to the image, and a quadruple</p><formula xml:id="formula_6">(b k , a * k , O k , c(p k ))</formula><p>as local descriptions for each selected bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Candidate Synthesis</head><p>We then format the collected visual information into the structured visual clues, which can be directly used as the prompt of the language model. Figure <ref type="figure" target="#fig_1">2</ref> shows an example of the visual clues. We observe that the information near the end of the prompt will have a more significant influence on the language model output. As the tags T are usually more informative and the local extractions are noisier, we input the visual clues with the order of local descriptions, caption, and tags.</p><p>To incorporate each local description, a naive way is to inject the coordinates of the bounding boxes directly into the prompt. However, we find the current language models still lack the capability to handle the inference task with numbers, especially in a zero-shot manner. Therefore, we reformat the bounding boxes b k into plain language by describing its location and size. Specifically, we adopt rule-based method to divide the locations into 9 classes {"upper left", "upper middle", "upper right", "left", "middle", "right", "lower left", "lower middle", "lower right"}, and divide the sizes into 3 classes {"large", "moderate-sized", "small"}, and incorporate these descriptions into the prompt.</p><p>The other visual clues are inputted straightforwardly in the format as showed Figure <ref type="figure" target="#fig_1">2</ref>. The prompt is then fed into a large-scale language model to synthesize K candidate paragraphs {T i } K i=1 full of descriptive details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Candidate Selection</head><p>Finally, we use the vision-language model again, to select the candidate that aligns best with the image,</p><formula xml:id="formula_7">S = argmax Ti,i=1,??? ,K f v (I), f t (T i ) .<label>(5)</label></formula><p>To further rule out the unrelated concepts in S, we filter the output again in sentence level. Specifically, we split it into sentences (s 1 , s 2 , ? ? ? , s U ), and use a threshold ? to remove the sentences with lower similarities,</p><formula xml:id="formula_8">T * = (s i | f v (I), f t (s i ) &gt; ?, i = 1, ? ? ? , U ).<label>(6)</label></formula><p>In this way, we obtain the final output T * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatic Evaluation Metric: SPIPE</head><p>As indicated by Figure <ref type="figure" target="#fig_0">1</ref>, the generated paragraphs of images can be very flexible. This makes the n-gram based metrics, e.g., <ref type="bibr">BLEU (Papineni et al., 2002)</ref>, <ref type="bibr">ROUGE (Lin, 2004)</ref>, <ref type="bibr">CIDEr (Vedantam et al., 2015)</ref>, METEOR <ref type="bibr" target="#b8">(Denkowski and Lavie, 2014)</ref>, unsuitable for evaluating the generated text. Instead, we focus on the semantic propositional content. For example, given an image with content "A man sitting in front of a blue snowboard", a good evaluation metric for IPC should evaluate whether each of the semantic propositions is correct, namely, a). a man is sitting; b). a man is in front of a snowboard; c). the snowboard is blue, instead of the exact words used in the text. To do so, SPICE <ref type="bibr" target="#b1">(Anderson et al., 2016)</ref> extracts the scene graphs <ref type="bibr" target="#b14">(Johnson et al., 2015)</ref> from the generated texts and the reference texts, respectively, and computes an F-score between the graphs. SPICE targets image caption tasks, where there are usually multiple good references for each image, and the generation is less flexible. However, IPC tasks usually only have one reference <ref type="bibr" target="#b16">(Krause et al., 2017)</ref>, which is not enough to evaluate the flexible generation. Therefore, we propose to directly compare the scene graphs extracted from the generated text to human-annotated graphs. Figure <ref type="figure" target="#fig_2">3</ref> shows an example of the generated graph from text and the human-annotated graph for the image.</p><p>Specifically, a scene graph consists of the objects, the attributes of the objects, and the relationships between the objects. To parse the generated text into a scene graph, we use a two-stage approach following <ref type="bibr" target="#b1">Anderson et al. (2016)</ref>. First, we use the pretrained dependency parser <ref type="bibr" target="#b15">(Klein and Manning, 2003)</ref> to establish the synthetic dependency between the words. Then we map from the dependency trees to scene graphs using a rule-based system <ref type="bibr">(Schuster et al., 2015)</ref>. Given scene graphs extracted from the text and the human-annotated graphs <ref type="bibr" target="#b17">(Krishna et al., 2017)</ref>, our metric computes an F-score </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Analysis</head><p>The basic evaluation of the generated output should include three aspects:</p><p>1. Accuracy. Most of the contents appearing in the paragraph should be from the image; 2. Completeness. Most of the contents appearing in the image should be included in the paragraph;</p><p>3. Coherence. Paragraphs should be more than concatenating the sentences together.</p><p>We evaluate the accuracy and completeness of the generated descriptions using the proposed automatic evaluation metric SPIPE, and do human evaluation to quantify the coherence. We include more examples in Appendix A and 500 randomly sampled outputs in this file for readers to perform a qualitative study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Specification</head><p>Models. We adopt Florence-H <ref type="bibr" target="#b26">(Yuan et al., 2021)</ref> as the vision-language model, BLIP-large <ref type="bibr" target="#b18">(Li et al., 2022)</ref> finetuned on COCO captions dataset <ref type="bibr" target="#b4">(Chen et al., 2015)</ref> as the captioner with its default setting, and one-stage detector as a general object detector. To be more specific about the detector, we first omit the category information from COCO <ref type="bibr" target="#b4">(Chen et al., 2015)</ref> datatset and train Dynamic Head <ref type="bibr" target="#b7">(Dai et al., 2021)</ref> on the bounding boxes only to formulate a class-agnostic object detector. We then use non-maximum suppression (NMS) to select the top 100 object proposals.</p><p>We use GPT-3 <ref type="bibr" target="#b2">(Brown et al., 2020)</ref> Davinci-text-001 model as the language model. To enable more difference in the generated candidates, we adopt temperature as 0.8. We adopt the frequency penalty as 0.5 and the maximum number of tokens as 100.</p><p>Customized sets. To construct a general domain tag set, we collect the most frequently searched 400 thousand queries in Bing Search as the tags {t i } N i=1 . We adopt the attribute set of the Visual Genome dataset <ref type="bibr" target="#b17">(Krishna et al., 2017)</ref> as the attribute set {a i } V i=1 . Parameters. We adopt number of tags M = 5, thresholds ? = ? = 0.2, and number of candidates K = 40. Among K = 40 candidates, half of them are generated without caption information while the remaining half are with them. This is because we notice the caption model sometimes cannot output good captions due to too small bounding boxes. We also remove the bounding boxes that are smaller than 1/400 of the image sizes.</p><p>BEST: This image features a group of people standing under a tree with pink blossoms. The people are all dressed in various types of clothing, and some are carrying bags or backpacks. The tree is a sakura tree, and the blossoms are in full bloom. There is a bench in the background, and the ground is covered in fallen petals. SM: People gather under a blossoming cherry tree, enjoying the beauty of natural together. BLIP: A group of people standing around a blue table.</p><p>BEST with only tags: The image is of people gathered in a park, looking at cherry blossoms. The parks are full of people enjoying the festival and the beautiful view. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Automatic Evaluation</head><p>In this section, we use SPIPE to benchmark the accuracy and completeness of our framework. We evaluate our framework on the test set of Stanford dataset <ref type="bibr" target="#b16">(Krause et al., 2017)</ref>. The dataset is a subset of Visual Genome (VG) dataset 2 <ref type="bibr" target="#b17">(Krishna et al., 2017)</ref>, and therefore we can obtain the human-annotated scene graphs from VG as well. We compare the following frameworks.</p><p>BLIP <ref type="bibr" target="#b18">(Li et al., 2022)</ref>. This is the BLIP-large model finetuned on COCO captions dataset.</p><p>Socratic model <ref type="bibr" target="#b27">(Zeng et al., 2022)</ref>. We adopt the image captioning code 3 without alternation.</p><p>BEST-general domain. This is our framework with the customized set listed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BEST-VG domain.</head><p>With open-vocabulary capability, our framework can adapt to a specific domain.</p><p>Here, we replace the customized tag set {t i } N i=1 for the local objects as the object set of VG datasets. The results are shown in Table <ref type="table" target="#tab_1">1</ref>. Our general domain framework significantly outperforms the BLIP model and the Socratic model. With the domain specified to VG, the performance is further boosted. Figure <ref type="figure" target="#fig_3">4</ref> shows an example with a image cropped from the Socratic model paper <ref type="bibr" target="#b27">(Zeng et al., 2022)</ref> directly. We find that caption generation does not require the complex prompt used in Socratic model. Our framework with only tagging information T can generate texts with a similar degree of detail.</p><p>To evaluate the representation capability of our visual clues, we also compare it to a naive scene graph generation method. We use the vision-language model to assign objects, attributes, and relationships between the objects, using the object set, attribute set, and relationship set of VG. And then we compare the generated scene graph to the human-annotated graph. The F-score is 0.3, with precision 0.8 and recall 0.2. We discuss more on why this does not work in Appendix D.</p><p>We also build an oracle model to see the limit of our framework. The oracle model in Table <ref type="table" target="#tab_1">1</ref> uses the ground truth objects with ground truth attributes to replace the corresponding concepts in the visual clues of our framework. It significantly outperforms the human annotation, either from Stanford dataset or from VG. This reveals the large potential of BEST with the development of object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation</head><p>We perform an ablation study to see how each of the components contributes to the final performance. Especially, we consider replacing the open-vocabulary object detector with YOLO v5 <ref type="bibr" target="#b13">(Jocher, 2020)</ref>, which is a closed-set detector trained with COCO classes. Table <ref type="table" target="#tab_2">2</ref> shows the results. The performance of the YOLO v5 alternation is competitive compared to our general domain version. The precision is 1 Tuples are considered to be matched if their lemmatized word forms are equal or if they are found in the same WordNet <ref type="bibr">(Miller, 1995)</ref> synset. 2 We remark that the VG caption data is included in the training data of BLIP model. This is hardly a data leakage issue since the VG caption data is of a different style compared to the BLIP outputs. The VG captions are more like semantic propositions rather than captions.</p><p>3 https://github.com/google-research/google-research/tree/master/socraticmodels 15.9 6.7</p><p>higher than ours, which may be a consequence that YOLO models tend to recognize fewer objects <ref type="bibr" target="#b30">(Zou et al., 2019)</ref>. However, it is still inferior to our VG domain model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Human Evaluation</head><p>To further evaluate our framework, we perform human evaluation. We first compare BEST to human annotation. Specifically, we randomly sample 200 descriptions from the test set of the two sources.</p><p>For each assignment, we present one image and two corresponding descriptions, and ask human evaluators to evaluate on accuracy, completeness, coherence, and ask an additional question "which of the descriptions is written by human" for the humanlikeness aspect. They will choose one answer from {Description 1, Description 2, Cannot determine}. For each assignment, we hire 5 workers using the Amazon Mechanical Turk platform.</p><p>As the difference between the long texts can be subtle, we perform two statistical tests to see whether the difference is statistically significant. First, we adopt the voted results of the 5 workers, and perform a binomial test, treating the 200 outcomes as i.i.d samples. The null hypothesis here is</p><p>Given an image, the probability that human annotation is better than BEST output is 0.5.</p><p>To further utilize the detailed outcomes for each image, we perform a Mann-Whitney U test with the null hypothesis as</p><p>The number of people that think (the human annotation is better than the BEST output) is similar to the number of people that think (the BEST output is better than the human annotation).</p><p>Table <ref type="table" target="#tab_3">3</ref> shows the results. There is no significant evidence (p value ? 0.5) showing human annotation is better than BEST in terms of completeness and humanlikeness. However, BEST still falls behind in terms of accuracy and coherence. The failure cases are usually because the BEST outputs might contain small mistakes that cannot be easily filtered out, mostly from the hallucination of the language model. We show more examples in Appendix C.</p><p>We then compare BEST to BLIP and the Socratic model using similar hypothesis tests. The results show BEST are significantly better than BLIP and Socratic models under most of the metrics (p value &lt; 0.05). Note that here accuracy is defined slightly different than the precision used in Table <ref type="table" target="#tab_1">1</ref>: In human evaluation, providing background information about concepts in the image is not viewed as inaccurate, while in Table <ref type="table" target="#tab_1">1</ref> it will hurt the precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Variants and Real-world Applications</head><p>The proposed framework opens up many creative real-world applications. For example, people with vision deficiencies may not be able to view images easily. BEST can help convert it into precise and comprehensive descriptions for general domain images.</p><p>Another example is the closed-loop training of the large models. The large-scale vision-language model and language model used in BEST are trained on tremendous amounts of data, and thus can memorize knowledge beyond human capability. We can use it to automatically annotate data, which is easy to scale up. Furthermore, it can incorporate commonsense knowledge into the text naturally. For example, in the second example of Figure <ref type="figure" target="#fig_0">1</ref>, the text contains "This image captures the everyday life of Cubans, with their traditional horse-drawn carts still in use." This is because our tags contain "Cuba" and "buggy", and the language model knows traditional horse-drawn carts are still in use in Cuba. We finetune a BLIP-large model on our BEST generated data. The training set is similar to Stanford dataset <ref type="bibr" target="#b16">(Krause et al., 2017)</ref>. After finetuning, the F-score improves more than 50%.</p><p>With small modifications, the proposed framework enables us to free human labor for even more applications. To list a few examples, Visual storytelling. As shown in Figure <ref type="figure" target="#fig_4">5</ref> (a), the framework can generate charming stories based on the input image. To do so, we simply change the end of the prompt to be "Tell me a creative story:".</p><p>Automatic ads generation: As shown in Figure <ref type="figure" target="#fig_4">5</ref> (b), with the framework, the merchants only need to upload an image, and make small modifications to the generated advertisement as wanted. As there is usually one product in an image, we adopt the number of input tags M = 1. We also change the end of the prompt to be "Write a product description to sell in eBay or Amazon marketplace to get lots of engagement:".</p><p>Social media post. As shown in Figure <ref type="figure" target="#fig_4">5</ref> (c), the framework can be a social media bot, which may alleviate the workload of internet celebrities. We change the end to be "Social media post:".</p><p>Background generation. As shown in Figure <ref type="figure" target="#fig_4">5</ref> (d), the framework can also be used to provide some background knowledge. To do so, we change the end of the prompt to be "Textbook text:".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations and Further Improvements</head><p>Prompt tuning. As suggested in <ref type="bibr" target="#b2">Brown et al. (2020)</ref>, language models can infer better when they are shown examples in the prompt. In our experiments, however, this results in model directly copying sentence pieces from example paragraphs, introducing unnecessary noise. We suspect this prompt (a)</p><p>As the Hogwarts Express made its way through the Scottish countryside, a young wizard named Harry peered out the window at the stunning Glenfinnan Viaduct. He had always dreamed of traveling on the venerable steam train, and now he was finally doing it! The locomotive chugged along, its great iron wheels clanking over the rails, as passengers in every car gazed in wonder at the beautiful scenery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>Looking for a stylish and practical coat to keep you warm this winter? Look no further than our long parka! Made from high quality beige fabric, this coat is sure to keep you warm and comfortable all winter long. It features a stylish and flattering fit, as well as a warm and cozy hood, making it perfect for cold days out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c)</head><p>What do you think of this Chevy Impala Lowrider? I spotted it at the Sema Car Show and it's definitely a head-turner! (d)</p><p>A tench is a fish that lives in fresh water. It is a common fish that can be found in many lakes and rivers. The tench is a large fish, and can grow to be quite large. It has a greenish-brown body, and is covered in scales.</p><p>The tench feeds on insects, plants, and small fish. tuning approach may work better if the input examples are similar to the generated one. This may be a promising direction as we can better control writing style.</p><p>Visual clues as a scene graph. Our visual clue extraction process is motivated by the fact that an image can be comprehensively represented by a scene graph <ref type="bibr" target="#b14">(Johnson et al., 2015)</ref>. As mentioned in Section 4, a scene graph contains objects, attributes of objects, and the relationship between objects.</p><p>In BEST, however, we do not include the relationships, as we observe in our initial study that the current vision-language models, although powerful, are not good at inferring relationships (echoing findings from Thrush et al. ( <ref type="formula">2022</ref>)). Yet, relationships among the objects are important components of an image. This can be plugged into our framework if better vision-language models are developed.</p><p>A well-trained filter model. We find that the current filtering strategy (6) is not immune to certain types of mistakes. As also mentioned in Thrush et al. ( <ref type="formula">2022</ref>), the vision-language model cannot accurately associate attributes to their corresponding objects. For example, in the second image of Figure <ref type="figure" target="#fig_0">1</ref>, if there is a sentence "The man wears a green shirt.", it will lead to a high image-text relevance score, since there is a man, a shirt, and green bush in the image. To handle this issue, we crop the image into local regions and pair each region with an attribute in the clue extraction stage. Still, if it is the language model who hallucinates new attributes and the attributes happen to be in the image, these captions cannot be filtered out. We suspect an adversarially trained filter is needed to perform the task.</p><p>Offensive language, social biases, and stereotypes. BEST inherits the risks of large vision and language models. It can potentially output offensive language and propagate social biases and stereotypes. For real applications, we can use rule-based methods or train a specific filter to filter out the offensive text. This is an area that we plan to explore to gain more insights further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Other Automatic Evaluation Metrics</head><p>In Table <ref type="table" target="#tab_5">5</ref> we compare different zero-shot methods using the metrics inherit from image captioning tasks. We can see that our framework BEST still performs the best. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Negative Examples</head><p>Figure <ref type="figure" target="#fig_5">10</ref> shows a few failure cases of BEST generation, which may highlight the direction for further improvement. The first example demonstrates two issues. first one is the vision-language model cannot accurately match attributes to the objects, as we discussed in Section 7. The second issue is the language model sometimes repeat the input sentences. This issue will be alleviated if we adopt better language models, e.g., replacing Davinci-text-001 by its upgraded version Davinci-text-001.</p><p>The second example suggests the Optical Character Recognition (OCR) capability of the visionlanguage model is not well-trained. So if for datasets with images involving text photos, we should explicitly plug in an OCR model.</p><p>The third example shows the current VL model cannot differentiate the relationship between the objects. This may be because the training loss, e.g., contrastive loss, does not require the model to differentiate the complex relationships -only recognizing the objects is enough.</p><p>In this image, we see a young boy riding a bicycle with a backpack on. To the right, we see another boy skateboarding. In the background, we see people walking and running. This image is about streetsurfing, street surfing, street sesh, skatejoring, and longboarding.</p><p>This image captures a homeless man taking a much-needed nap in the middle of the day. The bright red fire hydrant is a stark contrast to his muted red clothing, and stands out in the image.</p><p>The man is lying on his back, with his arms and legs sprawled out, and appears to be asleep. He is surrounded by empty bottles and cans, as well as a few pieces of trash. There is an inscription on the wall behind him that reads "Siesta".</p><p>This image features Andy Roddick serving during the US Open Tennis Championships in 2009. Roddick is a former world number one tennis player and is known for his powerful serve. In the image, he is shown hitting a forehand shot. The ball is seen speeding towards the opponent's court, and the racquet appears to be at the top of its arc. The umpire can be seen in the background, and the top of the racquet is visible in the foreground. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of generated image paragraph.</figDesc><graphic url="image-2.png" coords="1,130.29,512.01,81.88,54.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework demonstration. The orange open vocab tagger box corresponds to the image encoder f v (?) and the text encoder as f t (?). The blue captioner box is the caption model c(?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of the human-annotated graph and the text extracted graph.</figDesc><graphic url="image-6.png" coords="5,140.29,72.00,331.42,208.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example cropped from Zeng et al. (2022) paper, with other outputs for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples for real-world applications with variants of the proposed framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Negative examples of generated image paragraph.</figDesc><graphic url="image-26.png" coords="18,141.61,611.98,76.37,57.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>?, ? as inner product between two vectors, |A| as the cardinality of set A.</figDesc><table><row><cell>2 Related Works</cell></row><row><cell>Image paragraph generation. The task of generating image paragraphs is first introduced by</cell></row><row><cell>Krause et al. (2017). Conditioned on the visual features, they first train a sentence recurrent neural</cell></row><row><cell>network (RNN) to output sentence topics, and then feed each of the topics into another RNN to</cell></row><row><cell>generate the paragraphs. Liang et al. (2017) further improve the hierarchical RNN framework by</cell></row><row><cell>introducing an adversarial discriminator for smoother transitions between sentences. Chatterjee</cell></row><row><cell>and Schwing (2018) also address cross-sentence topic consistency by a global coherence vector.</cell></row><row><cell>Melas-Kyriazi et al. (</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison between different methods using SPIPE metric on the test set of the Stanford dataset<ref type="bibr" target="#b16">(Krause et al., 2017)</ref>.<ref type="bibr" target="#b8">Denkowski and Lavie (2014)</ref> between the two graphs among the conjunction of three sets of concepts: (object), (object, attribute), and (object, relationship, subject). Paying homage to<ref type="bibr" target="#b1">Anderson et al. (2016)</ref>, we name our approach SPIPE, Semantic Propositional Image Paragraph Evaluation.</figDesc><table><row><cell></cell><cell>Name</cell><cell cols="3">F-score Precision Recall</cell></row><row><cell></cell><cell>BLIP-large</cell><cell>7.6</cell><cell>38.0</cell><cell>4.4</cell></row><row><cell>Models</cell><cell>Socratic model BEST-general domain</cell><cell>3.2 8.8</cell><cell>13.9 15.3</cell><cell>1.9 6.6</cell></row><row><cell></cell><cell>BEST-VG domain</cell><cell>10.0</cell><cell>17.5</cell><cell>7.6</cell></row><row><cell>Oracle</cell><cell cols="2">BEST with human extracted visual clues 22.9</cell><cell>32.8</cell><cell>19.0</cell></row><row><cell>Annotation</cell><cell>Stanford dataset Concatenation of VG captions</cell><cell>17.3 18.9</cell><cell>27.7 40.0</cell><cell>14.0 14.1</cell></row><row><cell cols="2">based on the synonym match 1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation on each components.</figDesc><table><row><cell>Name</cell><cell cols="3">F-score Precision Recall</cell></row><row><cell>BEST-VG domain</cell><cell>10.0</cell><cell>17.5</cell><cell>7.6</cell></row><row><cell>Extraction with YOLOv5</cell><cell>9.0</cell><cell>19.0</cell><cell>6.3</cell></row><row><cell>Remove local information</cell><cell>8.0</cell><cell>14.4</cell><cell>6.0</cell></row><row><cell>Remove caption model c(?)</cell><cell>8.7</cell><cell>15.0</cell><cell>6.6</cell></row><row><cell>Input tags T only</cell><cell>5.9</cell><cell>10.7</cell><cell>4.4</cell></row><row><cell cols="2">Smaller language model (curie) 8.9</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation. p-value 1 is with the binomial test, and p-value 2 is with Mann-Whitney U test. The blue regions in the voted proportion section represent the proportion that the descriptions from the first source are better than the second, while the orange ones represent the second are better than the first. The 1.2% and 0.6% in the middle of row 4 and 12 represent "Cannot determine".</figDesc><table><row><cell>Sources</cell><cell>Criteria</cell><cell cols="2">Voted proportion</cell><cell></cell><cell>p-value 1</cell><cell>p-value 2</cell></row><row><cell></cell><cell>Accuracy</cell><cell>61.0%</cell><cell></cell><cell>39.0%</cell><cell>2 ? 10 -3</cell><cell>9 ? 10 -6</cell></row><row><cell>Anno. / BEST</cell><cell>Completeness</cell><cell>51.5%</cell><cell></cell><cell>48.5%</cell><cell>0.38</cell><cell>0.50</cell></row><row><cell></cell><cell>Coherence</cell><cell>59.7%</cell><cell></cell><cell>40.3%</cell><cell>5 ? 10 -3</cell><cell>8 ? 10 -4</cell></row><row><cell></cell><cell>Humanlikeness</cell><cell>54.7%</cell><cell cols="2">1.2% 44.2%</cell><cell>0.10</cell><cell>0.50</cell></row><row><cell></cell><cell>Accuracy</cell><cell>57.2%</cell><cell></cell><cell>42.8%</cell><cell>0.03</cell><cell>3 ? 10 -3</cell></row><row><cell>BEST / BLIP</cell><cell>Completeness</cell><cell>73.6%</cell><cell></cell><cell>26.4%</cell><cell cols="2">2 ? 10 -11 2 ? 10 -28</cell></row><row><cell></cell><cell>Coherence</cell><cell>56.2%</cell><cell></cell><cell>43.8%</cell><cell>0.05</cell><cell>5 ? 10 -3</cell></row><row><cell></cell><cell>Humanlikeness</cell><cell>58.3%</cell><cell></cell><cell>41.7%</cell><cell>9 ? 10 -3</cell><cell>5 ? 10 -4</cell></row><row><cell></cell><cell>Accuracy</cell><cell>59.4%</cell><cell></cell><cell>40.6%</cell><cell>6 ? 10 -3</cell><cell>7 ? 10 -5</cell></row><row><cell>BEST / Socratic</cell><cell>Completeness</cell><cell>71.3%</cell><cell></cell><cell>28.7%</cell><cell cols="2">2 ? 10 -9 1 ? 10 -24</cell></row><row><cell></cell><cell>Coherence</cell><cell>68.7%</cell><cell></cell><cell>31.3%</cell><cell cols="2">3 ? 10 -7 2 ? 10 -12</cell></row><row><cell></cell><cell>Humanlikeness</cell><cell cols="3">53.4% 0.6% 46.0%</cell><cell>0.18</cell><cell>0.35</cell></row><row><cell></cell><cell cols="5">Table 4: Finetune BLIP-large on different data.</cell></row><row><cell>Name</cell><cell></cell><cell></cell><cell cols="3">F-score Precision Recall</cell></row><row><cell cols="2">No finetune</cell><cell></cell><cell>7.6</cell><cell>38.0</cell><cell>4.4</cell></row><row><cell cols="4">With Socratic model generated data 3.9</cell><cell>17.5</cell><cell>2.3</cell></row><row><cell cols="3">With BEST generated data</cell><cell>11.6</cell><cell>23.1</cell><cell>8.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>LIANG, X., HU, Z., ZHANG, H.,GAN, C. and XING, E. P. (2017). Recurrent topic-transition gan for visual paragraph generation. In Proceedings of the IEEE international conference on computer vision.LIN, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.THRUSH, T., JIANG, R., BARTOLO, M., SINGH, A., WILLIAMS, A.,KIELA, D. and ROSS, C.  (2022). Winoground: Probing vision and language models for visio-linguistic compositionality. In CVPR.</figDesc><table><row><cell>LIU, B., FU, J., KATO, M. P. and YOSHIKAWA, M. (2018). Beyond narrative description: Generating</cell></row><row><cell>poetry from images by multi-adversarial training. In Proceedings of the 26th ACM international</cell></row><row><cell>conference on Multimedia.</cell></row><row><cell>LUO, Y., HUANG, Z., ZHANG, Z., WANG, Z., LI, J. and YANG, Y. (2019). Curiosity-driven</cell></row><row><cell>reinforcement learning for diverse visual paragraph generation. In Proceedings of the 27th ACM</cell></row><row><cell>International Conference on Multimedia.</cell></row><row><cell>MAO, Y., ZHOU, C., WANG, X. and LI, R. (2018). Show and tell more: Topic-oriented multi-</cell></row><row><cell>sentence image captioning. In IJCAI.</cell></row><row><cell>MARR, D. (2010). Vision: A computational investigation into the human representation and</cell></row><row><cell>processing of visual information. MIT press.</cell></row><row><cell>MELAS-KYRIAZI, L., RUSH, A. M. and HAN, G. (2018). Training for diversity in image paragraph</cell></row><row><cell>captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language</cell></row><row><cell>Processing.</cell></row><row><cell>MILLER, G. A. (1995). Wordnet: a lexical database for english. Communications of the ACM, 38</cell></row><row><cell>39-41.</cell></row><row><cell>SHI, Y., LIU, Y., FENG, F., LI, R., MA, Z. and WANG, X. (2021). S2td: A tree-structured decoder</cell></row><row><cell>for image paragraph captioning. In ACM Multimedia Asia. 1-7.</cell></row><row><cell>SINGH, A., HU, R., GOSWAMI, V., COUAIRON, G., GALUBA, W., ROHRBACH, M. and KIELA,</cell></row><row><cell>D. (2021). Flava: A foundational language and vision alignment model. arXiv preprint</cell></row><row><cell>arXiv:2112.04482.</cell></row><row><cell>SU, Y., LAN, T., LIU, Y., LIU, F., YOGATAMA, D., WANG, Y., KONG, L. and COLLIER, N.</cell></row><row><cell>(2022). Language models can see: Plugging visual controls in text generation. arXiv preprint</cell></row><row><cell>arXiv:2205.02655.</cell></row></table><note><p><p><p>PAPINENI, K., ROUKOS, S.,</p>WARD, T. and ZHU, W.-J. (2002)</p>. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. RADFORD, A., KIM, J. W., HALLACY, C., RAMESH, A., GOH, G., AGARWAL, S., SASTRY, G., ASKELL, A., MISHKIN, P., CLARK, J. ET AL. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. PMLR. RAJPURKAR, P., ZHANG, J., LOPYREV, K. and LIANG, P. (2016). Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. SALVADOR, A., DROZDZAL, M., GIR?-I NIETO, X. and ROMERO, A. (2019). Inverse cooking: Recipe generation from food images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. SCHUSTER, S., KRISHNA, R., CHANG, A., FEI-FEI, L. and MANNING, C. D. (2015). Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In Proceedings of the fourth workshop on vision and language. VEDANTAM, R., LAWRENCE ZITNICK, C. and PARIKH, D. (2015). Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison between different methods using captioning metrics on the test set of the Stanford dataset<ref type="bibr" target="#b16">(Krause et al., 2017)</ref>.</figDesc><table><row><cell>Name</cell><cell cols="7">Bleu-1 Bleu-2 Bleu-3 Bleu-4 METEOR ROUGE-L CIDEr</cell></row><row><cell>BLIP-large</cell><cell>0.4</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>5.4</cell><cell>15.1</cell><cell>0.6</cell></row><row><cell>Socratic model</cell><cell>0.6</cell><cell>0.2</cell><cell>0.1</cell><cell>0.0</cell><cell>3.0</cell><cell>9.3</cell><cell>0.5</cell></row><row><cell cols="2">BEST-general domain 30.9</cell><cell>15.9</cell><cell>8.1</cell><cell>4.2</cell><cell>11.9</cell><cell>23.7</cell><cell>11.3</cell></row><row><cell>BEST-VG domain</cell><cell>31.8</cell><cell>16.5</cell><cell>8.5</cell><cell>4.4</cell><cell>12.5</cell><cell>24.3</cell><cell>11.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A More Examples of Generated Paragraphs BEST: This image features a doughnut and a cup of coffee on a table. The doughnut is chocolate and glazed and the coffee is black. There is also a daypack visible in the image, as well as some straps that may be used to hold the backpack in place. SM: Home office sink with book and coffee. BLIP: A doughnut and a cup of coffee on a table. BEST with only tags: This image is of a coffee and donut in a daypack. The donut is labelled the "donut of shame" as it is a bit battered and not as fresh looking as the coffee. Human Annotation: A table has some items on it. The table is made of wood. There is a donut on a plate. The donut is made of chocolate. The plate is white. The plate is round. There is a cup of coffee on the table. The cup is white with a blue rim. The is a backpack by the cup and plate. The backpack is brown and white. There is a book on the backpack. The book is a paperback. The table is by a wall. The table has a chair by it.</p><p>BEST: This image shows a train traveling over a bridge over a street in Las Vegas. The train is a monorail, and is traveling next to tall buildings and the Stratosphere casino. There is a blinking red light on the train, and a blinking green light on the bridge. SM: A photo of an indoor parking garage. BLIP: A train traveling over a bridge over a street. BEST with only tags: This image is of the Las Vegas Monorail. It is a highspeed, driverless railway system that runs on a single beam. The Monorail connects the Las Vegas Strip to the Las Vegas Convention Center and other major hotels, including the Stratosphere, MGM Grand, and Sahara. Human Annotation: There are two skyscrapers. There is also a train going around a track above the road. The traffic lights are lit to red and green. There are palm trees to the left.</p><p>BEST: This image captures a surfer performing a cutback maneuver at the Superbank surf competition. The surfer is positioned in the middle of the frame and is relatively small in comparison to the surrounding waves. The waves are large and crashing, providing an impressive backdrop for the surfing action. SM: A person is surfing on a wave at a medina. BLIP: A man riding a wave on top of a surfboard. BEST with only tags: This image captures a surfer cutting back in a competition at the Superbank. The surfer is Bruna Surfistinha and the competition is the Chris Ward Surf Invitational. The waves are huge and the surfer is doing a great job of carving through them. Human Annotation: A man is in a black wetsuit. The man is riding a white surfboard. There is a large wave in the water the man is surfing on. BEST: This image shows a kitchen with white appliances and wooden cabinets. The stove is in the middle of the image and the refrigerator is on the right. There is also a sink on the left. SM: A photo of a kitchen, with a stove, kitchen range, and range hood. BLIP: A kitchen with white appliances and wooden cabinets. BEST with only tags: This image is of a kitchen that has been remodeled. It is a galley style kitchen with an oak cabinet. There is a sink, stove, and refrigerator in the kitchen. The countertops are granite and the flooring is wood. Human Annotation: A kitchen that has a brown wooden floor,wooden brown cabinets with a white stove and white refrigerator. the sink is empty and clean along with the counter which is white in color with a window above the sink which has brown blinds.</p><p>BEST: This image is of a group of people who appear to be waiting to enter a running room. The people are standing on a sidewalk and most are wearing athletic clothing. In the background, there is a yellow school bus parked on the side of the road. SM: Runners in a fastfood restaurant. BLIP: A group of people standing outside of a running room. BEST with only tags: This image is of a marathon coach and two runners. The marathon coach is giving instructions to the runners and they are getting ready to run. There is a banner in the background that says "Run Around". Human Annotation: A store named the Running Room is to the left of the road. A yellow school bus is parked next to the store. There are people in running clothes lined up the side of the building. Large windows surround the first level of the store. Some bikes are leaned up against the front of the building. There are clouds in the sky and electrical wires are on poles above the road.</p><p>BEST: This image is a red fire hydrant that is sitting on the side of the road. The fire hydrant has a black hat on top of it and it is made out of metal. The hydrant is red and the hat is black. The hat might be used to keep the hydrant from freezing in the winter. SM: The fire hydrant stands out against the snowy background. BLIP: A red fire hydrant sitting on the side of a road. BEST with only tags: This image shows a firefighter in front of a fire hydrant. The firefighter is wearing a fire hat and has a woodford hydrant. The freeze proof hydrant is behind the woodford hydrant. Human Annotation: A red fire hydrant with a white top and a black knit cap on top of it sits on a sidewalk at the side of a parking lot. The hydrant is casting a shadow on the sidewalk where leaves are scattered around it. There is a manhole cover a bit down from the hydrant. Behind the manhole cover is a small area with small green shrubs and mostly covered by leaves. At the end of the sidewalk are rows of cars with a store in the far back. A couple of people are near their cars in the parking lot. At the end of one row of cars is a wooden fence with a brick building behind it. BEST: This image shows a group of people gathered around luggage carts at an airport. There is a mix of different types of luggage, including suitcases, backpacks, and duffel bags. There are also a few people in the image who are not carrying any luggage. SM: People waiting in an airport terminal. BLIP: A group of people standing around carts with luggage. BEST with only tags: The image shows several people waiting in a line at an airport baggage claim. The airport is likely in China, as both Pudong and Shenzhen airports are located there. Beijing airport is also located in China. All three airports are major international hubs, with flights arriving from all over the world. Human Annotation: There are people standing in a room. There are signs on the walls behind them. There are three scooters that have luggage on top of them in front of the people.</p><p>BEST: This black and white photo shows an old train station that is now abandoned. The train itself looks quite old and is in the middle of the image, taking up most of the space. There are other objects in the image such as a funicular and cog railroad, but the old train is the main focus. SM: Abandoned railcar at a forgotten railway platform. BLIP: A black and white photo of an old train. BEST with only tags: This image features an abandoned train station. The station is located in the canfranc international railway station, and is part of the durango and silverton narrow gauge railroad. The funiculaire can also be seen in the background, as well as the cog railroad. Human Annotation: A worn wooden building is on the side of a set of train tracks. The building has many windows with screens over the windows. Many of the screens are torn or missing. The paint on the building is severely thinned and peeling. A small foliage bush is at the front of the building. Tall mountains are in the far distance. A large metal structure is suspended in the air above the building and the train tracks.</p><p>BEST: This image features a group of people, all of whom are standing on a tennis court. The people in the image are all dressed in appropriate tennis gear, and each is holding a racquet of some kind. In the background, there is a fence and beyond that, some trees. SM: A group of people enjoying a game of tennis. BLIP: A group of people standing on top of a tennis court. BEST with only tags: This image features the Bollettieri Tennis Academy, which is a well-known junior tennis training center. The academy is situated in Bradenton, Florida and has produced many top tennis players, such as Andre Agassi, Monica Seles, and Jim Courier. The academy has a kids' court that is visible in this image, as well as a grand slam tennis court in the background. Human Annotation: A group of kids and teenagers are posing for a picture on a tennis court. An adult instructor is standing at the far right of the kids. A net is behind them and the court ground is dark green with white painted lines. There are tennis balls on the ground in front of the kneeling kids. A chain link fence is behind the court and a group of trees is in the background. Everyone is wearing shorts, t-shirts and sneakers. BEST: This image is of a kitchen with white cabinets and a sink. The kitchen has a retro feel to it, with the white cabinets and old-fashioned sink. There is a small amount of light in the image, coming from what may be a window in the upper left corner of the photograph. The rest of the image is fairly dark, with only a small amount of light illuminating the countertop and floor. SM: A person in a kitchen, cooking food on a stove. BLIP: A kitchen with white cabinets and a sink. BEST with only tags: This image is of a retro kitchen in the night. It is a small, first kitchen with white cabinets and appliances. There is a light over the sink and a window with a curtain. Human Annotation: White cabinets are in a kitchen. There is a window above the sink with white curtains. There is a white microwave on the counter.</p><p>BEST: This is a black and white photo of a toilet. The toilet is white and it looks like it is sitting outside of a building next to a bush. SM: A photo of a toilet. BLIP: A toilet sitting outside of a building next to a bush. BEST with only tags: This image is of a toilet throne. It is a Universal Rundle toilet with a Portland Loo in the background. The American Standard toilet is to the right of the Portland Loo. The Toto Drake is to the right of the American Standard toilet. Human Annotation: A white toilet with no lid is sitting outside of a brick building on the concrete ground. The toilet is white and is placed against the brick wall of a building. It is in a corner. A tree is on one side of the toilet. A window is in the wall just above the toilet. There are cracks in the concrete on the ground. A door with a doormat is beside the tree.</p><p>BEST: This image captures the traffic on the Subic-Clark-Tarlac Expressway in the Klang Valley region of Malaysia. The expressway is a 4-lane, controlled-access highway that connects the two major metropolitan areas of the country. The image shows several cars and trucks travelling on the highway, as well as an ambulance and a police motorcycle. SM: A divided highway with many cars and people. BLIP: A busy city street filled with lots of traffic. BEST with only tags: This image is of the Tarlac-Pangasinan-La Union Expressway (TPLEX), a toll expressway in the Philippines. The expressway is seen here in the Klang Valley area of Selangor, Malaysia, and is crowded with cars and carreolas (two-wheeled carts). Human Annotation: This is a photo of a busy roadway. Many different vehicles can be seen on the road. A long narrow grass strip is separating the different sides of the roads. Tall lamp poles are lining the roadway. A large blue and white sign is standing on the left side of the roadway. Tall green trees and a long steep hill can be seen in the background of the photo. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Scene Graph Generation Baseline</head><p>As mentioned earlier, to evaluate the representation capability of our visual clues, we also compare it to a naive scene graph generation method. We use the vision-language model to assign objects, attributes, and relationships between the objects, using the object set, attribute set, and relationship set of VG. Algorithm 1 shows the complete scene graph generation algorithm. And we compare the generated scene graph to the human-annotated graph.</p><p>Algorithm 1 Algorithm for the Scene Graph Generation Baseline Select the bounding boxes in the same way as BEST Associate each of the bounding boxes with one object name and one attribute for each pair of bounding boxes do Compute the minimum bounding box covering the union of the two bounding boxes Assign a relationship to the minimum bounding box using the open-vocabulary tagger Choose from (object 1, relationship, object 2) and (object 2, relationship, object 1) with tagger Add the chosen relationship with its object and subject to the output end for</p><p>The F-score is 0.3, with precision 0.8 and recall 0.2. We show an example of the output graphs in Figure <ref type="figure">11</ref>. The particularly bad performance is majorly due to the sets from VG are noisy, so the synonym match between nodes yields very low score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Generation</head><p>Objects: plastic cups, woman eats pizza, spinach on pizza, enjoying a sandwich, eating contest, plastic cup, ribs, case is brown, griddle, foot pedal, parsley on plate Attributes: (object: plastic cups, attr: solo cup), (object: woman eats pizza, attr: eating italian), (object: spinach on pizza, attr: looking at pizza), (object: enjoying a sandwich, attr: getting her lunch), (object: plastic cup, attr: solo cup), (object: ribs, attr: bacon) ... Relationships: (object: woman eats pizza, subject: plastic cups, rel: eating pizza), (object: spinach on pizza, subject: plastic cups, rel: eating pizza), (object: plastic cups, subject: enjoying a sandwich, rel: eating pizza), (object: case is brown, subject: plastic cups, rel: cup on), (object: griddle, subject: plastic cups, rel: table has items)...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Annotation</head><p>Objects: "bracelet", "glasses", "face", "cup", "phone", "table", "pizza", "hand", "tray", "neon sign", "wall", "napkin holder", "picture", "paper plate", "women", "pizza", "cups", "paper napkins", "woman", "tanktop", "woman", "blue shirt", "napkin", "plate", "some food", "advertisement", "cell phone", "short hair", "white napkins", "toppings", "ale", "plate and napkin", "dark hair", "wrist" Attributes: (object: bracelet, attr: blue), (object: glasses, attr: black), (object: cup, attr: plastic), (object: pizza, attr: large), (object: cell phone, attr: black) ... Relationships: (object: face, subject: glasses, rel: on), (object: table, subject: phone, rel: on), (object: hand, subject: pizza, rel: in), (object: table, subject: tray, rel: on), (object: wall, subject: picture, rel: on)... </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakan-Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diverse and coherent paragraph generation from images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards diverse and natural image descriptions via a conditional gan</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic head: Unifying object detection heads with attentions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Searching for computer vision north stars</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Daedalus</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="85" to="99" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Matching visual features to hierarchical semantic topics for image paragraph captioning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04143</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual storytelling</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4154370</idno>
		<ptr target="https://github.com/ultralytics/yolov5.https://doi.org/10.5281/zenodo.4154370" />
		<title level="m">ultralytics/yolov5: v3.1 -Bug Fixes and Performance Improvements</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual meeting of the association for computational linguistics</title>
		<meeting>the 41st annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalan-Tidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03957</idno>
		<title level="m">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolutional auto-encoding of sentence topics for image paragraph generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00249</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03052</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<title level="m">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive key-value memoryaugmented attention for image paragraph captioning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An empirical study of gpt-3 for few-shot knowledge-based vqa</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05014</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Coca: Contrastive captioners are image-text foundation models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">Florence: A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00598</idno>
		<title level="m">Socratic models: Composing zero-shot multimodal reasoning with language</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unified visionlanguage pre-training for image captioning and vqa</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01522</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.05055" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Object detection in 20 years: A survey</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
