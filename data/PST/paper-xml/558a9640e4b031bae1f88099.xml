<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Differential Evolution Algorithms for Handling Noisy Optimization Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Swagatam</forename><surname>Das</surname></persName>
							<email>swagatamdas19@yahoo.co.in</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electronics &amp; Telecomm. Eng. Jadavpur University</orgName>
								<address>
									<postCode>700032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><surname>Konar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electronics &amp; Telecomm. Eng. Jadavpur University</orgName>
								<address>
									<postCode>700032</postCode>
									<settlement>Kolkata</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uday</forename><forename type="middle">K</forename><surname>Chakraborty</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Math &amp; Comp. Sc</orgName>
								<orgName type="institution">University of Missouri</orgName>
								<address>
									<postCode>63121</postCode>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Differential Evolution Algorithms for Handling Noisy Optimization Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6D0D9D21A26B8198549BDF7C3FBEDFC0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differential Evolution (DE) is a simple and efficient algorithm for function optimization over continuous spaces. It has reportedly outperformed many types of evolutionary algorithms and other search heuristics when tested over both benchmark and real-world problems. However, the performance of DE deteriorates severely if the fitness function is noisy and continuously changing. In this paper two improved DE algorithms have been proposed that can efficiently find the global optima of noisy functions. This is achieved firstly by weighing the difference vector by a random scale factor and secondly by employing two novel selection strategies as opposed to the conventional one used in the original versions of DE. An extensive performance comparison of the newly proposed scheme, the original DE (DE/Rand/1/Exp), the canonical PSO and the standard real-coded EA has been presented using well-known benchmarks corrupted by zero-mean Gaussian noise. It has been found that the proposed method outperforms the others in a statistically significant way.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The problem of optimizing noisy or imprecise (not exactly known) functions occurs in diverse domains of engineering application, especially in the task of experimental optimizations. In some applications the function to be optimized is only known within some (often unknown and low) precision. This might be due to the fact that evaluation of the function involves measuring some physical or chemical quantity or performing a finite element calculation in order to solve partial differential equations. The function values obtained are corrupted by noise, namely stochastic measurement errors and/or quantization errors. Although the underlying function may be smooth, the function values available may show a discontinuous behavior. Moreover, no gradient information may be available. A number of evolutionary computation methods for dealing with these noisy optimization problems have been proposed in the last few years in the fields of evolutionary programming (EP) <ref type="bibr" target="#b0">[1]</ref>, evolution strategies (ES) <ref type="bibr" target="#b1">[2]</ref>, genetic algorithms (GA) <ref type="bibr" target="#b2">[3]</ref> and particle swarm optimization (PSO) <ref type="bibr" target="#b3">[4]</ref>. Although DE is a simple and fast technique for numerical optimization, it has been shown in <ref type="bibr" target="#b4">[5]</ref> that the performance of DE becomes poorer in comparison to EA when the function to be optimized is corrupted by noise. Thus noisy fitness functions pose a serious problem for conventional DE algorithms. The present work is an attempt to contribute in this context. In this study an improved DE (DE/Randl/Exp) algorithm is presented where the scalar factor used to weigh the difference vector has been made completely random. This approach makes the DE more stochastic and equips it with better exploration capabilities in the continuously changing fitness landscape corrupted by noise. This scheme is referred to as DE-RSF (Differential Evolution with Random Scale Factor) <ref type="bibr" target="#b12">[13]</ref>. Additionally, a threshold based selection strategy (inspired by [6]) has been incorporated into the DE-RSF algorithm and named DE- RSF-TS (DE-RSF with Threshold-based Selection). Under this scheme an offspring vector replaces its parent in the next generation only if its (the child's) fitness is better than the parent's by a certain threshold value. This provides prevention from accepting poor candidate solutions which may deceptively appear fitter due to noise. Another stochastic selection scheme is proposed for DE-RSF and the resulting algorithm is referred to as DE- RSF-SS (DE-RSF with Stochastic Selection). An extensive performance comparison has been carried out among conventional DE, the new scheme, canonical PSO and EA. The rest of the paper is organized as follows. In section 2, the DE algorithm is briefly introduced. The new variants of DE are explained in section 3. Section 4 contains the details of the benchmark functions and simulation strategies. The results are presented in section 5 and discussed in section 6. Finally, we conclude the paper in section 7.</p><p>2 The Classical DE -A Brief Introduction DE <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref> searches for a global optimum point in an N-dimensional hyperspace. It begins with a randomly initialized population of N-dimensional real-valued parameter vectors. Each vector forms a candidate solution to the multi-dimensional optimization problem. Unlike the conventional GA, the reproduction scheme in DE is implemented as follows. For each individual vector GkD belonging to generation D, randomly sample three other 0-7803-9363-5/05/$20.00 Â©2005 IEEE. individuals GD, GD and GmD from the same generation (for distinct i, j, k and m), calculate the difference of the components (chromosomes) of G? and God, scale it by a scalar R (c [0,1]), and create a trial vector by adding the result to the chromosomes of Gk:</p><p>Gk,lnD+l = Gm nD + R. (Gi,nD j, D) if randn (0, 1) &lt; CR&gt; (1)   = Gkn,D otherwise J for the n-th component of each parameter vector. CR (c [0,1]) is the crossover constant. The trial solution is evaluated and is used to replaces its parent GkD deterministically if its fitness is better. This alteration and selection procedure is also known as the 'DE/rand/1/exp' operator. There exist other DE operators, such as 'DE/rand/l/bin' and'DE/best/2/bin', which we did not investigate in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Schemes</head><p>It is a well-known fact now that the DE needs relatively less parameter tuning and performs better than the PSO, GA and EA in a large number of static optimization problems. However, DE finds the situation difficult whenever there is noise in the scenario.</p><p>In the original DE (1) the difference vector (Gi-Gj) is scaled by a constant factor 'R'. Usually the most popular choice for this control parameter in DE is in the range of (0.4, 1). However, in DE-RSF we let this scale factor change in a random fashion in the range (0.5, 1), following the relation R = 0.5*(1+ rand (0, 1))</p><p>(2)</p><p>where rand (0,1) is a uniformly distributed random number within the range [0, 1]. The mean value of the scale factor remains at 0.75. This allows for stochastic variations in the amplification of the difference vector and thus helps retain population diversity as the search progresses. Even when the tips of most of the population vectors point to locations clustered near a local optimum, a new trial vector has fair chances to point at an even better location on the multimodal functional surface. Therefore the fitness of the best vector in a population has a low probability of getting stagnant until a truly global optimum is reached. Following the work done on (1+1)-EA in [6] we take up a threshold-based selection procedure for DE-RSF. Here the offspring vector substitutes its parent vector in the new generation if its fitness is less than the fitness of the parent (in case of minimization problems) by a threshold margin r. We keep the threshold margin proportional to the noise strength or variance (a,n2) i.e. r = k.a2 The new algorithm may be outlined as follows:</p><p>Procedure DE-RSF-TS Begin Initialize population; Evaluate fitness;</p><p>(3) While termination condition not satisfied For j = 0 to population-size do Create Difference-Offspring using random weighing factor as given in (2); Evaluate fitness; If an offspring is better than its parent at least by the threshold margin T Then replace the parent by offspring in the next generation; End If; End For; End While; End.</p><p>Selection (e.g., [12]) is crucial to the success of any evolutionary algorithm. Here we propose a new stochastic selection scheme for DE-RSF. Let J(xp) be the fitness value of the parent and let fAx0) be that of the offspring.</p><p>Then the offspring is selected for replacing the parent in the next generation with a probability</p><formula xml:id="formula_0">selection min(1,-) f(x0)</formula><p>for a minimization problem.</p><p>(</p><formula xml:id="formula_1">)<label>3</label></formula><p>It means that if f(xp) &gt; f(x0) then Pselection = 1 and the offspring is surely selected for replacing the parent in the next generation. However, if f(xp) &lt; f(xo) (i.e., the parent is better), then there is a non-zero probability that the poor offspring is selected. Hence the selection method is not deterministic, rather stochastically biased towards accepting the better solution.</p><p>4 Experimental Setup and Simulation Strategy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark Functions Tested</head><p>We have used the noisy versions of the following well- known benchmark functions[l 1]. All of these are minimization problems.</p><p>Sphere (SOD):</p><formula xml:id="formula_2">50 fRs(x)b= (ODi i=l Rosenbrock (50D) : with -100&lt;x, &lt;100 n f2 (X) = E [100 (Xill -Xi 2)2 + (Xi -1)2] i-1 with -50&lt;x &lt;.50 Rastrigin (50D): n f3(X) =[X2 -l0cos(2xi)+10]</formula><p>i= with -5.12 &lt; x, . 5.12 Griewank (50D): Beale (2D): </p><formula xml:id="formula_3">f6X)= 1 . -x,(1-X2 )] + [2.25 -xl (I _ X,2)]2+ [2.625 -x, (</formula><formula xml:id="formula_4">(X) E i.X, 4+ rand(0,1)</formula><p>with -1.28 &lt; x, &lt; 1.28</p><p>Generalized Penalized Function (30D) n-I 2 A1O (x) =-{10 sin 2 (gy ) + E(Yl -1) [1 +10 sin 2 (Z jl ) n + (yn 1)}+ u(x,l 0,1 00,4)</p><formula xml:id="formula_5">Yi = + I (+ xi) 41 and .u. a k t i). tti .x F. * N Ik(xr -(X) . 0, k*(-x-a)0tv, X.</formula><p>&gt; a'--a &lt; ., &lt; : a.</p><p>Az &lt;-a.</p><p>The noisy versions of the benchmark functions are defined as:</p><formula xml:id="formula_6">f,,o (x) =f() + N(O, 2)(4)</formula><p>with N(O, a2) = Normal (or Gaussian) distribution with mean 0 and variance a2. To generate N we use the Box and Muller method <ref type="bibr" target="#b8">[9]</ref> with various values of a2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods Compared and Algorithmic Settings</head><p>In this work we compare the performance of Particle Swarm Optimization (PSO), classical DE, the new scheme and Evolutionary Algorithm [10] on the noisy benchmarks listed above. Below we introduce very briefly two of the competitor algorithms (the classical DE has already been described in section 2).</p><p>PSO is a global search technique inspired by the swarming behavior of social insects, bird flocks, etc. In classical PSO [4] a population of particles is initialized with random positions Xi and velocities Vi, and a function, fJ is evaluated, using the particle's positional coordinates as input values. In an n-dimensional search space Xi = (xil, xi2, xi3 ...xin) and Vi = (vil, vi2, vi3 ...vin).</p><p>Positions and velocities are adjusted, and the function is evaluated with the new coordinates at each time-step. The fundamental velocity and position updating equations for the d-th dimension of the i-th particle in PSO may be given as Vid (t+l) (i.Vd (t) + Cl.-(PI (Pld xid (t)) + C2-(P2. (Pgd-X id(t))} ( <ref type="formula">5</ref>)</p><formula xml:id="formula_7">Xid (t+1) = Xid (t) + Vid (t+1)</formula><p>The variables (Pl and Y2 are random positive numbers, drawn from a uniform distribution and defined by an upper limit (Pmax which is a parameter of the system. Cl and C2 are called acceleration constants whereas X is called inertia weight. P1 is the local best solution found so far by an individual particle while Pg represents the coordinates of the fittest particle found so far in the entire community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evolutionary Algorithm (EA)</head><p>An evolutionary algorithm (EA) is a search method that takes inspiration from natural selection and the survival-of -the-fittest principle in the biological world. EAs differ from more traditional optimization techniques in that they involve a search from a "population" of solutions, not from a single point. Each iteration of an EA involves a competitive selection that weeds out poor solutions. The solutions with high fitness are "recombined" with other solutions by swaping parts of a solution with another. Solutions are also "mutated" by making a small change to a single element of the solution.</p><p>In this study we have used an EA with arithmetic crossover and Gaussian mutation as alteration operators and tournament selection of size 2 for selection of next generation solutions. In the arithmetic crossover part, a candidate solution i is either retained in the next generation or is replaced by the offspring solution obtained from i and another randomly chosen individual j with probability pc such that for the d-th dimension of the solution vector, Xod =-(Pd Xid + (l d )Xjd <ref type="bibr" target="#b5">(6)</ref> where (Pd is a uniformly distributed random number in [0,1]. The Gaussian mutation uses a fixed mutation rate and probability pm for each individual i such that Xid = Xid+ N(0,1)c.m (xmaxd Xmind ) <ref type="bibr" target="#b6">(7)</ref> In each generation an elite pool of size n containing n best solutions is determined and kept unaltered by the crossover or mutaion operators in the next generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Control Parameters for the Different Algorithms</head><p>In Table 1 we summerise the values of the control parameters used for different competitor algorithms. Table <ref type="table">1</ref>. Parameter settings for the algorithms where PS = Population Size, RSF = Random Scale Factor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Simulation Strategy</head><p>In this study each experiment has been repeated 50 times and the mean best fitness of these 50 independent runs has been reported. For comparing different algorithms the first thing we require is a fair metric. The number of iterations or generations cannot be used as a time measure as the algorithms perform different amounts of work in their inner loops. We have used the number of function evaluations as the metric of comparison. (The advantage of measuring performance by function evaluations is that there is a strong relationship between this measure and the processor time as the function complexity increases.) Numerical results in the form of the mean and the standard deviation of the best fitnesses of 50 independent runs are tabulated. Each run of each experiment was continued up to 105 function evaluations. (We have not used any re-sampling technique where the same candidate solution is evaluated m times and the true fitness is taken as the mean of these m readings.) 5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Simulation</head><p>We have experimented with increasing noise variance values from 0 to 1. In Tables 2(1-10) we present the mean and standard deviation of the best fitness values for the ten benchmark functions for noise standard deviations of 0, 0.2, 0.4, 0.6, 0.8 and 1.0. All the readings given are the averages over 50 independent runs per function per noise variance value. Missing values of standard deviation in these tables indicate a zero standard deviation. The best solution in each case has been shown in bold.  Next we compare the various algorithms on the basis of both robustness and convergence speed. Here the term 'robustness' indicates in how many runs an algorithm succeeded in reducing the function value below a specified threshhold using fewer than the maximum allocated number of function evaluations. Table <ref type="table" target="#tab_2">5</ref> shows, for all test functions and all algorithms, the number of runs (out of 50) that managed to find the optimum solution (within a given tolerance) and also the average number of function evaluations (in parentheses) needed to find that solution. The tolerance value is selected to be 0.01 for all functions except f5 where it is -176.3. The maximum number of function evaluations has been fixed at 105. As another comparison, in Figure <ref type="figure" target="#fig_3">2</ref> we graphically show how the mean best solution found by the algorithm varies with increasing noise variance. For space limitations we show this only for two of the most difficult functions, namely Ackley and Rastrigin function.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(i + l)x, + i]x I jcos[(j + 1)x2 + j] + (xl + 1.42513)2 i=l j=1 + (x1 + 0.80032)2 with -10 &lt;xi &lt; 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig.1 Progress towards the optimum value: performance comparison among different algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>Fig.2Plot of variation of optimum solution quality with increasing noise variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean and Standard Deviation of the Best Fitness after 105 Function Evaluations. (Mean of 50 runs for eachBelow we show graphically how the mean fitness value of the best vector in successive generations proceeds towards the global optimum of the search space.</figDesc><table><row><cell cols="4">different value of noise variance)</cell><cell></cell><cell></cell></row><row><cell cols="3">1) f1 (Sphere Function)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(yn</cell><cell>DE</cell><cell>PSO</cell><cell>EA</cell><cell>DE-RSF-</cell><cell>DE-RSF-SS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TS</cell><cell></cell></row><row><cell>0</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell></row><row><cell>0.2</cell><cell>0.000</cell><cell>0.00423</cell><cell>0.0675</cell><cell>0.000</cell><cell>0.000</cell></row><row><cell></cell><cell>(0.000)</cell><cell>(0.029)</cell><cell>(0.000)</cell><cell>(0.000)</cell><cell>(0.000)</cell></row><row><cell>0.4</cell><cell>0.0121</cell><cell>0.24</cell><cell>0.0231</cell><cell>0.001</cell><cell>0.0005</cell></row><row><cell></cell><cell>(0.008)</cell><cell>(0.066)</cell><cell>(0.0572)</cell><cell>(0.043)</cell><cell>(0.000)</cell></row><row><cell>0.6</cell><cell>0.069</cell><cell>0.2976</cell><cell>0.0498</cell><cell>0.004</cell><cell>0.043</cell></row><row><cell></cell><cell>(0.326)</cell><cell>(0.0973)</cell><cell>(0.087)</cell><cell>(0.0887)</cell><cell>(0.1232)</cell></row><row><cell>0.8</cell><cell>0.22</cell><cell>0.3021</cell><cell>0.055</cell><cell>0.0082</cell><cell>0.0162</cell></row><row><cell></cell><cell>(0.000)</cell><cell>(0.0786)</cell><cell>(0.771)</cell><cell>(0.331)</cell><cell>(0.554)</cell></row><row><cell>1.0</cell><cell>0.2145</cell><cell>0.3562</cell><cell>0.05002</cell><cell>0.0201</cell><cell>0.0019</cell></row><row><cell></cell><cell cols="2">(0.02408) (0.0602)</cell><cell>(0.0056)</cell><cell>(0.018)</cell><cell>(0.021)</cell></row><row><cell cols="4">2) f2 (Rosenbrock Function)</cell><cell></cell><cell></cell></row><row><cell>on</cell><cell>DE</cell><cell>PSO</cell><cell>EA</cell><cell>DE-RSF-</cell><cell>DE-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TS</cell><cell>RSF-SS</cell></row><row><cell>0</cell><cell>6.2289</cell><cell>734.23</cell><cell>45.8189</cell><cell>40.077</cell><cell>56.781</cell></row><row><cell></cell><cell>(0.3238)</cell><cell>(441.221)</cell><cell>(18.331)</cell><cell>(12.453)</cell><cell>(16.887)</cell></row><row><cell>0.2</cell><cell>17.221</cell><cell>1210.03</cell><cell>57.075</cell><cell>12.067</cell><cell>19.000</cell></row><row><cell></cell><cell>(0.201)</cell><cell>(552.109)</cell><cell>(10.605)</cell><cell>(2.332)</cell><cell>(6.739)</cell></row><row><cell>0.4</cell><cell>22.0121</cell><cell>1880.234</cell><cell>61.0231</cell><cell>11.001</cell><cell>8.105</cell></row><row><cell></cell><cell>(0.811)</cell><cell>(160.066)</cell><cell>(7.572)</cell><cell>(4.543)</cell><cell>(6.658)</cell></row><row><cell>0.6</cell><cell>26.069</cell><cell>2231.297</cell><cell>65.223</cell><cell>15.004</cell><cell>7.043</cell></row><row><cell></cell><cell>(0.326)</cell><cell>(387.093)</cell><cell>(8.137)</cell><cell>(1.457)</cell><cell>(8.222)</cell></row><row><cell>0.8</cell><cell>31.822</cell><cell>3981.321</cell><cell>93.017</cell><cell>20.132</cell><cell>10.362</cell></row><row><cell></cell><cell>(0.221)</cell><cell>(539.548)</cell><cell>(10.771)</cell><cell>(1.246)</cell><cell>(0.554)</cell></row><row><cell>1.0</cell><cell>24.2145</cell><cell>4450.431</cell><cell>115.044</cell><cell>18.0101</cell><cell>18.0719</cell></row><row><cell></cell><cell>(0.124)</cell><cell>(833.123)</cell><cell>(14.251)</cell><cell>(3.328)</cell><cell>(0.921)</cell></row><row><cell cols="3">3) f3 (Rastrigin Function)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>on,</cell><cell>DE</cell><cell>PSO</cell><cell>EA</cell><cell>DE-RSF-TS</cell><cell>DE-RSF-SS</cell></row><row><cell>0</cell><cell>0.0002</cell><cell>14.283</cell><cell>32.4439</cell><cell>4.665</cell><cell>0.745</cell></row><row><cell></cell><cell>(0.03238)</cell><cell>(1.221)</cell><cell>(2.361)</cell><cell>(0.653)</cell><cell>(1.427)</cell></row><row><cell>0.2</cell><cell>0.0034 (0.1151)</cell><cell>21.039 (1.519)</cell><cell>34.075 (1.697)</cell><cell>0.67 (2.332)</cell><cell>1.131 (1.766)</cell></row><row><cell>0.4</cell><cell>0.121</cell><cell>38.234</cell><cell>35.774</cell><cell>0.056</cell><cell>0.075</cell></row><row><cell></cell><cell>(0.133)</cell><cell>(1.744)</cell><cell>(1.353)</cell><cell>(0.543)</cell><cell>(0.623)</cell></row><row><cell>0.6</cell><cell>0.387</cell><cell>43.276</cell><cell>37.823</cell><cell>0.064</cell><cell>0.053</cell></row><row><cell></cell><cell>(0.0125)</cell><cell>(1.993)</cell><cell>(1.007)</cell><cell>(0.454)</cell><cell>(0.276)</cell></row><row><cell>0.8</cell><cell>1.422</cell><cell>50.321</cell><cell>37.567</cell><cell>0.012</cell><cell>0.009</cell></row><row><cell></cell><cell>(0.0521)</cell><cell>(2.548)</cell><cell>(0.771)</cell><cell>(0.633)</cell><cell>(0.554)</cell></row><row><cell>1.0</cell><cell>3.2725</cell><cell>57.43 1</cell><cell>39.044</cell><cell>0.0008</cell><cell>0.0018</cell></row><row><cell></cell><cell>(0.068)</cell><cell>(3.1461)</cell><cell>(1.351)</cell><cell>(0.328)</cell><cell>(0.421)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1694</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Number of trials that converged to the stopping criteria and mean number of function evaluations for convergence for 50 runs of each algorithm No. of trials converged to the stopping criteria</figDesc><table><row><cell>Dim.</cell><cell></cell><cell cols="5">(Average number of Function Evaluations)</cell></row><row><cell></cell><cell>DEl</cell><cell>PSO</cell><cell></cell><cell>EA</cell><cell>DE-RSF-TS</cell><cell>DE-RSF-SS</cell></row><row><cell>f,,50</cell><cell>38 (21268.7)</cell><cell cols="2">25 (44227.4)</cell><cell>43 (23189.7)</cell><cell>50 (11248.3)</cell><cell>50 (17667.4)</cell></row><row><cell>f2,50</cell><cell>16 (63241.2)</cell><cell cols="2">4 (85800.4)</cell><cell>13 (43287.4)</cell><cell>32 (12006.7)</cell><cell>31 (24009.6)</cell></row><row><cell>f3,50</cell><cell>26 (22721.7)</cell><cell cols="2">19 (32996.4)</cell><cell>29 (34325.6)</cell><cell>46 (12298.2)</cell><cell>48 (16775.3)</cell></row><row><cell>C4,50</cell><cell>30 (35432.1)</cell><cell cols="2">33 (53629.8)</cell><cell>36 (25079.9)</cell><cell>22 (12066.5)</cell><cell>32 (22196.5)</cell></row><row><cell>f4, 2</cell><cell>46</cell><cell>41</cell><cell></cell><cell>48</cell><cell>46</cell><cell>50</cell></row><row><cell></cell><cell cols="3">(10267.8) (22463.6)</cell><cell>(11344.8)</cell><cell>(23521.5)</cell><cell>(52303.1)</cell></row><row><cell>f6, 2</cell><cell>41</cell><cell>38</cell><cell></cell><cell>45</cell><cell>42</cell><cell>40</cell></row><row><cell></cell><cell cols="3">(12931.4) (23245.8)</cell><cell>(22121.4)</cell><cell>(34821.56)</cell><cell>(33272.8)</cell></row><row><cell>f7,50</cell><cell>24</cell><cell>0</cell><cell></cell><cell>22</cell><cell>43</cell><cell>40</cell></row><row><cell></cell><cell>(74538.2)</cell><cell></cell><cell></cell><cell>(66579.2)</cell><cell>(44712.6)</cell><cell>(37842.2)</cell></row><row><cell>f8, 2</cell><cell>44</cell><cell>32</cell><cell></cell><cell>47</cell><cell>50</cell><cell>50</cell></row><row><cell></cell><cell>(20945.1)</cell><cell cols="2">(33459.3)</cell><cell>(23897.5)</cell><cell>(22132.5)</cell><cell>(22619.3)</cell></row><row><cell>00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">74}/4e</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0</cell><cell>4 05</cell><cell cols="2">5 0&amp; N0' i</cell><cell></cell></row><row><cell cols="3">(a) Acke Function</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>4.2.1 PSO</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>Discussion</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Tables <ref type="table">3</ref> and<ref type="table">4</ref> we report results of unpaired t-tests run between (i) each of the two new algorithms and (ii) the second best among DE1, PSO and EA in each case (standard error of difference of the two means, 95% confidence interval of this difference, the t value, and the two-tailed P value are presented). For all cases in Tables <ref type="table">3</ref> and<ref type="table">4</ref>, sample size = 50 and degrees of freedom = 98. It is interesting to see from Tables <ref type="table">3</ref> and<ref type="table">4</ref> that the proposed methods meet or beat the nearest competitor in a statistically meaningful way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bibliography</head><p>The experimental results confirm that for the non-noisy versions of the selected benchmarks, the classical DE outperforms all other competitor algorihms used here. But as noise creeps into the fitness landscape, DE eventually loses its accuracy, speed of convergence and robustness.</p><p>A scrutiny of the results reveals that PSO stagnates long before the effect of noise becomes significant in the case of the Rosenbrock (f2) function. It also stagnates much earlier in the case of noisy Griewank (f4) and Rastrigin (f3) functions. For a highly multi-modal function like fl0, the fitness landscape becomes very complicated with noise and the performance of both PSO and DE becomes worst. Compared to PSO, both DE and EA seem to be less badly affected by noise, however the test data indicate that these algorithms had difficulty (due to noise) achieving results close to the optimal value of 0. The new variants of DE appear to be the most effective, firstly in avoiding convergence to local optima when the fitness landscape itself is very complex and then getting very close to the optimum at 0 even when noise is a predominant factor. It can also be observed that the performance of these new schemes improves considerably over the others even when the noise variance is rather high. Note that for Levy no. 5 we have plotted the logarithm of the absolute value of the fitness as it has its minima at x* = [-1.3068,-1.4248] with f(x*) = -176.1375. All the other benchmarks used here have their minima at the origin or very close to the origin. Although the performance of PSO seems rather bleak here, we believe that PSO's performance can be improved by extensive problem-dependent tuning of the parameters in the algorithm. We have however used just one parametric setup for all the experiments here -one we identified as a good compromise for most of the test problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The limitations of DE when applied to noisy fitness functions were pointed out in <ref type="bibr" target="#b4">[5]</ref>. We are not aware of any published paper that solved this problem. As a remedy to this problem, we modified the basic algorithm with a randomly changing scale factor and two selection schemes (one borrowed from <ref type="bibr" target="#b5">[6]</ref>, the other our own). Empirical evidence has been provided to justify the usefulness of the proposed approach. The new methods have been compared against (a) the basic DE, (b) the PSO, and (c) EA, using a ten-function test suite. Our methods have been shown to outperform the best-known competitors statistically significantly. Future research may focus upon an in-depth mathematical analysis of DE applied to noisy fitness functions so that some analytical guidelines may be obtained about the choice of the optimal threshold value T used here for selection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Walsh</surname></persName>
		</author>
		<title level="m">Artificial Intelligence through Simulated Evolution</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Evolution Strategy: Optimization of technical systems by means of biological evolution</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rechenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Fromman-Holzboog</publisher>
			<pubPlace>Stuttgart</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artificial Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Univ. of Michigan Press</publisher>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. conf</title>
		<meeting>IEEE Int. conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Noisy Optimization Problems -A Particular Challenge for Differential Evolution?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Krink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bodgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sixth Congress on Evolutionary Computation (CEC-2004</title>
		<meeting>Sixth Congress on Evolutionary Computation (CEC-2004</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Threshholding -a Selection Operator for Noisy ES</title>
		<author>
			<persName><forename type="first">S</forename><surname>Markon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beislstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Congress on Evolutionary Computation (CEC-2001)</title>
		<meeting>Congress on Evolutionary Computation (CEC-2001)</meeting>
		<imprint>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Differential evolution -A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to differential evolution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Ideas in Optimization</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Come</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eds</forename><surname>Mcgrow-Hill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">London</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="79" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A note on the generation of random deviates</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="610" to="611" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How to Solve It: Modern Heuristics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evolutionary programming made faster</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="102" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analysis of selection algorithms: A Markov chain approach</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="167" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two improved differential evolution schemes for faster global search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM-SIGEVO Proceedings of GECCO</title>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<pubPlace>Washington D.C.</pubPlace>
		</imprint>
	</monogr>
	<note>to appear in the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
