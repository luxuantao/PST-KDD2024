<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Structure in Multiple Learning Tasks: The TC Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>O'sullivan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213-3891</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Structure in Multiple Learning Tasks: The TC Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">133BD95E9CBC45CB122AFAAFD5DFD1FC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there has been an increased interest in "lifelong" machine learning methods, that transfer knowledge across multiple learning tasks. Such methods have repeatedly been found to outperform conventional, single-task learning algorithms when the learning tasks are appropriately related. To increase robustness of such approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading. This paper describes the task-clustering (TC) algorithm. TC clusters learning tasks into classes of mutually related tasks. When facing a new learning task, TC first determines the most related task cluster, then exploits information selectively from this task cluster only. An empirical study carried out in a mobile robot domain shows that TC outperforms its non-selective counterpart in situations where only a small number of tasks is relevant.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One of the exciting new developments in the field of machine learning are algorithms that can gradually improve their ability to learn when applied to a sequence of learning tasks. Motivated by the observation that humans encounter more than just a single learning task during their lifetime, and that they successfully improve their ability to learn <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>, several researchers have proposed algorithms that are able to acquire domain-specific knowledge and re-use it in future learning tasks. For example, in the context of face recognition, methods have been developed that improve the recognition accuracy significantly when learning to recognize a face, by transferring face-specific invariances learned in other, previous face recognition tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>. Similar results in the context of object recognition, robot navigation and chess are reported in <ref type="bibr" target="#b25">[26]</ref>.</p><p>The first author is also affiliated with the University of Bonn, Germany, where part of the research was carried out.</p><p>Technically speaking, the problem of learning from multiple tasks can be stated as follows. Given 1. training data for the current learning task (training set), <ref type="bibr" target="#b1">2</ref>. training data for N other, previous learning tasks (called: support sets), and 3. a performance measure find a hypothesis which maximizes the performance in the current (the N+1-th) learning task. Notice that item 2 in this list, the data of previous learning tasks (the support sets), does not appear in the usual formulation of machine learning problems. This is because support data might only be indirectly related, e.g., carry different class labels. To utilize this data, mechanisms are required that can acquire and re-use domain-specific knowledge in order to guide the generalization in a knowledgeable way.</p><p>To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> for a more detailed survey and comparison): learning internal representations for artificial neural networks, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>, learning distance metrics, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref>, learning to re-represent the data, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>, learning invariances in classification, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>, learning algorithmic parameters and choosing algorithms, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>, and learning domain models, e.g., <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task. However, all of them weigh previous learning tasks equally strongly when transferring knowledge-thus, they may fail when only a small subset of learning tasks is related appropriately. For example, the approaches to object recognition described in <ref type="bibr" target="#b25">[26]</ref> generalize better if previous learning tasks involve the recognition of other objects; however-if the learner faced previously unrelated learning tasks (such as stock market prediction), these approaches will most likely fail due to their non-selective nature of their transfer mechanisms. Consequently, it is common practice for human designers to pick a set of tasks which is known to be related appropriately. To overcome this obvi-ous limitation of current approaches, it is desirable to design algorithms that can discover the relation between multiple learning tasks by themselves, and transfer knowledge selectively across related learning tasks.</p><p>This paper describes such an algorithm, called the TC (task clustering) algorithm. Unlike previous methods, TC transfers knowledge selectively, from the most related set of learning tasks only. In order to do so, TC estimates the mutual relatedness between tasks, and builds up an entire hierarchy of classes of learning tasks. When a new learning task arrives, TC determines the most related task cluster in the hierarchy of previous learning tasks. Knowledge is transferred selectively from this single cluster only-other task clusters are not employed. The clustering strategy enables TC to handle multiple classes of tasks, each of which may exhibit different characteristics.</p><p>To elucidate TC in practice, this paper reports results of a series of experiments carried out in a mobile robot domain <ref type="bibr" target="#b28">[29]</ref>. The three key results of this empirical study are: 1. The sample complexity can be reduced significantly when domain-specific knowledge is transferred from previous learning tasks. 2. TC reliably succeeds in partitioning the task space into a (surprisingly) meaningful hierarchy of related tasks. 3. Selective transfer significantly improves the results in cases where only few support tasks are relevant (yet does not hurt the performance when all support tasks are appropriately related).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE TC ALGORITHM</head><p>The TC algorithm has been designed to support fast learning of large sets of (binary) classification tasks, that are defined over the same input space. This research has been driven by our interest in fast and data-efficient robot learning algorithms. TC will be introduced in five steps, the first two of which have been adopted from recent literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NEAREST NEIGHBOR GENERALIZATION</head><p>At the underlying function approximation level, the TC algorithm uses nearest neighbor for generalization (see e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>). To determine the proximity of data points, TC uses a globally weighted Euclidean distance metric:</p><formula xml:id="formula_0">dist d (x ; y) = s X i d (i) x (i) ? y (i) 2</formula><p>Here d denotes an adjustable vector of weighting factors, and the superscript (i) is used to refer to the i-th component of a vector. d parameterizes the space of Euclidean distance metrics. Obviously, d determines the generalization properties of nearest neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ADJUSTING THE DISTANCE METRIC</head><p>TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28</ref> Here the subscript n denotes a particular learning task. Let d = argmin d E(d) denote the parameter vector that mini- mizes E, henceforth called E-optimal, and let dist be the corresponding optimal distance metric. By minimizing the distance when class n (x) = class n (y) and simultaneously maximizing the distance when class n (x) 6 = class n (y), dist focuses on the relevant input dimensions for the n-th learning task. In our implementation, d is found using gradient descent.</p><p>Notice that d can be optimized simultaneously for multiple learning tasks. Let A f1; 2; : : :Ng denote a subset of the support tasks. Then</p><formula xml:id="formula_1">d A = argmin d X n2A E n (d)<label>(1)</label></formula><p>is the E-optimal parameter vector and dist A the corresponding distance metric for the task set A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">THE TASK TRANSFER MATRIX</head><p>Using the E-optimal distance metric obtained for one task when learning another task is only likely to improve the results when both tasks demand a similar feature weighting. To determine the degree to which tasks are related to each other, TC computes the matrix</p><formula xml:id="formula_2">C = (c n;m )</formula><p>which is called the task transfer matrix. The task transfer matrix contains a value c n;m for each pair of learning tasks n and m. c n;m is the expected generalization accuracy obtained in task n when using m's E-optimal distance metric. Each element c n;m is estimated via k-fold cross-validation, using the E-optimal distance metric of task m and the training set of task n. The task transfer matrix is the basis for clustering tasks and building task hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CLUSTERING TASKS AND THE TASK HIERARCHY</head><p>TC clusters all N learning tasks into T N disjunct bins, denoted by A 1 ; : : :; A T . This is done by maximizing the following functional:</p><formula xml:id="formula_3">J = 1 N T X t=1 X n2At 1 jA t j X m2At</formula><p>c n;m J measures the averaged estimated generalization accuracy that is obtained when task n 2 A t uses the E-optimal distance metrics of another task m 2 A t in the same cluster.</p><p>In other words, maximizing J groups those tasks together that are most related, i.e., those between which transferring E-optimal distance metrics leads to the largest performance gain. 1 Notice that each of the T resulting task clusters defines a cluster-specific E-optimal distance metric d</p><formula xml:id="formula_4">At = argmin d X n2At E n (d):</formula><p>which is obtained by minimizing E At (cf. ( <ref type="formula" target="#formula_1">1</ref>)). By repeating the clustering process for different values of T (T =1; 2; : : :N). of task hierarchies, which will be explained in more detail in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">SELECTIVE TRANSFER TO NOVEL TASKS</head><p>When a new learning task arrives, TC identifies the most related task cluster in the hierarchy of previous learning tasks. This is done by minimizing c n;At over all task clusters A t (c n;At denotes the task transfer coefficient for using the E-optimal distance metric of task cluster A t ). Notice if an appropriate number of clusters T is unknown, the entire hierarchy is consulted when searching for the most related task cluster. Having determined the most appropriate task cluster, TC uses the E-optimal distance metric d At of that task cluster for nearest neighbor generalization in the new task.</p><p>To summarize, the steps of the TC algorithm are:</p><p>1. TC classifies by nearest neighbor, using a globally weighted distance metric. 2. It transfers knowledge across multiple learning tasks by learning a distance metric for some tasks, and re-using it for nearest neighbor generalization in others. 3. To focus the transfer on the most related tasks, TC computes the task transfer matrix, which measures the mutual relation of learning tasks. 4. It constructs the task hierarchy by clustering learning tasks according to the task transfer matrix. 5. When facing a new thing to learn, the distance metric is transferred selectively, from the most related task cluster only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SETUP</head><p>To evaluate TC thoroughly under a variety of circumstances, it was applied to three different families of binary classification tasks that were obtained using a set of databases shown in Figure <ref type="figure" target="#fig_2">2</ref>. The databases were collected with the mobile robot shown in Figure <ref type="figure" target="#fig_1">1</ref>, using the color camera (top of the robot) and the 24 sonar proximity sensors (arranged in a ring around the robot) as input. Each database consists of 30 to 200 of snapshots which show examples and counterexamples of a particular concept (such as persons, landmarks, objects, and locations). The first six datasets were constructed with a particular person somewhere in front of We defined three families of learning tasks:<ref type="foot" target="#foot_1">2</ref> </p><p>T 1 : Task family T 1 consists of thirteen tasks involving the recognition of people, landmarks and locations, using the databases a, b, c, d, g, j, k, and l in Figure <ref type="figure" target="#fig_2">2</ref>. The "new" task (testing task) is the task of recognizing the status of doors (open vs. closed). The input space is 324-dimensional: Camera images are subsampled to a 10 by 10 matrix, yielding a total of 300 RGB pixels per image. In addition, the 24 sonar measurements are also presented. Each dataset contains exactly 200 examples.</p><p>T 2 : T 2 is aimed to test TC in situations where most tasks are unrelated. It contains three groups of four tasks each:</p><p>The first group (called 2,4,5,6) consists of four tasks adopted from task family T 1 , all involving the recog- nition of people. The second group (called 2 0 ,4 0 ,5 0 ,6 0 ) consists of the same four tasks, but here the input pixels are permuted randomly (using the same permutation for all tasks). The third group (2 00 ,4 00 ,5 00 ,6 00 ) consists, too, of the same four tasks, but this time the input space of each task is permuted differently (also randomly). The testing task is the same as in task family T 1 . Task family T 2 , thus, contains only four tasks that are potentially related to the testing tasks. Four other tasks are mutually related but unrelated to the testing task, and four tasks are neither related to the testing task, nor mutually related. As we will see below, non-selective transfer suffers from such unrelated tasks.</p><p>T 3 : Task family T 3 consists of nine tasks that corre- spond to the databases a, d, e, f, h, i, j, k, and l in Figure <ref type="figure" target="#fig_2">2</ref>. T 3 uses a more sophisticated input representa- tion. Following the ideas in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, images in T 3 are encoded using a "view-based" approach to scene recognition. In this approach, feature dimensions are chosen so that large changes in object pose and orientation produce small changes in feature space, yet small changes in object "quality" (shape, texture, color) produce relatively large differences in feature space. In particular the representation comprises of 720 features corresponding to color, intensity, texture and correlation,augmented by 24 sonar measurements. The datasets in T 3 were generally smaller; some of them contained as few as 30 examples.</p><p>When clustering tasks, each value c mn was estimated using 100-fold cross-validation with a training set of 10 examples per dataset and a testing set of 24 to 190 examples (depending on the dataset). The distance metric (Sect. 2.2) was optimized by gradient descent, which was iterated for 100 steps using a step-size of 0.1 and a momentum of 0.9. Convergence, however, was consistently observed much earlier (often after 6 epochs). The results were not sensitive to these learning parameters. After bounding the distance metric (with 0:01 d (i) 1), we did not observe noticeable over-fitting, neither for the tasks that the distance metrics were optimized for, nor for the testing task. All experimental results reported below are test set results (i.e., performance was measured for data points that were not part of a training set). They are all averaged over 20 to 100 experiments using different sets of training examples. To illustrate the effect of transfer across tasks, we will compare the Eoptimal distance metric (transfer) with a non-optimized (i.e., equally-weighted) distance metric, or, alternatively, with a distance metric that is E-optimal only for the training set.</p><p>The latter two metrics do not rely on the support sets; thus, there is no transfer. Whenever appropriate, the diagrams also show 95% confidence bars for the true value. All performance graphs show the generalization accuracy (testing set accuracy) for the testing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NON-SELECTIVE TRANSFER</head><p>The first question investigated here addresses the effectiveness of learning a distance metric based on support sets (Step 1 and 2 of the TC algorithm, cf. Section 2). How much does a learner benefit from a distance metric that has previously been optimized for other, related tasks? We first conducted experiments using (non-selectively) all support tasks for computing the E-optimal distance metric. Nonselective transfer can be understood as a special case of the TC algorithm in which the number of clusters T is set to one.</p><p>The first key empirical result of this paper is shown in Figure <ref type="figure">3</ref>, which compares the accuracy of nearest neighbor as a function of the number of training examples. Both the grey and the thin black curve in Figure <ref type="figure">3a</ref> illustrate nearest neighbor in the absence of support tasks: The grey curve shows the generalization accuracy of the equally-weighted distance metric, and the thin curve shows the generalization accuracy for the distance metric that is E-optimal for the training set. The thick curve depicts the generalization accuracy when transferring knowledge, using the metric that is E-optimal distance for all 12 support tasks. As can be seen from these graphs, the latter approach shows significantly better results than the other two approaches, particularly in the early phase of learning. This result illustrates the benefit of transferring knowledge across tasks.</p><p>There are two ways to quantify these results.</p><p>1. Relative generalization accuracy. The generalization error is obtained by averaging the curves in Figure <ref type="figure">3a</ref>.</p><p>The support set-E-optimal distance metric infers an average classification error of 15.1%, which is only 52.8% of that of the equally-weighted distance metric, and 63.6%</p><p>of the distance metric that is E-optimal for the training set. 2. Relative sample complexity. The second quantity measures the reduction in sample complexity. Figure <ref type="figure">3b</ref> shows the result of statistical tests on the generalization accuracy for the training set-E-optimal distance metric versus the support set-E-optimal metric, using different numbers of training examples. In the white region, ? training set-optimal metric (no transfer) ?</p><p>support set-optimal metric (non-selective transfer)</p><p>? ? the training set-optimal distance metric (no transfer) outperforms the support set-optimal metric (transfer) with at least 95% confidence. In the large grey region, the opposite is the case. In between, both methods work about equally well and their generalization accuracies do not differ significantly (at the 95% level). Notice that, on average, the support set-E-optimal distance metric uses only 55.2% of the number of training examples that are required when using the training-set optimal metric, and 39.2% of the training examples required for the equally-weighted metric. Thus, transfer cuts the sample complexity roughly in half.</p><p>To summarize, the generalization error when transferring knowledge is only 63.6% of that inferred by the best nontransfer approach, and it requires only 55.2% of the samples required without transfer. These results apply to task family</p><formula xml:id="formula_5">T 1 .</formula><p>The positive impact of the knowledge transfer depends crucially on the fact that the support tasks are sufficiently related to the testing task. Task family T 2 , in which the majority of tasks is unrelated, shows quite the opposite effect. As can be seen in Figure <ref type="figure">4</ref>, the E-optimal distance metric when transferring knowledge non-selectively is in fact inferior to the best non-transfer approach. When transferring knowledge unselectively the average generalization error is 19.7%, which is 9.9% larger than that of the best non-transfer approach. The sample complexity increases by 18.7% through the (non-selective) transfer of knowledge. These findings support our claim that unselective transfer hurts the performance if the tasks are not appropriately related. As will be shown in the next sections, selectively transferring knowledge from the right cluster of tasks can avoid the damaging effects stemming from poorly related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CLUSTERING TASKS</head><p>Figure <ref type="figure" target="#fig_5">5</ref> shows a normalized version of the transfer matrix (c n;m ) for each task family. Each row depicts how a particular task n (including the testing task) benefits from knowledge transferred from task m. White boxes indicate that the generalization accuracy of task n improves when the E-optimal distance metric of task m is used instead of the equally-weighted distance metric. Black boxes indicate that the opposite is the case, meaning that tasks are "antirelated." The size of the box visualizes the magnitude of the effect.</p><p>In task family T 1 , most tasks are either related to the testing task or unrelated, but none of them is notably "antirelated" (first row in Figure <ref type="figure" target="#fig_5">5a</ref>). The diagram for the more diverse task family T 2 shows that some of the tasks, in par- ticular 2 0 , 2 00 , and 4 00 , are anti-related to the testing task. In other words, using their respective E-optimal distance metrics will hurt the performance in the testing tasks. However, Figure <ref type="figure" target="#fig_5">5a</ref> also shows that the non-permuted tasks 2, 3, 4, and 6 are indeed well-related to the testing task, showing that there exists the opportunity for synergy through knowledge transfer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TASK HIERARCHIES</head><p>Figures 6-8 depict the task hierarchies for the three task families. These figures illustrate the second key result of the empirical study: In all three task families, TC manages to discover surprisingly meaningful tasks clusters. This is most apparent in task family T 3 (Figure <ref type="figure" target="#fig_8">8</ref>). Early on in Figure <ref type="figure" target="#fig_8">8</ref>, starting with T=3 clusters, three major clusters have been found, that split the set of tasks into (a) people recognition, (b) determining obstacle proximity, and (c) landmarks/locations. The latter class (c) is then split into one class containing both door-related tasks, and one containing the single location-related task. Notice that the information of the type of learning task has not been communicated explicitly to the TC algorithm-it is discovered from the importance of individual input features the different learning tasks.</p><p>Similar results can be found in the hierarchy of task family T 2 (Figure <ref type="figure" target="#fig_7">7</ref>). Here the two major task families that use the same encoding (f2; 4; 5; 6g and f2 0 ; 4 0 ; 5 0 ; 6 0 g) are grouped together. For example, for T=4 partitions TC generates the following task clusters: f2; 4; 5; 6g; f2 0 ; 4 0 ; 5 0 ; 6 0 g; f2 00 ; 4 00 ; 6 00 g; f5 00 g. When T 4 all three different task types are clustered into separate clusters. When T=6, TC groups exactly those tasks together that rely on the same input encoding. Here the clusters are f2; 4; 5; 6g, f2 0 ; 4 0 ; 5 0 ; 6 0 g, f2 00 g, f4 00 g, f5 00 g, and f6 00 g.</p><p>In task family T 1 (Figure <ref type="figure" target="#fig_0">6</ref>), where the differences be- tween different tasks are more subtle, it is interesting to note that the tasks involving the recognition of people form the most similar subgroup (particularly those involving two different people, cf. Figure <ref type="figure" target="#fig_5">5a</ref>). When T 4, those tasks that involve the recognition of a person (1 to 9) and than those that do not <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12)</ref> are always arranged in different clusters. These findings clearly illustrate the second key result of this research: TC indeed manages to find meaningful clusters. In all our experiments, TC discovered the structure that inherently exists for the different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">SELECTIVE TRANSFER</head><p>Figure <ref type="figure">9</ref> shows performance results obtained using the TC algorithm in task family T 2 , for T=3 clusters (thick black curve in Figure <ref type="figure">9a</ref>), and when using the entire hierarchy (grey curve). In both experiments, only one of the clusters, namely f2; 4; 5; 6g, is appropriately related to the testing task, i.e., leads to results that are better than those obtained with the equally-weighted distance function. Across the board, TC selects the best task cluster considerably often, hence generalizes well. For example, when T=4 and only</p><p>two training examples are given in the test task (one example of an open door, and one of a closed door), TC picks in 59% of our experiments the correct task cluster f2; 4; 5; 6g.</p><p>In 24% of all experiments, however, TC selects task cluster f5 00 g, in 9% task cluster f2 0 ; 4 0 ; 5 0 ; 6 0 g, and in 8% task clus- ter f2 00 ; 4 00 ; 6 00 g. This illustrates that TC, with some error (when training data is scarce), manages to identify the most relevant tasks.</p><p>The performance results obtained in family T 2 illustrate the third key result of the empirical study: Selective transfer is superior to non-selective transfer in situations where many tasks are unrelated (irrelevant). For example, if T=3, TC achieves 14.5% average generalization error in the test task, if knowledge is transferred selectively from the support tasks. Relatively speaking, this is only 73.46% of the average error that is being observed in the non-selective approach (which is 19.7%, cf. thick curve in Figure <ref type="figure">4a</ref>), and it is also considerably close to the best possible distance metric (see <ref type="bibr" target="#b28">[29]</ref>). The relative improvement in the sample complexity is even more significant: The sample complexity in the test set is only 58.7% when TC transfers knowledge selectively, when compared to the non-selective counterpart.</p><p>When TC is compared to the best non-transfer approach, TC with T=3 uses only 78.5% of the samples to reach the same level of generalization accuracy, and its generalization accuracy is on average 80.8% of that inferred by the   equally-weighted distance metric. These results are remarkably close to those that could have been achieved if one knew in advance which ones of the 12 support sets were appropriately related. In our experiments, we observed that the number of task clusters T only weakly impacts the results, as long as T 3. For smaller values of T, the number of task clusters is insufficient, and TC's performance degrades to that of the regular nearest neighbor with an equally-weighted distance metric.</p><p>Figure <ref type="figure" target="#fig_1">10</ref> shows the results obtained when applying TC in task family T 3 . These results basically match the results obtained for T 1 and T 2 . The most notable difference here is that optimizing the distance metric based on the training set does not lead to an improvement over the equallyweighted, non-optimized distance metric-a finding which we attribute to the fact that the input features inT 3 are more appropriate for image classification tasks (see Section 3.1).</p><p>Compared to the equally-weighted distance metric, the relative generalization accuracy of TC is 73.1% and the relative sample complexity is 74.3%. Not shown here are results obtained in task family T 1 , in which case TC performs ap- proximately as well as its non-selective counterpart (see <ref type="bibr" target="#b28">[29]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>This paper considers situations in which a learner faces an entire collection of learning tasks. It shows how hierarchical structure can be discovered in the space of learning tasks, and how it can be used to selectively transfer knowledge to other, new learning tasks, in order to boost generalization. The TC algorithm proposed here employs a nearest neighbor algorithm, which transfers knowledge by adjusting the distance metric in some tasks while re-using it in others.  These results are well in tune with other results obtained in robot perception, robot control and game playing domains <ref type="bibr" target="#b25">[26]</ref>, which illustrate that a lifelong learner can generalize more accurately from less data if it transfers knowledge acquired in previous learning tasks.</p><p>A key assumption made in the TC approach is the existence of groups of tasks so that all tasks are related within each group. Little is known for cases where the class boundaries are smoother. In such cases, smoother arbitration schemes (e.g., weighting the impact of a task cluster in proportion to c n;m ) might produce superior results. The results presented in this paper, however, illustrate that even hard class boundaries consistently improve the generalization accuracy. Hard boundaries have the advantage that the cluster-optimal distance metric can be computed off-line, before the arrival of a new learning tasks, which makes TC very fast in practice.</p><p>One of the main potential limitations of TC arises from the fact that task clustering is based on pairwise comparisons. TC will not capture effects of transfer that arise if only three or more tasks are involved. It remains to be seen whether pairwise comparisons will prevent TC from finding useful clusters in different application domains. However, a full evaluation of transfer in all subsets of tasks requires time exponentially in the number of tasks N, whereas TC time requirements are quadratic. It even appears feasible to design incremental strategies whose time requirements are in O(NT), which will be more efficient than the current implementation of TC if T is small. A third limitation of the current implementation arises from the fact that the space of all partitions is searched exhaustively (which can only be done when the overall number of tasks is sufficiently small, which was the case in our experiments). Clearly, the complexity of exhaustive search prohibits global optimization for large values of N and T . However, we do not view this as a principal limitation of the TC algorithm, since heuristic and/or stochastic optimization methods are certainly applicable <ref type="bibr" target="#b6">[7]</ref>. If learning tasks arrive one after another, task clusters may also be learned incrementally, by determining cluster membership when a task arrives. Little is known concerning how much the results presented here depend on the fact that the partitioning always represents the global minimum of J.</p><p>The reader may notice that the general scheme underlying the TC approach may be applicable to other approaches that transfer knowledge across multiple learning tasks, such as those surveyed in <ref type="bibr" target="#b25">[26]</ref> (see also Section 1). Many of these approaches can learn and transfer more than just a global weighting vector. Of course, for some approaches this will be computationally infeasible, since the general scheme underlying the TC algorithm requires in the order of N 2 comparisons, each involving repeated experiments with transfer across tasks. The key difference of the TC approach to previous approaches lies in TC's ability to transfer knowledge selectively. Rather than weighting all previous learning tasks equally when learning bias for a new one, TC structures the space of learning tasks and reasons about their relatedness. In the light of the experimental findings, we conjecture that the TC approach scales much better application domains in which there are many diverse tasks to be learned, i.e., domains in which the learning tasks are not all just of a single type.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figures 6 -</head><label>6</label><figDesc>8 on page 7 show examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The mobile robot XAVIER is equipped with a camera and 24 sonar sensors.</figDesc><graphic coords="3,334.02,99.23,117.84,119.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of the learning tasks (image and sonar scan). The actual images are in color.</figDesc><graphic coords="4,76.26,453.51,66.96,53.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 4: Non-selective transfer in task family T2. Obviously, unselective trans- fer increases the need for training data in T2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Task transfer matrices (cn;m) for the different task families. White values indicate that the error in task n is reduced when the E-optimal distance metric of task m is used. Black values indicate the opposite: tasks are anti-related. The relation of the individual tasks to the testing tasks is also depicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The task hierarchy for task family T1. The hierarchy is obtained by clustering the task space using different numbers of clusters T. The right part of the diagram depicts the corresponding value for J, and the difference between the best and the worst partitioning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The task hierarchy for task family T2. Notice that early on, the task hierarchy separates the three different task types.The only related task cluster, f2; 4; 5; 6g, is identified when T 3 clusters are available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The task hierarchy for task family T3. Despite the small size of the datasets, TC reliably discovers the different types of learning problems, as it groups different types of learning problems into different branches of the hierarchy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: (a) Selective transfer (TC) in task family T2, using T=3 clusters, and the entire task hierarchy (T =1 : : : N).(b) depicts the statistical comparison of sample complexity for TC with T=3 clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The situation changes as more training data arrives. With 20 training examples, TC correctly guesses the best task cluster in 91% of all experiments, and with 32 or more patterns it reliably (100%) identifies the best cluster.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Notice that maximizing J defined over a pairwise matrix (C) is a well-understood combinatorial data clustering problem for which various algorithms exist (see for example<ref type="bibr" target="#b6">[7]</ref>).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Since our experiments were performed in two stages, not all task families utilized each data set.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors wish to thank the anonymous reviewers for their thoughtful comments.</p><p>This research is sponsored in part by the National Science Foundation under award IRI-9313367, and by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of NSF, Wright Laboratory or the United States Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A method for learning from hints</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Abu-Mostafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 5</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Psychological studies of explanation-based learning</title>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Brewer</surname></persName>
		</author>
		<editor>G. DeJong</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston/Dordrecht/London</pubPlace>
		</imprint>
	</monogr>
	<note>Investigating Explanation-Based Learning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using locally weighted regression for robot learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1991 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 1991 IEEE International Conference on Robotics and Automation<address><addrLine>Sacramento, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning Internal Representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Australia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Flinders University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Example based image analysis and synthesis. A.I. Memo No. 1431</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Recursive Automatic Algorithm Selection for Inductive Learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Amherst, MA 01003</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data clustering and learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Brain Theory and Neural Networks</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Arbib</surname></persName>
		</editor>
		<imprint>
			<publisher>Bradfort Books/MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based of source of inductive bias</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Utgoff</surname></persName>
		</editor>
		<meeting>the Tenth International Conference on Machine Learning<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scattered data interpolation: Tests of some methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Franke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Mathematics of Computation</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="181" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Flexible metric nearest neighbor classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Discriminant adaptive nearest neighbor classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-speaker/speaker-independent architectures for the multi-state time delay neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generalizing from a single view in face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
		<idno>CS-TR 95-02</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">76100</biblScope>
			<pubPlace>Israel</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual learning and recognition of 3-d objects from appearance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seemore: A view-based approach to 3-d object recognition using multiple visual cues</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hasselmo</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory and Natural Learning Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Judd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generalization across changes in illumination and viewing position in upright and inverted faces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edelman</surname></persName>
		</author>
		<idno>CS-TR 93-14</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">76100</biblScope>
			<pubPlace>Israel</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explanationbased neural network learning from mobile robot perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symbolic Visual Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Transferring Previously Learned Back-Propagation Neural Networks to New Learning Tasks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>New Brunswick, NJ 08904</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rutgers University, Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Layered conceptlearning and dynamically-variable bias management</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seshu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tcheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-87</title>
		<meeting>IJCAI-87</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="308" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive generalization and the transfer of knowledge</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Sharkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J C</forename><surname>Sharkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Irish Neural Networks Conference</title>
		<meeting>the Second Irish Neural Networks Conference<address><addrLine>Belfast</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward a model of consolidation: The retention and transfer of neural net task knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the INNS World Congress on Neural Networks</title>
		<meeting>the INNS World Congress on Neural Networks<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards memory-based reasoning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stanfill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Waltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1213" to="1228" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Symbolic neural systems and the use of hints for developing complex systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Suddarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Studies</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adapting bias by gradient descent: An incremental version of delta-bar-delta</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Tenth National Conference on Artificial Intelligence AAAI-92</title>
		<meeting>eeding of Tenth National Conference on Artificial Intelligence AAAI-92<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI, AAAI Press/The MIT Press</publisher>
			<date type="published" when="1992-07">July 1992</date>
			<biblScope unit="page" from="171" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Explanation-Based Neural Network Learning: A Lifelong Learning Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning one more thing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>CMU-CS-94-184</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-95</title>
		<meeting>IJCAI-95</meeting>
		<imprint>
			<date type="published" when="1994">1995. 1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Also appeared as CMU</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Clustering learning tasks and the selective cross-task transfer of knowledge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>O'sullivan</surname></persName>
		</author>
		<idno>CMU-CS-95-209</idno>
		<imprint>
			<date type="published" when="1995">15213. 1995</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Utgoff</surname></persName>
		</author>
		<title level="m">Machine Learning of Inductive Bias</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
