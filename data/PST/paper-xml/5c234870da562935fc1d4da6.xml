<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CounterMiner: Mining Big Performance Data from Hardware Counters</title>
				<funder ref="#_rbvkTrw #_4t3nqYw #_7pKBGSJ #_wbUG8fV">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_yrHfHkT #_AChtMzy">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_ARqjv6J">
					<orgName type="full">CAS</orgName>
				</funder>
				<funder ref="#_QgaVXan">
					<orgName type="full">Shenzhen Technology Research Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yirong</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HICAS Shenzhen Institute of Advanced Technology</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Sun</surname></persName>
							<email>sun@126.com</email>
							<affiliation key="aff1">
								<orgName type="department">Information Engineering College Capital Normal University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China brad</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qinyi</forename><surname>Luo</surname></persName>
							<email>qingyiluo@usc.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California Los Angles</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
							<email>jwang@cnu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Information Engineering College Capital Normal University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhibin</forename><surname>Yu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">HICAS Shenzhen Institute of Advanced Technology</orgName>
								<address>
									<postCode>CAS</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
							<email>xuehai.qian@usc.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CounterMiner: Mining Big Performance Data from Hardware Counters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/MICRO.2018.00056</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>performance</term>
					<term>big data</term>
					<term>computer architecture</term>
					<term>performance counters</term>
					<term>data mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events 1 . These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running in a "24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.</p><p>In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Modern processors typically provide 4-8 hardware counters to measure hundreds of crucial events such as cache and TLB misses <ref type="bibr" target="#b1">[1]</ref>- <ref type="bibr" target="#b4">[4]</ref>. These events can generally reveal root causes and key insights about the performance of computer systems. Therefore, performance counter based analysis is applied in a wide range of applications, including task scheduling <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, workload characterization <ref type="bibr" target="#b7">[7]</ref>- <ref type="bibr" target="#b14">[14]</ref>, performance optimization of applications <ref type="bibr" target="#b15">[15]</ref>- <ref type="bibr" target="#b18">[18]</ref>, compiler optimization <ref type="bibr" target="#b19">[19]</ref>- <ref type="bibr" target="#b21">[21]</ref>, architecture optimization <ref type="bibr" target="#b22">[22]</ref>, and many more. A number of programable performance measurement tools have therefore been developed, including PAPI <ref type="bibr" target="#b23">[23]</ref>, VTune <ref type="bibr" target="#b24">[24]</ref>, Perfmon <ref type="bibr" target="#b25">[25]</ref>, Oprofile <ref type="bibr" target="#b26">[26]</ref>, and many others <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p><p>However, there is a fundamental tension between accuracy and efficiency when using a small number of hardware counters to measure a large number of events. On the one side, the one counter one event (OCOE) approach can achieve high accuracy because a number of events are measured by the same number of hardware counters at a time. Obviously, OCOE becomes inefficient with more than one hundred and up to fourteen hundreds of measurable events <ref type="bibr" target="#b29">[29]</ref>.</p><p>This motivates the multiplexing (MLPX) approach, which improves the measurement efficiency by scheduling events from a fraction of execution to be counted on hardware counters and extrapolating the full behavior of each event from its samples. However, MLPX incurs large measurement errors due to time-sharing and sampling <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b30">[30]</ref>- <ref type="bibr" target="#b32">[32]</ref>.</p><p>In cloud computing era, this problem is exaggerated for two reasons. First, resolving the measurement errors of MLPX is a requirement. A modern cloud computing platform usually consists of more than thousand of servers and millions of complex workloads running services with diverse characteristics in a "24/7/365" manner. The major companies have good incentives to understand the performance behavior because a small performance improvement (e.g., 1%) can result in millions of dollars of savings <ref type="bibr" target="#b7">[7]</ref>. In this context, randomly selecting and measuring a small number of events with OCOE is not sufficient because the cloud services' performance may be affected by the unmeasured events. To measure a large number of events, using MLPX and handling its measurement errors are mandatory. Prior work shows that the errors cannot be effectively avoided during the sampling <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>.</p><p>Second, it becomes more difficult to extract insights of performance behavior. The events of server processors in a cloud computing platform can generate a huge amount of data, leading to big performance data. For example, GWP (Googlewide-Profiler) <ref type="bibr" target="#b7">[7]</ref> could generate several GBs of performance data per day. If we treat the data equally, the high event dimensionality (typically &gt; 100) incurs extremely high cost.</p><p>In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of the big performance data with data mining and machine learning techniques. It includes three components: 1) data cleaner, which improves the counter data quality by replacing outliers and filling in missing values after the sampling of MLPX, which is complementary to <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>; 2) importance ranker, which iteratively quantifies, ranks, and prunes events based on their importance with respect to performance; 3) interaction ranker, which quantifies interaction intensity between two events by residual variance.</p><p>We use sixteen benchmarks that eight from CloudSuite <ref type="bibr" target="#b35">[35]</ref> and eight from the Spark version of HiBench <ref type="bibr" target="#b36">[36]</ref> to evaluate CounterMiner. The experimental results show that Counter-Miner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.</p><p>Moreover, CounterMiner reveals a number of interesting findings: 1) the event of stall cycles due to instruction queue full is the most important event for most cloud programs; 2) the branch related events interact with other events the most strongly; 3) there is a 'one-three significantly more important' law ('one-three SMI' for short) which concludes that generally one to three events of a benchmark are significantly more important than others with respect to performance; 4) a number of noisy events of a modern processor can be definitely removed; 5) there are common important events related to branches, TLBs (instruction, data, and second-level TLBs), and remote memory and remote cache operations; 6) the eight Spark benchmarks from HiBench surprisingly show more diversity than those from CloudSuite when we look at the top ten important events. These findings are valuable to guide cross-layer performance optimizations of architecture and applications of cloud systems.</p><p>The rest of this paper is organized as follows. Section II discusses the background and motivation. Section III presents our CounterMiner framework. Section IV describes the experimental methodology. Section V provides the results and analysis. Section VI discusses the related work, and Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION A. Hardware Counters</head><p>Every modern processor has a logical unit called Performance Monitoring Unit (PMU) which consists of a set of hardware counters. A counter counts how many times a certain event occurs during a time interval of a program's execution. The number of counters may vary across microarchitectures or even processor modes within the same microarchitecture. For example, ARM Cortex-A53 MPCore processor has six counter registers <ref type="bibr" target="#b2">[2]</ref> while recent Intel processors have three fixed counters (which can only measure specific events such as clock cycles and retired instructions) and eight programmable counters per core (four per SMT thread if it is enabled) <ref type="bibr" target="#b1">[1]</ref>.</p><p>The events that can be measured by hardware counters are predefined by processor vendors. The number of events may also be significantly different for different generations of processors within the same microarchitecture, let alone different microarchitectures. For instance, the basic Ivy-Bridge model defines only 338 events whereas the high-end IvyTown chips support 1423 events <ref type="bibr" target="#b29">[29]</ref>. In summary, the number of events greatly outnumbers that of hardware counters (more than one hundred vs. less than ten).</p><p>There are two ways to program hardware counters to capture events. In the one counter one event (OCOE) approach, one hardware counter is only programmed for counting one event during the whole profiling period of a program. OCOE is accurate because one counter is dedicated to one event at a time. However, OCOE is ineffective because one usually needs to measure a large number of events to identify the root causes of performance bottlenecks for an unknown system <ref type="bibr" target="#b37">[37]</ref>. The number of events that can be simultaneously measured in OCOE is equal to or less than that of the available hardware counters of a processor.</p><p>Multiplexing (MLPX) is developed to improve the measurement efficiency by letting multiple events timeshare a single hardware counter <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b38">[38]</ref>. In MLPX, events are scheduled to be sampled during a fraction of execution. Based on the samples, the full behavior of each event is extrapolated <ref type="bibr" target="#b30">[30]</ref>.</p><p>However, large measurement errors occur with MLPX because information may be lost when the event does not happen during a sampled interval <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> but happens during a un-sampled interval. Mathur et al. <ref type="bibr" target="#b38">[38]</ref> reported that higher than 50% of errors were observed when the SPEC CPU 2000 benchmarks were profiled with MLPX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motivation</head><p>1) Measurement Errors: while some prior works such as <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref> employ OCOE to measure performance, MLPX is mandatory when a large number of events need to be sampled, e.g., for emerging workloads in cloud computing. Moreover, quantifying measurement errors is a fundamental challenge due to the sampling nature of MLPX.</p><p>We conduct the following experiments to observe the errors caused by MLPX. We firstly run a set of cloud computing programs and measure their events by OCOE. Since different runs of the same program in cloud computing platforms may take different times, which is caused by the non-deterministic nature of modern operating systems, the different time series for the same event may have different lengths. The traditional approaches such as calculating the Euclidean or Manhattan distance between two vectors cannot calculate the performance behavior differences <ref type="bibr" target="#b39">[39]</ref> because they require the two vectors have the same length. In order to compute the distance (difference) between two time series, we must "wrap" the time axis of one (or both) sequences to achieve a better alignment. Dynamic time wrapping (DTW) <ref type="bibr" target="#b40">[40]</ref> is a technique for efficiently achieving this wrapping. It employs a dynamic programming approach to align one time series to one another so that the distance measurement is minimized. We therefore employ DTW to calculate the distance between two event time series as follows:</p><formula xml:id="formula_0">dist = DT W (S 1 , S 2 )<label>(1)</label></formula><p>where S 1 and S 2 are the first and second time series for the same event, respectively. Note that the length of S 1 is not necessarily equal to that of S 2 . First, we calculate the DTW between two time series collected by OCOE, denoted by dist ref .</p><formula xml:id="formula_1">dist ref = DT W (S ocoe1 , S ocoe2 )<label>(2)</label></formula><p>where S ocoe1 and S ocoe2 are the two time series for a certain event of the same program collected by OCOE. Due to the accuracy of OCOE and non-deterministic nature of OS, dist ref is a nonzero value but is theoretically close to zero. Second, we compute the DTW between one time series collected by MLPX and one by OCOE, represented by dist mea .</p><formula xml:id="formula_2">dist mea = DT W (S mlpx , S ocoe )<label>(3)</label></formula><p>where S mlpx and S ocoe are the time series collected by MLPX and OCOE, respectively. S mlpx and S ocoe are for the same program with the same event. Theoretically, dist mea is larger than dist ref due to MLPX. Finally, we define the error caused by MLPX as follows:  average error achieves 28.3%. For other events, we observe similar or even higher errors. By carefully analyzing the errors, we find two root causes: outliers and missing values. Figure <ref type="figure" target="#fig_1">2</ref> (a) shows an example of outliers. In the end of the time series of event IDQ.DSB UOPS collected by MLPX, the number of UOPS is 4.2? of the normal numbers collected by OCOE. Such outliers will significantly "improve" the overall results if they are taken into account. On the other hand, Figure <ref type="figure" target="#fig_1">2</ref> (b) illustrates an example of missing values. By using OCOE, we observe a large number of instruction cache misses at the beginning of the time series of event ICACHE.MISSES. This is reasonable because at the beginning of a program execution, the instruction cache is empty (a.k.a "cold cache") and a large number of misses must happen. However, these instruction cache misses are not observed by MLPX. These examples indicate that MLPX may indeed cause large errors. Moreover, simultaneously measuring more events in MLPX generally exacerbates the problem, as shown in Figure <ref type="figure">3</ref>. These errors, however, are difficult to be removed by event scheduling <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> and estimation algorithms <ref type="bibr" target="#b38">[38]</ref>during the sampling procedure. This motivates us to reduce them by data cleaning techniques after the sampling procedure of MLPX, which is very important for mining the CPU big performance data in an off-line manner.</p><formula xml:id="formula_3">error = |1 - dist ref dist mea | ? 100%<label>(4)</label></formula><p>2) Are all events equally important?: Due to the large number of measurable events compared to the available hardware counters (e.g., 207? for HaswellX <ref type="bibr" target="#b29">[29]</ref>), MLPX is therefore mandatory, but it comes at a high cost. Figure <ref type="figure">3</ref> shows that the MLPX errors increase with more events measured simultaneously with the limited hardware counters (red line indicates the trend). To lower the errors, the number of events measured by the same counter during a program's execution needs to be limited. However, this makes the same program need to run many times to measure all the events of the processor being used.</p><p>On the other side, measuring all events may not be necessary. First, even if we measure all the events accurately in an ideal scenario at the same time, most values are zeros, which is inconceivable. Second, the optimization overhead would be extremely high when considering all the events of a processor. In fact, to improve performance, an OS can only leverage a small number of events to provide feedback to a runtime system for optimization <ref type="bibr" target="#b29">[29]</ref>.</p><p>Therefore, it is crucial to identify a subset of events that are The number of events collected simultaneously Fig. <ref type="figure">3</ref>. The error variation with the number of events (represented by the numbers along with X axis) measured simultaneously. strongly relevant for a given architecture or workload <ref type="bibr" target="#b29">[29]</ref>. It can reduce both measurement and optimization overhead. The key to achieve this goal is to quantify the importance of events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COUNTERMINER METHODOLOGY</head><p>CounterMiner is a methodology designed to mine the big performance data collected from hardware counters. It reduces the measurement errors of MLPX by leveraging data cleaning techniques rather than traditional event scheduling <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> and estimation algorithms <ref type="bibr" target="#b38">[38]</ref>. CounterMiner is complementary to <ref type="bibr" target="#b34">[34]</ref> [33] <ref type="bibr" target="#b38">[38]</ref> because CounterMiner does that after (not during) the sampling. Moreover, it quantifies the importance of events and the interactions between two events with respect to performance. Figure <ref type="figure" target="#fig_2">4</ref> shows the workflow of CounterMiner. It consists of four components: data collector, data cleaner, importance ranker, and interaction ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collector</head><p>The data collector is in charge of sampling the values of events when a program is running. It supports two modes: OCOE and MLPX. The data collector can be any available counter profiling tools such as Perf <ref type="bibr" target="#b41">[41]</ref> and Permon2 <ref type="bibr" target="#b25">[25]</ref>. In this paper, we use the Linux Perf <ref type="bibr" target="#b41">[41]</ref> because it is available in all Linux distributions.</p><p>We take the sampled values of an event as a time series since it is important to observe the time varying behaviors of the event <ref type="bibr" target="#b30">[30]</ref>. We formally describe the time series as follows:</p><formula xml:id="formula_4">T S ei = {V i1 , V i2 , ..., V ij , ..., V in } (5)</formula><p>where T S ei is the time series for the i th event of a program, V ij is the j th value of the i th event, and n is the total number of sampled values for T S ei . The important feature as well as challenge of these time series is that their lengths may be significantly different even for the same event of the same program because of the non-deterministic behavior of a modern OS. This property makes storing and analyzing these time series challenging. We store the collected time series in a database management system (DBMS) which can be any popular DBMS such as MySQL and SQL Server. In CounterMiner, we employ SQLite because it seamlessly integrates with Python which is the program language we used for data analysis. In the database, we design a two-level table organization. The first level tables store information including the name of a program, the names of the measured events, the execution times of the program, and the names of the second-level tables. The second-level tables store the time series for the measured events for a program at each run. Note that these two level tables may need to be re-initialized when CounterMiner is applied on a different microarchitecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Cleaner</head><p>The outliers and missing values of events are the main error sources of the collected data with MLPX. Data cleaner replaces the outliers by normal values and fills in the missing values. To evaluate the accuracy of the data cleaner, we take the values of events collected by OCOE as golden references.</p><p>1) Replacing Outliers: To determine outliers, we first perform a rigorous statistic testing (see Section IV-C for the testing tool) on the value distributions of all 229 events. We find that, only for 100 events, their values follow Gaussian distribution. We further investigate the value distributions of the other 129 events and find they all show long tail distribution but with different levels. In order to find out the long tail distributions, we perform the statistic testing on them using several different distribution functions such as logistic and Gumbel (general extreme value -GEV) distributions. We find the GEV distribution best fits the long tail distributions, which has been confirmed by <ref type="bibr" target="#b15">[15]</ref>.</p><p>After knowing the value distributions of the events, we design the criterion to replace the outliers. We employ a general technique from data mining to determine outliers by the following equation:</p><formula xml:id="formula_5">threshold = mean + n ? std (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where threshold is the criterion for replacing outliers, mean and std are the mean value and the standard deviation of a series of values of an event, respectively; n is a control variable which needs to be determined according to the distribution or user requirements such as the percentage of data within the threshold. According to <ref type="bibr" target="#b42">[42]</ref>, if a data series obeys Gaussian distribution, n equals 3.</p><p>Since the values of a large number of events do not obey the Gaussian distribution but instead long tail distribution. We need to determine n by controlling the percentage of data within the threshold. In this study, we specify that 99% of the collected data for events of a program are within the threshold because there are not many outliers based on our confirmed observation. Table <ref type="table">I</ref> shows the percentage of data within the threshold with different n values. We see that when n is 5, the percentages of data within the threshold for all programs exceed 99%, we therefore set n to 5.</p><p>When an outlier is found, we use the median value of the interval where the outlier locates to replace the outlier. The interval length is calculated as follows. with Max(T S ei ) and Min(T S ei ) the maximum and minimum values in T S ei , respectively, Count(T S ei ) the number of values in T S ei , Sqrt the square root, and Roundup the round up value.</p><formula xml:id="formula_7">L = Max(T S ei ) -Min(T S ei ) Roundup(Sqrt(Count(T S ei )), 0)<label>(7)</label></formula><p>2) Filling in Missing Values: To fill in missing values, we first classify the event values into two categories: zero values and none-zero values. Based on our observation, zero values are highly likely to be missing values. However, it is still possible that the values of a certain event at some points are indeed zeros. It is therefore difficult to distinguish them from the missing values. To address this issue, we check the maximum and minimum values of the event in the past. If the minimum value is zero and the maximum value is less than 0.01, we consider that the zero value for the event is not a missing value. The rationale is that the maximum value of an event is only 0.01 which is very close to zero, and even the actual value is not zero, the error would not be high.</p><p>For the none-zero value category, we employ KNN (K-Nearest Neighbor) algorithm <ref type="bibr" target="#b43">[43]</ref> to fill in missing values. For example, a series of data for ICACHE.MISSES is as follows:</p><formula xml:id="formula_8">{X 1 , X 2 , X 3 , X 4 , X 5 , 0, X 7 , ..., X i , ..., X m } (8)</formula><p>where X i is the i th value of ICACHE.MISSES, m is the total number of values, and 0 is the missing value. We use KNN regression to calculate the missing value and it is represented by the average value of the k nearest neighbors. The determination of k is important because it affects the accuracy. In this study, we tried several values from 3 to 8 based on <ref type="bibr" target="#b42">[42]</ref> and find that k = 5 is accurate enough to represent the missing value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Importance Ranker</head><p>In order to quantify the importance of events with respect to IPC (Instructions Per Cycle), an accurate performance model needs to be constructed. The inputs of the model are event values and the output is IPC, which can be represented by:</p><formula xml:id="formula_9">IP C = perf (e 1 , e 2 , ..., e i , ..., e n ) (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where e i is the value of the i th event, and n is the total number of events. As discussed before, n is typically larger than 100 and up to 1423 for modern processors. For the processors used in this paper, n is 229. It is extremely challenging to build an accurate performance model with such a large number of input parameters. Analytical modeling techniques are not suitable for this case because they allow only a few parameters (e.g., less than 5). Moreover, analytical models are not accurate when dealing with complex cases. Statistical modeling techniques are either not accurate with high dimensional inputs because they make an unrealistic assumption that the relationship between the input parameters are linear. However, complex nonlinear relationships typically exist between events of modern processors.</p><p>To solve this problem, machine learning techniques are applied to construct accurate models with high dimensional inputs. To mitigate over-fitting, we use an ensemble learning algorithm, Stochastic Gradient Boosted Regression Tree (SG-BRT) <ref type="bibr" target="#b44">[44]</ref>, to construct the model. The key insight is that SGBRT combines a number of tree models in a stagewise manner, where each one reflects a part of the performance. The final model is called ensemble model. With performance model constructed, we can leverage the model to quantify the importance of events and their interactions. Clearly, a more accurate performance model results in more precise event importance quantification.</p><p>For a single tree T in an ensemble model, one can use I 2 j (T ) as a measure of importance for each event e j , which is based on the number of times e j is selected for splitting a tree weighted by the squared improvement to the model as a result of each of those splits <ref type="bibr" target="#b45">[45]</ref>. This measure of importance is calculated as follows:</p><formula xml:id="formula_11">I 2 j (T ) = nt ? nt i=1 P 2 (k),<label>(10)</label></formula><p>where nt is the number of times e j is used to split tree T , and P 2 (k) is the squared performance improvement to the tree model by the k th split. In particular, P (k) is defined as the relative IPC error which is</p><formula xml:id="formula_12">(IP C k -IP C k-1 )/IP C k-1</formula><p>after the k th split. If e j is used as a splitter in R trees in the ensemble model, the importance of e j to the model equals:</p><formula xml:id="formula_13">I 2 j = 1 R R m=1 I 2 j (T m ).<label>(11)</label></formula><p>To make the results intuitive, the importance of an event is normalized so that the sum across all events adds up to 100%. A higher percentage indicates stronger influence of the event on performance.</p><p>After the importance of each event with respect to performance is obtained, we rank them in a descending order. Then, we remove the 10 least important events and take the remainders as input parameters to construct a performance model by using the SGBRT algorithm again. We conduct the same procedure on the new model to quantify the importance of the remaining events and rank them. This procedure may iteratively repeat several times until we obtain the Most Accurate Performance Model (MAPM) for a program and we call this procedure Event Importance Refinement (EIR). The event importance obtained by MAPM is the most accurate <ref type="bibr" target="#b45">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interaction Ranker</head><p>After we obtain a set of important events, we construct a linear regression model per pair of the events and consider the residual variance of the model as an indication for interaction intensity. The intuition is that if two microarchitecture events are orthogonal (i.e., they do not interact), the residual variance will be small because the linear model will be able to accurately predict the combined effect of both events. If on the other hand, the microarchitecture events interact substantially, this will be reflected in the residual variance being significantly larger than zero, because the linear model is unable to accurately capture the combined effect of the event pair. The linear regression model is trained for each pair of important events by setting the values of all other events to their respective means. This process is repeated for each possible event pair. The residual variance or interaction intensity for a particular event pair is computed as:</p><formula xml:id="formula_14">v = n i=1 (p i -p) 2 , (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>where p i is the performance (IPC) predicted by the linear regression model, p is the observed performance, and n is the number of predictions. Zero indicates that there is no interaction between two microarchitecture events, and a higher value indicates a stronger interaction.</p><p>To indicate the importance of interactions among all possible pairs, we normalize interaction intensity against the other pairs as follows:</p><formula xml:id="formula_16">I i = ? ? ? ? v i n j=1 v j ? ? ? ? ? 100%,<label>(13)</label></formula><p>where I i is the importance of the i th event-pair interaction and v i is the i th event-pair interaction intensity. After the normalization, we can tell how much more/less important an event pair is compared to another event pair, -reflecting the relative interaction intensity of event pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Cluster</head><p>Our experimental cluster consists of four Dell servers, one serves as the master node and the other three serve as slave nodes. Each server is equipped with 12 Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz eight-core processor with Haswell-E microarchitecture and 64GB PC3 memory. The OS of each node is SUSE Linux Enterprise Server 12. The OS of the whole cluster is Mesos1.0. Although our experimental cluster is small compared to a cloud platform, we believe that evaluating CounterMiner in this environment is still sufficient, because it is essentially a performance analysis methodology. In a real cloud platform, CounterMiner can easily work with the Google Wide profiler (GWP) to provide more meaningful results. In addition, it can be integrated with cluster management tools such as Quasar <ref type="bibr" target="#b46">[46]</ref> and other cloud computing researches such as <ref type="bibr" target="#b47">[47]</ref>- <ref type="bibr" target="#b49">[49]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarks</head><p>We employ CloudSuite 3.0 <ref type="bibr" target="#b35">[35]</ref> and select eight programs from HiBench with version of Spark 2.0 <ref type="bibr" target="#b36">[36]</ref> (a.k.a "Spark-Bench") to evaluate CounterMiner in this study. CloudSuite 3.0 is a benchmark suite for cloud services and it consists of eight applications based on their popularity in today's datacenters. HiBench with Spark 2.0 consists of a broad set of MapReduce-like programs implemented by Spark 2.0. The benchmarks are listed in Table <ref type="table">II</ref>. The benchmarks in Cloud-Suite use different frameworks while the ones in HiBench employ the same framework but represent four categories of applications including websearch, SQL, machine learning, and microbenchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Modeling Tools</head><p>We use Python, a freely available high-level programming language, to perform our SGBRT modeling and KNN modeling. The Python version is 2.7 and we use scikit-learn 0.19.0 which is a machine learning algorithm library implemented in Python to construct our performance models. To perform a rigorous statistic testing for the value distribution of events, we employ SciPy <ref type="bibr" target="#b50">[50]</ref> which is an open-source software for mathematics, science, and engineering. In which, we use  The number of events collected simultanesously RAW Fig. <ref type="figure">7</ref>. The measurement error comparison between before and after our data cleaning approach is employed when different number of events are measured simultaneously by MLPX.</p><p>scipy.stats.anderson to perform Anderson-Darling test for data coming from a particular distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Error Reduction</head><p>Figure <ref type="figure">5</ref> shows the cleaning results for the outlier and missing value examples shown in Figure <ref type="figure" target="#fig_1">2</ref>. MLPX-CLN represents the cleaned times series for the corresponding events. The benchmark is wordcount in these two examples. Figure <ref type="figure">5 (a)</ref> illustrates that the outliers are correctly replaced and Figure <ref type="figure">5</ref> (b) shows that most missing values are filled in.</p><p>Figure <ref type="figure">6</ref> compares the measurement errors before (blue bars) and after (red bars) applying our data cleaning techniques on the times series of ICACHE.MISSES for the sixteen benchmarks. We see that our data cleaner significantly reduces the errors caused by MLPX. In particular, the average error is reduced from 28.3% to 7.7% thanks to data cleaning.</p><p>Figure <ref type="figure">7</ref> illustrates the behavior of the data cleaner when we increase the number of events measured by MLPX at a time. We made several interesting observations. 1) The data cleaner significantly reduces the errors caused by MLPX in each case. With 10 events, the error is reduced to only 5.3%. 2) The data cleaner accurately follows the error trend when number of simultaneously measured events increases. 3) Although no error is larger than 30% in all cases, some errors are high such as 23.6% in the case of 24 events. This indicates that we can not measure too many (e.g., 24) events at the same time even with data cleaner. As a general recommendation, the number should not be larger than 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Important Events</head><p>As mentioned in Section III-C, the event importance obtained by MAPM (the most accurate performance model) is the most accurate <ref type="bibr" target="#b45">[45]</ref>. We therefore perform the EIR (event importance refinement) procedure aiming to get MAPM before  </p><formula xml:id="formula_17">err = |IP C meas -IP C pred | IP C meas ? 100%<label>(14)</label></formula><p>where IP C meas is the measured IPC of a program and IP C pred is the IPC predicted by the performance model. Figure <ref type="figure" target="#fig_4">8</ref> show the error variation when we perform the EIR for HiBench benchmarks. We see that considering more events may not necessarily result in higher performance model accuracy. For the experimented processors and benchmarks, the average error is 14% when we take all 229 events as the model inputs, while the lowest average error is only 6.3% when around 150 events are taken as the model input parameters. This indicates that the exhausted list of events of modern processors may contain a large number of noisy events. Processor vendors could leverage the proposed approach to systematically select proper events for their processors.</p><p>However, the accuracy of performance models decreases when we further reduce the number of input events of the models after they achieve the highest accuracy (e.g., 150 events in this study), as shown in Figure <ref type="figure" target="#fig_4">8</ref>. When the number of events decreases to 99, the average performance model error increases to 9.6%, but is still very low. When we decrease the number to 59, the average error further increases to 14% which is the same as that of models with all 229 events. This implies that using only 59 events can achieve the same results of performance analysis by using 229 events. The benchmarks from CloudSuite show the similar results. We therefore do not show the figure due to limited space.</p><p>Figure <ref type="figure">9</ref> and 10 show the importance ranking of events obtained by MAPM for the benchmarks from HiBench and from CloudSuite, respectively. The Y axis represents event importance and the X axis denotes the abbreviations of events which are shown in Table <ref type="table" target="#tab_6">III</ref>. Due to space limit, we show the 10 most important events for each benchmark.We highlight four key findings as follows.</p><p>First, the importance of one to three events of a benchmark is significantly higher than that of other events of the same benchmark, and this is true for all benchmarks,both HiBench and CloudSuite. For example, the three most important events of the benchmark wordcount are ISF, BRE, and ORA. Their importance exceeds 5% while those of the other events are less than 2.2%. We call this phenomenon one-three significantly more important law (one-three SMI law). Note that this is different from Pareto principle because the accumulated importance of 20% of events is not around 80%. The one-three SMI law indicates that there is always an opportunity to optimize cloud programs significantly more efficiently by first tuning the parameters related to the top one to three events than by tuning other ones. We will demonstrate this in a case study in Section V-D.</p><p>Second, the importance rank of events may vary across benchmarks. For instance, the most important event for wordcount is ISF while that for pagerank is BRE. This indicates that different benchmarks have different characteristics at the microarchitecture level.</p><p>Third, the common most important events for all the experimented cloud benchmarks are related to instruction queue, branch, TLBs (instruction TLB, data TLB, and second level TLB), memory load, and remote memory or cache access. The second insight indicates that application-specific tuning is needed at application level whereas the third one indicates that common optimization approaches for different applications are also effective at lower-level, e.g., microarchitecture or compiler. For example, enlarging the length of instruction queue of processors used in cloud or enhancing the performance of the memory sub-system of cloud servers may improve the performance of most cloud services significantly because our results show that ISF (stall cycles due to instruction queue is full) is the most important event for most experimented benchmarks.</p><p>Fourth, we surprisingly find that the eight benchmarks from HiBench show more diversity than those from CloudSuite based on the 10 most important events for each benchmark. Only four important events (MUL, MLL, DSP, and DSH) of CloudSuite benchmarks are not included in those of the eight HiBench benchmarks while thirteen important events (ORA, URA, URS, BRC, BAA, LRC, IMC, IM4, CAC, ORO, IDU, LRA, and OTS) of the HiBench programs are not in those of the eight CloudSuite benchmarks. The common belief is that, the benchmarks from CloudSuite should be more diverse than the ones from HiBench because the CloudSuite benchmarks use different frameworks such as Hadoop, Spark, and MemCached while the HiBench benchmarks only use Apache Spark. Our counter-intuitive results indicate that, to achieve application diversity, different frameworks may not be more important than the algorithms and codes of benchmarks. Moreover, our results indicate that more diverse benchmarks need to be included in CloudSite3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Important Event Interactions</head><p>Figure <ref type="figure" target="#fig_0">11</ref> and Figure <ref type="figure" target="#fig_1">12</ref> show the interaction intensity ranks for the eight Spark benchmarks from HiBench and all the benchmarks from CloudSuite, respectively. Again, we only show the 10 most important interactions of event pairs. The Y axis represents the importance of interaction intensity of event pairs and the X axis denotes the event pairs. Fig. <ref type="figure" target="#fig_0">10</ref>. The importance rank of the eight benchmarks from CloudSuite when we employ the events which can construct the most accurate performance models. Y axis represents the importance of events and the X axis denotes the abbreviations of the events. Fig. <ref type="figure" target="#fig_0">11</ref>. The interaction rank of the important event pairs for the eight Sark benchmarks from HiBench. The Y axis represents the importance of interaction intensity of event pairs. The X axis represents abbreviations of event pairs. XXX-YYY denotes an event pair. Fig. <ref type="figure" target="#fig_1">12</ref>. The interaction rank of the important event pairs for the benchmarks from CouldSuite. The Y axis represents the importance of interaction intensity of event pairs. The X axis represents abbreviations of event pairs. XXX-YYY denotes an event pair. Fig. <ref type="figure" target="#fig_0">13</ref>. The interaction rank of spark configuration parameter and event pairs. The Y axis represents the importance of the interaction of configuration parameter and event pairs and the X axis denotes the abbreviations of the parameter and event pairs. In XXX-YYY, XXX represents an event, and YYY represents a configuration parameter.</p><formula xml:id="formula_18">0% 2% 4% 6% 8% 10% 12% 14% 16% BRB-BMP ORA-BRB URA-URS BRB-ITM ORA-BMP ISF-BRB BRB-URA BRE-BRB ORA-ITM ISF-BRE BRB-BMP BRE-ISF BRE-BRB BRE-BMP ISF-BRB ISF-BMP BRB-BRC BRE-PI3 BRE-ITM ISF-ITM BRE-MSL ISF-MSL MSL-BMP MSL-BAA MMR-BMP ISF-BRE MSL-PI3 BRB-BMP BRB-MSL BRE-BRB BRB-BMP BRE-BRB ISF-BMP ISF-BRB BRE-ISF BRE-BMP LRC-BRB LRC-BMP BRE-IPD BMP-IMC ISF-BMP ISF-LMH BRE-BMP LMH-MMR LMH-BMP BRE-LMH BRE-ISF MMR-BMP ISF-MMR BRE-MMR ISF-MST LRA-MST ORO-MST BRE-MST IDU-MST BMP-LMH LRA-BRE BMP-MST ORO-LRA BRE-MSL ISF-BRB BRE-BRB BRE-ISF PI3-BRB ISF-PI3 BRE-PI3 MSL-MST MMR-LMH BRB-LMH BRE-LMH BRB-BMP ISF-BMP ISF-BRB ITM-BMP BRB-ITM BRE-BRB BRE-BMP PI3-BMP MSL-BMP BRB-PI3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0%</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0%</head><p>First, we see that all benchmarks have one or two dominant pairs of events which interact with each other more strongly than other event pairs. This indicates that we can focus on analyzing the dominant interaction pairs with limited time budget. Second, the branch related events interact strongly with other events. In the 160 most important interaction pairs for the 16 benchmarks (10 most important interaction pairs for each), one branch related event is involved in 98 interaction pairs. Two branches related events are involved in 36 interaction pairs. It means that 83.4% of the 160 most important interaction pairs contain branch related events. These results imply that branch related events are critical for cloud computing environment. Moreover, the pair BRB-BMP (see Table <ref type="table" target="#tab_6">III</ref>) appears in 12 of the 16 experimented benchmarks and it is ranked as the most important interaction pair in 10 benchmarks (wordcount, pagerank, join, kmeans, DataCaching, DataServing, In-memoryAnalytics, Me-diaStreaming, WebSearch, and WebServing). This indicates the number of successfully retired branch instructions (BRB) and that of mispredicted but finally successfully retired branch instructions (BMP) interact strongly in most benchmarks. This is because a small BRB surely results in a small BMP and a large BMP is definitely caused by a large BRB.</p><p>Another interesting phenomenon is that the the events in dominant interaction pairs of benchmarks from CloudSuite interact much more strongly with each other than those in the dominant pairs of benchmarks from HiBench, see Figure <ref type="figure" target="#fig_0">11</ref> and Figure <ref type="figure" target="#fig_1">12</ref>. This indicates that a benchmark containing more software tiers results in stronger interactions between events than a benchmark only implementing algorithms. For example, WebServing has four tiers: the web server, the database server, the memcached server, and the clients. The interaction intensity of its dominant interaction pair achieves 64%. In contrast, GraphAnalytics only implements the pagerank algorithm on a Spark Library GraphX. The interaction intensity of its dominant interaction pair is only 19%.</p><p>Knowing the importance of interaction pairs is important for performance analysis. First, it can explain why one event value changes significantly when the other event value is changed in the same interaction pair. Second, it can explain why performance variation is larger with the change of two event values at the same time than that with the change of one of two event values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Case Study</head><p>This case study shows an usage example of CounterMiner. After we know the important events, we first leverage our interaction intensity quantification approach to determine which configuration parameters of the Spark framework strongly interact with the important events. We then tune two configuration parameters, e.g., A and B, which tightly correlate with a more important and a less important event, respectively. Finally, we observe the performance variation when we tuning the two parameters. Note that the default values of Spark configuration parameters can be found at <ref type="bibr" target="#b51">[51]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">13</ref> shows the importance of interactions between a Spark configuration parameter and an event. The Spark configuration parameter names and their abbreviations are shown in Table <ref type="table">IV</ref>. For each benchmark, we see that there exists one or two pairs of a Spark configuration parameter and an event whose interaction intensities are much stronger than other pairs. This implies that we should tune the configuration parameter in the strongest interaction pair first, which more likely leads to more performance gain. Second, the most important pair of interaction between a Spark configuration parameter and an event varies across benchmarks. It is because of different characteristics between benchmarks and indicates that different configuration parameters should be tuned first for different programs for efficiently optimizing performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head><p>Fig. <ref type="figure" target="#fig_0">15</ref>. An illustration of profiling times.c i denotes the value of the i th configuration parameter; t represents the execution time of a benchmark; vm denotes the m th value of a certain event.</p><p>We study an example to demonstrate how to optimize the performance of Spark programs by using our event importance quantification. As shown in Figure <ref type="figure" target="#fig_0">13</ref>, the most important interaction pair of benchmark sort is ORO-bbs and Figure <ref type="figure">9</ref> shows that ORO is the most important event of sort. Looking at Table IV, we know bbs corresponds to spark.broadcast.blockSize. We then choose an event not in the 10 most important event list and find the Spark configuration parameter that is tightly correlated with it. In this example, we choose I4U and the corresponding configuration parameter is nwt (spark.network.timeout). We tune spark.broadcast.blockSize and spark.network.timeout separately and compare the performance variation. Figure <ref type="figure" target="#fig_2">14</ref> shows the results. We see that, the execution time reduction is significantly larger when tuning bbs than tuning nwt. Specifically, average execution time variation is 111.3% when tuning bbs while that is only 29.4% by tuning nwt. These results confirm that CounterMiner can indeed provide the "handle" to users to optimize performance more quickly and efficiently.</p><p>Nevertheless, one may think that it is unnecessary to identify the important parameters of Spark programs by using our event importance quantification (method A). Instead, one can quantify the importance of parameters directly by using our importance ranker (method B). We argue that it is untrue, because method B takes much longer time than method A. We use Figure <ref type="figure" target="#fig_0">15</ref>  Then we identify the important configuration parameters. For method A, we only run the benchmark 60 times to build a performance model as a function of events with 90% of accuracy. To find the tightly coupled configuration parameter and event pairs, we need to additionally run the benchmark 1520 times. Therefore, we need to run pagerank 1580 times in total to identify its important configuration parameters by method A, which is nearly only 1/4 as the time needed for method B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Co-located Workloads</head><p>We now demonstrate how to use CounterMiner with colocated benchmarks running on a shared cluster, which is a typical scenario in cloud computing environment. We consider two cases. 1) An application is submitted to the cluster to run when the same application is running. 2) An application is submitted to the cluster to run when another application is running. These are two typical cases people use cloud platforms. In this study, we use 'DataCaching + DataCaching' and 'DataCaching + GraphAnalytics' to demonstrate the first and second case, respectively.</p><p>Figure <ref type="figure" target="#fig_0">16</ref> shows the importance ranking of events for the two cases. Note that CounterMiner can not show the importance ranking for individual benchmarks in this context because hardware counters and events are shared resources among co-located benchmarks. The left part shows the importance ranking of events for 'DataCaching + DataCaching'. Compared with Figure <ref type="figure">9</ref>, we see that the most important event is still ISF with the similar importance of 3.7%. However, the importance order and events in the other top 9 important event list of 'DataCaching + DataCaching' are only slightly different from those of 'DataCaching'. This indicates that two 'DataCaching' programs do not interfere with each other severely.</p><p>In contrast, we see that the importance order of events and events of 'DataCaching + GraphAnalytics' are significantly different from both of those of 'DataCaching' and 'Graph-Analytics'. This indicates that 'GraphAnalytics' churns the execution of 'DataCaching' severely. More interestingly, 6 L2 cache related events are ranked in the top 10 important event list for 'DataCaching + GraphAnalytics'. No L2 cache related events have been in the 10 most events for both 'DataCaching' and 'GraphAnalytics'. This indicates that the mixed running of these two benchmarks cause a lot of L1 cache misses for both instruction and data caches, which should be avoided.</p><p>As observed above, we see that CounterMiner can capture not only significant churns but also small ones in the context of co-located workloads running in cloud platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Counter Data Management and Analysis</head><p>Google recently developed a profiling infrastructure, named Google-Wide Profiling (GWP), to provide performance insights for cloud applications <ref type="bibr" target="#b7">[7]</ref>. GWP employs a two-level sampling technique (sample machines and sample time intervals within a machine for profiling) to collect counter data in Google data centers. Huck et al. first developed a framework to manage performance data <ref type="bibr" target="#b28">[28]</ref> and then proposed to leverage statistics techniques such as clustering to analyze the performance data of parallel machines <ref type="bibr" target="#b52">[52]</ref>. Dong et al. proposed to use statistical techniques such as PCA (Principle Component Analysis) to extract important features from performance counters <ref type="bibr" target="#b53">[53]</ref>. CounterMiner differs from these studies with twofold: 1) CounterMiner proposes data cleaning techniques to clean the counter data collected by MLPX; 2) CounterMiner not only extracts important events but also directly quantifies the importance of an event with respect to performance. The related studies that use PCA or random linear projection can implicitly tell the important events as a form of principle components or projected metrics but can not explicitly quantify how important an event is. This hinders one to directly leverage the important events to optimize application performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Error Reduction</head><p>The measurement errors caused by MLPX have been observed for nearly two decades <ref type="bibr" target="#b29">[29]</ref>- <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b54">[54]</ref>- <ref type="bibr" target="#b56">[56]</ref> and several approaches were proposed to reduce them. In MLPX, the values of unsampled time intervals of an event are usually estimated by linear interpolating a value between the predecessor and successor intervals. Mathur et al. tried to develop a fine-grained estimation algorithm which divides the time interval into several sub-intervals. They found that performing a linear interpolation estimation for each sub-interval results in better accuracy <ref type="bibr" target="#b38">[38]</ref>. Weaver et al. found that experimental setup significantly affects the accuracy of measurements by hardware counters and they therefore provided corresponding suggestions to reduce the errors <ref type="bibr" target="#b31">[31]</ref>.</p><p>Recently, Lim et al. propose a scheduling algorithm that schedules n events on m counters (n &gt; m) (vs. the traditional round-robin algorithm) to improve the measurement accuracy <ref type="bibr" target="#b34">[34]</ref>. The key idea is to monitor the most recent three values of an event for determining whether another event should be scheduled to monitor on a counter. If the values of an event are not significantly different, another event will be scheduled and vice-versa. Dimakoupoulou et al. found that the measurement error of MLPX increases when Intel hyper-thread is enabled. They then proposed a dynamic event scheduling algorithm based on graph matching to reduce the errors <ref type="bibr" target="#b33">[33]</ref>. These studies try to reduce errors before or during the performance measurement. In contrast, our approach decreases the errors after the performance measurement has been completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Counter Applications</head><p>1) Workload Characterization: Recently, Kanev et al. leveraged hardware counters to profile a warehouse-scale computer <ref type="bibr" target="#b14">[14]</ref> and they found a number of interesting observations. For example, the instruction locality of emerging cloud computing workloads is getting weaker and therefore the instruction cache needs to be redesigned. Ferdman et al. employed hardware counters to characterize a group of scale-out workloads and released the CloudSuite <ref type="bibr" target="#b9">[9]</ref>. Later on, Yasin et al. performed a deep characterization by using hardware counters for the CloudSuite <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>. Jia et al. use performance counters to characterize data analysis workloads in datacenters <ref type="bibr" target="#b8">[8]</ref>. Wang et al. characterized big data workloads for internet services <ref type="bibr" target="#b10">[10]</ref>. Xiong et al. employed performance counters to characterize the big data analysis in city transportation industry and they proposed a transportation big data benchmark suite <ref type="bibr" target="#b13">[13]</ref>.</p><p>2) Architecture and Compiler Optimization: Kozyrakis et al. employ hardware counters and other tools to analyze how large-scale online services use resources in data centers and then they provide several insights for server architecture design in data centers <ref type="bibr" target="#b22">[22]</ref>. Chen et al. leveraged hardware-event sampling to generate edge profiles to perform feedback-directed optimization for application runtime performance <ref type="bibr" target="#b19">[19]</ref>. Moseley et al. used hardware counters to optimize compilers and show speedups between 32.5% and 893% on selected regions of SPEC CPU 2006 benchmarks <ref type="bibr" target="#b20">[20]</ref>.</p><p>3) Application Optimization: Chen et al. used hardware counters to observe the cache behavior and then proposed a task-stealing algorithm for multisocket multicore architectures <ref type="bibr" target="#b5">[5]</ref>. Blagodurov et al. developed a user level scheduling algorithm for NUMA multicore systems under Linux by analyzing information from hardware performance counters <ref type="bibr" target="#b6">[6]</ref>. By observing the CPI (Cycle Per Instruction) collected from hardware counters, Zhang et al. proposed a CPU performance isolation strategy for shared compute clusters <ref type="bibr" target="#b15">[15]</ref>. Tam et al. firstly carefully analyzed the L2 cache miss rate behavior of commodity systems and subsequently proposed an algorithm to approximate the L2 miss rate curves which can be used for online optimizations <ref type="bibr" target="#b16">[16]</ref>. He et al. proposed to leverage fractals to approximate the L2 miss rate curves with much lower overhead <ref type="bibr" target="#b17">[17]</ref>. Based on hardware counters, Blagodurov et al. proposed an algorithm to manage the contention of NUMA multicore systems <ref type="bibr" target="#b18">[18]</ref>.</p><p>Although CounterMiner does not focus on workload characterization and optimization for architectures, compilers, and applications, it provides an important step stone toward these goals. After CounterMiner cleans the performance counter data, these approaches can achieve better results. After Coun-terMiner quantifies the importance of microarchitecture events, these approaches can be more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>This paper proposes CounterMiner, a methodology that enables the measurement and understanding of the big performance data with three novel techniques: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking and pruning events based on the importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. For various applications, experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. The real-world case study shows that identifying important parameters of Spark programs by event importance is much faster than directly ranking the importance of parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The measurement errors caused by MLPX. WDC-wordcount, PGRpagerank, AGG-aggregation, JON-join, SCN-scan, SOT-sort, BAY-Bayes, KME-kmeans, DAA-DataAnalytics, DAC-DataCaching, DAS-DataServing, GPA-GraphAnalytics, IMA -In-memoryAnalytics, MES-MediaStreaming, WSH-WebSearch, WSG-WebServing. AVG-average error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The outlier and missing value examples in benchmark WordCount. (a) The outliers in the time series of event IDQ.DSB UOPS (the # of uops delivered to Instruction Decode Queue from the Decode Stream Buffer path). (b) The missing values in the time series of event ICACHE.MISSES (the # of instruction cache misses per 1K instructions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Block Diagram of CounterMiner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. The event cleaning examples. (a) The replaced outliers in the time series of IDQ.DSB UOPS. (b) The filled in values in the time series of ICACHE.MISSES. MLPX-CLN represents the cleaned time series. 0% 10% 20% 30% 40% 50%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The error variation of the models when we reduce the number of events used as their inputs for HiBench benchmarks. The X axis represents the numbers of events. we show the results for important microarchitecture events. During EIR, we employ a number of training examples (m) to train the model and use one-quarter of m unseen test examples to evaluate the model accuracy. The error of the models is defined as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>of u ops(x10 3 ) measurement samples</head><label></label><figDesc>This error definition roughly quantifies how close the time series generated by MLPX is to the one obtained by OCOE. Since S ocoe1 and S ocoe2 are both collected by OCOE for the same program with the same event, dist ref is a very small value that is close to zero. In contrast, dist mea is larger than dist ref . Thus, error reflects the error caused by MPLX.</figDesc><table><row><cell>0 0.5 1 1.5 2 2.5 3 3.5 # OCOE 1 39 77 115 153 191 229 267 305 343 MLPX outliers</cell><cell>381</cell><cell>419</cell><cell># of misses</cell><cell>0 10 20 30 40 50 60</cell><cell>1</cell><cell>33</cell><cell>65</cell><cell>measurement samples 97 129 161 193 225 257 289 321 353 OCOE MLPX missing values</cell><cell>385</cell><cell>417</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>Figure 1 shows that the errors caused by MLPX for the event</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ICACHE.MISSES for 16 programs (the program description</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>is presented in Section IV-B). We can see that the minimum</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and maximum errors are 8.8% and 43.3%, respectively. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III EVENT</head><label>III</label><figDesc>NAME AND DESCRIPTION FOR THOSE APPEARED IN THE 10 MOST IMPORTANT EVENT LIST OF EACH BENCHMARK.</figDesc><table><row><cell>error</cell><cell>0% 10% 20% 30% 60% 50% 40%</cell><cell>37% 5.30%</cell><cell>35% 17.08%</cell><cell>41% 6.80%</cell><cell cols="2">55% 23.61% 28.96% 50%</cell><cell>44% 13.38% 54% 29.39%</cell></row><row><cell></cell><cell>10</cell><cell>16</cell><cell>20</cell><cell>24</cell><cell>28</cell><cell>32</cell><cell>36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The importance rank of the eight Spark benchmarks from Hibench when we employ the events which can construct the most accurate performance models. Y axis represents the importance of events and the X axis denotes the abbreviations of the events.</figDesc><table><row><cell>0% 1% 2% 3% 4% 5% 6%</cell><cell cols="4">ISF BRE ORA IPD BRB BMP MSL URA URS ITM BRE ISF BRB LMH BMP ITM PI3 MCO BRC TFA ISF BRE BRB MSL BAA MMR PI3 BMP IPD MCO BRE LRC ISF BRB LMH IPD BMP IMC IM4 ITM BRE ISF LMH BRB MSL PI3 MMR BMP MIE CAC ORO IDU ISF LRA BRE BRB BMP LMH MSL MST BRE ISF PI3 MSL BRB IPD MST TFA MMR LMH ISF BRE IPD BRB IMT MSL PI3 OTS BMP MCO wordcount pagerank aggregation join scan sort bayes kmeans 6.1% 6.7% 6.6% 7.6%</cell></row><row><cell>2% Fig. 9. 0% 4% 6%</cell><cell>DataAnalytics</cell><cell>DataCaching</cell><cell>DataServing GraphAnalytics In-mAnalytics MediaStreaming WebSearch 6.4%</cell><cell>WebServing</cell></row><row><cell></cell><cell cols="4">ISF BRB BRE IPD MMR MSL LMH MUL MST MLL ISF BRB IPD BRE MSL BMP MMR LMH MST MLL ISF PI3 BRE BRB IPD MMR MSL LMH ITM BMP ISF BRE BRB MSL DSP TFA MMR DSH MST BMP BRE ISF BRB MSL IPD MMR BMP PI3 LMH MLL BRE ISF BRB MMR IPD MSL LMH BMP MCO PI3 ISF MSL IPD BRE MMR BMP BRB MST LHN MLL MSL ISF BMP MMR LHN IPD ISL BRE MLL LMH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Fig. 14. Execution time (in second) optimization for sort by tuning bbs and nwt. bbs tightly correlates with the most important event (ORO) of sort. nwt tightly correlates with the less important event I4U. SeeTable III for bbs and nwt, Table IV for ORO and I4U.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">THE NAMES AND ABBREVIATIONS OF SPARK CONFIGURATION</cell></row><row><cell cols="13">PARAMETERS THAT ARE INTERACT WITH THE IMPORTANT EVENTS</cell></row><row><cell>0 50 100 150 200 250 300 exec time (s)</cell><cell cols="7">STRONGLY. 2M 4M 8M 16M 32M bbs 40 0 80 120 160 200 exec time (s)</cell><cell cols="5">50 100 150 200 250 300 350 400 450 500</cell></row><row><cell>t</cell><cell>v 1 v 2 v m</cell><cell>c 1 c 2 c 1 c 2 c 1 c 2 ...</cell><cell>... ... ...</cell><cell>c i c i c i</cell><cell>... ... ...</cell><cell>c n c n c n</cell><cell>t</cell><cell>c 1 c 2</cell><cell>...</cell><cell>c i</cell><cell>...</cell><cell>c n</cell></row></table><note><p>nwt</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>to illustrate the two methods. To quantify the importance of either configuration parameters or events, we need to collect a number of training examples. For method B,</figDesc><table><row><cell>4% 6%</cell><cell></cell><cell cols="7">DataCaching+DataCaching</cell><cell></cell><cell cols="2">10.1%</cell><cell></cell><cell cols="7">DataCaching+GraphAnalytics</cell></row><row><cell>2%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ISF</cell><cell>PI3</cell><cell>BRB</cell><cell>CRX</cell><cell>IPD</cell><cell>MLL</cell><cell>MUL</cell><cell>MSL</cell><cell>BNT</cell><cell>BRE</cell><cell>BRE</cell><cell>L2H</cell><cell>L2R</cell><cell>DSP</cell><cell>L2C</cell><cell>TFA</cell><cell>L2A</cell><cell>L2M</cell><cell>L2S</cell><cell>MSL</cell></row><row><cell cols="20">Fig. 16. The importance rank of events for co-located workloads: 'Data-</cell></row><row><cell cols="18">Caching + DataCaching' and 'DataCaching + GraphAnalytics'.</cell><cell></cell><cell></cell></row><row><cell cols="20">we have to run a benchmark k times with k different configu-</cell></row><row><cell cols="20">rations to collect k training examples because we can collect</cell></row><row><cell cols="20">the execution time of a program only after it completes its</cell></row><row><cell cols="20">execution. In contrast, for method A, we can collect m training</cell></row><row><cell cols="20">examples for an event during one execution of a benchmark</cell></row><row><cell cols="20">because we sample a number of values for the event during</cell></row><row><cell cols="20">one run of the benchmark with a certain configuration. Method</cell></row><row><cell cols="20">A therefore needs a much smaller number of benchmark runs</cell></row><row><cell cols="4">than method B.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="20">Taking pagerank as an example, we have to run it 6000</cell></row><row><cell cols="20">times to collect 6000 training examples to build a performance</cell></row><row><cell cols="18">model with around 90% of accuracy by method B.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use events to represent microarchitecture events throughput the paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use Spark to represent Apache Spark throughput the paper.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors would also like to thank the anonymous reviewers for their valuable comments. This work is supported by the <rs type="programName">national key research and development program</rs> under Grant No. <rs type="grantNumber">2016YFB1000204</rs>, <rs type="funder">Shenzhen Technology Research Project</rs> (Grant No. <rs type="grantNumber">JSGG20160510154636747</rs>), outstanding technical talent program of <rs type="funder">CAS</rs>, and <rs type="funder">NSFC</rs> under (Grant No. <rs type="grantNumber">61672511</rs> and <rs type="grantNumber">61702495</rs>). The research is also partially supported by the <rs type="funder">National Science Foundation</rs> grants <rs type="grantNumber">NSF-CCF-1657333</rs>, <rs type="grantNumber">NSF-CCF-1717754</rs>, <rs type="grantNumber">NSF-CNS-1717984</rs>, and <rs type="grantNumber">NSF-CCF-1750656</rs>. <rs type="person">Zhibin Yu</rs> is the corresponding author. Contact: zb.yu@siat.ac.cn.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QgaVXan">
					<idno type="grant-number">2016YFB1000204</idno>
					<orgName type="program" subtype="full">national key research and development program</orgName>
				</org>
				<org type="funding" xml:id="_ARqjv6J">
					<idno type="grant-number">JSGG20160510154636747</idno>
				</org>
				<org type="funding" xml:id="_yrHfHkT">
					<idno type="grant-number">61672511</idno>
				</org>
				<org type="funding" xml:id="_AChtMzy">
					<idno type="grant-number">61702495</idno>
				</org>
				<org type="funding" xml:id="_rbvkTrw">
					<idno type="grant-number">NSF-CCF-1657333</idno>
				</org>
				<org type="funding" xml:id="_4t3nqYw">
					<idno type="grant-number">NSF-CCF-1717754</idno>
				</org>
				<org type="funding" xml:id="_7pKBGSJ">
					<idno type="grant-number">NSF-CNS-1717984</idno>
				</org>
				<org type="funding" xml:id="_wbUG8fV">
					<idno type="grant-number">NSF-CCF-1750656</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">-exc BMP-exc BRC-dpl MCO-bbs ISF-exm ISF-dpl LMH-exm BMP-rdm ITM-mmf BRE-mmf BAA-mmf PI3-mmf BRB-mmf MMR-mmf MSL-mmf IPD-mmf MMR-kbf BAA-nwt PI3-ssb ITM-ics ITM-sfb BRE-rdm ISF-kbm IPD-dpl BRE-kbm IM4-ics IM4-kbm BRE-dmm IPD-mmf BRE-dmm BRE-mmf MMR-mmf MMR-ics CAC-dmm CAC-mmf MSL-kbm ISF-kbm BRE-bbs BRB-mmf ORO-bbs IDU-nwt MSL-rdm ORO-exm ISF-nwt ISF-mmf ISF-kbm MSL-nwt LMH-bbs MST-kbf PI3-ssb MST-nwt ISF-ssb MSL-ssb</title>
		<idno>ISF-dmm ISF-mmf MSL-mmf MSL-ics ITM-dmm ITM-mmf BRB-kbm BRE-kbm ISF-bbs IPD-mmf TFA-exc PI3</idno>
		<ptr target="BRE-dplMSL-dplIPD-nwtISF-rdmMSL-excPI3-bbsBRE-exmPI3-excMSL-exmMSL-kbfBMP-kbfBMP-dplMSL-excBMP-dmmMCO-dplPI3-dplREFERENCES" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intel 64 and ia-32 architectures developer&apos;s manual</title>
		<author>
			<persName><forename type="first">I</forename><surname>Coorporation</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-manual-325462.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Arm cortex-a53 mpcore processor technical reference manual</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coorporation</surname></persName>
		</author>
		<idno>ddi0500d/DDI0500D cortex a53 r0p2 trm.pdf</idno>
		<ptr target="http://infocenter.arm.com/help/topic/com.arm.doc" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bios and kernel developer&apos;s guide for amd family 10h processors</title>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<ptr target="http://support.amd.com/TechDocs/31116.pdf" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Advanced Micro Devices</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mpx: Software for multiplexing hardware performance counters in multithreaded programs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Parallel and Distributed Processing</title>
		<meeting>IEEE International Symposium on Parallel and Distributed Processing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cats: Cache aware task-stealing based on online profiling in multi-socket mukti-core architectures</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Supercomputing</title>
		<meeting>the International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">User-level scheduling on numa multicore systems under linux</title>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Linux Symposium</title>
		<meeting>Linux Symposium</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Googlewide profiling: A continuous profiling infrastructure for data centers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterizing data analysis workloads in data centers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting>the IEEE International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bigdatabench: a big data benchmark suite from internet services</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>the International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep-dive analysis of the data analytics workload in cloudsuite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ben-Asher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting>the IEEE International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A top-down method for performance analysis and counters architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Performance Analysis of Sytems and Software (ISPASS)</title>
		<meeting>the IEEE International Symposium on Performance Analysis of Sytems and Software (ISPASS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shenzhen transportation system (szts): A novel big data benchmark suite</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="4337" to="4364" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cpi2:cpu performance isolation for shared compute clusters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jnagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Systems (EuroSys)</title>
		<meeting>European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rapidmrc: Approximating l2 miss rate curves on commodity systems for online optimizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stumm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fractalmrc: Online cache miss rate curve prediction on commodity systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<meeting>the IEEE International Parallel and Distributed Processing Symposium (IPDPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A case for numa-aware contention management on multicore systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuravlev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Annual Technical Conference (ATC)</title>
		<meeting>USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taming hardware event samples for fdo compilation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Code Generation and Optmization (CGO)</title>
		<meeting>the International Symposium on Code Generation and Optmization (CGO)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optiscope: Performance accountability for optimizing compilers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Code Generation and Optmization</title>
		<meeting>the International Symposium on Code Generation and Optmization</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mapping parallelism to multi-cores: A machine learning based approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>O'boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIG-PLAN symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 14th ACM SIG-PLAN symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Server engineering insights for large-scale online services</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A portable programming interface for performance evaluation on modern processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="189" to="204" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Intel vtune amplifier</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/intel-vtune-amplifier-xe/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Perfmon2: the hardware-based performance monitoring interface for linux</title>
		<author>
			<persName><forename type="first">P</forename><surname>Team</surname></persName>
		</author>
		<ptr target="http://perfmon2.sourceforge.net/docsv4.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Oprofile</title>
		<author>
			<persName><forename type="first">O</forename><surname>Team</surname></persName>
		</author>
		<ptr target="http://oprofile.sourceforge.net/news/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hpctoolkit: Tools for performance analysis of optimized parallel programs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Adhianto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Tallent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Design and implementation of a parallel performance data management framework</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Malony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Processing (ICPP)</title>
		<meeting>the International Conference on Parallel Processing (ICPP)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">So many performance events, so little time</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zellweger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Asia-Pacific Workshop on Systems (APSys)</title>
		<meeting>the ACM Asia-Pacific Workshop on Systems (APSys)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Time interpolation: So many metrics, so few registers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauswirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Microarchitecture</title>
		<meeting>IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Can hardware performance counters be trusted</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Workload Characterization</title>
		<meeting>IEEE International Symposium on Workload Characterization</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Producing wrong data without doing anything obviously wrong</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauswirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM Symposium on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 14th ACM Symposium on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reliable and efficient performance monitoring in linux</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dimakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koziris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage, and Analysis</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Computationally efficient multiplexing of events on hardware counters</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carrillo-Cisneros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Alkowaileet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Scherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ottawa Linux Symposium</title>
		<meeting>the Ottawa Linux Symposium</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Cloudsuite 3.0: A benchmark suite for cloud services</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pourhabibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kassir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<ptr target="http://cloudsuite.ch//pages/download/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sparkbench: The big data micro benchmark suite for spark 2.0</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://github.com/intel-hadoop/HiBench/blob/master/docs/run-sparkbench.md" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hardware performance monitoring for the rest of us: A position and survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jalby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IFIP International Conference on Network and Parallel Computing (NPC)</title>
		<meeting>IFIP International Conference on Network and Parallel Computing (NPC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Toward accurate performance evaluation using hardware counters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mathus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ITEA Modeling and Simulation Workshop</title>
		<meeting>the ITEA Modeling and Simulation Workshop</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using dynamic time warping to find pattern in time series</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Berndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases (KDD)</title>
		<meeting>the AAAI-94 Workshop on Knowledge Discovery in Databases (KDD)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Linux profiling with performance counters</title>
		<ptr target="https://perf.wiki.kernel.org/index.php/MainPage#perf" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Linux profiling with performance counters</publisher>
		</imprint>
	</monogr>
	<note>linux community, &quot;perf</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<title level="m">Data Mining -Concetps and Techniques</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>3rd ed</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="2012-02">February 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic Gradient Boosting</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2002-10">October 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple Additive Regression Trees with Application in Epidemiology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Meulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1365" to="1381" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quasar: Resource-efficient and qosaware cluster management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Paragon: Qos-aware scheduling for heterogeneous datacenters</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hcloud: Resource-efficient provisioning in shared cloud systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Twenty First International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>Twenty First International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bolt: I know what you did last summer... in the cloud</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Twenty Second International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>Twenty Second International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Scipy: An open-source software for mathematics, science, and engineering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://docs.scipy.org/doc/scipy/reference/index.html" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Hive performance benchmarks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://spark.apache.org/docs/latest/configuration.html" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Perfexplorer: A performance data mining framework for large-scale parallel computing</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Malony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC)</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scalable analysis techniques for microprocessor performance counter metrics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Supercomputing (ICS)</title>
		<meeting>the International Conference on Supercomputing (ICS)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Experiences and lessons learned with a portable interface to hardware performance counters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terpstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<meeting>IEEE International Parallel and Distributed Processing Symposium (IPDPS)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting hardware performance counters with flow and context sensitive profiling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Non-determinism and overcount on modern hardware performance counter implementations</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terpatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<meeting>IEEE International Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
