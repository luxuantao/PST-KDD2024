<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-29">29 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
							<email>xiang_chen@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaozhuan</forename><surname>Liang</surname></persName>
							<email>liangxiaozhuan@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
							<email>chuanqi.tcq@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-29">29 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.14704v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RETROPROMPT with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RETROPROMPT constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstrate that RETROPROMPT can obtain better performance in both few-shot and zero-shot settings. Besides, we further illustrate that our proposed RETROPROMPT can yield better generalization abilities with new datasets. Detailed analysis of memorization indeed reveals RETROPROMPT can reduce the reliance of language models on memorization; thus, improving generalization for downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large parametric language models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref> have achieved dramatic empirical success in natural language processing (NLP). Notably, pre-trained language models (PLMs) have learned a substantial amount of in-depth knowledge from data, and have archived tremendous promise in few-shot/zero-shot learning ability with the natural language prompts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54]</ref>. However, Recent studies <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56]</ref> observe that prompt learning with PLMs usually generalizes unstably in an extremely low-resource setting or emerging domains. One potential reason is that, it is non-trivial for parametric models to learn rare or hard patterns well with rote memorization, thus, resulting in inefficient generalizable performance.</p><p>Intuitively, if we regard the whole training data as a book and the test phase as the examination, the current training-test procedure of prompt learning (based on batch data training) can be viewed as page-by-page memorization and closed-book examination <ref type="bibr" target="#b39">[40]</ref>. During training, vanilla prompt learning may struggle to memorize atypical instances in a fully-supervised setting or overfit shallow patterns with low-shot data <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b8">9]</ref>. Specifically, recent studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> have proposed a long-tail theory, which states that if training data form a long-tail distribution and have small "sub-populations" with atypical instances, then PLMs indeed predict on the test data through rote memorizing these atypical instances rather than learning the common patterns <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>The limitations of rote memorization remind us of the human learning process of "learn by analogy" and the proverb that "the palest ink is better than the best memory". Note that humans can perform associative learning to recall relevant skills in deep memories for reinforcing each other, thus, owning the extraordinary abilities to solve few-shot and zero-shot tasks. Motivated by these, we endeavor to improve the generalization ability of prompt learning with retrieval and association. Our intuition is that the difficulty of resolving the above limitations can be substantially alleviated if we can decouple the knowledge from memorization by constructing an open-book knowledge-store from the training data; thus, referring to related knowledge could provide a strong enhancement signal to help the model strike a balance between generalization and memorization. Specifically, we introduce a novel retrievalaugmented framework based on prompt learning (RETROPROMPT) as shown in Figure <ref type="figure" target="#fig_0">1</ref>. The open-book knowledge store (K, V), defined as the set of key: promptbased example embeddings and value: corresponding label words constructed from the training data, are served as additional references for the model to decouple knowledge from pure memorization to some extent. Specifically, to integrate retrieved knowledge into the input, Firstly, we design to incorporate neural demonstrations into the input sequences as in-context augmentation, where the demonstration is retrieved from the knowledge-store. Then, we apply a non-parametric algorithm kNN over the input query and knowledge store, and regard kNN results as an indication of easy vs. hard examples in the training set. More specifically, we automatically force the model to focus on the hard examples identified by kNN by assigning a scaling during training. Lastly, the kNN results are further employed at the output of the PLM head to participate in masked prediction during inference. The model retrieves Top-k nearest reference instances as cues from (K, V) and makes inference by linearly interpolating the output of prompt learning with a non-parametric nearest neighbor distribution.</p><p>The considerable performance gains on nine tasks in few-shot and zero-shot settings demonstrate that our systemic retrieval mechanism helps the model generalize better with scarce data. Experiments in the fully-supervised setting with long-tail distribution illustrate that our RETROPROMPT can deal with atypical instances more robustly. We further adopt self-influence <ref type="bibr" target="#b24">[25]</ref> as our memorization scoring function to analyze the memorization process between fine-tuning, prompt learning and our RETROPROMPT. The final analysis results show that 1) the training instances with the highest memorization scores tend to be atypical, 2) RETROPROMPT generalize better than fine-tuning and convention prompt-tuning with decoupling knowledge from memorization to alleviate the rote of PLMs. In a nutshell, our work may open up new avenues to improve the generalization of prompting PLMs by retrieving knowledge from memorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries of Prompt Learning</head><p>Assuming that M, T respectively denotes the PLM and the template function for prompt tuning. Formally, the text classification task takes a query sentence x = (x 0 , x 1 , ..., x n ) as input, and classify it into a class label y ? Y. While prompt learning converts classification task into a masked language modeling problem with cloze-style objectives. Specifically, the template function T inserts pieces of texts into x as x = T (x), where x is the corresponding input of M with a [MASK] token in it. For example, assuming we need to classify the sentence x ="The movie makes absolutely no sense." into label NEGATIVE (labeled as 0) or POSITIVE (labeled as 1), we wrap it into</p><formula xml:id="formula_0">x = [CLS]x It was [MASK][SEP]<label>(1)</label></formula><p>The verbalizer f : Y ? V is defined as a mapping from the label space Y to a few words in the vocabulary, which form the label word set V. The base component of M produces the sequence     Note that e(?) denotes word embedding function in the PLM M, while "M","t" and "g" in e(?) specifically refers to "[MASK]", "terrible" and "great".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLM Head</head><p>representation over x, and we choose the hidden vector at the [MASK] position as the contextual representation h x ? R d , where d is the dimension of hidden states. Then the MLM head of M can operate on h x to calculate the probability of each word v in the vocabulary being filled in [MASK] token P M ([MASK] = v| x). We let V y to represent the subset of V that is connected with a specific label y, ? y?Y V y = V. Then the probability distribution over the label y is calculated as:</p><formula xml:id="formula_1">P (y|x)=g (P M ([MASK]= v|T (x))|v ? V y ) ,<label>(2)</label></formula><p>where g is a function transforming the probability of label words into the probability of the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RETROPROMPT: Retrieval-augmented Prompt Learning</head><p>We introduce a simple and general retrieval-augmented framework for prompt learning, named RETROPROMPT, whose basis is the dense retriever ( ?3.1) with an open-book knowledge-store to decouple knowledge from memorization. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, RETROPROMPT consists of three components: retrieval of neural demonstration for enhancing input ( ?3.2), the kNN guided training ( ?3.3) and the kNN-based probability for cloze-style prediction ( ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dense Retriever</head><p>Open-book Knowledge-store The first step of our proposed framework is to build a knowledgestore for retrieval that can decouple from memorization and captures the semantics of the instance from the training set C. Specifically, we utilize the encoder to embed prompt-based instance representation over the C to construct the knowledge-store. Given the i-th example (c i , y i ) in the training data C, we compute the key-value pair (h ?i , v i ), in which ?i = T (c i ), h ?i ? R d is the embedding of the [MASK] token in the last layer of the underlying PLM, and v i = f (y i ) denotes the label word of the i-th example. We store all pairs (h ?, v) in a key-value datastore (K, V) where h ? serves as key and v as value as follows:</p><formula xml:id="formula_2">(K, V) = {(h ?i , v i ) | (c i , y i ) ? C}<label>(3)</label></formula><p>The knowledge-store is flexible to add, edit or delete any instances and can be asynchronously updated during the training procedure. Note that our knowledge-store is constructed from few-shot trainsets in the corresponding few-shot settings rather than the whole available training data.</p><p>Efficient Searching Considering that the size of the training data C can be enormous, we must ensure an efficient retrieval process. As shown in the above creation of open-book knowledge-store, we can build the matrix D ? R |C|?d as the index of training examples. Given a query set Q, we first encode each query example with template mapping function T (?) to get a set of prompt-based query vectors h q for retrieval augmentation on the fly. Then, we utilize query vectors to search for the closest examples over the index D via maximum inner product search (MIPS). For the retrieval process, we choose FAISS <ref type="bibr" target="#b18">[19]</ref> to query the open-book knowledge-store efficiently. FAISS is an excellent open-source library for fast nearest neighbor retrieval in high-dimensional spaces.</p><p>Asynchronous Refresh of the Knowledge-store Since the neural demonstration may lead to the variable contextual representation of instance as the parameters of the PLM are continually updated, we thus propose to "refresh" the index of retrieval by asynchronously re-embedding and re-indexing all embeddings in an open-book knowledge-store every j training epochs<ref type="foot" target="#foot_0">2</ref> . In ? 4.6, we empirically demonstrate that this procedure results in performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval of Neural Demonstration</head><p>To enhance the PLMs with the ability to learn by analogy through the knowledge-store, we further combine RETROPROMPT with neural demonstrations, an orthogonal technique enhancing language models, to improve the generalization ability of our model. For the t-th query instance q t , we first utilize prompt-based representation h qt to query the cached representations of open-book knowledgestore. Then we retrieve m nearest neighbors {{c</p><formula xml:id="formula_3">(1) 1 , ..., c<label>(1)</label></formula><p>m }, ..., {c</p><formula xml:id="formula_4">(L) 1 , ..., c<label>(L)</label></formula><p>m }} of q t for each class, where the superscript L denotes the total number of the classes and the c (l) i is retrieved as the i-th nearest neighbor in the l-th class. After the model retrieves the Top-m candidates for each class, their corresponding representation h (l) ?i and label word v (l) from knowledge-store will be incorporated into the encoder to act as a demonstration learning. Since the h (l) ?i is already vector, we intuitively aggregate the m neighbor vectors for each class according to their similarity and incorporate the demonstration into the input representation of x after the word embedding layer of the M as follows:</p><formula xml:id="formula_5">I = e( x) ? [ i?[1:m] ? (1) i h (1)</formula><p>?i , e(v (1) </p><formula xml:id="formula_6">)] ? ... ? [ i?[1:m] ? (L) i h (L) ?i , e(v (L) )]; ? (l) i = e h q ?h (l) ?i i?[1:m] e h q ?h (l) ?i (4)</formula><p>where e(?) represents the word embedding layer of M, ? denotes the concatenation of input sequences, ? (l) i is the softmax score for the i-th retrieval belonging to l-th class label to denote their relevance with q, and I is the sequence features for inputting the next layer of PLM. As shown in the above equation, we encode demonstration representation with the weighted sum of the retrieval representation. Thus, retrieval scores are directly used in the final representation, making the framework differentiable. To this end, we denote this style of demonstration as neural demonstration, significantly different from prior work of discrete demonstration <ref type="bibr" target="#b11">[12]</ref>.</p><p>Neural vs. Discrete Demonstration Compared with prior discrete demonstrations described in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26]</ref>, retrieving weighted neural demonstrations from the knowledge-store to augment prompt learning has advantages in the following three major aspects: (1) neural demonstrations could be more tolerant of the model's maximum input length than discrete demonstrations, while the discrete demonstration is usually not suitable for multi-class classification tasks due to the limitation of input length, such as relation extraction, etc. (2) the model needs to deal with large retrieval tokens for discrete demonstration, making it time-consuming and computationally intensive to perform cross-attention operations due to the quadratic attention complexity. In contrast, dealing with much shorter instance representations as neural demonstrations unleashes the potential of cross-attention and accelerates the inference. <ref type="bibr" target="#b2">(3)</ref> when sampling examples based on the similarity between instances, our cloze-style contextual representation is more informative and consistent than the contextual representation from [CLS] of Sentence-BERT <ref type="bibr" target="#b44">[45]</ref> (adopted in LM-BFF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Retrieve kNN for Guiding Training</head><p>Eager learners, such as PLMs, are trained to provide a global approximating function that maps from input to output space. Lazy learners such as k-nearest neighbor classifiers, on the contrary, focus on approximating the neighborhoods around test examples <ref type="bibr" target="#b1">[2]</ref>. Since kNN can easily predict for each encountered query instance based on pre-trained representation without an extra classifier, it is intuitively to leverage the kNN's classification results as the prior external knowledge to guide the PLMs' parameters attending to hard examples (hard samples usually refer to atypical samples) during the training process (also referred as kNN-train for the abbreviation). Particularly, our intuition is to differentiate between easy and hard examples according to the prediction of kNN. Given the t-th query instance q t , we leverage the h qt querying the open-book knowledge-store (K, V) to retrieve the k-nearest neighbors N of q t according to a similarity function d(?, ?), where d(?, ?) typically adopt the inner product similarity. Then, we compute a distribution over neighbors based on a softmax of their similarities and aggregate probability mass for each label word across all its occurrences in the retrieved targets:</p><formula xml:id="formula_7">P kNN (y | qt) ? (c i ,y i )?N 1y=y i exp (d (h qt , h ?i )) . (5)</formula><p>Given the probability p kNN of the query instance q t being predicted as the gold class, we propose to retrieve the kNN for guiding the training process of prompt learning. The kNN guider reweights the cross-entropy loss L CE by adjusting the relative loss for the correctly-classified or misclassified instances identified by kNN, respectively. Specifically, we apply the negative log-likelihood as the modulating factor F (p kNN ). The final loss L is defined as:</p><formula xml:id="formula_8">F (p kNN ) = -log (p kNN ), L = (1 + ?F (p kNN )) LCE,<label>(6)</label></formula><p>where ? denotes a scalar to determine the proportion of each loss term. Note that p kNN is computed using the leave-one-out distribution on the training set due to the fact that each example in the training set cannot retrieve itself. The motivation of modulating factor here is similar to Focal-loss <ref type="bibr" target="#b31">[32]</ref>, while we focus on exploit the application of kNN in tuning PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">kNN based probability for Cloze-style Prediction</head><p>Apart from the neural demonstration on the input side and kNN guided training process (also referred as kNN-test for the abbreviation), we further present kNN based probability for Cloze-style prediction on the inference process, providing the PLM ability to retrieve nearest neighbors for decisions rather than making predictions only based on memorized parameters. Given the non-parametric k nearest neighbor distribution P kNN of the query instance q t being predicted as y, the P (y | q t ) is reformulated by interpolating the P kNN with the already-trained base PLM's MLM prediction P M using parameter ? to produce the final probability of the label:</p><formula xml:id="formula_9">P (y | q t ) = ?P kNN (y | q t ) + (1 -?)g (P M ([MASK] = v|T (q t ))) .<label>(7)</label></formula><p>Different from kNN-LM <ref type="bibr" target="#b14">[15]</ref> that uses tokens to augment the language modeling directly, we explicitly take advantage of prompt-based instance representation for classification tasks, which is more deeply rooted in prompt learning. In this way, we can unlock the model prediction process as an open-book examination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Baselines</head><p>Datasets We evaluate RETROPROMPT on several types of natural language understanding tasks, including single sentence classification tasks (SST-2 <ref type="bibr" target="#b50">[51]</ref>, MR <ref type="bibr" target="#b40">[41]</ref>, and CR <ref type="bibr" target="#b16">[17]</ref>) and sentence pair classification tasks (MNLI <ref type="bibr" target="#b54">[55]</ref>, QNLI <ref type="bibr" target="#b43">[44]</ref>, and QQP<ref type="foot" target="#foot_1">3</ref> ). To further evaluate the effectiveness of the proposed approach with multi-class classification, we also conduct experiments on the information extraction tasks, including FewNERD <ref type="bibr" target="#b7">[8]</ref>, SemEval 2010 Task 8 (SemEval) <ref type="bibr" target="#b15">[16]</ref>, and TACRED <ref type="bibr" target="#b57">[58]</ref>.</p><p>Baselines We compare with LM-BFF <ref type="bibr" target="#b11">[12]</ref> for single sentence and sentence pair classification tasks and adopt SOTA prompt learning model KnowPrompt <ref type="bibr" target="#b4">[5]</ref> as the baseline for information extraction tasks. Note that the discrete demonstration method cannot be applied to multi-class classification tasks due to the input length limitations; thus, we leave out the experimental table about the results of KnPr (D-demo). We also compare our RETROPROMPT with the knowledge-enhanced prompt learning method KPT <ref type="bibr" target="#b17">[18]</ref> since KPT leverages the external knowledge base for enhancing prompt learning while we focus on utilizing internal trainsets as a knowledge-store. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation protocols and details</head><p>The experiments are implemented on 1 NVIDIA V100 and utilize Pytorch <ref type="bibr" target="#b41">[42]</ref> as the base library. We adopt RoBERTa large <ref type="bibr" target="#b35">[36]</ref> as the PLM and employ AdamW as the optimizer for all experiments.</p><p>To mitigate the influence of diverse templates, we conduct baselines and RETROPROMPT with the same templates for each dataset. The specific templates we use for each dataset are in Appendix. As for few-shot and zero-shot experiments, we leverage different settings, respectively.</p><p>Few-shot Setting. We follow the few-shot setting of LM-BFF <ref type="bibr" target="#b11">[12]</ref> to conduct 4-shot and 16-shot experiments and evaluate the average performance with a fixed set of seeds, S seed , across different sampled D train for each task. Note that our knowledge-store is constructed with the few-shot training set in this setting.</p><p>Zero-shot Setting. We leverage vanilla RoBERTa large for all baselines (except LOTClass <ref type="bibr" target="#b38">[39]</ref>) to directly inference on the test set. To take advantage of retrieval mechanism, RETROPROMPT follows LOTClass <ref type="bibr" target="#b38">[39]</ref> to utilize unlabeled trainsets for retrieval. Specifically, we take the vanilla RoBERTa large to tag the pseudo labels on unlabeled trainset and create the open-book knowledge-store with the unlabeled trainsets and pseudo labels. Lastly, RETROPROMPT make predictions on the test set based on the constructed datastore without tuning any of the model parameters. Few-shot Results. As shown in Table <ref type="table" target="#tab_4">1</ref>, we find RETROPROMPT consistently outperforms baseline method LM-BFF and KnowPrompt, both in 4-shot and 16-shot experiments. Especially for information extraction tasks with multiple classes, discrete demonstrations cannot be applied to the input due to the limited input sequence length, while our neural demonstration can also work and achieves improvement on these multi-class datasets. Moreover, RETROPROMPT obtain better performance compared with KPT. Compared with KPT with external knowledge, we only focus on referencing the internal few-shot trainsets without visiting the external knowledge base. Besides, we observe that RETROPROMPT has a relatively lower standard deviation than the baselines. The reason may lie that the retrieval mechanism can compensate for instabilities in parametric predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST</head><p>Zero-shot Results. From Table <ref type="table" target="#tab_4">1</ref>, we also observe that RETROPROMPT achieves improvements in the zero-shot setting. Another notable point is that RETROPROMPT performs even better than KPT in the zero-shot setting, revealing that exploring own data to decouple knowledge from memorization has more potential than leveraging external knowledge. Moreover, we achieve superior performance to LOTClass even though we utilize the vanilla RoBERTa large without any training. Fully-supervised Results. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the experiments in fully-supervised settings with long-tail distribution illustrate that RETROPROMPT achieves improvement compared with baselines. This indicates that our retrieval mechanism extends the LM's ability to learn hard examples in the fully-supervised datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Generalization to New Domains</head><p>The scarce data may bring the overfitting problem for the lots of memory parameters of PLMs, even though prompt learning. Thus, we conduct cross-domain experiments to validate the generalization of our RETROPROMPT. Specifically, we utilize the model trained on the source datasets and directly test on the other target datasets. From Table <ref type="table" target="#tab_5">2</ref>, we can find that our method consistently outperforms baselines. This finding illustrates that RETROPROMPT achieves great model generalization to new domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Memorization</head><p>It is necessary and interesting to further explore the memorization mechanism to help us better understand the utility of retrieval for memorization in NLP.</p><p>Definition of Memorization Measurement. Inspired by the idea of <ref type="bibr" target="#b9">[10]</ref> in the computer vision area, we define memorization measures as to how the classification varies when a training instance z is deleted from the trainset. We follow <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b58">59]</ref> to define and derive the memorization score for a training instance z as follows:</p><formula xml:id="formula_10">Sdelate(z) def = - dP (y|x; ??,-z ) d? ?=0 = -? ? P (y|x; ?) d ??,-z d? ?=0 = -? ? P (y|x; ?) H -1 ? ? ? L(z, ?),<label>(8)</label></formula><p>where ??,-z denotes the parameters of the model trained with the instance z down-weighted by ?, ? is the parameters of the model trained with all instances and</p><formula xml:id="formula_11">H ? = 1 n n i=1 ? 2 ? L(z i , ?).</formula><p>Thus S delate (z) is the amount of change of P (y|x; ?) when the instance z is down-weighted by a small amount ?. Top-memorized Instances: Typical or Atypical? Since the SST-2 dataset provides the annotations of phrase-level sentiment polarity labels, we adopt SST-2 to analyze the memorization by judging the atypical of an instance by checking the percentage of positive phrases. We collect such statistics from SST-2 and find that a typical positive instance has a relatively high percentage of positive phrases, and a typical negative instance should have a relatively low percentage of positive phrases. Based on the above observation, we apply the memorization score defined in Eq. 8 to select Top-10% and Bottom-10% memorized instances from the trainset and collect the average percentage of positive phrases in these instances.</p><p>As shown in Table <ref type="table" target="#tab_6">3</ref>,we can conclude following findings: <ref type="bibr" target="#b0">(1)</ref> The PLM tends to give atypical samples deeper memory attention. Specifically, no matter LM-BFF or our method, the top-10% memorized negative instances have a higher percentage of positive phrases than the average percentage of positive phrases of all negative instances. 2) LM-BFF has lower memorization scores on hard samples than fine-tuning. We think it owns to prompt learning can help PLMs recall what they learned from pre-training without strengthening memory for downstream data. 3) RETROPROMPT further has lower average memorization scores than fine-tuning and LM-BFF, which illustrates that our method is less memory dependent. This result may be attributed to decoupling knowledge from memorization through retrieval to alleviating the rote of PLMs.</p><p>Table <ref type="table">4</ref>: Detailed ablation experiments in few-shot settings. "N-demo" donates the neural demonstration, and "refresh" refers to the asynchronous refresh of the knowledge-tore. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Study</head><p>Component Ablation.</p><p>As shown in Table <ref type="table">4</ref>, the performance of component ablation experiments with four variants has a clear drop, which proves the effectiveness of our retrieval component. We also find that neural demonstration and kNN-train have more improvement in the few-shot setting than kNN-test. Note that kNN-test is similar to kNN-LM <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref> and the results reveals that simply incorporate kNN in the test process of prompt learning has little influence in a few-shot setting. Key Representation and kNN Acquisition.</p><p>We study the effect of using different representations of the key in the knowledge-store. We experiment with two types of representations: (1) prompt-based representation, which is the default setting, and (2) [CLS] based representation of current LM. We also experiment with two types of calculation of kNN distribution: (1) representation based similarity score (refer as rep-similar), which is the default setting, and (2) BM25 based score, which calculates the correlation score between the query and each key examples with BM25 <ref type="bibr" target="#b45">[46]</ref> algorithm. Results in Table <ref type="table" target="#tab_8">5</ref> show that using prompt-based representations for key and representation based similarity scores for kNN leads to the best performance. It suggests that prompt learn better representations for context similarity and the representation similarity based kNN distribution is better than BM25 based scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Retrieval-enhanced PLMs. Our pipeline is partly inspired by discrete demonstration methods such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> that retrieves few training examples in a natural language prompt, while we propose neural demonstration for enhancing the input to alleviate the limitations of input length. Another line researches of retrieval augmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> retrieve useful information from a external knowledge corpus (e.g., Wikipedia) for a particular task (e.g., an open-domain question). Unlike these works, we focus on retrieving examples from the internal training data. Besides, semiparametric methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b39">40]</ref> have risen to leverage k-nearest neighbor classifier that makes the prediction based on representation similarities, to enhance pre-trained language models. However, unlike these models using nearest neighbors only for augmenting the process of prediction, we aim to develop a comprehensive retrieval mechanism for input, training and test process.</p><p>Prompt learning for PLMs. With the birth of GPT-3 <ref type="bibr" target="#b2">[3]</ref>, prompt learning <ref type="bibr" target="#b33">[34]</ref> has recently arisen to fill the gap between masked LM objective of PLMs and downstream fine-tuning objective. Prompt learning has achieves very impressive performance on various tasks <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5]</ref>, especially under the setting of few-shot learning. Moreover, continuous prompts have also been proposed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref> to reduce prompt engineering, which directly appends a series of learnable continuous embeddings as prompts into the input sequence. Our work is orthogonal to previous prompt learning approaches, which aim to optimize prompts, while we focus on the systematic study of retrieving related examples from training data to enhance prompt learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We propose RETROPROMPT that decouples knowledge from memorization by introducing retrieval augmentation to further improve the generalization ability of prompt learning on the input side and the whole process of model training and prediction. RETROPROMPT, is a straightforward yet effective retrieval method that combines both neural demonstrations, kNN guider for training and prediction.</p><p>Our extensive results show that it outperforms other demonstration-enhanced prompt methods and knowledge-enhanced prompt methods in few-shot, zero-shot and fully-supervised settings. Analyzing the essence of memorization validates the effectiveness of decoupling knowledge from memorization. Interesting future directions include: 1) apply to other tasks, such as QA and NLG, 2) explore the noise data mining for unsupervised learning, 3) further improve the retrieve efficiency for large datasets, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets and Templates</head><p>In this section, we introduce the information of datasets as shown in Table <ref type="table" target="#tab_10">7</ref> and list the templates we use in experiments as follows.</p><p>SST-2, MR, CR. For the single sentence classification tasks, we follow the LM-BFF <ref type="bibr" target="#b11">[12]</ref> to design the templates:</p><formula xml:id="formula_12">T (x) = [CLS]x It was [MASK].</formula><p>We set Verbalizer: (great/terrible) ? (positive/negative) for SST-2 MR and CR. For the Yahoo dataset, we assign the Verbalizer following the original labels.</p><p>MNLI, QNLI, QQP. For the sentence pair classification tasks, we follow LM-BFF <ref type="bibr" target="#b11">[12]</ref> to set Verbalizer: (Yes/Maybe/No) ? (entailment/neutral/contradiction), and define the following templates:</p><formula xml:id="formula_13">T (x1, x2) = [CLS]x1?[MASK],<label>x2</label></formula><p>FewNERD, SemEval, TACRED. FewNERD, SemEval and TACRED are datasets for information extraction, which require inserting the entity into the template. Therefore, we follow <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b4">[5]</ref> to define the template and verbalizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Compared Baselines</head><p>In this subsection, we introduce the baselines we compare with and re-produce them under the same settings with their open-source codes.</p><p>LM-BFF uses several other tricks, such as prompt ensemble, while KPT utilizes tremendous external knowledge. We do not use any of these tricks and external knowledge since we get the most out of the data to decouple part of knowledge from parametric memorization. Our RETROPROMPT mechanism is orthogonal to other methodological improvements of prompt-tuning (such as continuous prompt in P-tuning <ref type="bibr" target="#b34">[35]</ref> and DART <ref type="bibr" target="#b56">[57]</ref> ) and can be combined with other prompt-tuning methods in future work.</p><p>Fine-tuning (FT). The traditional fine-tuning method regard the hidden embedding of [CLS] token of the PLM as the representation of the sentence and then feeds them into a classification layer to make predictions.</p><p>LM-BFF. LM-BFF <ref type="bibr" target="#b11">[12]</ref> is a typical prompt-tuning method wrapping an input sentence into a handcrafted template. Here we re-produce LM-BFF based on their open-source codes <ref type="foot" target="#foot_2">4</ref> with the same manual prompts as RETROPROMPT for a fair comparison.</p><p>LM-BFF (+Demo). This approach is the above LM-BFF <ref type="bibr" target="#b11">[12]</ref> combined with the demonstration <ref type="bibr" target="#b2">[3]</ref>. Different from RETROPROMPT, it uses examples of natural language as demonstrations, which is restricted by the input length of the language model. Thus, LM-BFF (+demo) is not suitable for multi-class classification tasks.   KnowPrompt. KnowPrompt <ref type="bibr" target="#b4">[5]</ref> is a SOTA prompt-tuning method for relation extraction tasks with multiple classes. We apply our RETROPROMPT over KnowPrompt on information extraction tasks for comparison, aiming to verify the broad applicability of our method.</p><p>Incorporating Knowledge into Prompt (KPT). KPT <ref type="bibr" target="#b17">[18]</ref> focuses on incorporating external knowledge into the verbalizer by refining the expanded label word space to improve and stabilize prompttuning, which is a solid baseline for comparison. We follow their public codes <ref type="foot" target="#foot_3">5</ref> to conduct experiments in the same setting for a fair comparison.</p><p>LOTClass. LOTClass <ref type="bibr" target="#b38">[39]</ref> is the SOTA method in unsupervised text classification that utilizes the PLM to extract the label-related words from the whole unlabeled training corpus. Then it leverages the Masked Category Prediction task to train on the unlabeled corpus with pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Settings</head><p>We report the hyper-parameters in Table <ref type="table" target="#tab_11">8</ref>. Most of the hyper-parameters are the default parameters of LM-BFF <ref type="foot" target="#foot_4">6</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis of Efficiency</head><p>We make the comparison between LM-BFF (man), LM-BFF (+demo) and RETROPROMPT in speed on the MR dataset for the 16-shot setting. We observe that the speed of RETROPROMPT and LM-BFF (+demo) are approximately 1.12 and 20 times slower than LM-BFF (man) on the 16-shot MR dataset. The slow inference of LM-BFF (+demo) is due to the fact that they sample from the top r% instances (r = 50) for each class to use as demonstrations and vastly increase the length of the input, thus, increasing computational complexity significantly. And the bottleneck of computational speed is general limitations of retrieval methods, and our method is no exception. We will leave the engineering optimization about retrieval speed in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Tuning Retrieve Parameters</head><p>The final distribution of the label is affected by the hyperparameters of ?, k and ? when conducting kNN-train and kNN-test. Thus, we provide insight into the effect of ?, k and ? on the final results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decoupling knowledge from memorization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of RETROPROMPT. Note that e(?) denotes word embedding function in the PLM M, while "M","t" and "g" in e(?) specifically refers to "[MASK]", "terrible" and "great".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance on fully-supervised datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of the hyperparameters of the retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Results across 9 NLU datasets in the few-shot and zero-shot setting. We report mean (and standard deviation) results over five different few-shot splits. "D-demo" refers to discrete demonstration, and "KnPr" is the abbreviation of KnowPrompt. LOTClass<ref type="bibr" target="#b38">[39]</ref> is the SOTA model in unsupervised text classification with self-training. ? donates the model uses extra knowledge and ? means they train the PLM on the whole unlabeled trainset, while we and the other baselines only leverage the vanilla PLM to test without training. The average scores with * denote that we reuse the results of the "non-demo" version of the related model to fill in the default values. Note that the 16-shot results of LM-BFF on GLUE<ref type="bibr" target="#b52">[53]</ref> tasks are taken from the original paper of LM-BFF<ref type="bibr" target="#b11">[12]</ref> </figDesc><table><row><cell></cell><cell></cell><cell cols="3">Single Sentence</cell><cell></cell><cell>Sentence Pair</cell><cell></cell><cell></cell><cell cols="3">Information Extraction</cell><cell></cell></row><row><cell cols="2">St. Model</cell><cell>SST-2</cell><cell>MR</cell><cell>CR</cell><cell>MNLI</cell><cell>QNLI</cell><cell>QQP</cell><cell>Model</cell><cell>FewN</cell><cell cols="2">SemEval TACRED</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>(acc)</cell><cell>(acc)</cell><cell>(acc)</cell><cell>(acc)</cell><cell>(acc)</cell><cell>(F1)</cell><cell></cell><cell>(acc)</cell><cell>(acc)</cell><cell>(F1)</cell><cell></cell></row><row><cell></cell><cell>FT</cell><cell>81.4 (3.8)</cell><cell cols="6">76.9 (5.9) 75.8 (3.2) 45.8 (6.4) 60.2 (6.5) 60.7 (4.3) FT</cell><cell cols="2">52.7 (2.2) 66.1 (1.2)</cell><cell>25.8 (2.8)</cell><cell>60.6</cell></row><row><cell></cell><cell>LM-BFF (man)</cell><cell cols="7">92.7 (0.9 ) 87.0 (1.2) 90.3 (1.0) 68.3 (2.3) 64.5 (4.2 ) 65.5 (5.3) KnPr</cell><cell cols="2">65.3 (1.1) 80.9 (2.5)</cell><cell>33.2 (2.0)</cell><cell>72.0</cell></row><row><cell>16</cell><cell>LM-BFF (D-demo)</cell><cell cols="7">92.6 (0.5 ) 86.6 (2.2) 90.2 (1.2) 70.7 (1.3) 69.2 (1.9) 69.8 (1.8) KnPr (D-demo)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.2  *</cell></row><row><cell></cell><cell>KPT  ?</cell><cell>90.3 (1.6)</cell><cell cols="6">86.8 (1.8) 88.8 (3.7) 61.4 (2.1) 61.5 (2.8) 71.6 (2.7) KPT  ?</cell><cell cols="2">65.9 (1.5) 78.8 (2.1)</cell><cell>32.8 (1.7)</cell><cell>70.9</cell></row><row><cell></cell><cell>Ours</cell><cell>93.9 (0.4)</cell><cell cols="6">88.0 (0.8) 91.9 (0.7) 71.1 (1.8) 71.6 (1.8) 74.0 (2.0) Ours</cell><cell cols="2">67.3 (0.9) 81.5 (1.3)</cell><cell>40.7 (0.7)</cell><cell>75.6</cell></row><row><cell></cell><cell>FT</cell><cell>60.2 (2.8)</cell><cell cols="6">57.6 (1.4) 66.4 (5.5) 35.0 (0.3) 54.2 (3.9) 52.8 (4.7) FT</cell><cell cols="2">32.7 (2.9) 38.8 (2.0)</cell><cell>14.7 (2.8)</cell><cell>45.8</cell></row><row><cell></cell><cell>LM-BFF (man)</cell><cell>90.7 (0.8)</cell><cell cols="6">85.2 (2.8) 89.9 (1.8) 51.0 (2.5) 61.1 (6.1) 48.0 (4.9) KnPr</cell><cell cols="2">52.5 (1.5) 58.4 (3.7)</cell><cell>28.8 (2.5)</cell><cell>62.8</cell></row><row><cell>4</cell><cell>LM-BFF (D-demo)</cell><cell>90.2 (1.5)</cell><cell cols="6">85.5 (2.1) 89.7 (0.6) 56.1 (1.0) 61.7 (7.6) 63.2 (5.6) KnPr (D-demo)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>65.1  *</cell></row><row><cell></cell><cell>KPT  ?</cell><cell>88.2 (5.7)</cell><cell cols="6">83.4 (1.5) 87.2 (2.5) 53.7 (2.7) 59.2 (2.8) 54.9 (7.9) KPT  ?</cell><cell cols="2">58.8 (2.2) 57.2 (3.2)</cell><cell>27.5 (2.2)</cell><cell>63.3</cell></row><row><cell></cell><cell>Ours</cell><cell>91.5 (0.4)</cell><cell cols="6">87.4 (0.5) 91.4 (0.6) 57.6 (5.5) 62.8 (4.5) 66.1 (4.1) Ours</cell><cell cols="2">60.9 (1.9) 59.9 (1.9)</cell><cell>32.1 (2.0)</cell><cell>67.7</cell></row><row><cell></cell><cell>LOTClass ?</cell><cell>71.8</cell><cell>81.7</cell><cell>50.1</cell><cell>50.4</cell><cell>36.5</cell><cell>55.9</cell><cell>LOTClass ?</cell><cell>11.5</cell><cell>9.8</cell><cell>2.5</cell><cell>41.1</cell></row><row><cell></cell><cell>FT</cell><cell>49.1</cell><cell>50.0</cell><cell>49.8</cell><cell>34.4</cell><cell>49.5</cell><cell>31.6</cell><cell>FT</cell><cell>10.0</cell><cell>6.2</cell><cell>0.5</cell><cell>31.2</cell></row><row><cell></cell><cell>LM-BFF (man)</cell><cell>83.5</cell><cell>80.3</cell><cell>78.4</cell><cell>49.7</cell><cell>50.5</cell><cell>49.7</cell><cell>KnPr</cell><cell>15.9</cell><cell>10.3</cell><cell>2.3</cell><cell>46.7</cell></row><row><cell>0</cell><cell>LM-BFF (D-demo)</cell><cell>82.9</cell><cell>80.7</cell><cell>81.4</cell><cell>52.2</cell><cell>53.5</cell><cell>44.0</cell><cell>KnPr (D-demo)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.0  *</cell></row><row><cell></cell><cell>KPT  ?</cell><cell>78.4</cell><cell>81.9</cell><cell>71.4</cell><cell>37.1</cell><cell>58.4</cell><cell>47.5</cell><cell>KPT  ?</cell><cell>24.6</cell><cell>11.6</cell><cell>0.8</cell><cell>45.7</cell></row><row><cell></cell><cell>Ours</cell><cell>89.1</cell><cell>86.1</cell><cell>79.7</cell><cell>53.7</cell><cell>60.1</cell><cell>65.1</cell><cell>Ours</cell><cell>41.3</cell><cell>12.2</cell><cell>3.6</cell><cell>54.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results of model generalization to new domains.</figDesc><table><row><cell>Model</cell><cell>Source</cell><cell cols="2">Target Domain</cell></row><row><cell></cell><cell>16-shot MR</cell><cell>SST-2</cell><cell>CR</cell></row><row><cell>FT</cell><cell>76.9</cell><cell>71.4</cell><cell>64.7</cell></row><row><cell>LM-BFF (man)</cell><cell>87.0</cell><cell>88.9</cell><cell>86.9</cell></row><row><cell>LM-BFF (D-demo)</cell><cell>86.6</cell><cell>89.3</cell><cell>87.5</cell></row><row><cell>KPT</cell><cell>86.8</cell><cell>89.1</cell><cell>86.7</cell></row><row><cell>RETROPROMPT</cell><cell>88.0</cell><cell>91.4</cell><cell>88.8</cell></row><row><cell></cell><cell cols="2">16-shot QQP MRPC</cell><cell>RTE</cell></row><row><cell>FT</cell><cell>60.7</cell><cell>43.7</cell><cell>48.0</cell></row><row><cell>LM-BFF (man)</cell><cell>65.4</cell><cell>20.9</cell><cell>65.5</cell></row><row><cell>LM-BFF (D-demo)</cell><cell>68.2</cell><cell>38.8</cell><cell>66.2</cell></row><row><cell>KPT</cell><cell>71.6</cell><cell>42.3</cell><cell>65.8</cell></row><row><cell>RETROPROMPT</cell><cell>74.0</cell><cell>49.4</cell><cell>67.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The upper part shows the average percentage of positive phrases over different memory groups of positive/negative instances. The lower part denotes the mean values of memorization score on the SST-2 dataset.</figDesc><table><row><cell>Mem Group</cell><cell></cell><cell>Negative</cell><cell></cell><cell></cell><cell>Postive</cell><cell></cell></row><row><cell></cell><cell>FT</cell><cell cols="2">LM-BFF OURS</cell><cell>FT</cell><cell cols="2">LM-BFF OURS</cell></row><row><cell>Top-10%</cell><cell>34.29</cell><cell>32.78</cell><cell>30.23</cell><cell>68.75</cell><cell>69.71</cell><cell>75.67</cell></row><row><cell>ALL</cell><cell></cell><cell>23.40</cell><cell></cell><cell></cell><cell>86.39</cell><cell></cell></row><row><cell>Bottom-10%</cell><cell>17.63</cell><cell>16.25</cell><cell>14.42</cell><cell>95.92</cell><cell>95.08</cell><cell>94.53</cell></row><row><cell></cell><cell></cell><cell>FT</cell><cell cols="2">LM-BFF</cell><cell>OURS</cell><cell></cell></row><row><cell>MEM SCORE</cell><cell></cell><cell>4.597</cell><cell cols="2">0.121</cell><cell>0.032</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Note that F (p kN N ) is defined to represent the difficulty of the sample discriminated by kNN distribution. And the Table 6 also shows that F (p kN N ) indeed reflect atypicality of examples, which validate the effectiveness of the kNN guided training.</figDesc><table><row><cell>Case Analysis. As shown in Table 6, we manu-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ally list the top-ranked and bottom-ranked training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>instances of SST-2 according to our model. It re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>veals that the top-ranked memorized instances seem</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>to show universal opinions indirectly. Thus, we in-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>spect them as atypical/hard for sentiment classifica-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tion. While those instances with 0 memorization</cell><cell>Model</cell><cell></cell><cell></cell><cell>16-shot</cell><cell></cell><cell></cell></row><row><cell>scores are straightforward to show their opinion for</cell><cell></cell><cell cols="5">SST-2 CR MNLI QQP TACRED</cell></row><row><cell>sentiment classification, representing the typical in-</cell><cell>OURS</cell><cell>93.9</cell><cell>91.9</cell><cell>71.1</cell><cell>74.0</cell><cell>40.7</cell></row><row><cell>stance.</cell><cell>w/o kNN-test w/o kNN-train</cell><cell>93.2 92.0</cell><cell>91.2 91.2</cell><cell>70.4 68.8</cell><cell>73.0 71.3</cell><cell>38.2 36.5</cell></row><row><cell></cell><cell>w/o N-demo</cell><cell>92.4</cell><cell>90.8</cell><cell>69.1</cell><cell>72.0</cell><cell>37.6</cell></row><row><cell></cell><cell>w/o refresh</cell><cell>93.5</cell><cell>91.5</cell><cell>70.7</cell><cell>73.6</cell><cell>39.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Performance on 16-shot CR and TACRED with different representations of key and calculate function of kNN distribution.</figDesc><table><row><cell cols="2">Key Repres. kNN Acq.</cell><cell>CR TAC.</cell></row><row><cell>Prompt</cell><cell cols="2">Rep-similar 91.9 40.7</cell></row><row><cell>[CLS]</cell><cell cols="2">Rep-similar 89.0 37.2</cell></row><row><cell>Prompt</cell><cell>BM25</cell><cell>89.5 38.8</cell></row><row><cell>[CLS]</cell><cell>BM25</cell><cell>88.7 36.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Case examples of Top-3 and Bottom-3 memorized instance of ours from trainset of SST-2.</figDesc><table><row><cell>Negative</cell><cell></cell><cell></cell><cell>Positive</cell><cell></cell><cell></cell></row><row><cell>Content</cell><cell cols="3">Mem F (p kNN ) Content</cell><cell cols="2">Mem F (p kNN )</cell></row><row><cell>Although god is great addressed interesting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>matters of identity and heritage, it's hard to shake the feeling that it was intend to be a</cell><cell cols="2">0.066 1.17</cell><cell>A b-movie you can sit through, enjoy on a certain level and then forget.</cell><cell cols="2">0.020 0.18</cell></row><row><cell>different kind of film.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>A film that will be best appreciated by those</cell><cell></cell><cell></cell></row><row><cell>A standard police-oriented drama that, were</cell><cell></cell><cell></cell><cell>willing to its extremely languorous</cell><cell></cell><cell></cell></row><row><cell>it not for deniro's participation, would have</cell><cell cols="2">0.011 1.48</cell><cell>rhythms, waiting for happiness is ultimately</cell><cell cols="2">0.010 0.43</cell></row><row><cell>likely wound up a tnt original.</cell><cell></cell><cell></cell><cell>thoughtful without having much dramatic</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>impact.</cell><cell></cell><cell></cell></row><row><cell>A hit and miss affair, consistently amusing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>but not as outrageous or funny as cho may have intended or as imaginative as one might</cell><cell cols="2">0.010 2.74</cell><cell>What's invigorating about is that it doesn't give a damn.</cell><cell cols="2">0.003 0.06</cell></row><row><cell>have hoped.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>A fun family movie that's suitable for all</cell><cell></cell><cell></cell></row><row><cell>It's a loathsome movie, it really is and it makes absolutely no sense.</cell><cell>0.00</cell><cell>0.00</cell><cell>ages-a movie that will make you laugh, cry and realize, 'it's never too late to believe in</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell>your dreams.'</cell><cell></cell><cell></cell></row><row><cell>It is that rare combination of bad writing,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bad direction and bad acting -the trifecta of</cell><cell>0.00</cell><cell>0.00</cell><cell>It's a cool event for the whole family.</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>badness.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>This thing is virtually unwatchable.</cell><cell>0.00</cell><cell>0.00</cell><cell>Good fun, good action, good acting, good dialogue, good pace, good cinematography.</cell><cell>0.00</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Detailed dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell cols="2"># Class Test Size</cell></row><row><cell>SST-2</cell><cell>Sentiment</cell><cell>2</cell><cell>872</cell></row><row><cell>MR</cell><cell>Sentiment</cell><cell>2</cell><cell>2,000</cell></row><row><cell>CR</cell><cell>Sentiment</cell><cell>2</cell><cell>2,000</cell></row><row><cell>MNLI</cell><cell>NLI</cell><cell>3</cell><cell>9,815</cell></row><row><cell>QNLI</cell><cell>NLI</cell><cell>2</cell><cell>5,463</cell></row><row><cell>QQP</cell><cell>Paraphrase</cell><cell>2</cell><cell>40,431</cell></row><row><cell>FewNERD</cell><cell>Entity Typing</cell><cell>66</cell><cell>96,901</cell></row><row><cell>SemEval</cell><cell>Relation Extraction</cell><cell>19</cell><cell>2,717</cell></row><row><cell cols="2">TACRED Relation Extraction</cell><cell>42</cell><cell>15,509</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameter settings.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell>maximum sequence length</cell><cell>{128, 256}</cell></row><row><cell>max training step</cell><cell>800</cell></row><row><cell>evaluation step</cell><cell>80</cell></row><row><cell>learning rate</cell><cell>{1e-5, 2e-5, 5e-5}</cell></row><row><cell>batch size</cell><cell>{2, 4, 8}</cell></row><row><cell>adam epsilon</cell><cell>1e-8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Specifically, we refresh the knowledge-store for each epoch in our experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://www.quora.com/q/quoradata/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/princeton-nlp/LM-BFF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/ShengdingHu/KnowledgeablePromptTuning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/princeton-nlp/LM-BFF</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? varies: Figure <ref type="figure">4</ref>(a) shows the performance of the model when the ? increases and reveals that the model performs worse as the ? increases on a 16-shot CR dataset. This finding indicates that a moderate degree of kNN guiding training is essential since kNN can help the model attend to hard examples, but excessive attendance of kNN-train also can bring the noise. ? varies: From Figure <ref type="figure">4</ref>(b), we find that model achieves optimal results on a 16-shot MR dataset when ? is set to be 0.2 while attaining the best results on MR in the zero-shot setting when ? is set to be 0.7. We think the model may require more reference when there is no data for training.</p><p>k varies: As shown in Figure <ref type="figure">4</ref>(c), the model performance in the 16-shot MR dataset fluctuates very little. In contrast, the result in the zero-shot MR dataset continues to improve as k increases until it converges when reaching a threshold (k = 256). It illustrates that the knn retrieval provides more evidence for reference in zero-setting.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neurosymbolic language modeling with automaton-augmented retrieval</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The local paradigm for modeling and control: from neuro-fuzzy to lazy learning. Fuzzy sets and systems</title>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Bontempi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Bersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Birattari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS 2020</title>
		<meeting>NeurIPS 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lightner: A lightweight generative framework with prompt-guided attention for low-resource NER</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2109.00720</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2104.07650</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<title level="m">Prompt-learning for fine-grained entity typing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-nerd: A few-shot named entity recognition dataset</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<editor>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3198" to="3213" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memorization vs. generalization : Quantifying data leakage in NLP performance evaluation</title>
		<author>
			<persName><forename type="first">Aparna</forename><surname>Elangovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="1325" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Does learning require memorization? a short tale about a long tail</title>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020</title>
		<editor>
			<persName><forename type="first">Konstantin</forename><surname>Makarychev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yury</forename><surname>Makarychev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Madhur</forename><surname>Tulsiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gautam</forename><surname>Kamath</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julia</forename><surname>Chuzhoy</surname></persName>
		</editor>
		<meeting>cedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">June 22-26, 2020. 2020</date>
			<biblScope unit="page" from="954" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What neural networks memorize and why: Discovering the long tail via influence estimation</title>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">REALM: retrieval-augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>CoRR, abs/2002.08909</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">PTR: prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2105.11259</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient nearest neighbor language models</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Won</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><surname>Dumouchel</surname></persName>
		</editor>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">August 22-25, 2004. 2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2108.02035</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bert-knn: Adding a knn search component to pretrained language models for better QA</title>
		<author>
			<persName><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nearest neighbor machine translation</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. OpenReview.net, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reordering examples helps during priming-based few-shot learning</title>
		<author>
			<persName><forename type="first">Sawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4507" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Good examples make A faster learner: Simple demonstration-based learning for low-resource NER</title>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahak</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshen</forename><surname>Kadakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno>CoRR, abs/2110.08454</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP 2021</title>
		<meeting>ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">What makes good in</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>context examples for gpt-3? CoRR, abs/2101.06804, 2021</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/2103.10385</idno>
	</analytic>
	<monogr>
		<title level="j">GPT understands</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno>CoRR, abs/2104.08786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Template-free prompt tuning for few-shot NER</title>
		<author>
			<persName><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno>CoRR, abs/2109.13532</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Text classification using label names only: A language model self-training approach</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">GNN-LM: language modeling based on global contexts via GNN</title>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2110.08743</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</editor>
		<meeting><address><addrLine>University of Michigan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2005-06-30">25-30 June 2005. 2005</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Jian</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</editor>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11-01">2016. November 1-4, 2016. 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sentence-bert: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning to retrieve prompts for in-context learning</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno>CoRR, abs/2112.08633</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno>CoRR, abs/2110.08207</idno>
		<editor>Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F?vry, Jason Alan Fries, Ryan Teehan</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatically identifying words that can serve as labels for few-shot text classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2020</title>
		<meeting>EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013<address><addrLine>Grand Hyatt Seattle, Seattle</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-10-21">18-21 October 2013. 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Memorisation versus generalisation in pre-trained language models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>T?nzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7564" to="7578" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019. OpenReview.net, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/2109.01652</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">June 1-6, 2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Do prompts solve NLP tasks using natural language?</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2203.00902</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Differentiable prompt makes pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2108.13161</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">An empirical study of memorization in NLP</title>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR, abs/2203.12171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
