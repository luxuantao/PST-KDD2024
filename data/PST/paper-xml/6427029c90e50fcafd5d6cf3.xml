<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ah-Q: Quantifying and Handling the Interference within a Datacenter from a System Perspective</title>
				<funder ref="#_utk2Qu6 #_S7HYbKf">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_sAQdCf5">
					<orgName type="full">National Key RD Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuhang</forename><surname>Liu</surname></persName>
							<email>liuyuhang@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Lab of Processors</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Deng</surname></persName>
							<email>dengxin19g@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Lab of Processors</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiapeng</forename><surname>Zhou</surname></persName>
							<email>zhoujiapeng22s@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Lab of Processors</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Lab of Processors</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Zhongguancun Laboratory</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">State Key Lab of Processors</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ah-Q: Quantifying and Handling the Interference within a Datacenter from a System Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interference among applications frequently occurs in a datacenter and significantly influences the cost-efficiency and the user experience. However, it is challenging for us to quantify the exact intensity of the interference that occurred in the overall system of a datacenter, because there are many concurrent applications in a datacenter, and their type can be either latency-critical (LC) and best-effort (BE). To address this issue, we present the Ah-Q which includes a theory and a strategy.</p><p>First, we propose the "system entropy" (E S ) theory to holistically and analytically quantify the interference in a datacenter to address this vital issue. The interference is caused by the scarcity of resources or/and the irrationality of scheduling. As more appropriate scheduling can compensate for resource scarcity, we derive the concept of "resource equivalence" to quantify the effectiveness of a resource scheduling strategy. We evaluate different resource scheduling strategies to validate the correctness and effectiveness of the proposed theory.</p><p>Moreover, using the theory to eliminate interference, we propose a new resource scheduling strategy; i.e., ARQ, which dynamically allocates the isolated resources and the shared resources to simultaneously harvest the benefits of isolation and sharing. Our results show that compared to the state-of-theart strategies (PARTIES and CLITE), ARQ is more effective to reduce the tail latency of the LC applications and to increase the IPC of the BE applications. Compared with PARTIES and CLITE, ARQ increases the yield (the ratio of satisfactory LC applications) by 25% and 20%, respectively; when the load is low, ARQ increases IPC of BE applications by 63.8% and 37.1%, respectively; ARQ reduces E S by 36.4% and 33.3%, respectively. The effectiveness of ARQ has saved resources significantly to achieve the same satisfactory overall user experience.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In a typical datacenter, there are two types of applications. The first type is latency-critical (LC) applications, such as Redis <ref type="bibr" target="#b0">[1]</ref> and Moses <ref type="bibr" target="#b23">[24]</ref>. User experience of these LC applications is affected by tail latency and user expectation. The second type is best-effort (BE) applications, e.g., Spark <ref type="bibr" target="#b53">[54]</ref> and Fluidanimate <ref type="bibr" target="#b2">[3]</ref>. Performance of these BE applications is usually quantified in terms of instructions-per-cycle (IPC).</p><p>To improve resource efficiency in a datacenter, multiple applications are typically collocated on the same node. However, interference and contention in shared hardware resources negatively affect applications' performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. For LC applications, interference can make an exceedingly destructive impact because the user experience is pretty sensitive to the tail latency. For BE applications, though not fatal effects, the drop of IPC due to interference is still desired to be as small as possible. The user experience of BE applications should not be overlooked. We use relative importance (RI) to term the importance difference between LC and BE applications.</p><p>As many different applications are concurrently running in a datacenter, we have an array of tail latency or instructions per cycle (IPC) values, which makes it challenging for us to tell the exact intensity of the interference that occurred in the overall system of a datacenter (a detailed example will be presented in Section II-C). The reason is that the tail latency and IPC are from an individual application perspective rather than the system perspective. Hence, how to quantify and reduce the interference collectively in a datacenter is a vital issue that needs to be addressed.</p><p>Prior work has used various methods to quantify interference, including the ratio of tail latency over instruction throughput <ref type="bibr" target="#b43">[44]</ref>, reduced service rate of a virtual machine (VM), and the duration of interference <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. These methods are effective and make sense in special cases. However, they are mainly ad hoc, and their units are not well defined, making it difficult to apply in different scenarios.</p><p>In this study, we propose system entropy (E S ) to quantify the interference in a datacenter, following a three-step paradigm inspired by information entropy. Recall that Shannon has quantified uncertainty using information entropy in three steps <ref type="bibr" target="#b39">[40]</ref>. Shannon first gave the required properties of information entropy, then proposed an analytical expression, and proved that the expression satisfies the required properties. E S is fundamentally different from information entropy, but will also be proposed in a similar three-step manner. That is, we first itemize the required properties of E S , and then propose the analytical expression of E S , and finally validate that the expression has the required properties. Based on the analysis of the interference phenomenon of datacenters, we analyze the reasons for high tail latency, and then we distinguish and quantify three different types of interference.</p><p>E S systematically quantifies the degree of interference in a datacenter. Specifically, E S can be decomposed into LC entropy (E LC ) and BE entropy (E BE ). E LC quantifies the interference that LC applications have received out of their tolerance. E BE quantifies the interference that BE applications have suffered. If a datacenter only runs LC applications, E S is just E LC . Similarly, if there are only BE applications on the node, E S is simply E BE . If LC and BE applications co-exist, E S is the linear combination of E LC and E BE . E S accommodates the performance degradation of all the collocated applications in the same datacenter, regardless of their characteristics and their possibly different performance metrics. In this way, we can use a single value to quantify interference of the overall system, and the metric is robust to various collocation scenarios.</p><p>We conducted a series of experiments and show how E S changes with varying resource allocations and scheduling strategies. We prove that E LC is correct and effective for guiding and evaluating different resource scheduling strategies. For instance, when E LC = 0, the tail latency requirements of all the LC applications have been satisfied; i.e., the yield (the ratio of satisfactory LC applications) is 100%. When E LC &gt; 0, the absolute value of E LC reflects how much of the overall user experience of the LC applications has not been satisfied.</p><p>Many state-of-the-art resource managers in prior studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> used software and hardware resource isolation techniques to strictly isolated collocated applications, eliminating resource interference. Some researchers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37]</ref> shown that resource isolation may reduce resource utilization. However, their methods only focus on cache partitioning, and are only designed for BE applications. In this study, we find that strict isolation usually reduces resource utilization when interference among applications is not severe, and allowing resources to be flexibly isolated or shared among applications gives huge potential to mitigate the interference of LC and BE applications.</p><p>In this paper, we make the following main contributions:</p><p>We propose the Ah-Q toolkit which includes a theory to quantify and a strategy to handle the interference within a datacenter.</p><p>We propose the required properties and the analytical expression of system entropy, E S . E S is a dimensionless single "figure of merit" of a datacenter, very useful for interference quantification and evaluation. Based on E S , we propose the concept of "resource equivalence" to evaluate the effectiveness of different scheduling strategies.</p><p>Using the detected entropy as the feedback signal, to reduce interference, we design an associative scheduling strategy, ARQ, which allows partial resource sharing among BE and LC applications, and dynamically adjusts the size of isolated and shared resources. A space-time resource utilization model has been built to reveal the cause of interference and interpret the advantages of ARQ over previous strategies.</p><p>We compare ARQ with the state-of-the-art strategies, CLITE <ref type="bibr" target="#b35">[36]</ref> and PARTIES <ref type="bibr" target="#b7">[8]</ref>. Our evaluation results show that, compared with PARTIES and CLITE, ARQ reduces E S by 36.4% and 33.3% on average, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE SYSTEM ENTROPY (E S )</head><p>Below, we propose the system entropy (E S ) to quantify the interference occurring in a datacenter, following Shannon's information entropy paradigm <ref type="bibr" target="#b39">[40]</ref>. First, we present the required properties of the measure (in sub-section II-A, then propose an analytical definition (in sub-sections II-B to II-B), and finally, validate their consistency (in section III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Required Properties of E S</head><p>Considering the crucial influences of resource amount and resource scheduling strategy on interference, we require E S to satisfy the following three properties.</p><p>Dimensionless: E S should have no dimension (e.g., its unit should not be a time or resource unit), and its value should be between 0 and 1. The closer the value is to 1, the greater the interference is.</p><p>Resource amount sensitiveness: Given a set of co-running applications and a resource scheduling strategy, when the number of available resources in a datacenter increases, E S should decrease or at least not increase.</p><p>Scheduling strategy sensitiveness: Given a set of corunning applications and a fixed number of available resources, when the scheduling strategy has reduced the resource contention among applications, E S should decrease.</p><p>In the rest of this section, we introduce the analytical expression of E S in three different scenarios in a datacenter. Table <ref type="table" target="#tab_0">I</ref> lists the symbol abbreviations used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Analytical Expression of E S</head><p>The first scenario is that only N different LC applications are running, but no BE application exists in a datacenter. In this scenario, E S is E LC , which is defined as follows.</p><p>There are three basic attributes for each LC application in a datacenter. For application i (i = 1, 2, ..., N), T L i0 denotes application i's ideal tail latency (i.e., the tail latency when application i has not suffered any interference), T L i1 is the tail latency of application i when application i is under collocation, potentially suffering interference, and M i is the maximum tail latency that application i can tolerate. Note that the ideal latency T L i0 can be obtained through resource isolation technology to temporarily allocate sufficient resources to application i. We can quantify the interference tolerance of application i as Eq. <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">A i = 1 - T L i0 M i (1)</formula><p>According to our observations, a user of an LC application determines M i following two principles: (1) The more critical the application is, the smaller the tail latency threshold will be.</p><p>(2) Users usually choose a value from the flat to small-slope region as M i . Therefore, the user-defined target is a threshold affected by many factors and is only of a reference significance <ref type="bibr" target="#b41">[42]</ref>, and thus has some elasticity. In this study, we assume that the relative elasticity of M i is 5%.</p><p>Since T L i0 &lt; M i , the range of A i is [0, 1]. The smaller M i is, the closer A i is to 0, and the smaller the application's interference tolerance is, and vice versa. We use R i in Eq.</p><p>(2) to quantify the interference that application i suffers.</p><formula xml:id="formula_1">R i = 1 - T L i0 T L i1<label>(2)</label></formula><p>Since T L i0 &lt; T L i1 , the range of R i is (0, 1). The smaller T L i1 is, the closer R i is to 0, indicating that the interference suffered by the application is small, and vice versa. We use ReT i in Eq.</p><p>(3) to represent the remaining tolerance of application i after being interfered.</p><formula xml:id="formula_2">ReT i = A i &gt; R i ? 1 - T L i1 M i : 0<label>(3)</label></formula><p>We use Q i in Eq. ( <ref type="formula" target="#formula_3">4</ref>) to represent the interference that application i cannot tolerate. When the interference application i suffers (i.e., R i ) is larger than the interference tolerance (i.e., A i ),</p><formula xml:id="formula_3">Q i = 1 -M i T L i1 . Otherwise, Q i = 0. Q i = R i &gt; A i ? 1 - M i T L i1 : 0<label>(4)</label></formula><p>The A i , R i and Q i above inspired us to develop a resource scheduling strategy (referred to as ARQ) which will be presented in Section IV. Moreover, we define E LC as the interference that the LC applications cannot tolerate, which can be expressed as Eq. ( <ref type="formula">5</ref>).</p><formula xml:id="formula_4">E LC = 1 N N ? i=1 Q i (5)</formula><p>The second scenario is that only M different BE applications are running, but no LC application exists in a datacenter. In this scenario, E S is the BE entropy (E BE ). As shown in Eq. ( <ref type="formula" target="#formula_5">6</ref>), we define E BE as the slowdown incurred by the interference that the BE applications have suffered, where IPC solo (i) denotes the IPC when the BE application i runs alone and IPC real (i) denotes the IPC when the BE application i suffers interference.</p><formula xml:id="formula_5">E BE = 1 - M ? M i=1 IPC solo (i) IPC real (i)<label>(6)</label></formula><p>When none of the BE applications suffers any interference, E BE is 0. The higher the interference occurring for application i is, the larger the ratio of IPC solo (i) over IPC real (i) is, and the closer the value of E BE is to 1.</p><p>The third scenario is that LC and BE applications co-exist in a datacenter. In this scenario, as shown in Eq. ( <ref type="formula" target="#formula_6">7</ref>), E S is the linear combination of E LC and E BE , where the relative importance (RI) is involved.</p><formula xml:id="formula_6">E S = RI ? E LC + (1 -RI) ? E BE (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>The rationale behind Eq. ( <ref type="formula" target="#formula_6">7</ref>) is to eliminate E LC and E BE simultaneously to achieve the minimum E S . Generally, the range of RI is [0, 1]. However, when the resources are insufficient, reducing E LC should take precedence over reducing E BE , which changes the range of RI into [0.5, 1].</p><p>Interestingly, Scenario 1 and 2 are the extreme cases of Scenario 3. Specifically, when only BE applications exist in a datacenter, only E BE needs to be considered in system entropy, and therefore RI is chosen to be 0. This is typical in conventional high-performance computing. When only LC applications are running in the system, RI is assigned to be 1. The larger the value of RI, the higher the priority of the LC applications over that of the BE applications. Datacenter managers can determine the value of RI by considering several factors (e.g., the criticality of LC applications, the fairness among all the applications, and the economic benefit of the datacenter). In this study, without losing representativeness, we set RI to 0.8.</p><p>In our current model, all LC applications are treated equally, and so as BE applications. The reason is that we focus on the criticality difference between LC and BE applications. If necessary, the E S model can be extended to involve different RI factors among the same type of applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Advantages of E S</head><p>This section will show the advantages of the proposed E S over tail latency and IPC with a simple example. Figure <ref type="figure" target="#fig_0">1</ref> shows the tail latency, tail latency threshold of the LC applications and IPC of the BE applications under two different strategies (i.e., A and B). With the IPC and tail latency values shown in Figure <ref type="figure" target="#fig_0">1</ref>, it is not straightforward for us to distinguish which strategy is better, yet we can precisely and reasonably do this with E S . The reason lies in the following advantages of E S .</p><p>First, E S is concise and easy-to-use in practice. If IPC and tail latency are used, there will be many individual performance data that must be considered simultaneously. Assume N different LC applications and M different BE applications coexist in a datacenter. For each strategy, we need to examine 2N + M different performance values (i.e., the tail latency and the target threshold of each LC application, and the IPC of each BE application). In the example of Figure <ref type="figure" target="#fig_0">1</ref>, even though there are only three LC applications and one BE application (the number of collocated applications is much larger in the real cloud <ref type="bibr" target="#b17">[18]</ref>), for each strategy, we still need to examine 7 values at the same time (3 tail latency values, 3 tail latency thresholds and 1 IPC value), which is a challenging task.</p><p>Second, E S reflects the overall user experience of many collocated applications more comprehensively. The change in the resource scheduling strategy may improve the performance of some applications and degrade the performance of others. In this example, for strategy B, although the tail latency of LC application Img-dnn is improved, the BE application's IPC has dramatically deteriorated. Therefore, with IPC and tail latency, it is difficult to determine whether the overall user experience of the datacenter is improved or not. QoS guarantee does not necessitate reducing E LC to zero; that is, a small E LC is tolerable. E S reflects this observation in its definition. It is noteworthy that strategy A is not inferior to strategy B because the QoS violation in strategy A is tolerable. The LC application (Img-dnn) QoS violation is small (i.e., 4.4%), which is less than the elasticity of the tail latency threshold (i.e., 5%), and the IPC improvement of the BE application (Fluidanimate) is significant (from 1.15 to 2.63, that is 128.7%), so it is more reasonable to prefer strategy A over strategy B.</p><p>Third, E S can be used to define resource equivalence. Given the budget and power constraints, it becomes increasingly difficult to increase the available resources of a datacenter <ref type="bibr" target="#b48">[49]</ref>. Therefore, it is crucial to focus on improving the usage and increasing the utilization of resources rather than increasing available resources. When evaluating the optimization of a datacenter, we can express the effectiveness in the following form: when achieving the same "overall user experience", how many resources can be saved by a new strategy compared to the baseline strategy. We can use E S to evaluate the improvement of a scheduling strategy over another one in terms of resource saving. We say a scheduling strategy p 1 is inferior to p 2 if p 1 has to use more resources to achieve the same E S as p 2 . Suppose the amount of resources used by p 2 is R, and p 1 uses ?R more sources, this means that E S (p 1 , R + ?R) = E S (p 2 , R). The improvement from p 1 to p 2 is equivalent to increasing ?R amount of resources, and ?R is referred to as the resource equivalence of strategy p 2 relative to p 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VERIFICATION OF E S</head><p>In this section, we verify that the analytical expression of E S has satisfied all the required properties listed in Section II-A. It is easy to prove that E S has the "dimensionless" property. We only need to focus on the other two properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Resource Amount Sensitiveness of E S</head><p>To verify how E S varies with the number of available resources, we run one BE application (Fluidanimate) and three LC applications (Xapian, Moses and Img-dnn with 20% of max load) concurrently in a datacenter.  Table <ref type="table" target="#tab_1">II</ref> shows E LC , E BE and E S of Unmanaged when these applications run on 6-8 cores and all LLC ways. The max tail latency that an application can tolerate (i.e., M i ) and the ideal tail latency T L i0 are constant values. They are measured with enough resources, so the interference tolerance A i does not depend on the number of available resources. When 6 processor cores are available, the real tail latency T L i1 of the three applications is higher than the M i , so ReT i is equal to 0. When more processor cores are available, ReT i increases to 0.23, indicating that the remaining tolerance of the system is high at present, and there exist redundant resources that can be used to handle more requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cores Applications</head><formula xml:id="formula_8">T L i0 T L i1 M i A i R i ReT i Q i E LC E BE E S<label>6</label></formula><p>When resources are scarce (the number of processor cores is 7), E LC is large (i.e., 0.23). At this time, reducing the number of available processing units to 6, will make the tail latency deviate significantly from M i and thus E LC is increased to 0.64. However, if the number of processor cores increases to 8, the interference among applications will be reduced to an application-tolerable level (?i, R i &lt; A i ), and E LC becomes 0 at this time.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows E S of Unmanaged and ARQ when the number of available processing units ranges from 4 to 10, and the number of LLC ways per set ranges from 4 to 20. When the number of available resources decreases for both strategies, E S shows an increasing trend, verifying the second property of E S . When the number of resources is sufficient (e.g., 10 processing units, 20 LLC ways), even if with the Unmanaged strategy, the interference among applications is small, with E S only 0.006. When the number of resources is insufficient (e.g., 6 processing units, 20 LLC ways), resource contention is severe, which causes E S as high as 0.53. For the ARQ strategy, when the resources are sufficient (e.g., 10 processing units, 20 LLC ways), E S is 0.008. However, when the number of resources is insufficient (6 processing units, 20 LLC ways), E S of the ARQ strategy is 0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scheduling Strategy Sensitiveness of E S</head><p>Resource equivalence describes the difference in the number of resources between two different scheduling strategies when they reach the same E S . The concept of "resource equivalence" is illustrated in different forms as shown in Figure <ref type="figure" target="#fig_2">3</ref>(a) and (b). Figure <ref type="figure" target="#fig_2">3(a)</ref> shows E S of two strategies: Unmanaged and ARQ (the experimental setup will be described in Section V). The x-axis is the total number of available processing units, and the y-axis is the corresponding E S . To make E S reach 0.25, the Unmanaged strategy requires 7.61 cores, while the proposed ARQ strategy only requires 5.61 cores. The two-core resource that ARQ saved is the resource equivalence of the ARQ strategy compared to the Unmanaged strategy. Similarly, when E S is 0.4, the resource equivalence is 1.83 cores.</p><p>Figure <ref type="figure" target="#fig_2">3</ref>(b) shows the isentropic lines of different scheduling strategies when E S = 0.3. Each line represents the number of processing cores (y-axis) and LLC ways (x-axis) required to achieve the same E S (i.e., 0.3). As shown in Figure <ref type="figure" target="#fig_2">3(b)</ref>, when there are more than 10 LLC ways (the right side of the red dashed line), the isentropic lines of ARQ, CLITE and PARTIES are close to each other (i.e., resource equivalence R is close to 0). However, when the number of available LLC ways &lt; 10, the total amount of available resource is scarce and resource conflict is severe, and ARQ is able to achieve the same E S with much fewer processing cores. For example, when 8 LLC ways are available, compared to PARTIES and CLITE, ARQ has saved 1 processing core for the data center, that is, the resource equivalence is 1 processing core (i.e., 12.5% processing cores). Similarly, using ARQ instead of the Unmanaged strategy brings a resource equivalence of 2 processing cores to the data center when 8 LLC ways are available (i.e., 25% processing cores have been saved).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE ARQ SCHEDULING STRATEGY</head><p>In this section, we propose a scheduling strategy called ARQ to combine the advantages of resource sharing and resource isolation to reduce system entropy. The name of the strategy is to denote that A i , R i and Q i in section II-B are three vital factors of an LC application. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Demonstrating the Key Insight via a Space-time Model</head><p>It is observed that, resource isolation can reduce performance uncertainty, and resource sharing can increase resource utilization and overall throughput. Therefore, we exploit the combination of resource isolation and resource sharing to make E S as small as possible.</p><p>The state-of-the-art resource scheduling strategies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref> used resource isolation techniques to guarantee the QoS. That is, each application can only use the resources allocated to itself but cannot use the resources allocated to other applications. However, the resource isolation in these strategies leads to low resource utilization.</p><p>Take the processing unit resources as an example. We assume that only when the datacenter can provide a service rate of at least U, the QoS target of the LC applications can be satisfied. We also assume that one core can provide a service rate of 0.8U, and two cores can provide a service rate of 1.6U. If we allocate only one core to the LC application, the tail latency of the LC applications will violate the QoS target, since the service rate is 0.8U, which is less than U. However, if two cores are allocated to the LC application, the QoS target of the LC application can be met, but it would degrade the throughput of the BE application due to the waste of resources.</p><p>As shown in Figure <ref type="figure" target="#fig_3">4</ref>, a space-time model is presented to illustrate different resource scheduling schemes. For brevity, we only consider two LC applications (i.e., LC 1 and LC 2 ) and one BE application (i.e., BE), and examine the usage of only one resource-slice (e.g., one processing unit or one LLC way) and eight time-slices. There are three different scenarios.</p><p>In scenario (a), each application is running alone, so we can know exactly the space-time resource requirement of each application. For a time-slice, when there exist two or more ticks, resource contention will occur. For instance, in timeslice 6, all the three applications need the same resource-slice, thus resource conflict occurs.</p><p>In scenario (b), the resource-slice is isolated, and is exclusively allocated to LC 1 , so only LC 1 can use the resourceslice, guaranteeing the QoS of LC 1 . However, during some time-slices (e.g., time-slice 3), the resource-slice is not needed by LC 1 , but other applications that require the resource-slice cannot use it, incurring resource waste.</p><p>In scenario (c), the resource is shared among all the applications although the LC applications take precedence over the BE applications. At the beginning of time-slice 3, the resource owner is changed from LC 1 to BE, increasing the throughput of BE. Meanwhile, it is noteworthy that the change of the resource ownership is not free, due to the context switching overhead and/or the cache pollution. The triangle represents that the resource-slide can boost the application performance with overhead. At the beginning of time-slice 4, the resource owner is transferred from BE to LC 2 , improving the QoS of LC 2 .</p><p>Comparing (c) with (b), the number of crosses is reduced from 10 to 6 and there are four more triangles in scenario(c), and the resource utilization ratio has been almost doubled. The key insight is that, although resource isolation is an effective means for reducing performance uncertainty, resource sharing is crucial for improving system utilization. Therefore, in terms of the overall user experience, neither complete isolation nor sharing is the optimal strategy, and we need to simultaneously harvest the advantages of both isolation and sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design of the ARQ Strategy</head><p>A resource region includes a number of cores and cache ways. ARQ divides resources into shared and isolated regions based on the aforementioned key insight. Each LC application can use not only the resource of its own isolated region, but also the resources of the shared region, while the BE application can only run in the shared region. If an LC application running in the shared region can satisfy its QoS target, the resources of the isolated region will be reduced to 0, indicating that it can safely share resources with other applications. Once the QoS of an LC application is severely interfered with while running in the shared region, the ARQ strategy will detect this interference, and gradually increase the resources of its isolated region until the QoS target is satisfied.</p><p>Algorithm 1 shows the ARQ strategy. ARQ periodically (e.g., every 500ms <ref type="bibr" target="#b7">[8]</ref>, 1s <ref type="bibr" target="#b32">[33]</ref> or 2s <ref type="bibr" target="#b35">[36]</ref>) monitors the tail latency of each LC application and the IPC of each BE application to calculate ReT of each LC application and E S . Then, ARQ adjusts resource allocation according to ReT and evaluates the effectiveness of the adjustment by E S . If the adjustment increases E S , we cancel the adjustment and try to take new adjustment action to avoid trapping in local optimum, Algorithm 1 ARQ Resource Scheduling Algorithm. // ReT is an array, the elements of which are the remaining tolerance of each LC application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>ReT ? computeRemainingTolerance() 9:</p><p>if isAd just and E S &gt; E S then 10:</p><p>Cancel the last adjustment and do not allow the last victim region to be penalized in the next 60s. Identify the application i that has the smallest ReT. end if 43: end function that is, do not allow the old adjustment to occur again in the next 60s.</p><p>In the AdjustResource function, the goal is to move one slice of resource from a rich region to a poor region, hopefully decreasing E S . We determine the victim and beneficiary regions by the findVictimRegion and the findBeneficiaryRegion functions according to the ReT array which records the ReT of each LC application. Then, using the findVictimResource, we determine which type of resources will be moved or penalized. Then, we move the selected resource from the victim region to the beneficiary region.</p><p>In the function findVictimResource, we maintain a finite state machine which is as same as that in <ref type="bibr" target="#b7">[8]</ref> to determine the order of resource adjustment. Each state of the state machine represents a resource type (e.g., processing units, LLC capacity, and memory bandwidth). The function will turn to the next type when the current resource type cannot be penalized.</p><p>The function findVictimRegion takes the ReT array as input and outputs the victim region which donates resources to other regions. It traverses the ReT array in descending order to identify the application whose ReT is larger than 0.1. An application with a large ReT may not have isolated resources, so we need to traverse the ReT array in descending order to determine the victim region. If no isolated region satisfies the requirements, the shared region will be returned.</p><p>Then, the findBeneficiaryRegion function takes the ReT array as input and outputs the beneficiary region which receives resources from the victim region. We only need to concern the application with the smallest ReT. if its remaining tolerance is less than 0.05, the isolated region of the application will become the beneficiary region. If all the LC applications have high ReT, the shared region will be the beneficiary region.</p><p>If the victim and beneficiary regions are both shared regions, no LC applications need more resources and no LC applications can donate resources, thus an equilibrium has been reached and the resources adjustment will not be enforced.</p><p>Monitoring interval is configured to be 500ms, which is consistent with that of PARTIES (see Section 4.3 in <ref type="bibr" target="#b7">[8]</ref>). We find that smaller interval allows the scheduler to detect and react to QoS violation more timely, but tail latency becomes less stable, and increases the difficulty to accurately calculate the tail latency. Larger intervals ease tail latency calculations, but each QoS violation will last for a longer period. We find 500ms to be a practically suitable interval from evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Allocation Comparison</head><p>ARQ combines the benefits of resource sharing and isolation, sharing resources among the applications that have high ReT, and isolating the applications that have low ReT. In the following, we present two snapshots to illustrate and compare the allocation processes of ARQ and PARTIES, when the load of Xapian is low and high, respectively. The experimental setup will be described in Section V.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> shows the resource allocation snapshot when the load of Xapian is 30%. Compared with PARTIES, ARQ makes the BE application (i.e., Stream) have more available resources. PARTIES allocates the isolated resources for each application to preferentially reduce E LC . However, the user experience of the BE application is low because it can only use 10% processing unit and 30% LLC ways. ARQ finds that sharing resources among all the applications except Xapian can also make E LC 0. Therefore, in ARQ strategy, Xapian was allocated isolated resources (10% processing unit and 25% LLC ways) to isolate interference, while Img-dnn and Moses shared the shared region resources with the BE application. Although E LC of PARTIES and ARQ are both 0, ARQ is better since it achieves a much lower E BE .</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the resource allocation snapshot when the load of Xapian is 90%. Compared with PARTIES, ARQ makes the high-load LC application (i.e., Xapian) have more available resources, since the other LC applications can be satisfied only with the shared region resources. Given the high load of Xapian, both PARTIES and ARQ want to allocate more isolated resources for Xapian. However, to simultaneously satisfy the QoS targets of Moses and Img-dnn, PARTIES  allocates 50% cores and 60% LLC ways while ARQ only allocates 30% cores and 35% LLC ways by sharing resources among applications. As a result, PARTIES can only allocate 50% cores and 40% LLC ways to Xapian which is not enough to satisfy the QoS target of Xapian, while ARQ allocates 70% cores and 65% LLC ways to Xapian which can significantly reduce E LC and E S . We will evaluate ARQ to compare it with the state-of-the-art strategies in detail in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overhead Comparison</head><p>Like other QoS-aware scheduling strategies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, ARQ involves overhead from two parts: monitoring the system state (e.g., the tail latency and IPC of each application), and allocating system resources (e.g., cache, processor core) periodically. Unlike ML-based scheduling strategies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref>, ARQ does not require complex computations of resource allocations, so there is negligible overhead of computing the resource allocation. The monitoring overhead is negligible since we only read a few counters every 500ms (see the last paragraph in Section IV-B for discussion on the monitoring interval). The overhead of resource adjustment mainly comes from warmup of cache ways for cache re-partitioning and context switching for core re-assignment, and it depends on the frequency of resource re-adjustment.</p><p>When the contention is high, PARTIES can result in pingponging effects between severely resource-starved applications, which incurs overhead everytime resources are switched. Moreover, due to the long queues that have been built up in the system, core allocation in PARTIES may would need more than 500ms to take effect. Compared to PARTIES, ARQ has much less ping-ponging effects, because ARQ applies the shared region as a resource pool which provides more resources for LC applications, reducing the likelihood of QoS violations.</p><p>Our evaluations have involved the overhead of ARQ mentioned above, and the experimental results that will be pre-sented in next sections show that ARQ has small overhead, achieving much less QoS violations than PARTIES (see Figure <ref type="figure" target="#fig_8">8</ref>, 9 and 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL METHODOLOGY</head><p>We conduct experiments on a real server of a datacenter. Table <ref type="table" target="#tab_3">III</ref> shows our experimental platform. We use the taskset command to set the core affinity for each application and use Intel's Cache Allocation Technology (CAT) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> to allocate the LLC for each core. CAT allows for a given number of ways to be assigned to a specific application to limit the amount of LLC space the application can occupy. Consistent with previous studies <ref type="bibr" target="#b7">[8]</ref>, we disabled Hyper-Threading in our experiments. We evaluate the scheduling strategies with several application combinations. Each combination contains multiple LC and BE applications from different domains. Xapian is a search engine that is widely used in popular websites and software frameworks. In our experiments, the search index is built from a dump of the English version of Wikipedia, and query terms are chosen randomly, following a Zipfian distribution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>. Moses is a statistical machine translation system. We drive Moses using randomly chosen dialogue snippets from the English-Spanish corpus <ref type="bibr" target="#b44">[45]</ref>. Imgdnn is a handwriting recognition application. We drive the application using randomly chosen samples from the MNIST database <ref type="bibr" target="#b12">[13]</ref>. Masstree <ref type="bibr" target="#b27">[28]</ref> is a scalable in-memory keyvalue store. We drive Masstree using a modified version of the Yahoo Cloud Serving Benchmark <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. Sphinx <ref type="bibr" target="#b49">[50]</ref> is an accurate speech recognition system. Silo <ref type="bibr" target="#b45">[46]</ref> is a inmemory transactional database. These LC applications are from Tailbench <ref type="bibr" target="#b22">[23]</ref> and are instantiated with 4 threads. We present an example to show how we determine the maximum load that each LC application can tolerate. We select 4 LC applications (i.e., Xapian, Moses, Img-dnn and Sphinx), run each application with different number of processing units, gradually increase their arrival rate of requests, and measure the corresponding tail latency. In this study, the 95 th percentile Fig. <ref type="figure">7</ref>. The relationship between tail latency and arrival rate of requests with 1, 2, 4 and 8 processing units (the dashed lines denote the maximal service rate under varying core counts).</p><p>tail latency is used without losing generality. As shown in Figure <ref type="figure">7</ref>, the lines of different colors correspond to the number of processor cores as 1, 2, 4, and 8. For each LC application, as the arrival rate of requests gradually increases, tail latency increases slowly at the beginning. When the arrival rate of requests exceeds a certain threshold, the tail latency increases exponentially. Similar to previous research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>, we refer to the tail latency at the load threshold as tail latency threshold, which also means the maximum tail latency that an application can tolerate (i.e., the M i in Eq. ( <ref type="formula">1</ref>)), and refer to the load threshold as max load, which means the maximum load that an application can sustain under a reasonable tail latency target. Table <ref type="table" target="#tab_4">IV</ref> summarizes the max load and the tail latency threshold.</p><p>We run different BE applications in our experiments: Fluidanimate, Stream and Streamcluster, respectively. Fluidanimate and Streamcluster are taken from PARSEC benchmark suite <ref type="bibr" target="#b2">[3]</ref>. Fluidanimate conducts a liquid simulation that uses a computational method to solve the Navier-Stokes equation. Streamcluster solves the online clustering problem. Like the LC applications, Fluidanimate and Streamcluster are both instantiated with 4 threads. Stream <ref type="bibr" target="#b31">[32]</ref> is a well-known memory intensive benchmark that performs computation on a large array that cannot fit in the LLC. To generate severe interference to other applications on the processing units, LLC and memory bandwidth, we instantiate Stream with 10 threads.</p><p>In addition to the proposed ARQ, we will evaluate the following scheduling strategies using the theory of system entropy and resource equivalence.</p><p>Unmanaged: This strategy does not distinguish between LC and BE applications, and relies on the default scheduling strategy of the operating system (i.e., Linux's Completely Fair Scheduler), and does not use any isolation mechanism.</p><p>LC-first: This strategy relies on the real-time scheduling strategy of the operating system (i.e., round-robin). It sets the LC applications to the real-time priority. When the real-time process is ready, if the current core is running a non-real-time process, the real-time process immediately preempts the nonreal-time process.</p><p>PARTIES <ref type="bibr" target="#b7">[8]</ref>: This strategy leverages hardware and software resource partitioning technology to adjust resource allocations dynamically. It strictly partitions resources between collocated applications without resource sharing. It calculates the slack of multiple LC applications during a fixed time interval and determines whether resources need to be upsized or downsized according to the slack of each LC application. In this way, it ensures that the QoS targets of the LC applications are not violated.</p><p>CLITE <ref type="bibr" target="#b35">[36]</ref>: This strategy is also based on resource isolation. It uses Bayesian optimization to identify or predict desirable resource allocations, and builds a predictive model for different resource partitioning configurations by sampling several points in large configuration space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION OF THE ARQ STRATEGY</head><p>In this section, we evaluate the ARQ strategy with LC, BE and system entropy in the situation of constant load and fluctuating load, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Case of Constant Load</head><p>Collocated with Fluidanimate: In this experiment, we concurrently run three LC applications (i.e., Xapian, Moses, and Img-dnn) and one BE application (i.e., Fluidanimate), and the load of the LC applications is constant.</p><p>Figure <ref type="figure" target="#fig_8">8</ref> shows E LC , E BE and E S of different strategies when the load of Moses and Img-dnn is 20% (left) and 40% (right) of the max load, respectively, and Xapian's load varies from 10% to 90%.</p><p>When the load of the LC applications is low, the Unmanaged strategy achieves the lowest E S among all the strategies, showing the benefits of resource sharing. The reason is that the interference between applications is not severe at this time, and resource sharing can achieve higher resource utilization than other strategies. However, when the load is high, despite low E BE , the rapid increase in E LC makes E S also increase rapidly, since the Unmanaged strategy does not take any action to guarantee the QoS of the LC applications.</p><p>Compared with the Unmanaged strategy, the LC-first strategy allows the LC applications, to preempt the resources of the BE applications if needed. Although the LC-first strategy has a much lower E LC than the Unmanaged strategy, it incurs a substantial increase in E BE .</p><p>Both PARTIES and CLITE use complete resource isolation to mitigate interference among applications and satisfy the QoS of LC applications. When the load of the LC applications is low, many resources are allocated for the BE application with the premise of guaranteeing the QoS of the LC applications, which leads to low E BE and E S . When the load is high (e.g., the load of Moses and Img-dnn is 20%, respectively, the Xapian's load is larger than 50%), they allocate most resources to the LC applications but few resources to the BE application, which incurs high E BE and E S . As shown in Figure <ref type="figure" target="#fig_8">8</ref>(a), ARQ achieves the lowest E S among all the strategies. ARQ reduces E LC more significantly than other strategies, implying that QoS of the LC applications has been guaranteed preferentially. ARQ has the lowest E BE during most time among all the strategies based on resource isolation. When the load is extremely high, it is reasonable that ARQ has higher E BE than other strategies, because ARQ lets LC applications preferentially occupy the resources of the shared region. In this manner, the characteristic of all the applications has been well utilized to improve the overall user experience of all applications.</p><p>Figure <ref type="figure" target="#fig_8">8</ref>(b) shows more detailed data regarding the tail latency and IPC for one scenario (i.e., when the load of Moses and Img-dnn is 40%). Taking the Unmanaged as the baseline, ARQ reduces the tail latency by 66.5% on average, while CLITE reduces by 43.6% and PARTIES reduces by 37.2%. When the load is low (i.e., Xapian's load 50%), compared with PARTIES and CLITE, ARQ increases IPC by 63.8% and 37.1%, respectively. When the load is pretty high (i.e., Xapian's load 70%), ARQ preferentially optimizes tail latency rather than IPC, and allocates resources to guarantee the QoS of the LC applications.</p><p>Collocated with Stream: In this experiment, we instantiate Stream with 10 threads to represent another type of severe interference among applications. Figure <ref type="figure" target="#fig_9">9</ref> shows E LC , E BE and E S of each strategy and detailed tail latency and IPC.</p><p>Neither the Unmanaged nor the LC-first strategy can satisfy the QoS of the LC applications even if the load is low, resulting  in high E LC and E S . When the load of the LC applications is low (e.g., Xapian's load 50%, and the load of Moses and Img-dnn is 20%, respectively), the other four strategies except the Unmanaged strategy can maintain low E LC . When the load of Xapian is 70%, and the load of Moses and Img-dnn is 40%, respectively, only CLITE and ARQ can eliminate E LC and E S to an ideal level. When the load of LC applications is pretty high (i.e., the load of Xapian is 90%, and the load of Moses and Img-dnn is 40%, respectively), only ARQ can achieve low E LC and E S .</p><p>When the load of Img-dnn and Moses is 40%, respectively, and the load of Xapian is 90%, neither PARTIES nor CLITE can find a feasible resource allocation to satisfy the QoS of the LC applications. Nevertheless, ARQ has reduced E LC to nearly zero (i.e., 0.06). Moreover, ARQ reduces E S by 73.4% (i.e., from 0.94 to 0.25), while CLITE and PARTIES reduce E S by 53.2% and 22.3%, respectively.</p><p>In the experiments above, ARQ achieves the highest yield (the ratio of satisfactory LC applications) and the lowest system entropy. Compared with PARTIES and CLITE, ARQ increases the yield by 25% and 20% respectively (from 60% and 65% to 85%). ARQ reduces E S by 36.4% and 33.3% respectively (from 0.22 and 0.21 to 0.14).</p><p>Collocation with diverse loads: In this experiment, while the load of Moses is fixed to 20%, the load of Xapian and Img-dnn both varies from 10% to 90% when collocated with Stream to comprehensively show the benefits brought by ARQ. Figure <ref type="figure" target="#fig_10">10</ref> shows the entropy heatmap of E LC , E BE and E S when either PARTIES or ARQ scheduling strategy is used.</p><p>By adopting the shared region, ARQ allocates more resources for BE applications when the load of LC applications is low (e.g., E LC equals 0), thus leading to lower E BE . When the load is low (i.e., for the top-left region of each subgraph), more resources are allocated to the shared region since the QoS target of LC applications can be easily satisfied with small private regions. BE applications can obtain more resources from the shared region compared to PARTIES and achieve lower E BE (details are shown in the two subgraphs in the middle of Figure <ref type="figure" target="#fig_10">10</ref>). When the load is high (i.e., for the bottom-right region of each subgraph), LC applications can obtain more resources from the shared region, thus leading to lower E LC at the expense of higher E BE . The detailed reasons are described in Figure <ref type="figure" target="#fig_6">5</ref> and Figure <ref type="figure" target="#fig_7">6</ref> in Section IV-C.</p><p>Another application collocation: We use another combination of applications (Img-dnn, Moses and Sphinx as the LC applications and Stream as the BE application) to further evaluate the scheduling strategies.</p><p>Figure <ref type="figure" target="#fig_11">11</ref> shows E LC , E BE and E S of different strategies when the load of Moses and Sphinx is 20% (left) and 40% (right) of the max load, respectively, and Img-dnn's load varies from 10% to 90%. When the load is low, E S of ARQ is almost as the same as that of PARTIES. When the load is high, Fig. <ref type="figure" target="#fig_1">12</ref>. Tail latency, IPC and E S when 6 LC applications and 2 BE applications are collocated (the load of the LC applications is 20%). compared with the PARTIES, ARQ guarantees the QoS target of the LC applications, reducing E S by 40.93% on average.</p><p>Collocation of even larger number of applications: In this experiment, to further evaluate the effectiveness and robustness of the strategies, we double the number of collocated applications. We concurrently run 6 different LC applications (Moses, Xapian, Img-dnn, Sphinx, Masstree and Silo from Tailbench) and 2 different BE applications (Fluidanimate and Streamcluster from PARSEC). Figure <ref type="figure" target="#fig_1">12</ref> shows tail latency and IPC when the load of each of the LC applications is 20%.</p><p>As shown in Figure <ref type="figure" target="#fig_1">12</ref>, the number of collocated applications has been doubled on the same datacenter, and resource contention becomes even more severe. Compared with PAR-TIES, ARQ drastically reduces the tail latency of Moses and Sphinx (from 29.88 to 5.75 ms, and from 7904 to 2514 ms, respectively) at the cost of a slight increase on the tail latency of Xapian (from 4.06 to 4.17 ms, still satisfying the QoS target according to Table <ref type="table" target="#tab_4">IV</ref>), thus reducing E S significantly. Compared to PARTIES, ARQ reduces E S by 36.4% (from 0.33 to 0.21). Considering Figure <ref type="figure" target="#fig_8">8</ref> and 12, we can conclude that the scalability of ARQ is very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Case of Fluctuating Load</head><p>As many LC applications in a datacenter experience load fluctuations (e.g., high load in the daytime and low load at night) during execution <ref type="bibr" target="#b5">[6]</ref>, in this section, we evaluate different strategies with a fluctuating load. We still choose Xapian, Moses and Img-dnn as LC applications, and Stream as BE applications. We set the load of Moses and Img-dnn as 20% and vary the load of Xapian from 10% to 90%. Figure <ref type="figure" target="#fig_12">13(a)</ref> shows how Xapian's load fluctuates. Figure <ref type="figure" target="#fig_12">13(b)</ref> shows the changes of E LC , E BE and E S for LC-first, PARTIES and ARQ strategies. Figure <ref type="figure" target="#fig_12">13(c</ref>) shows how ARQ and PARTIES dynamically schedule resources to adapt to load fluctuations.</p><p>Figure <ref type="figure" target="#fig_12">13</ref> shows the data during 250 seconds (i.e., 500 data points). During this process, ARQ has 59 tail latency violations, while PARTIES has 105 tail latency violations. These tail latency violations are mainly due to the resource adjustment after the load fluctuations.</p><p>In the beginning, the load of all the three LC applications is low, and thus both PARTIES and ARQ can satisfy the QoS target of all the LC applications. PARTIES only allocates 1 processing unit and 6 LLC ways to the BE application, while ARQ allocates 7 processing units and 15 LLC ways to the shared region. Consequently, compared with PARTIES, ARQ has reduced E BE by 22.3% (i.e., from 0.85 to 0.66), and thus the user experience of the BE applications has been improved significantly.</p><p>During the 100s-120s, Xapian's load is increased to 70%. PARTIES fails to find an allocation to satisfy the QoS target, leading to high E LC . ARQ succeeds in the exploration to find an allocation that satisfies the QoS target. Although it causes some increase in E BE , it deserves and is reasonable because the overall user experience in terms of E S has been improved significantly. During 120s-140s, Xapian's load is increased to 90%; although neither PARTIES nor ARQ can reduce E LC to 0, ARQ has much lower E LC and E S than PARTIES.</p><p>In Figure <ref type="figure" target="#fig_12">13</ref>, there are some spikes in the E LC curve of PARTIES, because PARTIES tentatively downsizes the resources of an LC application to maximize the resources of the BE application. If the LC application no longer satisfies the QoS target after downsizing, it would immediately recover from the previous incorrect downsize action. As shown in Figure <ref type="figure" target="#fig_12">13</ref>, ARQ effectively mitigates the spiking phenomenon, even though it has a downsize action that is more aggressive than that of PARTIES.</p><p>ARQ eliminates the spiking phenomenon by occupying the resources of the shared region. When the load of the LC applications increases and the available resource is insufficient, PARTIES gradually allocate more resources to satisfy the QoS target. In ARQ, to avoid the rapid rise of the tail latency, the LC applications quickly preempt the resources in the shared region from the BE applications. Although this would harm the throughput of the BE applications, it is worthwhile because it guarantees the QoS of the LC applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Interference Characterization: Scott et al. <ref type="bibr" target="#b47">[48]</ref> proposed using the service rate under interference and the duration of time interference lasts to characterize interference. Prior work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref> used the values of IPC or execution time before and after the applications have been interfered with to quantify the interference. However, for the LC applications, users do not concern about IPC, and the change of IPC may be caused by interference from other applications or by fluctuations in their load. Therefore, it is not appropriate to use IPC to quantify the interference for LC applications. Many researchers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref> use the tail latency before and after the interference to quantify the interference of the LC applications. However, the ideal tail latency for different applications varies greatly (from the microsecond level to the second level). Hence, we propose E S to unify the interference of different LC and BE applications. As a measure of interference, E S has interpretability and measurability, and its analytical expression has all the required properties. Therefore, E S is more formal, reasonable, and systematical than the ad hoc metrics <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. There may be applications that care about both latency and IPC. In that case, we cloud either choose a more critical performance metric, or come up with an aggregated metric that takes various metrics into account. It is a challenging scenario even without colocation, and we will leave it as future work.</p><p>Resource Scheduling: How to schedule resources to satisfy the QoS target of different types of applications in a datacenter is a vital issue. Previous studies used software and hardware level resource isolation techniques to manage resources to eliminate interference on specific resources. Many feedbackbased resources managers have been proposed to detect and respond to QoS violations using application state information (such as tail latency and input load). Heracles <ref type="bibr" target="#b26">[27]</ref> collocates the LC applications and the BE applications safely using a threshold-based method to manage interference. PARTIES <ref type="bibr" target="#b7">[8]</ref> dynamically adjusts the resource allocation of each application by monitoring the tail latency to further improve the resource utilization. CLITE <ref type="bibr" target="#b35">[36]</ref> uses Bayesian optimization to explore resource sensitivity to find an allocation with optimal performance. Sturgeon <ref type="bibr" target="#b33">[34]</ref> uses decision trees and binary search to find out an allocation that can satisfy power consumption constraints and QoS targets. Twig <ref type="bibr" target="#b32">[33]</ref> uses multi-agent deep reinforcement learning to improve the energy efficiency of multiple LC applications. Although CLITE, Sturgeon and Twig can all coordinately schedule multiple resources in one step, they have limitations. Specifically, Sturgeon relies on prior application knowledge and offline pre-training; CLITE and Twig involve large amount of computations at runtime to find the best allocation among a large pool of candidate allocations, incurring more overhead and potentially worsening applications' performance. Besides, CuttleSys <ref type="bibr" target="#b24">[25]</ref> regularly evaluates the effect of current allocation and makes decisions to adapt to the changes of the applications by collaborative filtering and dynamically dimensioned search. Sinan <ref type="bibr" target="#b55">[56]</ref> uses CNN and Boosted Tree to predict end-to-end latency and QoS violation probability based on historical system information. Stretch <ref type="bibr" target="#b28">[29]</ref> proposes a method to statically partition the ROB and LSQ capacity resources of collocated tasks. They all perform complete resource isolation for all the applications, but have not explored the opportunities of sharing resources at the right time to maximize resource utilization and system throughput. Dunn <ref type="bibr" target="#b38">[39]</ref> also uses CAT to partition the cache. However, Dunn cares more about system fairness while ARQ focuses on both fairness (between LC and BE applications) and overall system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>As cloud workloads are rapidly changing, it is challenging to achieve the perfect match between applications and the underlying architecture in a datacenter. However, it is crucial for a datacenter to simultaneously achieve high task concurrency (for high resource utilization) and high QoS (in terms of yield and IPC). In this study, we present the Ah-Q which includes a theory and a strategy to address this issue. Specifically, we have proposed system entropy, E S , a holistic and analytical solution to the problem of quantifying the interference incurred by resource contention in a datacenter. We have proposed the ARQ algorithm to harvest the benefits of resource isolation and sharing. We demonstrate the correctness and effectiveness of E S and ARQ on the platform of a real datacenter. Extensive experiments validated that E S is correct and useful, and the associated ARQ strategy has improved the overall user experience significantly. We also show that E S and ARQ are easy-to-use and robust in diverse scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tail latency of the LC applications, IPC of the BE application and the entropy values under resource scheduling strategies A and B. The dotted box represents the QoS target of the LC applications.</figDesc><graphic url="image-1.png" coords="3,323.47,51.79,228.48,76.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Impact of the size of available resources on E S (Xapian (20%), Moses (20%), Img-dnn (20%), Fluidmanate).</figDesc><graphic url="image-2.png" coords="4,318.32,231.61,237.12,90.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of the concept of "resource equivalence".</figDesc><graphic url="image-3.png" coords="5,37.29,49.35,246.96,80.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An illustration of the space-time model (for brevity, only one resource slice and eight time-slices are considered).</figDesc><graphic url="image-4.png" coords="5,328.91,50.65,217.20,262.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1: function ARQ 2 : 3 :True do 4 :</head><label>234</label><figDesc>isAd just ?False, E S ? 1 while Monitor the tail latency values of the LC applications and the IPC values of BE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A snapshot of the resource allocation of PARTIES and ARQ (Xapian (30%), Moses (20%), Img-dnn (20%) and Stream). Compared with PARTIES, ARQ makes the BE application (i.e., Stream) have more available resources (from the shared region).</figDesc><graphic url="image-5.png" coords="7,336.87,53.44,196.32,55.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. A snapshot of the resource allocation of PARTIES and ARQ (Xapian (90%), Moses (20%), Img-dnn (20%) and Stream). Compared with PARTIES, ARQ makes the high-load LC application (i.e., Xapian) have more available resources, since the other LC applications can be satisfied only with the shared region resources.</figDesc><graphic url="image-6.png" coords="7,336.87,165.53,196.32,55.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Results when Xapian, Moses, Img-dnn and Fluidanimate are collocated.</figDesc><graphic url="image-8.png" coords="9,313.98,52.53,247.20,285.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Results when Xapian, Moses, Img-dnn and Stream are collocated.</figDesc><graphic url="image-9.png" coords="10,57.36,49.46,235.20,287.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. E LC , E BE and E S when Xapian, Moses, Img-dnn and Stream are collocated (the load of Moses is fixed to 20%).</figDesc><graphic url="image-10.png" coords="10,51.64,365.70,247.20,150.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. E LC , E BE and E S when Img-dnn, Moses, Sphinx and Stream are collocated (the load of the LC applications is constant).</figDesc><graphic url="image-11.png" coords="10,319.13,48.27,230.16,147.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. E LC , E BE and E S and the corresponding scheduling process of LCfirst, PARTIES, and ARQ (Xapian's load is fluctuating).</figDesc><graphic url="image-13.png" coords="11,333.47,47.54,208.08,454.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I LIST</head><label>I</label><figDesc>OF SYMBOL ABBREVIATIONS.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>T L i0</cell><cell>Application i's ideal tail latency</cell></row><row><cell>T L i1</cell><cell>Tail latency of application i when it is suffering interference</cell></row><row><cell>M i</cell><cell>Maximum tail latency that application i can tolerate</cell></row><row><cell>A i</cell><cell>Interference tolerance of application i</cell></row><row><cell>R i</cell><cell>Interference that application i suffers</cell></row><row><cell>ReT i</cell><cell>Remaining tolerance of application i</cell></row><row><cell>Q i</cell><cell>Interference that the application i cannot tolerate</cell></row><row><cell>IPC solo (i)</cell><cell>IPC when application i is running alone</cell></row><row><cell>IPC real (i)</cell><cell>IPC when application i is suffering</cell></row><row><cell>RI</cell><cell>Relative importance</cell></row><row><cell>E LC</cell><cell>LC entropy</cell></row><row><cell>E BE</cell><cell>BE entropy</cell></row><row><cell>E S</cell><cell>System entropy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DETAILS</head><label>II</label><figDesc>OF THE LC, BE AND SYSTEM ENTROPY UNDER THE UNMANAGED STRATEGY WITH DIFFERENT NUMBERS OF PROCESSING UNITS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Choose one type of the resources (i.e., core, LLC, or memory bandwidth, etc) of victimRegion.if ReT i &gt; 0.1 and application i has isolated resource that allows to be penalized then</figDesc><table><row><cell>11:</cell><cell>isAd just ? False</cell></row><row><cell>12:</cell><cell>else</cell></row><row><cell>13:</cell><cell>isAd just ? AdjustResource(ReT )</cell></row><row><cell>14:</cell><cell>end if</cell></row><row><cell>15:</cell><cell>end while</cell></row><row><cell cols="2">16: end function</cell></row><row><cell>17:</cell><cell></cell></row><row><cell cols="2">18: function ADJUSTRESOURCE</cell></row><row><cell>19:</cell><cell>victimRegion ? findVictimRegion(ReT )</cell></row><row><cell>20:</cell><cell>bene f iciaryRegion ? findBeneficiaryRegion(ReT )</cell></row><row><cell cols="2">21: // 22: ?R ? findVictimResource(victimRegion)</cell></row><row><cell>23:</cell><cell>Move one unit resource of type ?R from the victimRegion to the</cell></row><row><cell></cell><cell>bene f iciaryRegion</cell></row><row><cell>24:</cell><cell>return whether the resource has been actually adjusted</cell></row><row><cell cols="2">25: end function</cell></row><row><cell>26:</cell><cell></cell></row><row><cell cols="2">27: function FINDVICTIMREGION</cell></row><row><cell>28:</cell><cell>for each ReT i in descending order do</cell></row><row><cell>29:</cell><cell></cell></row><row><cell>30:</cell><cell>return the isolated region of application i</cell></row><row><cell>31:</cell><cell>end if</cell></row><row><cell>32:</cell><cell>end for</cell></row><row><cell>33:</cell><cell>return the shared region</cell></row><row><cell cols="2">34: end function</cell></row><row><cell>35:</cell><cell></cell></row><row><cell cols="2">36: function FINDBENEFICIARYREGION</cell></row><row><cell>37:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III EXPERIMENTAL</head><label>III</label><figDesc>PLATFORM.</figDesc><table><row><cell>Component</cell><cell>Specification</cell></row><row><cell>CPU</cell><cell>Intel Xeon E5-2630 v4 (10 cores)</cell></row><row><cell>Processor Core Frequency</cell><cell>2.2GHz</cell></row><row><cell>Operating System</cell><cell>CentOS 7 (kernel 5.6.11)</cell></row><row><cell>L1 Caches</cell><cell>32KB?10, 8-way set associative, split D/I</cell></row><row><cell>L2 Caches</cell><cell>256KB?10, 8-way set associative</cell></row><row><cell>L3 Caches</cell><cell>25MB, 20-way set associative</cell></row><row><cell>Main Memory</cell><cell>16GB?7, 2400MHz DDR4</cell></row><row><cell>NIC</cell><cell>Intel Corporation I350 Gigabit</cell></row><row><cell></cell><cell>Network Connection (1Gbps)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PARAMETER</head><label>IV</label><figDesc>OF THE LC APPLICATIONS.</figDesc><table><row><cell></cell><cell cols="5">Xapian Moses Img-dnn Masstree Sphinx Silo</cell></row><row><cell cols="2">Tail Latency Threshold (ms) 4.22</cell><cell>10.53 3.98</cell><cell>1.05</cell><cell cols="2">2682 1.27</cell></row><row><cell>Max Load (QPS)</cell><cell>3400</cell><cell>1800 5300</cell><cell>4420</cell><cell>4.8</cell><cell>220</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their insightful comments. We would also like to thank <rs type="person">Professor Zhiwei Xu</rs> and <rs type="person">Ninghui Sun</rs> for their valuable suggestions. The first author thanks <rs type="person">Professor Mingfa Zhu</rs> for his guidance. This work is supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62090023</rs>, <rs type="grantNumber">61772497</rs>) and <rs type="funder">National Key RD Program of China</rs> (No. <rs type="grantNumber">2016YFB1000201</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_utk2Qu6">
					<idno type="grant-number">62090023</idno>
				</org>
				<org type="funding" xml:id="_S7HYbKf">
					<idno type="grant-number">61772497</idno>
				</org>
				<org type="funding" xml:id="_sAQdCf5">
					<idno type="grant-number">2016YFB1000201</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Redis</title>
	</analytic>
	<monogr>
		<title level="j">Available: Redis.io</title>
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Applications of web query mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Information Retrieval</title>
		<meeting>the European Conference on Information Retrieval</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="7" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The parsec benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>the 17th International conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bitirgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 41st IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 2008 41st IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interference and locality-aware task scheduling for mapreduce applications in virtual clusters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International symposium on High-performance Parallel and Distributed Computing (HPDC)</title>
		<meeting>the 22nd International symposium on High-performance Parallel and Distributed Computing (HPDC)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Workloads in the clouds</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Calzarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Della Vedova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Massari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Tabash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tessera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Performance and Reliability Modeling and Evaluation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="525" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alita: comprehensive performance isolation through bias resource management for public clouds</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parties: Qos-aware resource partitioning for multiple interactive services</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="107" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with ycsb</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<idno type="DOI">10.1145/1807128.1807152</idno>
		<ptr target="https://doi.org/10.1145/1807128.1807152" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Symposium on Cloud Computing, ser. SoCC &apos;10</title>
		<meeting>the 1st ACM Symposium on Cloud Computing, ser. SoCC &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Paragon: Qos-aware scheduling for heterogeneous datacenters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="88" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quasar: Resource-efficient and qosaware cluster management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="127" to="144" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bolt: I know what you did last summer... in the cloud</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="599" to="613" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research [best of the web]</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kpart: A hybrid cache partitioning-sharing technique for commodity multicores</title>
		<author>
			<persName><forename type="first">N</forename><surname>El-Sayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="104" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Workload modeling for computer systems performance evaluation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Feitelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caladan: Mitigating interference at microsecond timescales</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intel? 64 and ia-32 architectures software developer&apos;s manual</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Part</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>: System programming Guide</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Who limits the resource efficiency of my datacenter: an analysis of alibaba datacenter traces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Symposium</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving real-time performance by utilizing cache allocation technology</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-04">April, 2015</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring interference between live datacenter applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rubik: Fast analytical power management for latency-critical systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Bartolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<meeting>the 2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="598" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ubik: Efficient cache sharing with strict qos for latency-critical workloads</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="729" to="742" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tailbench: a benchmark suite and evaluation methodology for latency-critical applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting>the 2016 IEEE International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</title>
		<meeting>the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cuttlesys: Data-driven resource management for interactive services on reconfigurable multicores</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gonzalez-Pumariega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<meeting>the 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="650" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards energy proportionality for large-scale latency-critical workloads</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heracles: Improving resource efficiency at scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="450" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cache craftiness for fast multicore key-value storage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1145/2168836.2168855</idno>
		<ptr target="https://doi.org/10.1145/2168836.2168855" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM European Conference on Computer Systems, ser. EuroSys &apos;12</title>
		<meeting>the 7th ACM European Conference on Computer Systems, ser. EuroSys &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stretch: Balancing qos and throughput for colocated server workloads on smt cores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez-Alberquilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bubbleup: Increasing utilization in modern warehouse scale computers via sensible co-locations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Directly characterizing cross core interference through contention synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on High Performance and Embedded Architectures and Compilers</title>
		<meeting>the 6th International Conference on High Performance and Embedded Architectures and Compilers</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Memory bandwidth and machine balance in current high performance computers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mccalpin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society technical committee on computer architecture (TCCA) newsletter</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Twig: Multiagent task management for colocated latency-critical cloud services</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sjalander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="167" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sturgeon: Preference-aware co-location for improving utilization of power constrained computers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<meeting>the 2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="718" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Copart: Coordinated partitioning of last-level cache and memory bandwidth for fairness-aware workload consolidation on commodity servers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
		<meeting>the Fourteenth EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Clite: Efficient and qos-aware co-location of multiple latency-critical jobs for warehouse scale computers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Phase-aware cache partitioning to target both turnaround time and system performance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Selfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2556" to="2568" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fact: a framework for adaptive contention-aware thread migrations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Pusukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vengerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalogeraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM International Conference on Computing Frontiers</title>
		<meeting>the 8th ACM International Conference on Computing Frontiers</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Application clustering policies to address system fairness with intel&apos;s cache allocation technology</title>
		<author>
			<persName><forename type="first">V</forename><surname>Selfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sahuquillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>G?mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 26th International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Bell system technical journal</title>
		<imprint>
			<date type="published" when="1948">1948</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Symbiotic job scheduling for a simultaneous multithreaded processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth International conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the ninth International conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unfair data centers for fun and profit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wild and Crazy Ideas (ASPLOS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The application slowdown model: Quantifying and controlling the impact of inter-application interference at shared caches and main memory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The rise of high-throughput computing</title>
		<author>
			<persName><forename type="first">N.-H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-R</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers of Information Technology and Electronic Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1245" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Speedy transactions in multicore in-memory databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<idno type="DOI">10.1145/2517349.2522713</idno>
		<ptr target="https://doi.org/10.1145/2517349.2522713" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, ser. SOSP &apos;13</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles, ser. SOSP &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimal markovian dynamic control of interference-prone server farms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Votke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delasay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doroudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</title>
		<meeting>the 2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling and analysis of performance under interference in the cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Votke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</title>
		<meeting>the 2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="232" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The chips are down for moore&apos;s law</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Waldrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature News</title>
		<imprint>
			<biblScope unit="volume">530</biblScope>
			<biblScope unit="issue">7589</biblScope>
			<biblScope unit="page">144</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sphinx-4: A flexible open source framework for speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gouvea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>W?lfel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sun Microsystems</title>
		<imprint>
			<date type="published" when="2004">12 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Effective capacity modulation as an explicit control knob for public cloud profitability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kesidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Birke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Autonomous and Adaptive Systems (TAAS)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Small is better: Avoiding latency traps in virtualized data centers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jahanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th annual Symposium on Cloud Computing</title>
		<meeting>the 4th annual Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bubble-flux: Precise online qos management for increased utilization in warehouse scale computers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Breslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="607" to="618" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd USENIX Workshop on Hot Topics in Cloud Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>HotCloud 10</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cpi2: Cpu performance isolation for shared compute clusters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jnagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM European Conference on Computer Systems</title>
		<meeting>the 8th ACM European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="379" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sinan: Ml-based and qos-aware resource management for cloud microservices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3445814.3446693</idno>
		<ptr target="https://doi.org/10.1145/3445814.3446693" />
	</analytic>
	<monogr>
		<title level="m">ser. ASPLOS 2021</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="167" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rhythm: component-distinguishable workload deployment in datacenters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth European Conference on Computer Systems</title>
		<meeting>the Fifteenth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
