<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking the Chase</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Benedikt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Konstantinidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giansalvatore</forename><surname>Mecca</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Basilicata Potenza</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boris</forename><surname>Motik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Arizona State University Phoenix</orgName>
								<address>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donatello</forename><surname>Santoro</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Basilicata Potenza</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Efthymia</forename><surname>Tsamoura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmarking the Chase</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C51D38585F42A7CC9833C8AC437524CB</idno>
					<idno type="DOI">10.1145/3034786.3034796</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The chase is a family of algorithms used in a number of data management tasks, such as data exchange, answering queries under dependencies, query reformulation with constraints, and data cleaning. It is well established as a theoretical tool for understanding these tasks, and in addition a number of prototype systems have been developed. While individual chase-based systems and particular optimizations of the chase have been experimentally evaluated in the past, we provide the first comprehensive and publicly available benchmark-test infrastructure and a set of test scenarios-for evaluating chase implementations across a wide range of assumptions about the dependencies and the data. We used our benchmark to compare chase-based systems on data exchange and query answering tasks with one another, as well as with systems that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of chase implementations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The chase <ref type="bibr" target="#b24">[25]</ref> is a long-standing technique developed by the database community for reasoning with constraints, also known as dependencies, expressed as universal implications possibly containing existential quantification in the conclusion. When applied to a set of dependencies and a set of facts, the chase extends the facts in a forward-chaining manner to satisfy the dependencies.</p><p>The chase has been intensively studied as a theoretical tool, but over the last decade practical aspects, such as developing optimizations of the chase algorithms and building chase-based systems for various tasks, have also been considered. Although algorithms for chasing source-to-target dependencies have shown promise in practice, with target dependencies scalability has been achieved only in quite restricted cases. The performance of chase implementations on large sets of complex dependencies and large datasets remains unknown. This suggests that it is time to evaluate the extent to which computing the chase is practically feasible.</p><p>The chase is closely related to and can be seen as a special case of theorem proving calculi such as tableaux and resolution, and it can also be seen as a generalization of standard query evaluation in Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.</p><p>databases. But while the theorem proving and the database communities have a long history of creating benchmarks and detailed evaluation methodologies (e.g., SMTLib <ref type="bibr" target="#b36">[37]</ref> and TPTP <ref type="bibr" target="#b39">[40]</ref> in the former, the TPC family <ref type="bibr" target="#b38">[39]</ref> in the latter), there is little corresponding infrastructure to support experimental validation of techniques such as the chase that combine reasoning and data management.</p><p>This paper aims to take a major step in changing this situation. We present a new benchmark for chase systems covering a wide range of scenarios. Since the systems in the literature support different kinds of dependencies, we have developed dependency sets with different structural properties, and datasets of varying sizes.</p><p>We also define example tasks for two main applications of the chase: (i) data exchange, which involves materializing an instance of a target schema satisfying a given set of dependencies with respect to an instance of a source schema; and (ii) computing certain answers to conjunctive queries over databases with dependencies.</p><p>We then analyze a variety of publicly available systems on our benchmark in order to answer the following questions:</p><p>• How do existing chase-related systems fare in absolute terms on these tasks? That is, to what extent can they be considered as proof that the chase-based approaches to solving these tasks are practically feasible? • What algorithmic and architectural choices are most critical for the performance of chase-related systems? • Are there other approaches or other kinds of systems that can perform these same tasks and, if so, how do they compare to tools that use the chase? In an attempt to answer these questions, we considered a number of systems that implement the chase as a component, including systems motivated from data exchange, data cleaning, query reformulation, and query answering.</p><p>We mentioned above that many communities have looked at techniques similar to the chase, and at problems similar to data exchange and query answering. To better understand the connection with the related communities, we also applied our benchmark to systems that are not specifically "branded" as chase systems, but that can nonetheless perform some of the tasks that the chase addresses. In particular, we looked at Datalog engines that support function symbols as they can solve both data exchange and query answering problems, as well as a leading theorem prover that can solve various query answering problems.</p><p>Organization. In the rest of this paper, we first present some background about the chase (Sections 2 and 3). Next, we describe our test systems (Section 4), and discuss our testing infrastructure and test scenarios (Section 5). Then, we present the system comparison results (Section 6), followed by a discussion of the insights gained (Section 7) and the future challenges that emerged from our study (Section 8). Finally, we close with a discussion of the re-lated work and conclusions (Sections 9 and 10). We emphasize that full details regarding the systems under test, our test infrastructure, and our scenarios are available on the benchmark Web page (http://dbunibas.github.io/chasebench).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>Database basics. Let Const, Nulls, and Vars be mutually disjoint, infinite sets of constant values, labeled nulls, and variables, respectively. Intuitively, constant values are unique; labeled nulls represent unknown values; and variables are used in dependencies and queries. A value is a constant value or a labeled null, and a term is a value or a variable. We often abbreviate an n-tuple of terms t 1 , . . . ,t n as t, and we often treat it as a set and write t i ∈ t.</p><p>A schema is a set of relation names (or just relations), each associated with a nonnegative integer called arity. An instance I of a schema assigns to each n-ary relation R in the schema a (possibly infinite) set I(R) of n-tuples of values. The active domain of I is the set of all values occurring in a tuple of some I(R). A relational atom has the form R( t) where R is an n-ary relation and t is an ntuple of terms. An equality atom has the form t 1 = t 2 where t 1 and t 2 are terms. A fact is an atom that does not contain variables. An instance can equivalently be seen as a set of relational facts, so we use notation R( t) ∈ I and t ∈ I(R) interchangeably. An atom (resp. an instance) is null-free if it does not contain null values.</p><p>Term mappings. A term mapping σ is a partial mapping of terms to terms; we write σ = {t 1 → s 1 , . . . ,t n → s n } to denote that σ (t i ) = s i for 1 ≤ i ≤ n. For α a term, an atom, a conjunction of atoms, or a set of atoms, σ (α) is obtained by replacing each occurrence of a term t in α that also occurs in the domain of σ with σ (t) (i.e., terms outside the domain of σ remain unchanged). The composition of σ with a term mapping µ is the term mapping σ • µ whose domain is the union of the domains of σ and µ, and it is defined by (σ • µ)(t) = σ (µ(t)). A substitution σ is a term mapping whose domain contains only variables and whose range contains only values; moreover, σ is null-free if its range contains only constants; finally, σ is a homomorphism of a conjunction of atoms ρ = i A i into an instance I if the domain of σ is the set of all variables occurring in ρ and σ (ρ) ⊆ I.</p><p>Dependencies and solutions. Semantic relationships between relations can be described using dependencies, which come in two forms. A Tuple Generating Dependency (TGD) is a logical sentence of the form <ref type="bibr" target="#b0">(1)</ref>, where λ ( x) and ρ( x, y) are conjunctions of relational, null-free atoms whose free variables are contained in x, and x ∪ y correspondingly.</p><formula xml:id="formula_0">∀ x λ ( x) → ∃ y ρ( x, y)<label>(1)</label></formula><p>An Equality Generating Dependency (EGD) is a logical sentence of the form (2), where λ ( x) is a conjunction of relational, null-free atoms over variables x, and {x i , x j } ⊆ x.</p><formula xml:id="formula_1">∀ x λ ( x) → x i = x j<label>(2)</label></formula><p>The left-hand side of a TGD or an EGD (i.e., the conjunction λ ( x)) is the body of the dependency, and the right-hand side is the head.</p><p>A dependency is linear if it has exactly one atom in the body. By a slight abuse of notation, we often treat heads and bodies as sets of atoms, and we commonly omit the leading universal quantifiers.</p><p>Let I be an instance and let τ be a TGD of the form (1) or an EGD of the form <ref type="bibr" target="#b1">(2)</ref>. The notion of dependency τ holding in I (or I satisfying τ, written I |= τ) is given by first-order logic, and it can be restated using homomorphisms as follows. A trigger for τ in I is a homomorphism h of λ ( x) into I. Moreover, an active trigger for τ in I is a trigger h for τ in I such that, if τ is a TGD, then no extension of h to a homomorphism of ρ( x, y) into I exists, and if τ is an EGD, then h(x i ) = h(x j ). Finally, dependency τ is satisfied in I if there does not exist an active trigger for τ in I.</p><p>Let Σ be a set of dependencies and let I be a finite, null-free instance. Instance J is a solution for Σ and I if I ⊆ J and J |= τ for each τ ∈ Σ. A solution J for Σ and I is universal if, for each solution J for Σ and I, a term mapping µ from the active domain of J to the active domain of J exists such that µ(J) ⊆ J and µ(c) = c holds for each constant value c ∈ Const. Solutions for Σ and I are not unique, but universal solutions are unique up to homomorphism.</p><p>Queries. A conjunctive query (CQ) is a formula of the form ∃ y i A i , where A i are relational, null-free atoms. A substitution σ is an answer to Q on instance I if the domain of σ is precisely the free variables of Q, and if σ can be extended to a homomorphism of i A i in I. By choosing a canonical ordering for the free variables x of Q, we often identify σ with an n-tuple σ (x 1 ), . . . , σ (x n ). The output of Q on I is the set Q(I) of all answers to Q on I.</p><p>Answering queries under dependencies. Let Σ be a set of dependencies, let I be a finite, null-free instance, and let Q be a CQ. A substitution σ is a certain answer to Q on Σ and I if σ is an answer to Q on each solution J for Σ and I. The task of finding all certain answers to Q on Σ and I is called query answering under dependencies, and we often abbreviate it to just query answering. The following fundamental result connects universal solutions and query answering: for each substitution σ and each universal solution J for Σ and I, substitution σ is a certain answer to Q on Σ and I if and only if σ is a null-free answer to Q on J <ref type="bibr" target="#b14">[15]</ref>.</p><p>The chase. The chase modifies an instance by a sequence of chase steps until all dependencies are satisfied. Let I be an instance, let τ be a TGD or an EGD of the form (1) or <ref type="bibr" target="#b1">(2)</ref>, and let h be a trigger for τ in I. If τ is a TGD, applying the chase step for τ and h to I extends I with facts of the conjunction h (ρ( x, y)), where h is a substitution such that h (x i ) = h(x i ) for each variable x i ∈ x, and h (y j ), for each y j ∈ y, is a fresh labeled null that does not occur in I. Moreover, if τ is an EGD, applying the chase step for τ and h to I fails if h(x i ) = h(x j ) and {h(x i ), h(x j )} ⊆ Const, and otherwise it computes µ(I) where µ = {h(x j ) → h(x i )} if h(x i ) ∈ Const, and</p><formula xml:id="formula_2">µ = {h(x i ) → h(x j )} if h(x i ) ∈ Const.</formula><p>For Σ a set of TGDs and EGDs and I a finite, null-free instance, a chase sequence for Σ and I is a (possibly infinite) sequence I 0 , I 1 , . . . such that I = I 0 and, for each i &gt; 0, instance I i (if it exists) is obtained from I i-1 by applying a successful chase step to a dependency τ ∈ Σ and an active trigger h for τ in I i-1 . The sequence must be fair: for each τ ∈ Σ, each i ≥ 0, and each active trigger h for τ in I i , some j &gt; i must exist such that h is not an active trigger for τ in I j (i.e., no chase step should be postponed indefinitely). The result of a chase sequence is the (possibly infinite) instance I ∞ = i≥0 j≥i I j . Since EGD chase steps can fail, a chase sequence for a given Σ and I may not exist. Moreover, chase steps can be applied in an arbitrary order so a chase sequence for Σ and I is not unique. Finally, EGD steps are not monotonic (i.e., I i-1 ⊆ I i holds when I i is obtained by applying an EGD step to I i-1 ), and so I ∞ is not uniquely determined by Σ and I. Still, each result I ∞ of a chase sequence for Σ and I is a universal solution for Σ and I <ref type="bibr" target="#b14">[15]</ref>.</p><p>A finite chase sequence is terminating. A set of dependencies Σ has terminating chase if, for each finite, null-free instance I, each chase sequence for Σ and I is terminating. For such Σ, the chase provides an effective approach to computing certain answers to a CQ Q on Σ and I: we compute (any) chase I ∞ of Σ and I, we compute the output Q(I ∞ ), and finally remove all substitutions that are not null-free. Checking if a set of dependencies Σ has terminating chase is undecidable <ref type="bibr" target="#b12">[13]</ref>. Weak acyclicity <ref type="bibr" target="#b14">[15]</ref> was the first suffi- for each trigger h for τ in I such that h(λ ( x)) ∩ ∆I = / 0 do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>if h is an active trigger for τ in µ(N ∪ I) then 7:</p><formula xml:id="formula_3">if τ = ∀ x λ ( x) → ∃ y ρ( x, y</formula><p>) is a TGD then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>h := h ∪ { y → v} where v ⊆ Nulls is fresh 9:</p><formula xml:id="formula_4">N := N ∪ h (ρ( x, y))</formula><p>10:</p><formula xml:id="formula_5">else if τ = ∀ x λ ( x) → x i = x j is an EGD then 11: if {h(x i ) = h(x j )} ⊆ Const then fail<label>12</label></formula><p>:</p><formula xml:id="formula_6">ω := {max(h(x i ), h(x j )) → min(h(x i ), h(x j ))}</formula><p>13:</p><formula xml:id="formula_7">µ := µ • (ω • µ) 14: ∆I := µ(N ∪ I) \ I 15: I := µ(I) ∪ ∆I</formula><p>cient polynomial-time condition for checking if Σ has terminating chase. Stronger sufficient (not necessarily polynomial-time) conditions have been proposed subsequently <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>. Data exchange. In relational-to-relational data exchange <ref type="bibr" target="#b14">[15]</ref>, a transformation of an arbitrary instance of a source schema into an instance of a target schema is described using</p><p>• a set Σ st of s-t (source-to-target) TGDs where all body atoms use relations of the source schema and all head atoms use relations of the target schema, and • a set Σ t of target dependencies (i.e., TGDs or EGDs) whose atoms use relations of the target schema. Given a finite, null-free instance I of the source schema, the objective of data exchange is to compute a universal solution to the set of dependencies Σ = Σ st ∪ Σ t and I. If Σ has terminating chase, then a universal solution can be computed using the chase, and it can be used to answer queries <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPLEMENTING THE CHASE</head><p>In this section we discuss the main challenges that chase implementations must address. Our discussion is largely independent of the specific technology, but we discuss briefly how these issues impact RDBMS-based implementations.</p><p>General structure. Computing the chase is naturally expressed as a fixpoint computation. Algorithm 1 achieves this by following the definition of a chase sequence from Section 2: in each iteration (lines 2-15), the algorithm examines each dependency τ (line 4) and trigger h (line 5), checks whether h is active (line 6), and, if so, applies the chase step (lines 8-9 for TGDs and lines 11-13 for EGDs). Although chase implementations may and do depart from the specifics of Algorithm 1, the algorithm is useful because it allows us to discuss the following issues, which each implementation must address in one form or another.</p><p>• To avoid an important source of inefficiency, we must ensure that each relevant trigger for τ and I is considered in line 5 at most once. • Applying chase steps should not interfere with the enumeration of triggers. • The EGD chase steps are complex and require care.</p><p>• Checking whether a trigger is active can be expensive. Identifying triggers. Let τ be a TGD or an EGD of the form (1) or (2), respectively, and let I be a finite instance. Clearly, substitution h is a trigger for τ and I if and only if h is an answer to λ ( x) on I. Hence, the triggers for τ can be determined in line 5 by evaluating λ ( x) as a CQ over I. The latter can be solved using any known join algorithm, and an RDBMS-based implementation can translate λ ( x) into SQL in a straightforward manner. However, evaluating λ ( x) in each iteration "from scratch" would be very inefficient since it would repeatedly consider triggers from all previous iterations. Algorithm 1 thus uses seminaïve evaluation <ref type="bibr" target="#b0">[1]</ref>. It maintains an auxiliary set ∆I of "newly derived facts" from the most recent iteration, and it requires at least one atom of h(λ ) in line 5 to be contained in ∆I. As a consequence, each combination of τ and h is considered at most once during algorithm's execution.</p><p>Applying chase steps. An important consideration is how query λ (x) is evaluated over I in line 5: we must ensure that we do not miss any applicable triggers, and that we retrieve each such trigger once. One possibility is to compute and store the result in a temporary relation, but this can impose a significant overhead. It is therefore often preferable to evaluate λ (x) over I in a "streaming" mode, where a trigger h is returned as soon as it is computed; but then, set I should not change in lines 4-13 or the modifications to I could affect the enumeration of triggers in line 5. To ensure this, the chase steps in Algorithm 1 do not modify I directly. Instead, all changes are accumulated in an auxiliary set N and a term mapping µ; after each iteration, ∆I contains the "newly derived subset" of N (line 14), which is propagated to I (line 15). The algorithm terminates when ∆I becomes empty in line 2 since another iteration then cannot derive new facts.</p><p>Handling EGDs. Applying a chase step to an EGD τ of the form (2) and trigger h poses several problems. A minor issue is that, when h(x i ) and h(x j ) are both labeled nulls, we can replace either value with the other. To make this choice deterministic, one can totally order all values so that all constant values are smaller than all labeled nulls, and then always replace the larger value with the smaller one (line 12). Constant values are thus never replaced with labeled nulls, and this also ensures uniqueness of the chase for a specific chase variant that we discuss shortly.</p><p>A more complex issue is to ensure that I does not change in lines 5-13. To achieve this, Algorithm 1 uses a term mapping µ that accumulates all the required changes to I (line 13): at the end of each iteration, µ(v) is defined for each labeled null v that is to be replaced with µ(v). The expression in line 13 ensures that µ is correctly updated when several EGD steps are applied in single iteration. For example, consider</p><formula xml:id="formula_8">µ = {v 1 → v 2 , v 3 → v 4 } and ω = {v 2 → v 3 }; then, ω • µ = {v 1 → v 3 , v 2 → v 3 , v 3 → v 4 } "nor- malizes" µ with ω, and µ • (ω • µ) = {v 1 → v 4 , v 2 → v 4 , v 3 → v 4 }</formula><p>reflects the cumulative effects of all EGDs.</p><p>At the end of each iteration (lines 14-15), the facts in both I and N may require updating, and all facts newly derived in the iteration most be added to ∆I to correctly further trigger dependencies.</p><p>Checking active triggers. Assume that h is a trigger for a dependency τ in an instance I, and consider checking whether h is an active trigger for τ in µ(N ∪ I) in line 6. It should be clear that h is a trigger for τ in µ(N ∪ I) if and only if µ(h(x)) = h(x) for each variable x from the domain of h. In other words, we simply check whether "h up-to-date with µ." Thus, to complete the active trigger check, if τ is of the form (2), we check whether h(x i ) = h(x j ) holds; and if τ is of the form (1), we check γ(I) = / 0 where γ = ∃ y ρ(h( x), y) is a boolean CQ. The latter can be implemented using standard query evaluation techniques, and RDBMSbased systems can simply extend the CQ from line 5 with a NOT EXISTS (SELECT * WHERE ...) condition. In the rest of this section and in Section 7 we discuss several theoretical and practical drawbacks of checking for active triggers and present alternatives.</p><p>Chase variants. The chase variant given in Algorithm 1 is called the restricted chase to stipulate that triggers are restricted only to the active ones (cf. line 6). A drawback of the restricted chase is that the chase solution is not unique (even up to isomorphism of labeled nulls), as Example 1 shows. EXAMPLE 1. Let Σ and I be as in (3) and (4).</p><formula xml:id="formula_9">R(x 1 , x 2 ) → ∃y R(x 1 , y) ∧ A(y) ∧ A(x 2 ) (3) I = { R(a, b), R(b, b) } (4)</formula><p>Dependencies in Σ are weakly acyclic <ref type="bibr" target="#b14">[15]</ref> so the restricted chase terminates on all finite instances, but the chase solution depends on the ordering of chase steps. Triggers h 1 = {x 1 → a, x 2 → b} and h 2 = {x 1 → b, x 2 → b} for (3) are both active in I. Applying the TGD step to h 1 makes h 2 inactive by deriving R(a, v 1 ), A(v 1 ), and A(b). In contrast, applying the TGD step to h 2 makes h 1 inactive by deriving R(b, v 1 ), A(v 1 ), and A(b).</p><p>The chase can be optimized by normalizing TGDs prior to applying the chase: for each TGD τ ∈ Σ of the form (1) such that ρ( x, y) can be rewritten as ρ 1 ( x 1 , y 1 ) ∧ ρ 2 ( x 2 , y 2 ) so that y 1 ∩ y 2 = / 0, we replace τ in Σ with ∀ x λ ( x) → ∃ y i ρ i ( x i , y i ) for i ∈ {1, 2}. Example 2 shows how normalization can lead to smaller instances. EXAMPLE 2. Normalizing TGD (3) produces (5) and (6). Now by applying (6) to I from Example 1 we derive A(b), which makes triggers h 1 and h 2 for (5) both inactive.</p><formula xml:id="formula_10">R(x 1 , x 2 ) → ∃y R(x 1 , y) ∧ A(y) (5) R(x 1 , x 2 ) → A(x 2 )<label>(6)</label></formula><p>Checking whether trigger h is active (line 6) can be difficult in practice, particularly for a test against µ(N ∪ I); we discuss these issues in detail in Section 7. Furthermore, the dependence on the ordering of chase steps can make the chase more difficult to analyze from a theoretical point of view. This motivates several variants in which the check in line 6 is either eliminated or weakened.</p><p>The unrestricted (or oblivious) chase simply eliminates line 6. Such a simple solution removes the overhead of checking active triggers and the dependence on the ordering of chase steps. But, as Example 3 shows, the unrestricted chase does not necessarily terminate even for weakly acyclic TGDs. EXAMPLE 3. For Σ and I defined as in Example 1, the unrestricted chase derives the following infinite set of facts. R(a, v 1 ), A(v 1 ), R(a, v 2 ), A(v 2 ), . . . A(b), R(b, w 1 ), A(w 1 ), R(b, w 2 ), A(w 2 ), . . . The unrestricted Skolem chase <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28]</ref> also eliminates line 6, but it also skolemizes TGDs: in each TGD τ of the form (1), each existentially quantified variable y ∈ y is replaced with a function term f ( z) where f is a fresh function symbol and z contains all variables occurring in both the head and the body of τ. The chase then proceeds as in Algorithm 1, but without line 6 and by using the preprocessed τ in line 7. The result of the Skolem chase is unique (if the EGD steps are determinized as in line 8). Although cases exist where the restricted chase terminates but the Skolem chase does not, the known acyclicity conditions <ref type="bibr" target="#b19">[20]</ref> ensure termination of both. Example 4 illustrates the Skolem chase. EXAMPLE 4. Normalizing and then skolemizing TGD (3) produces <ref type="bibr" target="#b6">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_11">R(x 1 , x 2 ) → R(x 1 , f (x 1 )) ∧ A( f (x 1 )) (7) R(x 1 , x 2 ) → A(x 2 )<label>(8)</label></formula><p>Applying TGDs after which the chase terminates: functional term f (x 1 ) in (7) captures the fact that the fresh null depends only on x 1 , and so applying <ref type="bibr" target="#b6">(7)</ref> to R(a, f (a)) and R(b, f (b)) does not introduce more nulls as in Example 3. Normalization is very important since it eliminates variables within Skolem terms; for example, skolemizing (3) directly produces <ref type="bibr" target="#b8">(9)</ref>, and applying (9) to I does not terminate.</p><formula xml:id="formula_12">R(x 1 , x 2 ) → R(x 1 , f (x 1 , x 2 )) ∧ A( f (x 1 , x 2 )) ∧ A(x 2 )<label>(9)</label></formula><p>Since functional terms provide canonical "names" for labeled nulls, a global counter of labeled nulls is not required, which may simplify implementation. For example, deriving the first atom of ( <ref type="formula">7</ref>) can be implemented using the SQL query <ref type="bibr" target="#b9">(10)</ref>, which does not interact with other TGDs.</p><formula xml:id="formula_13">INSERT INTO R(a,b) SELECT DISTINCT R.a, append(' _ Sk _ f(',R.a,')') FROM R (10)</formula><p>The parallel chase <ref type="bibr" target="#b12">[13]</ref> weakens line 6 so that it checks whether h is active in I, rather than in µ(N ∪ I); since I is fixed in an iteration, this can make checking active triggers much easier to implement. Known acyclicity conditions <ref type="bibr" target="#b19">[20]</ref> ensure termination of the parallel chase, and the solution is deterministic, although it may be larger than the one produced by the restricted chase. Example 5 illustrates the parallel chase. EXAMPLE 5. Let Σ, I, h 1 , and h 2 be as in Example 1. Both h 1 and h 2 are active for I, so the parallel chase applies the TGD step to both triggers independently and derives R(a, v 1 ), A(v 1 ), A(b), R(a, w 1 ), and A(w 1 ). No active triggers exist after this step so the parallel chase terminates.</p><p>The single-TGD-parallel (or 1-parallel) chase checks active triggers w.r.t. all facts derived thus far apart from the ones derived by the TGD τ currently considered in line 4. As with the parallel chase, implementing this variant can be much easier than for the restricted chase (see <ref type="bibr">Section 7)</ref>.</p><p>While the check in line 6 is not needed with skolemized TGDs, it can still be used, in which case we obtain the restricted (or parallel or 1-parallel) Skolem chase: each of these two chase variants never produces more facts than the original variant, and in certain cases it can produce fewer facts.</p><p>The frugal chase <ref type="bibr" target="#b21">[22]</ref> further considers triggers that are only partially active. Let τ be a TGD τ of the form (1) such that ρ( x, y) can be rewritten as ρ 1 ( x 1 , y 1 ) ∧ ρ 2 ( x 2 , y 2 ) where x 1 and x 2 , and y 1 and y 2 are not necessarily disjoint, and let h be a trigger for τ in an instance I. Then, h is partially active for τ in I if h can be extended to a homomorphism h such that (i) ρ 1 (h ( x 1 ), h ( y 1 )) ⊆ I, (ii) h ( y 1 ) contains only labeled nulls, and (iii) for each fact P( w) ∈ I such that w ∩ h ( y 1 ) = / 0, we have P( w) ∈ ρ 1 (h ( x 1 ), h ( y 1 )) (i.e., each atom in I that joins "existentially" with the image of ρ 1 is contained in the image of ρ 1 ). Since ρ 1 is already satisfied, the frugal chase satisfies the TGD τ by extending I with ρ 2 (h ( x 2 ), h ( y 2 )), where h is a homomorphism that extends (one such) h by mapping the variables in y 2 \ y 1 to fresh labeled nulls. EXAMPLE 6. Let Σ contain the TGD (5) from Example 2 and TGD <ref type="bibr" target="#b10">(11)</ref>, and let I be as in <ref type="bibr" target="#b11">(12)</ref>.</p><formula xml:id="formula_14">B(x) → ∃y R(x, y)<label>(11)</label></formula><formula xml:id="formula_15">I = { B(a) }<label>(12)</label></formula><p>Assume now that the frugal chase first applies <ref type="bibr" target="#b10">(11)</ref> and thus extends I with fact R(a, v 1 ). Then, h = {x 1 → a, x 2 → v 1 } is a partially active trigger for TGD (5): we can decompose the head of (5) such that ρ 1 = R(x 1 , y) and ρ 2 = A(y), and we can extend h to</p><formula xml:id="formula_16">h = {x 1 → a, x 2 → v 1 , y → v 1 } such that h (ρ 1</formula><p>) is contained in the current instance, and h (y) = v 1 does not occur in the instance outside of h (ρ 1 ). Hence, the frugal chase satisfies (5) by adding A(v 1 ) to the instance, instead of introducing a fresh labelled null.</p><p>The core chase <ref type="bibr" target="#b12">[13]</ref> omits the active trigger check completely; however, after each iteration it replaces the current instance with its core <ref type="bibr" target="#b15">[16]</ref>-the smallest subset of I that is homomorphically equivalent to I. The core chase produces a smallest finite solution whenever one exists, but efficient computation of the core on instances of nontrivial sizes is an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE SYSTEMS TESTED</head><p>As we explained in Section 1, our objectives are to determine whether existing chase implementations can support data exchange and query answering on nontrivial inputs, and to identify the implementation decisions most relevant to performance. To answer these questions, we used nine publicly available systems shown in Table <ref type="table" target="#tab_0">1</ref>. We group the systems based on their primary motivation.</p><p>Systems motivated by data exchange. DEMO <ref type="bibr" target="#b33">[34]</ref> was one of the first data exchange engines. It implements the restricted chase for s-t TGDs and target TGDs and EGDs; moreover, upon termination, it computes the core of the solution using the algorithm by Gottlob and Nash <ref type="bibr" target="#b18">[19]</ref>. The system runs on top of PostgreSQL or HSQLDB, and we used the former in our experiments.</p><p>CHASEFUN <ref type="bibr" target="#b11">[12]</ref> is a more recent data exchange system. It supports only s-t TGDs and functional dependencies, and it implements a variant of the unrestricted Skolem chase that orders TGD and EGD chase steps to reduce the size of the intermediate chase results. We used an implementation that runs on top of PostgreSQL.</p><p>Systems motivated by data cleaning. LLUNATIC <ref type="bibr" target="#b17">[18]</ref> was initially developed for data cleaning, but has since been redesigned as an open-source data exchange system. It handles s-t TGDs and target TGDs and EGDs, and it can also compute certain query answers over the target instance. It implements the 1-parallel Skolem chase (the default), the unrestricted and restricted Skolem chase, and the restricted chase with fresh nulls (i.e., without Skolem terms). The system runs on top of PostgreSQL.</p><p>Systems motivated by query reformulation. Three of our test systems use the chase for query reformulation.</p><p>PEGASUS <ref type="bibr" target="#b28">[29]</ref> is a system for finding minimal queries equivalent to a given query with respect to a set of TGDs and EGDs. It uses the Chase &amp; Backchase method of Deutsch, Popa, and Tannen <ref type="bibr" target="#b13">[14]</ref>, which internally uses the restricted chase implemented in RAM.</p><p>PDQ <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> takes a query, a set of integrity constraints, and a set of interface descriptions (e.g., views or access methods), and it produces an equivalent query that refers only to the interfaces and whose cost is minimal according to a preselected cost function. By extending the Chase &amp; Backchase method, PDQ reduces query reformulation to checking query containment under TGDs and/or EGDs, and the latter problem is solved using an implementation of the restricted chase on top of PostgreSQL.</p><p>Systems motivated by query answering. GRAAL <ref type="bibr" target="#b6">[7]</ref> is an open-source toolkit developed for computing certain answers to queries under dependencies. Although the system was not originally designed for chase computation, it uses a "saturation algorithm" that can be seen as variant of the standard chase. GRAAL can be used both in an RAM and on secondary storage, such an RDBMS, a triple store, or a graph database. We used the RAMbased version as we found it to be faster.</p><p>Chase-related systems. A prominent goal of our work was to investigate how chase implementations fare against systems from other communities that either (i) implement algorithms related to the chase, or (ii) can answer queries over dependencies using very different approaches. Many systems satisfy these requirements, so we decided to restrict our attention to several prominent representatives. In particular, we considered Datalog engines in the former, and a resolution-based theorem prover in the latter category.</p><p>DLV <ref type="bibr" target="#b23">[24]</ref> is a mature disjunctive Datalog system supporting a range of features such nonmonotonic negation, aggregates, and user-defined functions. The system comes in several flavors: a RAM-based version that supports function symbols in the rules, another RAM-based version with native support for TGDs <ref type="bibr" target="#b22">[23]</ref>, and an RDBMS-based version that supports neither function symbols nor TGDs. The latter is not applicable to our setting, and we used the version with function symbols since it proved to be more stable. We implemented a preprocessing skolemization step, allowing DLV to support the unrestricted Skolem chase for TGDs; however, the system does not support EGDs <ref type="bibr" target="#b32">[33]</ref>. The system can be used without a query, in which case it computes and outputs the chase solution. If a query is provided, the system evaluates the query over the chase and outputs the result. To obtain certain answers, we externally postprocess the query output to remove functional terms.</p><p>RDFOX <ref type="bibr" target="#b29">[30]</ref> is a high-performance RAM-based Datalog engine. It was originally designed for Datalog with EGDs over the RDF data model and without the unique name assumption. To support the chase, RDFOX was extended as follows. First, a builtin function was added that produces a labeled null unique for the function's arguments, which emulates Skolem terms. Second, a builtin function was added that checks whether a CQ is not satisfied in the data, thus enabling both the restricted and the unrestricted chase variants. Third, a mode was implemented that handles EGDs under unique name assumption. Fourth, to support relations of arbitrary arity, a preprocessor was implemented that shreds n-tuples into RDF triples and rewrites all dependencies accordingly. E <ref type="bibr" target="#b35">[36]</ref> is a state of the art first-order theorem prover that has won numerous competitions. It takes as input a set of axioms F and a conjecture H, and it decides the unsatisfiability of F ∧ ¬H. E implements the paramodulation with selection <ref type="bibr" target="#b30">[31]</ref> calculus, of which the unrestricted Skolem chase is an instance: each inference of the Skolem chase is an inference of paramodulation (but not vice versa). Paramodulation requires F to be represented as a set of clauses-that is, first-order implications without existential quantifiers but possibly containing function symbols and the equality predicate. Thus, F can capture EGDs and the result of preprocessing TGDs as described in Section 3. Moreover, E can also be used in a mode where F contains arbitrary first-order formulas, thus capturing TGDs directly without any preprocessing; however, this approach proved less efficient, so we did not use it in our experiments. Finally, conjecture H can contain free variables, in which case E outputs each substitution σ that makes F ∧ ¬σ (H) unsatisfiable; thus, E is interesting as it can answer queries without computing the chase in full. Ordered paramodulation targets firstorder logic, which is undecidable; hence, E is not guaranteed to terminate on all inputs, not even if F encodes dependencies on which the unrestricted Skolem chase terminates. The system's behavior can be configured using many parameters, and we used the values suggested by the system's author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">THE BENCHMARK</head><p>Our CHASEBENCH benchmark consists of two distinct parts. The first part is described in Section 5.1 and it comprises several tools that allow the generation and processing of test data in a common format. The second part is described in Section 5.2 and it comprises a number of test scenarios, each consisting of a (i) schema  <ref type="bibr" target="#b3">[4]</ref>, as well as instances of varying sizes produced by the instance-generation tool TOXGENE <ref type="bibr" target="#b7">[8]</ref>. We divide our scenarios into the following two groups.</p><p>Correctness scenarios were designed to verify that the systems correctly produce universal solutions. Checking homomorphisms between solutions is computationally challenging so these scenarios generate very small solutions. Moreover, these scenarios do not contain queries, as correctness of query answering can be verified by simply comparing the certain answers on larger scenarios.</p><p>Data exchange and query answering scenarios aim to test the performance of computing the target instance and of answering queries over the target instance in data exchange. These scenarios vary the data size and the complexity of the dependencies to simulate different workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Test Infrastructure</head><p>1. The common format. In their "native" versions, the systems from Section 4 take a wide range of input formats. For example, RDBMS-based systems expect the source instance to be preloaded into a relational database, whereas RAM-based systems typically read their input from files. Moreover, the structure of the inputs varies considerably; for example, RDFOX expects data to be represented as triples, and E expects data to be encoded as first-order sentences in the TPTP format. The translation between various input formats is often straightforward, but in certain cases (e.g., if schema shredding or the transformation of TGDs is required) it involves choices that can significantly affect a system's performance.</p><p>To allow for an automated and fair comparison of all systems regardless of their implementation details, we standardized the input structure for all systems. Theorem-proving formats such as TPTP can express first-order sentences and can thus represent dependencies and queries, but such formats are difficult to read for humans and so we found them inappropriate for CHASEBENCH. We thus developed our own "common format" for describing all parts of a scenario (i.e., the schema description, the source instance, the dependencies, and the queries). We also wrote a parser for the common format, which was used to develop wrappers for the systems. The wrapper was provided by the system designers whenever possible, but for E, DLV, and PEGASUS we developed the wrappers ourselves. Our tests required each system to read the scenarios in the common format, so our test results cover all times necessary to parse and transform the inputs.</p><p>2. The instance repair tool. Generating scenarios with EGDs and large source instances is complex since, due to the size of the source instance, it is difficult to ensure that no EGD chase step fails. For example, TOXGENE does not necessarily generate instances that are consistent with a collection of TGDs and EGDs, and in fact the chase failed on all instances initially produced by TOXGENE so this problem is not merely hypothetical. Thus, we developed a simple instance repair tool. Given a set of dependencies Σ and an instance I on which the chase of Σ fails, the tool proceeds as follows. First, it computes the chase of Σ and I without the unique name assumption: when equating two distinct constant values, one value is selected as representative and the other one as conflicting, the latter is replaced with the former, and the chase continues. Second, the tool removes from I all facts containing a conflicting value encountered in the previous step. The chase of Σ and the subset of I produced in this way is guaranteed to succeed. This strategy proved very effective in practice: on average it removed slightly more than 1% of the facts from I, and so the size and the distribution of the source instance remain largely unaffected.</p><p>3. The target TGD generator. In addition to generating large source instances, generating scenarios with a significant number of s-t TGDs, target TGDs, and target EGDs was critical for adequately evaluating the performance of the tested systems. One of our goals was to push the systems to their limit by testing them on deep chase scenarios that generate very large instances using long chase sequences. To this end, we developed a custom target TGD generator that can generate weakly acyclic TGDs while controlling their depth and complexity. In our experiments we used the generator to develop scenarios from scratch, but the generator can also be used to increase the difficulty of existing scenarios.</p><p>Our generator is based on our earlier approach <ref type="bibr" target="#b21">[22]</ref> for generating linear dependencies, which we extended to support an arbitrary number of body atoms. The generator creates a predicate name space of a certain required size, and it uniformly chooses a (parameterized) number of predicates to populate a conjunction of atoms (which can become a query or a dependency). A parameter is used to control the maximum number of repeated relations in this formula, and another parameter is used to determine the arity of the atoms. The generator can create "chain" conjunctions (the last variable of an atom is joined with the first one of the next atom), "star" conjunctions (the first variable of all atoms also occurs in a designated "center" atom that has a different arity than the other atoms), and "random" ones in which variables are chosen out of a variable name space and are distributed in a conjunction (the larger the variable name space, the fewer joins are introduced). Each dependency is generated by first creating a conjunction and then selecting the subset of atoms that make up the head. The size of the dependencies' bodies can be fixed; for example, this parameter was 1 in our DEEP scenarios, thus producing linear TGDs. The generator maintains a weakly-acyclic dependency graph and tests each generated dependency against the ones there were created previous; if the newly created dependency violates weak acyclicity, it is discarded and a new dependency is produced.</p><p>4. The query generator. To evaluate the performance of computing certain answers, nontrivial queries over the target schema are required. Existing benchmarks such as LUBM <ref type="bibr" target="#b20">[21]</ref> come with several manually curated queries, which we incorporated into our scenarios. To obtain queries for scenarios where the target schema is automatically generated, we developed a new query generator. The tool is given a desired number of joins in the output query N J , the desired number of selections conditions in the output query N S , the percentage of attributes that will be projected P, and a set of tables of the target schema. It first identifies a set of joinable attribute pairs from the tables of the source and target schema by analyzing the dependencies. Two attributes are considered joinable if they join either in the head or the body of the rule; moreover, if a variable is passed from the body to the head of the rule, then the attributes that map to these variables are considered joinable as well; and finally, the joinable attributes are transitively closed. Next, the algorithm iteratively creates N J join conditions by choosing at random a table T and attribute a from the schema and then choosing a random table and attribute that is joinable with T.a. The algorithm then proceeds analogously to create N S selections by first choosing an attribute that does not appear in a join condition and then a introducing randomly selected value from the attribute's domain. Finally, the algorithm randomly projects P attributes. By focusing on joinable attributes, the algorithm is more likely to produce a query with a nonempty output, but does not guarantee it; therefore, we discarded and regenerated each query that produced an empty output on the target instance as long as needed.</p><p>5. The homomorphism checker. Checking correctness of computing certain query answers is easy: certain answers are unique for the query so the systems' outputs can be compared syntactically. Checking correctness of the chase is more involved since the result of the restricted chase is not unique and, even with the unrestricted Skolem chase, it is unique only up to the renaming of Skolem functions. Hence, to verify the correctness of the systems, we developed a tool that can check the existence of homomorphisms, mutual homomorphisms, and isomorphisms between instances. The tool enumerates all candidate homomorphisms using brute force, so it can be used only on relatively small instances (i.e., few thousands facts). Consequently, we designed our correctness scenarios so that they produce small solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Test Scenarios</head><p>Our benchmark consists of a total of 23 scenarios, each comprising a source and target schema, a source instance, a set of dependencies, and possibly a set of queries; all dependencies in all scenarios are weakly acyclic. We classify the scenarios into five families, as shown in Table <ref type="table" target="#tab_1">2</ref>. The first family contains six small scenarios for testing correctness of data exchange, whereas all other families are aimed at testing the performance of computing the target instance and the certain answers. The IBENCH and the LUBM families were derived from the well established benchmarks in the database and the Semantic Web communities, respectively. Finally, we developed the MANUALLY CURATED and the DEEP families ourselves to test specific aspects of the chase. We identify each scenario using a unique identifier; for DOCTORS and LUBM scenarios, the identifier is obtained by combining the scenario name with the source instance size, such as LUBM-90k. We discuss next the main features of our scenario families.</p><p>a. Correctness tests. As we explained in Section 5.1, our homomorphism checker can handle only small instances. We thus prepared six scenarios that produce small chase results, while aiming to cover exhaustively the different combinations of various features. All scenarios contain s-t TGDs and test different combina-tions such as joins over the source schema, vertical partitioning, and self-joins both in the source and in the target schemas. The scenarios cover standard examples from some of the prior papers and surveys on the chase (e.g., <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b26">27]</ref>), including cases where TGDs and EGDs interact, where the chase fails, and where various acyclicity conditions are used to ensure chase termination.</p><p>b. Manually curated scenarios. Our scenarios from this family are based on the DOCTORS data integration task from the schema mapping literature <ref type="bibr" target="#b16">[17]</ref>. These scenarios are relatively small in terms of the number of relations, attributes, and dependencies (cf. Table <ref type="table" target="#tab_1">2</ref>), but we believe that they represent a useful addition to the benchmark for two reasons. First, these scenarios are based on schemas inspired by real databases about medical data. Second, they simulate a common use case for data exchange: take two databases from the same domain but with different schemas and bring them to a unified target representation. We used TOX-GENE to generate instances of the source schema of 10 k, 100 k, 500 k, and 1 M facts. DOCTORS contains EGDs that refer to more than one relation in the body, which cannot be handled by all systems in our evaluation. Hence, we also generated a simplified version, called DOCTORSFD, that contains only EGDs corresponding to functional dependencies. Consequently, the manually curated family contains eight scenarios. We also used the query generator described in Section 5.1 to generate nine queries covering most of the possible joins among the three target relations.</p><p>c. LUBM scenarios. LUBM <ref type="bibr" target="#b20">[21]</ref> is a popular benchmark in the Semantic Web community. It does not simulate a data exchange task, but it is useful as it comes with nontrivial target TGDs and queries designed to test various aspects of query answering. Using the LUBM data generator we produced instances with 90 k, 1 M, 12 M, and 120 M facts and transformed them as follows.</p><p>• The LUBM generator produces data as RDF triples, which we converted into a source instance using vertical partitioning: a triple s, p, o is transformed into a unary fact o src (s) if p = rdf :type, and into a binary fact p src (s, o) otherwise. • For each unary relation o src from the previous step, we added the s-t TGD ∀x o src (x) → o(x), and similarly for each binary relation. Thus, the s-t TGDs simply copy the source instance into the target instance. • The dependencies in LUBM are encoded using an ontology, which we converted into target TGDs using vertical partitioning and the known correspondences between description logics and first-order logic <ref type="bibr" target="#b4">[5]</ref>. • We manually converted all SPARQL queries into CQs; this was possible for all queries. As a consequence of these transformations, the source and the target schemas of LUBM contain only unary and binary relations. Also, the source instance of LUBM-120M is much larger than any other source instance in our scenarios.</p><p>d. IBENCH scenarios. IBENCH <ref type="bibr" target="#b3">[4]</ref> is a tool for generating dependencies whose properties can be finely controlled using a wide range of parameters. For our purpose, we selected two existing sets of dependencies [4, Section 5] that consist of second-order TGDs, primary keys, and foreign keys. To obtain dependencies compatible with most of our systems, we modified a parameter in the IBENCH scripts to generate ordinary s-t TGDs instead of secondorder TGDs, thus obtaining two scenarios of the IBENCH family.</p><p>• STB-128, derived from an earlier ST-benchmark <ref type="bibr" target="#b2">[3]</ref>, is the smaller scenario of the family. For both of these scenarios, we used the integration of IBENCH and TOXGENE to generate 1 k facts per source relation. Next, we used our instance repair tool from Section 5.1 to ensure that the chase does not fail. Finally, we generated 20 queries for each scenario using our query generator. e. The DEEP scenarios. The final family of scenarios was developed as a "pure stress" test. We used our target TGD generator to generate three scenarios with 1000 source relations, 299 target relations, 1000 linear s-t TGDs, and increasing numbers (100, 200, and 300) of linear target TGDs. Moreover, to generate the source instance, we globally fixed a substitution σ that maps each variable x ∈ Vars into a distinct constant value σ (x) ∈ Const; then, for each linear s-t TGD with R( x) in the body, we added σ (R( v)) to the source instance. Thus, all source instances contain just one fact per relation; however, the TGDs are very complex so they produce over 500 M facts on the largest DEEP300 scenario.</p><p>The TGDs were taken from previous work <ref type="bibr" target="#b21">[22]</ref> and they have the following structure. All TGD heads have three relations joined in a chain. Each atom has arity four and each dependency can have up to three repeated relations. The three head predicates and the body predicate have been chosen randomly out of a space of 300 predicates. We generated 10% of all TGDs from a smaller subset of predicates of size 60. Also, around 10% of the s-t TGD heads were constructed by getting the body and two (out of three) head atoms of a target TGD. This causes some target TGDs to almost map entirely to these particular s-t TGDs. After generating each dependency, a weak acyclicity test was used to discard the dependency if acyclicity was violated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SYSTEM COMPARISON</head><p>We ran a total of 40 tests per system: 6 correctness tests, 17 chase computation tests, and 17 query answering tests. Our correctness tests revealed a number of errors in the systems, most of which were corrected by the system authors during the evaluation period. All systems eventually passed all correctness tests, except for PE-GASUS which failed two correctness tests but could not be updated. Complete results of the performance tests, including a breakdown of all times, are given in the appendix and on the benchmark Web site. Figure <ref type="figure" target="#fig_2">1</ref> summarizes some results that we discuss next.</p><p>Hardware configuration. We ran all tests on a server with six physical 1.9 GHz Xeon v3 cores, 16 GB of RAM, and a 512 GB SSD, running Ubuntu v16. Our configuration is thus not very far from that of a high-end laptop.</p><p>Test Setup. All systems apart from E and DLV (as we discuss shortly) were required to perform the following steps:</p><p>1. load the source instance from .csv files on disk; 2. load the dependencies in the common format; 3. run the chase of the s-t TGDs; 4. run the chase of the target dependencies; 5. save the target instance to .csv files on disk; and 6. run each query and save its results to .csv files on disk. For each scenario, each system was allowed three hours to complete all of these tasks; if the system ran out of time or memory, we count the scenario in question as failure and report no results (not even the loading times). In order to analyze the relative contribution of all the steps, we captured the times for all steps independently whenever the systems supported such time measurements. We also repeated the experiments with s-t TGDs only.</p><p>Figure <ref type="figure" target="#fig_2">1</ref> shows (I) the chase execution times (steps 3+4), (II) the source import and target export times (steps 1 + 5), (III) the s-t TGDs chase times (step 3), (IV) the total chase times (steps 1 + 2 + 3 + 4), and (V) the query execution times (step 6) for the scenarios where the results vary significantly between different system configurations. Figure <ref type="figure" target="#fig_2">1</ref> shows the results only for the 1-parallel Skolem chase for LLUNATIC and the unrestricted Skolem chase for RDFOX. All results for all scenarios and all chase variants supported by the systems are given in the appendix. The data sizes in the STB-128 and ONT-256 scenarios do not vary so we report our results as a single bar chart; for all other scenarios we use line charts that show scaling with the data size. Test coverage of different systems is reported separately.</p><p>E and DLV do not report any intermediate times so we treated them differently. The reasoning strategy of DLV is closely related to the Skolem chase so we report its "total chase time"; however, we found no reasonable analogous time for E. Moreover, to compare E and DLV with the other systems, in Figure <ref type="figure" target="#fig_2">1</ref>.VI we show the query evaluation times measured in a different way: (i) for DLV and E, we report the total time needed to answer all queries; and (ii) for all other systems we report the import and chase times multiplied by the number of queries (thus compensating for the fact that, for each query, DLV and E load the dataset and perform reasoning from scratch), plus the sum of all query times.</p><p>A "db-to-db" test protocol, where the source and the target instances are stored in an RDBMS, might have been more natural for RDBMS-based systems. Our "file-to-file" test protocol, however, has two advantages. First, even in RDBMS-based systems importing the data may require indexing or building supporting data structures (e.g., we discuss the dictionary encoding in Section 7), which Test coverage. On some tests, certain systems did not support all required features, they terminated with an exception, or did not finish in three hours; such tests are not shown in Figures 1. Table <ref type="table">3</ref> summarizes test coverage for all tested systems. For each system and each of the three test categories, we report the number of tests that the system was applicable to and the number of successfully completed tests; furthermore, we report the causes of failures (if any). Note that for most systems a failure in data exchange on a test scenario implies failure in the corresponding query answering tests, and we count such failures only once in the table.</p><p>As one can see, LLUNATIC was the only system that completed all tests, closely followed by RDFOX which ran out of memory on LUBM-120M and DEEP300. DEMO and the systems that use chase for query reformulation exhibited low coverage; for example, PDQ completed only five out of the 17 data exchange tests. CHASEFUN completed all tests that it was applicable to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">LESSONS LEARNED</head><p>Our tests should not be misunderstood as aiming primarily for a performance competition: the tested systems were designed under completely different assumptions, often targeting tasks currently not covered by our benchmark. A key feature of our benchmark is that it covers a range of scenarios, which allowed us to answer the questions we posed in Section 1. Specifically, we could observe the behavior of the chase on a diverse set of systems and inputs, which allowed us to derive many general conclusions about the tradeoffs in implementing different chase variants (see Section 3). Moreover, we could compare the effectiveness of the systems specifically designed for the chase with that of the systems that tackle related problems. We summarize our findings in the rest of this section.</p><p>Restricted vs. unrestricted vs. parallel chase. A major decision facing a chase system designer is whether to implement the active trigger check (line 6 of Algorithm 1). By analyzing the performance of different chase variants implemented in LLUNATIC and RDFOX on our scenarios (Table <ref type="table" target="#tab_2">4</ref> shows some, and Table <ref type="table" target="#tab_6">7</ref> in the appendix shows all results), we obtained several important insights. Interestingly, the tradeoffs associated with this question are quite different for RDBMS-and RAM-based systems.</p><p>Implementing the restricted chase in RDBMS-based systems is quite challenging. Triggers are retrieved in such systems using SQL queries, so it is natural to embed the active trigger check into these queries. For example, query <ref type="bibr" target="#b12">(13)</ref>   RDBMSs, however, evaluate queries fully before any updates, so query (13) cannot detect that applying a chase step to one trigger makes another trigger inactive (as in Example 1). To properly implement the restricted chase, we must check triggers one-by-one using independent queries; for example, we can add LIMIT 1 to (13) and run one query per trigger, but this is very slow on large instances in most RDBMSs. Table <ref type="table" target="#tab_6">7</ref> confirms this: with both the 1-parallel and the unrestricted Skolem chase, LLUNATIC runs orders of magnitude faster than with the restricted Skolem chase, and some of the latter tests timed out. Query <ref type="bibr" target="#b12">(13)</ref>, however, can be used to implement the 1-parallel chase, which can even eliminate a common source of overhead: by combining DISTINCT and the active triggers check, the query never produces duplicate answers, so separate duplicate elimination is not needed. Indeed, as Table <ref type="table" target="#tab_6">7</ref> shows, LLUNATIC is faster with the 1parallel Skolem chase than with the unrestricted Skolem chase.</p><p>In contrast, RAM-based systems can more efficiently interleave queries with updates, which can make the active triggers check easier. For example, RDFOX identifies active triggers using a subquery similar to <ref type="bibr" target="#b12">(13)</ref>, but its optimized RAM-based indexes <ref type="bibr" target="#b29">[30]</ref> can efficiently answer the NOT EXISTS subqueries while taking into account the result of all concurrent updates. Thus, as Table <ref type="table" target="#tab_6">7</ref> shows, the performance of the restricted and the unrestricted Skolem chase in RDFOX differs by only a couple of seconds.</p><p>Solution sizes and getting to the core. Another question to consider is the impact of the chase variant on the size of the universal solution. Our benchmark again allowed us to investigate this issue: Table <ref type="table" target="#tab_2">4</ref> shows the solution sizes obtained by the chase variants in LLUNATIC and RDFOX (Table <ref type="table" target="#tab_6">7</ref> in the appendix shows all results). As one can see, solutions produced by the restricted chase can be between 4% and 21% smaller than those produced by the unrestricted chase; however, we did not observe any significant impact on the performance of query answering. Thus, we conclude that the choice of the chase variant can be mainly driven by the ease of implementation, rather than the solution size.</p><p>An interesting question is whether computing the core can further reduce solution sizes. To this end, we ran DEMO to compute the core of the universal solutions for scenarios in our benchmark. DEMO computed the core for DOCTORS-10k and LUBM-90k; the computation failed on all other scenarios, and the core was in both cases of the same size as the result of the restricted chase. We were not able to test the impact of computing the core on most of our scenarios: as we discuss in Section 8, computing the core of large instances is an important open problem. We could, however, answer this question partially: a set Σ of s-t TGDs can be rewritten into a set Σ such that the restricted chase of Σ returns the core of the universal solution for Σ <ref type="bibr" target="#b27">[28]</ref>. We ran this experiment on the DOCTORS scenario and only s-t TGDs; full results are shown in Table <ref type="table" target="#tab_7">8</ref> in the appendix. On the 10k and 100k scenarios, the core target instances were 18% smaller then the ones produced using unrestricted Skolem chase, suggesting that scalable methods for core computation in a more general setting could be practically relevant.</p><p>Implementing EGDs. In order to apply the EGDs, a system typically first retrieves and deletes all affected facts, applies µ, and inserts the result back into the instance. These operations require mass updates that are much more expensive in an RDBMS-based than a RAM-based system, which makes supporting EGDs in an RDBMS very challenging. Our benchmark results offer evidence for this observation: the chase times of LLUNATIC were considerably higher than of RDFOX on the ONT-256 scenario, which had the largest and most complex set of EGDs (see Figure <ref type="figure" target="#fig_2">1</ref>.I.d).</p><p>Representing labeled nulls. Chase systems must consistently distinguish labeled nulls from constant values, which turned out to be a source of complexity in RDBMS-based systems. We noticed two common solutions to this problem.</p><p>RDBMS-based systems represent labeled nulls using a suitable encoding, which is often string-based; for example, string values are encoded in query (10) using the ' _ Sk _ ' prefix. Each attribute is thus represented using just one relation column, so join conditions are expressed naturally. Nevertheless, value decoding requires pattern matching; for example, to compute certain answers, LLU-NATIC filters string-typed labeled nulls using NOT LIKE ' _ Sk _ ' in the WHERE clause. This is a considerable source of overhead, for two reasons. First, pattern matching is costly. Second, this kind of conditions are not handled nicely by the query planner: as usual, the optimizer pushes selections down towards the scans, but this forces the database engine to apply pattern matching to most facts in the target relations. In most cases, filtering out labeled nulls as a last step of the query would be faster, since the number of facts to analyze is much smaller. This effect can be observed in the query times of LLUNATIC on DEEP300, one of our very large scenarios: the system answers Query 16 in 147 s, but of these, 118 s (80%) are used to filter out labeled nulls; similarly, Query 20 takes 44 s, of which 30 s (68%) are used to filter out labeled nulls.</p><p>There are no obvious solutions to this issue. One could implement an alternative query execution strategy that forces the query planner to materialize the intermediate result of the query and then filter labeled nulls at the very end, but this incurs a materialization overhead. An alternative is to adopt a multi-column representation of labeled nulls. For example, DEMO represents labeled nulls using three columns per attribute of the target relation: one boolean column determines whether the attribute contains a constant value or a labeled null, one column stores constant values, and one column stores labels of labeled nulls. This cleanly separates labeled nulls from constant values; however, it triples the size of the database, and it greatly complicates join conditions, which also often confuses the query optimizer.</p><p>In summary, our benchmark allowed us to examine the drawbacks of both approaches, to the point that we consider the development of new, performance-oriented representations of labeled nulls an interesting open research question.</p><p>Query execution and query characteristics. All systems that successfully computed the chase also successfully evaluated all relevant queries. Most queries can be answered very quickly (typically in under 1 s). Query evaluation was slowest on DEEP300 and LUBM-120M because the target instances of that scenario are much larger than in the case of other scenarios.</p><p>We analyzed the queries in our benchmark to detect a possible correlation between execution times and various complexity parameters, such as the number of variables in the head and the body, the number of joins and so on; the parameters for all queries are shown on the benchmark Web site. We observed no clear correlation between the query parameters and the query answering times. Moreover, we found removing duplicates and filtering out labeled nulls to be significant sources of overhead for query answering, and these have no clear connection to the shape and size of the query.</p><p>Dictionary encoding. Columnar databases often compress data using a dictionary encoding, which can be applied in the chase setting using the following steps:</p><p>• one fixes an invertible mapping e of values to integers;</p><p>• the set of dependencies Σ and the input instance I are encoded as Σ e = e(Σ) and I e = e(I), respectively; • the encoded chase J e of Σ e and I e is computed; and • J e is decoded as J = e -1 (J e ) by inverting the mapping. Clearly, J is the chase of Σ and I. This process improves performance in a number of ways. First, the encoded instance is usually much smaller than the original. Second, comparing integers is faster than comparing strings. Third, data access structures such as indexes are smaller so joins are faster. Fourth, dictionary encoding removes a problem specific to the Skolem chase: target TGDs may produce very "deep" Skolem terms that can be expensive to manage, but dictionary encoding reduces the depth of such terms.</p><p>DLV, GRAAL, and RDFOX all use a variant of this idea as they load the input, and E achieves similar benefits using term indexing <ref type="bibr" target="#b34">[35]</ref>. Dictionary encoding is less common in RDBMS-based systems, but it is even more useful as it reduces the amount of data to be transferred from secondary storage, and we were again able to evaluate the impact of this optimization using the benchmark. To this end, we ran LLUNATIC with and without the encoding on LUBM-120M and DEEP300. To isolate the impact of the encoding, the source instance was available (but not encoded) in the RDBMS. First, we measured the chase time on the unencoded instance. Second, we measured the total time needed to encode the source instance, run the chase, and decode the solution. Despite the overhead of the encoding process, the execution with dictionary encoding was 46% faster on LUBM-120M and 58% faster on DEEP300. In our performance comparison, LLUNATIC was configured to turn on the encoding on all scenarios with target TGDs.</p><p>Chase vs. first-order theorem proving. E answered all queries in less than a minute on the DOCTORS-10k and DOCTORSFD-10k scenarios, but it was not as efficient on the remaining scenarios (all times are given in the appendix): it took more than an hour for DOCTORS-100k and DOCTORSFD-100k, and it ran out of memory on LUBM-1M, LUBM-12M, and LUBM-120M. Nevertheless, although E was not specifically designed for answering queries over dependencies, it could still process nontrivial scenarios. Note that the Skolem chase is actually an instance of the theorem proving calculus used in E, so the performance gap between E and the other systems is most likely due to the generality of the former.</p><p>This generality can actually be beneficial in some cases. As Figure <ref type="figure" target="#fig_2">1</ref>.VI.o shows, E performed very well on DEEP100 by answering each query in several seconds, so we analyzed the saturated set of clauses produced by E. We noticed that, by a combination of forward and backward reasoning, the system derived many intermediate clauses (i.e., "lemmas"). Some lemmas were obtained by composing s-t TGDs with target TGDs to obtain new s-t TGDs, which introduced "shortcuts" in proofs and thus sped up query answering. In fact, queries over weakly-acyclic linear dependencies (i.e., dependencies with just one atom in the body, which covers all DEEP scenarios) can always be answered fully using such an approach <ref type="bibr" target="#b1">[2]</ref>. Thus, E "discovered" this method using a general first-order theorem proving technique.</p><p>Query reformulation vs. query answering. Systems that use chase to support query reformulation (i.e., PEGASUS and PDQ) fared badly on all tests. The chase implementation in these systems is optimized for small instances obtained from the queries being reformulated, rather than for sizes found in data exchange.</p><p>Maturity of the chase. Despite the increasing complexity of the test scenarios, some consisting of over 1000 dependencies and generating up to 500 M facts, several systems successfully completed most tests on mainstream hardware (a CPU with 6 cores, 16 GB of RAM, and a 512 GB SSD). In addition, some systems were able to complete the chase and answer the queries in the majority of the tests within a few minutes. Thus, our results suggest that applying the chase to tasks of nontrivial sizes is practically feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">FUTURE CHALLENGES</head><p>Our experiments also gave us some insight regarding directions for future work in the community.</p><p>Modular implementations. While a good benchmark should provide a range of workloads for testing the chase, systems with a more modular architecture are needed in order to test hypotheses about the performance of chase. A prominent example of this kind is comparing RDBMS-and RAM-based systems: one would ideally use a system that can work either in RAM or on top of an RDBMS, and it would allow one to measure the impact of this choice independently of the myriad of other implementation factors; however, no such system exists at present. A more modular chase implementation would also be beneficial in practice since many design choices (including the one above) are appropriate for some scenarios and not for others.</p><p>Understanding the chase with EGDs. The handling of EGDs in the chase is an area in need of much further study, both in terms of theory (e.g., termination guarantees and static analysis) and implementation. Equality reasoning has been studied extensively in the theorem-proving community (e.g., <ref type="bibr" target="#b5">[6]</ref>), and thus a first step would be to better understand the applicability of these techniques to the chase with EGDs.</p><p>Computing the core. An open question is to what extent can computing the core reduce the size of the target instance. We investigated this for s-t TGDs, but to answer this question more generally scalable techniques for computing the core in the presence of target dependencies are needed.</p><p>Alternative approaches to query answering. The chase is often used for computing the certain answers to queries-a task that, in some cases, can also be tackled using radically different reasoning techniques such as theorem provers. In this paper we took a first look at comparing the chase to the other approaches, but more work is needed to further compare and possibly even combine the chase with the related approaches. The classical chase procedure is particularly well-suited for use cases where the certain answers for many queries must be computed for the same instance. For comparison with other approaches, it might be appropriate to consider a more dynamic setting, as well as possibly develop a set of complexity measures appropriate to such a setting.</p><p>Answering queries without materializing the chase. As our experiments show, the chase can be very large so computing and maintaining it can be impractical. It can sometimes thus be desirable to answer queries without first materializing the chase. A lot of work has already been invested in query rewriting, where a query is transformed to take into account the effect of dependencies so that the transformed query can be evaluated directly over the source instance. This, however, is feasible only with simple dependencies such as linear TGDs, and answering queries in cases where query rewriting is not applicable remains an open problem. Goal-oriented query answering techniques from Datalog such as magic sets <ref type="bibr" target="#b8">[9]</ref> provide a promising starting point for this investigation.</p><p>Ordering of steps. We found considerable evidence in our experiments that ordering chase steps is important in practice; for example, the good performance of CHASEFUN is due to its careful ordering of EGD and TGD steps. More research is needed to understand the impact of the ordering: even with TGDs only, a particular order can make the active triggers check more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">RELATED WORK</head><p>Experimental evaluations of many chase based systems have already been conducted <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref>. In many cases we reused and extended systems from these earlier studies, but our work differs in several important ways. First, previous studies have involved a limited number of systems, typically one or two. To the best of our knowledge, this is the first attempt to compare a large number of a very different, yet related systems. Second, earlier studies have considered only a narrow range of scenarios closely related to the function of the tested systems. We evaluate the systems on over 20 scenarios covering a wide range of assumptions, testing both correctness and scalability, covering a broad spectrum of steps related to the chase from data loading to query execution. Finally, the systems, datasets, and scenarios from the earlier studies have not been made available to the community-an important step towards advancing the experimental culture of the community.</p><p>Previous efforts that are more similar in spirit to our work include ST-Benchmark <ref type="bibr" target="#b2">[3]</ref> and iBench <ref type="bibr" target="#b3">[4]</ref>. ST-Benchmark is concerned with evaluating tools for generating schema mappings. It consists of a number of mapping tasks, where each task comprises two schemas, possibly in different data models, and a description of a transformation between them. In addition, ST-Benchmark provides a generator for mapping specifications themselves. It also modifies TOXGENE-a generator of schema instances with certain properties. The tests reported in <ref type="bibr" target="#b2">[3]</ref> focus on common transformations on a nested relational model, as opposed to relational data exchange. The ST-Benchmark suite of tools is no longer available, so we could not reuse it. However, several pieces of the infrastructure used in the ST-benchmark, such as TOXGENE and IBENCH (a successor of the specification generator from the ST-benchmark), play a prominent role in our work.</p><p>IBENCH <ref type="bibr" target="#b3">[4]</ref> is a tool for generating metadata for benchmarking mapping scenarios. It can generate source and target schemas, s-t TGDs, and target EGDs. It is publicly available, it provides some support for data generation via TOXGENE, and it has already been used for testing schema mapping tools. We complemented IBENCH with a number of additional tools, as discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">CONCLUSIONS</head><p>Our work provides the first broad look at the performance of chase systems. Our contributions include a new benchmark that comprises test infrastructure and many test scenarios, experimental results for nine prominent systems, and insights about aspects of the systems' implementation. We intend to maintain and extend our infrastructure for benchmarking the chase presented in this paper as new systems appear and as existing systems are improved. We feel that our work can be easily extended to support new classes of dependencies and other chase-related tasks, such as query reformulation with respect to dependencies.</p><p>We hope that our infrastructure will have an impact beyond the specific techniques examined here. As mentioned in the introduction, many automated reasoning communities, from SMT solving to description logics, have invested enormous effort in evaluation in the past years. In contrast, while the research literature on extending database systems with reasoning capabilities is extensive, evaluation methodologies are much less developed. A major contribution of this work, beyond its results on the chase, is as a preliminary step in addressing this gap. Evaluation infrastructure is not just a matter of providing synthetic data and dependency generators: it includes common formats, common test harnesses, and much more (see <ref type="bibr">Section 5)</ref>. We hope that some of our infrastructure and methods will spur activity in other evaluation tasks around reasoning in database systems.</p><p>Finally, in this work we took an initial step in comparing reasoning systems produced by the database community with the systems developed by other related communities.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 4 :</head><label>14</label><figDesc>RESTRICTED-CHASE(Σ, I) 1: ∆I := I 2: while ∆I = for each τ ∈ Σ with body λ ( x) do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 7 )</head><label>7</label><figDesc>and (8) to the set of facts I from Example 1 produces facts R(a, f (a)), A( f (a)), A(b), R(b, f (b)), and A( f (b)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>VI.Figure 1 :</head><label>1</label><figDesc>Figure 1: Experimental results for scenarios where times vary significantly between system configurations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>attempts to retrieve the active triggers for TGD (3) from Example 1. SELECT DISTINCT R.a FROM R WHERE NOT EXISTS (SELECT * FROM R AS R1, A AS A1, A AS A2 WHERE R.a=R1.a AND R1.b=A1.a AND R.b=A2.a)<ref type="bibr" target="#b12">(13)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Full charts of the chase experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the tested systems description, (ii) a source instance, (iii) sets of s-t TGDs, target TGDs, and/or target EGDs, and (iv) possibly a set of queries. We used existing resources whenever possible; for example, we repurposed scenarios produced by IBENCH</figDesc><table><row><cell>System</cell><cell>s-t TGDs</cell><cell>t TGDs</cell><cell>EGDs</cell><cell>Cert. Ans.</cell><cell>Engine</cell><cell>Strategy</cell><cell>Sources</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Explicit chase implementations</cell><cell></cell></row><row><cell>CHASEFUN</cell><cell></cell><cell></cell><cell>FDs only</cell><cell></cell><cell>RDBMS</cell><cell>unrestricted skolem chase</cell><cell></cell></row><row><cell>DEMO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDBMS</cell><cell>restricted chase + core computation</cell><cell></cell></row><row><cell>GRAAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAM</cell><cell>restricted chase</cell><cell></cell></row><row><cell>LLUNATIC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RDBMS restr./unrestr./1-parallel skolem/fresh-nulls chase</cell><cell></cell></row><row><cell>PDQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RDBMS</cell><cell>restricted chase</cell><cell></cell></row><row><cell>PEGASUS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAM</cell><cell>restricted chase</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Chase-related systems</cell><cell></cell><cell></cell></row><row><cell>DLV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAM</cell><cell>unrestricted skolem chase</cell><cell></cell></row><row><cell>E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAM</cell><cell>paramodulation</cell><cell></cell></row><row><cell>RDFOX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAM</cell><cell>restricted/unrestricted skolem chase</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>• ONT-256, a scenario motivated by ontologies, has several times larger source instance. Summary of the test scenarios</figDesc><table><row><cell></cell><cell>Scenario</cell><cell cols="2">Source Schema</cell><cell cols="2">Target Schema</cell><cell>s-t</cell><cell></cell><cell>t TGDs</cell><cell cols="2">EGDs</cell><cell>Qrs</cell><cell>Source Instance Facts</cell></row><row><cell>Family</cell><cell>Name</cell><cell>Rel</cell><cell>Attr</cell><cell>Rel</cell><cell>Attr</cell><cell>TGDs</cell><cell>Tot</cell><cell>Inc.Dep</cell><cell>Tot</cell><cell>FDs</cell><cell></cell><cell></cell></row><row><cell>CORR.</cell><cell>EMPDEPT</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>CORR.</cell><cell>TGDS-A</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>12</cell><cell>2</cell><cell>5</cell><cell>5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>CORR.</cell><cell>TGDS-B</cell><cell>2</cell><cell>8</cell><cell>3</cell><cell>9</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>8</cell></row><row><cell>CORR.</cell><cell>EGDS</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>3</cell></row><row><cell>CORR.</cell><cell>TGDSEGDS-A</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>12</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>4</cell><cell>4</cell><cell>0</cell><cell>4</cell></row><row><cell>CORR.</cell><cell>TGDSEGDS-B</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>12</cell><cell>6</cell><cell>5</cell><cell>5</cell><cell>4</cell><cell>4</cell><cell>0</cell><cell>80</cell></row><row><cell cols="2">MAN.C. DOCTORSFD</cell><cell>3</cell><cell>24</cell><cell>5</cell><cell>17</cell><cell>5</cell><cell>0</cell><cell>0</cell><cell>8</cell><cell>8</cell><cell>9</cell><cell>10 k, 100 k, 500 k, 1 M</cell></row><row><cell cols="2">MAN.C. DOCTORS</cell><cell>3</cell><cell>24</cell><cell>5</cell><cell>17</cell><cell>5</cell><cell>0</cell><cell>0</cell><cell>10</cell><cell>8</cell><cell>9</cell><cell>10 k, 100 k, 500 k, 1 M</cell></row><row><cell>LUBM</cell><cell>LUBM</cell><cell>30</cell><cell>76</cell><cell>74</cell><cell>179</cell><cell>30</cell><cell>106</cell><cell>91</cell><cell>0</cell><cell>0</cell><cell cols="2">14 90 k, 1 M, 12 M, 120 M</cell></row><row><cell>IBENCH</cell><cell>STB-128</cell><cell>111</cell><cell>535</cell><cell>176</cell><cell>832</cell><cell>128</cell><cell>39</cell><cell>39</cell><cell cols="2">193 193</cell><cell>20</cell><cell>150 k</cell></row><row><cell>IBENCH</cell><cell>ONT-256</cell><cell>218</cell><cell>1210</cell><cell>444</cell><cell>1952</cell><cell>256</cell><cell>273</cell><cell>273</cell><cell cols="2">921 921</cell><cell>20</cell><cell>1 M</cell></row><row><cell>DEEP</cell><cell>DEEP100</cell><cell>1000</cell><cell>5000</cell><cell>299</cell><cell>1495</cell><cell cols="2">1000 100</cell><cell>50</cell><cell>0</cell><cell>0</cell><cell>20</cell><cell>1 k</cell></row><row><cell>DEEP</cell><cell>DEEP200</cell><cell>1000</cell><cell>5000</cell><cell>299</cell><cell>1495</cell><cell cols="2">1000 200</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>20</cell><cell>1 k</cell></row><row><cell>DEEP</cell><cell>DEEP300</cell><cell>1000</cell><cell>5000</cell><cell>299</cell><cell>1495</cell><cell cols="2">1000 300</cell><cell>150</cell><cell>0</cell><cell>0</cell><cell>20</cell><cell>1 k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results for variants of the chase</figDesc><table><row><cell>System</cell><cell></cell><cell cols="2">Tests Run</cell><cell></cell><cell></cell><cell cols="2">Tests Completed</cell><cell></cell><cell></cell><cell>Failures</cell></row><row><cell></cell><cell cols="4">Corr. Chase Query Total</cell><cell cols="4">Corr. Chase Query Total</cell><cell cols="2">Timeouts Memory</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Explicit chase implementations</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CHASEFUN</cell><cell>1</cell><cell>4</cell><cell>0</cell><cell>5</cell><cell>1</cell><cell>4</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell>0</cell></row><row><cell>DEMO</cell><cell>6</cell><cell>17</cell><cell>0</cell><cell>23</cell><cell>6</cell><cell>3</cell><cell>0</cell><cell>9</cell><cell>11</cell><cell>3</cell></row><row><cell>GRAAL</cell><cell>3</cell><cell>7</cell><cell>7</cell><cell>17</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>11</cell><cell>0</cell><cell>3</cell></row><row><cell>LLUNATIC</cell><cell>6</cell><cell>17</cell><cell>17</cell><cell>40</cell><cell>6</cell><cell>17</cell><cell>17</cell><cell>40</cell><cell>0</cell><cell>0</cell></row><row><cell>PDQ</cell><cell>6</cell><cell>17</cell><cell>17</cell><cell>40</cell><cell>6</cell><cell>5</cell><cell>5</cell><cell>16</cell><cell>12</cell><cell>0</cell></row><row><cell>PEGASUS</cell><cell>6</cell><cell>17</cell><cell>0</cell><cell>23</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>17</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Chase-related systems</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DLV</cell><cell>3</cell><cell>7</cell><cell>7</cell><cell>17</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>13</cell><cell>2</cell><cell>0</cell></row><row><cell>E</cell><cell>6</cell><cell>0</cell><cell>17</cell><cell>23</cell><cell>6</cell><cell>0</cell><cell>3</cell><cell>9</cell><cell>14</cell><cell>0</cell></row><row><cell>RDFOX</cell><cell>6</cell><cell>17</cell><cell>17</cell><cell>40</cell><cell>6</cell><cell>15</cell><cell>15</cell><cell>36</cell><cell>0</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Table 3: Test coverage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">LUBM-90k</cell><cell></cell><cell></cell><cell cols="2">DEEP200</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ch.Time</cell><cell cols="3"># Facts Query T.</cell><cell>Ch.Time</cell><cell cols="3"># Facts Query T.</cell></row><row><cell cols="3">LLUNATIC 1-Parallel</cell><cell cols="3">2.67 141,213</cell><cell>0.29</cell><cell cols="3">19.25 902,636</cell><cell>0.41</cell></row><row><cell cols="3">LLUNATIC Unrest</cell><cell cols="3">6.37 177,738</cell><cell>0.31</cell><cell cols="3">33.35 926,324</cell><cell>0.44</cell></row><row><cell cols="2">LLUNATIC Rest</cell><cell></cell><cell cols="3">2196.00 141,213</cell><cell>0.23</cell><cell cols="3">7521.00 893,990</cell><cell>0.36</cell></row><row><cell cols="2">RDFOX Unrest</cell><cell></cell><cell cols="3">0.19 177,738</cell><cell>0.06</cell><cell cols="3">15.23 926,324</cell><cell>0.03</cell></row><row><cell cols="2">RDFOX Rest</cell><cell></cell><cell cols="3">0.21 141,213</cell><cell>0.06</cell><cell cols="3">24.02 892,516</cell><cell>0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Full results of the chase experiments</figDesc><table><row><cell></cell><cell cols="3">Query Answering Times</cell></row><row><cell></cell><cell cols="2">Doctors DoctorsFD</cell><cell></cell></row><row><cell></cell><cell>10k</cell><cell>10k</cell><cell></cell></row><row><cell>E</cell><cell>399,00</cell><cell>383,00</cell><cell></cell></row><row><cell>Llunatic</cell><cell>138,05</cell><cell>38,63</cell><cell></cell></row><row><cell>RDFox</cell><cell>6,96</cell><cell>7,02</cell><cell></cell></row><row><cell></cell><cell></cell><cell>LUBM</cell><cell></cell></row><row><cell></cell><cell>001</cell><cell>010</cell><cell>100</cell></row><row><cell>DLV</cell><cell>31,00</cell><cell>356,00</cell><cell>3960,00</cell></row><row><cell>Graal</cell><cell>176,32</cell><cell>2479,28</cell><cell></cell></row><row><cell>Llunatic</cell><cell>95,42</cell><cell>267,31</cell><cell>2393,79</cell></row><row><cell>RDFox</cell><cell>9,26</cell><cell>66,47</cell><cell>653,21</cell></row><row><cell></cell><cell cols="2">Deep</cell><cell></cell></row><row><cell></cell><cell>100</cell><cell>200</cell><cell></cell></row><row><cell>DLV</cell><cell>16,00</cell><cell>184,00</cell><cell></cell></row><row><cell>E</cell><cell>79,00</cell><cell></cell><cell></cell></row><row><cell>Graal</cell><cell>62,82</cell><cell>3843,99</cell><cell></cell></row><row><cell>Llunatic</cell><cell>609,39</cell><cell>783,85</cell><cell></cell></row><row><cell cols="2">PDQ 1566,79</cell><cell></cell><cell></cell></row><row><cell>RDFox</cell><cell>212,64</cell><cell>333,78</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Full results of the query answering experiments</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Doctors</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10k</cell><cell></cell><cell></cell><cell>100k</cell><cell>500k</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">LUBM</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>001</cell><cell></cell><cell></cell><cell>010</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">iBench</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>STB-128</cell><cell></cell><cell></cell><cell>Ont-256</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Deep</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>100</cell><cell></cell><cell></cell><cell>200</cell><cell></cell></row><row><cell></cell><cell>Chase</cell><cell>#Target</cell><cell>Query</cell><cell>Chase</cell><cell>#Target</cell><cell>Query</cell></row><row><cell></cell><cell>Time</cell><cell>Tuples</cell><cell>Time</cell><cell>Time</cell><cell>Tuples</cell><cell>Time</cell></row><row><cell>Llunatic -1-Parallel</cell><cell>12,13</cell><cell>18.386</cell><cell>0,17</cell><cell>19,25</cell><cell>902.636</cell><cell>0,41</cell></row><row><cell>Llunatic -Unrestricted</cell><cell>15,89</cell><cell>19.537</cell><cell>0,22</cell><cell>33,35</cell><cell>926.324</cell><cell>0,44</cell></row><row><cell>Llunatic -Restricted</cell><cell>22,23</cell><cell>18.347</cell><cell cols="2">0,17 7521,92</cell><cell>893.990</cell><cell>0,36</cell></row><row><cell>RDFox -Unrestricted</cell><cell>9,49</cell><cell>19.537</cell><cell>0,02</cell><cell>15,23</cell><cell>926.324</cell><cell>0,03</cell></row><row><cell>RDFox -Restricted</cell><cell>18,86</cell><cell>18.385</cell><cell>0,02</cell><cell>24,02</cell><cell>892.516</cell><cell>0,03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Impact of the chase variants</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Doctors (S-T tgds only)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Unrestricted</cell><cell cols="2">Restricted Less Favorable</cell><cell cols="2">Restricted Random</cell><cell cols="2">Restricted Favorable</cell><cell>Core</cell><cell></cell></row><row><cell></cell><cell>Chase Time</cell><cell cols="2"># Tuples Chase Time</cell><cell cols="2"># Tuples Chase Time</cell><cell cols="2"># Tuples Chase Time</cell><cell cols="2"># Tuples Chase Time</cell><cell># Tuples</cell></row><row><cell>10k</cell><cell>0,17</cell><cell>11.808</cell><cell>79,40</cell><cell>10.208</cell><cell>89,33</cell><cell>9.867</cell><cell>87,96</cell><cell>9.734</cell><cell>0,25</cell><cell>9.734</cell></row><row><cell>100k</cell><cell>1,14</cell><cell>97.500</cell><cell>12404,00</cell><cell>95.634</cell><cell>11340,00</cell><cell cols="2">85.168 103720,00</cell><cell>81.000</cell><cell>0,73</cell><cell>81.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Impact of the core</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>PODS'17, May 14-19, 2017, Chicago, Illinois, USA © 2017 ACM. ISBN 978-1-4503-4198-1/17/05. . . $15.00 DOI: http://dx.doi.org/10.1145/3034786.3034796</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">ACKNOWLEDGMENTS</head><p>The work by Benedikt, Motik, and Konstantinidis was funded by the EPSRC grants PDQ (EP/M005852/1), ED <ref type="bibr" target="#b2">3</ref> (EP/N014359/1), DBOnto (EP/L012138/1), and MaSI 3 (EP/K00607X/1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chase Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Target Tuples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chase Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Target Tuples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chase Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Target Tuples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Time</head><p>Llunatic -1-Parallel </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Foundations of Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vianu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing certain answers in the presence of dependencies</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Afrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiourtis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="169" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">STBenchmark: towards a benchmark for mapping systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Velegrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The iBench integration metadata generator</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Arocena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glavic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ciucanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Baader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Calvanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<title level="m">The Description Logic Handbook: Theory, Implementation and Applications</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Term Rewriting and All That</title>
		<author>
			<persName><forename type="first">Franz</forename><surname>Baader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Nipkow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graal: A toolkit for query answering with existential rules</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Baget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leclère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><surname>Mugnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sipieter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RuleML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ToXgene: A template-based data generator for XML</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keenleyside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the power of magic</title>
		<author>
			<persName><forename type="first">C</forename><surname>Beeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="269" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PDQ: Proof-driven query answering over web-based data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benedikt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsamoura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Querying with access patterns and integrity constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benedikt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsamoura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Functional Dependencies Unleashed for Scalable Data Exchange</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bonifati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ileana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSDBM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The chase revisited</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Remmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Query reformulation with constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="73" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data Exchange: Semantics and Query Answering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Kolaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCS</title>
		<imprint>
			<biblScope unit="volume">336</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="124" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Data Exchange: Getting to the Core</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Kolaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TODS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="210" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mapping and Cleaning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">That&apos;s All Folks! LLUNATIC Goes Open Source</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient Core Computation in Data Exchange</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gottlob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Acyclicity notions for existential rules and their application to query answering in ontologies</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Cuenca</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kupke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Magka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="741" to="808" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LUBM: A benchmark for OWL knowledge base systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heflin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing the chase: Scalable data integration under constraints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ambite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficiently computable Datalog ∃ programs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Terracina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veltri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>KR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gottlob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarcello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The DLV system for knowledge representation and reasoning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="499" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Testing implications of data dependencies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Mendelzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sagiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TODS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="469" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized Schema Mappings: From Termination to Tractability</title>
		<author>
			<persName><forename type="first">B</forename><surname>Marnette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scalable Data Exchange with Functional Dependencies</title>
		<author>
			<persName><forename type="first">B</forename><surname>Marnette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Core Schema Mappings: Scalable Core Computations in Data Exchange</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raunich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="677" to="711" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The backchase revisited</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="495" to="516" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parallel Materialisation of Datalog Programs in Centralised, Main-Memory RDF Systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olteanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Paramodulation-Based Theorem Proving</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nieuwenhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Automated Reasoning</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The chase procedure and its applications in data exchange</title>
		<author>
			<persName><forename type="first">A</forename><surname>Onet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DEIS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhancing DLV instantiator by backjumping techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Perri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarcello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Catalano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Art. Int</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="195" to="228" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DEMo: Data Exchange Modeling Tool</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Savenkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Term Indexing</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voronkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Automated Reasoning</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">System Description: E 1.8</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LPAR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Smt-Lib</surname></persName>
		</author>
		<ptr target="http://smtlib.cs.uiowa.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Laconic Schema Mappings: Computing Core Universal Solutions by Means of SQL Queries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chiticariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kolaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="http://www.tpc.org/" />
		<title level="m">TPC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="http://www.cs.miami.edu/~tptp/" />
		<title level="m">TPTP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">73 Llunatic -Restricted RDFox -Unrestricted 10</title>
		<imprint>
			<biblScope unit="page" from="73" to="74" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
