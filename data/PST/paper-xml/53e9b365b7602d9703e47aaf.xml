<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Competition on Counter Measures to 2-D Facial Spoofing Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohan</forename><surname>Murali</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andre</forename><surname>Chakka1</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastien</forename><surname>Anjosl</surname></persName>
							<email>sebastien.marcel@idiap.ch1</email>
						</author>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Marcell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniele</forename><surname>Tronci2</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gianluca</forename><surname>Muntoni2</surname></persName>
							<email>muntoni@sardegnaricerche.it2</email>
						</author>
						<author>
							<persName><forename type="first">Maurizio</forename><surname>Fadda2</surname></persName>
							<email>fadda@sardegnaricerche.it2</email>
						</author>
						<author>
							<persName><forename type="first">Nicola</forename><surname>Pili2</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Sirena2</surname></persName>
							<email>sirena@sardegnaricerche.it2</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Murgia2</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Ristori2</surname></persName>
							<email>ristori@sardegnaricerche.it2</email>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Roli2</surname></persName>
							<email>roli@diee.unica.it2</email>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Yan3</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Yi3</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiwei</forename><surname>Lei3</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Zhang3</surname></persName>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Li3</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anderson</forename><surname>Robson Schwartz4</surname></persName>
							<email>anderson.rocha@ic.unicamp.br4</email>
						</author>
						<author>
							<persName><forename type="first">Relio</forename><surname>Rocha4</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Javier</forename><surname>Pedrini4</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Modesto</forename><surname>Lorenzo-Navarro5</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jukka</forename><surname>Castrill6n-Santana5</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abdenour</forename><surname>Miiiittii6</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matti</forename><surname>Radid6</surname></persName>
						</author>
						<author>
							<persName><surname>Pietikiiinen6</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Ambient Intelligence Laboratory2</orgName>
								<orgName type="institution" key="instit1">Idiap Research Institutel</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences3</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Campinas4</orgName>
								<orgName type="institution" key="instit2">Universidad de Las Palmas de Gran Canaria5</orgName>
								<orgName type="institution" key="instit3">University of Oulu6</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Competition on Counter Measures to 2-D Facial Spoofing Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2873A7CFD5097C56B83281B5946AE376</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spoofing identities using photographs is one of the most common techniques to attack 2-D face recognition systems. There seems to exist no comparative stud ies of different techniques using the same protocols and data. The motivation behind this competition is to com pare the performance of different state-of-the-art algo rithms on the same database using a unique evaluation method. Six different teams from universities around the world have participated in the contest. Use of one or multiple techniques from motion, texture analysis and liveness detection appears to be the common trend in this competition. Most of the algorithms are able to clearly separate spoof attempts from real accesses.</p><p>The results suggest the investigation of more complex attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition has been an active research topic in the last two decades and its techniques are currently deployed in access control systems. Facial recognition has the advantage of non-intrusiveness over the other biometric identification techniques such as irises and finger prints. However, spoofing attacks is a major threat causing problems to face recognition to be used as a biometrics for high-security applications.</p><p>The use of facial photographs of a valid user to spoof face recognition is the most common attack method, as the photographs of the users are widely available 978-1-4577-1359-0/11/$26.00 Â©2011 IEEE through web sites like social networks. Even videos of the users can be easily captured from distant cameras without prior consent. To make face recognition as a successful biometric identification technology, there ex ist the necessity of answering the spoofing attack prob lem .</p><p>Based on the clues used for attack detection, anti spoofing techniques for 2-D face recognition can be roughly classified as motion, texture and liveness. Mo tion analysis techniques use the fact that, planar ob jects move significantly different from real human faces which are 3-D objects. Kollreider et al. <ref type="bibr" target="#b13">[ 13]</ref> evaluate the trajectories of selected part of the face from the short sequence of images using a simplified optical flow analysis followed by a heuristic classifier. The same authors introduce a method <ref type="bibr" target="#b14">[14]</ref> to fuse these scores with liveness properties such as eye-blinks or mouth movements. Bao et al . <ref type="bibr" target="#b1">[2]</ref> propose a method to detect attacks produced with planar media using optical flow based motion estimation.</p><p>Te xture analysis techniques take the advantage of detectable texture patterns such as printing failures, and overall image blur to detect attacks. Li et al . <ref type="bibr" target="#b15">[15]</ref> detect print-attacks by exploiting differences in the 2-D Fourier spectra of hard-copies of faces and real accesses. The method works well for down-sampled photo attacks, but is likely to fail for higher-quality samples. Bai et al . <ref type="bibr" target="#b0">[1]</ref> analyze the micro-textures us ing a linear SVM classifier to detect spoof attacks.</p><p>Liveness detection tries to classify attacks based on the signs of life such as eye-blinks and mouth-movements. Pan et al . <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref> bring a real-time live ness detection specifically against photo-spoofing using eye-blinks.</p><p>In spite of several advances in anti-spoofing for face recognition, there seems to exist no comparative studies of different techniques on a publicly available database. Therefore, the motivation behind this com petition is to compare the performance of different state-of-the-art algorithms on the same database using a unique evaluation method. Six different teams from universities world-wide had participated in the contest, they are Ambient Intelligence Laboratory (AMILAB), Italy; In Section 2, we briefly review the database and eval uation protocols used for the competition. Section 3 presents the algorithms of all six participants. We dis cuss the consolidated results of all algorithms in Section 4 and finally conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Database &amp; Protocols</head><p>For the competition, we used the publicly available PRINT-ATTACK biometric (face) databasel. The database consists of 200 videos of real accesses and 200 videos of attack attempts of 50 different identi ties. Real access video sequences are captured with 320 by 240 pixel (QVGA) resolution, at 25 frames-per second and for 15 Seconds duration each. These real access videos are recorded under two different back ground and illumination conditions. Attack attempts are captured with the same resolution and frame rate, for up to nine seconds duration under the same back ground and illumination conditions. Hard copies of the digital photographs printed on plain A4 paper us ing color laser printer are used for recording attacks. For attacks, two different support mechanisms are in stalled in-front of the input camera of the acquisition system. The supports used are, hand-based in which an attacker holds the client's print in his hands and fixed-support in which the print is attached to the wall.</p><p>In the competition, contestants were given access to training and development data sets, each set contain ing 60 real accesses and 60 attack attempts. The sam ple identities for these data sets were drawn randomly without repetition. All the teams were given a couple of months to train and develop their classification sys tem. For training and development, participants were Ihttp://www.idiap.ch/dataset/printattack free to use the entire video of each client, 375 frames for real accesses and 230 for attack attempts. Then we released the test data set containing 80 real access and 80 attack videos. All videos in the test set contained 230 frames. Files of the test data set are anonymized to conceal the type of the video (real/attack, hand based/fixed-support) for true evaluation. Every par ticipant's algorithm is supposed to yield a score after processing 230 frames of each video in the test data set. We had asked all the teams to provide two files con taining such scores, one for the development set and the other for the test set. We were not interested in speed, latency and complexity of the contestant's method.</p><p>In order to compute the performance measure of the spoofing detection systems, we first computed a threshold at Equal Error Rate (EER) on the devel opment set scores. Then, on the test set scores us ing the same threshold, we computed Half Total Er ror Rate (HTER), which combines the False Rejection Ratio (FRR) and the False Acceptance Ratio (FAR) with 0.5 weight. Spoof detection accuracy is good if FAR/FRR/HTER value is close to zero percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we consolidate the algorithms pro posed by the participants of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.AMILAB</head><p>We faced the problem of detecting 2-D face spoofing attacks performed by placing a printed photo of a real user in front of the camera. Unfortunately, it is not possible to relay just on the face movement as a clue of vitality because the attacker can easily simulate such a case and also because real users often show a "lower vitality" during the authentication session. Therefore, our approach consists of performing both video and still image analysis in order to employ complementary information about vitality and consequently to obtain a more robust classification.</p><p>From our experience, an image analysis performed over videos shows clear peculiar visual characteristics for captured printed photos and real scenes. To de tect the differences we explored several different types of visual features (e.g. color features, edges, textures etc.), and we used a set of support vector machines (SVMs) to compute a frame level confidence score of being a real session or not. To obtain an high sepa ration between score distributions we combined these similarity scores by means of the Dynamic Score Com bination methodology <ref type="bibr" target="#b25">[25]</ref>. This type of analysis can be performed also frame by frame.</p><p>Even if the peculiarities described above can be de tected, the vitality detection must be used to assess a certain degree of scene reality. We computed two more vitality scores on videos: the first one is based on the average movement caught by a common motion de tection technique <ref type="bibr" target="#b16">[16]</ref>, the second one depends on the number of eye blinks <ref type="bibr" target="#b4">[4]</ref>. Blinks are a proof of vitality, but nothing can be inferred in case of their absence. Conversely low degrees of movement imply an attack but movement is not distinctive for hand attacks and real sessions.</p><p>Combination was performed at score level as a weighted sum: photo detection provided excellent re sults, therefore it was given a higher weight in combina tion; movement measures provided some contribution only for fixed photo attacks therefore the score is used only in case of very little movement. By taking into ac count the above considerations, we used the following score combination scheme:</p><formula xml:id="formula_0">{a. Simage analysis + (1 -a) . Sblinks, * if Smotion is high S = al . Simage analysis + a2 . Sblin ï¿½ s + a3 . s :" otion, If Smotion IS low 3.2. CASIA</formula><p>Our method is based on the following intuitive three observations: (1) Real access videos tend to have non rigid motions, especially in the eye and mouth regions, while printed photos only have rigid transformations like translation, scaling, and rotation; (2) Real ac cess videos tend to have less noise than those spoofing videos; (3) Real access videos only have local motions in the face region while spoofing videos usually have global motions spread-out the whole support. We an alyze these three clues and construct three classifiers based on them respectively.</p><p>Classifier 1 -Non-Rigid Motion: The non rigid motions in real faces are detected by a batch image alignment technique proposed in <ref type="bibr" target="#b21">[ 21]</ref>, called "RA SL". Since the spoofing videos are generated by the fixed or hand-shaking printed photos, the motion of these videos can be well modeled by affine trans formation. RA SL algorithm has the ability to align a series of affine transformed images, therefore the frames of spoofing videos can be well aligned. For real faces, due to the non-rigid motions, there still be having large variations in the aligned frames. For this reason, the differences of the aligned frames are used to construct the first classifier.</p><p>Classifier 2 -Noise: The spoofing videos usually are noisier than those in real faces. Following the meth ods in <ref type="bibr" target="#b8">[8]</ref> and <ref type="bibr" target="#b7">[7]</ref>, noise variance can be estimated by a robust median estimator as follows (J = Medi an{ l y(i, j) l } y(i, j) E HHI (1) 0.6745 where HHI means the first order wavelet decomposi tion of a image. In our experiments, we have tested several wavelet basis functions and find that the Haar wavelet is efficient to evaluate the noise in images. The noise differences between the real and spoofing videos are used to construct the second classifier. Generally, the motion of a real face is independent of the background. On the contrary, the background around the face is usually moving together with the face in printed photo attacks. This is another significant feature to differentiate the real and spoofing videos.</p><p>Specifically, we use the GMM background modeling method <ref type="bibr" target="#b12">[12]</ref> to detect the background and Viola-Jones face detector <ref type="bibr" target="#b26">[26]</ref> to detect the face area. The ratio of motion in the face region and background is used to evaluate the face-background dependency, which is used to construct the third classifier.</p><p>Classifiers Fusion: In our system, the three clas sifiers are learned by logistic regression respectively. Classifier 1 and classifier 3 are fused to predict scores of videos with complex background, and classifier 1 and classifier 2 are used to predict scores of videos with uni form background. Since the background condition can be easily classified by edge detection, all the procedures are automatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">UNICAMP</head><p>A careful observation of the facial spoofing attack samples provides some insights regarding the charac teristics that can be explored to design a classification method. In a real access to the system, the person is able to perform slight movements with the head as well as there may exist eye blinking. On the other hand, in an attempt of attack, since a picture is being used, the movements of the head are not independent from the background, the face and the background are in the same plane, there is no eye blinking, and the quality of the printed photo might be a clue by itself.</p><p>It is valuable, therefore, to explore both spatial and temporal information to learn differences (even slight ones ) between the two classes. This suggests the use of an approach able to locate discriminative regions within the face. Our solution employs a holistic repre sentation of the face region through a robust set of low level feature descriptors, so that differences between classes can be estimated directly in the feature space, which is less prone to variations resulting from uncon trolled acquisition conditions <ref type="bibr" target="#b23">[23]</ref>, common in this do main.</p><p>Given that a holistic representation is being consid ered without explicit modeling of the characteristics to be captured (e.g., head movements and eye blink-ing), it is important to use a robust description of the samples so that models dependent on the application domain can be expendable. Such a description can be obtained with the combination of feature channels fo cusing on different image characteristics, such as shape, color, and texture <ref type="bibr" target="#b24">[24]</ref>.</p><p>The anti-spoofing proposed solution integrates fea ture descriptors based on histogram of oriented gra dients (H OG) <ref type="bibr" target="#b6">[6]</ref>, gray level co-occurrence matrix (GLCM) <ref type="bibr" target="#b11">[11]</ref>, and histograms of shearlet coefficients (H SC) <ref type="bibr" target="#b22">[22]</ref> with a weighting scheme based on partial least squares (PL S) regression <ref type="bibr" target="#b27">[27]</ref>.</p><p>To exploit both temporal and spatial information, a sample video containing n frames is divided into m parts, such that the feature extraction is performed for every k-th frame, where k = Ln/mJ. The feature extraction for the t-th frame of the j-th sample (after face detection, cropping, and resizing) is performed as follows. The frame is split into overlapping blocks with different sizes and strides to create a feature vector djâ¢t . Finally, when a video sample has descriptors extracted from all its selected frames, a high-dimensional feature vector Vj = [dj,l, dj,k+l, dj,2k+1, ... , dj,(m-l)k+1F is composed to describe the j-th sample.</p><p>To estimate a PL S regression model, we use a set of real access and attempt of attack training samples S r = {Sd' S r 2, ' .. , s ro } and So = {Sal, Sa2, ' .. , Sap}, respectively. Once the faces are detected, cropped and rescaled to a common size, descriptors are ex tracted from a selected number of frames and then concatenated to compose a feature vector. This pro cess results in two matrices, Vr = [Vd, Vr 2, " ., vro] and Va = [Va l , Va 2, ... , vap] representing the real ac cess and attempt of attack classes, respectively, with feature vectors on their columns.</p><p>With the availability of a matrix X = [ Vr, Va]T and the response vector y with its first 0 elements equals to + 1 and its last p elements equals to -1, indicating the sample class labels, the PL S regression model can be learned. The estimated regression coefficients are stored in a vector (3 to be used during the test.</p><p>When a sample video is presented to the system, the face is detected and the frames are cropped and rescaled. Then, the vector Vj, resulting from the fea ture extraction, is projected onto (3. The response in dicates whether this sample is a real access or an attack attempt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.4.IDIAP</head><p>Photograph attacks, when executed with hard copies, may suffer from print artifacts or failures that can be used as counter-measure to spoofing. Our pro posed scheme processes each video by accumulating a single Local Binary Pattern (LBP) code histogram <ref type="bibr" target="#b17">[17]</ref> with data from every single image in the stream. The histogram is matched against a pre-calculated model, using the X 2 method as proposed on the same reference to generate a final score.</p><p>The input video is first converted into gray-scale be fore passing through the LBP operator configured to use a radius R = 1 and the 8 surrounding pixels. The 2-D outputs of the LBP operator are accumulated in a global histogram with 256 bins for all input images in the sequence. After the video input has finished, the global histogram is compared to a reference histogram for attacks, generated using all images from all videos of attacks available at the training set. The output of the X 2 statistics is ready for classification without any further treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">SIANI</head><p>To solve the task, we have made the assumption that the evolution of the face appearance and its location in the image are important cues to distinguish between real and attack videos. Thus, this approach is based on the detection data collected by the ENCARA2 face detector <ref type="bibr" target="#b2">[ 3]</ref>. The face detector features each sequence as a whole to include the temporal information.</p><p>The detection data provided by ENCARA2 when ever a face is located are: the face container, and, if available, the eyes, nose, and mouth locations.</p><p>To compute the sequence features, we have analyzed the information given by the facial element locations, and a simple difference image with the previous frame. This analysis provides information of interest based on the face and facial elements motion to detect an attack.</p><p>For that reason we have computed basic statistics on the facial element locations and specific areas of the difference image. In this sense, the mean position and variance are computed for each element location.</p><p>Also, the difference image is analyzed in some spe cific areas. These specific areas are defined according to the face container, which divides the image into two areas: the face and the non face areas. Additionally, if both eyes are located, the distance between them is calculated and used to estimate the areas of interest around both eyes and the mouth. In summary, up to five areas of interest are featured: face, non face, eyes and mouth. A measure based on the difference image is computed for all those areas and normalized attending to their respective size.</p><p>The classification of the videos between real and at tacks is done with the Bayesian Network approach <ref type="bibr" target="#b20">[20]</ref> included in the Weka open source software <ref type="bibr" target="#b10">[10]</ref>. Among the different algorithms available we have selected the Chow and Liu algorithm <ref type="bibr" target="#b5">[5]</ref>, that reported the best re-suIts for the devel set. The results achieved for both sets allow the possibility of selecting a threshold to have 0% FAR for both test and devel sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">UO UL U</head><p>Our spoofing detection approach was inspired by im age quality assessment and characterization of printing artifacts <ref type="bibr" target="#b9">[9]</ref>. It is assumed that the face prints contain printing quality defects which can be recognized using texture features. Based on this, the system can detect whether there is a live person in front of the camera or a face print.</p><p>The proposed system considers only single video frames for liveness detection and performs texture anal ysis on a window containing the face area and its surrounding regions. First, a Sobel horizontal edge emphasizing filter is applied to highlight the image de fects and to produce a gradient image from which a single local binary pattern (LBP) feature histogram is computed <ref type="bibr" target="#b17">[17]</ref>. The LBP representation (LBP?d) computed over the preprocessed face image and its sur rounding regions encodes the occurrences of local tex ture primitives and describes well the differences be tween a real face and a printed face image. The 59-bin histogram is fed to an SVM classifier which determines whether or not the window contains a real face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the performance of the algo rithms of all the participants. We have observed that all teams are using one or multiple clues obtained clearly from three types of techniques -motion anal ysis, texture analysis and liveness detection. Three teams had achieved HTER of zero percent on test set. Two teams, IDIAP and UOULU have obtained zero percent EER on development set and zero percent HTER on test set based on texture analysis method. This leads to the conclusion that, the attack videos in this database mainly consist of detectable texture patterns. Incidentally both teams use LBP as the base technique to compute scores for classification. The CA SIA team has presented a method with the combina tion of motion and texture analysis techniques, and the method also allows switching between detection schemes based on the scene context. Even though this method has reported perfect detection only on test set, the scheme appears to be robust, but at the expense of complexity. The AMILAB and the UNICAMP teams used all three experts in deriving the detection scheme. The AMI LAB team has reported near-perfect result with only one false rejection on the test set. The UNI CAMP team also has reported near-perfect results with one false acceptance and one false rejection on the de-   V means the team is using corresponding technique.</p><formula xml:id="formula_1">V V V CASIA V V IDIAP V SIANI V UNICAMP V V V UOULU V</formula><p>velopment set, and one false acceptance on the test set. In spite of good performance on the development set, the SIANI team's detection method is not able to generalize the classification on the test set. Table <ref type="table" target="#tab_3">2</ref> presents the usage of techniques by different teams for detecting attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>To the best of our knowledge this is the first com petition to compare state-of-the-art counter measures to 2-D facial spoofing attacks on a publicly available database. From the results, we clearly see that mo tion analysis, texture analysis, and liveness detection are three important means to obtain the clues for de tecting print based spoof attacks. The usage of one or multiple techniques for detection appears to be a com mon trend. However, the usage of a single technique also has shown to be efficient.</p><p>A possible future investigation would be to compute performance at regular intervals of time instead of ob taining score for the whole video at once. The majority of the teams are able to clearly separate attacks from real accesses. This suggests that the problem should be made more complex, for instance expanding the database with photo quality print attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>Authors would like to thank Swiss Innovation Agency (CTI Project Replay) and FP7 European TABULA RA SA Project, http://www.tabularasa-euproject.org (257289) for the financial support. Au thors would also like to thank Christine Marcel, and Flavio Tarsetti for building the acquisition system to make the database. UNICAMP's authors are thankful to FAPE SP, CNPq, Microsoft and CAPE S for the fi nancial support. This research was partially supported by FAPE SP grants 2010/10618-3 and 2010/05647-4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Classifier 3 -</head><label>3</label><figDesc>Face-Background Dependency:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Center for Biometrics and Security Research, In stitute of Automation, Chinese Academy of Sciences (CASIA), China; Idiap Research Institute (IDIAP), Switzerland; Universidad de Las Palm as de Gran Ca naria, (SIANI), Spain; Institute of Computing (UNI CAMP), Brazil; and Machine Vision Group (UOUL U), University of Oulu, Finland.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance figures of the different teams. All values are in percentage (%).</figDesc><table><row><cell>Team</cell><cell cols="2">Development FAR FRR</cell><cell>FAR</cell><cell>Test FRR</cell><cell>HTER</cell></row><row><cell>AMILAB</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.25</cell><cell>0.63</cell></row><row><cell>CASIA</cell><cell>1.67</cell><cell>1.67</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>IDIAP</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SIANI</cell><cell>1.67</cell><cell>1.67</cell><cell>0.00</cell><cell>21.25</cell><cell>10.63</cell></row><row><cell>UNICAMP</cell><cell>1.67</cell><cell>1.67</cell><cell>1.25</cell><cell>0.00</cell><cell>0.63</cell></row><row><cell>UOULU</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Different teams on the usage of techniques. Here,</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is Physics-based Liveness Detection Truly Possible with a Single Im age</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl . Symposium on Circuits and Sys tems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3425" to="3428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Liveness De tection Method for Face Recognition based on Optical Flow Field</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Inti. Conference on Image Anal ysis and Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Castrill6n</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">EN CARA2: Real-time Detection of Multiple Faces at Different Resolutions in Video Streams</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Deniz</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Guerra</forename><surname>Hernandez Tejera</surname></persName>
		</author>
		<author>
			<persName><surname>Artal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="140" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Real time Eye Tracking and Blink Detection with USB Cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximating Discrete Prob ability Distributions with Dependence Trees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="426" to="467" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gra dients for Human Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl . Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">De-noising by Soft-thresholding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="613" to="627" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ideal Spatial Adapta tion via Wavelet Shrinkage</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometr ika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rip petoe. Characterization of Electrophotographic Print Artifacts: Banding, Jitter, and Ghosting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Eid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1313" to="1326" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reute mann, and 1. H. Witten. The WEKA Data Mining Software: An Update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shanmugam, and 1. Dinstein. Tex ture Features for Image Classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Texture-based Method for Modeling the Background and Detecting Moving Objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heikkila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="662" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kollreider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fronthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
		<title level="m">Evaluat ing Liveness by Face Images and the Structure Tensor</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ve rifying Liveness by Multiple Experts in Face Biometrics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kollreider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fronthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vi sion and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Live Face Detection based on the Analysis of Fourier Spectra</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Proceedings on Biometr ic Technology for Human Identification</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Foreground Object Detection from Videos Containing Complex Background</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">1</forename><forename type="middle">Y H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Intl. Conference on Multime dia</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mul tiresolution Gray-Scale and Rotation Invariant Tex ture Classification with Local Binary Patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intel ligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eyeblink-based Anti-Spoofing in Face Recognition from a Generic We bcamera</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intl . Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Liveness Detection for Face Recognition. Recent Advances in Face Recogni tion</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="109" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Handbook of Br ain Theory and Neural Networks, chapter Bayesian Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Cam bridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RASL: Robust Alignment by Sparse and Low-rank De composition for Linearly Correlated Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wr Ight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recogni tion</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="763" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Novel Feature Descriptor Based on the Shearlet Transform</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pedrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Inti. Conference on Im age Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Robust and Scalable Approach to Face Identification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro pean Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="476" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human Detection Using Partial Least Squares Analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Inti</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic score combination: A supervised and unsupervised score combination method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tronci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Data Mining in Pattern Recognition</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer Berlin / Heidelberg</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5632</biblScope>
			<biblScope unit="page" from="163" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust real-time face detection. Inti</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Partial least squares</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Statistical Sciences</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Kotz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>John Son</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="581" to="591" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
