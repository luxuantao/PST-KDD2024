<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MetAug: Contrastive Learning via Meta Feature Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-10">10 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangmeng</forename><surname>Li</surname></persName>
							<email>jiangmeng2019@iscas.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wenwen</forename><surname>Qiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Changwen</forename><surname>Zheng</surname></persName>
							<email>changwen@iscas.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>xionghui@ust.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute of Software Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Artificial Intelligence Thrust</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MetAug: Contrastive Learning via Meta Feature Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-10">10 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.05119v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What matters for contrastive learning? We argue that contrastive learning heavily relies on informative features, or "hard" (positive or negative) features. Early works include more informative features by applying complex data augmentations and large batch size or memory bank, and recent works design elaborate sampling approaches to explore informative features. The key challenge toward exploring such features is that the source multi-view data is generated by applying random data augmentations, making it infeasible to always add useful information in the augmented data. Consequently, the informativeness of features learned from such augmented data is limited. In response, we propose to directly augment the features in latent space, thereby learning discriminative representations without a large amount of input data. We perform a meta learning technique to build the augmentation generator that updates its network parameters by considering the performance of the encoder. However, insufficient input data may lead the encoder to learn collapsed features and therefore malfunction the augmentation generator. A new margin-injected regularization is further added in the objective function to avoid the encoder learning a degenerate mapping. To contrast all features in one gradient back-propagation step, we adopt the proposed optimization-driven unified contrastive * They have contributed equally to this work. loss instead of the conventional contrastive loss. Empirically, our method achieves state-of-the-art results on several benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Contrastive learning methods have achieved empirical success in computer vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. Under the setting of self-supervised learning (SSL), recent researches demonstrate the superiority of contrastive methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>. Typically, these approaches learn features by contrasting different views (e.g., different random data augmentations) of the image in hidden space. We recap the preliminaries of the conventional contrastive learning paradigm: every two views of the same image are considered to be a positive pair, and every two views of the different images are considered to be a negative pair; the contrastive loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> guides the learned features to bring positive pairs together and push negative pairs farther apart.</p><p>However, this learning paradigm suffers from the need for a large number of pairs to contrast, e.g., large batch size or memory bank size, because many pairs are not informative to the model, i.e., positive pairs are pretty close and negative pairs are already very apart in hidden space. These pairs have few contributions to the optimization. Contrastive methods need numerous pairs and expect to collect informative ones, and therefore complex data augmentations (e.g., jittering, random cropping, separating color channels, etc.) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> and large-scale memory banks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42]</ref> are effective in improving the performance of contrastive models on downstream tasks.</p><p>The success of the recent works depends on the elaborate selection of informative negative pairs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref>. These methods focus on designing sampling strategies to assign larger weights to informative pairs, which rely on enough and informative positive pairs and do not need large amounts of negative pairs. When the number of pairs to contrast is limited, the contrastive loss may cause conventional contrastive learning approaches to learn collapsed features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b50">51]</ref>, e.g., outputting the same feature vector for all images.</p><p>Nowadays, many researchers have noticed potential environmental problems brought by training deep learning models <ref type="bibr" target="#b49">[50]</ref>, for instance, <ref type="bibr" target="#b39">[40]</ref> reports a remarkable example that the carbon dioxide emissions generated by training a Transformer <ref type="bibr" target="#b44">[45]</ref> is equivalent to 200 round trips between San Francisco and New York by plane. Therefore, we motivate our method to perform an efficient self-supervised contrastive approach to learn anti-collapse and discriminative features based on a restricted amount of images in a training epoch (e.g., small batch size) and plain neural networks with limited parameters. Much research effort has been devoted to strong augmentations on data, but the informativeness of the features learned from the augmented data is hard to exactly measure, since the data is fed into mapping-agnostic deep neural networks to generate the features. Instead, we directly tackle augmentations on features and show that appropriate feature augmentations can sharply improve the optimization.</p><p>To this end, we propose Meta Feature Augmentation (MetAug), which learns view-specific encoders (with projection heads) and auxiliary meta feature augmentation generators (MAGs) by margin-injected meta feature augmentation and optimization-driven unified contrast. Suppose the input data has M views, and the multi-view data is fed into the encoder to generate the latent features. We initialize M neural networks as MAGs for views, which are used to augment the features of each view. We contrast all original and augmented features for bi-optimization training. Through such a learning paradigm, MetAug can improve the performance of self-supervised contrastive learning.</p><p>To learn anti-collapse and discriminative features from a restricted amount of images, MetAug relies on two key ingredients: 1) margin-injected meta feature augmentation, where MAGs use the performance of the encoder in one iteration to improve the view-specific feature augmentations for the next iteration. In this way, MAGs promote the encoder to efficiently explore the discriminative information of the input. For the original features and the augmented features generated by MAGs, we inject a margin R ? between the similarities of them, which avoids the instance-level feature collapse; 2) optimization-driven unified contrast, which contrasts all features in one gradient back-propagation step. Such proposed contrast can also amplify the impact of the instance similarity that deviates far from the optimum and weaken the impact of the instance similarity that is close to the optimum. We conduct head-to-head comparisons on various benchmark datasets, which prove the effectiveness of margin-injected meta feature augmentation and optimizationdriven unified contrast. Contributions:</p><p>? We propose margin-injected meta feature augmentation, which directly augments the latent features to generate informative and anti-collapse features. Benefiting from such features, encoders can efficiently capture discriminative information.</p><p>? We propose optimization-driven unified contrast to include all available features in one step of backpropagation and weight the similarities of paired features by measuring their contributions to optimization.</p><p>? Empirically, MetAug improves the downstream task performance on different benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Self-supervised learning. Under the setting of unsupervised learning, SSL methods have achieved impressive success, which constructs auxiliary tasks to learn discriminative information from the unlabeled inputs. Deep InfoMax <ref type="bibr" target="#b24">[25]</ref> explores to maximize the mutual information between an input and the output of a deep neural network encoder by different mutual information estimations. CPC <ref type="bibr" target="#b33">[34]</ref> proposes to adopt noise-contrastive estimation (NCE) <ref type="bibr" target="#b20">[21]</ref> as the contrastive loss to train the model to measure the mutual information of multiple views deduced by the Kullback-Leibler divergence <ref type="bibr" target="#b18">[19]</ref>. CMC <ref type="bibr" target="#b41">[42]</ref> and AMDIM <ref type="bibr" target="#b2">[3]</ref> employ contrastive learning on the multi-view data. SwAV <ref type="bibr" target="#b5">[6]</ref> compares the cluster assignments under different views instead of directly comparing features by using more views (e.g., six views). SimCLR <ref type="bibr" target="#b6">[7]</ref> and MoCo <ref type="bibr" target="#b22">[23]</ref> use large batch or memory bank to enlarge the amount of available negative features to learn good representations. Instead of exploring informative features by adopting various data augmentations and enlarging the number of features, our method focuses on straightforwardly generating informative features to contrast.</p><p>Recent works explore imposing stronger constraints on the conventional contrastive learning paradigm or propose alternative loss functions (instead of contrastive loss). De-biasedCL <ref type="bibr" target="#b10">[11]</ref> and HardCL <ref type="bibr" target="#b34">[35]</ref> consider to directly collect informative features to contrast by designing sampling strategies, which are inspired by positive-unlabeled learning methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. Motivated by <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b43">[44]</ref> proposes an information theoretical framework for SSL, which, guided by the theory, uses information bottleneck to restrict the learned features and maintain the sufficient self-supervision. In training, we first fix a? j and a? j , and then train f ? j (?), g ? j (?), f ? j (?) and g ? j (?) by using LMetAug. Next, we fix the encoders and projection heads, and train a? j and a? j in a meta updating manner. The networks are iteratively trained until convergence.</p><p>BYOL <ref type="bibr" target="#b19">[20]</ref>, W-MSE <ref type="bibr" target="#b16">[17]</ref>, and Barlow Twins <ref type="bibr" target="#b50">[51]</ref> present a crucial issue that insufficient self-supervision (e.g., not enough negative features) may lead to the feature collapse in hidden space. To tackle the mentioned issue, we propose a new margin-injected regularization in meta feature augmentation to avoid generating degenerate features. DACL <ref type="bibr" target="#b45">[46]</ref> proposes a new data augmentation that applies to domainagnostic problems. LooC <ref type="bibr" target="#b48">[49]</ref> learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces for each augmentation. These methods explore informative features from the perspective of data augmentation, while our straightforward idea behind our method is to augment features in the latent space.</p><p>Meta learning. The objective of meta learning is to automatically learn the learning algorithm. Early works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref> aim to guide the model (e.g., neural network) to learn prior knowledge about how to learn new knowledge, so that the model can efficiently learn new knowledge, e.g., the model can be quickly fine-tuned to specific downstream tasks with few training steps and achieve good performance. Recently, researchers explored to use meta learning to find optimal hyper-parameters <ref type="bibr" target="#b30">[31]</ref> and appropriately initialize a neural network for few-shot learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>. Recent approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> have focused on learning optimizers or generating a gradient-driven loss for deep neural networks in the field of NLP, computer vision, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to learn representations that capture information shared between multiple different views by performing self-supervised contrastive learning. Formally, we suppose the input multi-view dataset as X = {X 1 , X 2 , ..., X N }, where N denotes the number of samples. X i represents a collection of M views of the sample, where i ? {1, ..., N }. For each sample X i , we denote x i as a random variable representing views following x i ? P(X i ), and x j i denotes the j-th view of the i-th sample, where j ? {1, ..., M }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive learning preliminary</head><p>We recap the preliminaries of contrastive learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b41">42]</ref>: the foundational idea behind contrastive learning is to learn an embedding that maximizes agreement between the views of the same sample and separates the views of different samples in latent space. Given a multi-view dataset X, we treat pairs of the views of the same sample {x j i , x j i }, where j, j ? {1, ..., M }, as positives, versus pairs of the views of the different samples {x j i , x j i }, where i = i , as negatives. To impose contrastive learning, we feed the input x j i into a view-specific encoder f ?j (?) to learn a representation h j i , and h j i is mapped into a feature z j i by a projection head g ?j (?), where ? j and ? j are the network parameters of f ?j (?) and g ?j (?), respectively. A discriminating function d(?) is adopted to measure the similarity of {z j i , z j i }, where i = i . The encoder f ?j (?) and projection head g ?j (?) are trained by using a contrastive loss <ref type="bibr" target="#b33">[34]</ref>, which is formulated as follows:</p><formula xml:id="formula_0">L = -E X S ? ? ? ? ? log d ({z + }) d ({z + }) + K k=1 d ({z -} k ) ? ? ? ? ?<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">X S = {{z + }, {z -} 1 , {z -} 2 , ..., , {z -} k } is</formula><p>a set of pairs randomly sampled from X, which includes a positive {z + } and K negatives {z -} k , k ? {1, ..., K}, because contrastive loss can only use one positive in an iteration. In test, the projection head g ?j (?) is discarded, and the representation h j i is directly used for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Margin-injected meta feature augmentation</head><p>Recent contrastive methods rely on complex data augmentations to increase the informativeness of views. Yet this lack of guidance approach leads to the demand for a large number of training data (e.g., large batch size and memory bank). We propose a meta feature augmentation method, which creates informative augmented features by updating  parameters of its own network according to the performance (gradient) of the encoder (see Appendix A.3 for our rethinking of augmented features). A visualization of the overall MetAug architecture is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>To this end, we build a group of MAGs a ? (?) = {a ?1 (?), ..., a ? M (?)} for all M views, where ? = {? 1 , ..., ? M }. To be simplified, we define f ? and g ? as the groups of view-specific encoders and projection heads, respectively, i.e., ? = {? 1 , ..., ? M } and ? = {? 1 , ..., ? M }.</p><p>In training, the encoders f ? (?) and the projection heads g ? (?) are trained alongside the MAG a ? (?) (with the network parameters ?). Following the protocol of meta learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref>, we firstly train f ? (?) and g ? (?) under the learning paradigm of self-supervised contrastive learning. Then, a ? (?) is updated by computing its gradients with respect to the performance of f ? (?) and g ? (?). Here, we measure the performance of f ? (?) and g ? (?) by the gradients of them when the corresponding contrastive loss is back-propagated. Concretely, all of f ? (?), g ? (?), and a ? (?) are iteratively trained until convergence.</p><p>Specifically, we first update network parameters ? and ? of the encoders and projection heads by adopting the conventional contrastive loss. Then, we train the MAG a ? (?) in a meta learning manner. We encourage the augmented features to be informative, and the encoders f ? (?) can better explore the discriminative information by jointly using the original and augmented features to contrast. Hence, the performance of the encoders would be promoted on the same training data. To update network parameters ? of the a ? (?), we formalize the meta updating objective as follows:</p><formula xml:id="formula_2">arg min ? L g ? f ? X , a ? g ? f ? X (2)</formula><p>where X represents a minibatch sampled from the training dataset X, g ? f ? X , a ? g ? f ? X denotes a set including both original features and meta augmented features. ? and ? represent the parameter sets of the encoders and projection heads, respectively, which are computed by the updating of one gradient back-propagation using the contrastive loss:</p><formula xml:id="formula_3">? = ? -? ? ? L g ? f ? X , a ? g ? f ? X ? = ? -? ? ? L g ? f ? X , a ? g ? f ? X (3)</formula><p>where is the learning rate shared between ? and ?. The idea behind the meta updating objective is that we perform the second-derivative technique <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b52">53]</ref> to train a ? (?). Specifically, a derivative over the derivative (Hessian matrix) of the combination {?, ?} is used to update ?, where {?, ?} is a parameter set conjoining ? and ?. We compute the derivative with respect to ? by using a retained computational graph of {?, ?}.</p><p>However, in practice, we find a critical issue: when the original features are not informative enough, large gradients are difficult to generate by contrasting the uninformative features, the MAGs a ? (?) are inclined to create collapsed augmented features, e.g., the augmented features and the original features are very similar. We consider the reason for the feature collapse is that small gradient changes of the encoders alongside projection heads g ? (f ? (?)) lead to the update step-size of a ? (?) to become extensively small, which leaves the optimization of a ? (?) to fall into a local optimum. The augmented features are such that without any extra useful information. To tackle this issue, we further inject a margin to encourage a ? (?) to generate more complex and informative augmented features, which can be considered as a regularization term in the meta updating objective. See Figure <ref type="figure" target="#fig_2">2</ref>(a) for the details of the augmented feature collapse issue, and we observe that, without margin-injected regularization, MAGs tend to generate collapsed features that are very similar with the original features. Formally, we formulate the approach to generate margins for a ? (?) by</p><formula xml:id="formula_4">? + = min min d({z + } k + ) , max d({z -} k -) ? -= max min d({z + } k + ) , max d({z -} k -)<label>(4)</label></formula><p>where d({z + } k + ) is a set of the outputs (similarities) of positives computed by the discriminating function d(?), and k + ? {1, ..., K + } where K + represents the number of positives in a minibatch. d({z -} k -) is a set of the discriminating outputs of negatives, and k -? {1, ..., K -} where K -represents the number of negatives. Note that only original features are used in Equation <ref type="formula" target="#formula_4">4</ref>. We call the formulated margin generation approach as "Large", and we also propose two more approaches, called "Medium" and "Small". In Appendix A.2, we conduct comparisons to evaluate the effects of the three margin generation approaches.</p><p>We inject the margins between the augmented features and original features by adding a regularization term in the meta updating objective, and the regularization is defined as:</p><formula xml:id="formula_5">R ? = 1 K+ K+ k+ =1 d({? + } k+ ) -? + + + 1 K- K- k-=1 ? --d({? -} k-) +<label>(5)</label></formula><p>where {? + } k+ denotes a positive that includes one original feature and one augmented feature, and K+ denotes the number of such positives. {? -} k-represents likewise one of Knegatives, each of which includes one original feature and one augmented feature. [?] + denotes the cut-off-at-zero function, which is defined as [a] + = max(a, 0). We then integrate such regularization to the updating of ? by</p><formula xml:id="formula_6">? ? ? -? ? ? L g ? f ? X , a ? g ? f ? X + ? ? R ?<label>(6)</label></formula><p>where represents the learning rate of ?, and ? is a hyperparameter balancing the impact of the loss of margin-injected regularization term. R ? restricts MAGs to generate informative features that are more different with the original features (see Figure <ref type="figure" target="#fig_2">2(b)</ref>). In practice, Figure <ref type="figure" target="#fig_2">2</ref>(c) and (d) show that the features learned by our method (with margin-injected regularization) are more concentrated, e.g., the features of the same image are more similar and the gap between the features of the different images are enlarged, which proves informative augmented features can further lead the encoders to learn non-collapsed (scattered) features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization-driven unified contrast</head><p>We propose to jointly contrast all features (including the original features and the meta augmented features) in one gradient back-propagation step. Motivated by <ref type="bibr" target="#b36">[37]</ref>, we introduce the following optimization-driven unified loss function to replace the conventional contrastive loss as follows:</p><formula xml:id="formula_7">L OU CL = ? ? K - k -=1 d({z -} k -) - K + k + =1 d({z + } k + ) + ? ? ? + (7)</formula><p>where [?] + ensures that L ? 0 is always held. Note that all original features and augmented features are involved. ? is a margin between the summarized instance similarities to enhance the capability of the similarity separation. However, we find that the difference between</p><formula xml:id="formula_8">K - k -=1 d({z -} k -) and K + k + =1 d({z + } k + )</formula><p>is not the larger the better. Excessive increases of the difference may undermine the convergence in optimization. We thereby wish to adopt a margin ? that leads to preferable convergence. We reform the loss in Equation <ref type="formula">7</ref>by adding a temperature coefficient ? as follows:</p><formula xml:id="formula_9">L OU CL = 1 ? log 1 + K - k -=1 K + k + =1 exp ? d({z -} k -) -d({z + } k + ) + ?<label>(8)</label></formula><p>when ? ? +?, Equation 8 is Equation <ref type="formula">7</ref>. Inspired by <ref type="bibr" target="#b40">[41]</ref>, we use weighting factors ? -and ? + to modulate the impacts of d({z -} k -) and d({z + } k + ). Such approach aims to give greater weight to the similarity that deviates from the optimum and smaller weight to the similarity that has the close proximity with the optimum. </p><formula xml:id="formula_10">? -= [d({z -} k -) -O -] + and ? + = [O + -d({z + } k + )] + ,</formula><formula xml:id="formula_11">+ } k + ).</formula><p>Note that, we further propose a variation of ? including ? - and ? + , and the comparisons of them are demonstrated in Section 4.4. ? + and ? -is used to replace ? and add ? -and ? + in Equation <ref type="formula" target="#formula_9">8</ref>:</p><formula xml:id="formula_12">L OU CL = 1 ? log 1 + K - k -=1 K + k + =1 exp ? ? - (d({z -} k -) -? -) -? + (d({z + } k + ) -? + ) .<label>(9)</label></formula><p>We limit d({z -} k -) and d({z</p><formula xml:id="formula_13">+ } k + ) in the range of [0, 1] by normalizing the features in {z -} k -and {z -} k -, such that theoretically, the optimum of d({z -} k -) is 0, the opti- mum of d({z + } k + ) is 1. The positive of d({z -} k -) -O - and O + -d({z + } k + )</formula><p>can easily be guaranteed. To cut the number of hyperparameters, we reform Equation <ref type="formula" target="#formula_12">9</ref>into </p><formula xml:id="formula_14">L OU CL = 1 ? log 1 + K - k -=1 K + k + =1 exp ? (d({z + } k + ) -1) 2 + (d({z -} k -)) 2 -2? 2<label>(10)</label></formula><formula xml:id="formula_15">O + = 1 + ?, O -= -?, ? + = 1 -?, and ? -= ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model objective</head><p>Concretely, we adopt margin-injected meta feature augmentation in the contrastive learning paradigm to achieve desired discriminative multi-view representations, and the proposed L M etAug is incorporated to replace the conventional contrastive loss L. The final model objective is defined as:</p><formula xml:id="formula_16">L M etAug = L ori OU CL + ? ? L aug OU CL<label>(11)</label></formula><p>where L ori OU CL represents the loss NOT including the meta augmented features, L aug OU CL represents the loss including such features, and ? is a coefficient that controls the balance between them (we perform parameter comparisons in Appendix A.4). It is worthy to note that the margin-injected regularization R ? is only used in meta training the MAGs, i.e., a ? (?), while in regular training of encoders and projection heads, R ? is discarded. R ? restricts the augmented features to be informative so that such features can lead the encoder to efficiently and effectively learn discriminative representations. The training process is detailed by Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We benchmark our MetAug on five established datasets: Tiny ImageNet <ref type="bibr" target="#b28">[29]</ref>, STL-10 <ref type="bibr" target="#b11">[12]</ref>, CIFAR10 <ref type="bibr" target="#b28">[29]</ref>, CI-FAR100 <ref type="bibr" target="#b28">[29]</ref>, and ImageNet <ref type="bibr" target="#b27">[28]</ref>. The compared benchmark methods include: BiGAN <ref type="bibr" target="#b12">[13]</ref>, NAT <ref type="bibr" target="#b4">[5]</ref>, DIM <ref type="bibr" target="#b24">[25]</ref>, Split-Brain <ref type="bibr" target="#b51">[52]</ref>, CPC <ref type="bibr" target="#b25">[26]</ref>, SwAV <ref type="bibr" target="#b5">[6]</ref>, SimCLR <ref type="bibr" target="#b6">[7]</ref>, CMC <ref type="bibr" target="#b41">[42]</ref>, MoCo <ref type="bibr" target="#b22">[23]</ref>, SimSiam <ref type="bibr" target="#b7">[8]</ref>, InfoMin Aug. <ref type="bibr" target="#b42">[43]</ref>, BYOL <ref type="bibr" target="#b19">[20]</ref>, Barlow Twins <ref type="bibr" target="#b50">[51]</ref>, DACL <ref type="bibr" target="#b45">[46]</ref> LooC <ref type="bibr" target="#b48">[49]</ref>, Debiased <ref type="bibr" target="#b10">[11]</ref>, Hard <ref type="bibr" target="#b34">[35]</ref>, and NNCLR <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Efficiently performing MetAug</head><p>Implementations. To efficiently perform CL within a restricted amount of the inputs in training, we uniformly set the batch size as 64 (see Appendix A.1 for the comparisons under different setting of batch size). For the experiments with conv and fc as the backbone networks, we adopt a network with the 5 convolutional layers in AlexNet <ref type="bibr" target="#b29">[30]</ref> as conv and a network with further 2 fully connected layers as fc. Inspired by the backbone splitting setting of Split-Brain <ref type="bibr" target="#b51">[52]</ref>, we evenly split the AlexNet into sub-networks across the channel dimension, and each sub-network is the view-specific encoder (see Appendix B for the detailed implementation). For the experiments with ResNet-50, we directly change the encoder network to ResNet-50. All backbone encoders are not pre-trained. MetAug (only OUCL) is the ablation variant without margin-injected meta feature augmentation.</p><p>Given an RGB image, we convert it to the Lab image color space and split it into L and ab channels. During contrastive learning, RGB, L, and ab are used as three views</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MetAug</head><p>Input: Multi-view dataset X with M views of each sample, minibatch size n, and hyperparameters ?, ?, ?.</p><p>Initialize The neural network parameters: ? and ? for view-specific encoders f ? (?) and projection heads g ? (?), ? for MAGs, i.e., a ? (?). The learning rates: and . repeat for t-th training iteration do Iteratively sample minibatch  of the image. Before feeding the views into our model, we simply adopt the same data augmentations in CMC <ref type="bibr" target="#b41">[42]</ref>.</p><formula xml:id="formula_17">X = {X i } tn i=(t-1)n . # regular contrastive training step ? ? ? -? ? L M etAug (f ? , g ? , a ? , X) ? ? ? -? ? L M etAug (f ? , g ? , a ? , X) end for for t-th training iteration do Iteratively sample minibatch X = {X i } tn i=(t-1)n . # compute f ast weights # retain computational graph ? = ? -? ? L M etAug (f ? , g ? , a ? , X) ? = ? -? ? L M etAug (f ? , g ? , a ? , X) # meta training step using second derivative ? ? ?-? ? L M etAug (f ? , g ?, a ? , X) + ? ? R ? end for until ?,</formula><p>Especially, the major contribution of DACL is the proposed data augmentation (i.e., mixup-noise) so that we particularly add mixup data augmentation for DACL. In training, a memory bank <ref type="bibr" target="#b47">[48]</ref> is adopted to facilitate calculations. We retrieve 4096 past features from the memory bank to derive negatives. The learning rates and weight decay rates are uniform over comparisons.</p><p>Comparison on downstream tasks. We collect the results of 20 trials for comparisons. The average result of the last 20 epochs is used as the final result of each trial, and the 95% confidence intervals are also reported, while the results without 95% confidence intervals are quoted from the published papers. We compare MetAug against a fullysupervised method (similar to AlexNet <ref type="bibr" target="#b29">[30]</ref>) and the stateof-the-art unsupervised methods. Table <ref type="table" target="#tab_0">1</ref> shows the comparisons on four benchmark datasets. The last two rows of tables represent the results of our methods. As demonstrated in tables, MetAug beats the best prior methods on all datasets. Even compared with the fully-supervised method trained end-to-end (without fine-tuning) for the architecture presented, the proposed method has a significant improvement on most downstream tasks, which demonstrates that MetAug can better model discriminative information when supervision is insufficient (e.g., the training data is limited). The ablation model (i.e., MetAug (only OUCL)) outperforms most unsupervised methods but falls short of the performance of MetAug. Thus, the ablation study proves the effectiveness of our proposed margin-injected meta feature augmentation and optimization-driven unified contrast. DACL and LooC propose to enhance contrastive learning from the perspective of data augmentation, while MetAug improves contrastive learning from the perspective of feature augmentation. The idea behind our method is simple but effective, since contrastive learning works directly on features, and the augmented images need one step of encoding to become features. The experimental results support that MetAug achieves better performance on benchmarks.</p><p>Performing MetAug on ResNet. We perform classification comparisons on the CIFAR10 and STL-10 by using ResNet-50. Table <ref type="table" target="#tab_1">2</ref> shows that MetAug and the ablation variant outperform the compared methods, which indicates that MetAug has strong adaptability to different encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmarking MetAug on ImageNet</head><p>Implementation. To comprehensively understand the performance of our proposed MetAug, we conduct comparisons on ImageNet and make fair comparisons with benchmark methods. The backbone encoder is conv or ResNet-50, and the results are demonstrated in Table <ref type="table" target="#tab_2">3</ref>. MetAug is a decoupled approach so that we can introduce MetAug in the learning paradigm of state-of-the-art to improve the performance, e.g, for the experiments using conv or ResNet-50, we perform MetAug in CMC or NNCLR, respectively.</p><p>Results. As shown in Table <ref type="table" target="#tab_2">3</ref>, we find that MetAug can effectively promote the performance of benchmark methods in the comparisons using both conv and ResNet-50. The results support that our proposed meta feature augmentation can enable different encoders to model discriminative information even in the large-scale dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Is MetAug robust for data augmentation?</head><p>To illustrate the impacts of different data augmentations, we conducted multiple comparisons on CIFAR10 shown in Table <ref type="table" target="#tab_3">4</ref>. Note that horizontal flip and rotate are similar, and we use them together in the 1-th comparison. In the 5-th comparison, we take the same data augmentations as the setting of comparisons in Section 4.1. The data augmentations adopted in the 6-th comparison are as same as the setting of LooC <ref type="bibr" target="#b48">[49]</ref>. Additionally, mixup is proposed by DACL <ref type="bibr" target="#b45">[46]</ref>.</p><p>We observe from Table <ref type="table" target="#tab_3">4</ref> that MetAug outperforms the compared methods in all comparisons. It is worth noting that even using weak data augmentation degenerates the performance of our method as well as benchmark methods, but the performance degeneration of our method is minimal compared to others, e.g., from 8-th and 1-th comparison, we find that the gap of MetAug is 1.60%, while that of LooC is 2.44%. The results support that MetAug is robust for various data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Do the variant of ? promote MetAug?</head><p>In practice, we find that the introduction of the weighting factor ? cannot directly improve our proposed method. Our conjuncture lies in that ? may cause the loss to converge excessively fast, which leaves the network parameters at a local minimum. Therefore, we propose a variant to replace ? in Equation <ref type="formula" target="#formula_12">9</ref>, i.e., ? = ? ? dec where ? dec is a linear attenuation coefficient to linearly attenuate the impact of ? so that the difference between the current value and the optimum becomes smaller. We use MetAug (only OUCL) to demonstrate the effectiveness of the proposed variant. The results are shown in Figure <ref type="figure" target="#fig_5">3</ref>. We observe that the performance of our method get peak value when ? dec is 6, which manifests that introducing a certain linear attenuation to ? can promote MetAug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We conclude that exploring informative features is the key to contrastive learning. Different from the conventional contrastive methods that collect enough informative features to learn a good representation by enlarging the batch or memory bank, we motivate MetAug to learn a discriminative representation from a restricted amount of images. Our method proposes margin-injected meta feature augmentation to straightforwardly augment features to be informative and avoid learning degenerate features. To efficiently make use of all available features, MetAug further proposes optimization-driven unified contrast. Experimental evaluations demonstrate that MetAug achieves the state-of-the-art. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix -Extended comparisons</head><p>In this section, we provide several experimental analyses about the advantages of our proposed method. The experiments to find appropriate hyperparameters are conducted as well, and in detail, we conduct comparisons of using different hyperparameters on the validation set of corresponding benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Can MetAug perform consistently under different settings of batch size?</head><p>As the results shown in Table <ref type="table" target="#tab_0">1</ref>, 2, and 3, we observe that MetAug achieves our expectation that learning anti-collapse and discriminative representations from a restricted amount of images in a training step (i.e., the batch size is limited). However, we conduct further experiments to explore whether MetAug has consistent performance under settings of larger batch sizes.</p><p>From Figure <ref type="figure" target="#fig_6">4</ref>, we observe that with the increase of batch size, each compared method achieves better performance on the downstream task. We conjecture that with the enlarging of batch size, the number of available features in a training step is increased, so that models may explore more informative features to promote the performance of contrastive learning. Yet, comparing our method with the benchmark methods, we find that the gap between the performance of MetAug (only OUCL) and the compared methods becomes smaller. We extend the mentioned conjecture: as more informative features can be explored by all methods in a training step, OUCL's advantage becomes less significant. OUCL aims to include all available features to efficiently train the model and avoid the optimization fall into a local optimum, and the increase of batch size, which means sufficient self-supervision, can naturally promote the efficiency of optimization and avoid the fall into a local optimum. Yet  the advance of OUCL is always maintained, which is supported by the comparison. Only LooC's performance can gradually catch up with the performance of MetAug (only OUCL). We research the setting of LooC, and find that LooC leverages more than one (e.g., three) contrastive loss in a training step, which allows LooC to train the model multiple times. We observe that, even in the large batch size, MetAug can still improve the state-of-the-art methods by a significant margin.</p><p>Concretely, MetAug maintains its superiority over compared method under different settings of batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Variants of the injected margin</head><p>We denote min + as min({d({z + } k + )}) and max -as max({d({z -} k -)}). For "Medium", both ? + and ? -equal to mean[min + , max -]. ? + = max[min + , max -] and ? -= min[min + , max -] in "Small".</p><p>In Figure <ref type="figure" target="#fig_7">5</ref>, we conduct comparisons on CIFAR100 with fc. We observe that, whether our method uses L contrast or the proposed L OU CL , all three variants can improve MetAug, and our method with "Large" achieve the best performance. The experiments further prove the effectiveness of the two key ingredients of MetAug.  We conducted comparisons based on MetAug (onlu OUCL) on Tiny ImageNet with fc encoder. To measure the impact of ?, we iteratively select ? and observe the accuracy of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Understanding of the augmented features</head><p>To understand the augmented features, we conduct a comparison of MetAug by adopting the augmented features in the test or not. As shown in Table <ref type="table" target="#tab_4">5</ref>, the results of MetAug using the augmented features in the test are listed in the w/ augmented features column, and the results of MetAug NOT using the augmented features in the test are listed in the w/o augmented features column (which is the regular approach in the test). We select ? from the range of {10 -1 , 10 -2 , 10 -3 , 10 -4 , 10 -5 , 10 -6 , 10 -7 , 10 -8 } to generate different augmented features. Specifically, the approach of adopting the augmented features in the test is that we use MAGs to generate augmented features and such features are treated as the same as the original features, i.e., augmented features are regarded as additions of original features. Note that, in regular test (i.e., w/ augmented features), we use the representation h j i and discard the projection head g ?j (?) to feed into the classifier, while, in the test of using augmented features, we have to use the feature z j i generated by the projection head g ?j (?) to feed into the classifier, because MAGs work on the feature z j i . From Table <ref type="table" target="#tab_4">5</ref>, we observe that, generally, MetAug w/o augmented features beats MetAug w/ augmented features. The reasons behind such phenomenon are: 1) the augmented features are generated to lead the encoders to learn discriminative representations (e.g., h j i ), which indicates that the augmented features contribute to the improvement of the encoders, but this does not mean that the augmented features are discriminative for downstream tasks; 2) in the test of using augmented features, we do not discard the projection head g ?j (?), and recent works prove that the approach of using a projection head in training and discarding such head in the test can significantly improve the performance of the model on downstream tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Out of the understanding of the experimental results, we think that the augmented features contain useful information that can improve the encoder, but such information may not be discriminative to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Synthetic comparison of hyperparameters</head><p>To intuitively understand the impacts of hyperparameters, we conduct comparisons by using various combinations of them for the proposed MetAug. Specifically, ? controls the impact of the proposed margin-injected regularization term. The hyperparameter ? is proposed as a temperature coefficient in OUCL. ? is a specific parameter to replace the hyperparameters in OUCL such that the number of hyperparameters can be reduced. ? balances the impact of OUCL that uses augmented features and OUCL that does not use these features.</p><p>As demonstrated in Figure <ref type="figure" target="#fig_9">7</ref>, we first solely study ?'s impact on MetAug, because ? is only used in OUCL function, and in practice, we find that, compared with other hyperparameters, ? has less impact on our method. We conduct experiments on Tiny ImageNet with fc encoder and select ? from the corresponding range for MetAug (only OUCL) to clarify its impact, and the results indicate that appropriate selected ? can indeed promote the performance of our method, but the differences between the impacts of different ? are limited.</p><p>Then, we fix ? = 0.40 and study on the impacts of other hyperparameters. As the results are shown in Figure <ref type="figure">6</ref>, the plots further elaborate our parameter studies' results with MetAug on the CIFAR10 benchmark dataset with fc encoder. To explore the influence of ? and ?, we first fixed ? = 10 -13 , and then we selected ? from the range of {2, 2 2 , 2 3 , 2 4 , 2 5 , 2 6 , 2 7 , 2 8 } and ? from the range of {10 -1 , 10 -2 , 10 -3 , 10 -4 , 10 -5 , 10 -6 , 10 -7 , 10 -8 }. Following the same experimental principle as above, we selected ? from the range of {10 -3 , 10 -5 , 10 -7 , 10 -9 , 10 -11 , 10 -13 , 10 -15 , 10 -17 }. See Figure <ref type="figure">6</ref>(a), (b), and (c) for the details of the comparisons. In general, good classification performance highly depends on the ? and ? terms. Also, ? is an intensely necessary supplement for adapting the interval between similarities of augmented features and original features, which avoids to learn degenerate representations. We also find that the potential to improve the learned representations grows with the adjustment of term ?, e.g., the initial loss becomes relatively large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Appendix -Implementation</head><p>In this paper, we introduce a novel self-supervised representation learning approach, i.e., Meta Feature Augmentation (MetAug), of which Figure <ref type="figure" target="#fig_0">1</ref> depicts the overview framework. The following subsections provide the design details of MetAug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Network architecture</head><p>In the experiments, neural network classification methods (i.e., conv and fc) are adopted as the backbone networks, and the classifiers (i.e., the linear networks) on the representations extracted from the encoders are performed on downstream classification tasks.</p><p>According to the principle of building the encoders, the AlexNet is split across the channel dimension with the idea that split-AlexNet can also perform well in learning representations between views, which only has the halved learnable parameters <ref type="bibr" target="#b51">[52]</ref>. We build the AlexNet with 5 convolutional layers, 2 linear layers, and a fully connected layer followed by a l2 normalization function. Then the split-AlexNets (i.e., the sub-networks) are regarded as the encoders. In experiments, we use conv and fc, which use the corresponding layers of AlexNet. Note that we split AlexNet across channels for RGB, L, and ab views. in the test, we concatenate representations layer-wise from the encoders into one to achieve the final representations of the inputs.</p><p>We develop the classifier by leveraging a linear network followed by a softmax output function. Following the proposed experimental setting of the previous literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref>, we evaluate the quality of the learned representations by freezing the weights of backbone encoders and training the linear classifier in the test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Algorithm description</head><p>MetAug is an end-to-end representation learning method: we iteratively train the encoders and MAGs by backpropagating L M etAug , and the training process is based on Adam gradient optimization.</p><p>The proposed MetAug is a generalized approach, which can be used for various downstream tasks, e.g., classification, clustering, regression, etc. We can straightforwardly train the encoders, pretrained by MetAug, on downstream tasks.</p><p>Here, we provide a pseudo-code for MetAug training described in the style of PyTorch, which is without the inclusion of the detailed matrix processing or helper utility functions &amp; codes that are irrelevant to the algorithm: # phi_dec is a hyperparameter ap = torch.clamp(1/phi_dec -sp.detach().div(phi_dec) + 1, max=1000, min=0.) an = torch.clamp(sn.detach().div(phi_dec) + 1, max=1000, min=0.) delta_p = 1 -gamma delta_n = gamma logit_p = -ap * (sp -delta_p) * beta logit_n = an * (sn -delta_n) * beta loss = self.soft_plus(torch.logsumexp(logit_n, dim=1) + torch.logsumexp(logit_p, dim=1)).sum(). div(sp.shape[0]) loss = loss.div(beta).mul <ref type="bibr" target="#b15">(16)</ref> return loss def forward(self, out_ab2l, ..., out_ori2ab):</p><p># \/=======calculate pos and neg========# pos_ab2l,neg_ab2l = torch.split(out_ab2l,[1,out_ab2l.shape <ref type="bibr" target="#b0">[1]</ref>-1],dim=1) ... pos_ab2l = pos_ab2l.squeeze <ref type="bibr" target="#b1">(2)</ref>.add <ref type="bibr" target="#b0">(1)</ref>.div(2) ... pos = torch.cat((pos_ab2l, pos_l2ab, pos_ori2l, pos_l2ori, pos_ab2ori, pos_ori2ab), dim=1) neg = torch.cat((neg_ab2l, neg_l2ab, neg_ori2l, neg_l2ori, neg_ab2ori, neg_ori2ab), dim=1) loss = self.oucl_ite_de_to_softmax(pos, neg).sum() return loss def margin_injection_loss_calc(mtl_out_ab2l, ..., out_ori2ab): # split pos neg mtl_pos_ab2l, mtl_neg_ab2l = torch.split(mtl_out_ab2l, [1, mtl_out_ab2l.shape <ref type="bibr" target="#b0">[1]</ref>-1], dim=1) ... pos_ab2l, neg_ab2l = torch.split(out_ab2l, [1, out_ab2l.shape <ref type="bibr" target="#b0">[1]</ref>-1], dim=1) ... # post-process mtl_pos_ab2l = mtl_pos_ab2l.squeeze <ref type="bibr" target="#b1">(2)</ref>.add <ref type="bibr" target="#b0">(1)</ref>.div(2) ... mt_pos = torch.cat((mtl_pos_ab2l, ..., mtori_pos_ori2ab), dim=1) mt_neg = torch.cat((mtl_neg_ab2l, ..., mtori_neg_ori2ab), dim=1) pos_ab2l = pos_ab2l.squeeze <ref type="bibr" target="#b1">(2)</ref>.add <ref type="bibr" target="#b0">(1)</ref>.div(2) ... pos = torch.cat((pos_ab2l, pos_l2ab, pos_ori2l, pos_l2ori, pos_ab2ori, pos_ori2ab), dim=1) neg = torch.cat((neg_ab2l, neg_l2ab, neg_ori2l, neg_l2ori, neg_ab2ori, neg_ori2ab), dim=1) # get max pos min neg min_positive_value, min_positive_pos = torch.min(pos, dim=-1) max_negative_value, max_negative_pos = torch.max(neg, dim=-1) # margin_type: a hyperparameter if margin_type == 'small': lgamma_margin_pos, _ = torch.max(torch.cat((min_positive_value.unsqueeze(1), max_negative_value .unsqueeze(1)), dim=1), dim=-1) lgamma_margin_pos = lgamma_margin_pos.unsqueeze(1) lgamma_margin_neg, _ = torch.min(torch.cat((min_positive_value.unsqueeze(1), max_negative_value .unsqueeze(1)), dim=1), dim=-1) lgamma_margin_neg = lgamma_margin_neg.unsqueeze(1) elif margin_type == 'large': lgamma_margin_pos, _ = torch.min(torch.cat((min_positive_value.unsqueeze(1), max_negative_value .unsqueeze(1)), dim=1), dim=-1) lgamma_margin_pos = lgamma_margin_pos.unsqueeze(1) lgamma_margin_neg, _ = torch.max(torch.cat((min_positive_value.unsqueeze(1), max_negative_value .unsqueeze(1)), dim=1), dim=-1) lgamma_margin_neg = lgamma_margin_neg.unsqueeze(1) else: lgamma_margin_pos = torch.mean(torch.cat((min_positive_value.unsqueeze(1), max_negative_value. unsqueeze(1)), dim=1), dim=1, keepdim=True) lgamma_margin_neg = lgamma_margin_pos # get margin injection loss loss = torch.mean(torch.clamp(mt_pos -lgamma_margin_pos, min=0)) loss += torch.mean(torch.clamp(lgamma_margin_neg -mt_neg, min=0)) return loss</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. MetAug's architecture. Dashed blue box represents the data encoding process, and dashed red box represents the meta feature augmentation.In training, we first fix a? j and a? j , and then train f ? j (?), g ? j (?), f ? j (?) and g ? j (?) by using LMetAug. Next, we fix the encoders and projection heads, and train a? j and a? j in a meta updating manner. The networks are iteratively trained until convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Similarity histograms obtained by our method (with or without margin-injected regularization) on CIFAR-10. (a) and (b) demonstrate the summarized similarity of positives (i.e., {? + }) that include original features and augmented features. (c) and (d) demonstrate the statistical results of the original features learned by our model. Blue histograms represent the similarity between the features of the same image's views, and red histograms represent the similarity between the features of the different images' views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where O -and O + represents the expected optimums of d({z -} k -) and d({z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>?, and ? converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The impact of different ? dec on the performance of our method using ?. Comparisons are conducted on Tiny ImageNet with conv as the encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of different methods on classification accuracy (top1) under various settings of batch size. We conducted experiments on CIFAR10 with conv encoder.</figDesc><graphic url="image-27.png" coords="11,56.86,72.00,222.75,169.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Heatmap of injected margin variant comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )Figure 6 .</head><label>a6</label><figDesc>Figure 6. Impacts of the hyperparameters ?, ?, and ? of our proposed method. We conducted comparisons based on MetAug on CIFAR10 with fc encoder. To measure the influences, we iteratively fixed one parameter and then study on the others by selecting them in the ranges, respectively.</figDesc><graphic url="image-35.png" coords="12,63.20,73.55,150.69,115.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Impacts of the hyperparameter ? of our proposed method.We conducted comparisons based on MetAug (onlu OUCL) on Tiny ImageNet with fc encoder. To measure the impact of ?, we iteratively select ? and observe the accuracy of the method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>#</head><label></label><figDesc>inputs: input data in a batch # index: index for each sample (used for memory bank) # model: view-specific encoders and projection heads # optimizer: optimizer for model # contrast: performing d() # criterion: performing OUCL # l_mtgen, ab_mtgen, ori_mtgen: view-specific MAGs # l_mtgen_op, ab_mtgen_op, ori_mtgen_op: optimizer for MAGs model = MyAlexNetCMC() # split AlexNet l_mtgen = MyMetaGenNet() # 3 fully-connected layers ab_mtgen = MyMetaGenNet() # 3 fully-connected layers ori_mtgen = MyMetaGenNet() # 3 fully-connected layers contrast = Contrast() # d(), and memory bank only store original features criterion = OUCL() # performing OUCL # Step 1: Fix metagen aug, and optimize main_model for param in l_mtgen.parameters(): param.requires_grad = False for param in ab_mtgen.parameters(): param.requires_grad = False for param in ori_mtgen.parameters(): param.requires_grad = False for param in model.parameters(): param.requires_grad = True # ===================forward===================== optimizer.zero_grad() feat_l, feat_ab, feat_ori = model(inputs, False) mt_feat_l = l_mtgen(feat_l, False) mt_feat_ab = ab_mtgen(feat_ab, False) mt_feat_ori = ori_mtgen(feat_ori, False) # margin-injected loss calculation # ori loss calc out_ab2l, ..., out_ori2ab = contrast(feat_l, feat_ab, feat_ori, index) loss = criterion(out_ab2l, ..., out_ori2ab) # aug loss calc mtl_out_ab2l, ..., out_ori2ab = contrast(mt_feat_l, feat_ab, feat_ori, index) loss += delta * criterion(mtl_out_ab2l, ..., out_ori2ab) mtab_out_ab2l, ..., mtab_out_ori2ab = contrast(feat_l, mt_feat_ab, feat_ori, index) loss += delta * criterion(mtab_out_ab2l, ..., mtab_out_ori2ab) out_ab2l, ..., mtori_out_ori2ab = contrast(feat_l, feat_ab, mt_feat_ori, index) loss += delta * criterion(out_ab2l, ..., mtori_out_ori2ab) mt_out_ab2l, ..., mt_out_ori2ab = contrast(mt_feat_l, mt_feat_ab, mt_feat_ori, index) loss += delta * criterion(mt_out_ab2l, ..., mt_out_ori2ab) # margin-injection loss += alpha * margin_injection_loss_calc(mtl_out_ab2l, ..., out_ori2ab) # ==================backward===================== loss.backward() optimizer.step() # Step 2: Fix main_model, and optimize metagen aug for param in l_mtgen.parameters(): param.requires_grad = True for param in ab_mtgen.parameters(): param.requires_grad = True for param in ori_mtgen.parameters(): param.requires_grad = True # =================meta forward================== l_mtgen_op.zero_grad() ab_mtgen_op.zero_grad() ori_mtgen_op.zero_grad() feat_l, feat_ab, feat_ori = model(inputs, False) mt_feat_l = l_mtgen(feat_l, False) mt_feat_ab = ab_mtgen(feat_ab, False) mt_feat_ori = ori_mtgen(feat_ori, False) # margin-injected loss calculation ... # meta feat generation feat_l, feat_ab, feat_ori = model(inputs, False) # get meta weights fast_weights = OrderedDict((name, param) for (name, param) in l_mtgen.named_parameters()) # create_graph flag for computing second-derivative grads = torch.autograd.grad(loss, l_mtgen.parameters(), create_graph=True) data = [p.data for p in list(l_mtgen.parameters())] # compute \mathring{\theta} and \mathring{\vartheta} by applying sgd on OUCL loss # mathcal_l: \mathcal{l} fast_weights = OrderedDict((name, param -mathcal_l * grad) for ((name, param), grad, data) in zip( fast_weights.items(), grads, data)) mt_feat_l = l_mtgen(feat_l, fast_weights) # get meta weights fast_weights = OrderedDict((name, param) for (name, param) in ab_mtgen.named_parameters()) # create_graph flag for computing second-derivative grads = torch.autograd.grad(loss, ab_mtgen.parameters(), create_graph=True) data = [p.data for p in list(ab_mtgen.parameters())] # compute \mathring{\theta} and \mathring{\vartheta} by applying sgd on OUCL loss fast_weights = OrderedDict((name, param -mathcal_l * grad) for ((name, param), grad, data) in zip( fast_weights.items(), grads, data)) mt_feat_ab = ab_mtgen(feat_ab, fast_weights) # get meta weights fast_weights = OrderedDict((name, param) for (name, param) in ori_mtgen.named_parameters()) # create_graph flag for computing second-derivative grads = torch.autograd.grad(loss, ori_mtgen.parameters(), create_graph=True) data = [p.data for p in list(ori_mtgen.parameters())] # compute \mathring{\theta} and \mathring{\vartheta} by applying sgd on OUCL loss fast_weights = OrderedDict((name, param -mathcal_l * grad) for ((name, param), grad, data) in zip( fast_weights.items(), grads, data)) mt_feat_ori = ori_mtgen(feat_ori, fast_weights) # margin-injected loss calculation ... # ================meta backward================== loss.backward() l_mtgen_op.step() ab_mtgen_op.step() ori_mtgen_op.step() class OUCL(nn.Module): def __init__(self): super(OUCL, self).__init__() self.criterion = nn.CrossEntropyLoss() self.soft_plus = nn.Softplus() def oucl_ite_de_to_softmax(self, sp, sn):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different methods on classification accuracy (top 1). We use conv and fc as backbones in the experiments. ? denotes that the methods have reduced learnable parameters (See Appedix B.1). ? 0.2 41.37 ? 0.2 77.50 ? 0.2 79.73 ? 0.3 76.37 ? 0.3 79.30 ? 0.2 51.04 ? 0.2 52.31 ? 0.2 BYOL 41.59 ? 0.2 41.90 ? 0.1 81.73 ? 0.3 81.57 ? 0.2 77.18 ? 0.2 80.01 ? 0.2 53.64 ? 0.2 53.78 ? 0.2 Barlow Twins 39.81 ? 0.3 40.34 ? 0.2 80.97 ? 0.3 81.43 ? 0.3 76.63 ? 0.3 78.49 ? 0.2 52.80 ? 0.2 52.95 ? 0.2 DACL 40.61 ? 0.2 41.26 ? 0.1 80.34 ? 0.2 80.01 ? 0.3 81.92 ? 0.2 80.87 ? 0.2 52.66 ? 0.2 52.08 ? 0.3 LooC 42.04 ? 0.1 41.93 ? 0.2 81.92 ? 0.2 82.60 ? 0.2 83.79 ? 0.2 82.05 ? 0.2 54.25 ? 0.2 54.09 ? 0.2 SimCLR + Debiased 38.79 ? 0.2 40.26 ? 0.2 77.09 ? 0.3 78.39 ? 0.2 80.89 ? 0.2 80.93 ? 0.2 51.38 ? 0.2 51.09 ? 0.2 SimCLR + Hard 40.05 ? 0.3 41.23 ? 0.2 79.86 ? 0.2 80.20 ? 0.2 82.13 ? 0.2 82.76 ? 0.1 52.69 ? 0.2 53.13 ? 0.2 CMC ? + Debiased 41.64 ? 0.2 41.36 ? 0.1 83.79 ? 0.3 84.20 ? 0.2 82.17 ? 0.2 83.72 ? 0.2 58.48 ? 0.2 57.16 ? 0.2 CMC ? + Hard 42.89 ? 0.2 42.01 ? 0.2 83.16 ? 0.3 85.15 ? 0.2 83.04 ? 0.2 86.22 ? 0.2 58.97 ? 0.3 59.13 ? 0.2 MetAug (only OUCL) ? 42.02 ? 0.1 42.14 ? 0.2 84.09 ? 0.2 84.72 ? 0.3 85.98 ? 0.2 87.13 ? 0.2 59.21 ? 0.2 58.73 ? 0.2 MetAug</figDesc><table><row><cell>Model</cell><cell cols="2">Tiny ImageNet conv fc</cell><cell>conv</cell><cell cols="2">STL-10</cell><cell>fc</cell><cell cols="2">CIFAR10 conv</cell><cell>fc</cell><cell>CIFAR100 conv</cell><cell>fc</cell></row><row><cell>Fully supervised</cell><cell>36.60</cell><cell></cell><cell></cell><cell>68.70</cell><cell></cell><cell></cell><cell>75.39</cell><cell></cell><cell>42.27</cell></row><row><cell>BiGAN</cell><cell>24.38</cell><cell>20.21</cell><cell cols="2">71.53</cell><cell cols="2">67.18</cell><cell>62.57</cell><cell cols="2">62.74</cell><cell>37.59</cell><cell>33.34</cell></row><row><cell>NAT</cell><cell>13.70</cell><cell>11.62</cell><cell cols="2">64.32</cell><cell cols="2">61.43</cell><cell>56.19</cell><cell cols="2">51.29</cell><cell>29.18</cell><cell>24.57</cell></row><row><cell>DIM</cell><cell>33.54</cell><cell>36.88</cell><cell cols="2">72.86</cell><cell cols="2">70.85</cell><cell>73.25</cell><cell cols="2">73.62</cell><cell>48.13</cell><cell>45.92</cell></row><row><cell>SplitBrain  ?</cell><cell>32.95</cell><cell>33.24</cell><cell cols="2">71.55</cell><cell cols="2">63.05</cell><cell>77.56</cell><cell cols="2">76.80</cell><cell>51.74</cell><cell>47.02</cell></row><row><cell>SwAV</cell><cell cols="9">39.56 ? 0.2 38.87 ? 0.3 70.32 ? 0.4 71.40 ? 0.3 68.32 ? 0.2 65.20 ? 0.3 44.37 ? 0.3 40.85 ? 0.3</cell></row><row><cell>SimCLR</cell><cell cols="9">36.24 ? 0.2 39.83 ? 0.1 75.57 ? 0.3 77.15 ? 0.3 80.58 ? 0.2 80.07 ? 0.2 50.03 ? 0.2 49.82 ? 0.3</cell></row><row><cell>CMC  ?</cell><cell cols="2">41.58 ? 0.1 40.11 ? 0.2</cell><cell cols="2">83.03</cell><cell cols="2">85.06</cell><cell cols="3">81.31 ? 0.2 83.28 ? 0.2 58.13 ? 0.2 56.72 ? 0.3</cell></row><row><cell>MoCo</cell><cell>35.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>? 44.51 ? 0.2 45.36 ? 0.2 85.41 ? 0.3 85.62 ? 0.2 87.87 ? 0.2 88.12 ? 0.2 59.97 ? 0.3 61.06 ? 0.2 which is derived by setting</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance (accuracy) on the CIFAR10 and STL-10 datasets with ResNet-50<ref type="bibr" target="#b23">[24]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="3">CIFAR10 STL-10 Average</cell></row><row><cell>SwAV</cell><cell>83.15</cell><cell>82.93</cell><cell>83.04</cell></row><row><cell>SimCLR</cell><cell>84.63</cell><cell>83.75</cell><cell>84.19</cell></row><row><cell>CMC</cell><cell>86.10</cell><cell>86.83</cell><cell>86.47</cell></row><row><cell>BYOL</cell><cell>87.14</cell><cell>87.56</cell><cell>87.35</cell></row><row><cell>Barlow Twins</cell><cell>85.84</cell><cell>86.02</cell><cell>85.93</cell></row><row><cell>DACL</cell><cell>86.93</cell><cell>88.11</cell><cell>87.52</cell></row><row><cell>LooC</cell><cell>87.80</cell><cell>88.62</cell><cell>88.21</cell></row><row><cell>SwAV + Hard</cell><cell>83.99</cell><cell>84.51</cell><cell>84.25</cell></row><row><cell>SimCLR + Hard</cell><cell>86.91</cell><cell>85.48</cell><cell>86.20</cell></row><row><cell>CMC + Hard</cell><cell>88.25</cell><cell>87.79</cell><cell>88.02</cell></row><row><cell>MetAug (only OUCL)</cell><cell>88.79</cell><cell>88.31</cell><cell>88.55</cell></row><row><cell>MetAug</cell><cell>91.09</cell><cell>90.26</cell><cell>90.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Linear evaluation results on ImageNet. We follow the setting of<ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref> to compare with other benchmark SSL methods with conv and ResNet-50.</figDesc><table><row><cell></cell><cell>ImageNet</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>conv top 1</cell><cell cols="2">ResNet-50 top 5</cell></row><row><cell>Fully supervised</cell><cell>50.5</cell><cell>-</cell><cell>-</cell></row><row><cell>SplitBrain</cell><cell>32.8</cell><cell>-</cell><cell>-</cell></row><row><cell>CPC v2</cell><cell>-</cell><cell>63.8</cell><cell>85.3</cell></row><row><cell>SwAV</cell><cell>38.0 ? 0.3</cell><cell>71.8</cell><cell>-</cell></row><row><cell>SimCLR</cell><cell>37.7 ? 0.2</cell><cell>71.7</cell><cell>-</cell></row><row><cell>CMC</cell><cell>42.6</cell><cell>66.2</cell><cell>87.0</cell></row><row><cell>MoCo</cell><cell>39.4 ? 0.2</cell><cell>71.1</cell><cell>-</cell></row><row><cell>SimSiam</cell><cell>-</cell><cell>71.3</cell><cell>-</cell></row><row><cell>InfoMin Aug.</cell><cell>-</cell><cell>73.0</cell><cell>91.1</cell></row><row><cell>BYOL</cell><cell>41.1 ? 0.2</cell><cell>74.3</cell><cell>91.6</cell></row><row><cell>Barlow Twins</cell><cell>39.6 ? 0.2</cell><cell>-</cell><cell>-</cell></row><row><cell>NNCLR</cell><cell>-</cell><cell>75.4</cell><cell>92.3</cell></row><row><cell>DACL</cell><cell>41.8 ? 0.2</cell><cell>-</cell><cell>-</cell></row><row><cell>LooC</cell><cell>43.2 ? 0.2</cell><cell>-</cell><cell>-</cell></row><row><cell>SimCLR + Debiased</cell><cell>38.9 ? 0.3</cell><cell>-</cell><cell>-</cell></row><row><cell>SimCLR + Hard</cell><cell>41.5 ? 0.2</cell><cell>-</cell><cell>-</cell></row><row><cell>MetAug + CMC</cell><cell>45.1 ? 0.2</cell><cell>-</cell><cell>-</cell></row><row><cell>MetAug + NNCLR</cell><cell>-</cell><cell>76.0</cell><cell>93.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of applying benchmark SSL methods with different data augmentations by using the fc backbone on CIFAR10.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Data augmentations</cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell></cell></row><row><cell>ID</cell><cell>horizontal flip</cell><cell>rotate</cell><cell>random crop</cell><cell>random grey</cell><cell>color jitter</cell><cell>mixup</cell><cell>DACL</cell><cell>LooC</cell><cell>MetAug</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>80.73</cell><cell>87.05</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>81.16</cell><cell>87.53</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>80.70</cell><cell>86.81</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>81.64</cell><cell>87.79</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>82.05</cell><cell>88.12</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>82.16</cell><cell>88.01</cell></row><row><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80.87</cell><cell>82.21</cell><cell>88.22</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82.09</cell><cell>83.17</cell><cell>88.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Performance (accuracy) of MetAug with or without the augmented features on CIFAR10 with conv encoder.</figDesc><table><row><cell>Model</cell><cell>?</cell><cell cols="2">w/ augmented w/o augmented features features</cell></row><row><cell>SimCLR</cell><cell>-</cell><cell>80.58</cell><cell></cell></row><row><cell>DACL</cell><cell>-</cell><cell>81.92</cell><cell></cell></row><row><cell>LooC</cell><cell>-</cell><cell>83.79</cell><cell></cell></row><row><cell>CMC + Hard</cell><cell>-</cell><cell>83.04</cell><cell></cell></row><row><cell></cell><cell>10 -1</cell><cell>85.85</cell><cell>85.48</cell></row><row><cell></cell><cell>10 -2</cell><cell>85.91</cell><cell>85.99</cell></row><row><cell></cell><cell>10 -3</cell><cell>86.57</cell><cell>86.65</cell></row><row><cell>MetAug</cell><cell>10 -4 10 -5</cell><cell>87.42 87.72</cell><cell>87.41 87.87</cell></row><row><cell></cell><cell>10 -6</cell><cell>87.26</cell><cell>87.47</cell></row><row><cell></cell><cell>10 -7</cell><cell>86.90</cell><cell>87.19</cell></row><row><cell></cell><cell>10 -8</cell><cell>86.12</cell><cell>86.35</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the optimization of a synaptic learning rule</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Succ A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Cloutier</surname></persName>
		</author>
		<author>
			<persName><surname>Gecsei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Philip Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><surname>Buchwalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cloutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ijcnn-91-seattle International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05310</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to learn without gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N De</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<title level="m">Debiased contrastive learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of learning from positive and unlabeled data</title>
		<author>
			<persName><forename type="first">Marthinus</forename><surname>Christoffel Du Plessis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Las Vegas, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">August 24-27, 2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Whitening for self-supervised representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ermolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017. 2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An efficient image similarity measure based on approximations of kldivergence between two gaussian mixtures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Azar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oj H?naff</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sma</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avd</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Decoupled neural interfaces using synthetic gradients</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>of IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Self-supervised generalisation with meta auxiliary learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Visual question answering with memory-augmented networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avd</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning complex, extended sequences using the principle of history compression</title>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An information theoretic framework for multi-view learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Selfsupervised learning from a multi-view perspective</title>
		<author>
			<persName><forename type="first">Yhh</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards domain-agnostic contrastive learning</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
		</imprint>
	</monogr>
	<note>Virtual Event, Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">What should not be contrastive in contrastive learning</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A survey on green deep learning</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Deny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Splitbrain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fine-grained visual categorization using meta-learning optimization with sample selection of auxiliary data</title>
		<author>
			<persName><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 8-14, 2018. 2018</date>
			<biblScope unit="page" from="241" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
