<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure and Motion from Line Segments in Multiple Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Camillo</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">U.C. Berkeley</orgName>
								<address>
									<postCode>94720-1770</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Systems Science</orgName>
								<orgName type="department" key="dep2">Department of Elec-trical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<postCode>06520-8267</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
							<email>kriegman@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">U.C. Berkeley</orgName>
								<address>
									<postCode>94720-1770</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure and Motion from Line Segments in Multiple Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F716AB85261D27282EF23740AF7AA77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Structure from motion</term>
					<term>straight lines</term>
					<term>threedimensional reconstruction</term>
					<term>perspective projection</term>
					<term>numerical minimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new method for recovering the three dimensional structure of a scene composed of straight line segments using the image data obtained from a moving camera. The recovery algorithm is formulated in term of an objective function which measures the total squared distance in the image plane between the observed edge segments and the projections (perspective) of the reconstructed lines. This objective function is minimized with respect to the line parameters and the camera positions to obtain an estimate for the structure of the scene. The effectiveness of this approach is demonstrated quantitatively through extensive simulations and qualitatively with actual image sequences. The implementation is being made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ANY applications such as vehicle navigation, robot mo-M tion planning, object recognition and automatic acquisition of CAD models and architectural drawings involve recovering a representation for the geometric structure of the world from sensor data. This paper presents a new method for recovering the three dimensional structure of a scene composed of straight line segments using the image data obtained from a moving camera.</p><p>Since first being considered by <ref type="bibr">Ullman [l]</ref>, the structure from motion (SFM) problem has received quite a bit of attention, and a number of algorithms have emerged. These algorithms differ in the type of input (e.g., dense gray scale images, feature points, lines, etc.), the number of required images (e.g., two, three, unlimited), the camera model (e.g., perspective or orthographic projection), and the assumed knowledge of the camera motion (e.g., completely known, constant velocity, unknown velocity, etc.).</p><p>Several researchers have looked at the problem of recovering the structure of a scene composed of point features from a set of images. Longuet-Higgins [2], and others [3], [4], <ref type="bibr">[5]</ref>, <ref type="bibr">[6]</ref> have shown that the structure of a set of points can be recovered from two images even when the relative position of the two cameras is unknown. Other researchers have developed Manuscript received Jan. <ref type="bibr">25, 1994;</ref><ref type="bibr">revised Mar. 8, 1995.</ref> Recommended algorithms that use multiple images of a rigid scene to produce a more accurate reconstruction, and these fall into two classes: batch algorithms, which process all of the data simultaneously, and incremental algorithms, which maintain some notion of state that is updated with each new sensor measurement. Several incremental structure from motion algorithms based on the Extended Kalman Filter (EKF) have been proposed in the literature <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>, <ref type="bibr">[lo]</ref>.</p><p>Most batch algorithms formulate the SFM problem in terms of a nonlinear objective function that is then minimized to obtain an estimate for the structure of the environment. Since these algorithms avoid the linearizing assumptions inherent in the EKF they usually yield more accurate reconstruction results <ref type="bibr">[6]</ref>, <ref type="bibr">[4]</ref>. Tomasi and Kanade [ 111 demonstrated a batch algorithm that can be used to reconstruct a set of points viewed under orthographic projection from a sequence of camera positions. By modeling orthographic projection as a linear map in baricentric coordinates, they showed how singular value decomposition could be used to minimize an objective function which measures the disparity between the projections of the reconstructed points and the measured point features in the images. <ref type="bibr">Szeliski and Kang [ 121 and Hu and Ahuja [4]</ref> have also obtained good results by applying nonlinear minimization algorithms to point based SFM problems.</p><p>A few researchers have considered the problem of reconstructing scenes composed of straight line segments. Straight line features are prominent in most man-made environments, they can be detected and tracked relatively easily in image data, and they provide a great deal of information about the structure of the scene. Additionally, since edge features have more image support than point features, they can be localized more accurately. These properties make them ideal candidates for use in structure from motion algorithms.</p><p>Several approaches based on the Extended Kalman Filter (EKF) have been proposed. <ref type="bibr">Faugeras et al. [8]</ref> presented a method that solves for the motion of a camera and the structure of the scene from three perspective images. They applied an Extended Kalman Filter to minimize an objective function which represented a version of the epipolar constraint. <ref type="bibr">Crowley et al. [7]</ref> describe a method for reconstructing a rigid scene in which an EKF was employed to update the estimate for the structural parameters using the measurements obtained for acceptance by S. Peleg. Systems Science at Yale University. from a moving camera. In this work, the absolute position of the moving camera is known accurately. Both Jezouin and Ayache <ref type="bibr">[9]</ref> and Vieville and Faugeras [ 101 have developed Vieville [ 131 and Giai-Checa and Vieville [ 141 have investigated the feasibility of using image flow information to recover the geometry of a scene composed of straight line features. They relate the velocity of the edge features in the image to the line parameters and the camera motion; algorithms are presented for reconstructing a rigid scene using the information obtained from a tracking system. Since this problem is underconstrained additional assumptions are required in order to obtain a solution. Vieville assumes that the camera is moving with constant rotational and translational velocity while Giai-Checa et al. explicitly search for junctions in the image and then use these intersections to induce extra constraints on the solution. <ref type="bibr">Navab et al.</ref> [ 151 have developed a novel method for recovering the 3D position and velocity of the linear features from the image velocities and edge measurements obtained from a calibrated stereo pair.</p><p>Another approach to th~s structure from motion problem was inspired by the linear algorithm developed by Longuet-Higgins for point features <ref type="bibr">[2]</ref>. Given a set of at least 13 linear features viewed in three frames, it is possible to derive a set of equations which represent an analog to the epipolar constraint that Longuet-Higgins exploited in his work. <ref type="bibr">Liu et al. [16]</ref>, Spetsalas and Aloimonos <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and <ref type="bibr">Faugeras [19]</ref> have all developed algorithms that recover the structure of the scene and the positions of the cameras based on these equations. One obvious advantage of these linear algorithms is that they provide a closed form solution. In practice, however, these methods tend to be very sensitive to errors in the image measurements. Section V presents a series of simulation experiments that compare the algorithm presented in this paper to the three frame linear algorithm proposed in <ref type="bibr">[16]</ref>. These experiments demonstrate that our algorithm is much more robust to image noise than the linear methods and can be expected to provide much more accurate results.</p><p>In this paper, the reconstruction problem is formulated in terms of an objective function which measures the total squared distance in the image plane between the observed edge segments and the projections (perspective) of the reconstructed lines. This objective function is minimized with respect to the line parameters and camera positions in order to obtain an estimate for the structure of the scene. A minimum of six edge correspondences in three images is required to solve this structure from motion problem [8], but the algorithm can take advantage of any number of additional images or straight line features that may be available. To the best of our knowledge, this algorithm produces the most accurate results ever achieved on this type of structure from motion problem. In the special case where the line segments are vertical and the camera motion is confined to the horizontal plane, the presented algorithm degenerates to the least squares approach to recovering planar point locations and motion presented in <ref type="bibr" target="#b18">[20]</ref>. <ref type="bibr">Weng et al. [6]</ref> also describe an approach to this problem based on minimizing a non-linear objective function. The objective function that they propose, however, measures the Mahalanobis distance between the parameterization of the observed line and that of the predicted line. This objective function differs from the one advocated in this paper since it does not directly measure the retinal disparity in the image.</p><p>It has often been argued that a point based SFM algorithm could be used to directly recover the 3D coordinates of the endpoints of the line segments. Unfortunately, in practice it is very difficult to precisely locate the endpoints of an image edge for a number of reasons. Edge detection algorithms such as Canny's <ref type="bibr" target="#b19">[21]</ref> are not designed to find junctions, and they often fail to accurately locate an isolated endpoint of a line. Various parts of the edge including the endpoints may be occluded in the images. The algorithm proposed in this paper avoids all of these problems by reconstructing the infinite straight line that supports the observed edge segments rather than the endpoints of the line. Consequently, the algorithm can be used even when multiple edges in a single image correspond to different portions of the same 3D line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">PROBLEM FORMULATION</head><p>In this paper, the structure from motion problem is formulated in terms of an objective function 0 that measures the disparity between the actual image measurements and the image measurements that would be predicted from the recovered 3D structure and camera locations. This section describes how the objective function is constructed.</p><p>The image formation process can be modeled as a function 7</p><p>which takes the position and orientation of a camera q and the position of the three-dimensional line p and returns a representation for the corresponding edge in the image. Consider the case where there are m images of a static scene containing n straight line features, let uii be the measurement of the projection of feature i in imagej. Let Error ( ) be a positive real valued function that measures the disparity between the observed image feature uij and the expected image feature 7(pi, qj). The objective function 0 can now be defined as follows:</p><formula xml:id="formula_0">m n o = C E r r o r ( F ( p i , si), uij) j = l j=l</formula><p>The aim of the structure from motion algorithm is to find a choice of parameters, pi and qi, that is most consistent with the image measurements by minimizing the objective function 0 with respect to these parameters using the techniques described in Section 111. In the remainder of this section, the elements of (1) are discussed in more detail. The camera positions Q, are represented in the usual manner; by a translation vector, tj E R3, and a rotation matrix, Rj E SO(3). These parameters represent the position and orientation of the camera with respect to some base frame of reference. In the sequel we will arbitrarily define the base frame of reference to be the first camera position in the sequence. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the line and the camera center define a plane whose normal is denoted by the vector m. Notice that under perspective projection the image of the line corresponds to the intersection of this plane and the image plane. Assuming unit focal length, the edge in the image will be defined by the equation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Geometry of Straight Lines</head><p>(</p><p>where m = (mx, my, m,) .</p><p>A measured edge segment in an image uU is represented by its endpoints, (XI, y , ) and (xz, y2). As we noted in the introduction, the endpoints of these edges do not necessarily correspond to the endpoints of the three-dimensional line segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Projection Function, 7</head><p>Fig. <ref type="figure">2</ref> shows a single infinite straight line viewed from two different positions. The rigid transformation between the base reference frame and the camera frame is defined by a translation vector "t, and a rotation matrix $R.3 Given these definitions, we can write the following equations that relate the vectors representing the line in the camera frame (' ?, 'm, 'd) to the vectors representing the same line in the base frame ("?, "m, "d). Points on the observed edge segment can be parameterized by a single scalar variable s E [0, I] where 1 is the length of the edge, 1 = J(xl -x 2 ) 2 + ( y l -y 2 ) 2 . Let h(s) be a function which measures the shortest distance from a point on the segment, p(s), to the predicted edge as shown in Fig. <ref type="figure" target="#fig_1">3</ref>.</p><formula xml:id="formula_2">h2 -4 h(s) = hl+ s - 1 (4)</formula><p>where the scalar values hl and h2 in (4) represent the shortest distances from the endpoints of the edge segment to the predicted edge, and are given by:</p><formula xml:id="formula_3">myy, + m, mxx2 + myy2 + m, hi = Jq Jm ( 5 )</formula><p>h2=</p><p>With these definitions in place, we can define the total error between the observed edge segment and the predicted edge as:</p><formula xml:id="formula_4">1 1 Error = Joh2(s)ds = -(b2 + h l h + g ) 3 (6) \ I</formula><p>= mT(ATBA)m</p><p>where: A = ("' y1 i), B = A( 1 0.5 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x2 Y2</head><p>3(m:+m:) 0.5 1 Although the projection of the recovered line onto the image plane has infinite extent, only the portion nearest to the observed image edge contributes to the error measure. This can be contrasted with other approaches where the observed edge is treated as though it were infinite [8].</p><p>It should also be noted that in this formulation of the error function, the contributions from various edges are explicitly weighted by their lengths, 1. This is a desirable property since longer edges can be localized more accurately in the image than shorter ones. If a line does not appear in a particular image, this error term is set to zero. Note that in order to reconstruct a particular straight line, it must appear in at least two images in the sequence.</p><p>If each edge was viewed as a collection of edgels (xi, yi) rather than as a continuous line segment, we could measure the disparity between the expected edge and the observed edgels by summing the squared distances between these points and our reprojected line, m = (mx, my, mz). The resulting error function could be expressed as follows: Error = mTAm/(m: + m:)</p><p>where A = ci(xj, yj, l ) ( x j , yi, l)T. This error function has exactly the same form as the one given in (6) so the same minimization algorithm could be used to recover the unknown parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">RECOVERY ALGORITHM</head><p>To obtain an estimate for the structure of the scene and the motion of the camera, the objective function 0, described in the previous section, must be minimized. Like most non-linear cost functions 0 can exhibit multiple local minima. We propose to use a hybrid optimization technique that generates a series of random initial estimates for the unknown parameters; these estimates are then used as starting points for a gradient descent minimization procedure that locates local minima of the cost function 0. In this implementation, the global minimization algorithm only needs to generate initial estimates for the camera orientations, Rj, since initial estimates for the other parameters (ti, i i , di)are obtained as part of the local minimization algorithm described in Section 1II.B. The effectiveness of this global minimization strategy will be demonstrated experimentally in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Global Minimization</head><p>The basic idea behind the global minimization strategy is to randomly generate initial estimates for the unknown parameters and then apply a standard gradient decent minimization procedure to locate a local minimum of the cost function. If this local minimum satisfies certain conditions it will be returned as the final answer, otherwise, the algorithm tries again with a new set of random initial estimates. This is the same technique that Horn employed to recover the relative orientation of two cameras from point correspondences [5].</p><p>The first stage of the global minimization process involves generating a set of random initial estimates for the camera orientations, Ri. The reconstruction algorithm should be provided with an indication for the orientation of each camera in the form of a rotation interval (Ro, e) : Ro E S0(3), e, , E R. Ro represents an estimate of the camera orientation while e, , represents the maximum amount of angular error in the estimate in radians. On a mobile robot, for example, initial estimates for the camera orientations could be obtained from odometry while the parameter e, , would reflect the uncertainty in these measurements. The rotation interval actually denotes a closed set of rotation matrices defined as follows: {RlR = R, exp(J(w)}, Jw'w I Om,} where J(w) denotes the skew symmetric matrix associated with the vector w, and exp is the matrix exponential operator. Note that if initial estimates for the camera orientation are unavailable, then Ro is simply the identity matrix and e, , is set to n.</p><p>Initial estimates for the camera orientations are generated by choosing random elements from the rotation intervals associated with each camera position. In each case a random element is selected from the closed ball {WIG I e,,,}, and the corresponding rotation matrix is taken as an initial estimate. These initial estimate are used as a starting point for the gradient descent minimization procedure described in Section 1II.B which converges to a local minimum of the objective function.</p><p>At the end of the local minimization step, the disparity between the predicted edges and the observed edges is compared to some preset threshold. If it is below, the minimum is accepted as a feasible estimate; otherwise, a new set of initial estimates for the camera orientations is generated, and the local minimization process is restarted. Since 0 is based on image error, the value of this threshold is determined by considering the maximum amount of error expected in the image measurements.</p><p>The number of local minimizations performed before a feasible estimate is obtained will depend upon the shape of the error surface and on the size of the rotation intervals provided to the algorithm. A set of simulation experiments was carried out to evaluate the convergence of this global minimization technique, and the results are presented in Section IV.A.6. In practice, on the configurations that were used in the simulations and in the real data experiments, a feasible answer was usually obtained on the first try, although some of the experiments took as many as twenty or thirty passes before the global minimum was found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Minimization</head><p>This subsection describes the procedure used to obtain a local minimum of the objective function 0 from a set of initial estimates of the camera orientations, Rj. The reader will notice that the objective function given in ( <ref type="formula">1</ref>) is actually expressed in terms of four different types of parameters; camera orienta-tions, Rj, camera translations, ti, line directions, ii, and line positions, di. Before carrying out the minimization of 0, initial estimates (tjy vi, di) are generated from the image measurements uij and the random initial estimate of Rj provided by the global optimization procedure.</p><p>The local minimization process is decomposed into four stages which are summarized in Table <ref type="table" target="#tab_0">I</ref> and now described in detail. A multistage method was employed in order to reduce the overall time taken by the algorithm. The simpler initial stages were designed to provide good initial estimates for the final optimization stage which minimizes the non-linear objective function with respect to all the unknown parameters simultaneously. Each iteration at the final stage is relatively expensive from a computational standpoint. Computing these initial estimates helps to reduce the number of iterations required at this stage which reduces the overall compute time.  and in stage D the objective function 0 is minimized with respect to all of the free parameters to produce the final estimates for the structure of the scene and the positions of the cameras.</p><p>We begin our discussion with a careful examination of (3) which determines how a particular line projects onto the image plane at a particular camera location.</p><formula xml:id="formula_5">'m = "i x Cd = ~R { W ~~( w d -" t , ) } (7)</formula><p>From this equation the following constraints can be deduced.  The global minimum of C, with respect to the ii parameters can actually be determined by minimizing each CA, term independently since each of these terms involves a different t i vector. Each CA, term is a simple homogeneous quadratic form which can be readily minimized using standard techniques like singular value decomposition.</p><p>In stage B the nonlinear objective function Cl is minimized with respect to all of the free parameters, Rj and i i , to yield better estimates for both the camera orientations and the line directions. Once estimates for Ri and ii have been obtained, the constraint described in (9) can be used to obtain initial estimates for the di and tj parameters. The following objective function reflects how well our estimates obey this constraint:</p><formula xml:id="formula_6">2 m n C2= c(miTRj(di -t j ) ) j=1 i=l</formula><p>From each line direction, ii, two unit vectors, if, and $: , can be constructed such that ;.if = 0, v.vr = 0 , and if.;: = 0. Since vi is orthogonal to d;, the line position vector di can be expressed in terms of these two vectors as di = a?; +pi:. This allows us to rewrite the objective function given in (13) as follows:</p><formula xml:id="formula_7">A A m n</formula><p>Notice that this cost function is actually a simple quadratic form with respect to the parameters a, p, fx, f,,, and fz. This means that the standard, closed form linear least squares equations can be applied to obtain initial estimates for these parameters.</p><p>Other researchers have used the constraints described in equations (8) and ( <ref type="formula">9</ref>) to recover the position of an observer with respect to a known constellation of straight line features from image data. <ref type="bibr">Liu,</ref><ref type="bibr">Huang,</ref><ref type="bibr">and Faugeras [24]</ref> presented an algorithm that solves for the camera orientation first and then the camera translation. <ref type="bibr">Kumar and Hanson [25]</ref> proposed a related technique that solves for the rotational and translational parameters simultaneously. In this case, these constraints are being used to estimate both the camera positions and the structure of the scene.</p><p>Once we have initial estimates for Rj, Gi, di ,and tj, the main using the technique described in <ref type="bibr" target="#b24">[26]</ref> to obtain the final estimate for the structure of the scene and the positions of the camera. This minimization involves a total of 4n + 6(m -1) -1 independent parameters. The optimization method used at this stage is very similar to the approach advanced by Steven Smith in his dissertation <ref type="bibr" target="#b25">[27]</ref>. Smith showed how to carry out a version of the Newton minimization algorithm on Riemannian manifolds and proved that this method shared the quadratic convergence properties of its Euclidean counterpart. The final results produced after this stage are generally twice as accurate as the estimates provided by stage C.</p><p>In an earlier version of this work <ref type="bibr" target="#b18">[20]</ref> the unknown parameters were divided into two sets: the structural parameters Gi, di, and the camera position parameters Rj, tj. On every iteration of the optimization algorithm, the objective function was minimized with respect to each set of parameters independently as described in <ref type="bibr" target="#b26">[28]</ref> in order to reduce the computational complexity of the overall procedure. More recently, <ref type="bibr">Szeliski and Kang [12]</ref> have published work which indicates that a more direct approach to the optimization problem can actually yield a performance improvement. The argument against minimizing the objective function with respect to all the parameters simultaneously using a variant of Newton's method is that it would involve inverting a very large Hessian matrix at every iteration. Szeliski and Kang observed that this Hessian matrix was actually sparse which makes the problem much simpler. They also observed that the direct methods required very few iterations to converge to the final minima so the overall time taken to produce a result was quite reasonable.</p><p>It is a well known fact that SFM algorithms can only reour implementation, the scale factor is set by holding one of the nonzero translation parameters constant during the last two stages.</p><p>Stage D has a much greater computational complexity than stages A or B since it involves a larger number of parameters.</p><p>One technique for reducing the overall time required by the algorithm involves storing the initial estimates for the camera orientation provided to stage D by stage B in a database. On Thousands of simulation trials were carried out to determine rameters of the simulation were varied. These parameters included the amount of error in the image measurements, the number of camera positions, the number of straight lines in the scene, calibration errors, and the effective baseline. The algorithm was also applied to a number of data sets obtained from actual image sequences. These experiments qualitatively demonstrate that the method can be used to successfully reconstruct a variety of scenes. Section V presents the results from a series of simulation experiments that compare the proposed algorithm to the three frame linear method presented in [ 161.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Experiments</head><p>In all of the simulation experiments a similar arrangement of camera positions and straight line features was used. The cameras were arranged in a circular stereo configuration as shown in Fig. <ref type="figure" target="#fig_6">4</ref>. The first camera position is located at the origin of the coordinate system and the other (m -1) positions are equally spaced along the circumference of a circle of radius 250 mm. centered at the origin. This arrangement of camera positions ensures that the maximum stereo baseline between any two positions remains the same regardless of the number of camera positions used in the simulation.</p><p>The reference object is composed of a set of randomly rotated wire frame cubes 200 mm. on a side placed 1 meter away from the origin along the z-axis as shown in Fig. <ref type="figure" target="#fig_6">4</ref>. Experiments were also carried out on configurations of random line segments, but we found that it was much easier to visually interpret the results of the reconstruction when polygonal structures were used. objective function 0 given in (1) can be minimized directly how the accuracy of the algorithm changed as different pacover the structure of the environment up to a scale factor. In  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before describing the for</head><p>Of these experiments, we need the accuracy Of the recov-to have Some ery process. Below, we define three metrics that characterize the accuracy of the reconstructed lines and camera locations and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . J . Measuring cumera ~~~~~ Error</head><p>before invoking the final minimization stage.</p><p>Once the infinite straight lines have been recovered, the endpoints of the lines can be reconstructed by projecting the endpoints in the images back onto the infinite straight lines. Note that these reconstructed endpoints need not correspond to the physical endpoints of the line due to occlusions or edge detector failures, and so we make no claims about the accuracy of the endpoints.</p><p>Any mation matrix R E so(3) can be written in the form R = exp{QJ(G)} where [O, 4, GTG =l. The magnitude of a rotation can be defined as llRll= ((exp{@J(G)j(( = 8. Given two rotation matrices R I and R2 we can define a metric function ( R I , R2), as ( R I , R2) = llR~R211. It is relatively straightforward to show that this function is actually a metric on the Lie carried out on both real and synthetic image data in order to Group SO(3). This function effectively measures the evaluate the effectiveness of the proposed algorithm.</p><p>"distance" between two rotation matrices in radians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>This section describes a series of experiments that were</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Measuring Structural Error</head><p>In order to evaluate the accuracy of the reconstructed lines, we need to define an error measure that reflects the difference between the recovered scene structure and the actual scene. This can be accomplished by calculating the mean squared distance between each of the recovered infinite 3D lines and the actual 3D line segments. For every point on the line segment, the square of the distance to the closest point on the infinite straight line can be determined in closed form. This value can be integrated along the extent of the segment to obtain the total squared distance between the segment and the line. This integral is then divided by the length of the segment to yield the final result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Experiment I : Image Noise</head><p>The first set of simulation experiments was designed to determine how the accuracy of the reconstruction would vary as the amount of error in the image measurements was increased. A uniformly distributed random image error was added to the endpoints of the image segments. In these simulations the virtual camera had a focal length of 8 mm with a 30 degree field of view and the pixels on the image plane were assumed to be 8.37e-3 mm. on side. These parameters closely model the camera system that was used for the real data experiments. Fig. <ref type="figure" target="#fig_7">5</ref> shows how the accuracy of the reconstruction varied as a function of the measurement error. Each point on these graphs represents the average of 50 trials, each of these trials involved six camera positions and 36 straight line features. As expected, the reconstruction error increased as the random image errors were increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random error in edge endpoints in pixels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Experiment 2: Calibration Errors in Camera Center</head><p>As is well known, it is very difficult to accurately and reliably determine the center of projection of a camera through calibration. This experiment was designed to explore the impact of this type of calibration error on the SFM algorithm by adding a randomly chosen camera center bias to all of the edges in the sequence. The simulation parameters were the same as in Experiment 1, however no random error was added to the endpoints.</p><p>The results indicated that the algorithm is much more sensitive to random errors in the edge endpoints than it is to camera center bias, even with a camera center bias of 25 pixels the average error in the recovered camera orientations was only 0.0045 rads. while the error in the reconstructed lines was only 0.1 sq. mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Experiment 3: Number of Images and Number of Lines</head><p>The third set of simulation experiments was designed to determine how the accuracy of the reconstruction changed as a function of the number of camera positions m and the number of straight line features n. The random image errors were on the order of 0.5 pixels while the camera center errors were on the order of 5.0 pixels. For each combination of n and m, 100 random scenes were generated. The graphs in Fig. <ref type="figure" target="#fig_8">6</ref> clearly demonstrate that the accuracy of the method improves as the number of camera positions and the number of features is increased.</p><p>Note that since these experiments were carried out with the cameras arranged in a circular stereo configuration, increasing the number of camera positions does not increase the effective stereo baseline of the image sequence. Nonetheless, this experiment shows that the additional constraints obtained from the extra images can be used to improve the accuracy of the reconstruction. Similarly, each additional line in the scene contributes a set of image measurements which help to constrain the structure of the scene.</p><p>These graphs also show a pattern of diminishing returns, that is increasing the number of images used beyond 6 or the number of straight line features beyond 50 does not improve the accuracy of the method significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Experiment 4: Global Convergence</head><p>This experiment was designed to investigate the efficiency of the global minimization procedure outlined in Section 1II.A. In these experiments the accuracy of the initial estimates for the camera orientations provided to the algorithm, e, , was varied between 20 degrees and 180 degrees. Each experiment involved 36 line segments and 4 camera positions. The random image errors were on the order of 0.5 pixels while the camera center errors were on the order of 5.0 pixels. For each value of e, , 50 experiments were run and the number of global iterations performed before a feasible minimum was obtained was I recorded (see Table <ref type="table">11</ref>). Note that the term global iterations refers to the number of times that the algorithm generates a new set of random initial estimates for the unknown parameters and not the number of iterations required to converge from a random initial estimate to a local minima.  the method changes with the radius of the circular stereo configuration and the number of camera positions. The experiment was designed to determine whether the accuracy of the method was more sensitive to the number of images used or the stereo baseline of the system. Fig. <ref type="figure" target="#fig_10">7</ref> clearly shows that the accuracy of the method improves as the effective stereo baseline of the camera arrangement increases. The results also suggest that a small number of widely spaced camera positions can actually provide better structural estimates than a large number of closely spaced positions. Each point on the graph represents the average results obtained over 100 experiments. The random image errors in these experiments were on the order of 0.5 pixels while the camera center errors were on the order of 5.0 pixels, each experiment involved 36 line segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I1 HOW THE NUMBER OF GLOBAL ITERATIONS REQUIRED FOR CONVERGENCE VARIES</head><p>Radius dcirrvlnr s t m o system in mm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real Data Experiments</head><p>Experiments were carried out on a variety of image sequences taken in and around our laboratory. Images were digitized to 464 x 572 pixels using a CCD camera with an 8 mm lens. In each of these experiments the image edges were obtained using a variation of the Canny edge detector <ref type="bibr" target="#b19">[21]</ref>. The line correspondences were determined manually, and initial estimates for the camera orientations were obtained by taking eyeball estimates. The intrinsic parameters (aspect ratio, camera center and quadratic radial distortion) of the camera system were obtained from a set of calibration images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.I. Scene I : Building Blocks</head><p>Fig. <ref type="figure" target="#fig_12">8</ref> shows two of the eight images taken of a stack of building blocks, and Fig. <ref type="figure" target="#fig_13">9</ref> shows various views of the reconstruction of that scene. These scenes are rendered assuming perspective projection with the same focal length as the actual camera. As the amount of error in the original estimates increases, the median number of iterations required to locate a feasible global minimum rises since the algorithm is forced to investigate a larger portion of the parameter space. Even when the algorithm has no information about the camera orientations (Omox = 180 degrees), it still manages to locate the global minimum when given enough trials.  In this experiment, the camera was moved around the object in order to obtain views of different sides of the cube; note that opposite sides of the cube (A and E) cannot be seen simultaneously. The algorithm was able to take advantage of all available image data to reconstruct four of the six faces of the cube. Qualitatively, the lines appear to be well reconstructed. The figures show that the relationships between lines in the reconstruction reflect the relationships between the corresponding lines in the actual structure; parallel, perpendicular or coplanar lines on the actual object appear in the same configuration in the reconstruction. Notice that the algorithm could not provide estimates for the positions of the line segments on the far side of the structure since none of these lines were visible in any of the images. Note that we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Scene 3: Inside a building</head><p>Fig. <ref type="figure" target="#fig_0">12</ref> shows four of the twenty four images taken from an area inside our office complex. A large number of views were taken to cover a large area of the scene; in contrast to the two previous examples where the objects being viewed were relatively small, here the robot is immersed within the scene being reconstructed. Several panoramic views of the scene  were used in this experiment along with more focused views (Figs. <ref type="figure" target="#fig_0">12c-12d</ref>) which captured various details. make no claims about the locations of the endpoints since these are simply taken to be the extrema of the projections of the measured image endpoints onto the reconstructed lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Scene 2: The Eli Cube</head><p>Fig. <ref type="figure" target="#fig_0">10</ref> shows two of the ten images taken of a marked box, a.</p><p>b.</p><p>., while Fig. <ref type="figure" target="#fig_0">11</ref> shows various views of the reconstruction of that scene including the location of the cameras. Fig. <ref type="figure" target="#fig_14">13</ref> shows various views of the reconstruction of that scene. The algorithm successfully reconstructed the walls, the doors and the markings on the floor. Because of the limited field of view, more images were needed to capture the structure of the scene than in the previous two examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Summary</head><p>Note that each of these image sequences contained edges that were visible in some images but not in others. The presented algorithm can handle this situation quite naturally and still provide acceptable reconstruction results.</p><p>The reconstruction times are given in Table <ref type="table">111</ref>. The algorithm was implemented in C and all the computations were performed on a Silicon Graphics R4000 Indigo. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. COMPARISON WITH LINEAR ALGORITHM</head><p>A series of simulation experiments were carried out in order to compare the algorithm presented in this paper to the three frame linear techniques proposed in [16], <ref type="bibr" target="#b27">[29]</ref>, <ref type="bibr" target="#b16">[17]</ref>. We chose to implement the algorithm described in [ 161 because it was considered to be one of the best linear structure from motion methods.</p><p>These experiments simulated a trinocular stereo configuration viewing a set of three rotated cubes (a total of 36 line segments). In this configuration the three cameras were placed at regular intervals along a circle of radius 250 mm., the rotated cubes were 200 mm. on side and were positioned 1 meter away from the center of the trinocular system. This arrangement is almost identical to the one shown in Fig. <ref type="figure" target="#fig_6">4</ref> the only difference being that the camera at the center of the circle was omitted.</p><p>These experiments were designed to determine how the accuracy of the reconstruction would vary as the amount of error in the image measurements was increased. A uniformly distributed random image error was added to the endpoints of the image segments. The simulated cameras had a focal length of 8mm with a 30 degree field of view, and the pixels in the image plane were assumed to be 8.373e-3 mm. on a side. The same input data was supplied to the three frame linear algorithm (WLHA88) and the non-linear algorithm (TK93).</p><p>Fig. <ref type="figure" target="#fig_15">14</ref> shows a side by side comparison of the results obtained from the WLHA88 algorithm and the TK93 algorithm for the same data set. The image error in the input data set was 0.5 pixels. Fig. <ref type="figure" target="#fig_7">15</ref> shows how the accuracy of the reconstruction varied as a function of image error. Each point in these graphs represents the average of 50 trials.</p><p>These experiments demonstrate that the linear method does not perform well in the presence of image error. In fact, the results become unusable as the image error is increased beyond 0.5 pixels. When the errors in the image measurements are relatively large, 1.5 pixels or greater, the estimate for the camera orientations provided by the WLHA88 algorithm is essentially random since the average error in these estimates approaches and exceeds d 2 . This also means that no real advantage would be gained by using the results from the linear algorithm as initial estimates for a more sophisticated iterative technique. Fig. <ref type="figure" target="#fig_7">15</ref> also indicates that the TK93 algorithm produced results that were at least an order of magnitude more accurate than those produced by the WLHA88 algorithm. Unlike the WLHA88 algorithm, the TK93 algorithm is not limited to three frames of image data. It can take advantage of any additional frames that may be available to improve the accuracy of its estimates. The WLHA88 algorithm also requires a minimum of thirteen line correspondences in three frames while the TK93 algorithm only requires six line correspondences in three images to produce a result. This is the minimum number of correspondences required to solve this particular structure from motion problem [8].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper presented a novel algorithm for recovering the structure of a constellation of straight line features and the motion of an observer from a set of edge correspondences derived from an image sequence. The structure from motion problem is formulated in terms of a non-linear objective function that measures the total squared disparity between the actual image measurements and the predicted image measurements. The algorithm obtains an estimate for the structure of the environment and the positions of the camera by minimizing this non-linear objective function with respect to the unknown parameters. It avoids the linearizing assumptions inherent in the Extended Kalman Filter approaches and can, therefore, be expected to yield more accurate results. New techniques were developed in order to carry out the numerical minimization procedures over the non-linear parameter manifolds and to explore the critical points of this cost function. These techniques have proven to be quite effective for this class of problems.</p><p>Thousands of simulation experiments were carried out in order to investigate how the accuracy of the algorithm would be affected as different parameters were varied. These experiments show that the algorithm can produce very accurate results even in the presence of significant amounts of random error in the edge endpoints and large calibration errors in the image center parameters. The algorithm has been shown to be far more accurate than a well regarded linear one. It was also applied to a number of data sets obtained from actual image sequences. These experiments demonstrate that the method can be used to successfully reconstruct a variety of scenes,</p><p>The proposed algorithm also offers several advantages over other approaches: it can be applied to image sequences where various structural features do not appear in every image, it can be applied in situations where no initial estimates are available for the camera orientations and the algorithm's computations can be performed in a reasonable amount of time on a standard workstation.</p><p>The experiments also indicate that the accuracy of the method is critically dependent upon the positions of the camera with respect to the scene being observed. This is a fundamental property of any structure from motion algorithm. At present, there is no systematic technique for choosing an appropriate set of viewpoints for a structure from motion algorithm. This problem should definitely be addressed in future research.</p><p>The real data experiments were performed by applying a Canny edge detector to each of the images in the sequence and then determining the correspondences between the extracted features manually. We plan to automate this process by developing algorithms to track the edge features throughout a video sequence.</p><p>Finally, the world around us is composed of more than simply lines floating in space; these features typically lie on planar surfaces. While the presented approach is entirely a bottom-up process, more accurate estimates can be obtained if model constraints are imposed in a top-down fashion. For example, if corresponding straight lines form an L-junction or Yjunction in two or more images, then the lines must intersect at a point in the 3D world. This observation can be used to constrain the solution during the minimization. Other equality constraints such as coplanarity and parallelism could also be used to achieve greater accuracy. One further improvement would be to explicitly satisfy the inequality constraints imposed by T-junctions as is done in Sugihara's work on line drawing interpretation <ref type="bibr" target="#b28">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLEMENTATION</head><p>The implementation of this algorithm, our implementation of the algorithm presented in [16], and the data sets used during our experiments are available for noncommercial use. They can be accessed via anonymous ftp at daneel.eng. yale.edu or by contacting either of the authors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 shows how a straight line can be represented in terms of a unit vector i which indicates the direction of the line, and a vector d which designates the point on the line that is closest to the origin. In other words, we can represent a straight line by a tuple (G, d) where iTi = 1 and i T d = 0.' This set of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>=Fig. 3 .</head><label>3</label><figDesc>Fig.3shows a typical situation in the image plane. The heavy line in this figure represents the observed edge segment uy while the lighter line represents the edge predicted from 7 ( p i , qJ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FkeeD</head><label></label><figDesc>parametem : f, Number of degrees of freedom : 2n B Implove the entimates of Rj, 3; by minimizing C, Free parameters : +,, Rj Number of degrees of freedom : 2n + 3(m -1) non-zero t,ranslation parsmetera ia held constant to fix the scale. Freeparametas : &amp;,tj Number of degrees of freedom : 2n + 3(m -1) -1 Use the initial eatimates for R,, fi, di and tj provided by the previous stagen as initial estimates and minimize the main objective function 0 with respect to all of the free parameters to obtain the final estimates for R + d and t, ~e e parameters : f,, R,:&amp;,?, ' Number of degrees of freedom : 4% + 6(m -1) -1 C Construct initial estimates for di and t, by minimizing the quadratic functional C2. One of the In stage A the initial estimates for Rj are used to obtain estimates for ii. In stage B, better estimates for Rj and ii are determined. In stage C, we obtain initial estimates for di and tj,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>edge uu, the measured normal m' to the plane passing through the camera center and the observed edge can be obtained as follows:An objective function can be devised which indicates how well the estimates for Rj and ii satisfy the constraint given in (8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Notice that this objective function involves only, Rj and ii, which means that it can be used to estimate the line directions and camera orientation independently of the camera position and line position.In stage A the rotation matrices Rj are held constant, and the line directions are estimated by minimizing C, with respect to the ii parameters. Notice that this function can be decomposed into C, = e n 1 4 CA 8 where ;=I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A circular stereo camera configuration,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>aFig. 5 .</head><label>5</label><figDesc>Fig. 5. How the accuracy in the reconstruction varies as a function of the magnitude of the random errors in the edge endpoints: a. Camera orientation error; b. Structure error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Variation of reconstruction accuracy as a function of the number of images m and the number of lines n: a. Camera orientation emor; b. Structure error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>I</head><label></label><figDesc>AS A mNCTl0N OF THE AMOUNT OF UNCERTAINTY IN THE INITIAL ESTIMATES, e, , u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Variation of reconstruction accuracy as a function of the number of images m and the radius of the circular stereo configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Two images from a sequence of eight that were used for reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>cFig. 9 .</head><label>9</label><figDesc>Fig. 9. Two views of the reconstructed scene. The small coordinate axes in figure b represent the reconstructed camera positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FigFig. 13 .</head><label>13</label><figDesc>Fig. IO. Two images taken from a sequence of ten.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Figure a. shows the reconstruction produced by the WLHA88 three frame linear algorithm while figure b. shows the results obtained by the TK93 non-linear algorithm on the same data set. In this experiment, 0.5 pixels of random image error were added to the endpoints of the simulated image edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I A N OVERVIEW OFTHE FOUR STAGES IN THE LOCAL MINIMIZATION STRATEGY A Construct initial estimates for the line orientations 0, by minimizing C,</head><label>I</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>T A B L E 111 COMPUTE TIME REQUIRED FOR EACH SCENE ositions, m time in secs.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>437.09</cell></row><row><cell></cell><cell></cell><cell>553.40</cell></row><row><cell>33</cell><cell>24</cell><cell>790.47</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank P. Anandan for his contributions to this research.</p><p>Support for this work has been provided by a gift from the INMOS division of SGS-Thomson and by grants from the National Science Foundation, DDM-9112458 and NYI IRI-9257990.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Inferpretution of Visual Motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A computer algorithm for reconstructing a scene from two projections</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Jmnguet-Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="133" to="135" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Motion and structure h m two perspective views: Algorithms, enur analysis, and envr estimalion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T r m . Punem Anulysisand Mmhine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="549" to="570" />
			<date type="published" when="1989-05">May 1989. Nov. 1993</date>
		</imprint>
	</monogr>
	<note>Image and Vision Computing</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relative orientation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Motion und Structurefrrim h u g e Sequences</title>
		<title level="s">Springer Series on Information Sciences</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measurement and integration of 3D structures by tracking edge lines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stelmaszyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Skordas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Puget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="52" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion and structure from point and line matches</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lustaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toscani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int &apos;1 Cmj: Crimputer Vision</title>
		<meeting>Int &apos;1 Cmj: Crimputer Vision</meeting>
		<imprint>
			<date type="published" when="1987-06">June 1987</date>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D structure from a monocular sequence of images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jezouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Cor$ Computer Vision</title>
		<meeting>Int&apos;l Cor$ Computer Vision</meeting>
		<imprint>
			<date type="published" when="1990-12">Dec. 1990. 1990</date>
			<biblScope unit="page" from="59" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feed-forward recovery of motion and structure from a sequence of 2D-lines matches</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vieville</surname></persName>
		</author>
		<author>
			<persName><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inr&apos;l Conf: Computer Vision</title>
		<meeting>Inr&apos;l Conf: Computer Vision</meeting>
		<imprint>
			<date type="published" when="1990-12">Dec. 1990</date>
			<biblScope unit="page">517</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape and motion from image streams under orthography: A factorization method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recovering 3D shape and motion from image streams using nonlinear least squares</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="28" />
			<date type="published" when="1994-03">Mar. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimation of 3D-motion and structure from tracking 2Dlines in a sequence of images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vieville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf: Computer Vision</title>
		<meeting>European Conf: Computer Vision</meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="page">281</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D-vision for active visual loops using locally rectilinear edges</title>
		<author>
			<persName><forename type="first">B</forename><surname>Giai-Checa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vieville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. I992 IEEE Int &apos;1 Symp. Intelligent Conrrol</title>
		<meeting>I992 IEEE Int &apos;1 Symp. Intelligent Conrrol</meeting>
		<imprint>
			<date type="published" when="1992-08">Aug. 1992</date>
			<biblScope unit="page">341</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recovering 3D motion and structure from stereo and 2D token tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Con$ Computer Vision</title>
		<meeting>Int&apos;l Con$ Computer Vision</meeting>
		<imprint>
			<date type="published" when="1990-12">Dec. 1990</date>
			<biblScope unit="page">513</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating motiodstructure from line correspondences: A robust linear algorithm and uniqueness theorems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf: Comp. Vision and Pattern Recognition</title>
		<meeting>IEEE Conf: Comp. Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="387" to="392" />
		</imprint>
	</monogr>
	<note>with the Department of Berkeley</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Taylor received an AB in electrical, computer, and systems engineering magna cum laude with highest honors from Harvard University in 1988. He received the MS and PhD degrees in electrical dngineering from Yale University in 1990 and 1994, respectively. Dr. Taylor is a member of the Harvard chapter of Phi Beta Kappa and was the recipient of the Jamaica Scholarship in 1985. His research interests include structure from motion, mobile robots, and advanced recognition systems. He is currently a postdoctoral researcher and lecturer Electrical Engineering and Computer Science at U.C</title>
		<author>
			<persName><forename type="first">J</forename><surname>Camillo</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Kriegman summa cum laude from Princeton University with a BSE degree in electrical engineering and computer science in 1983 where he was awarded the Charles Ira Young Award for electrical engineering research. He received the MS degree in 1984 and the PhD in 1989 in electrical engineering from Stanford University where he studied under a Hertz Foundation Fellowship</title>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Currently, he is an associate professor at the Center for Systems Science in the Departments of Electrical</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure from motion using line correspondences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Spetsakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J . Computer vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="1990-06">June 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A linear algorithm for point and line based structure from motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spetsakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP:Imge Understanding</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1910">Sept. 1992. 191 0.. 1993</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
	<note>Faugeras, Three-Dimensional Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structure and motion in two dimensions from multiple images: A least squares approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop Visual Motion</title>
		<imprint>
			<date type="published" when="1991-10">Oct. 1991</date>
			<biblScope unit="page" from="242" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Craig</surname></persName>
		</author>
		<title level="m">Introduction to Robotics: Mechanics and Control</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computations</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Johns Hopkins Univ. Press</publisher>
			<pubPlace>Baltimore, Md</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Determination of camera location from 2D to 3D line and point correspondences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf: Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf: Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="82" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust estimation of camera location and orientation from noisy data having outliers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on the Interpretation of 3 0 Scenes</title>
		<meeting>Workshop on the Interpretation of 3 0 Scenes</meeting>
		<imprint>
			<date type="published" when="1989-11">Nov. 1989</date>
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Minimization on the lie group SO(3) and related manifolds</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<idno>9405</idno>
		<imprint>
			<date type="published" when="1994-04">Apr. 1994</date>
			<pubPlace>New Haven, Conn</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center for Systems Science, Dept. of Electrical Engineering, Yale Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geometric Optimization Methods for Adaptive Filtering</title>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Division of Applied Sciences</title>
		<meeting><address><addrLine>Cambridge Mass.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09">Sept. 1993</date>
		</imprint>
		<respStmt>
			<orgName>Harvard Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Manual of Photogrammetry</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>American Society of Photogrammetry</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A linear algorithm for motion estimation using straight line correspondences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1988-10">Oct. 1988</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="35" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An algebraic approach to the shape-from-image problem</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sugihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He was awarded a National Science Foundation Young Investigator Award in 1992. Dr. Kriegman&apos;s research interests include mobile robotics and compukrvision</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="59" to="95" />
		</imprint>
	</monogr>
	<note>Engineering and Computer Science at Yale University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
