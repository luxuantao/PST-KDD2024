<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-aware Pseudo Label Refinery for Entity Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dandan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty-aware Pseudo Label Refinery for Entity Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3511926</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge Graph</term>
					<term>Entity Alignment</term>
					<term>Unsupervised</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity alignment (EA), which aims to discover equivalent entities in knowledge graphs (KGs), bridges heterogeneous sources of information and facilitates the integration of knowledge. Recently, based on translational models, EA has achieved impressive performance in utilizing graph structures or by adopting auxiliary information. However, existing entity alignment methods mainly rely on manually labeled entity alignment seeds, limiting their applicability in real scenarios. In this paper, a simple but effective Uncertainty-aware Pseudo Label Refinery (UPLR) framework is proposed without manually labeling requirement and is capable of learning high-quality entity embeddings from pseudo-labeled data sets containing noisy data. Our proposed model relies on two key factors: First, a non-sampling calibration strategy is provided that does not require artificially designed thresholds to reduce the influence of noise labels. Second, the entity alignment model achieves goal-oriented uncertainty correction through a gradual enhancement strategy. Experimental results on benchmark datasets demonstrate that our proposed model outperforms the existing supervised methods in cross-lingual knowledge graph tasks. Our source code is available at: https://github.com/Jia-Li2/UPLR/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graphs (KGs), which store facts as triples in the form of (subject entity, relationship, object entity) or (subject entity, attribute, text value), play an essential role in today's knowledge graphs machine learning. Many tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> rely on the quality of the knowledge graph to successfully conduct their operations. As knowledge graph was initially proposed to enhance the searching results of the search engines, it is widely used in Web applications such as Google search. However, with different KGs usually extracted from different data sources or contributed by people with diverse expertise, KGs are incomplete, severely hindering knowledge graphs' practicality.</p><p>In order to use the information obtained in various knowledge graphs, the knowledge graphs are expected to be integrated. The entity alignment task <ref type="bibr" target="#b36">[37]</ref> is defined to identify entities that refer to the same object in the real world from two knowledge graphs, which plays a vital role in automatically integrating multiple knowledge graphs.</p><p>Existing methods usually aim to solve the alignment problem in a supervised manner through the artificially designed features <ref type="bibr" target="#b45">[46]</ref> or the entity representation learned from the KG embedding method <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. In addition, some semi-supervised methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref> are also proposed to improve the performance of supervised entity alignment, such as names, attributes, and text descriptions with auxiliary information data to help embed learning. Supervised or semi-supervised alignment methods have significantly discovered semantically related entities.</p><p>However, they rely on clean labeled entity pairs as the training set, and creating clean pre-aligned labeled entity pairs (seeds) requires many resources. Significantly, the lack of label sets will reduce the performance of traditional entity alignment models when applied in the real world since the knowledge graph usually contains millions of entities, relationships, and triples. Unsatisfactory alignment results will be obtained if the model does not have a sufficient ratio of labeled entity pairs. For example, the reduced seed seriously affects the model's optimization of the geometric arrangement of the vector space of the two knowledge graphs. In DBP15K ZH-EN , the model <ref type="bibr" target="#b28">[29]</ref> Hits@1 is about 50% when the proportion of seed alignment is 50%, whereas the model Hits@1 is about 25% when the proportion of entity alignment seed of the model is reduced to 10%.</p><p>In the process of entity alignment, in order to get rid of the constraint of labeling entity (seed), the idea of using entity semantic information to construct cross-lingual entity pseudo-label is proposed to realize unsupervised entity alignment.</p><p>Due to two main challenges, its implementation is not easy:</p><p>• The newly proposed pseudo-label pairs inevitably contain noise pairs. Through experiments (SE of Table <ref type="table" target="#tab_6">3</ref>), it is found that the accuracy of the constructed pseudo-labels is only 58.6% in the DBP15K ZH-EN dataset. It is not clear where the noise is, and it is expensive and laborious to distinguish whether the marked entity is noise or not. Therefore, the designed model should reduce the impact of noise without any supervision.</p><p>• An unified model for joint training is designed to reduce the impact of noise and cross-lingual entity alignment. On the one hand, the noisy pseudo-label data set is beneficial to solve the entity alignment problem in an unsupervised method.</p><p>On the other hand, a high-precision entity alignment model is conducive to constructing a high-quality pseudo-label data set. However, the difficulty in designing the expected unified model lies in building a high-precision entity alignment model because it requires clean labeled entity pairs as training data, and noisy labels are also challenging to detect. In addition, even if the influence of noise has been reduced, it is hard to design feedback from reducing the influence of noise to entity alignment. Therefore, designing a joint training strategy is a challenging issue.</p><p>This paper proposes an Uncertainty-aware Pseudo Label Refinery (UPLR) framework that aligns entities without using any cross-graph alignment label set. UPLR achieves higher performance than state-of-the-art contrastive methods without using negative pairs. It iteratively bootstraps the outputs of a network to serve as targets for an enhanced representation. Moreover, UPLR is more robust to the choice of label augmentations than contrastive methods; It is suspected that not relying on negative pairs is one of the leading reasons for its improved robustness. Though previous methods based on bootstrapping have used pseudo-labels <ref type="bibr" target="#b13">[14]</ref>, cluster indices <ref type="bibr" target="#b3">[4]</ref>, or a handful of labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34]</ref>, directly bootstrapping the representations is proposed. Specifically, a set of pseudo-labels denoting potential cross-graph entity pairs is created based on semantic embeddings of entities. When extracting entity characteristics, the GAT instead of the graph convolutional network is used. It is an advantage that GAT can implicitly specify different weights to different nodes in a neighborhood, avoiding the possible effects of noise propagation on nodes in the graph. The GAT model further enhances the validity of the representative feature vectors of the entity. Then, attention scores help us capture and filter the importance of the remote nodes related to the central node. Finally, a gate graph attention network (GateGAT) is trained with pseudo-labeling seeds to update semantic embeddings of entities and generate high-quality aligned entities. This process is performed iteratively to gradually improve the pseudo-labeling accuracy and obtain the final entity alignment result. Experiments on three benchmark datasets demonstrate that the UPLR model in a completely unsupervised environment has achieved better results than supervised methods trained with manually labeled alignment seeds.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the difference between traditional methods and our proposed model. Top (Traditional Method): The traditional method to implement the entity alignment model is to perform negative sampling based on the manual entity alignment seed. Bottom (UPLR): Our proposed model to implement the entity alignment model is to construct a pseudo-labeled data set by using semantic embedding and directly input the unlabeled data set without negative sampling.</p><p>The contributions of this paper are listed as follows:</p><p>• A new Uncertainty-aware Pseudo Label Refinery (UPLR) framework is proposed, which adaptively mines confident samples from unlabeled data that will be labeled into trusted  "positive" (pseudo label) classes. A hybrid loss is applied to both the augmented "labeled" examples and remaining unlabeled data for "supervision". • Considering the existence of errors in the pseudo labels, a novel non-sampling calibration strategy without any manually designed threshold is proposed to reduce the influence of noisy labels on entity representation learning. • The drawback with pseudo-labels is that if the domains are not similar enough, it is not easy for us to obtain high-quality pseudo-labels since the labeling noise might be too high to hurt the performance. Therefore, a gradual enhancement strategy is proposed to enhance the similarity of the two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 EA based on translational models</head><p>The TransE-based <ref type="bibr" target="#b1">[2]</ref> method represents the entity by modeling the triples involved, which provides fine-grained relational semantics of knowledge. Among the various translational methods, BootEA <ref type="bibr" target="#b30">[31]</ref>,</p><p>and TransEdge <ref type="bibr" target="#b31">[32]</ref> both utilize an iterative strategy to optimize model performance, which is a key technology to improve performance. NAEA <ref type="bibr" target="#b47">[48]</ref> designs an attentional mechanism based on TransE to learn neighbor-level representations through weighted combinations of neighbor representations. KECG <ref type="bibr" target="#b14">[15]</ref> combines the advantages of TransE and GCN, just like GCN, it convolves all the adjacent information of the entity while keeping the TransE translation between the head, relationship, and tail. PRASE <ref type="bibr" target="#b24">[25]</ref> establishes corresponding filtering rules through the pre-training model. Furthermore, the adversarial learning of AKE <ref type="bibr" target="#b15">[16]</ref> and SEA <ref type="bibr" target="#b22">[23]</ref> based on TransE fails to achieve the desired effect, which is due to the high heterogeneity of KG that makes it difficult to convert from one KG to another through linear mapping functions similar to multilingual lexical space transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EA based on graph structures</head><p>The GCN-based method GCN-Align <ref type="bibr" target="#b36">[37]</ref> represents the entity by recursively aggregating the features of its neighbors, which utilizes the global KG structure. AliNet <ref type="bibr" target="#b32">[33]</ref> shows the effectiveness of the multi-hop neighbors. HyperKA <ref type="bibr" target="#b27">[28]</ref> maps the entity embedding obtained by GNN to the hyperbolic space for entity alignment. REA <ref type="bibr" target="#b23">[24]</ref> has designed noise detection and noise perception to improve entity alignment performance. DGMC <ref type="bibr" target="#b7">[8]</ref> employs synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. RREA <ref type="bibr" target="#b21">[22]</ref> uses relational reflection transformation to obtain relation-specific embeddings for each entity in a more efficient way. DINGAL <ref type="bibr" target="#b44">[45]</ref> expands the coupling distance between the parameter matrix in GCN and the topology of the underlying graph. Dual-AMN <ref type="bibr" target="#b18">[19]</ref> not only models both intra-graph and cross-graph information, but also greatly reduces computational complexity. Although good results have been achieved, high-quality entity alignment seed are required. If the proportion of seeds is reduced, its performance will be greatly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">EA with auxiliary information</head><p>In addition to graph structure information, some studies also add entity features based on auxiliary information into the process of entity encoding, including semantic embedding <ref type="bibr" target="#b38">[39]</ref>, attributes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47]</ref>, images <ref type="bibr" target="#b16">[17]</ref> and description <ref type="bibr" target="#b45">[46]</ref>. GM-Align <ref type="bibr" target="#b42">[43]</ref>, RDGCN <ref type="bibr" target="#b38">[39]</ref> and HGCN <ref type="bibr" target="#b39">[40]</ref> are models that combine semantic embedding of entity names. They distinguish different neighbors by initializing the entity representation. Therefore, their performance is better than the original GCN-align model <ref type="bibr" target="#b36">[37]</ref>. AttrGNN <ref type="bibr" target="#b17">[18]</ref>, CEAFF <ref type="bibr" target="#b46">[47]</ref> and EPEA <ref type="bibr" target="#b37">[38]</ref> are models that combine entity attribute information. In the encoding process of the above methods, entity embedding is optimized by adding entity attribute information. However, additional information is not always available; thus, our proposed model solves entity alignment from a different perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Learning from Pseudo-labelling</head><p>Pseudo-labelling has been mainly adopted by methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> based on deep learning. For unlabelled samples, the probability distribution of the model prediction is used as an indicator for pseudolabel the data <ref type="bibr" target="#b13">[14]</ref>. By using the simple and efficient method, the system can easily add more data to help re-train the model. He and Sun. <ref type="bibr" target="#b10">[11]</ref> proved that using a batch of samples with the highest prediction probability of the model can help enhance the performance of the model. For pseudo-label based methods, Lee et al. <ref type="bibr" target="#b13">[14]</ref> and Xie et al. <ref type="bibr" target="#b41">[42]</ref> generated pseudo labels for unlabeled data and then added them to train with labeled samples for enlarging data quantity jointly. In this regard, the quality of the pseudo label is vital. The difference among these methods is how to promote the accuracy of generated pseudo labels: Lee et al. <ref type="bibr" target="#b13">[14]</ref> only utilized the high confidence pseudo labels by setting a threshold and avoided using the low-confidence label that may introduce the label noise; Xie et al. <ref type="bibr" target="#b41">[42]</ref> refined the generated pseudo labels round by round with the introduced noisy student model, which is based on the assumption that the model would learn the essential and distinct features under the noisy input of each round and gradually generate more precise pseudo labels. Our proposed model employs pseudo-labeling but inherits the spirit of label propagation. To avoid exhaustive labeling of unobserved samples, the graph structure information for selecting candidates that can be reliably labelled is exploited. Unlike previous studies, we propose a new loss function that can be used to efficiently train neural network models from pseudo-labeled and unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UPLR</head><p>Our proposed model builds a gate graph attention network based on the graph attention network and further develops a non-sampling calibration strategy combined with a gradual enhancement strategy to optimize the final knowledge graph embedding. Figure <ref type="figure" target="#fig_1">2</ref> shows the framework of the model, and our proposed model will be described in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Formally, a KG is represented as G = (E, R,T ) and E, R, T are the sets of entities, relations and triples respectively. Let </p><formula xml:id="formula_0">G s = (E s , R s , T s ) and G t = (E t ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gate Graph Attention Networks</head><p>In this section, we adapt the non-local model of <ref type="bibr" target="#b40">[41]</ref> to introduce gate to the Graph Attention Networks (GAT) <ref type="bibr" target="#b35">[36]</ref> framework, enabling the entity alignment model to efficiently model relationships between widely separated spatial regions. We call the proposed model Gate Graph Attention Networks (GateGAT) because of its gate module <ref type="bibr" target="#b32">[33]</ref> (see Figure <ref type="figure" target="#fig_1">2</ref>). GAT can be interpreted as performing the propagation of the attention message and updating on nodes. At each layer, the node features e i ∈ G (G = (e 1 , e 2 , . . . , e i , . . . , e n )) will be updated by its attention neighbor messages to e ′ i ∈ G as follows:</p><formula xml:id="formula_1">e ′ i = σ j ∈ {i }∪N i α i j W e i (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where σ is a non-linear activation, N i represents the one-hop neighbor set of node s, and W is a trainable parameter. The attention coefficient α i, j is computed as follows, with parameters of a weight vector W . Moreover, to further explore the translation information for relations based on triples, and inspired by TransE <ref type="bibr" target="#b1">[2]</ref>, our designs a relational translation matrix M r i, j as follows:</p><formula xml:id="formula_3">α i, j = so f tmax j e i, j = exp e i, j k ∈N s exp (e ik )<label>(2)</label></formula><p>where </p><formula xml:id="formula_4">e i, j = LeakyRelu M r i, j W T W e i ∥W e j ,<label>(3)</label></formula><formula xml:id="formula_5">M r i, j = r i r T j r i , r j ∈ R , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where R is the set of r relations in G.</p><p>The entity features from the previous hidden layer e ′ i ∈ R N ×N are first transformed into two feature spaces f , д to calculate the attention, where</p><formula xml:id="formula_7">β i, j = exp f W f e ′ i T д W д e ′ j N i=1 exp f W f e ′ i T д W д e ′ j ,<label>(5)</label></formula><p>and β i, j indicates the extent to which the model attends to the i th location when synthesizing the j th region.</p><formula xml:id="formula_8">W f ∈ R N ×N ,W д ∈ R C×N .</formula><p>Here, C is the number of channels, N is the number of feature locations of features from the previous hidden layer. The output of the attention layer is o</p><formula xml:id="formula_9">= (o 1 , o 2 , . . . , o i , . . . , o n ) ∈ R N ×N</formula><p>, where</p><formula xml:id="formula_10">o i = N i=1 β i, j f e ′ i − д e ′ j ,<label>(6)</label></formula><formula xml:id="formula_11">θ i = siдmoid (Mo i + b) ,<label>(7)</label></formula><p>where θ as the gate to control the combination of both source entity and target entity. M and b are the weight matrix and bias vector, respectively. Finally, the new vector of the node is obtained by inputting the node e i vector and the new information vector of node o i into Equation <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_12">e (l +1) i = (1 − θ i ) ⊙ o (l ) i + θ i ⊙ e (l ) i<label>(8)</label></formula><p>where (1 − θ i ⊙) and θ i ⊙ are working as selectors to choose information to be forgotten and remembered, respectively. e (l +1) i is the embedding of node i, which is the output feature vector of the (l+1)-th GateGAT layer with respect to node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Interactive Divergence</head><p>In order to mine parallel data without supervision signals, we use graph interaction divergence to measure the pair of source entities and target entities. We aim to construct a new metric that can measure the difference score between a pair of entities (e s , e t ). If two entities are similar, we expect their differences to be insignificant. We construct the divergence by calculating the distance between the source entity and the target entity, which are defined as follows:</p><formula xml:id="formula_13">D(e s , e t ) = cos(e s , e t )<label>(9)</label></formula><p>where D(., .) is the divergence between two entities, cos(., .) is the cosine distance, e s is the source entity, and e t is the target entity. To calculate the divergence of the cross-lingual knowledge graph neighbor entity nodes of the source entity and the target entity, respectively, and construct the graph interaction divergence GID(e s , e t ) between the entities, which is defined as follows:</p><p>GID(e s , e t ) = D(e s , e t ) − α div(D)(e s , e t )</p><p>where α represents a hyper-parameters, and</p><formula xml:id="formula_15">div(D)(e s , e t ) = p ∈N (e t )</formula><p>D(e s , p)</p><formula xml:id="formula_16">+ q ∈N (e s )</formula><p>D(e t , q)</p><p>where N (.) is the set of neighbor nodes of the entity, p and q represent the entity neighbor nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Building Entity Alignment Seeds</head><p>In the Unsupervised algorithm for Building Entity Alignment seeds (UBEA) algorithm (Algorithm 1), the key idea is to calculate the graph interaction divergence of cross-lingual knowledge graph entity pairs. An unsupervised algorithm is created to build a pseudolabels set based on the graph interaction divergence. Entity sets e s and e t are constructed in the source knowledge graph E s and the target knowledge graph E t , respectively. The initial entity embedding is obtained from the initial semantic embedding, and then the similarity of cross-lingual knowledge graph entities through the graph interaction divergence is measured. For each pair of the entities (e s , e t ), (e t , e s ), our constructs a rule (6 and 7 of Algorithm end if 15: end for 16: return P LS 1) to enhance the entity label pair (e s , e t 1 ),(e s 1 , e t ) (1 represents the 1-ranked entity), and aggregate all the cross-lingual knowledge graph entity pairs e 1 s , e 1 t mined into a pseudo-labels set. An outline of the algorithm is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Non-sampling calibration strategy</head><p>As KGs provide semantic relations between entities, it is natural to incorporate the semantics of the relational facts into entity modeling. As discussed in Section 2, GCN needs the structures of two KGs to be highly similar or relation alignment for entity alignment. Here, we borrow the translational assumption from TransE <ref type="bibr" target="#b1">[2]</ref>. The detailed steps are as follows:</p><formula xml:id="formula_18">r = 1 |T r | (s,t )∈ T r (e s − e t )<label>(12)</label></formula><p>where T r is the subject-target entity pairs of relation r . Then the following relation loss for refinement is minimized:</p><formula xml:id="formula_19">L r = r ∈R 1 |T r | (s,t )∈ T r ∥e s − e t − r∥ (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where R is the set of the total relations in the KGs. An efficient optimization method without sampling is introduced in this section, which is the basis for learning our proposed model from the whole heterogeneous feedback data. The observed interactions are somewhat limited for pseudo-labeled data, while the number of unobserved examples is high. To learn model parameters, He and Sun <ref type="bibr" target="#b11">[12]</ref> introduce a weighted regression loss that associates confidence to each prediction in the implicit data matrix. Following this idea, for a batch of pseudo labels PLS(e s , e t ) and the whole entity set E, their loss of a matrix M E is: </p><p>where W is a weight matrix to be learned. The time complexity of computing this loss is O(|P ||E|d). The final objective of UPLR is the combination of the relation loss and without negative sampling loss, aiming at injecting relation semantics to the preserved graph structures:</p><formula xml:id="formula_22">L N = λ(2L r − L s − L t ) + b<label>(16)</label></formula><p>where λ and b are hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Iterative Learning</head><p>Based on the construction algorithm of the unsupervised entity alignment seed (Algorithm 1), a gradual enhancement strategy is proposed to iteratively build the entity pseudo-label set and learn the knowledge graph representation (Algorithm 2). The goal is to optimize the pseudo-labels set and improve the mapping between the two spaces. In Algorithm 2, the pseudo-labels set and knowledge representation of entity are optimized by iterating T times. In the iteration process, updating the pseudo-labels set from the new training model every iteration will be updated (Algorithm 2, line 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>DBP15K <ref type="bibr" target="#b29">[30]</ref> contains three datasets built from DBpedia, namely DBP15K ZH-EN (Chinese-English), DBP15K JA-EN (Japanese-English) and DBP15K FR-EN (French-English). Each dataset has 15 thousand reference entity alignment, and the relation alignment is partially known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>Like previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref>, we use Hits@k (Hk) and Mean Reciprocal Rank (MRR) to evaluate the effectiveness of our proposed model, where a Hits@k (Hk) score (higher is better) is computed by measuring the proportion of correctly ranked cross-lingual knowledge graph entity pairs ranked in the top k. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The configuration is: Attention-heads= 2, α = 0.5, γ = 3 , lr = 0.005, Depth = 3, Node-hidden=125, Rel-hidden = 125, epoch = 6, T =5. In order to utilize entity names in different KGs for better initialization, they <ref type="bibr" target="#b38">[39]</ref> uses Google Translate to translate Chinese, Japanese, and French entity names into English, and then use pre-trained English semantic embedding GloVe.840B.300d to construct the input entity representations for the primal graph. Note: Google Translate can not guarantee accurate translations for named entities without any context. We use Tensorflow along with Python 3.6 to perform our experiments. All experiments are executed on a Linux machine with processor Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz, 256GB RAM, and GeForce RTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Settings of Experiments</head><p>In order to analyze the performance of the model, we selected some classic and state-of-the-art entity alignment methods as baselines in our experiments (the parameters of the baseline models are all default parameters set in public resources). Our proposed model evaluates the performance of the model from three aspects: (1) Main results;</p><p>(2) Ablation study; (3) Auxiliary Experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>Our proposed model has omitted some methods that require auxiliary entity information that is not used by others (see Section 2). Although the semantic information in the process of unsupervised entity alignment is added, we do not add semantic information to knowledge representation. Therefore, there are three types of comparative experiments designed: Graph structure, Graph structure + Iterative learning, and Unsupervised.</p><p>Only use graph structures.</p><p>Our proposed model compares the differences in methods based on graph structural features, which includes nine approaches. Table <ref type="table" target="#tab_5">2</ref> shows the performance comparison of the basic graph structure methods. Obviously, on all evaluation indicators, the model's performance is consistently rated as the best among all elementary graph structure methods. In particular, compared with the state-ofthe-art method RREA, the method on the DBP15K ZH-EN dataset improves by 12.6% on H 1, and the method on the DBP15K JA-EN dataset increases by 13.8% on H 1, and the method on the DBP15K FR-EN dataset improves by 18.0% on H 1. The main reason is that the unsupervised entity alignment algorithm establishes more seeds for entity alignment, and the information between knowledge graphs across languages can be better optimized through unsampled loss functions.</p><p>In addition, our proposed model unsupervised entity alignment algorithm can significantly improve the performance of all methods on all datasets. We observe that the performance gap of textual methods in different datasets is much more important than that of structural methods. The performance of the model has increased most in the French dataset. This is because French words are easily mapped into English when constructing entity alignment seeds based on semantic embeddings. All in all, UPLR breaks the upper limit of the performance of the purely structure-based entity alignment method, which proves that the design is efficient.</p><p>Use graph structure and iterative learning.</p><p>Our proposed model outperforms the eight models using only graph structures and iterative learning models and achieves the best results for all datasets. Specifically, BootEA is a transe-based model with better performance than some models that only use the graph structure because it uses a bootstrap strategy to expand entity alignment seeds iteratively. This demonstrates that the iterative strategy can significantly improve the performance of entity alignment.</p><p>Our proposed model adds an iterative strategy to improve H 1 by at least 6%. Our proposed unsupervised method enables better knowledge representation for KGs, thereby improving the entity alignment performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised.</head><p>Compared with other unsupervised entity alignment methods, Our proposed model has certain advantages. This method is improved on the DBP15K data set, which confirms that the seed based on semantic embedding can improve the performance of unsupervised entity alignment more effectively than the seed based on image information. In reality, image information is often not directly obtained, or it is not easy to get high-quality image-entity translation pairs. In addition, although SEU has achieved good results, it solves the task of entity alignment through static embedding (semantic embedding). Due to the difference between static embedding and real data (We discuss the performance of the static embedding based model in the SE in Table <ref type="table" target="#tab_6">3</ref>), the model performance is not as good as our proposed model. Therefore, this method is considered one of the most effective unsupervised entity alignment methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>In the above experiments, the overall effectiveness of the method is proved. In this section, we demonstrate the validity of each component of UPLR (GateGAT+L N +P+IL). Ablation studies were conducted from multiple aspects, and the results are shown in Table <ref type="table" target="#tab_6">3</ref>. Ls represents a margin-based scoring function (this is a commonly <ref type="bibr" target="#b38">[39]</ref> used method for calculating loss functions in supervised learning), GAT represents graph attention network, GateGAT represents gate graph attention network, L S represents supervised loss function <ref type="bibr" target="#b38">[39]</ref> requiring seeds, IL represents iterative learning, L N represents our non-sampling calibration strategy, S represents entity-aligned seeds (seed accounts for 30% of the total dataset) and P represents pseudo-label dataset. SE stands for semantic embedding (it comes from GloVe.840B.300d, and is detailed in (4.2)). In experiments requiring S, the H 1 value of GateGAT was higher than that of GAT H 1. However, the H 10 value of GateGAT is slightly lower than GAT H 10, which may be because Gate reduces the  <ref type="bibr" target="#b4">[5]</ref> 0.209 0.512 0.310 0.25 0.572 0.360 0.247 0.577 0.360 KECG <ref type="bibr" target="#b14">[15]</ref> 0.478 0.835 0.598 0.49 0.844 0.610 0.486 0.851 0.610 MuGNN <ref type="bibr" target="#b2">[3]</ref> 0.494 0.844 0.611 0.501 0.857 0.621 0.495 0.870 0.621 RSN <ref type="bibr" target="#b9">[10]</ref> 0.508 0.745 0.591 0.507 0.737 0.590 0.516 0.768 0.605 GMNN <ref type="bibr" target="#b43">[44]</ref> 0.539 0.826 -0.433 0.681 -0.465 0.728 -AliNet <ref type="bibr" target="#b32">[33]</ref> 0.539 0.826 0.628 0.549 0.831 0.552 0.645 0.852 0.657 HyperKA <ref type="bibr" target="#b27">[28]</ref> 0.572 0.865 0.678 0.564 0.865 0.673 0.597 0.891 0.704 MRAEA(Basic) <ref type="bibr" target="#b20">[21]</ref> 0.638 0.886 0.736 0.646 0.891 0.735 0.666 0.912 0.765 RREA(Basic) <ref type="bibr" target="#b21">[22]</ref> 0.715 0.929 0.794 0.713 0.933 0.793 0.739 0.946 0.816 Dual-AMN(Basic) <ref type="bibr" target="#b18">[19]</ref> 0.731 0.923 0.799 0.726 0.927 0.799 0.756 0.948 0.827 UPLR(Basic) 0.841 0.965 0.887 0.851 0.974 0.898 0.919 0.991 0.948 Graph structure + Iterative Learning BootEA <ref type="bibr" target="#b30">[31]</ref> 0.629 0.847 0.703 0.622 0.854 0.701 0.653 0.874 0.731 NAEA <ref type="bibr" target="#b47">[48]</ref> 0.650 0.867 0.720 0.641 0.873 0.718 0.673 0.894 0.752 TransEdge <ref type="bibr" target="#b31">[32]</ref> 0.735 0.919 0.801 0.719 0.932 0.795 0.710 0.941 0.796 MRAEA(Iter) <ref type="bibr" target="#b20">[21]</ref> 0.757 0.930 0.827 0.758 0.934 0.826 0.781 0.948 0.849 DGMC <ref type="bibr" target="#b7">[8]</ref> 0.801 0.874 -0.848 0.897 -0.933 0.960 -RREA(Semi) <ref type="bibr" target="#b21">[22]</ref> 0.801 0.948 0.857 0.802 0.952 0.858 0.827 0.966 0.881 Dual-AMN(Semi) <ref type="bibr" target="#b18">[19]</ref> 0.808 0.940 0.857 0.801 0.949 0.855 0.840 0.965 0.888 RNM <ref type="bibr" target="#b48">[49]</ref> 0.840 0.919 0.870 0.872 0.944 0.899 0.938 0.981 0.954 Unsupervised PRASE <ref type="bibr" target="#b24">[25]</ref> 0.651 --0.726 --0.757 --EVA <ref type="bibr" target="#b16">[17]</ref> 0.761 0.907 0.814 0.762 0.913 0.817 0.793 0.942 0.847 SEU(w) <ref type="bibr" target="#b19">[20]</ref> 0.816 0.923 0.854 0.865 0.952 0.896 0.953 0.989 0.967 UPLR 0.902 0.970 0.927 0.912 0.978 0.937 0.967 0.994 0.974  <ref type="table" target="#tab_6">3</ref>, the performance of both designs is greatly improved. These ablation experiments show that the design is meaningful and has improved significantly. The randomly selected parameters are based on SE in Table <ref type="table" target="#tab_6">3</ref>. Our proposed method constructs a pseudo-labels set of crosslingual knowledge graph entity pairs with DBP15K ZH-EN : 58.6%, DBP15K JA-EN : 66.4%, and DBP15K FR-EN : 82.5% accuracy, respectively. The results are presented in Figure <ref type="figure" target="#fig_4">3</ref>. Although the randomly selected pseudo-labels set and the pseudo-labels set calculated by semantic embedding have the same number of correct label entity pairs and the same number of incorrect label entity pairs, their final results are different. The performance of the randomly selected pseudo-labels set deteriorates in the three languages pairs. This experiment found that with the same correct labels, entities with different types of mislabeled have different contributions to the model, and mislabeled entity pairs obtained through semantic embedding can play a positive role in the entity alignment process. Therefore, the model's good performance depends not only on the algorithm but also on the important cross-lingual knowledge graph entity pairs discovered through semantic embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Auxiliary Experiments</head><p>4.6.2 The impact of different ratios of semantic embedding. In practical applications, constructing entity pseudo-labels by semantic embedding consumes many computing resources, especially for large-scale KGs. We hope that this model can maintain a good performance under the condition of limited semantic embedding. The results are presented in Figure <ref type="figure" target="#fig_4">3(b)</ref>. In order to examine the impact of semantic embedding with different ratios on the performance of the entity alignment model, Our proposed model sets up multiple sets of experiments. To investigate the performance with different  pre-aligned ratios, we set the ratios from 0.1 to 1. It is noted that if we only use a 0.1 ratio of semantic embedding information is used, our proposed model can surpass AliNet <ref type="bibr" target="#b32">[33]</ref> (AliNet uses 30% seed). The growth rate of the model performance is the highest when semantic embedding with 40% coverage is added to the model. With more semantic embedding added to the model, the model performance increases slowly, although the model can be enhanced. These results show that the unsupervised entity alignment model is effective. A small amount of semantic embedding can achieve good results for entity alignment tasks without entity alignment seeds.</p><p>4.6.3 The impact of different epoch. In addition to our proposed architecture, the non-sampling calibration loss is one of our main contributions, and to verify its effectiveness, we compare DBP15K datasets of different epochs in different languages. The result is shown in Figure <ref type="figure" target="#fig_4">3</ref>(c).</p><p>It is observed that our proposed non-sampling calibration strategy can make the model converge faster and achieve the best performance. In the experiment, the supervised loss function (RREA <ref type="bibr" target="#b21">[22]</ref> uses 3000 epochs) usually requires thousands of epochs to converge. In the DBP15K ZH-EN data set, our proposed model with a non-sampling calibration strategy is 5.6% higher than the model with a supervised loss function (Table <ref type="table" target="#tab_6">3</ref>, UPLR (w/o L N )). Experimental results show that our proposed non-sampling calibration loss function significantly improves the convergence speed without affecting the accuracy.</p><p>4.6.4 The impact of different iteration. In order to verify the effectiveness of this method, the impact of different iteration times on performance is further analyzed. The result is shown in <ref type="bibr">Figure 3(d)</ref>. It is observed that the H 1 score of our model increases with the number of iterations. This observation is consistent with our expectations because pseudo-labels are continuously refined through iterations. The UPLR model will quickly converge to the optimal solution if an appropriate number of iterations is selected. However, if the number of iterations is too large, the algorithm cannot converge. However, UPLR still shows satisfactory performance without iteration compared with other methods. It is noted that our proposed model has exceeded the performance of SEU <ref type="bibr" target="#b19">[20]</ref> (SEU uses 10 iterations) without iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We break the dependence of entity alignment models on seeds, and we propose an uncertainty-aware pseudo label refinery (UPLR) for entity alignment. We find that the challenge of solving the entity alignment task using pseudo-labels is to reduce the impact of noise on the model. We propose a non-sampling calibration strategy and a progressive enhancement strategy to reduce the influence of noise on the model. The non-sampling calibration strategy can reduce the impact of noise on the model without artificially designing a threshold, and can effectively reduce the impact of noise on the model in model optimization. The gradual enhancement strategy can enhance the similarity of the two domains, so that high-quality pseudo-labels can be generated. In future work, we will apply our proposed model to more datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A comparison of different learning frameworks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall framework of the UPLR. The dotted line represents the process of initializing the pseudo label.</figDesc><graphic url="image-1.png" coords="4,144.00,83.68,324.00,163.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 6 . 1</head><label>61</label><figDesc>The impact of pseudo-labeling. Our proposed method constructs a new pseudo-labels set (The results are shown in Figure3(a)) for DBP15K by random selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The impact of different ratios of semantic embedding. The impact of different iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Auxiliary Experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1 Unsupervised algorithm for Building Entity Alignment seeds (UBEA)Input: E s , E t , SE(Semantic Embedding) //SE: GloVe embedding; Output: P LS //Pseudo-Labels Set //Construct pseudo-labels set through graph interaction divergence.</figDesc><table><row><cell>3:</cell><cell>e s , e t = SE[e s ], SE[e t ]</cell></row><row><cell>4:</cell><cell>A=sorted(GID(e s , e t ))</cell></row><row><cell>5:</cell><cell>B=sorted(GID(e s , e t ))</cell></row><row><cell>6:</cell><cell>for e s , e t in A do</cell></row><row><cell>7:</cell><cell>if B[e s ][1] = e s : then</cell></row><row><cell>8:</cell><cell>P LS .append[e s , e t ]</cell></row><row><cell>9:</cell><cell>end if</cell></row><row><cell>10:</cell><cell>end for</cell></row><row><cell>11:</cell><cell>if (e s , e t ) in P LS then</cell></row><row><cell>12:</cell><cell>E s .remove(e s )</cell></row><row><cell>13:</cell><cell>E</cell></row></table><note>1: PLS = [ ] 2: for e s in E t and e s in E t do t .remove(e t ) 14:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 2 Iterative Learning (IL) s ∈P s e ∈E W e s v (M (e s ) − M (e) ) 2 = e s ∈P s e ∈E W e s v M 2 (e s ) − 2M (e s ) M (e) + M 2</figDesc><table><row><cell>Input:</cell><cell></cell></row><row><cell cols="2">(1) E s and E t</cell></row><row><cell cols="2">(2) PLS //come from UBEA(Algorithm 1)</cell></row><row><cell cols="2">(3) total number of iterations T</cell></row><row><cell cols="2">1: while t &lt; T do</cell></row><row><cell>2:</cell><cell>E = GateGAT(E s , E t , PLS)</cell></row><row><cell>3:</cell><cell>PLS = UBEA(E s , E t , E)</cell></row><row><cell></cell><cell>// Until the iteration reaches T.</cell></row><row><cell cols="2">4: end while</cell></row><row><cell cols="2">5: return PLS</cell></row><row><cell></cell><cell>L s =</cell></row><row><cell></cell><cell>(14)</cell></row><row><cell></cell><cell>(e)</cell></row><row><cell></cell><cell>L t =</cell></row></table><note>e e t ∈P t e ∈E W e t v (M (e s ) − M (e) ) 2 = e t ∈P t e ∈E W e t v M 2 (e t ) − 2M (e t ) M (e) + M 2 (e)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the DBP15K</figDesc><table><row><cell>Datasets</cell><cell cols="3">Entities Relations Triples</cell></row><row><cell>DBP15K ZH-EN</cell><cell>ZH 66,469 EN 98,125</cell><cell>2,830 2,317</cell><cell>153,929 237,674</cell></row><row><cell>DBP15K JA-EN</cell><cell>JA EN 95,680 65,733</cell><cell>2,043 2,096</cell><cell>164,373 233,319</cell></row><row><cell>DBP15K FR-EN</cell><cell>FR EN 105,889 66,858</cell><cell>1,379 2,209</cell><cell>192,191 278,590</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance on DBP15K</figDesc><table><row><cell>Models</cell><cell>H 1</cell><cell>ZH-EN H 10</cell><cell>MRR</cell><cell>H 1</cell><cell>JA-EN H 10</cell><cell>MRR</cell><cell>H 1</cell><cell>FR-EN H 10</cell><cell>MRR</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Graph structure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MTransE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The ablation experiment of each component of the model.</figDesc><table><row><cell></cell><cell cols="2">ZH-EN</cell><cell></cell><cell>JA-EN</cell><cell></cell><cell>FR-EN</cell></row><row><cell></cell><cell>H 1</cell><cell>H 10</cell><cell>H 1</cell><cell>H 10</cell><cell>H 1</cell><cell>H 10</cell></row><row><cell>SE</cell><cell cols="6">0.586 0.710 0.664 0.784 0.825 0.897</cell></row><row><cell>GAT+L S + S</cell><cell cols="6">0.707 0.931 0.705 0.935 0.729 0.952</cell></row><row><cell>GAT+L S + S + I L</cell><cell cols="6">0.798 0.946 0.793 0.948 0.819 0.962</cell></row><row><cell cols="7">GateGAT+L S + S + I L 0.807 0.940 0.799 0.946 0.840 0.966</cell></row><row><cell>UPLR(w /o L N )</cell><cell cols="6">0.846 0.947 0.883 0.961 0.951 0.990</cell></row><row><cell>UPLR(w /o Gat e)</cell><cell cols="6">0.893 0.971 0.906 0.979 0.959 0.994</cell></row><row><cell>UPLR(w/o IL)</cell><cell cols="6">0.841 0.965 0.851 0.974 0.919 0.991</cell></row><row><cell>UPLR</cell><cell cols="6">0.902 0.970 0.912 0.978 0.967 0.994</cell></row><row><cell cols="7">weight of the noise entity, thereby increasing the critical metric H 1,</cell></row><row><cell cols="7">while H 10 is slightly lower. As discussed in Section 3, UPLR has two</cell></row><row><cell cols="3">novel designs : (1) Gate and (2) L</cell><cell></cell><cell></cell><cell></cell></row></table><note>N . From Table</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by National Key Research and Development Program of China (Grant No. 2020YFC0833402), National Natural Science Foundation of China (Grant Nos. 61976021, U1811262), and Beijing Academy of Artificial Intelligence (BAAI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with pseudoensembles</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ouais</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3365" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-channel graph neural network for entity alignment</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual knowledge graph embeddings for cross-lingual knowledge alignment</title>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating An Optimal Interview Question Plan Using A Knowledge Graph And Integer Linear Programming</title>
		<author>
			<persName><forename type="first">Soham</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabir</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangameshwar</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Indrajit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Palshikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching contextualized language model from knowledge graph for biomedical information extraction</title>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep graph matching consensus</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SMR: Medical knowledge graph embedding for safe medicine recommendation</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haofen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyue</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">100174</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to exploit long-term relational dependencies in knowledge graphs</title>
		<author>
			<persName><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semisupervised named entity recognition in chinese social media</title>
		<author>
			<persName><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised entity alignment via joint knowledge embedding model and cross-graph model</title>
		<author>
			<persName><forename type="first">Chengjiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guiding cross-lingual entity alignment via adversarial knowledge embedding</title>
		<author>
			<persName><forename type="first">Xixun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual Pivoting for (Unsupervised) Entity Alignment</title>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Boosting the Speed of Entity Alignment 10*: Dual Attention Matching Network with Normalized Hard Sample Mining</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MRAEA: an efficient and robust entity alignment approach for cross-lingual knowledge graph</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relational Reflection Entity Alignment</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semisupervised entity alignment via knowledge graph embedding with awareness of degree difference</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rea: Robust cross-lingual entity alignment between knowledge graphs</title>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised Knowledge Graph Alignment by Probabilistic Reasoning and Semantic Embedding</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejia</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transductive semi-supervised deep learning using min-max features</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Zhiheng MaXiaoyu Tao, and Nanning Zheng</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labeled nodes</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge Association with Hyperbolic Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-lingual entity alignment via joint attribute-preserving embedding</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bootstrapping Entity Alignment with Knowledge Graph Embedding</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transedge: Translating relation-contextualized embeddings for knowledge graphs</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingbing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge graph alignment network with gated multi-hop neighborhood aggregation</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Entity Alignment between Knowledge Graphs Using Attribute Embeddings</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Trsedya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge graph alignment via graph convolutional networks</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Knowledge Graph Alignment with Entity-Pair Embedding</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoju</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relation-aware entity alignment for heterogeneous knowledge graphs</title>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Jointly learning entity and relation representations for entity alignment</title>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge graph alignment via graph matching neural network</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-lingual knowledge graph alignment via graph matching neural network</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic Knowledge Graph Alignment</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aligning cross-lingual entities with multi-aspect information</title>
		<author>
			<persName><forename type="first">Hsiu-Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Collective Embedding-based Entity Alignment via Adaptive Features</title>
		<author>
			<persName><forename type="first">Wexin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuyang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neighborhood-Aware Attentional Representation for Multilingual Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Relation-Aware Neighborhood Matching Model for Entity Alignment</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonghai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingpeng</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
