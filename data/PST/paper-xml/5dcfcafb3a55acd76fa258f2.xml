<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Poisoning Attack in Federated Learning using Generative Adversarial Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiale</forename><surname>Zhang</surname></persName>
							<email>jlzhang@nuaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjun</forename><surname>Chen</surname></persName>
							<email>chenjj@mail.buct.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Information Science &amp; Technology</orgName>
								<orgName type="institution">Beijing University of Chemical Technology</orgName>
								<address>
									<postCode>100029</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<postCode>2007</postCode>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<postCode>211106</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shui</forename><surname>Yu</surname></persName>
							<email>shui.yu@uts.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Poisoning Attack in Federated Learning using Generative Adversarial Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F66BC05A4ED69837F2A8288B4DB4134</idno>
					<idno type="DOI">10.1109/TrustCom/BigDataSE.2019.00057</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Federated learning</term>
					<term>poisoning attack</term>
					<term>generative adversarial nets</term>
					<term>security</term>
					<term>privacy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Federated learning is a novel distributed learning framework, where the deep learning model is trained in a collaborative manner among thousands of participants. The shares between server and participants are only model parameters, which prevent the server from direct access to the private training data. However, we notice that the federated learning architecture is vulnerable to an active attack from insider participants, called poisoning attack, where the attacker can act as a benign participant in federated learning to upload the poisoned update to the server so that he can easily affect the performance of the global model. In this work, we study and evaluate a poisoning attack in federated learning system based on generative adversarial nets (GAN). That is, an attacker first acts as a benign participant and stealthily trains a GAN to mimic prototypical samples of the other participants' training set which does not belong to the attacker. Then these generated samples will be fully controlled by the attacker to generate the poisoning updates, and the global model will be compromised by the attacker with uploading the scaled poisoning updates to the server. In our evaluation, we show that the attacker in our construction can successfully generate samples of other benign participants using GAN and the global model performs more than 80% accuracy on both poisoning tasks and main tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Federated learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> is a recent concept which enables to train a deep learning model across thousands of participants in a collaborative manner. The main purpose of federated learning is to build a joint machine learning model upon localized datasets while providing privacy guarantee, which is an attractive technique for numerous emerging scenarios, such as edge computing and crowdsourced system <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Participants in federated learning act as the data provider to train a local deep model, and the server maintains a global model by averaging local model parameters (i.e., gradients) generated by randomly selected participants until it tends to convergence. One biggest achievement for federated learning is the corresponding model average algorithm <ref type="bibr" target="#b4">[5]</ref>, which can benefit from a wide range of non-IID and unbalanced data distribution among diversity participants. However, the server in federated learning is designed to have no privilege to access the participants' local data and training process due to the privacy concern. Such invisibility property would introduce a severe security threat -poisoning attack <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>. As one type of causative attack <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, poisoning attack enables an attacker to manipulate partial training data with attacking labels to change the model parameters of the target learning model in training phase, and then the poisoned learning model would have some attacker expected properties to misclassify the chosen inputs at the inference stage <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. We notice that it is possible to construct the poisoning attack in federated learning for the following reasons <ref type="bibr" target="#b5">[6]</ref>: 1) there are plenty of participants in federated learning system, it is most likely contains one or more malicious users; 2) since the participants' local data and training process are invisible to the server, it is impossible to verify the authenticity of a certain participant's update <ref type="bibr" target="#b12">[13]</ref>; 3) the local updates generated by multiple participants may be very different from each other and the secure aggregation protocol <ref type="bibr" target="#b13">[14]</ref> makes the local updates cannot be audited by the server.</p><p>In this work, we first devise a novel poisoning attack in federated learning based on Generative Adversarial Nets (GAN) <ref type="bibr" target="#b14">[15]</ref>, with the aim of successfully launching the poisoning attack without compromise any benign participant. Meaning that the attacker in our construction does not need to invade other participants' validation dataset to obtain the original local training data. Apparently, our attack is more stealth compared to the conventional poisoning attacks, because of we do not require the attacker to invade any participant's device and no need to struggle with intrusion detection mechanism embedded in the local system <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> presents an example of poisoning attack based on flipping labels in federated learning. In each communication round, the central server distributes the current global model M t to the participants, and then each participant trains it on the local dataset to generate the model update. On contrast, the attacker poisons the training data by flipping labels and train the model on the poisoned data. Finally, the attacker uploads the poisoned update along with other participants' updates to the server. After federated averaging, the global model will be replaced by poisoned model M t+1 .</p><p>In our construction, the attacker firstly acts as a benign insider to execute the federated learning protocol and deploys a GAN architecture to reconstruct the private training data from other participants, and then the attacker adds wrong labels to the GAN generated data. After that, those poisoned data is injected into the training procedure to obtain the local model updates to further compromise the global model. Finally, the poisoned global model would present high probability of the classes being classified as the attacker-chosen classes and have no influence about other non-poisoned classes. For example, misclassify '2's to class '7' and correctly classify '9's to class '9' in Fig. <ref type="figure" target="#fig_0">1</ref>. Note that the local learning procedure is fully controlled by the attacker, so he can change the training configurations such as the number of epochs and learning rate to make the poisoned global model performs well on both main task and poisoned task.</p><p>Our main contributions can be summarized as follows.</p><p>• We first demonstrate that the federated learning architecture is vulnerable to the poisoning attack which can be launched by any malicious participant. Then, we devise a novel poisoning attack based on GAN and successfully construct it in the federated learning system. • Our attack is generic and feasible under a realistic threat model, which only requires the attacker acting as a benign insider to participant in federated learning system and deploying a GAN architecture in the local to mimic other participants' private training data. • We conduct exhaustive experiments to demonstrate the effectiveness of both GAN-based generative method and poisoning attack approach in federated learning. On the MNIST and AT&amp;T datasets, the global model performs more than 80% classification accuracy on both poisoning tasks and main tasks. The rest of this paper is organized as follows. Section II-A introduces the background knowledge about federated learning and generative adversarial nets. Section III discusses the threat models, and then the proposed poisoning attack is detailed in Section IV. Experimental evaluation is conducted in Section V and the simple remarks about our attack are given in Section VI. Finally, Section VII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Federated Learning</head><p>Traditionally, a centralized training method require users to upload their personal data to train a model with all the collected data, which the users' privacy is under high leaking risk <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In contrast, federated learning can significantly preserve privacy by its typical distribution learning scheme. Unlike centralized method, federated learning only requires users to upload gradients which generated by the local training data and models, and the global models on the server side will share the same structure with local client models. Suppose there are n of users with same objective to join the federated learning, and the local dataset are totally different among users. For each iteration, users will download the parameters and global model from the server side, and then train the model by the local datasets by each client. Each user will upload the gradients after training stage, where the uploaded gradients will be averaged and accumulated to the current global model. Eq. 1 shows the updating procedure on the global model.</p><formula xml:id="formula_0">m i+1 = m i + 1 n n k=1 g k i (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where m i+1 indicates the current shared model at the i-th iteration, and g k i represents the gradient uploaded by the k-th user at iteration i. A federated learning framework can achieve high satisfaction when the users download the same model with the same initialization, which is averaged by the global model with all the valid uploads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generative Adversarial Nets</head><p>Generative adversarial nets (GAN) <ref type="bibr" target="#b14">[15]</ref> achieved extraordinary success in computer vision research area which can generate high quality fake images based on the original image set. There are two neural networks namely generator and discriminator in the GAN structure, where generator (G) generates images and discriminator (D) discriminate if the image is from generator or original image set which can be represent as 0/1 (fake/real). To train a GAN, G will generate an image from a random vector z which follows a prior distribution such as Gaussian or uniform distribution. The generated image will be the input of D. In the meantime, D is pre-trained by the original image set which can be used to discriminate the input is real or fake. The performance of both G and D can be improved by playing the adversarial game. Eq. 2 shows the training progress of GAN.</p><formula xml:id="formula_2">min G max D V (D, G) =E x∼pdata(x) [log D(x)]+ E z∼pz(z) [log(1 -D(G(z)))],<label>(2)</label></formula><p>where p data (x) indicates the original image distribution and p z (z) represents the distribution of the random vector z. D and G will be trained by several rounds until the adversarial game achieves the Nash equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THREAT MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Scenario</head><p>Following the federated learning protocol as described in Sec. II-A, our threat model considers multiple participants (N ≥ 2) to jointly train a global model on their localized training dataset. We assume there existing one or more attackers among these participants, whose main purpose is to mimic another participants' dataset and poison the global model. By contrast, we assume that the central server is not malicious and most part of participants is trusted except with the attackers.</p><p>In our settings, the attacker can only access to the global model through the federated learning procedure, but he cannot control the aggregation or average algorithm at server side. Besides, all the attackers also cannot affect any other benign participants' learning phase or training data D benign , while they must correctly execute the training algorithm on their poisoned dataset D poison according to the federated learning protocol to update their local models. Without loss of generality and simplicity, the global model plays a role as an image classifier in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attacker's Goals</head><p>The attacker under our construction aims to corrupt the global model in the training phase, while disguised as a benign participant. In the federated learning procedure, the attacker deploys a GAN architecture to generate similar samples which belong to other benign participants, then he inserts poison samples assigned with the wrong label to the local training dataset. The local model will be trained on these poisoned data to further generate the poisoned local update. After the model average at the server side, the prediction results on parts of benign participants' inputs will be changed to the attackerchosen classes.</p><p>The attacker's goals can be summarized as follows: (1) Samples generation: the attacker can successfully mimic the prototypical samples from the benign participants' training data without access their local dataset directly. ( <ref type="formula" target="#formula_2">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attacker's Capability</head><p>The attacker pretends to be an honest participant in the federated learning protocol but tries to compromise the global model by uploading the poisoned local updates trained on attack-poisoned fake dataset. These poisoned samples were totally generated by the attacker who trains a GAN to extract the prototypical samples of the other participants' training data he does not own. Specifically, we consider the following attacker's capabilities: (1) Active: the attack is assumed as an active insider since he can deploy a GAN architecture locally, modify any training inputs, manipulate the training procedure, or even change the scale of local update. ( <ref type="formula" target="#formula_2">2</ref>) Knowledgeable: the attacker has full knowledge of the model structure because all the participants in federated learning are agreed in advance on a common learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ATTACK CONSTRUCTION A. Attack Overview</head><p>To conduct our poisoning attack in federated learning, the attacker needs to execute the following strategy with three steps: firstly generates the poisoning samples based on generative adversarial nets model and adds these samples into the local training dataset, and then injects the generative samples with wrong label into the training procedure to compute the poisoned local parameters, lastly uploads the magnified parameters to the central server, aiming to misleading the global model to misclassify the correct inputs as the target wrong label at inference stage. For simplicity, we firstly consider two participants (one attacker A and one benign participant P) federated learning scenario to explain the construction of our proposed poisoning attack and the more complexity attack strategy will be described in Sec. IV-C. Fig. <ref type="figure" target="#fig_1">2</ref> presents the detailed procedure of our proposed GANbased poisoning attack. Assuming P and A train the data from class a and class b, respectively, and these two participants agree on a common learning objective and model structure as required in federated learning protocol. In this case, the information from class a is invisible for A. The attacker's goal is to mimic the samples from class a and further launch the poisoning attack. Thus, A locally deploys a GAN (unknown to the central server and P) to generate as much similar samples as possible about class a of participant P, and further injects these generated samples from class a, as class b into the local training procedure. In this way, the attacker can simply train his local model on the generated poison data and uploads the resulting parameters to the central server. Thus, the poisoning attack can be successfully launched in federated learning by scaling up the poisoned updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Poisoning Data Generation</head><p>The GAN procedure is an adversarial game between two neural networks, namely discriminator D and generator G. The discriminator is trained to distinguish between original data and generated data, while the generator is trained to mimic samples from the training set of the discriminative network.</p><p>Inspired by <ref type="bibr" target="#b19">[20]</ref>, to mimic samples from another participants in federated learning, we adopt a GAN architecture at the attacker side, which creates a replica of the global model as the discriminator. With the continuous execution of the federated learning protocol, each participant's updates will promote the convergence of the global model. Correspondingly, as a replica of the global model, the discriminator of GAN will be synchronously updated to guide the right direction for the generator. In this way, the attacker can generate high-quality mimic samples from other participants' dataset.</p><p>For example, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the goal of the attacker is to mimic as much samples (digital 2 from the MNIST dataset) as possible from other participants' training data (class b) that he does not own. Firstly, the attacker downloads the global model and duplicates a copy as the discriminator D to build a localized GAN. Then, the coupled generator G will send the generated image to the discriminator to evaluate the generated image is real or fake. In the meantime, the attacker will keep joining the federated learning procedure to get the latest global model and further update his discriminator. This training scheme will continue several rounds until the generator can generate high-quality images on class b.</p><p>Non-coincidentally, the coupled generator G would generate mimic samples that are very similar to the original samples if the attacker keep joining in the federated learning. This is because the attacker can use the high-performance global model as the discriminator to generate high-quality fake samples which do not belong to the attacker. These fake samples can be the poison to affect the classification performance on specific target classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GAN-based Poisoning Attack</head><p>From the above analysis, the key point for constructing our poisoning attack is that the attacker inserts the generated poisoning samples with artificial wrong label into his own training dataset and further uploads the poisoned model updates ΔL p to the central server. The proposed poisoning attack can be formulated as below: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Experiment Setup 1) MNIST:</head><p>MNIST is one of the benchmark datasets used in lots of deep learning scenarios which contains 70000 handwritten grayscale digits images ranging from 0 to 9 (i.e., 10 classes). Each image is with the size of 28×28 pixels, and the whole MNIST dataset is divided into the 60000 training records and 10000 testing data records.</p><p>2) AT&amp;T: AT&amp;T (Olivetti dataset) is another widely used dataset with 400 grayscale face images from 40 different persons (10 images for one person). The images in AT&amp;T were taken several times and present large difference in facial expression. The original size of AT&amp;T is 92×112.</p><p>3) Experiment Setup: As described in Sec. IV, the components involved in our proposed poisoning attack are the classifier, discriminator and the generator. We use the Convolutional Neural Network (CNN) based architecture to construct the classifier and discriminator, and the corresponding generator is constructed with deconvolution network. Table <ref type="table" target="#tab_2">I</ref>  ) to the central server end the inputs of two datasets as 64×64 in our experiments. For the classifier and discriminator, they share the same network structure where the model consists of 4 convolution layers and 2 dense layers. The kernel size of the first three convolutional layers is 4×4 and the last layer has a kernel of size 3×3. The strides for these convolutional layers are 2, 2, 4, and 1, respectively. In particular, we randomly sampled ten classes for AT&amp;T dataset to ensure the output is the same with MNIST dataset. For the generator, the kernel size is 4×4 and the input is adopted with random noise, which length is 512. The activation functions applied to both generator and discriminator are ReLU and LReLU, respectively.</p><p>Our experiments include two scenarios: single attacker and multiple attackers. For one attacker scenario, we set the number of participants is m = 10, where one participant is assumed as the attacker and the remaining 9 participants are benign. To explain the capability of our GAN-based generative model, we assign the training dataset for one class to the attacker and the remaining nine classes are allocated to benign participants. The multiple attackers setting is basically the  We implemented the experiments on a RHEL7.5 server with NVidia Quadro P4000 GPU and 32GB RAM.</p><formula xml:id="formula_3">64 2 × 1 Conv (stride=2) --------→ 32 2 × 128 Conv (stride=2) --------→ 16 2 × 256 Conv (stride=4) --------→ 4 2 × 512 Conv (stride=1) --------→ 2 2 × 1024 FC -→ 4096</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Effectiveness of Generative Model</head><p>We evaluate our poison data generative model under one attacker settings in federated learning, which the number of total participants N = 10. To mimic samples from other participants' training dataset, we adopt the GAN architecture at the attacker side, where the discriminative network of GAN is as same as the global model in federated learning protocol. We also set the attacker starts to generate samples after the accuracy of the global model is reached 95%. Fig. <ref type="figure" target="#fig_3">3</ref> shows the resulted samples generated by attacker in the federated learning. The 1st and 3rd rows are the real samples from other participants. The 2nd and 4th rows are the generated samples based on our poison data generative model. Note that we present the generated results about ten classes for MNIST dataset and six classes for AT&amp;T dataset, that is because we repeat our experiments for many times so that the participants can traverse almost all classes of two datasets. The results demonstrate that our GAN-based generative model can successfully mimic participants' original samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head><p>In our experiments, we define the poisoning task accuracy to represent the success rate of classify results turn to the attackchosen classes by the averaged global mode in the testing dataset, and the main task accuracy as the success rate of correctly classify except with the poisoned items. For MNIST dataset, each attack trains 14 epochs for the local model on the poisoned dataset to improve the poisoning task accuracy in the global model and the experiments are run 150 rounds in the setting of federated learning. For AT&amp;T dataset, we run the experiments for 50 rounds. Followed by <ref type="bibr" target="#b5">[6]</ref>, we also set a 10% degradation of attacker's learning rate and to further ensure the effectiveness of our poisoning attack.</p><p>Figs. 4(a) and 4(b) present the results of image classification accuracy on MNIST and AT&amp;T datasets where one attacker is selected to participant the federated learning system. We show all the 150 rounds after the poisoning attack. After the attacker starts to upload his poisoned update, the poisoning task accuracy has reached almost 100% immediately, that is because the poisoned update was generated by a welltrained local model on the poisoned data. With the continuous execution of the federated learning protocol, the benign participants will submit more and more normal updates to struggle with the attacker's poisoning updates and further affect the success rate of poisoning task. Despite this, our poisoning task accuracy can be kept 90% on average. We also design a protecting mechanism in our code, which the degradation of the attacker's learning rate will be canceled when the poisoning task accuracy is lower than 80%. So that we can see the accuracy shows large jitter between 80%-100% in Fig. <ref type="figure" target="#fig_4">4</ref>. Besides, the main task accuracy is almost unaffected and maintained at around 85% in the overall 150 communication rounds.</p><p>We also evaluate our attack with different scaling factors in the single attacker setting. We run the experiments for 50 rounds to compare the mean success of our attack under different scale factors, i.e., 20-100. Figs. 5(a) and 5(b) show that our poisoning attack can achieve high accuracy on both MNIST and AT&amp;T datasets. Specifically, when the scaling factor exceeds 40, our poisoning attack can achieve more than 80% mean accuracy. Fig. <ref type="figure" target="#fig_5">5</ref> also shows the poisoning task accuracy can achieve 60% even with a small-scale factor, e.g., 20. This is mainly benefited from training the poisoning  dataset for multiple epochs in the attacker's local training procedure. Moreover, we notice that different scaling factors have no big influence on the accuracy of the main tasks, meaning that our attack does not break the global model's performance except with poisoned tasks. Finally, we conducted a series of experiments to evaluate the impact of different attackers on the accuracy of the global model. Fig. <ref type="figure" target="#fig_6">6</ref> shows the mean accuracy of our poisoning and main tasks when multiple attackers are selected in a single round. We also run 50 communication rounds under different attackers' settings on MNIST and AT&amp;T datasets to compare the mean accuracy. From the experimental results, we can see that the accuracy of poisoning tasks maintains a stable trend with over 90% success rate and the main tasks accuracy keeps steady around 85% success rate which is not affected as the number of attackers increases. Furthermore, as respected by the black dash line in Figs. <ref type="figure" target="#fig_6">6(a</ref>) and 6(b), the scale factors can be split by deploying multiple attackers in the federated learning system, meaning that it is possible to evade unknown detection mechanism by increasing the proportion of attackers in all participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. REMARKS</head><p>As detailed in aforementioned Sections, we devise a poisoning attack in federated learning based on generative adversarial nets. The key point for successfully launching our attack is to deploying a GAN architecture in the attacker side which can mimic samples from other participants' training datasets. As discussed in <ref type="bibr" target="#b19">[20]</ref>, the validity of this GAN can be greatly guaranteed if the accuracy of the shared model improves over time, which is the main idea about federated learning. Moreover, the GAN-based generative model cannot be detected due to the attacker in our construction pretends to be an honest participant in the federated learning protocol. Therefore, the effectiveness of our proposed poisoning attack can be ensured.</p><p>Besides, existing defense mechanisms against poisoning attacks, such as robust losses <ref type="bibr" target="#b21">[22]</ref> and anomaly detection <ref type="bibr" target="#b22">[23]</ref>, are not suitable for federated learning because they all require the detector to access the participants' training data and training model, which is contradicted with the design idea of federated learning. For the defense, as our poisoning attack relies on GAN to mimic other participants' training data, it is possible to design a novel federated learning framework which hides the classification results of the global model to prevent attackers from using the GAN to obtain other participants' private training data, and ultimately to defend against poisoning attacks initiated by internal participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>In this work, we propose a novel poisoning attack against the federated learning system. We show that poisoning attacks can be launched by any internal participant, who deploying a generative adversarial nets (GAN) framework to mimic other participants' training samples. Our attack is more generic and effective than existing data poisoning attacks under realistic threat assumptions. The reason is that the attacker in our construction only need to act as a benign insider to participant in federated learning system and generate other participants' private training data, while previous data poisoning attacks require a powerful attacker to invade other participants' training procedure. Via evaluations on two benchmark datasets, we find that our poisoning attack can successfully be launched by an internal participant using GAN and the global model performs high accuracy on both poisoning tasks and main tasks. In regards of future work, we plan to explore a robust federated learning system which can provide sophisticated anomaly detection guarantees on the server side to against the type of active attack by internal participants.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustrating of poisoning attack in federated learning. The attacker labels '7' on the target samples and trains the model Mt at iteration t on the modified data. After model average, the global model is replaced by poisoned model M t+1 at iteration t + 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed GAN-based poisoning attack in the federated learning. The attacker generates one or more classes training data, training a model on the poisoned data using federated learning protocol, and submits the resulting model. After federated averaging, the global model is replaced by the attacker's poisoned model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) Poisoned task accuracy: once the poisoned local updates trained on generated faker samples were uploaded to the central server, the global model should perform high accuracy on the poisoned task for several rounds after the attack. (3) Main task accuracy: the global model should also present high accuracy on main tasks (non-poisoned) to prevent the global model from being discarded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results generated when attacker runs a GAN trained on MNIST and AT&amp;T datasets. same as one attacker scenario except with the number of the total participants is 100. During the training of the global classifier, each benign participant trains for E = 2 local epochs with the learning rate of lr = 0.1. Each attacker trains for E = 6 local epochs with the initial learning rate of lr = 0.05, and lr drops by 10% every two epochs. We run all experiments for 100 communication rounds of federated learning. At each communication round, the participants' models are trained individually and sequentially before being averaged into the global model. All the experiments are conducted under the federated learning settings using the PyTorch [21] framework. We implemented the experiments on a RHEL7.5 server with NVidia Quadro P4000 GPU and 32GB RAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Accuracy of poisoning task and main task with single attacker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Impacts of different scaling factors with single attacker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Impacts of different number of attackers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Step 1: Assuming P and A are two participants in the federated learning system, who own the different private training dataset (class a and b); • Step 2: Run the federated learning protocol for several rounds to upgrade the global model until the accuracy reaches a certain level; Scales up the ΔL p with deliberated factor λ and sends the poisoned update λΔL p to the central server; • Step 5: Repeat Steps 3 and 4 until the global model convergence. In particular, the substeps from (2) to (5) in step 4 represent the poison data generative model and the poisoning attack model that the attacker performed covertly to generate as much as possible samples of the targeted class b and further poison the global model. Note that the efficiency of our GAN-based generative model depends only on the accuracy of the global model (discriminator). The generalized poisoning attack with multiple attackers is depicted in Algorithm 1.</figDesc><table><row><cell>• Step 3: For the participant P:</cell></row><row><cell>(1) Downloads the global model to replace the parameters</cell></row><row><cell>of her local model;</cell></row><row><cell>(2) Trains the model on the local dataset and uploads the</cell></row><row><cell>local update ΔL i to the central server;</cell></row><row><cell>• Step 4: For the attacker A:</cell></row><row><cell>(1) Downloads the global model to replace the parameters</cell></row><row><cell>of his local model;</cell></row><row><cell>(2) Creates a replica of the new local model as D</cell></row><row><cell>(discriminator) and runs G (generator) on D to mimic</cell></row><row><cell>samples targeting class b from the participant P;</cell></row><row><cell>(3) Labels the generated samples of class a as class b</cell></row><row><cell>and inserts the poisoned data samples to the A's local</cell></row><row><cell>dataset;</cell></row><row><cell>(4) Trains his local model based on the merged dataset</cell></row><row><cell>and generates the poisoned local model update ΔL p ;</cell></row><row><cell>(5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>shows the network structures for MNIST and AT&amp;T. Note that we resize Algorithm 1: Poison Attack in Federated Learning.</figDesc><table /><note><p>Input: Global model M t ; Participants' updates ΔL i t ; Loss function ; Learning rate η. Output: Poisoned updates Δ Li t . Initialize generator G and discriminator D for t ∈ (1, 2, • • • , T ) do // Server execution Send M t to the participants Recive updates from participants: ΔL i t+1 Update the globa model: M t+1 // Participants execution Replace the local model: L i t ← M t if the user type is A then Initialize D by the new local model L i t for each epoch e ∈ (1, • • • , E) do Run G on D for targeted class Using D to update G Generate samples of targeted class by G Assign wrong label to generated samples Insert poison data to the local dataset D for each batch b p ∈ D poison do L p t+1 = L p t+1η adv ∇ (L p t , b p ) end end Calculate poisoned update: ΔL p t+1 = L p t+1 -L p t Scale up the update: Δ Lp t+1 = λΔL p t+1 ; end else Calculate benign update: ΔL i t+1 = L i t+1 -L i t ; end Upload the local update ΔL i t+1 (including Δ Lp t+1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I NETWORK</head><label>I</label><figDesc>STRUCTURE FOR MNIST AND AT&amp;T</figDesc><table><row><cell>Classifier/</cell></row><row><cell>Discriminator</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2019" xml:id="foot_0"><p>18th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/13th IEEE International Conference On Big Data Science And Engineering</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported in part by the National Key Research and Development Program of China, under Grant 2017YFB0802303, in part by the National Natural Science Foundation of China, under Grant 61672283, and in part by the Postgraduate Research &amp; Practice Innovation Program of Jiangsu Province under Grant KYCX18 0308.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Federated Learning: Strategies for Improving Communication Efficiency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacon</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1610.05492" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Federated Multi-task Learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Annual Conference on Neural Information Processing Systems. (NIPS)</title>
		<meeting>32nd Annual Conference on Neural Information essing Systems. (NIPS)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
			<biblScope unit="page" from="4427" to="4437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Privacy-Preserving Deep Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Conference on Computer and Communications Security. (CCS)</title>
		<meeting>22nd ACM Conference on Computer and Communications Security. (CCS)<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
			<biblScope unit="page" from="1310" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Federated Machine Learning: Concept and Applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Communication-Efficient Learning of Deep Networks from Decentralized Data</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th International Conference on Artificial Intelligence and Statistics. (AISTATS)</title>
		<meeting>20th International Conference on Artificial Intelligence and Statistics. (AISTATS)<address><addrLine>Fort Lauderadale, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">How To Backdoor Federated Learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Estrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno>arxiv.org/abs/1807.00459</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>N-Rotaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th IEEE Symposium on Security and Privacy. (S&amp;P)</title>
		<meeting>39th IEEE Symposium on Security and Privacy. (S&amp;P)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05">May. 2018</date>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting Unintended Feature Leakage in Collaborative Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th IEEE Symposium on Security and Privacy. (S&amp;P)</title>
		<meeting>40th IEEE Symposium on Security and Privacy. (S&amp;P)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05">May. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Poisoning Attacks against Support Vector Machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th International Conference on Machine Learning. (ICML)</title>
		<meeting>29th International Conference on Machine Learning. (ICML)<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page">18071814</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Poisoning Attacks to Graph-Based Recommender Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Annual Computer Security Applications Conference. (ACSAC)</title>
		<meeting>34th Annual Computer Security Applications Conference. (ACSAC)<address><addrLine>San Juan, PR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Is feature selection secure against training data poisoning?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32th International Conference on Machine Learning. (ICML)</title>
		<meeting>32th International Conference on Machine Learning. (ICML)<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1712.05526" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contamination Attacks and Mitigation in Multi-Party Machine Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ohrimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33th Annual Conference on Neural Information Processing Systems. (NIPS)</title>
		<meeting>33th Annual Conference on Neural Information essing Systems. (NIPS)<address><addrLine>Montral, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
			<biblScope unit="page" from="6604" to="6616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Practical Secure Aggregation for Privacy-Preserving Machine Learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kreuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ACM ACM Conference on Computer and Communications Security. (CCS)</title>
		<meeting>24th ACM ACM Conference on Computer and Communications Security. (CCS)<address><addrLine>Dallas, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="1175" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>P-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>W-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annual Conference on Neural Information Processing Systems. (NIPS)</title>
		<meeting>29th Annual Conference on Neural Information essing Systems. (NIPS)<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Certified Defenses for Data Poisoning Attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Annual Conference on Neural Information Processing Systems. (NIPS)</title>
		<meeting>32nd Annual Conference on Neural Information essing Systems. (NIPS)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12">Dec. 2017</date>
			<biblScope unit="page" from="3520" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th IEEE Symposium on Security and Privacy. (S&amp;P)</title>
		<meeting>40th IEEE Symposium on Security and Privacy. (S&amp;P)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05">May. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Learning with Differential Privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23th ACM ACM Conference on Computer and Communications Security. (CCS)</title>
		<meeting>23th ACM ACM Conference on Computer and Communications Security. (CCS)<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">Oct. 2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Privacy-Preserving Deep Learning via Additively Homomorphic Encryption</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Moriai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1333" to="1345" />
			<date type="published" when="2018-05">May. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hitaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>P-Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ACM ACM Conference on Computer and Communications Security. (CCS)</title>
		<meeting>24th ACM ACM Conference on Computer and Communications Security. (CCS)<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
			<biblScope unit="page" from="603" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">PyTorch Examples</title>
		<ptr target="http-s://github.com/pytorch/examples" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auror: defending against poisoning attacks in collaborative deep learning systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd Annual Computer Security Applications Conference. (ACSAC)</title>
		<meeting>32nd Annual Computer Security Applications Conference. (ACSAC)<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">Dec. 2016</date>
			<biblScope unit="page" from="508" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mitigating Poisoning Attacks on Machine Learning Models: A Data Provenance Based Approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Safavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>10th ACM Workshop on Artificial Intelligence and Security<address><addrLine>Dallas, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11">Nov. 2017</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
