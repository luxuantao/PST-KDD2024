<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Policy-Gradient Training of Fair and Unbiased Ranking Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Himank</forename><surname>Yadav</surname></persName>
							<email>himankyadav1@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Thorsten Joachims</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Policy-Gradient Training of Fair and Unbiased Ranking Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3404835.3462953</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>fairness</term>
					<term>bias</term>
					<term>learning-to-rank</term>
					<term>counterfactual learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While implicit feedback (e.g., clicks, dwell times, etc.) is an abundant and attractive source of data for learning to rank, it can produce unfair ranking policies for both exogenous and endogenous reasons. Exogenous reasons typically manifest themselves as biases in the training data, which then get reflected in the learned ranking policy and often lead to rich-get-richer dynamics. Moreover, even after the correction of such biases, reasons endogenous to the design of the learning algorithm can still lead to ranking policies that do not allocate exposure among items in a fair way. To address both exogenous and endogenous sources of unfairness, we present the first learning-to-rank approach that addresses both presentation bias and merit-based fairness of exposure simultaneously. Specifically, we define a class of amortized fairness-of-exposure constraints that can be chosen based on the needs of an application, and we show how these fairness criteria can be enforced despite the selection biases in implicit feedback data. The key result is an efficient and flexible policy-gradient algorithm, called FULTR, which is the first to enable the use of counterfactual estimators for both utility estimation and fairness constraints. Beyond the theoretical justification of the framework, we show empirically that the proposed algorithm can learn accurate and fair ranking policies from biased and noisy feedback. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>‚Ä¢ Information systems ‚Üí Learning to rank; ‚Ä¢ Computing methodologies ‚Üí Learning from implicit feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Implicit feedback from user behavior (e.g., clicks, dwell times, purchases, scroll patterns) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref> is an attractive source of training data for learning-to-rank (LTR), since it gives all users a voice in what the system learns. But does the participatory nature of implicit feedback automatically ensure that the learned ranking policies are fair and lead to desirable effects on the overall health of the ranking system? We argue that both endogenous and exogenous factors can lead to undesirable outcomes as follows.</p><p>Endogenous factors are design choices built into the allocation policies of the ranking system. Specifically, deciding which ranking to present to a user implies an explicit choice of how much exposure each item receives -where higher-ranked items receive more exposure and thus more opportunity <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. Conventional ranking algorithms <ref type="bibr" target="#b39">[40]</ref> make this exposure allocation in a haphazard way, which can lead to unfairness and undesirable dynamics <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. To address this problem, many have noted that there should be an explicit link between the amount of exposure an item receives, and the amount of merit (e.g. relevance) the item has. Specifically, merit-based exposure allocation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> can ensure that items of similar merit (e.g. relevance) also have similar outcomes (e.g. exposure). The particular choice of allocation policy can provide various fairness guarantees and provide a mechanism for steering the system dynamics (e.g. avoiding superstar economics <ref type="bibr" target="#b29">[30]</ref>).</p><p>While merit-based exposure allocation addresses an endogenous factor of unfairness, it is important to recognize that an LTR algorithm can still be unfair due to exogenous biases in the training data. In particular, position bias <ref type="bibr" target="#b25">[26]</ref> makes the use of implicit feedback for LTR challenging. Especially for positive-only feedback like clicks or purchases, highly-ranked items receive more positive feedback due to the increased attention they receive, which further skews the ranking system and affects future rankings. The result can be a dynamic amplification of position bias, leading to phenomena like rich-get-richer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref>. This means the even in the presence of merit-based fairness constraints like <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, naive and biased estimates of merit from implicit feedback signals can still lead to inequities. We, therefore, need to design fair LTR algorithms that ensure unbiased learning despite the biases in the data.</p><p>In this paper, we simultaneously address endogenous and exogenous causes of unfairness and present the first LTR algorithm -called FULTR (Fair Unbiased Learning-to-Rank) -for unbiased learning from logged implicit feedback that can enforce merit-based exposure-allocation constraints. We provide a theoretical analysis to show that a naive combination of conventional merit-based fairness constraints and counterfactual relevance estimators cannot guarantee unbiased and fair ranking policies. To overcome this problem, we define a new type of allocation constraint for groups of items and derive counterfactual estimators <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b44">45]</ref> that provably correct the position bias. To make the proposed framework operational and practical, we show how to search the space of fairness-constrained ranking policies via a novel policy-gradient approach that is unbiased. Note that FULTR is trained end-to-end, and thus can consider fairness during learning, eliminating failure cases of two-step approaches that rely on post-processing an unfair ranking to make it fair <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Beyond the theoretical justification, we also provide an extensive empirical evaluation on real-world datasets. We find that FULTR can effectively optimize utility and fairness over a range of settings even when trained with biased and noisy feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">A Motivating Example</head><p>Consider an online movie streaming service which presents a ranked list of movies for any incoming query. Further, consider the case where the movies were produced by only two studios. Studio ùê∫ 1 produced 30 movies that all have relevance 0.7 (i.e. there is a 70% probability that the user will want to watch each movie). Studio ùê∫ 2 also produced 30 movies, but they all have slightly lower relevance of 0.69. Ranking purely based on relevance to the user would place movies from ùê∫ 1 at spots 1 ‚àí 30 and movies from ùê∫ 2 at spots 31 ‚àí 60, leading to much higher exposure and revenue for movies from ùê∫ 1 than movies from ùê∫ 2 . This can be considered unfair since movies of almost equal merit (i.e. relevance) receive disproportionately worse outcomes (i.e. streaming revenue).</p><p>A first thought for addressing this shortcoming may be to enforce various forms of demographic parity between groups of items in the ranking <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. However, allocating exposure without consideration of merit has other undesirable effects. Assume that there is a third, incompetent, studio ùê∫ 3 that produced another 30 movies of low relevance 0.1. Clearly, these movies do not deserve as much exposure as those from ùê∫ 1 and ùê∫ 2 . We therefore follow an approach that explicitly links exposure to the merit (i.e. relevance) of a group. Specifically, merit-based exposure allocation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> can ensure that groups of similar merit (e.g. relevance) have similar outcomes (e.g. exposure).</p><p>Enforcing merit-based fairness of exposure is a factor endogenous to the ranking system, meaning that it is fully under the control of the system designer. However, it relies on accurate estimates of merit, which can be affected by exogenous factors. Considers, for example, a situation where movies from ùê∫ 1 have consistently been ranked higher than those from ùê∫ 2 in the past. If we now naively count the number of clicks as an estimate of merit, this estimate will be biased by how humans browse ranked lists <ref type="bibr" target="#b25">[26]</ref>. In particular, the movies from ùê∫ 2 are likely to get substantially fewer clicks than those from ùê∫ 1 not because they are substantially worse, but because they were less likely to be discovered by the users. In this way, even an endogenously fair ranking system can become unfair as its past actions create biased estimates that amplify past inequities and that can more generally lead to undesirable system dynamics <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Our approach is the first that can simultaneously control both sources of unfairness when learning a ranking policy, as we will detail in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There have been numerous approaches to defining fairness in different areas of machine learning, including online learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>, classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>, regression <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref>. We focus on fairness in the relatively under-explored domain of LTR, which has only recently caught attention despite its substantial implications in a broad range of real-world applications. To structure the discussion, we follow the distinction of endogenous and exogenous sources of unfairness introduced above.</p><p>Concerning endogenous fairness, several methods have followed the concept of demographic parity which enforces a proportional allocation of exposure between groups without considering merit. This is achieved by either reducing the difference in occurrences of different groups on a subset of the rankings <ref type="bibr" target="#b46">[47]</ref> or by placing a limit on the number of items from each group in the top-k positions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b47">48]</ref>. In contrast, merit-based fairness of exposure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref> allocates exposure to groups based on their merit rather than purely based on group size. However, these methods assume knowledge of the true relevance labels for all items and do not involve learning, while in practice, true relevance labels are not available. and one needs to use other learning methods to impute relevance labels at prediction time <ref type="bibr" target="#b31">[32]</ref>. This leads to a two-step process, where the first regression step is unaware of fairness considerations and fairness is only introduced during post-processing. Instead, our method is trained end-to-end in one step, such that it can, for example, discount features that lead to unfair relevance estimates <ref type="bibr" target="#b43">[44]</ref>.</p><p>More recently, Zehlike and Castillo <ref type="bibr" target="#b48">[49]</ref> proposed an LTR method that incorporates an exposure-based fairness regularizer into the ranking loss, but it only considers the exposure at the top-1 position. The limitation was removed by Singh and Joachims <ref type="bibr" target="#b43">[44]</ref>, who propose an end-to-end LTR method that optimizes both utility and fairness constraints for the full ranking. However, both methods assume that expert-labeled relevances for all items is available during training. This is a weaker assumption than the one made in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>, but it still does not apply to real-world settings where implicit feedback is often used for training.</p><p>Our work focuses on exposure-based fairness, but another approach to defining fairness in rankings is through pairwise comparisons <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>, without considering exposure as the key criterion. Such comparisons between items count all swaps in the ranking equally and do not reflect that swapping items at the top has a stronger effect on exposure than doing so at lower positions.</p><p>Switching to exogenous sources of unfairness, most work on traditional LTR algorithms <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b24">25]</ref> assumes that unbiased relevance judgments by experts are available. However, the field of information retrieval has long been conscious of the biases inherent to implicit feedback and its effect on the ability to rank well <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. In particular, various studies have demonstrated the presence of position bias, where the quantity and quality of the feedback depend on the rank at which an item is presented.</p><p>One approach to modeling and removing position bias is generative click modeling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Click models typically treat relevance as latent variables, and perform inference by maximizing log-likelihood of clicks. These inferred relevances can then be used as a substitute for expert labels in LTR. Unfortunately, most click models suffer from the limitation of requiring large amounts of repeat impressions for individual query-item pairs, which makes them inapplicable to tail queries.</p><p>Recent and more direct approaches to dealing with position bias are counterfactual learning methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b44">45]</ref>. These methods use techniques from causal inference like inverse propensity score (IPS) weighting <ref type="bibr" target="#b40">[41]</ref> and do not require repeated queries or latentvariable inference. Instead, they directly optimize over a debiased utility objective that incorporates click data in a principled fashion. Additional algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> that jointly estimate the propensities and optimize the performance have been proposed. Unlike our proposed FULTR framework, existing counterfactual learning methods do not control for endogenous unfairness. Moreover, we find that naively combining existing counterfactual learning techniques with endogenous fairness constraints fail to control for unfairness.</p><p>The only existing method that aims to address both endogenous and exogenous sources of unfairness in rankings is <ref type="bibr" target="#b31">[32]</ref>. However, it is the two-step method that learns relevance estimation with unbiased regression objectives first. The regression learning is unaware of fairness constraints and may yield unfairness in relevance estimation. We show empirically that our end-to-end FULTR can achieve better fairness-utility tradeoff than the two-step method. Additionally, it requires interactive experimental control in the form of an online algorithm, whereas FULTR can reuse logged implicit feedback data from past interactions for learning. To the best of our knowledge, no other existing method addresses fairness in rankings while directly learning from logged implicit feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING FAIR RANKING POLICIES FROM IMPLICIT FEEDBACK</head><p>In the following, we derive the first learning algorithm for enforcing merit-based fairness of exposure <ref type="bibr" target="#b42">[43]</ref> with logged implicit feedback.</p><p>A key component is a new amortized fairness criterion, and its corresponding disparity measure, for which we show that unbiased counterfactual estimators exist. We analyze the connection between this disparity measure and those in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>. Furthermore, we quantify the effect of click noise on the disparity measure and propose a noise-corrected estimator of disparity. The derivation of the algorithm is based on the standard definition of Learning to rank (LTR), which is the problem of learning a ranking policy ùúã for queries ùëû sampled from a distribution ùëÉ (ùëû). Each query ùëû has a set of candidate items ùíÖ ùëû that need to be ranked. Items can represent a wide variety of things depending on the application -web-pages in a search engine, jobs on a job board, or movies in a streaming service. Each item is associated with a feature vector ùë• ùëû,ùëë that describes the match between item ùëë and query ùëû.</p><p>In the so-called full-information setting, it is assumed that for a training query ùëû the relevances rel ùëû,ùëë of all items ùëë ‚àà ùíÖ ùëû are known. Instead, implicit feedback only reveals the relevances of part of the candidates. We denote this partial feedback as ùëê ùëû and call this the partial-information setting <ref type="bibr" target="#b27">[28]</ref>.</p><p>Unlike most existing works, we consider stochastic ranking policies, where ùúã (ùúé |ùëû) is a distribution over the rankings ùúé of the candidates. As will become clear later, stochastic ranking policies have the advantage of providing more fine-grained control of exposure and enabling gradient-based optimization. Note that deterministic ranking policies are a special case of stochastic ranking policies, where all probability mass lies on a single ranking.</p><p>Like in conventional LTR algorithms <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b24">25]</ref>, our objective is to learn a policy ùúã from policy space Œ† with high expected utility</p><formula xml:id="formula_0">ùëà (ùúã) = E ùëû‚àºùëÉ (ùëû) [ùëà (ùúã |ùëû)] = E ùëû‚àºùëÉ (ùëû) E ùúé‚àºùúã (ùúé |ùëû) ‚àÜ(ùúé, rel ùëû ) ,</formula><p>where ‚àÜ can be any ranking metric (e.g. DCG <ref type="bibr" target="#b23">[24]</ref>).</p><p>Utility optimization for the users, however, does not by itself ensure fairness to the items <ref type="bibr" target="#b42">[43]</ref>. We, therefore, include an additional constraint that addresses endogenous unfairness by enforcing a merit-based allocation of exposure. For simplicity, consider the case of two groups ùê∫ ùëñ and ùê∫ ùëó . A group is simply a collection of items by any criterion, such as gender, price, brand, etc. To ensure that exposure is allocated fairly between ùê∫ ùëñ and ùê∫ ùëó , we measure unfairness via a disparity measure ùê∑ ùëñ ùëó (ùúã) that will be defined in Section 3.2. A perfectly fair ranking policy has zero disparity. More generally, we want to restrict [ùê∑ ùëñ ùëó (ùúã)] 2 to be at most some threshold ùõø to get our disparity to be as close to zero as possible. We define our objective by combining the fairness constraint with the conventional utility optimization.</p><formula xml:id="formula_1">ùúã * = argmax ùúã ‚ààŒ† ùëà (ùúã) s.t. [ùê∑ ùëñ ùëó (ùúã)] 2 ‚â§ ùõø.<label>(1)</label></formula><p>Directly optimizing this objective is not possible, since neither the query distribution ùëÉ (ùëû) nor the relevances rel ùëû are known. We therefore take the following Empirical Risk Minimization (ERM) approach, where both ùëà (ùúã) and ùê∑ ùëñ ùëó (ùúã) are replaced by estimates ùëà (ùúã |ùë∏) and ùê∑ ùëñ ùëó (ùúã |ùë∏) based on a partial-info training set ùë∏.</p><formula xml:id="formula_2">ùúã = argmax ùúã ‚ààŒ† ùëà (ùúã |ùë∏) s.t. ùê∑ ùëñ ùëó (ùúã |ùë∏) 2 ‚â§ ùõø.<label>(2)</label></formula><p>This leads to three key challenges which we address as follows. First, in Section 3.1, we show how to design ùëà (ùúã |ùë∏) to get unbiased utility estimates even if we do not have access to true relevance labels rel ùëû but only have access to biased implicit feedback ùëê ùëû . Second, in Section 3.2, we show how to define estimators of the fairness disparity ùê∑ ùëñ ùëó (ùúã |ùë∏) that ensure merit-based exposure allocation while also being unbiased even with only the implicit feedback ùëê ùëû . Finally, in Section 4, we show how to efficiently solve the resulting training problem from Equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unbiased Utility Estimator</head><p>We consider the class of additive ranking measures that can be expressed as</p><formula xml:id="formula_3">‚àÜ(ùúé, rel ùëû ) = ùëë ‚ààùíÖ ùëû ùëì (ùúé (ùëë)) ‚Ä¢ rel ùëû,ùëë ,</formula><p>where ùúé (ùëë) denotes the rank of item ùëë in ranking ùúé and ùëì () can be any weighting function. For the DCG metric, ùëì (ùúé (ùëë)) = 1/log 2 (1 + ùúé (ùëë)), while for the Average Rank metric, ùëì (ùúé (ùëë)) = ‚àíùúé (ùëë). For simplicity, we assume binary relevances rel ùëû,ùëë ‚àà {0, 1}.</p><p>In the partial-information setting, the relevances rel ùëû are not known and we can only observe ùëê ùëû . What can we infer from ùëê ùëû about rel ùëû ? We defer the discussion of noise to Section 3.4 and consider the standard model that users click if and only if the result is relevant (i.e. rel ùëû,ùëë = 1) and examined by the user. We introduce the binary random variable ùëú ùëû,ùëë indicating whether the item ùëë is examined by the user. Based on this, we can model the "propensity" ùëù (ùëú ùëû,ùëë = 1|ùúé ùëû ), of observing rel ùëû,ùëë given that ranking ùúé ùëû was presented when the implicit feedback was logged. The propensity can be modelled in various ways <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>. Most common is the position-based examination model, where the propensity depends on only the position ùúé ùëû (ùëë) of ùëë in the ranking. This means ùëù (ùëú ùëû,ùëë = 1|ùúé ùëû ) = ùë£ ùúé ùëû (ùëë) , where ùë£ ùëò denotes the examination probability at position ùëò. These position biases ùë£ ùëò can be estimated with swap experiments <ref type="bibr" target="#b27">[28]</ref> or intervention harvesting <ref type="bibr" target="#b3">[4]</ref>. With knowledge of the propensity, we can use Inverse Propensity Score (IPS) weighting to arrive at the following estimator for ‚àÜ <ref type="bibr" target="#b27">[28]</ref> ‚àÜ(ùúé,</p><formula xml:id="formula_4">ùëê ùëû ) = ùëë:ùëê ùëû,ùëë =1 ùëì (ùúé (ùëë)) ùëù (ùëú ùëû,ùëë = 1|ùúé ùëû ) . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>The estimator is unbiased if all propensities are bounded away from zero <ref type="bibr" target="#b27">[28]</ref>. Furthermore, we can define an estimator of the utility as</p><formula xml:id="formula_6">ùëà (ùúã |ùë∏) = 1 |ùë∏ | ùëû ‚ààùë∏ ùëà (ùúã |ùëû) = 1 |ùë∏ | ùëû ‚ààùë∏ E ùúé‚àºùúã (ùúé |ùëû) [ ‚àÜ(ùúé, ùëê ùëû )]. (4)</formula><p>It is easy to verify that this estimator inherits unbiasedness from ‚àÜ. It is also consistent under the same condition on the propensities, thus implying that ùëà (ùúã |ùë∏) will converge to ùëà (ùúã) as |ùë∏ | increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unbiased Fairness Constraints</head><p>To remedy that the utility-maximizing ranking policy can be unfair even if the utility ùëà (ùúã) is perfectly known for all ùúã ‚àà Œ† [43], we enforce additional criteria of how exposure is allocated based on merit (i.e. relevance). In our training problem from Equation (2), this is implemented through the disparity measure ùê∑ ùëñ ùëó (ùúã |ùë∏) in the constraint, which we now define formally.</p><p>As a first step, we define the merit of an item ùëë ùëñ as its relevance, i.e. M ùëû (ùëë) = rel ùëû,ùëë ‚àà {0, 1}. The exposure of an item ùëë in ranking ùúé is defined as the probability of a user examining the item in the ranking. This is identical to the examination probability ùëù (ùëú ùëû,ùëë = 1|ùúé) defined in Section 3.1, but this model is now applied to all rankings instead of just the logged ranking ùúé ùëû . The exposure of ùëë under a stochastic ranking policy ùúã for a query ùëû, denoted as Exp ùëû (ùëë |ùúã), is the expected exposure over all the possible rankings. </p><formula xml:id="formula_7">ùê∑ ùëñ ùëó (ùúã |ùëû) = M ùëû (ùê∫ ùëó ) Exp ùëû (ùê∫ ùëñ |ùúã) ‚àí M ùëû (ùê∫ ùëñ ) Exp ùëû (ùê∫ ùëó |ùúã). (5)</formula><p>As discussed in more detail in the next section, this disparity measure formalizes the principle that proportionality between merit and exposure should hold for a fair ranking <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">43]</ref>. While Exp ùëû (ùê∫ ùëñ |ùúã) is an expectation that can be computed, we do have to estimate ùê∑ ùëñ ùëó (ùúã) with respect to the unknown query distribution and the unknown merits. We therefore take the estimator of ùê∑ ùëñ ùëó (ùúã) as</p><formula xml:id="formula_8">ùê∑ ùëñ ùëó (ùúã |ùë∏) = 1 |ùë∏ | ùëû ‚ààùë∏ ùê∑ ùëñ ùëó (ùúã |ùëû).<label>(6)</label></formula><p>Furthermore, in the partial-information setting, we can get an unbiased estimate of M ùëû (ùê∫ ùëñ ) using the IPS estimator M ùëû (ùê∫ ùëñ ) =</p><formula xml:id="formula_9">ùëë ‚ààùê∫ ùëû ùëñ ùëê ùëû,ùëë</formula><p>ùëù (ùëú ùëû,ùëë =1|ùúé ùëû ) . This means that M ùëû (ùê∫ ùëñ ) in Equation ( <ref type="formula">5</ref>) is replaced with M ùëû (ùê∫ ùëñ ) to arrive at the following empirical disparity measure</p><formula xml:id="formula_10">ùê∑ ùëñ ùëó (ùúã |ùëû) = M ùëû (ùê∫ ùëó ) Exp ùëû (ùê∫ ùëñ |ùúã) ‚àí M ùëû (ùê∫ ùëñ ) Exp ùëû (ùê∫ ùëó |ùúã). (7)</formula><p>Note that ùê∑ ùëñ ùëó (ùúã |ùëû) is unbiased since M ùëû (ùê∫ ùëñ ) is unbiased and the exposure terms are non-random constants.</p><formula xml:id="formula_11">E ùëú ùëû ùê∑ ùëñ ùëó (ùúã |ùëû) = E ùëú ùëû [ M ùëû (ùê∫ ùëó )] Exp ùëû (ùê∫ ùëñ |ùúã) ‚àí E ùëú ùëû [ M ùëû (ùê∫ ùëñ )] Exp ùëû (ùê∫ ùëó |ùúã) = M ùëû (ùê∫ ùëó ) Exp ùëû (ùê∫ ùëñ |ùúã) ‚àí M ùëû (ùê∫ ùëñ ) Exp ùëû (ùê∫ ùëó |ùúã)</formula><p>Since ùê∑ ùëñ ùëó (ùúã |ùëû) is unbiased for each query ùëû, the aggregate</p><formula xml:id="formula_12">ùê∑ ùëñ ùëó (ùúã |ùë∏) is also unbiased for ùê∑ ùëñ ùëó (ùúã), i.e. E ùëû E ùëú ùëû [ ùê∑ ùëñ ùëó (ùúã |ùë∏)] = E ùëû [ùê∑ ùëñ ùëó (ùúã |ùëû)] = ùê∑ ùëñ ùëó (ùúã)</formula><p>. Furthermore, through the use of Hoeffding bounds, it is possible to show that ùê∑ ùëñ ùëó (ùúã |ùë∏) converges to the true disparity ùê∑ ùëñ ùëó (ùúã) as |ùë∏ | increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with other Disparity Measures</head><p>In this section, we further highlight the differences between our disparity measure from Equation ( <ref type="formula">5</ref>) and various other disparity measures.</p><p>The Disparate Treatment constraint from <ref type="bibr" target="#b43">[44]</ref> ‚àÄùëû : Exp ùëû (ùê∫ ùëñ |ùúã)</p><formula xml:id="formula_13">M ùëû (ùê∫ ùëñ ) = Exp ùëû (ùê∫ ùëó |ùúã) M ùëû (ùê∫ ùëó )</formula><p>, expresses that for a given query, the exposure of each group should be proportional to its merit. However, evaluating this constraint requires knowledge of the full relevances which is infeasible to decipher from real-world implicit feedback. Directly replacing rel ùëû,ùëë with ùëê ùëû,ùëë when evaluating M ùëû (ùê∫ ùëó ) leads to biased estimation due to selection bias <ref type="bibr" target="#b27">[28]</ref>. To combat this, naively plugging in the IPS estimator M ùëû (ùê∫ ùëñ ) into the corresponding disparity measure</p><formula xml:id="formula_14">ùê∑ ‚Ä≤‚Ä≤ ùëñ ùëó (ùúã) = Exp ùëû (ùê∫ ùëñ |ùúã) M ùëû (ùê∫ ùëñ ) ‚àí Exp ùëû (ùê∫ ùëó |ùúã) M ùëû (ùê∫ ùëó )<label>(8)</label></formula><p>cannot eliminate bias effectively, since generally</p><formula xml:id="formula_15">E[1/ M ùëû (ùê∫ ùëñ )] ‚â† 1/E[ ùëÄ ùëû (ùê∫ ùëñ )].</formula><p>For an unbiased estimate of 1/M ùëû (ùê∫ ùëñ ) from ùëê ùëû , we need to know the joint distribution of ùëú ùëû,ùëë for all the items in ùê∫ ùëû ùëñ , which is difficult to model. Instead, the proposed disparity measure in Equation ( <ref type="formula">5</ref>) has an unbiased estimator (Equation ( <ref type="formula">7</ref>)) to overcome the selection bias, which we find to provide substantial empirical benefits in Section 5.3.</p><p>Another proposed disparity measure using implicit feedback replaces query-specific exposure and merit with an amortized notion <ref type="bibr" target="#b7">[8]</ref> over the query distribution. The disparity measure</p><formula xml:id="formula_16">ùê∑ ‚Ä≤ ùëñ ùëó (ùúã) = E ùëû [Exp ùëû (ùê∫ ùëñ |ùúã)] E ùëû [M ùëû (ùê∫ ùëñ )] ‚àí E ùëû [Exp ùëû (ùê∫ ùëó |ùúã)] E ùëû [M ùëû (ùê∫ ùëó )]<label>(9)</label></formula><p>expresses that for all the queries, the amortized exposure of each group should be proportional to its amortized merit. We will show how ùê∑ ‚Ä≤ ùëñ ùëó (ùúã) is related to our disparity measure ùê∑ ùëñ ùëó (ùúã) from Equation (5) under two conditions. First, assume that the total exposure of both groups is a constant Exp ùëû (ùê∫ ùëñ ) + Exp ùëû (ùê∫ ùëó ) = ùê∂ ùê∏ . In practice, the top items receive most of the users' attention, so the total exposure for each query is relatively stable even if the size of the candidate set varies. Second, assume that the covariance of the exposure of group ùê∫ ùëó and the total number of relevant items in both groups, cov[Exp(ùê∫ ùëó |ùúã), M ùëû (ùê∫ ùëñ ) + M ùëû (ùê∫ ùëó )], is zero. One sufficient condition for this assumption to be approximately satisfied is when the numbers of relevant items do not vary much between queries. Under these two assumptions, the disparity measures ùê∑ ùëñ ùëó (ùúã) and ùê∑ ‚Ä≤ ùëñ ùëó (ùúã) are equivalent up to a constant coefficient. This can be shown by transforming the disparity measure ùê∑ ‚Ä≤ ùëñ ùëó (ùúã) as follows</p><formula xml:id="formula_17">ùê∑ ùëñ ùëó (ùúã) + cov[Exp ùëû (ùê∫ ùëó |ùúã), M ùëû (ùê∫ ùëñ )] ‚àí cov[Exp ùëû (ùê∫ ùëñ |ùúã), M ùëû (ùê∫ ùëó )] E ùëû [M ùëû (ùê∫ ùëñ )] E ùëû [M ùëû (ùê∫ ùëó )]</formula><p>.</p><p>The difference between the covariance terms is zero under the two assumptions. Therefore, we have</p><formula xml:id="formula_18">ùê∑ ùëñ ùëó (ùúã) = E ùëû [M ùëû (ùê∫ ùëñ )] E ùëû [M ùëû (ùê∫ ùëó )]ùê∑ ‚Ä≤</formula><p>ùëñ ùëó (ùúã). However, <ref type="bibr" target="#b7">[8]</ref> requires the knowledge of true relevance labels and does not involve learning. Extending this approach to learn relevance labels can suffer from the failure case of a two-step approach that relies on post-processing an unfair relevance estimation to make it fair, since fairness constraints are not taking into account during the learning process. We can get a consistent estimator of ùê∑ ‚Ä≤ ùëñ ùëó (ùúã) by using the IPS estimator of and replacing the expectations of exposure and merit with the average over the dataset</p><formula xml:id="formula_19">ùê∑ ‚Ä≤ ùëñ ùëó (ùúã |ùë∏) = ùëû ‚ààùë∏ Exp ùëû (ùê∫ ùëñ |ùúã) ùëû ‚ààùë∏ M ùëû (ùê∫ ùëñ ) ‚àí ùëû ‚ààùë∏ Exp ùëû (ùê∫ ùëó |ùúã) ùëû ‚ààùë∏ M ùëû (ùê∫ ùëó )</formula><p>.</p><p>As ùëÅ increases, ùê∑ ‚Ä≤ ùëñ ùëó (ùúã |ùë∏) will converge to ùê∑ ‚Ä≤ ùëñ ùëó (ùúã). However, the estimator is biased, which means E[ ùê∑ ‚Ä≤ ùëñ ùëó (ùúã |ùë∏)] ‚â† ùê∑ ‚Ä≤ ùëñ ùëó (ùúã). This bias might increase the error of the estimator. We therefore conclude that our new disparity ùê∑ ùëñ ùëó (ùúã) with its unbiased estimator ùê∑ ùëñ ùëó (ùúã) in Equations ( <ref type="formula" target="#formula_8">6</ref>) and ( <ref type="formula">7</ref>) is preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Incorporating Click Noise</head><p>Up to now, we assumed that positive feedback ùëê ùëû,ùëë = 1 reveals the relevance label rel ùëû,ùëë in a noise-free way -precisely that ùëê ùëû,ùëë = 1 if and only if rel ùëû,ùëë = 1 and ùëú ùëû,ùëë = 1. In practice, the user might make mistakes when examining the relevances of items, and we now define the following noise model. With 1 ‚â• ùúñ + &gt; ùúñ ‚àí ‚â• 0, we have</p><formula xml:id="formula_20">ùëÉ ùëê ùëû,ùëë = 1|rel ùëû,ùëë = 1, ùëú ùëû,ùëë = 1 = ùúñ + ùëÉ ùëê ùëû,ùëë = 1|rel ùëû,ùëë = 0, ùëú ùëû,ùëë = 1 = ùúñ ‚àí If ùúñ + &lt; 1,</formula><p>users might ignore relevant items even after examination. Otherwise if ùúñ ‚àí &gt; 0, users might give false positive feedback to irrelevant items after examination. Fortunately, the IPS estimator of utility is order-preserving for two policies ùúã 1 and ùúã 2 under click noise <ref type="bibr" target="#b27">[28]</ref>, namely</p><formula xml:id="formula_21">E ùëû ùëà (ùúã 1 |ùë∏) &gt; E ùëû ùëà (ùúã 2 |ùë∏) ‚áî ùëà (ùúã 1 ) &gt; ùëà (ùúã 2 |ùëû).</formula><p>Therefore, given enough data, the click noise does not affect the ability to find the ranking policy with optimal utility.</p><p>In contrast, the disparity estimator in Equations ( <ref type="formula" target="#formula_8">6</ref>) and ( <ref type="formula">7</ref>) is not order-preserving when ùúñ ‚àí &gt; 0:</p><formula xml:id="formula_22">E ùëû ùê∑ ùëñ ùëó (ùúã 1 |ùë∏) ‚àí ùê∑ ùëñ ùëó (ùúã 2 |ùë∏) = (ùúñ + ‚àíùúñ ‚àí ) E ùëû ùê∑ ùëñ ùëó (ùúã 1 |ùë∏) ‚àí ùê∑ ùëñ ùëó (ùúã 2 |ùë∏) + ùúñ ‚àí E ùëû |ùê∫ ùëû ùëó |ùõø Exp ùëû (ùê∫ ùëñ ) ‚àí |ùê∫ ùëû ùëñ |ùõø Exp ùëû (ùê∫ ùëó ) ,</formula><p>where ùõø Exp ùëû (ùê∫ ùëñ ) denotes Exp ùëû (ùê∫ ùëñ |ùúã 1 ) ‚àí Exp ùëû (ùê∫ ùëñ |ùúã 2 ). However, if we know the level of negative noise ùúñ ‚àí , we can correct the IPS estimator for group disparity as</p><formula xml:id="formula_23">ùê∑ ùëñ ùëó (ùúã |ùëû, ùúñ ‚àí ) = M ùëû (ùê∫ ùëó )Exp ùëû (ùê∫ ùëñ |ùúã) ‚àí M ùëû (ùê∫ ùëñ )Exp ùëû (ùê∫ ùëó |ùúã) ‚àí ùúñ ‚àí |ùê∫ ùëû ùëó |Exp ùëû (ùê∫ ùëñ |ùúã) ‚àí |ùê∫ ùëû ùëñ |Exp ùëû (ùê∫ ùëó |ùúã) . (<label>10</label></formula><formula xml:id="formula_24">)</formula><p>We can prove that the corrected estimator is unbiased up to a constant factor that can be absorbed into the hyperparameter ùõø of Equation ( <ref type="formula" target="#formula_2">2</ref>)</p><formula xml:id="formula_25">E ùëê ùëû ùê∑ ùëñ ùëó (ùúã |ùëû, ùúñ ‚àí ) = (ùúñ + ‚àí ùúñ ‚àí )ùê∑ ùëñ ùëó (ùúã |ùëû).</formula><p>.</p><p>To estimate the probability ùúñ ‚àí for false-positive noise, we can use a simple intervention similar to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b44">45]</ref>. While identifying relevant items is difficult, identifying irrelevant items is typically easy (e.g., documents that contain none of the query terms). Therefore, we can insert an item ùëë that is known to be irrelevant at position ùëò of the ranking. The expected clickthrough rate for the item is ùëù (ùëê ùëû,ùëë = 1|rel ùëû,ùëë = 0) = ùë£ ùëò ‚Ä¢ ùúñ ‚àí . This means that given the position bias and the clickthrough rate estimated from the intervention data, we can compute the noise level ùúñ ‚àí . Note that this intervention only needs to be performed on a small set of users and only affects the quality of one position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">POLICY-GRADIENT ALGORITHM FOR FAIR LTR</head><p>In the previous section, we defined a general framework for learning ranking policies from biased and noisy feedback under amortized fairness constraints. However, we still need an efficient algorithm to search the specific policy space for the solution of the training problem in Equation ( <ref type="formula" target="#formula_2">2</ref>). To this effect, we first define a stochastic ranking policy space based on the Plackett-Luce ranking model as in <ref type="bibr" target="#b43">[44]</ref> and then present a policy-gradient algorithm that optimizes the training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Plackett-Luce Ranking Model</head><p>We define a stochastic ranking policy space Œ† based on the Plackett-Luce model <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically, each ranking policy ùúã ‚àà Œ† is defined by a scoring function ‚Ñé ùúÉ , which can be any differentiable model with parameters ùúÉ . ‚Ñé ùúÉ takes the feature vectors ùë• ùëû of all the items for the current query ùëû as input and outputs a vector of scores ‚Ñé ùúÉ (ùë• ùëû ) = (‚Ñé ùúÉ (ùë• </p><p>To sample rankings from ùúã ùúÉ (ùëü |ùëû), we can sample items from the distribution Softmax(‚Ñé ùúÉ (ùë• ùëû )) without replacement and rank the items in the order in which they are drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Policy Gradient Training Algorithm</head><p>We use a Lagrange multiplier to solve the constrained optimization problem from Equation ( <ref type="formula" target="#formula_2">2</ref> for which the constraint holds and then pick the optimal œÄùúÜ that satisfies ùõø ùúÜ ‚â§ ùõø and provides maximal utility ùëà ( œÄùúÜ |ùë∏).</p><p>It remains to find an efficient algorithm for solving the unconstrained optimization problem in Equation <ref type="bibr" target="#b11">(12)</ref>. We use stochastic gradient descent (SGD) to iteratively update the parameters of the ranking policy. However, it is intractable to compute the expectations ùëà (ùúã |ùëû) and ùê∑ ùëñ ùëó (ùúã |ùëû) over the exponential space of rankings. Instead, we use the log-derivative trick of the REINFORCE algorithm <ref type="bibr" target="#b45">[46]</ref> to compute the gradient of ùëà</p><formula xml:id="formula_27">‚àá ùúÉ ùëà (ùúã ùúÉ |ùë∏) = 1 |ùë∏ | ‚àá ùúÉ ùëû E ùúé‚àºùúã (ùúé |ùëû) Œî(ùúé, ùëê ùëû ) = 1 |ùë∏ | ùëû E ùúé‚àºùúã (ùúé |ùëû) [‚àá ùúÉ log ùúã ùúÉ (ùúé |ùëû) Œî(ùúé, ùëê ùëû )]. (<label>13</label></formula><formula xml:id="formula_28">)</formula><p>The gradient of the squared fairness disparity, ‚àá ùúÉ ùê∑ ùëñ ùëó (ùúã ùúÉ |ùë∏)</p><p>2 , can also be reformulated this way,</p><formula xml:id="formula_29">2 |ùë∏ | ùê∑ ùëñ ùëó (ùúã ùúÉ |ùë∏) ùëû ‚ààùë∏ E ùúé‚àºùúã ùúÉ (ùúé |ùëû) ‚àá ùúÉ log ùúã ùúÉ (ùúé |ùëû) diff ùëñ ùëó (ùúé |ùëû) ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_30">diff ùëñ ùëó (ùúé |ùëû) denotes ùëÄ ùëû (ùê∫ ùëó ) Exp ùëû (ùê∫ ùëñ |ùúé)‚àí ùëÄ ùëû (ùê∫ ùëñ ) Exp ùëû (ùê∫ ùëó |ùúé).</formula><p>The expectations over rankings in Equations ( <ref type="formula" target="#formula_27">13</ref>) and ( <ref type="formula" target="#formula_29">14</ref>) are approximated via ùëÜ = 32 Monte-Carlo samples in our experiments.</p><p>To apply SGD, we need to compute ùê∑ ùëñ ùëó (ùúã ùúÉ ùë° |ùë∏) on the whole dataset at each step ùë°, which is quite expensive. As a simple but practically effective solution, we use the running average</p><formula xml:id="formula_31">1 ùëõ ùëõ‚àí1 ùúè=0 ùê∑ ùëñ ùëó (ùúã ùúÉ ùë° ‚àíùúè |ùëû ùë° ‚àíùúè )</formula><p>as an approximation of ùê∑ ùëñ ùëó (ùúã ùúÉ ùë° |ùë∏) in Equation ( <ref type="formula" target="#formula_29">14</ref>). This is biased since the disparities are computed on previous parameters, but it can reduce the variance of the unbiased estimator ùê∑ ùëñ ùëó (ùúã ùúÉ ùë° |ùëû ‚Ä≤ ùë° ) with ùëû ‚Ä≤ ùë° from the query set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EMPIRICAL EVALUATION</head><p>We conduct various experiments on two real-world datasets from which we generate synthetic click data following the experiment setup in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. This allows us to control the experimental conditions and test multiple data distributions to evaluate robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>5.1.1 Datasets. We use the Microsoft Learning to Rank Dataset (Fold1), a large-scale LTR dataset <ref type="bibr" target="#b36">[37]</ref>, and the German Credit Dataset, a credit scoring dataset often used to study algorithmic fairness <ref type="bibr" target="#b16">[17]</ref>. The German Credit Dataset contains information about 1000 loan applicants, where each applicant is described by a set of attributes and labeled as creditworthy or non-creditworthy. We randomly split the applicants into train, validation, and test sets with ratio 1:1:1. To define groups, we use the binary feature A43 (indicating whether the purpose of the loan applicant is radio/television) as the group attribute. The ratio of applicants in two groups is around 8:2.</p><p>The Microsoft LTR Dataset contains a large number of queries from Bing with manually-judged relevance labels for retrieved webpages. We adopt the train, validation, and test split provided with the dataset. Since the dataset does not come with any designated groups, we select the QualityScore (feature id 133) as the group attribute and divide items into two groups with the 40th percentile on the attribute as the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Click</head><p>Simulation. We generate click data for training and validation from the full-information datasets following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>. We first train a Ranking SVM with 1 percent of the full-information training data as the logging policy. This logging policy is then used to generate the rankings for which click data is logged. The click data is generated by simulating the position-based examination model. The position bias decays with the presented rank ùëò of the item as ùë£ ùëò = (1/ùëò) ùúÇ . When not stated otherwise, we use ùúÇ = 1, ùúñ + = 1, and ùúñ ‚àí = 0. Since the click signal is always binary and cannot reflect the relevance ratings, we binarize relevances in the same way as <ref type="bibr" target="#b27">[28]</ref>, by assigning rel ùëû (ùëë) = 1 to all the items that were judged as 3 or 4 and rel ùëû (ùëë) = 0 to judgments 0, 1, and 2.</p><p>We preprocess both datasets into similar formats for LTR. Following <ref type="bibr" target="#b43">[44]</ref>, we build a query for the German Credit Dataset by sampling 20 individuals from the corresponding set with ratio 9:1 for non-creditworthy individuals to creditworthy individuals respectively. We sample 500 queries from train, validation, and test sets respectively. For MSLR, after the binarization step, the dataset becomes extremely sparse, with only about 2.5% relevant items per query. To better compare different methods and amplify differences, for each query we sample 20 candidate items with 3 relevant items to build a new query.</p><p>The performance of the methods is reported on the fullinformation test set for which all relevance labels are known. Ranking utility and fairness are measured with Average DCG <ref type="bibr" target="#b23">[24]</ref> and squared disparity [ùê∑ ùëñ ùëó (ùúã)] 2 respectively. 5.1.3 Models and Hyperparameters. We train FULTR for two types of models: a linear model and a feed-forward network (one hidden layer with ReLU activation). We use the SGD optimizer for the linear model and the Adam optimizer for the neural network. The learning rate is 0.001. Following <ref type="bibr" target="#b43">[44]</ref>, we subtract the average reward of the Monte-Carlo samples from the reward to act as a control variate for variance reduction <ref type="bibr" target="#b45">[46]</ref>. To encourage exploration and avoid premature convergence to suboptimal policies <ref type="bibr" target="#b30">[31]</ref>, we add the entropy of the probability distribution Softmax(‚Ñé ùúÉ (ùë• ùëû )) to the objective, with a regularization coefficient ùõæ. We initialize  the coefficient as ùõæ = 1.0 and reduce it by a factor of 3 each time the validation metric has stopped improving. Finally, we add an L2 regularization term and cross-validate for the best regularization coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Can FULTR learn accurate ranking policies from biased feedback?</head><p>We begin our empirical evaluation by establishing that FULTR without fairness constraints is competitive with conventional LTR methods when learning from partial feedback. Here, we ignore fairness and entirely focus on utility, just like in conventional LTR. For realism, we inject noise ùúñ ‚àí = 0.1. We compare FULTR with the following LTR methods based on linear models or neural networks:</p><p>‚Ä¢ LambdaRank [10]: This is a nonlinear ranking model based on neural networks to optimize DCG. ‚Ä¢ Propensity SVM-Rank <ref type="bibr" target="#b27">[28]</ref>: This is an unbiased version of SVM-Rank <ref type="bibr" target="#b24">[25]</ref> and utilizes the IPS estimator to correct position bias in implicit feedback. ‚Ä¢ PG-Rank <ref type="bibr" target="#b43">[44]</ref>: This is a full-information policy-gradient method that is training the same model as FULTR-Linear.</p><p>Note that the full-information methods LambdaRank and PG-Rank ignore the position bias and treat click signals naively as full relevances. We also add a linear model and a neural network which get to see all true relevance labels as two skylines, called Full-Linear and Full-MLP. They represent the maximum performance we could hope to achieve with FULTR, which has access to only strictly less informative click feedback.</p><p>In Figure <ref type="figure" target="#fig_2">1</ref>, we show the test-set performance of the conventional LTR methods compared to FULTR. Since the optimal policy is always deterministic, following <ref type="bibr" target="#b43">[44]</ref>, we use the highest probability ranking for each query when evaluating FULTR. With increasing amounts of click data on the x-axis, FULTR with the neural network and the linear model approaches the skyline performance of Full-MLP and Full-Linear respectively on both datasets. The performance of FULTR-Linear is similar to Propensity SVM-Rank. Baseline methods that naively ignore position bias, namely Lamb-daRank and PG-Rank, cannot make effective use of the increasing amount of click data, and their learning curves are quite flat. Their performance is well below FULTR given sufficient click data.</p><p>Overall, we conclude that FULTR is an LTR algorithm that achieves ranking performance that is at least competitive with existing baselines when learning linear or neural models from partialinformation feedback. This implies that FULTR is a promising contender for use in practical applications, and it thus makes sense to further investigate how far it can enforce fairness considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Can FULTR learn fair ranking policies from biased feedback?</head><p>Next, we investigate FULTR's ability to enforce merit-based fairness of exposure. We compare with the following methods as baselines:</p><p>‚Ä¢ Group-blind: A variant of FULTR without any fairness constraints, but with the group attribute masked (i.e., fairness through unawareness). ‚Ä¢ Fair-PG-Rank <ref type="bibr" target="#b43">[44]</ref>: PG-Rank with a fairness constraint.</p><p>‚Ä¢ Fair-PG-Rank-IPS: Fair-PG-Rank <ref type="bibr" target="#b43">[44]</ref>, naively using the IPS estimator as in Equation ( <ref type="formula" target="#formula_14">8</ref>). ‚Ä¢ Equity of Attention <ref type="bibr" target="#b7">[8]</ref>: Post-processing method that optimizes amortized group disparity. We train a regression model ùëü (ùë• ùëû,ùëë ) to predict item relevances using the position-biascorrected objective for relevance estimation in <ref type="bibr" target="#b31">[32]</ref>.</p><p>The resulting utility/disparity curves on the test set are shown in Figure <ref type="figure" target="#fig_3">2</ref>. First, note that group-blind training does not lead to meritbased fairness of exposure. Second, FULTR is able to effectively trade-off utility and fairness on both datasets. As ùúÜ increases, the disparity goes down with an associated drop in DCG as expected. Increasing ùúÜ further drives disparity close to zero. The neural network version, FULTR-MLP, achieves better utility-fairness trade-off than the linear version on MSLR dataset. The German Credit dataset was not large enough to train a reliable MLP model.</p><p>Third, Fair-PG-Rank cannot effectively reduce amortized disparity. On both datasets, as we vary the coefficient ùúÜ, utility drops while disparity increases. We conjecture that this is due to optimizing fairness per query instead of amortized fairness, and the propagation of bias in clicks to the merit-based disparity measure. The naive combination of Fair-PG-Rank and IPS weighting cannot reduce the disparity, since the disparity estimator is still biased.</p><p>Fourth, the post-processing method cannot reduce amortized disparity on the MSLR dataset but can approach zero disparity on German Credit dataset. To further analyze the reason, we ran the post-processing method with ground-truth relevances. This infeasible approach achieves disparities close to zero on MSLR, which demonstrates that the problem lies in the regression model. To investigate further, we compute the accumulated regression errors for both groups. We observe that on MSLR, the regression model underestimates the relevance of ùê∫ ùëñ but estimates the relevance of ùê∫ ùëó precisely (error -14.05% vs -0.09%), which means that the regression model is already unfair despite its bias-corrected training objective.</p><p>On the German Credit dataset, the regression model underestimates the relevance of two groups more uniformly (error -15.86% vs -19.47%). We conclude that the failure of the post-processing on MSLR is due to its two-step nature, where the regression model is trained oblivious to fairness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Can FULTR converge to the performance of training on the true relevance labels?</head><p>We now explore how ranking utility and fairness of FULTR converge as the amount of click data increases. The utility/disparity curves with various amounts of training clicks are shown in Figure <ref type="figure" target="#fig_4">3</ref>. We also show the policy learned by FULTR in the full-info setting, which serves as the skyline that has full knowledge of relevance labels in the training set. With increasing amounts of click data, FULTR approaches the skyline performance of the policy learned on the full-info data. The policy learned with 120k clicks is almost identical to that of the full-information policy. This demonstrates that the unbiased estimator of FULTR enables it to converge to the full-information policy given enough click data.  We conduct an ablation study to understand the effect of IPSweighting on utility and fairness. In particular, we compare the following variants of FULTR in Figure <ref type="figure" target="#fig_6">4:</ref> ‚Ä¢ No-IPS FULTR-Linear: Both utility and disparity estimators are not IPS-weighted, assuming that implicit feedback reveals full-information relevance labels. ‚Ä¢ Utility-IPS FULTR-Linear: The utility estimator is IPSweighted, but the disparity estimator is unweighted.</p><p>With biased estimates of both utility and disparity, the No-IPS variant of FULTR achieves suboptimal utility and cannot reduce disparity to zero. The Utility-IPS variant can achieve utility similar to that of FULTR, but its fairness disparity remains higher even for large values of ùúÜ. This implies eliminating position bias through IPS-weighting is essential for both utility and fairness.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Can the disparity estimator adjust to click noise?</head><p>Next, we investigate the effectiveness of the noise-corrected disparity estimator in Equation <ref type="bibr" target="#b9">(10)</ref>. We use partial MSLR with click noise ùúñ ‚àí = 0.1. Noise-corrected FULTR estimates ùúñ ‚àí via the intervention with 1% of the training data and applies the noise-corrected estimator in Equation <ref type="bibr" target="#b9">(10)</ref>. Figure <ref type="figure" target="#fig_8">6</ref> shows that the noise-corrected IPS estimator of ùê∑ ùëñ ùëó (ùúã) can reduce disparity effectively and achieve zero disparity, while the pure IPS estimator cannot. To verify that this is not due to a lack of data, we increase the amount of training data by a factor of three. However, more data alone does not remedy the problem. This is consistent with the theoretical argument, since more training data can only reduce the variance, while click noise leads to a bias in the disparity estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Can FULTR ensure fairness between more than two groups?</head><p>So far, all experiments are conducted with only two groups. To deal with multiple groups, we can modify the constraint in Equation ( <ref type="formula" target="#formula_2">2</ref>) to</p><formula xml:id="formula_32">ùê∫ ùëñ ‚â†ùê∫ ùëó ùê∑ ùëñ ùëó (ùúã |ùë∏) 2 ‚â§ ùõø.</formula><p>To validate the effectiveness of FULTR when dealing with multiple groups, we compare the utility-fairness trade-off for a varying number of groups. To create more than two groups for MSLR, we partition the group attribute into the desired number of intervals with the same numbers of items for each group. Figure <ref type="figure" target="#fig_5">5</ref> shows the results for 2, 5, and 7 groups. We can observe that with a fixed number of training clicks, the disparity of FULTR increases as the number of groups increases. With 7 groups, a substantial disparity exists even for the largest value of ùúÜ. This is due to a lack of data for each group, as the number of groups is increasing while the amount of training data remains fixed. If we increase the number of training clicks, FULTR can again achieve disparity close to zero even with 7 groups. This demonstrates that FULTR can deal with multiple groups, but that an increase in the number of groups also requires an increase in training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We presented a framework FULTR for learning accurate and fair ranking policies from biased feedback. Specifically, we introduced fairness-of-exposure constraints that can allocate amortized exposure to groups of items based on their amortized relevance. Furthermore, we proposed counterfactual estimators of the corresponding utility and disparity measures that remove the effects of position bias and click noise. For efficient training, we derived a policy gradient algorithm that directly optimizes both utility and fairness. Finally, we presented extensive empirical evidence that FULTR can effectively learn ranking policies under fairness constraints despite biased and noisy feedback. Given that FULTR estimates relevance based on position bias across multiple queries, one existing limitation of the framework is it's inability to deal with individual-level fairness due to a lack of sufficient data. In the future, it would be interesting to explore ways to scale FULTR to incorporate individual fairness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Formally, Exp ùëû (ùëë |ùúã) = E ùúé‚àºùúã (ùúé |ùëû) ùëù (ùëú ùëû,ùëë = 1|ùúé) Furthermore, the exposure of group ùê∫ ùëñ is the aggregate of the exposure of the group members, Exp ùëû (ùê∫ ùëñ |ùúã) = ùëë ‚ààùê∫ ùëû ùëñ Exp ùëû (ùëë |ùúã), where ùê∫ ùëû ùëñ = ùê∫ ùëñ ‚à© ùíÖ ùëû . Similarly, we define the merit of group ùê∫ ùëñ for query ùëû as M ùëû (ùê∫ ùëñ ) = ùëë ‚ààùê∫ ùëû ùëñ M ùëû (ùëë). With these definitions in hand, we define our fairness disparity of policy ùúã as ùê∑ ùëñ ùëó (ùúã) = E ùëû‚àºùëÉ (ùëû) [ùê∑ ùëñ ùëó (ùúã |ùëû)], where ùê∑ ùëñ ùëó (ùúã |ùëû) measures the disparate exposure of ùê∫ ùëñ and ùê∫ ùëó based on their merit for query ùëû as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Ranking utility performance in terms of training clicks in the partial-info setting (ùúÇ = 1, ùúñ ‚àí = 0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DCG-Disparity curves for FULTR against baselines in the partial-info setting (ùúÇ = 1, ùúñ ‚àí = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DCG-Disparity curves for FULTR in terms of training clicks on the partial MSLR dataset (ùúÇ = 1, ùúñ ‚àí = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5. 5</head><label>5</label><figDesc>How important is unbiasedness for utility and fairness?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: DCG-Disparity curves for FULTR and two variants with unweighted utility and disparity estimators on the partial MSLR dataset (ùëÅ = 12ùêæ, ùúÇ = 1, ùúñ ‚àí = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: DCG-Disparity curves for different group numbers on the partial MSLR dataset (ùëÅ = 4ùêæ and ùëÅ = 12ùêæ, ùúÇ = 1, ùúñ ‚àí = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: DCG-Disparity curves of noise-corrected estimators against pure IPS-weighting on the partial MSLR dataset with click noise (ùëÅ = 36ùêæ and ùëÅ = 120ùêæ, ùúÇ = 1, ùúñ ‚àí = 0.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Based on this score vector, the probability ùúã ùúÉ (ùëü |ùëû) of a ranking ùëü =&lt; ùëë 1 , ùëë 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ùëë ùëõ ùëû &gt; is defined as the product of softmax distributions ùúã ùúÉ (ùëü |ùëû) = ùúÉ (ùë• ùëû,ùëë ùëñ ))</figDesc><table><row><cell>ùëõ ùëû</cell><cell>exp(‚Ñé ùëõ ùëû</cell></row><row><cell>ùëñ=1</cell><cell></cell></row></table><note>ùëû,ùëë 1 ), ‚Ñé ùúÉ (ùë• ùëû,ùëë 2 ), ‚Ä¢ ‚Ä¢ ‚Ä¢ , ‚Ñé ùúÉ (ùë• ùëû,ùëë ùëõùëû )).ùëó=ùëñ exp(‚Ñé ùúÉ (ùë• ùëû,ùëë ùëó ))).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>): œÄ = argmax ùúã min ùúÜ ‚â•0 ùëà (ùúã |ùë∏) ‚àí ùúÜ ùê∑ ùëñ ùëó (ùúã |ùë∏) 2 ‚àí ùõø . Instead of solving the minimization problem w.r.t. ùúÜ, we search a specific range of ùúÜ ‚àà {ùúÜ 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ùúÜ ùëò }. For each ùúÜ,</figDesc><table><row><cell>we need to solve</cell></row><row><cell>œÄùúÜ = argmax</cell></row><row><cell>2</cell></row></table><note>ùúã ùëà (ùúã |ùë∏) ‚àí ùúÜ ùê∑ ùëñ ùëó (ùúã |ùë∏) 2 . (12)Afterwards, we can compute the corresponding ùõø ùúÜ = ùê∑ ùëñ ùëó ( œÄùúÜ |ùë∏)</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported in part by NSF Awards IIS-1901168 and IIS-2008139. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Power-law distribution of the world wide web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="page" from="2115" to="2115" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dud√≠k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02453</idno>
		<title level="m">A reductions approach to fair classification</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Addressing Trust Bias for Unbiased Learning-to-Rank</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Estimating Position Bias without Intrusive Interventions</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Zaitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno>WSDM. 474-482</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unbiased learning to rank with unbiased propensity estimation</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A convex framework for fair regression</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02409</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00780</idno>
		<title level="m">Fairness in Recommendation Ranking through Pairwise Comparisons</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Equity of Attention: Amortizing Individual Fairness in Rankings</title>
		<author>
			<persName><forename type="first">Asia</forename><forename type="middle">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A neural click model for web search</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Markov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<title level="s">Maarten de Rijke, and Pavel Serdyukov</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="531" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Rank with Nonsmooth Cost Functions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Viet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Celis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Straszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisheeth</forename><forename type="middle">K</forename><surname>Vishnoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06840</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Ranking with fairness constraints. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network click model for web search ranking</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Click models for web search</title>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Information Concepts, Retrieval, and Services</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="115" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An experimental comparison of click position-bias models</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onno</forename><surname>Zoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI Machine Learning Repository</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A user browsing model to predict search engine click data from past observations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Dupret</surname></persName>
		</author>
		<author>
			<persName><surname>Piwowarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intervention Harvesting for Context-Dependent Examination-Bias Estimation</title>
		<author>
			<persName><forename type="first">Zhichong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="825" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fairness-Aware Ranking in Search &amp; Recommendation Systems with Application to LinkedIn Talent Search</title>
		<author>
			<persName><forename type="first">Cem</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Ambler</surname></persName>
		</author>
		<author>
			<persName><surname>Kenthapadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2221" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online learning with an unknown fairness metric</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2600" to="2609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient multiple-click models in web search</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05818</idno>
		<title level="m">A Novel Algorithm for Unbiased Learning to Rank</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName><forename type="first">Kalervo</forename><surname>J√§rvelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaana</forename><surname>Kek√§l√§inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geri</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurately interpreting clickthrough data as implicit feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">A</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geri</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unbiased Learning-to-Rank with Biased Feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Individual Choice Behavior: A Theoretical analysis</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards a Fair Marketplace: Counterfactual Evaluation of the trade-off between Relevance, Fairness and Satisfaction in Recommendation Systems</title>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Asynchronous Methods for Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri√†</forename><surname>Puigdom√®nech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Controlling Fairness and Bias in Dynamic Learning-to-Rank</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Morik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="429" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning with complex loss functions and constraints</title>
		<author>
			<persName><forename type="first">Harikrishna</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Harikrishna</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05330</idno>
		<title level="m">Pairwise Fairness for Ranking and Regression</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Achieving Fairness in the Stochastic Multi-armed Bandit Problem</title>
		<author>
			<persName><forename type="first">Vishakha</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ghalme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><surname>Narahari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10516</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The analysis of permutations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Plackett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1975">1975. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.2597</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Introducing LETOR 4.0 Datasets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rizal</forename><surname>Fathony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Memarrast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ziebart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03910</idno>
		<title level="m">Fair Logistic Regression: An Adversarial Perspective</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The Probability Ranking Principle in IR</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="281" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Central Role of the Propensity Score in Observational Studies For Causal Effects</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Experimental study of inequality and unpredictability in an artificial cultural market</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">Sheridan</forename><surname>Salganik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><forename type="middle">J</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="page" from="854" to="856" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fairness of Exposure in Rankings</title>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Policy Learning for Fairness in Ranking</title>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5427" to="5437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to Rank with Selection Bias in Personal Search</title>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Measuring fairness in ranked outputs</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Stoyanovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Scientific and Statistical Database Management</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fa* ir: A fair top-k ranking algorithm</title>
		<author>
			<persName><forename type="first">Meike</forename><surname>Zehlike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hajian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Megahed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1569" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reducing disparate exposure in ranking: A learning to rank approach</title>
		<author>
			<persName><forename type="first">Meike</forename><surname>Zehlike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08716</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
