<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEFENSE-GAN: PROTECTING CLASSIFIERS AGAINST ADVERSARIAL ATTACKS USING GENERATIVE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Maryland Institute for Advanced Computer Studies University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
							<email>mayak@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Maryland Institute for Advanced Computer Studies University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Maryland Institute for Advanced Computer Studies University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEFENSE-GAN: PROTECTING CLASSIFIERS AGAINST ADVERSARIAL ATTACKS USING GENERATIVE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Despite their outstanding performance on several machine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015)</ref>. These attacks come in the form of adversarial examples: carefully crafted perturbations added to a legitimate input sample. In the context of classification, these perturbations cause the legitimate sample to be misclassified at inference time <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b16">Papernot et al., 2016b;</ref><ref type="bibr" target="#b11">Liu et al., 2017)</ref>. Such perturbations are often small in magnitude and do not affect human recognition but can drastically change the output of the classifier.</p><p>Recent literature has considered two types of threat models: black-box and white-box attacks. Under the black-box attack model, the attacker does not have access to the classification model parameters; whereas in the white-box attack model, the attacker has complete access to the model architecture and parameters, including potential defense mechanisms <ref type="bibr" target="#b19">(Papernot et al., 2017;</ref><ref type="bibr" target="#b21">Tramèr et al., 2017;</ref><ref type="bibr" target="#b2">Carlini &amp; Wagner, 2017)</ref>.</p><p>Various defenses have been proposed to mitigate the effect of adversarial attacks. These defenses can be grouped under three different approaches: (1) modifying the training data to make the classifier more robust against attacks, e.g., adversarial training which augments the training data of the classifier with adversarial examples <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015)</ref>, (2) modifying the training procedure of the classifier to reduce the magnitude of gradients, e.g., defensive distillation <ref type="bibr" target="#b18">(Papernot et al., 2016d)</ref>, and (3) attempting to remove the adversarial noise from the input samples <ref type="bibr" target="#b6">(Hendrycks &amp; Gimpel, 2017;</ref><ref type="bibr" target="#b13">Meng &amp; Chen, 2017</ref>). All of these approaches have limitations in the sense that they are effective against either white-box attacks or black-box attacks, but not both <ref type="bibr" target="#b21">(Tramèr et al., 2017;</ref><ref type="bibr" target="#b13">Meng &amp; Chen, 2017)</ref>. Furthermore, some of these defenses are devised with specific attack models in mind and are not effective against new attacks.</p><p>In this paper, we propose a novel defense mechanism which is effective against both white-box and black-box attacks. We propose to leverage the representative power of Generative Adversarial Networks (GAN) <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref> to diminish the effect of the adversarial perturbation, by "projecting" input images onto the range of the GAN's generator prior to feeding them to the classifier. In the GAN framework, two models are trained simultaneously in an adversarial setting: a generative model that emulates the data distribution, and a discriminative model that predicts whether a certain input came from real data or was artificially created. The generative model learns a mapping G from a low-dimensional vector z ∈ R k to the high-dimensional input sample space R n . During training of the GAN, G is encouraged to generate samples which resemble the training data. It is, therefore, expected that legitimate samples will be close to some point in the range of G, whereas adversarial samples will be further away from the range of G. Furthermore, "projecting" the adversarial examples onto the range of the generator G can have the desirable effect of reducing the adversarial perturbation. The projected output, computed using Gradient Descent (GD), is fed into the classifier instead of the original (potentially adversarially modified) image. We empirically demonstrate that this is an effective defense against both black-box and white-box attacks on two benchmark image datasets.</p><p>The rest of the paper is organized as follows. We introduce the necessary background regarding known attack models, defense mechanisms, and GANs in Section 2. Our defense mechanism, which we call Defense-GAN, is formally motivated and introduced in Section 3. Finally, experimental results, under different threat models, as well as comparisons to other defenses are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK AND BACKGROUND INFORMATION</head><p>In this work, we propose to use GANs for the purpose of defending against adversarial attacks in classification problems. Before detailing our approach in the next section, we explain related work in three parts. First, we discuss different attack models employed in the literature. We, then, go over related defense mechanisms against these attacks and discuss their strengths and shortcomings. Lastly, we explain necessary background information regarding GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ATTACK MODELS AND ALGORITHMS</head><p>Various attack models and algorithms have been used to target classifiers. All attack models we consider aim to find a perturbation δ to be added to a (legitimate) input x ∈ R n , resulting in the adversarial example x = x + δ. The ∞ -norm of the perturbation is denoted by <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref> and is chosen to be small enough so as to remain undetectable. We consider two threat levels: black-and white-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">WHITE-BOX ATTACK MODELS</head><p>White-box models assume that the attacker has complete knowledge of all the classifier parameters, i.e., network architecture and weights, as well as the details of any defense mechanism. Given an input image x and its associated ground-truth label y, the attacker thus has access to the loss function J(x, y) used to train the network, and uses it to compute the adversarial perturbation δ. Attacks can be targeted, in that they attempt to cause the perturbed image to be misclassified to a specific target class, or untargeted when no target class is specified.</p><p>In this work, we focus on untargeted white-box attacks computed using the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref>, the Randomized Fast Gradient Sign Method (RAND+FGSM) <ref type="bibr" target="#b21">(Tramèr et al., 2017)</ref>, and the Carlini-Wagner (CW) attack <ref type="bibr" target="#b2">(Carlini &amp; Wagner, 2017)</ref>. Although other attack models exist, such as the Iterative FGSM <ref type="bibr" target="#b9">(Kurakin et al., 2017)</ref>, the Jacobian-based Saliency Map Attack (JSMA) <ref type="bibr" target="#b16">(Papernot et al., 2016b), and</ref><ref type="bibr">Deepfool (Moosavi-Dezfooli et al., 2016)</ref>, we focus on these three models as they cover a good breadth of attack algorthims. FGSM is a very simple and fast attack algorithm which makes it extremely amenable to real-time attack deployment. On the other hand, RAND+FGSM, an equally simple attack, increases the power of FGSM for white-box attacks <ref type="bibr" target="#b21">(Tramèr et al., 2017)</ref>, and finally, the CW attack is one of the most powerful white-box attacks to-date <ref type="bibr" target="#b2">(Carlini &amp; Wagner, 2017)</ref>.</p><p>Fast Gradient Sign Method (FGSM) Given an image x and its corresponding true label y, the FGSM attack sets the perturbation δ to:</p><formula xml:id="formula_0">δ = • sign(∇ x J(x, y)).</formula><p>(1) FGSM <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref> was designed to be extremely fast rather than optimal. It simply uses the sign of the gradient at every pixel to determine the direction with which to change the corresponding pixel value.</p><p>Randomized Fast Gradient Sign Method (RAND+FGSM) The RAND+FGSM <ref type="bibr" target="#b21">(Tramèr et al., 2017)</ref> attack is a simple yet effective method to increase the power of FGSM against models which were adversarially trained. The idea is to first apply a small random perturbation before using FGSM. More explicitly, for α &lt; , random noise is first added to the legitimate image x:</p><formula xml:id="formula_1">x = x + α • sign(N (0 n , I n )).<label>(2)</label></formula><p>Then, the FGSM attack is computed on x , resulting in</p><formula xml:id="formula_2">x = x + ( − α) • sign(∇ x J(x , y)).<label>(3)</label></formula><p>The Carlini-Wagner (CW) attack The CW attack is an effective optimization-based attack model <ref type="bibr" target="#b2">(Carlini &amp; Wagner, 2017)</ref>. In many cases, it can reduce the classifier accuracy to almost 0% <ref type="bibr" target="#b2">(Carlini &amp; Wagner, 2017;</ref><ref type="bibr" target="#b13">Meng &amp; Chen, 2017)</ref>. The perturbation δ is found by solving an optimization problem of the form:</p><formula xml:id="formula_3">min δ∈R n ||δ|| p + c • f (x + δ) s. t. x + δ ∈ [0, 1] n ,<label>(4)</label></formula><p>where f is an objective function that drives the example x to be misclassified, and c &gt; 0 is a suitably chosen constant. The 2 , 0 , and ∞ norms are considered. We refer the reader to <ref type="bibr" target="#b2">(Carlini &amp; Wagner, 2017)</ref> for details regarding the approach to solving (4) and setting the constant c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">BLACK-BOX ATTACK MODELS</head><p>For black-box attacks we consider untargeted FGSM attacks computed on a substitute model <ref type="bibr" target="#b19">(Papernot et al., 2017)</ref>. As previously mentioned, black-box adversaries have no access to the classifier or defense parameters. It is further assumed that they do not have access to a large training dataset but can query the targeted DNN as a black-box, i.e., access labels produced by the classifier for specific query images. The adversary trains a model, called substitute, which has a (potentially) different architecture than the targeted classifier, using a very small dataset augmented by synthetic images labeled by querying the classifier. Adversarial examples are then found by applying any attack method on the substitute network. It was found that such examples designed to fool the substitute often end up being misclassified by the targeted classifier <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b19">Papernot et al., 2017)</ref>. In other words, black-box attacks are easily transferrable from one model to the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DEFENSE MECHANISMS</head><p>Various defense mechanisms have been employed to combat the threat from adversarial attacks. In what follows, we describe one representative defense strategy from each of the three general groups of defenses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">ADVERSARIAL TRAINING</head><p>A popular approach to defend against adversarial noise is to augment the training dataset with adversarial examples <ref type="bibr" target="#b20">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b14">Moosavi-Dezfooli et al., 2016)</ref>. Adversarial examples are generated using one or more chosen attack models and added to the training set. This often results in increased robustness when the attack model used to generate the augmented training set is the same as that used by the attacker. However, adversarial training does not perform as well when a different attack strategy is used by the attacker. Additionally, it tends to make the model more robust to white-box attacks than to black-box attacks due to gradient masking <ref type="bibr" target="#b17">(Papernot et al., 2016c;</ref><ref type="bibr">2017;</ref><ref type="bibr" target="#b21">Tramèr et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">DEFENSIVE DISTILLATION</head><p>Defensive distillation <ref type="bibr" target="#b18">(Papernot et al., 2016d)</ref> trains the classifier in two rounds using a variant of the distillation <ref type="bibr" target="#b7">(Hinton et al., 2014)</ref> method. This has the desirable effect of learning a smoother network and reducing the amplitude of gradients around input points, making it difficult for attackers to generate adversarial examples <ref type="bibr" target="#b18">(Papernot et al., 2016d)</ref>. It was, however, shown that, while defensive distillation is effective against white-box attacks, it fails to adequately protect against black-box attacks transferred from other networks <ref type="bibr" target="#b2">(Carlini &amp; Wagner, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">MAGNET</head><p>Recently, <ref type="bibr" target="#b13">Meng &amp; Chen (2017)</ref> introduced MagNet as an effective defense strategy. It trains a reformer network (which is an auto-encoder or a collection of auto-encoders) to move adversarial examples closer to the manifold of legitimate, or natural, examples. When using a collection of auto-encoders, one reformer network is chosen at random at test time, thus strengthening the defense. It was shown to be an effective defense against gray-box attacks where the attacker knows everything about the network and defense, except the parameters. MagNet is the closest defense to our approach, as it attempts to reform an adversarial sample using a learnt auto-encoder. The main differences between MagNet and our approach are: (1) we use GANs instead of auto-encoders, and, most importantly, (2) we use GD minimization to find latent codes as opposed to a feedforward encoder network. This makes Defense-GAN more robust, especially against white-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GENERATIVE ADVERSARIAL NETWORKS (GANS)</head><p>GANs, originally introduced by Goodfellow et al. ( <ref type="formula">2014</ref>), consist of two neural networks, G and D. G : R k → R n maps a low-dimensional latent space to the high dimensional sample space of x. D is a binary neural network classifier. In the training phase, G and D are typically learned in an adversarial fashion using actual input data samples x and random vectors z. An isotropic Gaussian prior is usually assumed on z. While G learns to generate outputs G(z) that have a distribution similar to that of x, D learns to discriminate between "real" samples x and "fake" samples G(z). D and G are trained in an alternating fashion to minimize the following min-max loss <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>:</p><formula xml:id="formula_4">min G max D V (D, G) = E x∼pdata(x) [log D(x)] + E z∼pz(z) [log(1 − D(G(z)))].<label>(5)</label></formula><p>It was shown that the optimal GAN is obtained when the resulting generator distribution p g = p data <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>.</p><p>However, GANs turned out to be difficult to train in practice <ref type="bibr" target="#b5">(Gulrajani et al., 2017)</ref>, and alternative formulations have been proposed. <ref type="bibr" target="#b1">Arjovsky et al. (2017)</ref> introduced Wasserstein GANs (WGANs) which are a variant of GANs that use the Wasserstein distance, resulting in a loss function with more desirable properties:</p><formula xml:id="formula_5">min G max D V W (D, G) = E x∼pdata(x) [D(x)] − E z∼pz(z) [D(G(z))].<label>(6)</label></formula><p>In this work, we use WGANs as our generative model due to the stability of their training methods, especially using the approach in <ref type="bibr" target="#b5">(Gulrajani et al., 2017)</ref>.</p><p>Figure <ref type="figure">1</ref>: Overview of the Defense-GAN algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MOTIVATION</head><p>As mentioned in Section 2.3, the GAN min-max loss in (5) admits a global optimum when p g = p data <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref>. It can be similarly shown that WGAN admits an optimum to its own minmax loss in (6), when the set {x | p g (x) = p data (x)} has zero Lebesgue-measure. Formally, Lemma 1 A generator distribution p g is a global optimum for the WGAN min-max game defined in ( <ref type="formula" target="#formula_5">6</ref>), if and only if p g (x) = p data (x) for all x ∈ R n , potentially except on a set of zero Lebesguemeasure.</p><p>A sketch of the proof can be found in Appendix A.</p><p>Additionally, it was shown that, if G and D have enough capacity to represent the data, and if the training algorithm is such that p g converges to p data , then</p><formula xml:id="formula_6">E x∼pdata min z ||G t (z) − x|| 2 −→ 0 (7)</formula><p>where G t is the generator of a GAN or WGAN<ref type="foot" target="#foot_1">1</ref> after t steps of its training algorithm <ref type="bibr" target="#b8">(Kabkab et al., 2018)</ref>.</p><p>This serves to show that, under ideal conditions, the addition of the GAN reconstruction loss minimization step should not affect the performance of the classifier on natural, legitimate samples, as such samples should be almost exactly recovered. Furthermore, we hypothesize that this step will help reduce the adversarial noise which follows a different distribution than that of the GAN training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DEFENSE-GAN ALGORITHM</head><p>Defense-GAN is a defense strategy to combat both white-box and black-box adversarial attacks against classification networks. At inference time, given a trained GAN generator G and an image x to be classified, z * is first found so as to minimize</p><formula xml:id="formula_7">min z ||G(z) − x|| 2 2 .<label>(8)</label></formula><p>G(z * ) is then given as the input to the classifier. The algorithm is illustrated in Figure <ref type="figure">1</ref>. As ( <ref type="formula" target="#formula_7">8</ref>) is a highly non-convex minimization problem, we approximate it by doing a fixed number L of GD steps using R different random initializations of z (which we call random restarts), as shown in Figures <ref type="figure" target="#fig_0">1  and 2</ref>.</p><p>The GAN is trained on the available classifier training dataset in an unsupervised manner. The classifier can be trained on the original training images, their reconstructions using the generator G, or a combination of the two. As was discussed in Section 3.1, as long as the GAN is appropriately trained and has enough capacity to represent the data, original clean images and their reconstructions should not defer much. Therefore, these two classifier training strategies should, at least theoretically, not differ in performance.</p><p>Compared to existing defense mechanisms, our approach is different in the following aspects: 1. Defense-GAN can be used in conjunction with any classifier and does not modify the classifier structure itself. It can be seen as an add-on or pre-processing step prior to classification.</p><p>2. If the GAN is representative enough, re-training the classifier should not be necessary and any drop in performance due to the addition of Defense-GAN should not be significant.</p><p>3. Defense-GAN can be used as a defense to any attack: it does not assume an attack model, but simply leverages the generative power of GANs to reconstruct adversarial examples.</p><p>4. Defense-GAN is highly non-linear and white-box gradient-based attacks will be difficult to perform due to the GD loop. A detailed discussion about this can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We assume three different attack threat levels:</p><p>1. Black-box attacks: the attacker does not have access to the details of the classifier and defense strategy. It therefore trains a substitute network to find adversarial examples.</p><p>2. White-box attacks: the attacker knows all the details of the classifier and defense strategy.</p><p>It can compute gradients on the classifier and defense networks in order to find adversarial examples.</p><p>3. White-box attacks, revisited: in addition to the details of the architectures and parameters of the classifier and defense, the attacker has access to the random seed and random number generator. In the case of Defense-GAN, this means that the attacker knows all the random initializations {z</p><formula xml:id="formula_8">(i) 0 } R i=1 .</formula><p>We compare our method to adversarial training <ref type="bibr" target="#b4">(Goodfellow et al., 2015)</ref> and MagNet <ref type="bibr" target="#b13">(Meng &amp; Chen, 2017</ref>) under the FGSM, RAND+FGSM, and CW (with 2 norm) white-box attacks, as well as the FGSM black-box attack. Details of all network architectures used in this paper can be found in Appendix C. When the classifier is trained using the reconstructed images (G(z * )), we refer to our method as Defense-GAN-Rec, and we use Defense-GAN-Orig when the original images (x) are used to train the classifier. Our GAN follows the WGAN training procedure in <ref type="bibr" target="#b5">(Gulrajani et al., 2017)</ref>, and details of the generator and discriminator network architectures are given in Table <ref type="table" target="#tab_7">6</ref>. The reformer network (encoder) for the MagNet baseline is provided in Table <ref type="table" target="#tab_8">7</ref>. Our implementation is based on TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> and builds on open-source software: CleverHans by <ref type="bibr" target="#b15">Papernot et al. (2016a)</ref> and improved WGAN training by <ref type="bibr" target="#b5">Gulrajani et al. (2017)</ref>. We use machines equipped with NVIDIA GeForce GTX TITAN X GPUs.</p><p>In our experiments, we use two different image datasets: the MNIST handwritten digits dataset <ref type="bibr" target="#b10">(LeCun et al., 1998)</ref> and the Fashion-MNIST (F-MNIST) clothing articles dataset <ref type="bibr" target="#b22">(Xiao et al., 2017)</ref>.</p><p>Both datasets consist of 60, 000 training images and 10, 000 testing images. We split the training images into a training set of 50, 000 images and hold-out a validation set containing 10, 000 images.</p><p>For white-box attacks, the testing set is kept the same (10, 000 samples). For black-box attacks, the testing set is divided into a small hold-out set of 150 samples reserved for adversary substitute training, as was done in <ref type="bibr" target="#b19">(Papernot et al., 2017)</ref>, and the remaining 9, 850 samples are used for testing the different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RESULTS ON BLACK-BOX ATTACKS</head><p>In this section, we present experimental results on FGSM black-box attacks. As previously mentioned, the attacker trains a substitute model, which could differ in architecture from the targeted model, using a limited dataset consisting of 150 legitimate images augmented with synthetic images labeled using the target classifier. The classifier and substitute model architectures used and referred to throughout this section are described in Table <ref type="table" target="#tab_6">5</ref> in the Appendix.</p><p>In Tables <ref type="table" target="#tab_2">1 and 2</ref>, we present our classification accuracy results and compare to other defense methods. As can be seen, FGSM black-box attacks were successful at reducing the classifier accuracy by up to 70%. All considered defense mechanisms are relatively successful at diminishing the effect of the attacks. We note that, as expected, the performance of Defense-GAN-Rec and that of Defense-GAN-Orig are very close. In addition, they both perform consistently well across different classifier and substitute model combinations. MagNet also performs in a consistent manner, but achieves lower accuracy than Defense-GAN. In addition, the Defense-GAN parameters used in this experiment were kept the same for both Tables, in order to study the effect of dataset complexity, and can be further optimized as investigated in the next section. Figure <ref type="figure">3</ref> shows the effect of varying the number of GD iterations L as well as the random restarts R used to compute the GAN reconstructions of input images. Across different L and R values, Defense-GAN-Rec and Defense-GAN-Orig have comparable performance. Increasing L has the expected effect of improving performance when no attack is present. Interestingly, with an FGSM attack, the classification performance decreases after a certain L value. With too many GD iterations on the mean squared error (MSE) ||G(z) − (x + δ)|| 2 2 , some of the adversarial noise components are retained. In the right Figure, the effect of varying R is shown to be extremely pronounced. This is due to the non-convex nature of the MSE, and increasing R enables us to sample different local minima.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">EFFECT OF ADVERSARIAL NOISE NORM</head><p>We now investigate the effect of changing the attack in Table <ref type="table" target="#tab_3">3</ref>. As expected, with higher , the FGSM attack is more successful, especially on the F-MNIST dataset where the noise norm seems to have a more pronounced effect with nearly 37% drop in performance between = 0.1 and 0.3. Figure <ref type="figure">7</ref> in Appendix D shows adversarial samples as well as their reconstructions with Defense-GAN at different values of . We can see that for large , the class is difficult to discern, even for the human eye.</p><p>Even though it seems that increasing is a desirable strategy for the attacker, this increases the likelihood that the adversarial noise is discernible and therefore the attack is detected. It is trivial for the attacker to provide adversarial images at very high , and a good measure of an attack's strength is its ability to affect performance at low . In fact, in the next section, we discuss how Defense-GAN can be used to not only diminish the effect of attacks, but to also detect them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">ATTACK DETECTION</head><p>We intuitively expect that clean, unperturbed images will lie closer to the range of the Defense-GAN generator G than adversarial examples. This is due to the fact that G was trained to produce images which resemble the legitimate data. In light of this observation, we propose to use the MSE of an image with it is reconstruction from (8) as a "metric" to decide whether or not the image was  (9)</p><p>We compute the reconstruction MSEs for every image from the test dataset, and its adversarially manipulated version using FGSM. We show the Receiver Operating Characteristic (ROC) curves as well as the Area Under the Curve (AUC) metric for different Defense-GAN parameters and values in Figures <ref type="figure" target="#fig_3">4 and 5</ref>. The results show that this attack detection strategy is effective especially when the number of GD iterations L and random restarts R are large. From the left and middle Figures, we can conclude that the number of random restarts plays a very important role in the detection false positive and true positive rates as was discussed in Section 4.1.1. Furthermore, when is very small, it becomes difficult to detect attacks at low false positive rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">RESULTS ON WHITE-BOX ATTACKS</head><p>We now present results on white-box attacks using three different strategies: FGSM, RAND+FGSM, and CW. We perform the CW attack for 100 iterations of projected GD, with learning rate 10.0, and use c = 100 in equation ( <ref type="formula" target="#formula_3">4</ref>). Table <ref type="table" target="#tab_4">4</ref> shows the classification performance of different classifier models across different attack and defense strategies. We note that Defense-GAN significantly outperforms the two other baseline defenses. We even give the adversarial attacker access to the random initializations of z. However, we noticed that the performance does not change much when the attacker does not know the initialization. Adversarial training was done using FGSM to generate the adversarial samples. It is interesting to mention that when CW attack is used, adversarial training performs extremely poorly. As previously discussed, adversarial training does not generalize well against different attack methods.</p><p>Due to the loop of L steps of GD, Defense-GAN is resilient to GD-based white-box attacks, since the attacker needs to "un-roll" the GD loop and propagate the gradient of the loss all the way across L steps. In fact, from Table <ref type="table" target="#tab_4">4</ref>, the performance of classifier A with Defense-GAN on the MNIST dataset drops less than 1% from 0.997 to 0.988 under FGSM. In comparison, from Figure <ref type="figure" target="#fig_5">8</ref>, when L = 25, the performance of the same network drops to 0.947 (more than 5% drop). This shows that using a larger L significantly increases the robustness of Defense-GAN against GD-based whitebox attacks. This comes at the expense of increased inference time complexity. We present a more detailed discussion about the difficulty of GD-based white-box attacks in Appendix B and time complexity in Appendix G. Additional white-box experimental results on higher-dimensional images are reported in Appendix F. In this paper, we proposed Defense-GAN, a novel defense strategy utilizing GANs to enhance the robustness of classification models against black-box and white-box adversarial attacks. Our method does not assume a particular attack model and was shown to be effective against most commonly considered attack strategies. We empirically show that Defense-GAN consistently provides adequate defense on two benchmark computer vision datasets, whereas other methods had many shortcomings on at least one type of attack.</p><p>It is worth mentioning that, although Defense-GAN was shown to be a feasible defense mechanism against adversarial attacks, one might come across practical difficulties while implementing and deploying this method. The success of Defense-GAN relies on the expressiveness and generative power of the GAN. However, training GANs is still a challenging task and an active area of research, and if the GAN is not properly trained and tuned, the performance of Defense-GAN will suffer on both original and adversarial examples. Moreover, the choice of hyper-parameters L and R is also critical to the effectiveness of the defense and it may be challenging to tune them without knowledge of the attack.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F ADDITIONAL RESULTS ON WHITE-BOX ATTACKS</head><p>We report results on white-box attacks on the CelebFaces Attributes dataset (CelebA) <ref type="bibr" target="#b12">(Liu et al., 2015)</ref> in Table <ref type="table" target="#tab_13">12</ref>. The CelebA dataset is a large-scale face dataset consisting of more than 200, 000 face images, split into training, validation, and testing sets. The RGB images were center-cropped and resized to 64 × 64. We performed the task of gender classification on this dataset. The GAN architecture is the same as that in Table <ref type="table" target="#tab_7">6</ref>, except for an additional ConvT(128, 5 × 5, 1) layer in the generator network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G TIME COMPLEXITY</head><p>The computational complexity of reconstructing an image using Defense-GAN is on the order of the number of GD iterations performed to estimate z * , multiplied by the time to compute gradients. The number of random restarts R has less effect on the running time, since random restarts are independent and can run in parallel if enough resources are available. Table <ref type="table" target="#tab_3">13</ref> shows the average running time, in seconds, to find the reconstructions of MNIST and F-MNIST images on one NVIDIA GeForce GTX TITAN X GPU. For most applications, these running times are not prohibitive. We can see a tradeoff between running time and defense robustness as well as accuracy.</p><p>Table <ref type="table" target="#tab_3">13:</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: L steps of Gradient Descent are used to estimate the projection of the image onto the range of the generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: Classification accuracy of Model F using Defense-GAN on the MNIST dataset, under FGSM black-box attacks with = 0.3 and substitute Model E. Left: various number of iterations L are used (R = 10). Right: various number of random restarts R are used (L = 100).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4: ROC Curves when using Defense-GAN MSE for FGSM attack detections on the MNIST dataset (Classifier Model F, Substitute Model E). Left: Results for various number of GD iterations are shown with R = 10, = 0.30. Middle: Results for various number of random restarts R are shown with L = 100, = 0.30. Right: Results for various are shown with L = 400, R = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ROC Curves when using Defense-GAN MSE for FGSM attack detections on the F-MNIST dataset (Classifier Model F, Substitute Model E). Left: Results for various number of GD iterations are shown with R = 10, = 0.30. Middle: Results for various number of random restarts R are shown with L = 100, = 0.30. Right: Results for various are shown with L = 200, R = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Examples from MNIST and F-MNIST. Left: Original, FGSM adversarial = 0.3, and reconstruction images for R = 1 and various L are shown. Right: Original, FGSM adversarial = 0.3, and reconstruction images for L = 25 and various R are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Classification accuracy of different models using Defense-GAN on the MNIST dataset, under FGSM white-box attack with = 0.3, for various number of iterations L and R = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Average time, in seconds, to compute reconstructions of MNIST/F-MNIST images for various values of L and R. ± 0.027 0.070 ± 0.003 0.137 ± 0.004 0.273 ± 0.006 L = 0.543 ± 0.017 R = 2 0.042 ± 0.026 0.067 ± 0.002 0.131 ± 0.003 0.261 ± 0.006 L = 0.510 ± 0.006 R = 5 0.043 ± 0.029 0.070 ± 0.002 0.136 ± 0.004 0.270 ± 0.004 L = 0.535 ± 0.008 R = 10 0.051 ± 0.032 0.086 ± 0.001 0.170 ± 0.002 0.338 ± 0.008 L = 0.675 ± 0.016 R = 20 0.060 ± 0.035 0.105 ± 0.003 0.209 ± 0.006 0.414 ± 0.012 L = 0.825 ± 0.022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Two adversarial training defenses are presented: the first one obtains the adversarial examples assuming the same attack = 0.3, and the second assumes a different = 0.15. With incorrect knowledge of , the performance of adversarial training generally decreases. In addition, the classification performance of this defense method has very large variance across the different architectures. It is worth noting that adversarial training defense is only fit against FGSM attacks, because the adversarially augmented data, even with a different , is generated using the same method as the black-box attack (FGSM). In contrast, Defense-GAN and MagNet are general defense mechanisms which do not assume a specific attack model.The performances of defenses on the F-MNIST dataset, shown in Table2, are noticeably lower than on MNIST. This is due to the large = 0.3 in the FGSM attack. Please see Appendix D for qualitative examples showing that = 0.3 represents very high noise, which makes F-MNIST images difficult to classify, even by a human.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Classifier/ Substitute</cell><cell>No Attack</cell><cell>No Defense</cell><cell>Defense-GAN-Rec</cell><cell>Defense-GAN-Orig</cell><cell>MagNet</cell><cell>Adv. Tr. = 0.3</cell><cell>Adv. Tr. = 0.15</cell></row><row><cell>A/B</cell><cell cols="2">0.9970 0.6343</cell><cell>0.9312</cell><cell>0.9282</cell><cell>0.6937</cell><cell>0.9654</cell><cell>0.6223</cell></row><row><cell>A/E</cell><cell cols="2">0.9970 0.5432</cell><cell>0.9139</cell><cell>0.9221</cell><cell>0.6710</cell><cell>0.9668</cell><cell>0.9327</cell></row><row><cell>B/B</cell><cell cols="2">0.9618 0.2816</cell><cell>0.9057</cell><cell>0.9105</cell><cell>0.5687</cell><cell>0.2092</cell><cell>0.3441</cell></row><row><cell>B/E</cell><cell cols="2">0.9618 0.2128</cell><cell>0.8841</cell><cell>0.8892</cell><cell>0.4627</cell><cell>0.1120</cell><cell>0.3354</cell></row><row><cell>C/B</cell><cell cols="2">0.9959 0.6648</cell><cell>0.9357</cell><cell>0.9322</cell><cell>0.7571</cell><cell>0.9834</cell><cell>0.9208</cell></row><row><cell>C/E</cell><cell cols="2">0.9959 0.8050</cell><cell>0.9223</cell><cell>0.9182</cell><cell>0.6760</cell><cell>0.9843</cell><cell>0.9755</cell></row><row><cell>D/B</cell><cell cols="2">0.9920 0.4641</cell><cell>0.9272</cell><cell>0.9323</cell><cell>0.6817</cell><cell>0.7667</cell><cell>0.8514</cell></row><row><cell>D/E</cell><cell cols="2">0.9920 0.3931</cell><cell>0.9164</cell><cell>0.9155</cell><cell>0.6073</cell><cell>0.7676</cell><cell>0.7129</cell></row><row><cell cols="7">4.1.1 EFFECT OF NUMBER OF GD ITERATIONS L AND RANDOM RESTARTS R</cell><cell></cell></row></table><note>Classification accuracies of different classifier and substitute model combinations using various defense strategies on the MNIST dataset, under FGSM black-box attacks with = 0.3. Defense-GAN has L = 200 and R = 10.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracies of different classifier and substitute model combinations using various defense strategies on the F-MNIST dataset, under FGSM black-box attacks with = 0.3. Defense-GAN has L = 200 and R = 10.</figDesc><table><row><cell>Classifier/ Substitute</cell><cell>No Attack</cell><cell>No Defense</cell><cell>Defense-GAN-Rec</cell><cell>Defense-GAN-Orig</cell><cell>MagNet</cell><cell>Adv. Tr. = 0.3</cell><cell>Adv. Tr. = 0.15</cell></row><row><cell>A/B</cell><cell cols="2">0.9346 0.5131</cell><cell>0.586</cell><cell>0.5803</cell><cell>0.5404</cell><cell>0.7393</cell><cell>0.6600</cell></row><row><cell>A/E</cell><cell cols="2">0.9346 0.3653</cell><cell>0.4790</cell><cell>0.4616</cell><cell>0.3311</cell><cell>0.6945</cell><cell>0.5638</cell></row><row><cell>B/B</cell><cell cols="2">0.7470 0.4017</cell><cell>0.4940</cell><cell>0.5530</cell><cell>0.3812</cell><cell>0.3177</cell><cell>0.3560</cell></row><row><cell>B/E</cell><cell cols="2">0.7470 0.3123</cell><cell>0.3720</cell><cell>0.4187</cell><cell>0.3119</cell><cell>0.2617</cell><cell>0.2453</cell></row><row><cell>C/B</cell><cell cols="2">0.9334 0.2635</cell><cell>0.5289</cell><cell>0.6079</cell><cell>0.4664</cell><cell>0.7791</cell><cell>0.6838</cell></row><row><cell>C/E</cell><cell cols="2">0.9334 0.2066</cell><cell>0.4871</cell><cell>0.4625</cell><cell>0.3016</cell><cell>0.7504</cell><cell>0.6655</cell></row><row><cell>D/B</cell><cell cols="2">0.8923 0.4541</cell><cell>0.5779</cell><cell>0.5853</cell><cell>0.5478</cell><cell>0.6172</cell><cell>0.6395</cell></row><row><cell>D/E</cell><cell cols="2">0.8923 0.2543</cell><cell>0.4007</cell><cell>0.4730</cell><cell>0.3396</cell><cell>0.5093</cell><cell>0.4962</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy of Model F using Defense-GAN (L = 400, R = 10), under FGSM black-box attacks for various noise norms and substitute Model E.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Defense-GAN-Rec</cell><cell>Defense-GAN-Rec</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MNIST</cell><cell>F-MNIST</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.10</cell><cell>0.9864 ± 0.0011</cell><cell>0.8844 ± 0.0017</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.15</cell><cell>0.9836 ± 0.0026</cell><cell>0.8267 ± 0.0065</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.20</cell><cell>0.9772 ± 0.0019</cell><cell>0.7492 ± 0.0170</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.25</cell><cell>0.9641 ± 0.0001</cell><cell>0.6384 ± 0.0159</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.30</cell><cell>0.9307 ± 0.0034</cell><cell>0.5126 ± 0.0096</cell></row><row><cell>0.0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4 False Positive Rate 0.5 0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracies of different classifier models using various defense strategies on the MNIST (top) and F-MNIST (bottom) datasets, under FGSM, RAND+FGSM, and CW white-box attacks. Defense-GAN has L = 200 and R = 10.</figDesc><table><row><cell>Attack</cell><cell>Classifier Model</cell><cell>No Attack</cell><cell>No Defense</cell><cell>Defense-GAN-Rec</cell><cell>MagNet</cell><cell>Adv. Tr. = 0.3</cell></row><row><cell></cell><cell>A</cell><cell>0.997</cell><cell>0.217</cell><cell>0.988</cell><cell>0.191</cell><cell>0.651</cell></row><row><cell>FGSM</cell><cell>B</cell><cell>0.962</cell><cell>0.022</cell><cell>0.956</cell><cell>0.082</cell><cell>0.060</cell></row><row><cell>= 0.3</cell><cell>C</cell><cell>0.996</cell><cell>0.331</cell><cell>0.989</cell><cell>0.163</cell><cell>0.786</cell></row><row><cell></cell><cell>D</cell><cell>0.992</cell><cell>0.038</cell><cell>0.980</cell><cell>0.094</cell><cell>0.732</cell></row><row><cell></cell><cell>A</cell><cell>0.997</cell><cell>0.179</cell><cell>0.988</cell><cell>0.171</cell><cell>0.774</cell></row><row><cell>RAND+FGSM</cell><cell>B</cell><cell>0.962</cell><cell>0.017</cell><cell>0.944</cell><cell>0.091</cell><cell>0.138</cell></row><row><cell>= 0.3, α = 0.05</cell><cell>C</cell><cell>0.996</cell><cell>0.103</cell><cell>0.985</cell><cell>0.151</cell><cell>0.907</cell></row><row><cell></cell><cell>D</cell><cell>0.992</cell><cell>0.050</cell><cell>0.980</cell><cell>0.115</cell><cell>0.539</cell></row><row><cell></cell><cell>A</cell><cell>0.997</cell><cell>0.141</cell><cell>0.989</cell><cell>0.038</cell><cell>0.077</cell></row><row><cell>CW</cell><cell>B</cell><cell>0.962</cell><cell>0.032</cell><cell>0.916</cell><cell>0.034</cell><cell>0.280</cell></row><row><cell>2 norm</cell><cell>C</cell><cell>0.996</cell><cell>0.126</cell><cell>0.989</cell><cell>0.025</cell><cell>0.031</cell></row><row><cell></cell><cell>D</cell><cell>0.992</cell><cell>0.032</cell><cell>0.983</cell><cell>0.021</cell><cell>0.010</cell></row><row><cell>Attack</cell><cell>Classifier Model</cell><cell>No Attack</cell><cell>No Defense</cell><cell>Defense-GAN-Rec</cell><cell>MagNet</cell><cell>Adv. Tr. = 0.3</cell></row><row><cell></cell><cell>A</cell><cell>0.934</cell><cell>0.102</cell><cell>0.879</cell><cell>0.089</cell><cell>0.797</cell></row><row><cell>FGSM</cell><cell>B</cell><cell>0.747</cell><cell>0.102</cell><cell>0.629</cell><cell>0.168</cell><cell>0.136</cell></row><row><cell>= 0.3</cell><cell>C</cell><cell>0.933</cell><cell>0.139</cell><cell>0.896</cell><cell>0.110</cell><cell>0.804</cell></row><row><cell></cell><cell>D</cell><cell>0.892</cell><cell>0.082</cell><cell>0.875</cell><cell>0.099</cell><cell>0.698</cell></row><row><cell></cell><cell>A</cell><cell>0.934</cell><cell>0.102</cell><cell>0.888</cell><cell>0.096</cell><cell>0.447</cell></row><row><cell>RAND+FGSM</cell><cell>B</cell><cell>0.747</cell><cell>0.131</cell><cell>0.661</cell><cell>0.161</cell><cell>0.119</cell></row><row><cell>= 0.3, α = 0.05</cell><cell>C</cell><cell>0.933</cell><cell>0.105</cell><cell>0.893</cell><cell>0.112</cell><cell>0.699</cell></row><row><cell></cell><cell>D</cell><cell>0.892</cell><cell>0.091</cell><cell>0.862</cell><cell>0.104</cell><cell>0.626</cell></row><row><cell></cell><cell>A</cell><cell>0.934</cell><cell>0.076</cell><cell>0.896</cell><cell>0.060</cell><cell>0.157</cell></row><row><cell>CW</cell><cell>B</cell><cell>0.747</cell><cell>0.172</cell><cell>0.656</cell><cell>0.131</cell><cell>0.118</cell></row><row><cell>2 norm</cell><cell>C</cell><cell>0.933</cell><cell>0.063</cell><cell>0.896</cell><cell>0.084</cell><cell>0.107</cell></row><row><cell></cell><cell>D</cell><cell>0.892</cell><cell>0.090</cell><cell>0.875</cell><cell>0.069</cell><cell>0.149</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>•</head><label></label><figDesc>Conv(m, k × k, s)  refers to a convolutional layer with m feature maps, filter size k × k, and stride s • ConvT(m, k × k) refers to the transpose (gradient) of Conv (sometimes referred to as "deconvolution") with m feature maps, filter size k × k, and stride s</figDesc><table><row><cell>• FC(m) refers to a fully-connected layer with m outputs</cell></row><row><cell>• Dropout(p) refers to a dropout layer with probability p</cell></row><row><cell>• ReLU refers to the Rectified Linear Unit activation</cell></row><row><cell>• LeakyReLU(α) is the leaky version of the Rectified Linear Unit with parameter α</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Neural network architectures used for classifiers and substitute models.</figDesc><table><row><cell>A</cell><cell>B, F*</cell><cell>C</cell><cell>D, E*</cell></row><row><cell>Conv(64, 5 × 5, 1)</cell><cell>Dropout(0.2)</cell><cell>Conv(128, 3 × 3, 1)</cell><cell>FC(200)</cell></row><row><cell>ReLU</cell><cell>Conv(64, 8 × 8, 2)</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell>Conv(64, 5 × 5, 2)</cell><cell>ReLU</cell><cell>Conv(64, 3 × 3, 2)</cell><cell>Dropout(0.5)</cell></row><row><cell>ReLU</cell><cell>Conv(128, 6 × 6, 2)</cell><cell>ReLU</cell><cell>FC(200)</cell></row><row><cell>Dropout(0.25)</cell><cell>ReLU</cell><cell>Dropout(0.25)</cell><cell>ReLU</cell></row><row><cell>FC(128)</cell><cell>Conv(128, 5 × 5, 1)</cell><cell>FC(128)</cell><cell>Dropout(0.5)</cell></row><row><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>FC(10) + Softmax</cell></row><row><cell>Dropout(0.5)</cell><cell>Dropout(0.5)</cell><cell>Dropout(0.5)</cell><cell></cell></row><row><cell>FC(10) + Softmax</cell><cell>FC(10) + Softmax</cell><cell>FC(10) + Softmax</cell><cell></cell></row><row><cell cols="4">[ * : F (resp. E) shares the same architecture as B (resp. D) with the dropout layers removed ]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Neural network architectures used for GANs.</figDesc><table><row><cell>Generator</cell><cell>Discriminator</cell></row><row><cell>FC(4096)</cell><cell>Conv(64, 5 × 5, 2)</cell></row><row><cell>ReLU</cell><cell>LeakyReLU(0.2)</cell></row><row><cell cols="2">ConvT(256, 5 × 5, 1) Conv(128, 5 × 5, 2)</cell></row><row><cell>ReLU</cell><cell>LeakyReLU(0.2)</cell></row><row><cell cols="2">ConvT(128, 5 × 5, 1) Conv(256, 5 × 5, 2)</cell></row><row><cell>ReLU</cell><cell>LeakyReLU(0.2)</cell></row><row><cell>ConvT(1, 5 × 5, 1)</cell><cell>FC(1)</cell></row><row><cell>Sigmoid</cell><cell>Sigmoid</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Neural network architecture used for the MagNet encoder.</figDesc><table><row><cell>Encoder</cell></row><row><cell>Conv(64, 5 × 5, 2)</cell></row><row><cell>LeakyReLU(0.2)</cell></row><row><cell>Conv(128, 5 × 5, 2)</cell></row><row><cell>LeakyReLU(0.2)</cell></row><row><cell>Conv(256, 5 × 5, 2)</cell></row><row><cell>LeakyReLU(0.2)</cell></row><row><cell>FC(128) + tanh</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Classification accuracy of Model F using Defense-GAN with various number of iterations L (R = 10), on the MNIST dataset, under FGSM black-box attack with = 0.3.</figDesc><table><row><cell>L</cell><cell>Defense-GAN-Rec No attack</cell><cell>Defense-GAN-Orig No attack</cell><cell>Defense-GAN-Rec Adversarial</cell><cell>Defense-GAN-Orig Adversarial</cell></row><row><cell>25</cell><cell>0.9273 ± 0.0215</cell><cell>0.9141 ± 0.0033</cell><cell>0.7955 ± 0.0045</cell><cell>0.7998 ± 0.0063</cell></row><row><cell>50</cell><cell>0.9567 ± 0.0203</cell><cell>0.9371 ± 0.0048</cell><cell>0.8516 ± 0.0078</cell><cell>0.8472 ± 0.0026</cell></row><row><cell>100</cell><cell>0.9728 ± 0.0164</cell><cell>0.9560 ± 0.0051</cell><cell>0.8953 ± 0.0027</cell><cell>0.8911 ± 0.0024</cell></row><row><cell>200</cell><cell>0.9860 ± 0.0010</cell><cell>0.9712 ± 0.0028</cell><cell>0.9210 ± 0.0023</cell><cell>0.9155 ± 0.0032</cell></row><row><cell>400</cell><cell>0.9869 ± 0.0082</cell><cell>0.9808 ± 0.0044</cell><cell>0.9332 ± 0.0027</cell><cell>0.9307 ± 0.0034</cell></row><row><cell>800</cell><cell>0.9934 ± 0.0009</cell><cell>0.9938 ± 0.0004</cell><cell>0.9319 ± 0.0038</cell><cell>0.9216 ± 0.0005</cell></row><row><cell>1600</cell><cell>0.9963 ± 0.0013</cell><cell>0.9967 ± 0.0005</cell><cell>0.9081 ± 0.0062</cell><cell>0.9008 ± 0.0095</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Classification accuracy of Model F using Defense-GAN with various number of iterations L (R = 10), on the F-MNIST dataset, under FGSM black-box attack with = 0.3.</figDesc><table><row><cell>L</cell><cell>Defense-GAN-Rec No attack</cell><cell>Defense-GAN-Orig No attack</cell><cell>Defense-GAN-Rec Adversarial</cell><cell>Defense-GAN-Orig Adversarial</cell></row><row><cell>25</cell><cell>0.8037 ± 0.0050</cell><cell>0.7595 ± 0.0009</cell><cell>0.4040 ± 0.0149</cell><cell>0.3910 ± 0.0119</cell></row><row><cell>50</cell><cell>0.8676 ± 0.0018</cell><cell>0.7898 ± 0.0016</cell><cell>0.4412 ± 0.0023</cell><cell>0.3980 ± 0.0114</cell></row><row><cell>100</cell><cell>0.9101 ± 0.0032</cell><cell>0.8190 ± 0.0043</cell><cell>0.4808 ± 0.0088</cell><cell>0.4221 ± 0.0255</cell></row><row><cell>200</cell><cell>0.9145 ± 0.0014</cell><cell>0.8373 ± 0.0054</cell><cell>0.5119 ± 0.0038</cell><cell>0.4594 ± 0.0056</cell></row><row><cell>400</cell><cell>0.9490 ± 0.0013</cell><cell>0.8557 ± 0.0049</cell><cell>0.5126 ± 0.0096</cell><cell>0.4754 ± 0.0102</cell></row><row><cell>800</cell><cell>0.9588 ± 0.0065</cell><cell>0.8832 ± 0.0042</cell><cell>0.5520 ± 0.0098</cell><cell>0.4644 ± 0.0092</cell></row><row><cell>1600</cell><cell>0.9640 ± 0.0010</cell><cell>0.9125 ± 0.0040</cell><cell>0.5335 ± 0.0226</cell><cell>0.4952 ± 0.0155</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Classification accuracy of Model F using Defense-GAN with various number of random restarts R (L = 100), on the MNIST dataset, under FGSM black-box attack with = 0.3.</figDesc><table><row><cell>R</cell><cell>Defense-GAN-Rec No attack</cell><cell>Defense-GAN-Orig No attack</cell><cell>Defense-GAN-Rec Adversarial</cell><cell>Defense-GAN-Orig Adversarial</cell></row><row><cell>1</cell><cell>0.7035 ± 0.0035</cell><cell>0.6436 ± 0.0017</cell><cell>0.5329 ± 0.0094</cell><cell>0.5011 ± 0.0085</cell></row><row><cell>2</cell><cell>0.8619 ± 0.0010</cell><cell>0.8080 ± 0.0029</cell><cell>0.6722 ± 0.0041</cell><cell>0.6605 ± 0.0050</cell></row><row><cell>5</cell><cell>0.9523 ± 0.0006</cell><cell>0.9213 ± 0.0024</cell><cell>0.8199 ± 0.0097</cell><cell>0.8228 ± 0.0038</cell></row><row><cell>10</cell><cell>0.9810 ± 0.0015</cell><cell>0.9560 ± 0.0051</cell><cell>0.8956 ± 0.0032</cell><cell>0.8911 ± 0.0024</cell></row><row><cell>20</cell><cell>0.9966 ± 0.0009</cell><cell>0.9753 ± 0.0010</cell><cell>0.9456 ± 0.0031</cell><cell>0.9310 ± 0.0023</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Classification accuracy of Model F using Defense-GAN with various number of random restarts R (L = 100), on the F-MNIST dataset, under FGSM black-box attack with = 0.3.</figDesc><table><row><cell>R</cell><cell>Defense-GAN-Rec No attack</cell><cell cols="3">Defense-GAN-Orig No attack</cell><cell>Defense-GAN-Rec Adversarial</cell><cell>Defense-GAN-Orig Adversarial</cell></row><row><cell>1</cell><cell>0.8425 ± 0.0008</cell><cell></cell><cell cols="2">0.5597 ± 0.0015</cell><cell>0.3504 ± 0.0102</cell><cell>0.3380 ± 0.0043</cell></row><row><cell>2</cell><cell>0.8994 ± 0.0051</cell><cell></cell><cell cols="2">0.7793 ± 0.0023</cell><cell>0.4050 ± 0.0148</cell><cell>0.3508 ± 0.0167</cell></row><row><cell>5</cell><cell>0.9260 ± 0.0028</cell><cell></cell><cell cols="2">0.6726 ± 0.0006</cell><cell>0.4521 ± 0.0177</cell><cell>0.4024 ± 0.0085</cell></row><row><cell>10</cell><cell>0.9101 ± 0.0032</cell><cell></cell><cell cols="2">0.8190 ± 0.0043</cell><cell>0.4808 ± 0.0088</cell><cell>0.4221 ± 0.0255</cell></row><row><cell></cell><cell></cell><cell>1.000</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.975</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.950</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.925</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.900</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.875</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.850</cell><cell>0</cell><cell cols="2">Number of GD iterations L 50 100 150 200 250 300 350 400</cell></row></table><note>AccuracyClassification accuracy of different models using Defense-GAN and varying L.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Classification accuracies of different classifier models using various defense strategies on the CelebA gender classification task, under FGSM, RAND+FGSM, and CW white-box attacks. Defense-GAN has L = 200 and R = 2.</figDesc><table><row><cell>Attack</cell><cell>Classifier Model</cell><cell>No Attack</cell><cell>No Defense</cell><cell>Defense-GAN-Rec</cell><cell>MagNet</cell><cell>Adv. Tr. = 0.3</cell></row><row><cell></cell><cell>A</cell><cell cols="2">0.9652 0.0870</cell><cell>0.9255</cell><cell>0.0985</cell><cell>0.1225</cell></row><row><cell>FGSM</cell><cell>B</cell><cell cols="2">0.9468 0.0995</cell><cell>0.9140</cell><cell>0.0920</cell><cell>0.2345</cell></row><row><cell>= 0.3</cell><cell>C</cell><cell cols="2">0.9459 0.0460</cell><cell>0.9255</cell><cell>0.1085</cell><cell>0.1130</cell></row><row><cell></cell><cell>D</cell><cell cols="2">0.9476 0.0605</cell><cell>0.9205</cell><cell>0.0975</cell><cell>0.7755</cell></row><row><cell></cell><cell>A</cell><cell cols="2">0.9652 0.0560</cell><cell>0.9280</cell><cell>0.1105</cell><cell>0.0700</cell></row><row><cell>RAND+FGSM</cell><cell>B</cell><cell cols="2">0.9468 0.1785</cell><cell>0.9030</cell><cell>0.1015</cell><cell>0.4515</cell></row><row><cell>= 0.3, α = 0.05</cell><cell>C</cell><cell cols="2">0.9459 0.0470</cell><cell>0.9200</cell><cell>0.1045</cell><cell>0.1055</cell></row><row><cell></cell><cell>D</cell><cell cols="2">0.9476 0.0665</cell><cell>0.9165</cell><cell>0.1105</cell><cell>0.696</cell></row><row><cell></cell><cell>A</cell><cell cols="2">0.9652 0.0460</cell><cell>0.8210</cell><cell>0.0985</cell><cell>0.5690</cell></row><row><cell>CW</cell><cell>B</cell><cell cols="2">0.9468 0.0575</cell><cell>0.7465</cell><cell>0.0955</cell><cell>0.0725</cell></row><row><cell>2 norm</cell><cell>C</cell><cell cols="2">0.9459 0.0435</cell><cell>0.7985</cell><cell>0.0985</cell><cell>0.2635</cell></row><row><cell></cell><cell>D</cell><cell cols="2">0.9476 0.0660</cell><cell>0.7740</cell><cell>0.1040</cell><cell>0.5010</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">PROPOSED DEFENSE-GANWe propose a new defense strategy which uses a WGAN trained on legitimate (un-perturbed) training samples to "denoise" adversarial examples. At test time, prior to feeding an image x to the classifier, we project it onto the range of the generator by minimizing the reconstruction error ||G(z) − x|| 2 2 , using L steps of GD. The resulting reconstruction G(z) is then given to the classifier. Since the generator was trained to model the unperturbed training data distribution, we expect this added step to result in a substantial reduction of any potential adversarial noise. We formally motivate this approach in the following section.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">For simplicity, we will use GAN and WGAN interchangeably in the rest of this manuscript, with the understanding that our implementation follows the WGAN loss.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A OPTIMALITY OF p g = p DATA FOR WGANS Sketch of proof of Lemma 1: The WGAN min-max loss is given by:</p><p>For a fixed G, the optimal discriminator D which maximizes V W (D, G) is such that:</p><p>Plugging D * G back into (12), we get:</p><p>Clearly, to minimize (15), we need to set p data (x) = p g (x) for x ∈ X . Then, since both pdfs should integrate to 1,</p><p>However, this is a contradiction since p g (x) &lt; p data (x) for x ∈ X c , unless µ(X c ) = 0 where µ is the Lebesgue measure. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DIFFICULTY OF GD-BASED WHITE-BOX ATTACKS ON DEFENSE-GAN</head><p>In order to perform a GD-based white-box attack on models using Defense-GAN, an attacker needs to compute the gradient of the output of the classifier with respect to the input. From Figure <ref type="figure">1</ref>, the generator and the classifier can be seen as one, combined, feedforward network, through which it is easy to propagate gradients. The difficulty lies in the orange box of the GD optimization detailed in Figure <ref type="figure">2</ref>.</p><p>For the sake of simplicity, let's assume that R = 1. Define L(x, z) = ||G(z) − x|| 2 2 . Then z * = z L , which is computed recursively as follows:</p><p>and so on. Therefore, computing the gradient of z * with respect to x involves a large number (L) of recursive chain rules and high-dimensional Jacobian tensors. This computation gets increasingly prohibitive for large L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C NEURAL NETWORK ARCHITECTURES</head><p>We describe the neural network architectures used throughout the paper. The detail of models A through F used for classifier and substitute networks can be found in Table <ref type="table">5</ref>. In Table <ref type="table">6</ref>, the GAN architectures are described, and in Table <ref type="table">7</ref>, the encoder architecture for the MagNet baseline is given.</p><p>In what follows:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein GAN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of Wasserstein GANs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Early methods for detecting adversarial images</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, Workshop Track</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Task-aware compressed sensing with generative models</title>
		<author>
			<persName><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, Workshop Track</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">MagNet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09064</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">Cleverhans v1. 0.0: an adversarial machine learning library</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards the science of security and privacy in machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arunesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wellman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03814</idno>
		<imprint>
			<date type="published" when="2016">2016c</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2016">2016d</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Asia Conference on Computer and Communications Security</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<publisher>Workshop Track</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
