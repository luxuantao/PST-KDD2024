<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Resource pool management: Reactive versus proactive or let&apos;s be friends</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-08-20">20 August 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Gmach</surname></persName>
							<email>daniel.gmach@hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hewlett-Packard Laboratories</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jerry</forename><surname>Rolia</surname></persName>
							<email>jerry.rolia@hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hewlett-Packard Laboratories</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ludmila</forename><surname>Cherkasova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hewlett-Packard Laboratories</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alfons</forename><surname>Kemper</surname></persName>
							<email>alfons.kemper@in.tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<postCode>85748</postCode>
									<settlement>Garching, München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Resource pool management: Reactive versus proactive or let&apos;s be friends</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-08-20">20 August 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">7E9A9C0CC6F6944D7EBF053D3CFA712A</idno>
					<idno type="DOI">10.1016/j.comnet.2009.08.011</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Virtualized data centres Resource pool management Enterprise workload analysis Simulation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The consolidation of multiple workloads and servers enables the efficient use of server and power resources in shared resource pools. We employ a trace-based workload placement controller that uses historical information to periodically and proactively reassign workloads to servers subject to their quality of service objectives. A reactive migration controller is introduced that detects server overload and underload conditions. It initiates the migration of workloads when the demand for resources exceeds supply. Furthermore, it dynamically adds and removes servers to maintain a balance of supply and demand for capacity while minimizing power usage. A host load simulation environment is used to evaluate several different management policies for the controllers in a time effective manner. A case study involving three months of data for 138 SAP applications compares three integrated controller approaches with the use of each controller separately. The study considers trade-offs between: (i) required capacity and power usage, (ii) resource access quality of service for CPU and memory resources, and (iii) the number of migrations. Our study sheds light on the question of whether a reactive controller or proactive workload placement controller alone is adequate for resource pool management. The results show that the most tightly integrated controller approach offers the best results in terms of capacity and quality but requires more migrations per hour than the other strategies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Virtualization is gaining popularity in enterprise environments as a software-based solution for building shared hardware infrastructures. Forrester Research estimates that businesses generally only end up using between 8% and 20% of the server capacity they have purchased. Virtualization technology helps to achieve greater system utilization while lowering total cost of ownership and responding more effectively to changing business conditions. For large enterprises, virtualization offers a solution for server and application consolidation in shared resource pools. The consolidation of multiple servers and their workloads has an objective of minimizing the number of resources, e.g., computer servers, needed to support the workloads. In addition to reducing costs, this can also lead to lower peak and average power requirements. Lowering peak power usage may be important in some data centres if peak power cannot easily be increased.</p><p>Applications participating in consolidation scenarios can make complex demands on servers. For example, many enterprise applications operate continuously, have unique time varying demands, and have performance-oriented Quality of Service (QoS) objectives. To evaluate which workloads can be consolidated to which servers, some preliminary performance and workload analysis should be done. In the simple naive case, a data centre operator may estimate the peak resource requirements of each workload and then evaluate the combined resource requirements of a group of workloads by using the sum of their peak demands. However, such an approach can 1389-1286/$ -see front matter Ó 2009 Elsevier B.V. All rights reserved. doi:10.1016/j.comnet.2009.08.011 lead to significant resource over-provisioning since it does not take into account the benefits of resource sharing for complementary workload patterns. To evaluate which workloads can be consolidated to which servers, in this work we employ a trace-based approach <ref type="bibr" target="#b0">[1]</ref> that assesses permutations and combinations of workloads in order to determine a near optimal workload placement that provides specific qualities of service.</p><p>The general idea behind trace-based methods is that historic traces offer a model of application demands that are representative of future application behaviour. Traces are used to decide how to consolidate workloads to servers. In our past work, we assumed that the placement of workloads would be adjusted infrequently, e.g., weekly or monthly <ref type="bibr" target="#b0">[1]</ref>. However, by repeatedly applying the method at shorter timescales we can achieve further reductions in required capacity. In this scenario, we treat the trace-based approach as a workload placement controller that periodically causes workloads to migrate among servers to consolidate them while satisfying quality requirements. Such migrations <ref type="bibr" target="#b1">[2]</ref> are possible without interrupting the execution of the corresponding applications. We enhance our optimization algorithm to better support this scenario by minimizing migrations during successive control intervals.</p><p>Though enterprise application workloads often have time varying loads that behave according to patterns <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, actual demands are statistical in nature and are likely to differ from predictions. Therefore, to further improve the efficiency and application quality of service of our approach, we manage workloads by integrating the workload placement controller with a reactive workload migration controller that observes current behaviour to: (i) migrate workloads off of overloaded servers, and (ii) free and shut down lightly-loaded servers. There are many management policies that can be used to guide workload management. Each has its own parameters. However, predicting and comparing the long term impact of different management policies for realistic workloads is a challenging task. Typically, this process is very time consuming as it is mainly done following a risky ''trial and error" process either with live workloads and real hardware or with synthetic workloads and real hardware. Furthermore, the effectiveness of policies may interact with the architecture for the resource pool of servers so it must be repeated for different alternatives.</p><p>To better assess the long term impact of management policies we exploit a host load simulation environment. The environment: models the placement of workloads on servers; simulates the competition for resources on servers; causes the controllers to execute according to a management policy; and dynamically adjusts the placement of workloads on servers. During this simulation process, the simulator collects metrics that are used to compare the effectiveness of the policies.</p><p>A case study involving three months of data for 138 SAP applications is used to evaluate the effectiveness of several management policies. These include the use of the workload placement and workload migration controllers separately and in an integrated manner. The study considers trade-offs between: (i) required capacity and power usage, (ii) resource access quality of service for CPU and memory resources, and (iii) the number of migrations. This paper significantly enhances our recent work <ref type="bibr" target="#b4">[5]</ref> by considering more advanced policies, by introducing new quality of service metrics, and by gaining new insights using an analysis of variance (ANOVA) upon data from more than 10 times the previous number of simulation runs. The results of the ANOVA show that for our policies and case study data thresholds that define underload conditions have a greater impact on capacity and quality than those that define overload conditions. Finally, we found that the tight integration of controllers outperforms the use of the controllers in parallel or separately but in general causes more migrations per hour.</p><p>The remainder of this paper is organized as follows. Section 2 describes the workload placement and migration controllers, management policies, and metrics. The host load simulation environment is introduced in Section 3. Section 4 presents case study results. Section 5 describes related work. Finally, conclusions are offered in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Management services, policies, and quality metrics</head><p>Our management policies rely on two controllers. This section describes the workload placement controller and the reactive workload migration controller. The management policies exploit these controllers in several different ways. Quality metrics are used to assess the effectiveness of management policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Workload placement controller</head><p>The workload placement controller has two components.</p><p>A simulator component simulates the assignment of several application workloads on a single server. It traverses the per-workload time varying traces of historical demand to determine the peak of the aggregate demand for the combined workloads. If for each capacity attribute, e.g., CPU and memory, the peak demand is less than the capacity of the attribute for the server then the workloads fit on the server. An optimizing search component examines many alternative placements of workloads on servers and reports the best solution found. The optimizing search is based on a genetic algorithm <ref type="bibr" target="#b5">[6]</ref>.</p><p>The workload placement controller is based on the Capman tool that is described further in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. It supports both consolidation and load levelling exercises. Load levelling balances workloads across a set of resources to reduce the likelihood of service level violations. Capman supports the controlled overbooking of capacity that computes a required capacity for workloads on a server that may be less than the peak of aggregate demand. It is capable of supporting a different quality of service for each workload <ref type="bibr" target="#b6">[7]</ref>. Without loss of generality, this paper considers the highest quality of service, which corresponds to a required capacity for workloads on a server that is the peak of their aggregate demand.</p><p>For this paper, we exploit Capman's multi-objective optimization functionality. Instead of simply finding the smallest number of servers needed to support a set of workloads, Capman evaluates solutions according to a second simultaneous objective. The second objective aims to minimize the number of changes to workload placement. When invoking Capman, an additional parameter specifies a target t as a bound for the number of workloads that it is desirable to migrate. Limiting the number of migrations limits the migration overhead and reduces the risk of incurring a migration failure. If it is possible to find a solution with fewer than t migrations, then Capman reports the workload placement that needs the smallest number of servers and has t or fewer migrations. If more changes are needed to find a solution, then Capman reports a solution that has the smallest number of changes to find a feasible solution. A data centre operator could choose a value t based on experience regarding the overhead that migrations place upon network infrastructure and servers. The case study explores the sensitivity of capacity and quality metrics to the parameter t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Workload migration controller</head><p>The migration controller is a fuzzy-logic based feedback control loop. An advisor module of the controller continuously monitors the servers' resource utilization and triggers a fuzzy-logic based controller whenever resource utilization values are too low or too high. When the advisor detects a lightly utilized, i.e., underload situation, or overload situation the fuzzy controller module identifies appropriate actions to remedy the situation. For this purpose, it is initialized with information on the current load situation of all affected servers and workloads and determines an appropriate action. For example, as a first step, if a server is overloaded it determines a workload on the server that should be migrated and as a second step it searches for a new server to receive the workload. Furthermore, these rules initiate the shutdown and startup of nodes. The architecture of the workload migration controller is illustrated in Fig. <ref type="figure">1</ref>.</p><p>The implementation of the workload migration controller uses the following rules:</p><p>A server is defined as overloaded if its CPU or memory consumption exceed a given threshold. In an overload situation, first, a fuzzy controller determines a workload to migrate away and then it chooses an appropriate target server. The target server is the least loaded server that has sufficient resources to host the workload. If such a server does not exist, we start up a new server and migrate the workload to the new one. An underload situation occurs whenever the CPU and memory usage averaged over all servers in the server pool drops below a specified threshold. While an overload condition is naturally defined with respect to a particular server, the underload situation is different. It is defined with respect to the average utilization of the overall system involving all the nodes. In this way, we try to avoid system thrashing: e.g., a new server generally starts with a small load and should not be considered immediately for consolidation. In an underload situation, first, the fuzzy controller chooses the least loaded server and tries to shut it down. For every workload on this server, the fuzzy controller determines a target server. If a target cannot be found for a workload then the shutdown process is stopped. In contrast to overload situations, this controller does not ignite additional servers. Section 4.4 evaluates the impact of various combinations of threshold values for overload and underload management. A more complete description of the fuzzy controller and its rules are presented in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Policies</head><p>The policies we consider evaluate whether a reactive controller or workload placement controller alone is adequate for resource pool management and whether the integration of controllers provides compelling benefits. Our study considers the following management policies:</p><p>MC: migration controller alone; WP: workload placement controller operating periodically alone; MC + WP: workload placement controller operating periodically with the migration controller operating in parallel; MC + WP on Demand: migration controller is enhanced to invoke the workload placement controller on demand to consolidate workloads whenever the servers being used are lightly utilized; and, MC + WP + WP on Demand: workload placement controller operating periodically and the migration controller is enhanced to invoke the workload placement controller on demand to consolidate workloads whenever the servers being used are lightly utilized.</p><p>The MC policy corresponds to using the workload migration controller alone for on-going management. The workload placement controller causes an initial workload placement that consolidates workloads to a small number of servers. The workload migration controller is then used to migrate workloads to alleviate overload and underload situations as they occur. The workload migration controller operates at the time scale that measurement data is made available. In this paper, the migration controller is invoked every 5 min.</p><p>With the WP policy, the workload placement controller uses historical workload demand trace information from the previous week that corresponds to the next control interval, e.g., the next 4 h. In this way it periodically recomputes a globally efficient workload placement. The historical mode is most likely appropriate for enterprise workloads that have repetitive patterns for workload demands.</p><p>The MC + WP policy implements the MC and WP policies in parallel, i.e., the workload placement controller is executed for each control interval, e.g., every 4 h, to compute a more effective workload placement for the next control interval. Within such an interval, the migration controller, independently, migrates workloads to alleviate overload and underload situations as they occur.</p><p>The MC + WP on Demand policy integrates the placement and migration controllers in a special way. Instead of running the workload placement controller after each workload placement control interval, the migration controller uses the workload placement algorithm to consolidate the workloads whenever servers being used are lightly utilized.</p><p>Finally, the MC + WP + WP on Demand policy is the same as MC + WP on Demand policy but also invokes the workload placement controller after every control interval, e.g., every 4 h, to periodically provide a globally efficient workload placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Efficiency and quality metrics</head><p>To compare the long term impact of management policies we consider several metrics. These include: total server CPU hours used and total server CPU hours idle; normalized server CPU hours used and normalized server idle CPU hours; minimum and maximum number of servers; the distribution of power usage in Watts; CPU and memory resource access quality per hour; and the number of migrations per hours.</p><p>The total server CPU hours used corresponds to the sum of the per workload demands. Total server CPU hours idle is the sum of idle CPU hours for servers that have workloads assigned to them. The server CPU hours idle shows how much CPU capacity is not used on the active servers. Normalized values are defined with respect to the total demand of the workloads as specified in the workload demand traces. Note that if normalized server CPU hours used is equal to 1 and normalized server CPU hours idle are equal to 1.5 then this corresponds to an average CPU utilization of 40%.</p><p>The minimum and maximum numbers of servers for a policy are used to compare the overall impact of a management policy on capacity needed for server infrastructure. This determines the cost of the infrastructure. Each server has a minimum power usage p idle , in Watts, that corresponds to the server having idle CPUs, and a maximum power usage p busy that corresponds to 100% CPU utilization. The power used by a server is estimated as</p><formula xml:id="formula_0">p idle þ u Á ðp busy À p idle Þ;</formula><p>where u is the CPU utilization of the server <ref type="bibr" target="#b8">[9]</ref>.</p><p>We define and introduce a new quality metric named violation penalty that is based on the number of successive intervals where a workload's demands are not fully satisfied and the expected impact on the customer. Longer epochs of unsatisfied demand incur greater penalty values, as they are more likely to be perceived by those using applications. For example, if service performance is degraded for up to 5 min customers would start to notice. If the service is degraded for more than 5 min then customers may start to call the service provider and complain. Furthermore, larger degradations in service must cause greater penalties.</p><p>The quality of the delivered service depends on how much the service is degraded. If demands greatly exceed allocated resources then the utility of the service suffers more than if demands are almost satisfied. Thus, for each violation a penalty weight w pen is defined that is based on the expected impact of the degraded quality on the customer. The violation penalty value pen for a violation with I successive overloaded measurement intervals is defined as pen ¼ I 2 max I i¼1 ðw pen;i Þ, where w pen;i is the penalty in the ith interval. Thus longer violations tend to have greater penalties than shorter violations. <ref type="foot" target="#foot_0">1</ref> The weight functions used for CPU and memory are given below. The sum of penalty values over all workloads over all violations defines the violation penalty for a metric.</p><p>Regarding CPU allocations, we estimate the impact of degraded service on a customer using a heuristic that compares the actual and desired utilization of allocation for the customer. An estimate is needed because we do not have measurements that reflect the actual impact on a customer. Let u a and u d &lt; 1 be the actual and desired CPU utilization of allocation for an interval. If u a 6 u d then we define the weight for the CPU penalty w CPU pen as w CPU pen ¼ 0 since there is no violation. If u a &gt; u d then response times will be higher than planned so we must estimate the impact of the degradation on the customer. We define:</p><formula xml:id="formula_1">w CPU pen ¼ 1 À 1 À u k a 1 À u k d :</formula><p>The penalty has a value between 0 and 1 and is larger for bigger differences and higher utilizations. The superscript k denotes the number of CPUs on the server. This formula is motivated by a formula that estimates the mean response time for the M=M=k queue <ref type="bibr" target="#b9">[10]</ref>, namely r ¼ 1= ð1 À u k Þ estimates the mean response time for a queue with k processors and unit service demand <ref type="bibr" target="#b10">[11]</ref>. The power term k reflects the fact that a server with more processors can sustain higher utilizations without impacting customer response times. Similarly, a customer that has a higher than desired utilization of allocation will be less impacted on a system with more processors than one with fewer processors.</p><p>Regarding memory allocations, we estimate the impact of degraded service on a customer using a heuristic that compares the actual allocation of memory l a and desired allocation of memory l d for a customer. If l a P l d then we define a memory penalty weight w Mem pen ¼ 0 since there is no violation. If l a &lt; l d , we define w Mem pen ¼ 1 À hr, where hr is the memory hit ratio. In our simulation, the hit ratio is measured as the percentage of satisfied memory demands in bytes.</p><p>To summarize, the CPU and memory violation penalties reflect two factors. They reflect the severity and length of the violation. The severity of the violation is captured by a weight function. Two weight functions were introduced, but others could be employed as well.</p><p>Finally, the number of migrations is the sum of migrations caused by the workload placement and workload migration controllers. A smaller number of migrations is preferable as it offers lower migration overheads and a lower risk of migration failures. We divide the total number of migrations for an experiment by the number of simulated hours to facilitate interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Host load simulator</head><p>Predicting the long term impact of integrated management policies for realistic workloads is a challenging task. We employ a flexible host load simulation environment to evaluate many management policies for resource pools in a time effective manner.</p><p>The architecture of the host load simulation environment is illustrated in Fig. <ref type="figure">2</ref>. The simulator takes as input historical workload demand traces, an initial workload placement, server resource capacity descriptions, and a management policy. The server descriptions include numbers of processors, processor speeds, real memory size, and network bandwidth. A routing table directs each workload's historical time varying resource requirement data to the appropriate simulated server. Each simulated server uses a fair-share scheduling strategy to determine how much of the workload demand is and is not satisfied. The central pool sensor makes time varying information about satisfied demands available to management controllers via an open interface. The interface also is used to integrate different controllers with the simulator without recompiling its code.</p><p>The controllers periodically gather accumulated metrics and make decisions about whether to cause workloads to migrate from one server to another. Migration is initiated by a call from a controller to the central pool actuator. In our simulation environment this causes a change to the routing table that reflects the impact of the migration in the next simulated time interval. During the simulation process the metrics defined in Section 2.4 are gathered. Different controller policies cause different behaviours that we observe through these metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Case study</head><p>This section evaluates the effectiveness of the proposed management policies using three months of real-world workload demand traces for 138 SAP enterprise applications. The traces are obtained from a data centre that specializes in hosting enterprise applications such as customer relationship management applications for small and medium sized businesses. Each workload was hosted on its own server so we use resource demand measurements for a server to characterize the workload's demand trace. The measurements were originally recorded using vmstat <ref type="bibr" target="#b11">[12]</ref>. Traces capture average CPU and memory usage as recorded every 5 min.</p><p>As many of the workloads are interactive enterprise workloads, a maximum utilization of 0.66 is desired to ensure interactive responsiveness. Hence, CPU demands in the historical workload traces are scaled with a factor of 1.5 to achieve a target utilization of 0.66. The resource pool simulator operates on this data walking forward in successive 5 min intervals. In addition to the three months of the real-world demand traces we used data from the previous month to initialize the demand buffers of the central pool sensor. This enables the integrated management services to access prior demand values at the start of a simulation run.</p><p>We consider the following resource pool configuration: 2 each server consists of 8 Â 2.93-GHz processor cores, 128 GB of memory, and two dual 10 Gb/s Ethernet network interface cards for network traffic and virtualization management traffic, respectively. Each server consumes 695 W when idle and 1013 W when it is fully utilized. Section 4.1 gives a workload characterization for the SAP workloads considered in the study. Section 4.2 begins our study at workload placement by considering the impact of migration overhead. Section 4.3 considers the question: how much capacity and power can be saved by periodically consolidating workloads? The section assumes perfect knowledge about future workload demands and its results give a baseline for capacity savings that is used for the rest of the case study. Sections 4.4 and 4.5 do not assume perfect knowledge of future demands. They consider tuning of migration controller parameters and compare the capacity savings offered by the migration controller with the workload placement controller and three scenarios where the controllers are integrated. Finally, Section 4.6 presents results that demonstrate that the number of migrations caused by the workload placement controller can be significantly reduced without a major impact on CPU capacity or violation penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Workload characteristics</head><p>Use of virtualization technology enables the creation of shared server pools where multiple application workloads share each server in the pool. Understanding the nature of enterprise workloads is crucial to properly design and provision current and future services in such pools.</p><p>Existing studies of internet and media workloads <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> indicate that client demands are highly variable (''peak-to-mean" ratios may be an order of magnitude or more), and that it is not economical to overprovision the system using ''peak" demands. Do enterprise workloads exhibit similar properties? We present results that illustrate the peak-to-mean behaviour for 138 enterprise application workloads. Understanding of burstiness for enterprise workloads can help to choosing the right trade-off between the application quality of service and resource pool capacity requirements. This section analyses burstiness and access patterns of the enterprise application workloads under study. It shows percentiles of demands, the maximum durations for contiguous demands beyond the 99th percentile, and a representative demand trace for an interactive enterprise application.</p><p>Fig. <ref type="figure">3</ref> gives the percentiles of CPU demand for the 138 applications over the period of four months. The illustrated demands are normalized as a percentage with respect to their peak values. Several curves are shown that illustrate the 99th, 97th, and 95th percentile of demand as well as the mean demand. The workloads are ordered by the 99th percentile for clarity. The figure shows that more than half of all studied workloads have a small percentage of points that are very large with respect to their remaining demands. The left-most 60 workloads have their top 3% of demand values between 10 and 2 times higher than the remaining demands in the trace. Furthermore, more than half of the workloads observe a mean demand less than 30% of the peak demand. These curves show the bursty nature of demands for most of the enterprise applications under study. Consolidating such bursty workloads onto a smaller number of more powerful servers is likely 2 Service providers can use the proposed approach for evaluating different hardware platforms. For example, in <ref type="bibr" target="#b4">[5]</ref> we made recommendations regarding server and blade based resource pool configurations.</p><p>to reduce the CPU capacity needed to support the workloads.</p><p>The corresponding percentiles for the memory demands of the 138 applications are shown in Fig. <ref type="figure">4</ref>. Again, the illustrated demands are normalized as percentage with respect to the peak memory demand. The curves show that the average memory demand of an application is closer to its the peak demand than it is observed for CPU. Forty-five percent of the workloads exhibit a mean demand above 80% of their peak demands. Thus, in a memory bound infrastructure the potential resource savings from resource sharing is expected to be smaller than in CPU bound systems.</p><p>An additional and complementary property for a workload is the maximum duration of its contiguous application demands. While short bursts in demand may not significantly impact a workload's users, a system must be provisioned to handle sustained bursts of high demand. However, if an application's contiguous demands above the 99th percentile of demand are never longer than 10 min then it may be economical to support the application's 99th percentile of demand and allow the remaining bursts to be served with degraded performance <ref type="bibr" target="#b6">[7]</ref>. We have analysed the maximum duration of bursts of CPU and memory demands for the workloads. Fig. <ref type="figure">5</ref> shows the duration of each workload's longest burst in CPU demand that is greater than its corresponding 99th percentile of demand.</p><p>From the figure, we see that:</p><p>83.3% of workloads have sustained bursts in CPU demand that last more than 15 min; and, 60% of workloads have sustained bursts in CPU demand that last more than 30 min.</p><p>These are significant bursts that could impact an end user's perception of performance. A similar analysis for memory demands shows that: 97.8% of workloads have sustained bursts in memory demand that last more than 15 min; and, 93.5% of workloads have sustained bursts in memory demand that last more than 30 min.</p><p>The numbers show that the length of the bursts matters. This justifies our use of the quality metric that takes the number of successive intervals where a workload's demands are not satisfied into account.</p><p>The analysis also shows that the CPU demands are much more variable than memory demands. CPU demands fluctuate with user load. Memory demands tend to increase then periodically decrease due to some form of memory garbage collection. For the applications in our case study, garbage collection appeared to occur each weekend. Fig. <ref type="figure">6</ref> illustrates the behaviour of a typical workload.</p><p>Finally, a workload pattern analysis (following the methodology introduced in <ref type="bibr" target="#b3">[4]</ref>) is conducted. Fig. <ref type="figure">7</ref> gives a summary of the pattern lengths for the 138 workloads. The pattern analysis discovered patterns with lengths between 3 h and seven weeks: 67.6% of the workloads exhibit a weekly behaviour; and, 15.8% of the workloads exhibit a daily behaviour.</p><p>To summarize, this section has shown that there are significant bursts in demand for the workloads and there is a greater opportunity for CPU sharing than for memory sharing. A workload pattern analysis shows that most of the enterprise workloads exhibit strong weekly or daily patterns for CPU usage. Memory usage tends to increase over a week then decrease suddenly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Impact of migration overhead with a workload placement controller</head><p>This section considers the impact of CPU overhead caused by migrations on required CPU capacity and on CPU violation penalty per hour. We do not focus on memory violation penalties, as these values were typically small.</p><p>Many virtualization platforms incur virtualization overhead. Virtualization overhead depends on the type of the virtualization and its implementation specifics. A migration requires the memory of a virtual machine to be copied from the source server to a target server. Typically, the ''amount" of CPU overhead is directly proportional to the ''amount" of I/O processing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Supporting a migration causes CPU load on both the source and target servers. The simulator reflects this migration overhead in the following way. For each workload that migrates, a CPU overhead is added to the source and destination servers. The overhead is proportional to the estimated transfer time based on the memory size of the virtual machine and the network interface card bandwidth. It is added to the source and destination servers over a number of intervals that corresponds to the transfer time. We assume that we use no more than half of the bandwidth available for management purposes, i.e., one of the two management network interface cards. For example, if a workload has 12 GB memory size and the networking interface is 1 Gb/s then additional CPU time is used for migrating the workload is ðC migr Á 12 GBÞ=1 Gb=s, where C migr is the coefficient of migration overhead.  To evaluate an impact of the additional CPU overhead caused by I/O processing during the workload migrations, we employ the workload placement controller with a 4 h control interval. All workloads migrate at the end of each control interval. Fig. <ref type="figure">8</ref> shows the results using migration overhead coefficient C migr varied from 0 to 2. The figure shows several different metrics. These include the normalized CPU hours used, the normalized idle CPU hours, and the CPU violation penalty per hour.</p><p>A higher migration overhead requires more CPU resources. The impact on CPU hours used is only noticeable in Fig. <ref type="figure">8</ref> when C migr P 1. The CPU violation penalty clearly increases for C migr P 1. In general, we find our results to be insensitive to values of C migr in the range between 0 to 1.0. We choose C migr ¼ 0:5 used during a workload migration for the remainder of the study. This value is not unreasonable because the network interface cards we consider support TCP/IP protocol offloading capabilities. There are many reports suggesting that such cards can be driven to 10 Gbps bidirectional bandwidth while using 50% or less of a CPU, e.g., <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance, quality, and power assuming perfect knowledge</head><p>In this section, we consider an ideal workload placement strategy. This approach assumes that we have perfect knowledge of future resource demands. It gives an upper bound for the potential capacity savings from consolidating workloads at different time scales. We use this bound later in the paper to determine how well our policies, that do not have perfect knowledge, perform compared to the ideal case.</p><p>Fig. <ref type="figure">9</ref> shows the results of an simulation where we use the workload placement controller to periodically consolidate the 138 workloads to a small number of servers in the resource pool. For this scenario, for a given time period, the workload placement controller chooses a placement such that each server is able to satisfy the peak of its workload CPU and memory demands. The figure shows the impact on capacity requirements of using the workload placement controller once at the start of the three months, and for cases with a control interval of 4 weeks, 1 week, 1 day, 4 h, 1 h, and 15 min. The figure shows that re-allocating workloads every 4 h captures most of the capacity savings that can be achieved, i.e., with respect to reallocation every 15 min. The 4 h and 15 min scenarios required a peak of 19 servers. All the other scenarios also had peaks between 19 and 21 servers. For the 4 h scenario, we note that the normalized server CPU hours used is approximately one-half of the idle CPU hours giving an average utilization close to 69% over the three month period with a negligible hourly CPU violation penalty value of 0.4. In subsequent subsections, we treat the results from the 4 h ideal case as the baseline for capacity and quality. Fig. <ref type="figure">9</ref> shows that as expected as the control interval drops to the hourly, fifteen minute, and five minute levels the number of migrations per hour increases proportionally as most workloads are likely to be reassigned. The resulting migration overheads increase the CPU quality violations. Table <ref type="table" target="#tab_3">1</ref> gives a more detailed breakdown of the violations for the 4 h control interval case.</p><p>The distribution of the Watts used is shown in Fig. <ref type="figure">10</ref>. We note that the power consumption of the 15 min, 1 h, and 4 h scenarios are pretty close to each other. For workload placement control intervals longer than 1 day, more servers are used resulting in higher power consumption.</p><p>In later subsections, we consider how much of these advantages we are able to achieve in practice without assuming perfect knowledge. The workload placement controller control interval is chosen as 4 h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Workload migration controller thresholds</head><p>This section evaluates the effectiveness of the migration controller. The experiments start with an ideal workload placement for the first 4 h and use the fuzzy-logic based migration controller to maintain the resource access quality of the workloads. The advisor module of the controller is configured as follows: it triggers the fuzzy controller if either a server is overloaded or the system is lightly utilized. A server is considered overloaded if the CPU or memory utilization exceeds a given threshold. In that case, it triggers the fuzzy controller that tries to migrate one workload from the concerned server to a less loaded one. Furthermore, the advisor deems a server pool lightly utilized, if the average CPU and memory utilization over all servers fall below their given thresholds. Then, the fuzzy controller chooses the least loaded server, migrates all of its workloads to other servers, and shuts down the server.</p><p>To evaluate the impact of the feedback controller, the following levels for the thresholds are considered: a: The CPU threshold defining overloaded servers varies from 80%, 85%, 90%, 95%, and 99% CPU utilization. b: The memory threshold defining overloaded servers varies from 80%, 85%, 90%, 95%, and 99% memory utilization. d: The CPU threshold defining a lightly utilized resource pool varies from 30%, 40%, 50%, and 60% average CPU utilization of the server pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c:</head><p>The memory threshold defining a lightly utilized resource pool varies from 30%, 40%, 50%, 60%, 70%, and 80% average memory utilization of the server pool.</p><p>A three months simulation is conducted for each of the factor level combinations resulting in a total number of 600 experiments.</p><p>An ANOVA model <ref type="bibr" target="#b17">[18]</ref> captures the effects of factor levels such as different values for thresholds on a metric, e.g., on the CPU violation penalty or CPU capacity metric. Each factor level has a numerical effect on the metric. The sum of each factor's effects adds to zero. The effect is defined as the difference between the overall mean value for the metric over all combinations of factor levels and the numerical impact of the factor level on the metric with respect to the overall mean value. Similarly, interactions between factor levels also have effects. An analysis of variance considers the sum of squares of effects. The sums of squares are variations for the metric. The analysis quantifies the impact of factors and interactions between factors on the total variation over all combinations of factor levels. When the assumptions of the ANOVA modelling approach hold, a statistical F-test can be used to determine which factors and interactions between factors have a statistically significant impact on the metric and to quantify the impact.</p><p>The assumptions of an ANOVA are:</p><p>the effects of factors are additive; uncontrolled or unexplained experimental variations, which are grouped as experimental errors, are independent of other sources of variation; variance of experimental errors is homogeneous; and, experimental errors follow a Normal distribution.</p><p>The model we employ for the CPU capacity and CPU violation penalty metrics is illustrated with the following equation: over all experiments l plus an effect that is due to each factor's level plus an effect that is due to pair-wise interactions for factors, e.g., the ith threshold for CPU overload and the kth threshold for CPU underload. The error term e includes the effects of higher level interactions, i.e., three and four factor interactions.</p><formula xml:id="formula_2">Metric ¼ l þ a i þ b j þ c k þ d l þ ðabÞ ij þ ðacÞ ik þ ðadÞ il þ ðbcÞ jk þ ðbdÞ jl þ ðcdÞ kl þ e i<label>2</label></formula><p>For the ANOVA models we consider, the factors have an additive impact on the metrics not a multiplicative impact. The experiments are fully controlled, experimental errors are defined as higher order interactions, which from a detailed analysis have small effects. Visual tests suggest that the errors are homogeneous. The Normality assumption for errors is discussed next.</p><p>For the CPU violation penalty the results of a Kolmogorov-Smirnov test (K-S test) <ref type="bibr" target="#b17">[18]</ref> for the 600 errors resulted in a D-value of 0.0627, which concludes that the error values are Normally distributed with a ¼ 0:01. However, after removing the 10 largest of the 600 errors, the K-S test indicates that the remaining 590 errors are Normally distrib-uted with significance a ¼ 0:2. This suggests that 20% of randomly generated Normally distributed data sets will have a greater difference from the Normal distribution than our experiment's error data. Hence, we conclude that the error values are Normally distributed and the ANOVA model can be applied. For the CPU capacity, the K-S test yields a D-value D ¼ 0:049122197 suggesting that the 600 errors are Normally distributed with a significance level a ¼ 0:1. As with the CPU violation penalty model, removing a few extreme values dramatically increases the significance level.</p><p>ANOVA results are presented in a table, e.g., Table <ref type="table" target="#tab_5">2</ref>. The columns identify the factor and factor interactions (Source), each source's sum of squares of effects (SS) as an absolute value and as a percentage (SS in %) of the total SS over all sources, the number of degrees of statistical freedom that contribute to the SS, the mean square value (MS) which is the SS for a source divided by its number of degrees of freedom, the computed F-value for the mean square value, the critical value for the F-value that determines statistical significance, and finally the conclusion regarding whether a source has a statistically significant impact on the metric.</p><p>Table <ref type="table" target="#tab_5">2</ref> gives the results of the ANOVA for the factors regarding CPU violation penalty. The table shows that factors d and c, the CPU and memory thresholds for defining underloaded servers, and their interaction explains 99% of the variation in CPU violation penalty over the 600 experiments. Interestingly, factors a and b, thresholds for overloaded CPU and memory, had little impact on quality. The bursts in demand for the traces under study were often larger than the headroom remaining on a server regardless of the chosen threshold level. The underload factors had a much bigger impact on quality. They guide consolidation. Lower threshold values limit consolidation so that the same applications use more servers and violations are less likely.</p><p>Table <ref type="table" target="#tab_6">3</ref> gives the results of an ANOVA for the factors regarding the CPU capacity. Factor c, the memory threshold for defining underloaded servers, has an even larger impact on capacity than on CPU violation penalty. Again, factors d and c and their interaction explain nearly all, 97%, of the variation. Recognizing underload conditions is clearly an important aspect of policy for managing resource pools.</p><p>The results of the 600 simulations are shown in Fig. <ref type="figure">11</ref> as small black dots. The figure illustrates CPU violation penalties versus normalized CPU capacity required under different policy configurations. Normalized capacity is defined as the sum of the total server CPU hours used and idle CPU hours divided by the sum of the total server CPU hours used and idle CPU hours used for the ideal case with a 4 h workload placement interval.</p><p>Each of the 600 simulations represents a combination of factor levels. As expected, the figure shows that as workloads are consolidated more tightly and capacity is reduced there is an increase in the CPU violation penalties. The specific shape of this curve is workload and resource pool specific and reflects the variability in workload demands.</p><p>A Pareto-optimal set of simulation runs is illustrated in Fig. <ref type="figure">11</ref> using a red line. These combinations of factor levels provided lowest CPU violation penalties and/or the lowest normalized CPU capacity. Ten of the Pareto-optimal combinations are chosen representing the best behaviours of the migration controller and serve as a baseline for the remainder of the paper. A data centre operator could choose any one of these as a best behaviour depending on the quality versus capacity trade-off desirable for the data centre's workloads. Migration controller thresholds for the 10 cases are given in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance, quality, and power achieved by management policies</head><p>We now consider the impact of integrated workload placement and workload migration controller policies for managing the resource pool. The management policies are described in Section 2.3. The management policies are each simulated for the 10 Pareto-optimal sets of migration controller threshold values as illustrated in Fig. <ref type="figure">11</ref>. Fig. <ref type="figure">12</ref> through Fig. <ref type="figure" target="#fig_5">14</ref> show simulation results for our baseline cases and the workload management policies we consider. The CPU metrics are discussed first followed by the memory and migration metrics. The use of a workload migration controller alone policy MC is most typical of the literature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The MC policy does very well as a starting point. Fig. <ref type="figure">12</ref> shows that when using approximately 8% more CPU capacity than the ideal case there is a CPU violation penalty per hour of 12.6. As the migration controller becomes less aggressive at consolidating workloads, i.e., using 50% more CPU capacity than the ideal case, the penalty drops to nearly zero.</p><p>The workload placement controller policy WP does not use the migration controller. It operates with a control interval of 4 h and consolidates to a given CPU and memory utilization, which is varied between 75% and 100%. In Fig. <ref type="figure">12</ref> the 100% is omitted for visibility reasons. It incurred hourly CPU violation penalties of 84. The WP policy does well when the systems are overprovisioned because there is little likelihood of a CPU violation penalty. As the workloads become more consolidated, the CPU violation penalty per hour increases dramatically.</p><p>The MC + WP policy is able to achieve much better CPU quality than either MC or WP alone while using much less CPU capacity. The periodic application of the workload placement controller globally optimizes the CPU usage for the resource pool. The migration controller alone does not attempt to do this. This policy and subsequent policies permit the workload placement controller to consolidate workloads onto servers using up to 100% CPU and memory utilization.</p><p>The MC + WP on Demand policy invokes the workload placement controller to consolidate the workloads whenever the resource pool is lightly loaded. It behaves better than MC alone but not as well as MC + WP because it does not periodically provide for a global optimization of CPU usage for the resource pool.</p><p>Finally, MC + WP + WP on Demand provides very good results from both a capacity and violation penalty point of view. It achieves nearly ideal CPU violation penalties while requiring only 10-20% more CPU capacity than the ideal case. We also note that the violation penalties for this approach are less sensitive to migration controller threshold values.</p><p>Fig. <ref type="figure">13</ref> shows the capacity versus quality trade-off for the memory metric. All of the cases provide for very low memory violation penalties except for the WP policy. The WP policy has no ability to react to the case where the demand for memory exceeds the supply of memory. As a result WP can incur violations with many measurement intervals and hence large violation penalties.</p><p>Fig. <ref type="figure" target="#fig_5">14</ref> shows the number of migrations for the different policies. For each policy the figure shows the minimum, first quartile<ref type="foot" target="#foot_2">3</ref> , median, third quartile and maximum number of migrations for all 10 chosen MC thresholds cases illustrated in Fig. <ref type="figure">11</ref>. The workload placement controller causes more migrations than the migration controller alone. The on-demand policies can cause significantly more migrations when implementing very aggressive migration controller policies. However, these policies also result in the most significant capacity savings with low violation penalties. The next subsection presents the results of a method that reduces the number of migrations caused by a workload placement controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Constraining migrations for workload placement</head><p>This section applies a multi-objective approach for the workload placement controller, as described in Section 2.1 to one of the 10 MC threshold cases. The approach constrains the number of migrations that the workload placement controller is permitted to recommend. Fewer migrations will cause lower migration overheads but also reduces the opportunity for consolidation. To evaluate the benefits of the approach we compare capacity, quality violations and migrations between the MC policy, which does not use the workload placement controller, and the MC + WP + WP on Demand policy. Fig. <ref type="figure" target="#fig_6">15</ref> shows the results.</p><p>In the figure, we vary the percentage of workloads that it is desirable for the workload placement controller to migrate from 100%, i.e., no constraint on migrations, down to 5%. The results show that introducing the constraint causes much fewer migrations. Without a limit, the average number of migrations every hour was 116.6 for the on-demand case. This value is nearly 50 times larger than the number   of migrations for the MC policy. With a 50% constraint, the migrations per hour drops below 12. With a 15% constraint, the number of migrations drops to 10.5 per hour using slightly less capacity as the MC case and yielding a significantly lower quality violation value. With a 5% constraint, the capacity increases slightly beyond the MC case because there are fewer gains from consolidation but the quality violation value decreases to nearly zero. This is achieved with the average number of migrations per hour being only four times greater than for the MC case. The peak number of migrations per hour for the unconstrained, 50%, 15%, and 5% cases are 1477, 231, 147, and 132, respectively. The peak values are high when the workload placement controller is triggered every 5 min.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>Server consolidation is becoming an increasingly popular approach in enterprise environments to better utilize and manage systems. Manufacturers of high-end commercial servers have long provided hardware support for server consolidation such as a logical partitioning and dynamic domains <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Although virtualization has been around for more than three decades, it has found its way into the mainstream only recently with a variety of solutions -both commercial and open source -that are now available for commodity systems. Many enterprises are beginning to exploit resource pool environments to lower their infrastructure and management costs. The problem of efficient workload placement and workload management in such environments is in a centre of attention for many research and product groups.</p><p>In our work, we chose to represent application behaviour via workload demand traces. Many research groups have used a similar approach to characterize application behaviour and applied trace-based methods to support what-if analysis in the assignment of workloads to consolidated servers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>. A consolidation analysis presented in <ref type="bibr" target="#b22">[23]</ref> packs existing server workloads onto a smaller number of servers using an Integer Linear Programming based bin-packing method. Unfortunately, the bin-packing method is NP-complete for this problem, resulting in a computation intensive task. This makes the method impractical for larger consolidation exercises and on-going capacity management. There are now commercial tools <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref> that employ trace-based methods to support server consolidation exercises, load balancing, ongoing capacity planning, and simulating placement of application workloads to help IT administrators improve server utilization.</p><p>We believe the workload placement service we employ has advantages over other workload placement services described above. It addresses issues including classes of service and placement constraints. The approach is able to minimize migrations over successive control intervals. Some researchers propose to limit the capacity requirement of an application workload to a percentile of its demand <ref type="bibr" target="#b23">[24]</ref>. This does not take into account the impact of sustained performance degradation over time on user experience as our required capacity definition does. Others look only at objectives for resources as a whole <ref type="bibr" target="#b2">[3]</ref> rather than making it possible for each workload to have an independently specified objective.</p><p>Recently, virtualization platforms such as VMware and Xen <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref> provide the ability to dynamically migrate VMs from one physical machine to another without interrupting application execution. They have implemented ''live" migration of VMs that results in extremely short downtimes ranging from tens of milliseconds to a second. VM migration has been used for dynamic resource allocation in Grid environments <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. In contrast, we focus on data centre environments with stringent quality of service requirements that necessitate design of highly responsive migration algorithms.</p><p>Wood et al. <ref type="bibr" target="#b19">[20]</ref> present Sandpiper, a system that automates the task of monitoring virtual machine performance, detecting hotspots, and initiating any necessary migrations. Sandpiper implements heuristic algorithms to determine which virtual machine to migrate from an overloaded server, where to migrate it, and a resource allocation for the virtual machine on the target server. Sandpiper implements a black-box approach that is fully OS-and application-agnostic and a gray-box approach that exploits OS-and application-level statistics. Sandpiper is closest to the migration controller presented in our paper though they implement different migration heuristics.</p><p>VMware's Distributed Resource Scheduler <ref type="bibr" target="#b33">[34]</ref> also uses migration to perform automated load balancing in response to CPU and memory pressure. DRS uses a user space application to monitor memory usage similar to Sandpiper, but unlike Sandpiper, it does not utilize application logs to respond directly to potential application service level violations or to improve placement decisions. 1000 Islands Project <ref type="bibr" target="#b34">[35]</ref> aims to provide an integrated capacity and workload management for the next generation data centres. In the paper, the authors evaluate one loose integration policy for different controllers, while our paper provides a detailed performance study evaluating outcome of the three different integration policies and uses a set of novel QoS metrics. The paper also considers the integration of a per-server workload manager and reports on some real system measurements whereas this paper does not.</p><p>Raghavendra et al. <ref type="bibr" target="#b18">[19]</ref> integrates sophisticated aspects of power and performance management for resource pools. They present a simulation study that optimizes with respect to power while minimizing the impact on performance. The results from simulations suggest that for integrated controllers between 3% and 5% of workload CPU demand units are not satisfied with their approach. Unsatisfied demands are not carried forward in their simulation. With our host simulation approach, we carry forward demands and focus more on per-workload quality metrics that characterize epochs of sustained overload. With our experiments, more than 99.9% of workload demands were satisfied for all cases. In <ref type="bibr" target="#b18">[19]</ref>, the authors conclude that 3-5% performance degradation is acceptable to save power. We concur, but suggest this is only true in exceptional circumstances when access to power is degraded. Otherwise workload QoS must be maintained to satisfy business objectives.</p><p>In our work, to further improve efficiency and application quality of service, we manage workloads by integrating the workload placement approach with a workload migration controller. Our simulation results show that such integrated approach provides unique performance and quality benefits.</p><p>Our approach is not application-centric. Commonly available resource demand traces are the basis for our management system. However, there are many research papers, e.g. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, which design dynamic provisioning systems for targeted classes of applications, e.g., multi-tier applications. There are excellent earlier works on load sharing systems that support batch-like workloads <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. These papers argue that simple adaptive techniques outperform static job placement policies and perform nearly as well at improving system performance as more complex optimization methods. Our results confirm these findings for complex enterprise applications with time varying demands and time varying resource pool size. However, for our complex scenario, by integrating two adaptive techniques that operate at different time scales we are able to significantly improve quality measures to a nearly optimal level without increasing the required capacity as compared to using the techniques separately.</p><p>There are many related works on policy-based management. For example, in <ref type="bibr" target="#b39">[40]</ref>, the authors statically derive and then dynamically refine low-level service level specifications to meet given SLAs while maximizing business profit.</p><p>There is a new research direction that has emerged from studying server consolidation workloads using a multicore server design <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. The authors show, across a variety of shared cache configurations, that a commercial workload's memory behaviour can be affected in unexpected ways by other workloads. In our work, we do not consider impact of cache sharing, while it is an interesting direction for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>This paper describes an approach for evaluating the impact of policies for resource pool management on required capacity, resource access quality violations, and workload migrations. The evaluation takes into account the impact of different controllers that may operate at different timescales. The approach can be applied to different sets of workloads and different configurations of resource pools. The results can be used to select appropriate policy for a given resource pool scenario. We provide a detailed workload analysis of 138 SAP workloads that operate in an industrial data centre environment. The workloads have sustained bursts in demand that must be taken into account during resource pool management. We propose a resource access quality violation penalty metric that reflects both the duration of violations and the expected impact of the violations on end customers.</p><p>Migration and workload placement controllers are studied in detail. A formal analysis is conducted that explores the impact of migration controller threshold values on metric capacity and violation penalty. The analysis quantifies the impact of the threshold values and show the particular importance of underload threshold values. Such results can be used to guide data centre operators in their choice of thresholds.</p><p>Over 600 simulation experiments are conducted to assess the impact of combinations of migration controller threshold parameters. We use a Pareto-optimal subset of these results as a baseline to further evaluate management policies. The set provides a range of quality versus capacity trade-offs that a resource pool operator could choose from. The migration and workload placement controllers are evaluated alone, in parallel, and in an integrated manner. We found that the integrated controllers had the best quality versus capacity trade-off for our resource pool scenario. The tightest integration had the most benefits, but caused a high workload migration rate. Finally, a version of the workload placement controller is employed that minimizes migrations thereby significantly reducing the migration rate with little impact on capacity and quality.</p><p>We conclude that a reactive migration controller or proactive workload placement controller alone is not adequate for effective resource pool management. A reactive migration controller does not exploit resource saving opportunities from global optimizations while a workload placement controller is unable to reduce violation penalties caused by bursts in demands and overload conditions between changes in workload placements. In addition, it does not take advantage of short term opportunities to remove servers in the same way as the migration controller does.</p><p>Our future work includes evaluating other instances of controllers and management policies, and to develop management policies that react well to more kinds of workloads and different kinds of simulated failures. Finally, we also plan to consider a greater variety of workloads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Top percentile of memory demand for applications under study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig.6. CPU and memory demands for a user interactive workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Migration overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Comparison of different management policies regarding CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Number of migrations for all 10 chosen MC threshold cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Constrained migrations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Top percentile of CPU demand for applications under study.</figDesc><table><row><cell>CPU Demand as % of Peak CPU Demand</cell><cell>0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%</cell><cell>99th Percentile 97th Percentile 95th Percentile Mean Value</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>120</cell><cell>140</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Workload Number</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Fig. 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>CPU quality violations assuming perfect knowledge for the 4 h control interval.</figDesc><table><row><cell>Interval duration</cell><cell cols="3">Total number</cell><cell></cell><cell cols="2">Average number</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5 min</cell><cell cols="2">1090</cell><cell></cell><cell></cell><cell>13 per Day</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 min</cell><cell cols="2">161</cell><cell></cell><cell></cell><cell>1.9 per Day</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>15 min</cell><cell cols="2">14</cell><cell></cell><cell></cell><cell>1.2 per Week</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20 min</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell>1 per Month</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">5 Minutes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell cols="2">15 Minutes 1 Hour</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4 Hours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 Day</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDF</cell><cell>0.6</cell><cell cols="4">1 Week 4 Weeks Initial Rearrangement Only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>2000</cell><cell>4000</cell><cell>6000</cell><cell>8000</cell><cell>10000</cell><cell>12000</cell><cell>14000</cell><cell>16000</cell><cell>18000</cell><cell>20000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Watt Consumption per Interval</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>Fig.</p>10</p>. Power consumption assuming perfect knowledge.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>ANOVA table for CPU violation penalty.</figDesc><table><row><cell>Source</cell><cell>SS</cell><cell>SS in %</cell><cell>df</cell><cell>MS</cell><cell>F-Value</cell><cell>Crit.</cell><cell>Conclusion</cell></row><row><cell>a</cell><cell>32209682</cell><cell>0.17</cell><cell>4</cell><cell>8052420</cell><cell>50.17</cell><cell>2.39</cell><cell>Significant</cell></row><row><cell>b</cell><cell>442121</cell><cell>0</cell><cell>4</cell><cell>110530.3</cell><cell>1</cell><cell>2.39</cell><cell>Not significant</cell></row><row><cell>d</cell><cell>3952996127</cell><cell>20.96</cell><cell>3</cell><cell>1317665376</cell><cell>8209.6</cell><cell>2.623</cell><cell>Significant</cell></row><row><cell>c</cell><cell>9077000224</cell><cell>48.13</cell><cell>5</cell><cell>1815400045</cell><cell>11310.6</cell><cell>2.232</cell><cell>Significant</cell></row><row><cell>ab</cell><cell>2997479</cell><cell>0.02</cell><cell>16</cell><cell>187342</cell><cell>1.167</cell><cell>1.664</cell><cell>Not significant</cell></row><row><cell>ad</cell><cell>20512291</cell><cell>0.11</cell><cell>12</cell><cell>1709358</cell><cell>10.65</cell><cell>1.772</cell><cell>Significant</cell></row><row><cell>ac</cell><cell>37595875</cell><cell>0.2</cell><cell>20</cell><cell>1879794</cell><cell>11.712</cell><cell>1.592</cell><cell>Significant</cell></row><row><cell>bd</cell><cell>701391</cell><cell>0</cell><cell>12</cell><cell>58449.2</cell><cell>0.3</cell><cell>1.772</cell><cell>Not significant</cell></row><row><cell>bc</cell><cell>2969998</cell><cell>0.02</cell><cell>20</cell><cell>148500</cell><cell>0.925</cell><cell>1.592</cell><cell>Not significant</cell></row><row><cell>dc</cell><cell>5653318689</cell><cell>29.98</cell><cell>15</cell><cell>376887913</cell><cell>2348.2</cell><cell>1.687</cell><cell>Significant</cell></row><row><cell>Error</cell><cell>78325016</cell><cell>0.41</cell><cell>488</cell><cell>160502.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell>18859068891</cell><cell>100</cell><cell>599</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>Analysis of variance table for CPU capacity. Chosen combinations of thresholds for further experiments.</figDesc><table><row><cell>Source</cell><cell>SS</cell><cell></cell><cell>SS in %</cell><cell>df</cell><cell>MS</cell><cell></cell><cell cols="2">F-Value</cell><cell>Crit.</cell><cell>Conclusion</cell></row><row><cell>a</cell><cell cols="2">18451760914</cell><cell>2.95</cell><cell>4</cell><cell cols="2">4612940228</cell><cell>1689.8</cell><cell>2.39</cell><cell>Significant</cell></row><row><cell>b</cell><cell cols="2">203448728</cell><cell>0.03</cell><cell>4</cell><cell cols="2">50862182</cell><cell>18.632</cell><cell>2.39</cell><cell>Significant</cell></row><row><cell>d</cell><cell cols="2">93715380384</cell><cell>14.97</cell><cell>3</cell><cell cols="2">31238460128</cell><cell cols="2">11443.3</cell><cell>2.623</cell><cell>Significant</cell></row><row><cell>c</cell><cell cols="2">4.34742E+11</cell><cell>69.44</cell><cell>5</cell><cell cols="2">86948493051</cell><cell cols="2">31851.1</cell><cell>2.232</cell><cell>Significant</cell></row><row><cell>ab</cell><cell cols="2">101095635</cell><cell>0.02</cell><cell>16</cell><cell cols="2">6318477</cell><cell>2.315</cell><cell>1.664</cell><cell>Significant</cell></row><row><cell>ad</cell><cell cols="2">299339472</cell><cell>0.05</cell><cell>12</cell><cell cols="2">24944956</cell><cell>9.138</cell><cell>1.772</cell><cell>Significant</cell></row><row><cell>ac</cell><cell cols="2">3502757297</cell><cell>0.56</cell><cell>20</cell><cell cols="2">175137865</cell><cell>64.157</cell><cell>1.592</cell><cell>Significant</cell></row><row><cell>bd</cell><cell cols="2">66662589</cell><cell>0.01</cell><cell>12</cell><cell cols="2">5555216</cell><cell>2.035</cell><cell>1.772</cell><cell>Significant</cell></row><row><cell>bc</cell><cell cols="2">462381183</cell><cell>0.07</cell><cell>20</cell><cell cols="2">23119059</cell><cell>8.469</cell><cell>1.592</cell><cell>Significant</cell></row><row><cell>dc</cell><cell cols="2">73166116866</cell><cell>11.69</cell><cell>15</cell><cell cols="2">4877741124</cell><cell>1786.8</cell><cell>1.687</cell><cell>Significant</cell></row><row><cell>Error</cell><cell cols="2">1332165080</cell><cell>0.21</cell><cell>488</cell><cell cols="2">2729846</cell><cell></cell></row><row><cell>Total</cell><cell cols="2">6.26044E+11</cell><cell>100</cell><cell>599</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Suboptimal Combinations</cell></row><row><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Pareto-Optimal Combinations Chosen Experiments</cell></row><row><cell></cell><cell>CPU Violation Penalties Per Hour</cell><cell>2 4 6 8 10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>1.1</cell><cell>1.2</cell><cell>1.3</cell><cell>1.4</cell><cell>1.5</cell><cell>1.6</cell><cell>1.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Normalized Capacity</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Fig. 11. Table 4</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Migration controller thresholds for 10 Pareto-optimal cases.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a (%)</cell><cell>b (%)</cell><cell>c (%)</cell><cell>d (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>99</cell><cell>99</cell><cell>60</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell>95</cell><cell>60</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>99</cell><cell>99</cell><cell>50</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell>95</cell><cell>50</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell>90</cell><cell>50</cell><cell>70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>99</cell><cell>95</cell><cell>40</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell>99</cell><cell>40</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>99</cell><cell>95</cell><cell>40</cell><cell>60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>99</cell><cell>90</cell><cell>30</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>99</cell><cell>99</cell><cell>40</cell><cell>40</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We note that such penalties may be translated to monetary penalties in financially driven systems and that monetary penalties are likely to be bounded in such systems.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>D. Gmach et al. / Computer Networks 53 (2009) 2905-2922</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The first and second quartile refer to the 25 and 50 percentiles.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>D. Gmach et al. / Computer Networks 53 (2009) 2905-2922</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(TUM), Germany. His research interests are in the realization of highly scalable, distributed database systems, data stream management, peerto-peer information systems, grid computing, query optimization and dynamic information fusion of Internet data sources to cope with the ever growing data explosion using automated analysis and query processing techniques. Beside numerous international research publications he is the author of the market leading German database textbook, which is currently available in its sixth edition by Oldenbourg-Verlag.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A capacity management service for resource pools</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andrzejak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Workshop on Software and Performance (WOSP)</title>
		<meeting>5th Int. Workshop on Software and Performance (WOSP)<address><addrLine>Palma, Illes Balears, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Live migration of virtual machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Limpach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>of the 2nd Symposium on Networked Systems Design and Implementation (NSDI)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="273" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AutoGlobe: an automatic administration concept for service-oriented database applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seltzsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krompass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Int. Conf. on Data Engineering (ICDE), Industrial Track</title>
		<meeting>of the 22nd Int. Conf. on Data Engineering (ICDE), Industrial Track<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Workload analysis and demand prediction of enterprise data center applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Symposium on Workload Characterization (IISWC)</title>
		<meeting>of the IEEE Int. Symposium on Workload Characterization (IISWC)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An integrated approach to resource pool management: policies, efficiency and quality metrics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Belrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turicchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 38th IEEE/IFIP Int. Conf. on Dependable Systems and Networks (DSN)</title>
		<meeting>of the 38th IEEE/IFIP Int. Conf. on Dependable Systems and Networks (DSN)<address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artificial Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R-Opus: a composite framework for application performability and QoS in shared resource pools</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Dependable Systems and Networks (DSN)</title>
		<meeting>of the Int. Conf. on Dependable Systems and Networks (DSN)<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive quality of service management for enterprise services</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krompass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on the Web (TWEB)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Full-system power analysis and modeling for server environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Economou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rivoire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Modeling, Benchmarking, and Simulation (MoBS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Kleinrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queueing Systems, Theorie</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1975">1975</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Predicting the performance of software systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frdrick</surname></persName>
		</author>
		<ptr target="&lt;http://linux.die.net/man/8/vmstat&gt;" />
		<title level="m">Linux Man Page: vmstat(8)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Web server workload characterization: the search for invariants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMETRICS Int. Conf. on Measurement and Modeling of Computer Systems, ACM</title>
		<meeting>of the ACM SIGMETRICS Int. Conf. on Measurement and Modeling of Computer Systems, ACM<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Characterizing locality, evolution, and life span of accesses in enterprise media server workloads</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Int. Workshop on Network and Operating Systems Support for Digital Audio and Video (NOSSDAV)</title>
		<meeting>of the 12th Int. Workshop on Network and Operating Systems Support for Digital Audio and Video (NOSSDAV)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Measuring CPU overhead for I/O processing in the Xen virtual machine monitor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ATEC &apos;05: Proc. of the USENIX Annual Techn. Conf., USENIX Association</title>
		<meeting><address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="24" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enforcing performance isolation across virtual machines in Xen</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM/IFIP/USENIX 7th Int. Middleware Conf</title>
		<meeting>of the ACM/IFIP/USENIX 7th Int. Middleware Conf<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Power and Cost Savings Using NetXen&apos;s 10GbE Intelligent NIC, White Paper</title>
		<author>
			<persName><surname>Netxen</surname></persName>
		</author>
		<ptr target="&lt;http://www.netxen.com/technology/pdfs/Power_page.pdf&gt;" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Art of Computer Systems Performance Analysis: Techniques for Experimental Design</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulation, and Modeling</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">No power struggles: coordinated multi-level power management for the data center</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS XIII: Proc. of the 13th Int. Conf. on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Black-box and gray-box strategies for virtual machine migration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yousif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>of the 4th USENIX Symposium on Networked Systems Design and Implementation<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="229" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><surname>Hp</surname></persName>
		</author>
		<ptr target="&lt;https://h30046.www3.hp.com/campaigns/2007/promo/VSE/index.php&gt;" />
	</analytic>
	<monogr>
		<title level="s">HP Virtual Server Environment</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic reconfiguration: basic building blocks for autonomic computing on IBM pSeries servers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Burugula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bounding the resource savings of utility computing models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andrzejak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>HP Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. HPL-2002-339</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Resource overbooking and application profiling in shared hosting platforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Issue: Cluster Resource Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="239" to="254" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>ACM SIGOPS Operating System Review</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical service assurances for applications in utility grid environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andrzejak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="319" to="339" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">VMWare capacity planner</title>
		<author>
			<persName><surname>Vmware</surname></persName>
		</author>
		<ptr target="&lt;http://www.vmware.com/products/capacity_planner/&gt;" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">HP Integrity Essentials Capacity Advisor</title>
		<author>
			<persName><surname>Hp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. &lt;http:// h71036.www7.hp.com/enterprise/cache/262379-0-0-0-121.html&gt;</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tivoli</forename><forename type="middle">Performance</forename><surname>Ibm</surname></persName>
		</author>
		<author>
			<persName><surname>Analyzer</surname></persName>
		</author>
		<ptr target="&lt;http://www.ibm.com/software/tivoli/products/performance-analyzer/&gt;" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">TeamQuest -IT Service Optimization</title>
		<author>
			<persName><surname>Teamquest</surname></persName>
		</author>
		<ptr target="&lt;http://www.teamQuest.com&gt;" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">VMware VMotion</title>
		<author>
			<persName><surname>Vmware</surname></persName>
		</author>
		<ptr target="&lt;http://www.vmware.com/products/vi/vc/vmotion.html&gt;" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual machine hosting for networked clusters: building the foundations for autonomic orchestration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yumerefendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st Int. Workshop on Virtualization Technology in Distributed Computing (VTDC 2006)</title>
		<meeting>of the 1st Int. Workshop on Virtualization Technology in Distributed Computing (VTDC 2006)<address><addrLine>Tampa, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Autonomic live adaptation of virtual computational environments in a multidomain infrastructure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kennell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goasguen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd IEEE Int. Conf. on Autonomic Computing (ICAC)</title>
		<meeting>of the 3rd IEEE Int. Conf. on Autonomic Computing (ICAC)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Increasing application performance in virtual environments through run-time inference and adaptation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Sundararaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Dinda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th IEEE Int. Symposium on High Performance Distributed Computing (HPDC)</title>
		<meeting>of the 14th IEEE Int. Symposium on High Performance Distributed Computing (HPDC)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">VMware dynamic resource scheduler</title>
		<author>
			<persName><surname>Vmware</surname></persName>
		</author>
		<ptr target="&lt;http://www.vmware.com/products/vi/vc/drs.html&gt;" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">1000 Islands: integrated capacity and workload management for the next generation data center</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th IEEE Int. Conf. on Autonomic Computing (ICAC&apos;08)</title>
		<meeting>of the 5th IEEE Int. Conf. on Autonomic Computing (ICAC&apos;08)<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Agile dynamic provisioning of multi-tier internet applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Autonomous and Adaptive Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>TAAS)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enabling resource sharing between transactional and batch workloads using dynamic application placement</title>
		<author>
			<persName><forename type="first">D</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Whalley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th ACM/IFIP/ USENIX Int. Conf. on Middleware (Middleware)</title>
		<meeting>of the 9th ACM/IFIP/ USENIX Int. Conf. on Middleware (Middleware)<address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive load sharing in homogeneous distributed systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Eager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zahorjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="662" to="675" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The limited performance benefits of migrating active processes for load sharing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Eager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zahorjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On leveraging policy-based management for maximizing business profit</title>
		<author>
			<persName><forename type="first">I</forename><surname>Aib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network and Service Management</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="25" to="39" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An evaluation of server consolidation workloads for multi-core designs</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vantrease</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symposium on Workload Characterization (IISWC 2007)</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Virtual hierarchies to support server consolidation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 34th Annual Int. Symposium on Computer Architecture (ISCA), ACM</title>
		<meeting>of the 34th Annual Int. Symposium on Computer Architecture (ISCA), ACM<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
