<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Exemplar Affinity Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chang-Dong</forename><surname>Wang</surname></persName>
							<email>changdongwang@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="middle">C Y</forename><surname>Suen</surname></persName>
							<email>suen@cenparmi.concordia.ca..</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="laboratory">and with Guangdong Province Key Laboratory of Information Security</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>Waihuan East Road</addrLine>
									<postCode>510006</postCode>
									<settlement>Panyu District, Guangzhou</settlement>
									<region>Guangdong</region>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Centre for Pattern Recognition and Machine Intelligence (CENPARMI)</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<addrLine>Suite EV3.403, 1445 de Maisonneuve Blvd West</addrLine>
									<postCode>H3G 1M8</postCode>
									<settlement>Montre´al</settlement>
									<region>Que´bec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Mathematics and Computational Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Exemplar Affinity Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2AD4A5ACD10A629598ADC252CCED5772</idno>
					<idno type="DOI">10.1109/TPAMI.2013.28</idno>
					<note type="submission">received 20 Feb. 2011; revised 22 June 2012; accepted 11 Jan. 2013; published online 24 Jan. 2013.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Clustering</term>
					<term>multi-exemplar</term>
					<term>affinity propagation</term>
					<term>factor graph</term>
					<term>max-product belief propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The affinity propagation (AP) clustering algorithm has received much attention in the past few years. AP is appealing because it is efficient, insensitive to initialization, and it produces clusters at a lower error rate than other exemplar-based methods. However, its single-exemplar model becomes inadequate when applied to model multisubclasses in some situations such as scene analysis and character recognition. To remedy this deficiency, we have extended the single-exemplar model to a multi-exemplar one to create a new multi-exemplar affinity propagation (MEAP) algorithm. This new model automatically determines the number of exemplars in each cluster associated with a super exemplar to approximate the subclasses in the category. Solving the model is NP-hard and we tackle it with the max-sum belief propagation to produce neighborhood maximum clusters, with no need to specify beforehand the number of clusters, multi-exemplars, and superexemplars. Also, utilizing the sparsity in the data, we are able to reduce substantially the computational time and storage. Experimental studies have shown MEAP's significant improvements over other algorithms on unsupervised image categorization and the clustering of handwritten digits.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A FFINITY propagation (AP) [1] is an exemplar-based clustering method developed recently. It takes as input the similarities between data points and produces a set of exemplars and the assignments of data points to the most appropriate exemplars. The exemplars are defined to be the data points best representing the data. It utilizes the maxproduct belief propagation algorithm over factor graph <ref type="bibr">[2]</ref> to generate clusters insensitive to initialization and converged to the neighborhood maximum <ref type="bibr">[3]</ref>. As reported in <ref type="bibr">[1]</ref>, the AP algorithm has three advantages over other exemplar-based clustering methods: 1) It is efficient, 2) it is insensitive to initialization, and 3) it can find clusters with less error than k-centers (exemplar version of k-means <ref type="bibr">[4]</ref>, <ref type="bibr">[5]</ref>). Therefore, it is very attractive and has been applied in many real-world applications, such as treatment portfolio design <ref type="bibr" target="#b12">[6]</ref>, ROI detection <ref type="bibr" target="#b13">[7]</ref>, tissue clustering <ref type="bibr" target="#b14">[8]</ref>, image categorization <ref type="bibr" target="#b15">[9]</ref>, subspace division <ref type="bibr" target="#b16">[10]</ref>, and so on. Meanwhile, many extensions have been developed, such as soft-constraint AP <ref type="bibr" target="#b17">[11]</ref>, Dirichlet process AP <ref type="bibr" target="#b18">[12]</ref>, streaming AP <ref type="bibr" target="#b19">[13]</ref>, semi-supervised AP <ref type="bibr" target="#b20">[14]</ref>, hierarchical AP <ref type="bibr" target="#b21">[15]</ref>, <ref type="bibr" target="#b22">[16]</ref>, and so on.</p><p>Despite significant success, one drawback of AP is that it cannot model the category consisting of multiple subclasses since it represents each cluster by a single exemplar. In many applications, such as image categorization <ref type="bibr" target="#b23">[17]</ref>, face categorization <ref type="bibr" target="#b24">[18]</ref>, multifont optical character recognition <ref type="bibr" target="#b25">[19]</ref>, and handwritten digit classification <ref type="bibr" target="#b26">[20]</ref>, each category may contain several subclasses. For instance, in the natural scene categorization experiments, a scene category often contains multiple "themes" <ref type="bibr" target="#b23">[17]</ref>, for example, the street scene may contain themes like "road," "car," "pedestrian," "building," and so on. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates two typical themes (with and without "sunset/sunrise") of the coast scene from the dataset of 13 natural scene categories (SceneClass13) <ref type="bibr" target="#b23">[17]</ref>. Similarly, in the face categorization experiments, images of the same person in different facial expressions should be taken as in distinct subclasses <ref type="bibr" target="#b24">[18]</ref>. In the applications of optical character recognition and handwritten digit classification, the class representing a letter or a digit could be composed of several subclasses, each corresponding to a different style or font <ref type="bibr" target="#b25">[19]</ref>, <ref type="bibr" target="#b26">[20]</ref>. The data containing multiple subclasses obviously cannot be represented by a single exemplar, which leads to the failure of k-centers and AP in clustering the multiple subclasses data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous Work</head><p>To solve the above problems, we can use the nonlinear clustering methods. Kernel-based methods <ref type="bibr" target="#b27">[21]</ref> and spectral clustering <ref type="bibr" target="#b28">[22]</ref> are two typical nonlinear techniques. In kernel-based clustering <ref type="bibr" target="#b27">[21]</ref>, <ref type="bibr" target="#b29">[23]</ref>, a kernel mapping is first used to embed the data into a feature space where the nonlinear pattern becomes linearly separable and then the clustering is performed in the feature space. Alternatively, in spectral clustering <ref type="bibr" target="#b28">[22]</ref>, <ref type="bibr" target="#b30">[24]</ref>, we first construct a weighted graph and then use the eigenvectors of an affinity matrix to obtain a clustering of the data. Unfortunately, as pointed out in <ref type="bibr" target="#b24">[18]</ref> and <ref type="bibr" target="#b30">[24]</ref>, two main problems prevent the efficient use of such techniques. The first one is that it is difficult to find the appropriate kernel for each particular problem.</p><p>Second, the nonlinear methods usually have an associated high computational cost.</p><p>One simple but effective alternative is the multiexemplar representation, which models a class via multiple exemplars. The multi-exemplar model can be easily found in the literatures. In supervised learning, the multiexemplar representation was first developed in <ref type="bibr" target="#b25">[19]</ref>. Subsequently, it has been widely used in handwritten digit classification <ref type="bibr" target="#b26">[20]</ref>, face recognition <ref type="bibr" target="#b31">[25]</ref>, and word meaning encoding <ref type="bibr" target="#b32">[26]</ref>. Also, it has been used to enable the classifier to grow and evolve due to the change of data distribution <ref type="bibr" target="#b33">[27]</ref>. In <ref type="bibr" target="#b34">[28]</ref>, Aiolli and Sperduti extended SVM to multiple exemplars per class so as to get very expressive decision functions without requiring the use of kernels. Another similar approach is the subclass analysis <ref type="bibr" target="#b24">[18]</ref>, where a mixture of Gaussians is used to approximate the underlying distribution of each class.</p><p>In the clustering literature, relatively less work has been done in multi-exemplar representation. The main reason is that without training samples, it is more difficult to estimate the number of clusters and subclusters used to represent each cluster. Additionally, it needs to tune more parameters than the single-exemplar representation. The first work is a hierarchical clustering method termed clustering using representatives <ref type="bibr" target="#b35">[29]</ref>, in which a constant number of wellscattered points are first chosen and then shrunk toward the cluster centroid to represent each cluster. Similarly, Liu et al. <ref type="bibr" target="#b36">[30]</ref> developed a multiprototype clustering (MCP) algorithm for the partitional clustering in which a number of prototypes are precomputed and a separation measure is used to decide whether two precomputed prototypes should be separated or merged. Another heuristic MCP method was developed based on the minimum spanning tree <ref type="bibr" target="#b37">[31]</ref>. Although these clustering methods can discover clusters consisting of multiple subclasses, unfortunately they have to tune some parameters. For instance, in <ref type="bibr" target="#b35">[29]</ref>, the number of points representing each cluster has to be prespecified beforehand. In <ref type="bibr" target="#b36">[30]</ref>, the thresholds deciding whether two precomputed prototypes should be separated or merged have a profound effect on the clustering results. The degree deciding whether a pattern should be taken as the potential prototype also affects the performance of the method in <ref type="bibr" target="#b37">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Extension of Single to Multi-Exemplar Model</head><p>Inspired by the previous work on the multi-exemplar representation and the subclass analysis, this paper extends the single-exemplar model to a multi-exemplar one and proposes a novel multi-exemplar affinity propagation (MEAP) algorithm. Each cluster is modeled by an automatically determined number of exemplars and a superexemplar. Each data point is assigned to the most appropriate exemplar and each exemplar is assigned to the most appropriate superexemplar. The superexemplar is defined as an exemplar best representing the exemplars belonging to the corresponding cluster. The objective of the model is to maximize the sum of all similarities between data points and the corresponding exemplars plus the sum of all linkages between exemplars and the corresponding superexemplars. Solving the model is NP-hard. To this end, the max-sum (the log -domain max-product) belief propagation <ref type="bibr">[2]</ref> is utilized, producing clusters insensitive to initialization and converged to the neighborhood maximum <ref type="bibr">[3]</ref>. To take advantage of the sparsity in data, we further implement a fast MEAP in which both the computational time and storage are dramatically reduced. The proposed MEAP algorithm has the following major advantages:</p><p>. It can model the category of more complex structure than AP without increasing the model complexity.</p><p>Although one can learn a suitable metric for AP to characterize multisubclass structures (e.g., mapping the original pattern to an appropriate kernel space), it would make the learning model complicated and time-consuming. The MEAP algorithm inherits the advantages of AP, i.e., the same computational complexity and convergence property, but can model more complex structures. . Compared with existing multi-exemplar clustering methods, the proposed MEAP algorithm can automatically estimate the number of clusters and the appropriate number of subclusters within each cluster. Additionally, it does not have to tune any parameter to realize this automatic estimation. On the contrary, its counterparts work well only when all the parameters are properly adjusted. The remainder of this paper is organized as follows: Section 2 briefly introduces the key elements of AP. In Section 3, we describe the new MEAP method. The multiexemplar model is first described and its underlying rationale is discussed. Then the max-sum belief propagation-based optimization for the model is introduced. We also implement a Fast MEAP to take advantage of the sparsity in data. The theoretical comparison between AP and MEAP is conducted to show that the AP algorithm can be viewed as a special case of MEAP. Sections 4 and 5 report the experimental results on three image categorization datasets and two handwritten digit datasets, respectively. Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AFFINITY PROPAGATION</head><p>AP is a single-exemplar clustering algorithm using maxsum (max-product) belief propagation to obtain good exemplars. Given a user-defined similarity matrix ½s ij NÂN of N points, it aims at searching for a valid configuration of labels c ¼ ½c 1 ; . . . ; c N to maximize the following objective function <ref type="bibr" target="#b38">[32]</ref>:  </p><formula xml:id="formula_0">¼ X N i¼1 s ici þ X N k¼1 k ðcÞ;<label>ð1Þ</label></formula><p>where k ðcÞ is an exemplar-consistency constraint such that if some data point i has selected k as its exemplar, i.e., c i ¼ k, then data point k must select itself as an exemplar, i.e.,</p><formula xml:id="formula_1">c k ¼ k, k ðcÞ ¼ À1; if c k 6 ¼ k but 9i : c i ¼ k; 0; otherwise: &amp;<label>ð2Þ</label></formula><p>AP is an optimized max-sum belief propagation algorithm over the factor graph in Fig. <ref type="figure">2</ref>. It begins by simultaneously considering all data points as potential exemplars, and recursively transmits real-valued messages between data points until high-quality exemplars emerge. There are two kinds of messages, which are rði; kÞ, sent from point i to the candidate exemplar k, reflecting the accumulated evidence for how well-suited point k is to serve as the exemplar for point i, and aði; kÞ, sent from the candidate exemplar k to point i, reflecting the accumulated evidence for how appropriate it would be for point i to choose point k as its exemplar (see Fig. <ref type="figure">2</ref>). They are initialized as zero and updated, respectively, as follows: The assignment vector c ¼ ½c 1 ; . . . ; c N is computed as c i ¼ arg max j ½aði; jÞ þ rði; jÞ after convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-EXEMPLAR AFFINITY PROPAGATION</head><p>Let ½s ij NÂN be a user-defined similarity matrix with s ij measuring the similarity between point i and the potential exemplar j, and ½l ij NÂN a linkage matrix with l ij measuring the linkage between exemplar i and its potential superexemplar j. We develop a multi-exemplar model that seeks two mappings, 1 : f1; . . . ; Ng ! f1; . . . ; Ng assigning data point i to exemplar 1 ðiÞ, and 2 : f 1 ð1Þ; . . . ; 1 ðNÞg ! f 1 ð1Þ; . . . ; 1 ðNÞg assigning exemplar 1 ðiÞ to superexemplar 2 ð 1 ðiÞÞ ¼ ð 2 1 ÞðiÞ, where denotes the function composition. The goal is to maximize the sum S 1 of all similarities between data points and the corresponding exemplars plus the sum S 2 of all linkages between exemplars and the corresponding superexemplars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Model</head><p>Let C ¼ ½c ij NÂN be an assignment matrix, where the nondiagonal elements c ij 2 f0; 1g (j 6 ¼ i) denote that point j is the exemplar of point i if c ij ¼ 1, i.e., 1 ðiÞ ¼ j, and the diagonal elements c ii 2 f0; . . . ; Ng denote that exemplar c ii is the superexemplar of exemplar i if c ii 2 f1; . . . ; Ng, i.e., 2 ðiÞ ¼ c ii ,</p><formula xml:id="formula_2">c ij ¼ 1; if j is an exemplar of i 0; otherwise; &amp; 8i 6 ¼ j;<label>ð3Þ</label></formula><formula xml:id="formula_3">c ii ¼ k 2 f1; . . . ; Ng; if k is a superexemplar of i; 0; if i is not an exemplar: &amp;<label>ð4Þ</label></formula><p>The sum of all similarities between data points and the corresponding exemplars, i.e., S 1 , and the sum of all linkages between exemplars and the corresponding superexemplars, i.e., S 2 , can be, respectively, expressed as</p><formula xml:id="formula_4">S 1 ¼ X N i¼1 X N j¼1 s ij Á ½c ij 6 ¼ 0; S 2 ¼ X N i¼1 l ic ii Á ½c ii 6 ¼ 0;<label>ð5Þ</label></formula><p>where ½Á is the Iverson notation with ½true ¼ 1 and ½false ¼ 0. We define a function matrix ½S ij ðc ij Þ NÂN with nondiagonal elements incorporating the similarities s ij between data point i and the potential exemplar j, and diagonal elements incorporating the exemplar preference s ii plus the linkage l icii between exemplar i and its superexemplar c ii . That is,</p><formula xml:id="formula_5">S ij ðc ij Þ ¼ s ij ; if i 6 ¼ j &amp; c ij 6 ¼ 0; s ii þ l icii ; if i ¼ j &amp; c ii 6 ¼ 0; 0; otherwise: 8 &lt; :<label>ð6Þ</label></formula><p>We have</p><formula xml:id="formula_6">S 1 þ S 2 ¼ P N i¼1 P N j¼1 S ij ðc ij Þ.</formula><p>The valid assignment matrix C must satisfy the following three constraints:</p><p>1. Exemplar's "1-of-N"constraint <ref type="bibr" target="#b39">[33]</ref>. Each data point i must be assigned to exactly one exemplar:</p><formula xml:id="formula_7">I i ðc i1 ; . . . ; c iN Þ ¼ À1; if P N j¼1 ½c ij 6 ¼ 0 6 ¼ 1; 0; otherwise: &amp;<label>ð7Þ</label></formula><p>2. Exemplar consistency constraint <ref type="bibr" target="#b39">[33]</ref>. If there exists a data point i selecting data point j as its exemplar, then data point j must be an exemplar itself:</p><formula xml:id="formula_8">E j ðc 1j ; . . . ; c Nj Þ ¼ À1; if c jj ¼ 0 but 9i : c ij ¼ 1; 0; otherwise: &amp;<label>ð8Þ</label></formula><p>3. Superexemplar consistency constraint. If some exemplar i has chosen exemplar k as its superexemplar, i.e., c ii ¼ k, then k must be a superexemplar itself:</p><formula xml:id="formula_9">F k ðc 11 ; . . . ; c NN Þ ¼ À1; if c kk 6 ¼ k but 9i : c ii ¼ k; 0; otherwise: &amp;<label>ð9Þ</label></formula><p>The goal of the multi-exemplar model is to maximize the following objective function:</p><formula xml:id="formula_10">Fig. 2.</formula><p>Factor graph of the AP method and its messages.</p><formula xml:id="formula_11">SðCÞ ¼ S 1 þ S 2 þ three constraints ¼ X N i¼1 X N j¼1 S ij ðc ij Þ þ X N i¼1 I i ðc i1 ; . . . ; c iN Þ þ X N j¼1 E j ðc 1j ; . . . ; c Nj Þ þ X N k¼1 F k ðc 11 ; . . . ; c NN Þ:<label>ð10Þ</label></formula><p>Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the multi-exemplar model. The data points, exemplars, and superexemplars form a two-layer structure by the two mappings 1 and 2 . The lower layer is modeled by the mapping 1 . The sum of all similarities between data points and the corresponding exemplars, i.e., S 1 , is used to measure the within-subcluster compactness. The higher layer is modeled by the mapping 2 . The sum of all linkages between exemplars and the corresponding superexemplars, i.e., S 2 , is used to measure the withincluster compactness. From the single-exemplar theory, maximizing the within-cluster similarity automatically maximizes the between-cluster separation <ref type="bibr" target="#b40">[34]</ref>. Therefore, the appropriate multi-exemplar model should be that both the within-subcluster compactness and the within-cluster compactness are maximized. Maximizing S 1 þ S 2 under the constraints of producing valid clusters (i.e., I; E; F ) makes the model effectively characterize clusters consisting of multiple subclusters. Fig. <ref type="figure">4</ref> compares MEAP with AP in one synthetic dataset. From the viewpoint of the maximum margin clustering <ref type="bibr" target="#b41">[35]</ref>, MEAP finds better decision boundaries than AP, i.e., larger margins are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization</head><p>Exactly searching for an optimal assignment matrix that maximizes the objective function ( <ref type="formula" target="#formula_11">10</ref>) is NP-hard since the multi-exemplar model is a generalization of the singleexemplar one, the optimization of which is proven to be NP-hard <ref type="bibr">[1]</ref>. To this end, the max-sum belief propagation is utilized, which is a local-message-passing algorithm guaranteed to converge to the neighborhood maximum <ref type="bibr">[3]</ref>. The factor graph is shown in Fig. <ref type="figure">5</ref>. There are seven types of messages passing between variable nodes and function nodes, as shown in Fig. <ref type="figure">6</ref>. In the max-sum algorithm, the message updating involves either a message from a variable to each adjacent function or that from a function to each adjacent variable. The message from a variable to a function sums together the messages from all adjacent functions except the one receiving the message <ref type="bibr">[2]</ref>:</p><formula xml:id="formula_12">x!f ðxÞ X h2neðxÞnffg h!x ðxÞ;<label>ð11Þ</label></formula><p>where neðxÞ denotes the set of adjacent functions of variable x.</p><p>The message from a function to a variable involves a maximization over all arguments of the function except the variable receiving the message <ref type="bibr">[2]</ref>:</p><formula xml:id="formula_13">f!x ðxÞ max Xnfxg fðXÞ þ X y2Xnfxg y!f ðyÞ 2 4 3 5 ;<label>ð12Þ</label></formula><p>where X ¼ neðfÞ is the set of arguments of function f. Since the messages associated with the nondiagonal variables (i.e., c ij ; i 6 ¼ j) and the diagonal variables (i.e., c ii ) are quite different, we will discuss them separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Messages of Nondiagonal Elements</head><p>As shown on the left of Fig. <ref type="figure">6</ref>, there are five types of messages associated with c ij ; i 6 ¼ j as follows (m ¼ 0; 1): Fig. <ref type="figure">4</ref>. MEAP versus AP. The two ground-truth categories are plotted by red hexagrams and blue squares, respectively. The decision boundaries are plotted by dash curves. Each point is assigned to the most appropriate exemplar by thin lines. In MEAP, each exemplar (small "?") is assigned to its most appropriate superexemplar (large "?") by thick lines. </p><formula xml:id="formula_14">ij ðmÞ ¼ Sij!cij ðmÞ ¼ S ij ðmÞ; ð13Þ ij ðmÞ ¼ c ij !E j ðmÞ ¼ ij ðmÞ þ ij ðmÞ; ð14Þ ij ðmÞ ¼ E j !c ij ðmÞ ¼ max c i 0 j :i 0 6 ¼i E j ðc 1j ; . . . ; c Nj Þ þ X i 00 6 ¼i i 00 j ðc i 00 j Þ ! ;<label>ð15Þ</label></formula><formula xml:id="formula_15">ij ðmÞ ¼ cij!Ii ðmÞ ¼ ij ðmÞ þ ij ðmÞ;<label>ð16Þ</label></formula><formula xml:id="formula_16">ij ðmÞ ¼ Ii!cij ðmÞ ¼ max c ij 0 :j 0 6 ¼j ½I i ðc i1 ; . . . ; c iN Þ þ X j 00 6 ¼j ij 00 ðc ij 00 Þ:<label>ð17Þ</label></formula><p>According to ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula">13</ref>), these messages take as input the similarity s ij , i 6 ¼ j. Consequently, these messages reflect the accumulated evidence for deciding the partial mapping 1 from data point i to the potential exemplar j (j</p><formula xml:id="formula_17">6 ¼ i). That is, m ¼ c ij ¼ 1 implies that 1 ðiÞ ¼ j, and m ¼ c ij ¼ 0 implies that 1 ðiÞ 6 ¼ j.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Messages of Diagonal Elements</head><p>There are seven types of messages associated with c ij , i ¼ j (the right of Fig. <ref type="figure">6</ref>) as follows (m ¼ 0; . . . ; N):</p><formula xml:id="formula_18">ii ðmÞ ¼ S ii !c ii ðmÞ ¼ S ii ðmÞ;<label>ð18Þ</label></formula><formula xml:id="formula_19">ii ðmÞ ¼ c ii !E i ðmÞ ¼ ii ðmÞ þ ii ðmÞ þ X N k¼1</formula><p>ik ðmÞ; ð19Þ</p><p>ii ðmÞ ¼ Ei!cii ðmÞ ¼ max</p><formula xml:id="formula_20">c i 0 i :i 0 6 ¼i ½E i ðc 1i ; . . . ; c Ni Þ þ X i 00 6 ¼i i 00 i ðc i 00 i Þ;<label>ð20Þ</label></formula><formula xml:id="formula_21">ii ðmÞ ¼ c ii !I i ðmÞ ¼ ii ðmÞ þ ii ðmÞ þ X N k¼1 ik ðmÞ; ð21Þ ii ðmÞ ¼ Ii!cii ðmÞ ¼ max c ii 0 :i 0 6 ¼i ½I i ðc i1 ; . . . ; c iN Þ þ X i 00 6 ¼i ii 00 ðc ii 00 Þ;<label>ð22Þ</label></formula><formula xml:id="formula_22">ik ðmÞ ¼ cii!Fk ðmÞ ¼ ii ðmÞ þ ii ðmÞ þ ii ðmÞ þ X k 0 6 ¼k ik 0 ðmÞ;<label>ð23Þ</label></formula><formula xml:id="formula_23">ik ðmÞ ¼ Fk!cii ðmÞ ¼ max c i 0 i 0 :i 0 6 ¼i ½F k ðc 11 ; . . . ; c NN Þ þ X j6 ¼i jk ðc jj Þ:<label>ð24Þ</label></formula><p>According to ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_18">18</ref>), these messages take as input both of the exemplar preference s ii and the linkage l icii between exemplars and superexemplars. Consequently, these messages reflect the accumulated evidence for simultaneously deciding the partial mapping 1 from point i to itself and the mapping 2 from exemplar i to the potential superexemplar k (k can be either</p><formula xml:id="formula_24">k ¼ i or k 6 ¼ i).</formula><p>That is, m ¼ c ii 2 f1; . . . ; Ng implies that 1 ðiÞ ¼ i; 2 ðiÞ ¼ c ii , and m ¼ c ii ¼ 0 implies that 1 ðiÞ 6 ¼ i and i is not in the domain of 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Simplified Messages</head><p>By applying some mathematical tricks used in <ref type="bibr">[1]</ref> and <ref type="bibr" target="#b39">[33]</ref>,</p><p>we can obtain the simplified messages as follows:</p><formula xml:id="formula_25">i 6 ¼ j ij s ij À max h max j 0 6 2fj;ig Â s ij 0 þ ij 0 Ã ; max m2f1;...;Ng ½l im þ im þ s ii þ ii i ;<label>ð25Þ</label></formula><formula xml:id="formula_26">ij min h 0; max m2f1;...;Ng m j þ X i 0 6 2fi;jg max½0; i 0 j i ;<label>ð26Þ</label></formula><formula xml:id="formula_27">8i ¼ 1; . . . ; N; k ¼ 1; . . . ; N k i s ii þ l ik À max i 0 6 ¼i ½s ii 0 þ ii 0 þ ik ;<label>ð27Þ</label></formula><p>ii</p><formula xml:id="formula_28">X i 0 6 ¼i max Â 0; i 0 i Ã ;<label>ð28Þ</label></formula><formula xml:id="formula_29">ik min l ik À max m6 ¼k ½l im þ im ; ii þ k i À ik ! ;<label>ð29Þ</label></formula><formula xml:id="formula_30">kk X i 0 6 ¼i max Â 0; i 0 k Ã ;<label>ð30Þ</label></formula><formula xml:id="formula_31">ik min 0; kk þ X i 0 6 2fi;kg max½0; i 0 k ! ; k 6 ¼ i;<label>ð31Þ</label></formula><formula xml:id="formula_32">where ij ¼ ij ð1Þ À ij ð0Þ; ij ¼ ij ð1Þ À ij ð0Þ; k i ¼ ii ðkÞ À ii ð0Þ; ii ¼ ii ðkÞ À ii ð0Þ; ik ¼ ik ðkÞ À max m6 ¼k ik ðmÞ; ik ¼ ik ðkÞ À ik ðm : m 6 ¼ kÞ;</formula><p>and ; are eliminated. The values of all messages , , , and are initialized as zero and updated via <ref type="bibr" target="#b31">(25)</ref> to <ref type="bibr" target="#b37">(31)</ref>. The derivation can be found in the supplemental material, which can be found in the Computer Society Digital Library at http://doi.ieeecomputersociety.org/ 10.1109/TPAMI.2013.28. The message-updating procedure may be terminated after the local decisions stay constant for some number of iterations t conv or after a fixed number of iterations t max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Computing Assignment Matrix</head><p>To estimate the value of an element c ij , we sum together all incoming messages to c ij and take the value ĉij that maximizes the sum. That is,</p><formula xml:id="formula_33">ĉij ¼ arg max c ij Â ij ðc ij Þ þ ij ðc ij Þ þ ij ðc ij Þ Ã ; ¼ 1; if ij þ ij ! 0 0; otherwise &amp; 8i 6 ¼ j;<label>ð32Þ</label></formula><formula xml:id="formula_34">ĉii ¼ arg max cii h ii ðc ii Þ þ ii ðc ii Þ þ ii ðc ii Þ þ X N k¼1 ik ðc ii Þ i ; ¼ arg max k k i ; if ii þ max k k i ! 0; 0; otherwise: (<label>ð33Þ</label></formula><p>From the assignment matrix, we obtain the two mappings 1 and 2 , and thus generate the clustering labels fð 2 1 Þð1Þ; . . . ; ð 2 1 ÞðNÞg of N data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sparse Similarity and Fast MEAP</head><p>Like AP, MEAP is well suited to taking advantage of the sparsity in data. <ref type="foot" target="#foot_0">1</ref> As discussed in <ref type="bibr" target="#b38">[32]</ref>, the computational complexity of the message passing through each iteration is OðN 2 Þ, which takes the P , max , and min operations as a single step. In some applications <ref type="bibr">[1]</ref>, the problems are structured in such a way that many data points cannot be represented by some others as exemplars and superexemplars. That is, 9i; j such that s ij ¼ À1; l ij ¼ À1. In this circumstance, ij is automatically À1 according to <ref type="bibr" target="#b31">(25)</ref>. The direct result is that we do not need to compute ij and can simplify any computation involving ij by eliminating the corresponding term, such as the sum of max½0; ij in ( <ref type="formula" target="#formula_26">26</ref>) and <ref type="bibr" target="#b34">(28)</ref>. Additionally, we do not need to compute the inverse message of ij , namely, ij , since ij becomes inconsequential. That is, the computations of max½s ij þ ij in <ref type="bibr" target="#b31">(25)</ref> and <ref type="bibr" target="#b33">(27)</ref> and the assignment matrix ij þ ij &gt; 0 in (32) are overwhelmed by s ij and ij , respectively, which are À1. Similarly, j i is automatically À1 according to (27) since l ij ¼ À1. ij is also equal to À1 according to <ref type="bibr" target="#b35">(29)</ref> since l ij ¼ À1; j i ¼ À1 and minðÀ1; À1Þ ¼ À1. The direct result is that we do not need to compute j i and ij . If we investigate the computations involving j i , we find that j i also does not need to be computed since j i is only used in computing j i and ij . Given a sparse dataset with N data points but only M (M &lt; N 2 ) pairs of ði; jÞ 2 f1; . . . ; Ng 2 such that s ij &gt; À1, l ij &gt; À1. Let &amp; f1; . . . ; Ng 2 denote the M pairs of ði; jÞ satisfying s ij &gt; À1, l ij &gt; À1. We can rewrite the messages from ( <ref type="formula" target="#formula_25">25</ref>) to <ref type="bibr" target="#b37">(31)</ref> as follows and call it Fast MEAP: </p><formula xml:id="formula_35">i 6 ¼ j&amp;ði; jÞ 2 ij s ij À max h max j 0 6 2fj;ig ði;j 0 Þ2 ½s ij 0 þ ij 0 ; max m:ði;mÞ2 ½l im þ im þ s ii þ ii i ;<label>ð34Þ</label></formula><formula xml:id="formula_36">k i s ii þ l ik À max i 0 6 ¼i ði;i 0 Þ2 ½s ii 0 þ ii 0 þ ik ;<label>ð36Þ</label></formula><formula xml:id="formula_37">ii X i 0 6 ¼i ði 0 ;iÞ2 max Â 0; i 0 i Ã ;<label>ð37Þ</label></formula><formula xml:id="formula_38">ik min l ik À max m6 ¼k ði;mÞ2 ½l im þ im ; ii þ k i À ik " # ;<label>ð38Þ</label></formula><formula xml:id="formula_39">kk X i 0 6 ¼i ði 0 ;kÞ2 max Â 0; i 0 k Ã ;<label>ð39Þ</label></formula><formula xml:id="formula_40">ik min 0; kk þ X i 0 6 2fi;kg ði 0 ;kÞ2 max½0; i 0 k 2 6 4 3 7 5; k 6 ¼ i:<label>ð40Þ</label></formula><p>In terms of storage, the sparse structure can be stored for quick traversal using 2M integers for the M pairs and 1M floating-point values for the similarity, rather than N 2 floating-point values; and all the messages are stored as 5 Â M þ 2 Â N floating-point values, rather than 5 Â N 2 floating-point values. In terms of computational complexity, only 5 Â M þ 2 Â N messages need to be computed and exchanged, and the computation can be more efficient since there are fewer terms in the P , max , and min operations in each message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison to Affinity Propagation</head><p>The proposed MEAP algorithm can be viewed as a generalization of AP. The single-exemplar margin is a lower bound on the margin for the multi-exemplar model since it can be obtained when all the exemplars belonging to the same cluster coincide, or that setting the linkage matrix ½l ij NÂN ¼ 0. Both of them can be implemented to take advantage of the sparsity in data. Additionally, they have the same computational complexity, which can be revealed by comparing their simplified messages. That is, the number of messages of MEAP is seven while the number of messages of AP is three and all the messages are of the same computational complexity.</p><p>They both do not require preselecting the number of clusters and initial exemplars/superexemplars by initially considering all data points as the potential exemplars/ superexemplars, i.e., initializing all messages as zero. Similarly to AP, setting the exemplar preference s ii to a higher value would generate a larger number of exemplars, yielding a more detailed description of the subcluster structure within each cluster and vice versa. Setting the superexemplar preference l ii to a higher value would generate a larger number of superexemplars and vice versa. Therefore, when the cluster number is unknown in advance and there is no prior knowledge of the complexity of subcluster structures, the exemplar preference s ii and superexemplar preference l ii should be, respectively, set to median values of the similarity and linkage matrices. This may generate a moderate number of subclusters per cluster and a moderate number of clusters.</p><p>Like AP, the main cause of failure mode of MEAP is that the objective function <ref type="bibr" target="#b16">(10)</ref> has multiple minima with corresponding multiple fixed points of the update rules, which may prevent convergence. In this case, the message update may oscillate, with data points alternating between being exemplars and nonexemplars and exemplars alternating between being superexemplars and nonsuperexemplars. One remedy is to introduce a damping to the message update, which could always avoid oscillations. Let denote any of the seven messages on the left-hand side of equations ( <ref type="formula" target="#formula_25">25</ref>) to <ref type="bibr" target="#b37">(31)</ref>; the damping is done as follows <ref type="bibr" target="#b38">[32]</ref>:</p><formula xml:id="formula_41">¼ old þ ð1 À Þ new :<label>ð41Þ</label></formula><p>The higher values of the damping factor unsurprisingly lead to slower convergence rates but often lead to more stable maximization (i.e., avoiding oscillations). According to experimental results, setting the damping factor to 0.9 is sufficient in most cases to ensure convergence <ref type="bibr" target="#b38">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNSUPERVISED IMAGE CATEGORIZATION</head><p>In this section, we investigate the improvement of MEAP w.r.t. AP in unsupervised image categorization on three commonly tested image datasets, which are the Japanese female facial expression database (JAFFE) <ref type="bibr" target="#b42">[36]</ref>, the Caltech101 dataset <ref type="bibr" target="#b43">[37]</ref>, and the 13 natural scene categories dataset (SceneClass13) <ref type="bibr" target="#b23">[17]</ref>. Some existing clustering methods have been performed and compared with MEAP, including the classical k-centers, kernel-based clustering, spectral clustering, MCP, and hierarchical AP.</p><p>Comparative results have shown that MEAP significantly outperforms AP and is comparable with state-of-the-art clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods and Settings</head><p>The compared methods and their parameter settings are summarized as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity Metric</head><p>Previous works <ref type="bibr" target="#b15">[9]</ref>, <ref type="bibr" target="#b23">[17]</ref>, <ref type="bibr" target="#b45">[39]</ref>, <ref type="bibr" target="#b46">[40]</ref> utilized SIFT features <ref type="bibr" target="#b47">[41]</ref> for image matching and category learning. Each feature is described by a 128D vector. We follow the procedure described by Lowe to count the number of significant feature matches comparing image i with image k (denoted as M ik ): For each local feature from image i, the nearest and second nearest features are sought in image k. The match is considered significant if the distance ratio between the nearest and second-nearest neighbors is greater than a threshold which is selected from 0.5 to 0.9 <ref type="bibr" target="#b47">[41]</ref>. Then the similarity matrix ½s ij NÂN is defined to be the number of significant feature matches normalized by subtracting means across both dimensions <ref type="bibr" target="#b15">[9]</ref>:</p><formula xml:id="formula_42">s ij ¼ M ij À 1 N X N k¼1 M ik À 1 N X N k¼1 M kj :<label>ð42Þ</label></formula><p>The linkage matrix ½l ij NÂN is defined as l ij ¼ s ij =N. The exemplar preference s ii (and, in consequence, the superexemplar preference) is adjusted over a range of values to produce different numbers of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Clustering Evaluations</head><p>Two widely adopted external evaluations, namely, normalized mutual information (NMI) <ref type="bibr" target="#b48">[42]</ref> and classification rate (CR), are used in measuring how closely the clustering and underlying class labels match. Although there exist many external clustering evaluation measurements, such as clustering errors, average purity, entropy-based measures <ref type="bibr" target="#b49">[43]</ref>, and pair counting-based indices <ref type="bibr" target="#b50">[44]</ref>, the mutual information provides a sound indication of the shared information between a pair of clusterings <ref type="bibr" target="#b48">[42]</ref>, <ref type="bibr" target="#b51">[45]</ref>. Given a dataset X of size n, the clustering labels of c clusters, and actual class labels of ĉ classes, a confusion matrix is formed first, where entry ði; jÞ, n ðjÞ i gives the number of points in cluster i and class j. Then NMI can be computed from the confusion matrix <ref type="bibr" target="#b48">[42]</ref>: For computing the CR, each learned category is first associated with the "ground-truth" category that accounts for the largest number of samples in the learned category. Then the CR is computed as the ratio of the number of correctly classified samples to the size of the dataset. That is,</p><formula xml:id="formula_43">NMI ¼ 2 P c l¼1 P ĉ h¼1 n<label>ðhÞ</label></formula><formula xml:id="formula_44">CR ¼ # correctly classified samples #samples in the dataset Â 100%:<label>ð44Þ</label></formula><p>Obviously, a higher CR indicates a more accurate clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">JAFFE Dataset</head><p>The JAFFE database <ref type="bibr" target="#b42">[36]</ref> contains 213 images of seven facial expressions posed by 10 Japanese females. The seven facial expressions include six basic facial expressions, i.e., happiness, sadness, surprise, anger, disgust, fear, plus one neutral expression. There are three or four examples for each expression per person. Images of the same person in different facial expressions should be taken as in distinct subclasses <ref type="bibr" target="#b24">[18]</ref>, <ref type="bibr" target="#b31">[25]</ref>. The goal is to cluster the 213 images into 10 groups according to identity. We plot in Fig. <ref type="figure">7</ref> the clustering results when different values are used in constructing the similarity metric. In this experiment, we run the algorithms in the different similarity metrics constructed with ranging from 0.5 to 0.9, when using the actual number of categories, i.e., 10. In Figs. <ref type="figure">7a</ref> and<ref type="figure">7b</ref>, the NMI and CR values reported as a function of have shown that on the JAFFE dataset, the most appropriate is 0.6, and the proposed MEAP method has generated the best results. Hence, in the following comparison, we will use ¼ 0:6 to construct the similarity metric on the JAFFE dataset.</p><p>We plot in Fig. <ref type="figure">8</ref> the complete results on the JAFFE dataset by MEAP when setting the preferences at the median of the similarities. In this setting, although the AP method results in a moderate number of clusters <ref type="bibr">[1]</ref>, yet it oversegments the images of 10 subjects into more clusters than the actual, with each cluster corresponding to a facial expression. When setting the preferences at the of similarities, leading to a small number of clusters in AP, some images are misclassified by AP due to the presence of facial expression. However, our proposed MEAP can overcome the problem and almost perfectly group images of the same subject into one cluster by assigning exemplars of the same subject to the corresponding super-exemplar. Among 213 images, only three images belonging to the YM category are misclassified to the TM category, leading to a 98.6 percent CR. The reason is that the exemplar of these three images is more similar to the superexemplar of the TM category than YM. On the other hand, the AP algorithm misclassifies 36 images, resulting in an 83.1 percent CR, as will be demonstrated later.</p><p>We plot in Figs. <ref type="figure" target="#fig_6">9a</ref> and<ref type="figure" target="#fig_6">9b</ref>, respectively, the NMI and CR values as a function of K, the number of clusters. Notice that K varies by adjusting the preference s ii over a range of values in AP and MEAP. And in other clustering methods, clusterings with different cluster numbers are also produced by preselecting in initialization. The best of 10,000 runs of k-centers with random initialization is plotted. Hereafter, this setting is used when reporting the NMI and CR values as a function of the cluster number. The figure shows that, in terms of NMI and CR, the proposed MEAP algorithm significantly outperforms its counterparts AP and k-centers consistently in the case of different cluster numbers, and generates comparable results when compared with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Caltech101 Dataset</head><p>The Caltech101 image dataset <ref type="bibr" target="#b43">[37]</ref> contains 8,677 pictures of objects belonging to 101 categories. We use the same experimental setting as that in the work <ref type="bibr" target="#b15">[9]</ref>. That is, 20 of the 101 classes representing a wide range of objects are selected, including (with numbers in parentheses denoting the number of images in each class) faces (435), leopards (200), motorbikes (798), binocular <ref type="bibr" target="#b39">(33)</ref>, brain (98), camera (50), car side (123), dollar bill (52), ferry (67), garfield <ref type="bibr" target="#b40">(34)</ref>, hedgehog (54), pagoda <ref type="bibr" target="#b53">(47)</ref>, rhino (59), snoopy <ref type="bibr" target="#b41">(35)</ref>, stapler <ref type="bibr" target="#b51">(45)</ref>, stop sign (64), water lilly <ref type="bibr" target="#b43">(37)</ref>, windsor chair (56), wrench <ref type="bibr" target="#b45">(39)</ref>, and yin yang (60). And only the first 100 images in the classes containing a very large number of images (e.g., faces, leopards, motorbikes, car side) are used, yielding a total dataset of 1,230 images. It should be noticed that, on this dataset, the presence of multiple subclasses within each category is not as obvious as on the JAFFE and   SceneClass13 datasets. However, as we will show, MEAP still outperforms AP in this case, which validates that MEAP is more flexible than AP.</p><p>Like on JAFFE, we first have to select the appropriate for constructing the similarity metric on Caltech101. We plot in Fig. <ref type="figure" target="#fig_7">10</ref> the clustering results when different values are used in constructing the similarity metric. In this experiment, we run the algorithms in the different similarity metrics constructed with ranging from 0.5 to 0.9 when using the actual number of categories, i.e., 20. The NMI and CR results plotted, respectively, in Figs. <ref type="figure" target="#fig_7">10a</ref> and<ref type="figure" target="#fig_7">10b</ref> have shown that the most appropriate value for the Caltech101 dataset is 0.8. Therefore, in later comparisons, we will use ¼ 0:8 for constructing the similarity metric on Caltech101.</p><p>As reported in <ref type="bibr" target="#b15">[9]</ref>, the snoopy category was partitioned into a few subclasses. In the largest subclass associated with the ground-truth snoopy category as shown in [9, Fig. <ref type="figure">5</ref>], some chair images were misclassified into the snoopy category. Fig. <ref type="figure" target="#fig_8">11</ref> shows the results by AP and MEAP when setting preferences at the minimum value of the similarity matrix. The AP algorithm produces three clusters associated with the snoopy category. However, MEAP can improve the results by avoiding oversegmentation through assigning exemplars to the most appropriate superexemplars.</p><p>We plot in Figs. <ref type="figure" target="#fig_9">12a</ref> and<ref type="figure" target="#fig_9">12b</ref>, respectively, the NMI and CR values as a function of K, the number of clusters.</p><p>Although the existence of multiple subclasses is not so clear compared with that of JAFFE, the comparative results have shown that in terms of NMI and CR, the proposed MEAP still outperforms AP and k-centers consistently on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">SceneClass13 Dataset</head><p>The SceneClass13 image dataset <ref type="bibr" target="#b23">[17]</ref> contains 3,759 images of 13 natural scene categories (with numbers in parentheses   denoting the image number in each class): highway (260), inside of cities (308), tall buildings (356), streets (292), suburb residence (241), forest (328), coast (360), mountain (374), open country (410), bedroom (174), kitchen (151), living-room (289), and office (216). As discussed in <ref type="bibr" target="#b23">[17]</ref>, a scene category may contain multiple themes. For instance, the coast scene contains at least two typical themes (with and without "sunset/sunrise"), as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The goal is to learn the scene categories consisting of multiple subclasses (themes).</p><p>Likewise, the NMI and CR values as a function of plotted, respectively, in Figs. <ref type="figure" target="#fig_10">13a</ref> and<ref type="figure" target="#fig_10">13b</ref> show that the most appropriate for the SceneClass13 dataset is 0.7 and it does not seriously affect the clustering results.</p><p>A significant improvement has been achieved on the dataset of SceneClass13. As illustrated in Fig. <ref type="figure" target="#fig_11">14</ref>, MEAP can correctly model the highway category from other similar categories (e.g., the coast category as shown in Fig. <ref type="figure" target="#fig_0">1</ref>) by integrating the two subclasses of the highway category (i.e., with/without "vehicles") into one cluster. On the contrary, AP separates the category into two clusters.</p><p>We plot in Figs. <ref type="figure" target="#fig_12">15a</ref> and<ref type="figure" target="#fig_12">15b</ref>, respectively, the NMI and CR values as a function of K, the number of clusters. Still, on this dataset, the proposed MEAP outperforms the k-centers and AP methods; meanwhile, it is comparable with other state-of-the-art clustering methods.</p><p>For the overall comparison on the performance of unsupervised image categorization, Table <ref type="table" target="#tab_2">1</ref> lists the average values and standard deviations of NMI and CR, and the average computational time in seconds, over 100 runs on the three image datasets using the actual cluster number. Apart from the previously compared methods, another exemplar-based clustering method in operation research termed vertex substitution heuristic (VSH) <ref type="bibr" target="#b52">[46]</ref> is performed and compared. From the table, the proposed MEAP algorithm significantly outperforms k-centers and AP, and is comparable with the state-of-the-art clustering methods. The advantage of MEAP is not only that it can model the category of more complex structure than AP without increasing the complexity of the generated model, but also that it can automatically estimate the number of clusters and the appropriate number of subclusters within each cluster when compared with existing multiexemplar clustering methods. The MPC method requires tuning many parameters to generate the desired number of clusters. On the other hand, MEAP inherits the advantages of AP in auto-initialization such that it does not need to tune any parameter <ref type="bibr">[1]</ref>. Interestingly, the clustering performances of MEAP and HAP are very similar, with MEAP being slightly better than HAP in these unsupervised image categorization experiments. By observing the computational time, the time ratio of MEAP to AP is less than 7 : 3. Although the proposed MEAP method is not the fastest, its time consumption is acceptable.  This section reports the experimental results in the applications of handwritten digit clustering over two widely tested handwritten digit datasets, which are the USPS dataset <ref type="bibr" target="#b53">[47]</ref> and the MNIST dataset <ref type="bibr" target="#b54">[48]</ref>. As discussed in <ref type="bibr" target="#b25">[19]</ref> and <ref type="bibr" target="#b26">[20]</ref>, the class representing a handwritten digit could be composed of several subclasses, each corresponding to a different handwriting style. The experimental results reported in this section coincide with this discussion and show that MEAP obtains more accurate clustering results than the compared methods. The United States postal service (USPS) digit dataset contains 11,000 scaled handwritten digit images of size 16 Â 16, with 1,100 images for each digit category <ref type="bibr" target="#b53">[47]</ref>. The Modified National Institute of Standards and Technology (MNIST) dataset used in this paper contains 5,000 scaled handwritten digit images of size 28 Â 28, with 500 images for each digit category. Some samples of the two datasets are shown in Figs. <ref type="figure" target="#fig_16">16a</ref> and<ref type="figure" target="#fig_16">16b</ref>, respectively.</p><p>The experiments are performed in the gray scale pixels of digit images. The similarity matrix ½s ij NÂN is computed as the negative euclidean distance, i.e., s ij ¼ maxdistÀ k x i À x j k 2 and the linkage matrix ½l ij NÂN is set to l ij ¼ s ij =N, where maxdist denotes the max k;l2f1;...;Ng k x k À x l k 2 , x i is the ith digit image, and N is the size of the dataset. The same experimental settings are used as in the image categorization.</p><p>We plot in Figs. 17 and 18, respectively, the NMI and CR values as a function of the number of generated clusters. Table <ref type="table" target="#tab_3">2</ref> lists the average values and standard deviations of NMI and CR, and the average computational time in seconds, over 100 runs on the two digit datasets using the actual cluster number. From the table, MEAP obtains 21.2 and 18.3 percent CR improvements, respectively, on USPS and MNIST over AP. This is a very significant improvement. When compared with the runner-up algorithm, namely, HAP, MEAP also outperforms HAP by obtaining 2.4 and 1.1 percent CR improvements, respectively, on USPS and MNIST. From the standard deviations of NMI and CR, we can see that the AP-like algorithms, namely, AP, GHAP, HAP, and MEAP, generate clustering results insensitive to initialization. By considering both the clustering accuracy and computational time, the comparative    results have validated the effectiveness of the proposed MEAP method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we have extended the single-exemplar model to a multi-exemplar one and proposed a MEAP algorithm. In the multi-exemplar model, each data point is assigned to the most appropriate exemplar and each exemplar is assigned to the most appropriate superexemplar. Each cluster contains one superexemplar and an automatically determined number of exemplars assigned to that superexemplar. The objective is to maximize the sum of all similarities between data points and the corresponding exemplars plus the sum of all linkages between exemplars and the corresponding superexemplars. The max-sum belief propagation is utilized to solve the NPhard optimization. By initializing all data points as exemplars and superexemplars and passing messages between data points and exemplars/superexemplars, between exemplars and superexemplars, MEAP produced clusters insensitive to initialization and converged to the neighborhood maximum.</p><p>The new MEAP algorithm has been found to be more effective than AP in the applications of multisubclasses clustering such as unsupervised image categorization and handwritten digit clustering. Image categorization experiments on three commonly tested datasets and handwritten digit clustering on two digit datasets have been conducted to compare MEAP with AP and k-centers, as well as other state-of-the-art clustering methods. The comparative results have confirmed the significant improvement made by our method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Two typical themes of the coast scene: with (left) and without (right) "sunset/sunrise." Two exemplars are needed to model the scene as bounded by the red lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>SðcÞ</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the multi-exemplar model. The mapping 1 assigns each data point to the most appropriate exemplar and the mapping 2 assigns each exemplar to the most appropriate superexemplar. This model can effectively characterize clusters consisting of multiple subclusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Factor graph of MEAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>entropy of cluster labels and class labels , respectively, with n i and n ðjÞ denoting the number of points in cluster i and class j. A high NMI indicates the clustering and class labels match well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. The NMI and CR values as a function of on JAFFE when using the actual number of categories, i.e., 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The and CR values as a function of the number of clusters learned on the JAFFE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The NMI and CR values as a function of on Caltech101 when using the actual category number, i.e., 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Comparing AP and MEAP in the snoopy category from Caltech101. By setting the reference at the minimum value of the similarity matrix, AP has produced three categories associated with the snoopy category, as shown by the red circle. However, through assigning the exemplars to the most appropriate superexemplars (the blue arrow), MEAP can group these subcategories into one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The NMI and CR values as a function of the number of clusters learned on the Caltech101 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The NMI and CR values as a function of on SceneClass13 when using the actual category number, i.e., 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Comparing AP and MEAP in the highway category from SceneClass13. By setting the reference at the minimum value of the similarity matrix, AP has produced two categories (with/without vehicles) associated with the highway category, as shown by the red circle. However, by assigning the exemplars to the most appropriate superexemplars (the blue arrow), MEAP can group these two subcategories into one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. The NMI and CR values as a function of the number of clusters learned on the SceneClass13 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. The NMI and CR values as a function of the number of clusters learned on the MNIST digit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. The NMI CR values as a function of the number of clusters learned on the USPS digit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig. 17. The NMI CR values as a function of the number of clusters learned on the USPS digit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Some samples of the USPS and MNIST datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 The</head><label>1</label><figDesc>Average and Standard Deviations (in Parentheses) of NMI and CR, and the Average Computational Time in Seconds, over 100 Runs on Three Image Datasets Using the Actual Cluster Number</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 The</head><label>2</label><figDesc>Average Values and Standard Deviations (in Parentheses) of NMI and CR, and the Average Computational Time in Seconds, over 100 Runs on the Two Digit Datasets Using the Actual Cluster Number</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The data is sparse in case the matrix of similarities has zero entries; in the log-domain the missing similarities become À1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 35, NO. 9, SEPTEMBER 2013</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by NSFC (61173084 and 61128009), NSFC-GuangDong (U0835005). The authors would like to thank Jianbo Shi for providing the Ncut code, Inderjit S. Dhillon for providing the Graclus code, Brendan J. Frey for providing the AP code, and Fei-Fei Li for providing the SceneClass13 and Caltech101 datasets.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ching Y. Suen received the MSc degree in engineering from the University of Hong Kong and the PhD degree from the University of British Columbia, Vancouver, BC, Canada. He is the director of Centre for Pattern Recognition and Machine Intelligence, Concordia University, Montreal, QC, Canada, and the Concordia Chair on Artificial Intelligence and Pattern Recognition. He has guided/hosted 80 visiting scientists and professors, and has supervised 82 doctoral and master's graduates. Currently, he is the editor-in-chief of the journal of Pattern Recognition and an advisory or associate editor of four other journals. He has founded and organized numerous international conferences on pattern recognition, handwriting recognition, and document analysis. He has also founded the IAPR ICDAR Awards. He has served numerous professional societies as president, vicepresident, governor, and director. He has given 180 invited talks at various industries and academic institutions around the world, and has been the principal investigator or consultant of 30 industrial projects. His publications include four conference proceedings, 12 books, and more than 480 papers, of which many have been widely cited. He is a fellow of the IAPR, and the Academy of Sciences of the Royal Society of Canada. He is a life fellow of the IEEE.</p><p>Jun-Yong Zhu received the BS and MS degrees from the School of Mathematics and Computational Science, Sun Yat-sen University, Guangzhou, P.R. China, in 2008 and 2010, respectively. He is currently working toward the PhD degree in the Department of Mathematics, Sun Yat-sen University. His current research interests include machine learning, transfer learning using auxiliary data, pattern recognition such as heterogeneous face recognition. He is a student member of the IEEE.</p><p>. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exemplar-based clustering methods. They are k-centers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The parameters for AP and MEAP are set as follows: t conv ¼ 100</title>
		<imprint/>
	</monogr>
	<note>t max ¼ 1,000, ¼ 0:9 as in</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The maximum number of iterations is set to 1,000 and in each run a random initialization is used in k-centers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Kernel-based clustering. The compared kernel clustering methods include kernel k-means (kk-means) [21] and conscience online learning (COLL) [23]. Gaussian kernel ðx i ; x j Þ ¼ e Àkx i Àx j k 2 =2 2 is used to construct the kernel matrix K. To obtain a meaningful comparison</title>
		<imprint/>
	</monogr>
	<note>on each dataset, the most appropriate obtained by the criterion proposed in [38] is used</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The widely used normalized cut (Ncut) [22] and recently proposed Graclus [24] are performed</title>
	</analytic>
	<monogr>
		<title level="m">Spectral clustering</title>
		<imprint/>
	</monogr>
	<note>Graphs are constructed the same as in [22], and in Graclus the ratio association is used</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The multiprototype clustering proposed in [30] is performed and compared</title>
		<author>
			<persName><surname>Mpc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The greedy hierarchical affinity propagation (GHAP) [15] and a theoretically improved version HAP [16] are performed. The layer number is set to 2. All the experiments are</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hierarchical</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>implemented in Matlab7.8.0.347 (R2009a) 64-bit edition on a workstation (Windows 64 bit, 8 Intel 2.00 GHz processors, 16 GB of RAM</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clustering by Passing Messages between Data Points</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<ptr target="http://www.psi.toronto.edu/index" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>php?q=affinity%20propagation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factor Graphs and the Sum-Product Algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-A</forename><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the Optimality of Solutions of the Max-Product Belief-Propagation Algorithm in Arbitrary Graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="736" to="744" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Some Methods for Classification and Analysis of Multivariate Observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Berkeley Symp. Math. Statistics and Probability</title>
		<meeting>15th Berkeley Symp. Math. Statistics and Probability</meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data Clustering: 50 Years Beyond K-Means</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="651" to="666" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constructing Treatment Portfolios Using Affinity Propagation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giaever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Emili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Musso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hegele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Ann. Int&apos;l Conf. Research in Computational Molecular Biology</title>
		<meeting>12th Ann. Int&apos;l Conf. Research in Computational Molecular Biology</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="360" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Collaborative Benchmark for Region of Interest Detection Algorithms</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On Detecting Subtle Pathology via Tissue Clustering of Multi-Parametric Data Using Affinity Propagation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>11th IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-Metric Affinity Propagation for Unsupervised Image Categorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>11th IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Matrix Modular Neural Network Based on Task Decomposition with Subspace Division by Adaptive Affinity Propagation Clustering</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Math. Modelling</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3884" to="3895" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised and Semi-Supervised Clustering by Message Passing: Soft-Constraint Affinity Propagation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Sumedha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Physical J. B</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="125" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flexible Priors for Exemplar-Based Clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Conf. Uncertainty in Artificial Intelligence</title>
		<meeting>24th Conf. Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="537" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data Streaming with Affinity Propagation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Furtlehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Machine Learning and Knowledge Discovery in Databases</title>
		<meeting>European Conf. Machine Learning and Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-Supervised Affinity Propagation with Instance-Level Constraints</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Givoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Artificial Intelligence and Statistics</title>
		<meeting>Conf. Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint Affinity Propagation for Multiple View Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical Affinity Propagation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Givoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Conf. Uncertainty in Artificial Intelligence</title>
		<meeting>24th Conf. Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="238" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Bayesian Hierarchical Model for Learning Natural Scene Categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Subclass Discriminant Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1274" to="1286" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple Subclass Pattern Recognition: A Maximin Correlation Approach</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Avi-Itzhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A V</forename><surname>Mieghem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="418" to="431" />
			<date type="published" when="1995-04">Apr. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Prototype Classification: Improved Modelling of the Variability of Handwritten Data Using Statistical Clustering Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F R</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fairhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1208" to="1210" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlinear Component Analysis as a Kernel Eigenvalue Problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scho ¨lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Mu ¨ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Conscience On-Line Learning Approach for Kernel-Based Clustering</title>
		<author>
			<persName><forename type="first">C.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int&apos;l Conf. Data Mining</title>
		<meeting>10th Int&apos;l Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weighted Graph Cuts Without Eigenvectors: A Multilevel Approach</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple-Exemplar Discriminant Analysis for Face Recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l Conf. Pattern Recognition</title>
		<meeting>17th Int&apos;l Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="191" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Prototype Vector-Space Models of Word Meaning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ann. Conf. North Am. Chapter of the Assoc. for Computational Linguistics</title>
		<meeting>Ann. Conf. North Am. Chapter of the Assoc. for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Multiple Hyper-Ellipsoidal Subclass Model for an Evolutionary Classifier</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="547" to="560" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiclass Classification with Multi-Prototype Support Vector Machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Aiolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="817" to="850" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CURE: An Efficient Clustering Algorithm for Large Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="58" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Multi-Prototype Clustering Algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="689" to="698" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Multi-Prototype Clustering Algorithm Based on Minimum Spanning Tree</title>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Int&apos;l Conf. Fuzzy Systems and Knowledge Discovery</title>
		<meeting>Seventh Int&apos;l Conf. Fuzzy Systems and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1602" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Affinity Propagation: Clustering Data by Passing Messages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD dissertation</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Binary Variable Model for Affinity Propagation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Givoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1589" to="1600" />
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<title level="m">Kernel Methods for Pattern Analysis</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Maximum Margin Clustering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coding Facial Expressions with Gabor Wavelets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gyoba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third IEEE Int&apos;l Conf. Automatic Face and Gesture Recognition</title>
		<meeting>Third IEEE Int&apos;l Conf. Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="200" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshop</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kernel Optimization in Discriminant Analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Hamsici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="631" to="638" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Categories from Sets of Partially Matching Image Features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshop</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2596" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiple Object Class Detection with a Generative Model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshop</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cluster Ensembles-A Knowledge Reuse Framework for Combining Multiple Partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Impact of Similarity Measures on Web-Page Clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Workshop AI for Web Search</title>
		<meeting>AAAI Workshop AI for Web Search</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comparing Partitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparing Clusterings-An Axiomatic View</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int&apos;l Conf. Machine Learning</title>
		<meeting>22nd Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Variable Neighborhood Search for the p-Median</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mladenovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Location Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="207" to="226" />
			<date type="published" when="1997-12">Dec. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Database for Handwritten Text Recognition Research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">He joined Sun Yat-sen University in 1989 as an assistant professor, where he is currently a professor with the Department of Automation of School of Information Science and Technology and vice dean of the School of Information Science and Technology. His current research interests include the areas of digital image processing, pattern recognition, multimedia communication, wavelet, and its applications. He has published more than 100 scientific papers in international journals and conferences on image processing and pattern recognition, for example</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He has published several scientific papers in international journals and conferences such as the IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Systems, Man, and Cybernetics-C, KAIS, Neurocomputing, and ICDM. His ICDM &apos;10 paper won the Honorable Mention for Best Research Paper Awards. He won the Student Travel Award from ICDM &apos;10 and ICDM &apos;11, respectively. He is a student member of the IEEE. Jian-Huang Lai received the MSc degree in applied mathematics and the PhD degree in mathematics from Yat-sen University</title>
		<meeting><address><addrLine>China; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">Nov. 1998. 2008 and 2010. January 2013. 1989 and 1999</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Neural Networks, IEEE TIP, IEEE Transactions on Systems, Man, and Cybernetics-B, Pattern Recognition, ICCV, CVPR, and ICDM. He serves as a standing member of the Image and Graphics Association of China and also serves as a standing director of the Image and Graphics Association of Guangdong. He is a senior member of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
