<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bagging for Gaussian process regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-10-02">2 October 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<email>chentao@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Chemical and Biomedical Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>637459</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianghong</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Automation</orgName>
								<orgName type="institution">Chongqing University</orgName>
								<address>
									<postCode>400044</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bagging for Gaussian process regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-10-02">2 October 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">635E3DF68D0312AF6BAAB3F5771A1F1C</idno>
					<idno type="DOI">10.1016/j.neucom.2008.09.002</idno>
					<note type="submission">Received 5 June 2008 Received in revised form 2 September 2008 Accepted 8 September 2008 Communicated by T. Heskes</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Bagging Bayesian method Bootstrap Gaussian process Model robustness Soft sensor</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes the application of bagging to obtain more robust and accurate predictions using Gaussian process regression models. The training data are re-sampled using the bootstrap method to form several training sets, from which multiple Gaussian process models are developed and combined through weighting to provide predictions. A number of weighting methods for model combination are discussed, including the simple averaging and the weighted averaging rules. We propose to weight the models by the inverse of their predictive variance, and thus the prediction uncertainty of the models is automatically accounted for. The bagging method for Gaussian process regression is successfully applied to the inferential estimation of quality variables in an industrial chemical plant.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Bagging, short for ''Bootstrap AGGregatING'', is a method of obtaining more robust and accurate models using bootstrap resamples of the training data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>. The procedure for bagging consists of two stages. First bootstrap samples are obtained from the original data to form a set of training sets, from which multiple models are developed. Then these models are combined in some way to make predictions. Bagging can be applied to both regression and classification models, whilst the focus is on regression in this study. It was shown <ref type="bibr" target="#b1">[2]</ref> that bagging is especially suitable for ''unstable'' models, i.e. the models that are sensitive (in terms of model parameters and predictive performance) to small changes in the training data. In practice, bagging has been applied to a large number of model structures, including regression trees <ref type="bibr" target="#b2">[3]</ref>, regression with variable selection <ref type="bibr" target="#b22">[23]</ref> and neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>The primary purpose of this paper is to apply the bagging procedure to improve the robustness and accuracy of Gaussian process regression models that have recently received significant interests in the community of machine learning and applied statistics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. Initially proposed by O'Hagan <ref type="bibr" target="#b19">[20]</ref>, Gaussian process regression was viewed as an alternative approach to artificial neural networks, primarily as a result of the seminal research of Neal <ref type="bibr" target="#b18">[19]</ref>. Neal showed that a large class of Bayesian regression models, based on artificial neural networks, converged to a Gaussian process, in the limit of an infinite network <ref type="bibr" target="#b18">[19]</ref>. Gaussian processes can also be derived from the perspective of non-parametric Bayesian regression <ref type="bibr" target="#b16">[17]</ref>, by directly placing Gaussian prior distribution over the space of regression functions. As a result of its good performance in practice and desirable analytical properties, Gaussian process models have seen wide applications, such as rehabilitation engineering <ref type="bibr" target="#b11">[12]</ref>, machining optimization <ref type="bibr" target="#b26">[27]</ref> and calibration of analytical sensors <ref type="bibr" target="#b4">[5]</ref>.</p><p>The motivation of bagging Gaussian process models is that we have observed that a single Gaussian process does not always give satisfactory predictions in practice. In particular we are considering a specific application scenario in chemical plants where the product quality variables (e.g. melt flow rate (MFR) of polypropylene) are inferred from process operational variables (e.g. reactor temperature and feed rate of raw material). The details are given in Section 4. Although instruments for measuring the quality variables are available, these instruments usually possess substantial delays, and thus are not suitable for the on-line quality assurance and control purpose. Therefore the key in inferential estimation is to identify the relationship between the difficult-tomeasure (quality) variables and the easy-to-measure (operational) variables <ref type="bibr" target="#b27">[28]</ref>. The inferential models serve as ''virtual'' sensors to provide the information about process quality variables. In this study, Gaussian process regression model, enhanced by the bagging procedure, is employed to achieve this purpose. To the best of our knowledge there has been little report of applying bagging to Gaussian process models in the literature. Hence the results in this paper could provide a guideline to other modeling practice where Gaussian process is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>The rest of this paper is organized as follows. Section 2 gives a brief overview of Gaussian process regression models, followed by the introduction of bagging in Section 3. The conventional model combination methods are discussed, and a novel model weighting strategy is proposed. The effectiveness of the bagging method for Gaussian process models is demonstrated through its industrial application in Section 4. Finally Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">An overview of Gaussian process</head><p>The idea of Gaussian process can be dated back to the classic paper by O'Hagan <ref type="bibr" target="#b19">[20]</ref>. However, the application of Gaussian process as a regression (and classification) technique in the community of pattern recognition was not common until the late 1990s, where the rapid advancement of computational power helped facilitate the implementation of Gaussian process for larger data sets. The Gaussian process regression model can be derived from the perspectives of neural networks and Bayesian non-parametric regression; see <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref> for details. In this section a brief overview of Gaussian process regression model is given, including the formulation and implementation of the model.</p><p>From the perspective of a regression problem, a functional relationship is identified between the D dimensional input variables, x, and the output variable, y. Here the formulation is restricted to univariate output. A discussion on Gaussian process model with multivariate output is given in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref>. A Gaussian process defined on multiple outputs has several difficulties, such as the complexity in the model and its implementation <ref type="bibr" target="#b4">[5]</ref>. In practice a simplified solution could be adopted to model each output variable separately <ref type="bibr" target="#b16">[17]</ref>. By adopting this approach, it can be argued that significant information contained in the correlation structure between the output variables is ignored. However, in the literature of regression modeling, there is no consensus on whether multi-output modeling can achieve better predictive performance than separate modeling. The use of separate models is also justified by their good performance in various applications <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The model</head><p>Consider a training data set consisting of N data points fx i ; y i ; i ¼ 1; . . . ; Ng. A Gaussian process for regression is defined such that the regression function yðxÞ has a Gaussian prior distribution with zero mean, or in discrete form</p><formula xml:id="formula_0">y ¼ ðy 1 ; . . . ; y N Þ T $Gð0; CÞ<label>(1)</label></formula><p>where C is an N Â N covariance matrix of which the ij-th element is defined by the covariance function:</p><formula xml:id="formula_1">C ij ¼ Cðx i ; x j Þ.</formula><p>An example of such a covariance function is</p><formula xml:id="formula_2">Cðx i ; x j Þ ¼ a 0 þ a 1 X D d¼1 x id x jd þ v 0 exp À X D d¼1 w d ðx id À x jd Þ 2 ! þ d ij s 2<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">x i ¼ ðx i1 ; . . . ; x iD Þ; d ij ¼ 1 if i ¼ j, otherwise it is equal to zero.</formula><p>We denote h ¼ ða 0 ; a 1 ; v 0 ; w 1 ; . . . ; w D ; s 2 Þ as ''hyper-parameters''</p><p>defining the covariance function. The hyper-parameters must be non-negative to ensure that the covariance matrix is non-negative definite. The term ''hyper-parameter'' is used to differentiate Gaussian processes from parametric regression, where the para-meter is required to be estimated. For the covariance function depicted in Eq. ( <ref type="formula" target="#formula_2">2</ref>), the first two terms represent a constant bias (offset) and a linear correlation term, respectively. The exponential term is similar to the form of a radial basis function, and it takes into account the potentially strong correlation between the outputs for nearby inputs. The term s 2 captures the random error effect. By combining both linear and non-linear terms in the covariance function, Gaussian process is capable of handling both linear and non-linear data structures <ref type="bibr" target="#b4">[5]</ref>. Other forms of covariance functions are discussed in <ref type="bibr" target="#b16">[17]</ref>.</p><p>For a new data point with input vector x Ã , the predictive distribution of the output y Ã conditional on the training data is also Gaussian, of which the mean ( ŷÃ ) and variance (s 2 ŷÃ ) are calculated as follows:</p><formula xml:id="formula_4">ŷÃ ¼ k T ðx Ã ÞC À1 y (3) s 2 ŷÃ ¼ Cðx Ã ; x Ã Þ À k T ðx Ã ÞC À1 kðx Ã Þ<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">kðx Ã Þ ¼ ½Cðx Ã ; x 1 Þ; . . . ; Cðx Ã ; x N Þ T .</formula><p>The capability to providing the prediction uncertainty in terms of the variance is an important feature of Gaussian process. The predictive variance in Eq. ( <ref type="formula" target="#formula_4">4</ref>) plays an important role in model combination that is presented in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Implementation</head><p>The hyper-parameters h can be estimated by maximizing the following log-likelihood function:</p><formula xml:id="formula_6">L ¼ À 1 2 log det C À 1 2 y T C À1 y À N 2 log 2p<label>(5)</label></formula><p>Most training algorithms also require the derivative of L with respect to each hyper-parameter y:</p><formula xml:id="formula_7">qL qy ¼ À 1 2 tr C À1 qC qy þ 1 2 y T C À1 qC qy C À1 y<label>(6)</label></formula><p>where qC=qy can be obtained from the covariance function.</p><p>In situations where the prior distribution is assigned to the hyper-parameters, it can be incorporated into the likelihood function to realize a maximum a posterior (MAP) estimation. Both maximum likelihood estimation and the MAP method are known to be sensitive to initializations and may converge to local optima <ref type="bibr" target="#b20">[21]</ref>. A more elaborate approach is to use Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b20">[21]</ref> method to generate samples for approximation of the posterior distribution of hyper-parameters. MCMC introduces significantly higher computation cost, and thus its application in engineering practice is restricted. In this study the maximum likelihood estimation is adopted for model training, where a conjugate gradient method is employed to search for the hyper-parameters that maximize the likelihood <ref type="bibr" target="#b20">[21]</ref>.</p><p>It should also be noted that the calculation of the likelihood and the derivatives involves a matrix inversion step and takes time of the order OðN 3 Þ, which is feasible for a moderate size of training data sets (less than several thousand) on a conventional computer. For larger data sets, sparse training strategies may be required to reduce the overall computational burden <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>. A comprehensive investigation of sparse Gaussian processes is beyond the scope of this paper. The computational aspect will be discussed within the case study in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bagging for Gaussian process models</head><p>The basic idea of bagging is straightforward. Instead of making predictions from a single model that is fit to the observed data, a number of models are developed to characterize the same relationship between input and output variables. Each model is developed from a bootstrap <ref type="bibr" target="#b7">[8]</ref> re-sampled set of the original training data. Then the predictions from the multiple models are combined to improve model accuracy and robustness. The procedure of bootstrap re-sampling is briefly described here.</p><p>Suppose the original training data are Z ¼ fx i ; y i ; i ¼ 1; . . . ; Ng. We randomly sample N data points with replacement from Z where the probability of each data point being selected is 1=N. These N data points are regarded as a re-sampled training set that is denoted Z 1 . Then we repeat the procedure for K times and obtain K re-sampled data sets: Z 1 ; . . . ; Z K . Finally K separate models can be developed from these re-sampled training sets.</p><p>The method of bagging is especially applicable for unstable modeling procedures, i.e. the models that are sensitive to small changes in the data, such as neural networks and classification and regression trees <ref type="bibr" target="#b1">[2]</ref>. Gaussian process is among the unstable modeling procedures where bagging is potentially useful. The model may be sensitive to data if the amount of training data is limited. The availability of data is an important factor when developing models for industrial applications, where the data collection process involves significant cost <ref type="bibr" target="#b27">[28]</ref>. In addition, the choice of training algorithms also affects the accuracy and robustness of a Gaussian process model. The conventional method, e.g. conjugate gradient, for hyper-parameter estimation is sensitive to initialization and may find a local optima that does not provide good prediction performance over the entire data space. Although the MCMC method can be utilized to partially address these issue, it is not considered in this paper due to the computational constraints in practice, and the focus is on the implementation of bagging for Gaussian process models.</p><p>Suppose K Gaussian process models, M ¼ fM 1 ; . . . ; M K g, have been developed from the bootstrap re-sampled data sets. The rest of this section discusses the methods to combine the multiple models for prediction, including simple averaging and weighted averaging rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Combination using simple averaging rule</head><p>The averaging rule for multi-model combination is the original method in bagging <ref type="bibr" target="#b1">[2]</ref>. Formally the prediction y Ã is the average of the predictive distributions from the K models:</p><formula xml:id="formula_8">pðy Ã jMÞ ¼ 1 K X K k¼1 pðy Ã jM k Þ<label>(7)</label></formula><p>where pðy Ã jM k Þ, k ¼ 1; . . . ; K, are Gaussian with mean and variance calculated from Eqs. ( <ref type="formula">3</ref>) and (4), i.e. pðy Ã jM k Þ ¼ Gð ŷÃ ðkÞ; s 2 ŷÃ ðkÞÞ. Essentially Eq. ( <ref type="formula" target="#formula_8">7</ref>) is not a Gaussian distribution any more but a mixture of K Gaussians, or the well known Gaussian mixture model <ref type="bibr" target="#b17">[18]</ref>. The predictive mean and variance for the combined models can be calculated based on the property of the Gaussian mixture model: </p><formula xml:id="formula_9">Eðy Ã Þ ¼ 1 K X K k¼1 ŷÃ<label>ðkÞ</label></formula><formula xml:id="formula_10">Varðy Ã Þ ¼ 1 K X K k¼1 s 2 ŷÃ ðkÞ þ 1 K X K k¼1 ð ŷÃ<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Combination using weighted averaging rule</head><p>An alternative combination approach is to assign different weights to the models, the rationale being that the models should be weighed according to the corresponding prediction capability. A linear regression method was proposed to learn these weights so that the combined predictors minimize the training error <ref type="bibr" target="#b27">[28]</ref>. More specifically for the entire training data set, the output variable y is regressed on the predicted outputs from each model ŷðkÞ:</p><formula xml:id="formula_12">y i ¼ w 1 ŷi ð1Þ þ w 2 ŷi ð2Þ þ Á Á Á þ w K ŷi ðKÞ; i ¼ 1; . . . ; N<label>(10)</label></formula><p>where fw k ; k ¼ 1; . . . ; Kg are the weights. The linear regression in Eq. ( <ref type="formula" target="#formula_12">10</ref>) is an ill-conditioned problem because predictions from the multiple models are highly correlated. This collinearity issue was addressed by using principal component analysis <ref type="bibr" target="#b27">[28]</ref>, whereas other approaches are also applicable, including ridge regression and partial least squares <ref type="bibr" target="#b23">[24]</ref>. This regression-based weighting approach tends to assign larger weights to the models that give better ''prediction'' to the training data. In fact the models are weighted according to how well they fit, as opposed to predict, the training data, since the data have already been used for model development. As a result, if some of the models severely over-fit the training data, the effect of over-fitting could be ''amplified'' by these models being assigned large weights. Therefore the generalization capability of the models may be compromised.</p><p>In this paper we propose a weighting strategy that relies on the prediction uncertainty. In effect the model that is uncertain about the prediction should be discounted with a smaller weight. Therefore the weights are not pre-calculated through fitting the training data and fixed for prediction; rather they are automatically adjusted based on the prediction uncertainty. The proposed strategy is originally motivated by the approach of Bayesian committee machine (BCM) <ref type="bibr" target="#b21">[22]</ref>. BCM was derived based on the assumption that the correlation between the K Gaussian process models is negligible, an assumption that is unrealistic in the application of bagging. Hence we do not follow the derivation in <ref type="bibr" target="#b21">[22]</ref>; rather we propose to combine the predictors based on the following product rule:</p><formula xml:id="formula_13">pðy Ã jMÞ / Y K k¼1 pðy Ã jM k Þ<label>(11)</label></formula><p>For Gaussian process models, each predictor pðy Ã jM k Þ is a Gaussian distribution. Therefore the product in Eq. ( <ref type="formula" target="#formula_13">11</ref>) is still Gaussian with mean and variance defined by</p><formula xml:id="formula_14">Eðy Ã Þ ¼ Varðy Ã Þ X K k¼1 ŷÃ ðkÞs À2 ŷÃ ðkÞ (12) Varðy Ã Þ ¼ X K k¼1 s À2 ŷÃ ðkÞ ! À1<label>(13)</label></formula><p>In Eqs. ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_14">13</ref>) the weights are w k ¼ s À2</p><formula xml:id="formula_15">ŷÃ ðkÞ= P K k¼1 s À2</formula><p>ŷÃ ðkÞ and the models are weighted by the inverse of the corresponding prediction variance. Models that are uncertain about their predictions, i.e. with large variances, are automatically given smaller weights than models that are certain about their predictions. Note that the weighting strategy of the BCM <ref type="bibr" target="#b21">[22]</ref> is similar to Eqs. ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_14">13</ref>) with the difference being the variance term. Specifically the variance term of the BCM is</p><formula xml:id="formula_16">Varðy Ã Þ ¼ ÀðK À 1ÞCðx Ã ; x Ã Þ þ X K k¼1 s À2 ŷÃ ðkÞ ! À1<label>(14)</label></formula><p>Conceptually the BCM is less appealing since the sum of weights is not equal to unity. We have not found that the BCM attains better prediction accuracy than the proposed product rule (detailed results not reported) when applied to the industrial example in the next section. Therefore the BCM method is not considered further in this paper for bagging Gaussian process models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Industrial case study</head><p>In this section we apply the bagging method for Gaussian process models to the inferential estimation of the quality variables, i.e. the MFR, in a polypropylene polymerization process. A major difficulty in the monitoring and control of product quality in industrial polymerization reactors is the lack of suitable on-line polymer quality measurements <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>. In this case study, MFR is measured through laboratory analysis every 2 h, which is typical in the polymerization industry. For the purpose of monitoring and control, on-line prediction of MFR is required.</p><p>The quality variables could be inferred through the mathematical modeling of the process based on physical and chemical mechanism <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>. However, the development of mechanistic models requires a deep understanding of the process and is typically time-consuming. To meet the rapid changing of the market, a fast modeling approach is desirable. As a result the technique of data-based empirical modeling has emerged as a solution to infer the quality variables from process measurements that are routinely recorded <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. The inferential model is also termed soft sensor in process engineering to differentiate it from hardware sensors.</p><p>A total of 360 data points were collected from a propylene polymerization plant operated in a continuous mode. Eight process variables are selected as the input to the Gaussian process model, including hydrogen concentration, feed rates of two catalysts, feed rates of propylene and hydrogen, reactor temperature, pressure and level measurements. The data are randomly partitioned into training and test sets. Different sizes of training data are used to investigate the impact of the amount of data on the prediction performance. The random partition is repeated 50 times under each situation so that the results are not susceptible to a specific split of the data. The covariance function in Eq. ( <ref type="formula" target="#formula_2">2</ref>) is adopted due to its reported good performance in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>, and the conjugate gradient method is used to find the maximum likelihood estimation of the hyper-parameters. Following the common practice, the data are pre-processed to have zero mean and unit standard deviation at each variable before it is used for training a Gaussian process.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> depicts the root mean square error (RMSE) of one of the random partitions with training set having 180 data points. Fig. <ref type="figure" target="#fig_0">1(a)</ref> shows that the 30 models, developed from bootstrap re-sampled training sets, achieve quite different prediction performance. The highest RMSE is 0.233 and the lowest 0.140. In practice the testing outputs, to be predicted, are not available to calculate the RMSE for the selection of the best model. We could select the model with the lowest training error, termed ''Train_Best'' in the figure, and it gives an RMSE of 0.191. Fig. <ref type="figure" target="#fig_0">1(b)</ref> shows that bagging, through the product rule as in Eqs. ( <ref type="formula">12</ref>) and <ref type="bibr" target="#b12">(13)</ref>, makes the combined model significantly more accurate and robust. It appears that the combination of five or more models result in very stable predictors. The RMSE for bagging 30 models is 0.128 that is lower than the best single Gaussian process.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> compares the prediction and measured (reference) MFR values for the ''Train_Best'' and ''Bagging'' models as labeled in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. The figure confirms the superior prediction accuracy of the bagging method to a single Gaussian process model.  Table <ref type="table" target="#tab_2">1</ref> summarizes the prediction performance for different training data size, where the average RMSE is calculated based on 50 random partitions of the training and test data. Bagging is performed using 10 bootstrap re-sampled training sets since Fig. <ref type="figure" target="#fig_0">1</ref> suggests that the combination of 10 models is sufficient to produce a robust inferential estimation model. The following modeling methods are compared:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>Single: the single model that is developed from the original training data.</p><p>Train_Best: the single model selected from 10 models to minimize the training errors.</p><p>Bagging_Aveg: Bagging 10 models through averaging as in Eqs. ( <ref type="formula" target="#formula_10">8</ref>) and <ref type="bibr" target="#b8">(9)</ref>. Bagging_Regn: Bagging 10 models where the weights are determined using linear regression as in Eq. <ref type="bibr" target="#b9">(10)</ref>. Principal component analysis is utilized to address the collinearity issue.</p><p>Bagging_Prod: Bagging 10 models where the weights are determined using the product rule as in Eqs. ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_14">13</ref>).</p><p>The general trend in Table <ref type="table" target="#tab_2">1</ref> is that the more training data, the better the prediction is. In industrial practice the training data must be sufficient so that the prediction accuracy satisfies the requirement of process monitoring and control. However, the cost involved in obtaining the data must be taken into account when deciding the amount of data to be collected. Typically data collection is a recursive procedure where an initial data set is obtained for modeling, and the prediction accuracy is assessed to decide whether more data are required.</p><p>Table <ref type="table" target="#tab_2">1</ref> indicates that selecting the ''best'' model (in terms of minimal training errors) from 10 models (i.e. Train_Best) is not particularly superior to developing a single model from the original training data. In other words the training errors may not be a reliable criterion for model selection. In addition, the results in Table <ref type="table" target="#tab_2">1</ref> clearly justify the application of bagging: model combination consistently gives lower RMSE in this example, regardless of the size of training data. A comparison between Bagging_Aveg and Bagging_Regn reveals that the weighting strategy based on regression does not outperform the simple averaging method for this data set. Finally the Bagging_Prod method, where the weights are assigned according to the prediction uncertainty, appears to be a promising approach to model combination that attains the lowest RMSE. Therefore the improvement in robustness and accuracy of the prediction is at the cost of additional computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and discussions</head><p>This paper applies the bagging method to improve the robustness and prediction accuracy of Gaussian process regression models. A novel model combination rule is proposed to assign the weights to models based on their predictive uncertainty. Application study on an industrial example suggested that (1) through bagging the predictions are more reliable and accurate; (2) the proposed model combination strategy significantly outperforms the conventional methods. Therefore we recommend to utilize the bagging procedure to enhance the Gaussian process regression models if the additional computational cost is permitted in practice.</p><p>The bagging procedure essentially assumes that the training data are independent and identically distributed. However, Gaussian process explicitly models the correlation between data points, which appears to contradict the underlying assumption. As a result, we hypothesize that the success of bagging, as demonstrated by the case study, is due to the fact that the instability of Gaussian process plays a more important role than the violation of the independence assumption in this example. Further theoretical study is needed to investigate this issue, which is an interesting on-going research topic.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Prediction errors of (a) single and (b) combined Gaussian process models. In (a) also depicted is the RMSE of bagging 30 models based on the product rule and of the ''best'' model selected from 30 models to minimize the training errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Prediction vs. measured MFR values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>illustrates the CPU time (s) for training of 10 Gaussian process models from bootstrap re-sampled data, where the model was implemented within Matlab environment under Windows XP system equipped with a Pentium 2.8 GHz CPU. The values quoted are averaged over 50 partitions of the training and test data. The computational cost increases considerably with the size of training data, yet it is still manageable from a practical perspective. If a large data set is available, the so-called sparse training techniques, mentioned in Section 2.2, should be employed to reduce the training time. Approximately the CPU time for the training of a single model from the original data set is 1 10 (or more generally 1=K) of that required for the bagging technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. CPU time for training of 10 models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Average RMSE for different training data size</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell></row><row><cell>Training data size</cell><cell>60</cell><cell>120</cell><cell>180</cell><cell>240</cell><cell></cell><cell>45</cell></row><row><cell>Single</cell><cell>0.249</cell><cell>0.216</cell><cell>0.145</cell><cell>0.124</cell><cell></cell><cell></cell></row><row><cell>Train_Best Bagging_Aveg Bagging_Regn Bagging_Prod</cell><cell>0.250 0.201 0.202 0.183</cell><cell>0.196 0.161 0.163 0.145</cell><cell>0.165 0.129 0.133 0.115</cell><cell>0.141 0.109 0.117 0.087</cell><cell>CPU Time (s)</cell><cell>35 40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell>120</cell><cell>180</cell><cell>240</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Training Data Size</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>T. Chen, J. Ren / Neurocomputing 72 (2009) 1605-1610</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving nonparametric regression methods by bagging and boosting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Borra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Di</forename><surname>Ciaccio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analyzing bagging</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bu ¨chlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="927" to="961" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Particle filters for state and parameter estimation in batch processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Process Control</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaussian process regression for multivariate spectroscopic calibration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics Intell. Lab. Syst</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="59" to="67" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaussian processes for ordinal regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1019" to="1041" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse on-line Gaussian processes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Csato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="641" to="668" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonparametric estimates of standard error: the jackknife, the bootstrap and other methods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="589" to="599" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical process modeling: describing within-run and between-run variations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H K</forename><surname>Entink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H L</forename><surname>Betlem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Process Control</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="349" to="361" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pricing and hedging derivative securities with neural networks: Bayesian regularization, early stopping, and bagging</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gencay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="726" to="734" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiplestep ahead prediction for non-linear dynamic systems-a Gaussian process treatment with propagation of the uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quin ˜onero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="529" to="536" />
			<date type="published" when="2003">2003</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear modeling of FESsupported standing-up in paraplegia for selection of feedback sensors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kamnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bajd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="40" to="52" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-based process monitoring, process control, and quality improvement: recent developments and applications in steel industry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Chem. Eng</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="12" to="24" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian calibration of computer models (with discussions)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>O'hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="425" to="464" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online optimizing control of molecular weight properties in batch free-radical polymerization reactors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kiparissides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seferlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mourikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ind. Eng. Chem. Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="6120" to="6131" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bagging linear sparse Bayesian learning models for variable selection in cancer diagnosis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Devos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomedicine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="338" to="347" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to Gaussian processes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks and Machine Learning, F: Computer and Systems Sciences</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="133" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<title level="m">Finite Mixture Models</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bayesian Learning for Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Curve fitting and optimal design for prediction (with discussion)</title>
		<author>
			<persName><forename type="first">A</forename><surname>O'hagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Bayesian committee machine</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2719" to="2741" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fermentation process tracking through enhanced spectral calibration modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Triadaphillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jeffkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stimpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biotechnol. Bioeng</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="554" to="567" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Principal component regression, ridge regression and ridge principal component regression in spectroscopy calibration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vigneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Devaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Qannari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chemometrics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="239" to="249" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gaussian processes for regression</title>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hasselmo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reliable multi-objective optimization of highspeed WEDM process based on Gaussian process regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Tools Manuf</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inferential estimation of polymer quality using bootstrap aggregated neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="927" to="938" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">His research interests are in the broad area of process systems engineering, including chemometrics, modeling, optimization and control of chemical processes, and process fault detection and diagnosis. Jianghong Ren received the B.E. and M.E. degrees in 1999 and 2004 from College of Automation</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen Received The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2002. 2006</date>
			<pubPlace>Newcastle upon Tyne, UK; Singapore; Chongqing, China; Chongqing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>both from Department of Automation, Tsinghua University, Beijing, China, and Ph.D. degree in Chemical Engineering from Newcastle University ; Nanyang Technological University ; Chongqing University ; Chongqing University</orgName>
		</respStmt>
	</monogr>
	<note>His research is focused on the areas of the machine learning and artificial intelligence, in particular statistical relational learning, inductive logic programming, and machine learning for multi-agent systems</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
