<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Convergence Theory for Deep Learning via Over-Parameterization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
							<email>&lt;yuanzhil@stanford.edu&gt;</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>3 Princeton University 4 UT-Austin 5 University Wash-ington</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
							<email>&lt;zhaos@utexas.edu&gt;.</email>
							<affiliation key="aff2">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Yuanzhi Li</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Convergence Theory for Deep Learning via Over-Parameterization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find global minima on the training objective of DNNs in polynomial time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed ε ∝ e −Ω(T ) , with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).</p><p>* Equal contribution . Full version and future updates are available at https://arxiv.org/abs/1811.03962.</p><p>This paper is a follow up to the recurrent neural network (RNN) paper <ref type="bibr" target="#b4">(Allen-Zhu et al., 2018b)</ref> by the same set of authors. Most of the techniques used in this paper were already discovered in the RNN paper, and this paper can be viewed as a simplification (or to some extent a special case) of the RNN setting in order to reach out to a wider audience. We compare the difference and mention our additional contribution in Section 1.2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks have demonstrated a great success in numerous machine-learning tasks <ref type="bibr" target="#b5">(Amodei et al., 2016;</ref><ref type="bibr" target="#b25">Graves et al., 2013;</ref><ref type="bibr" target="#b30">He et al., 2016;</ref><ref type="bibr" target="#b33">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b36">Lillicrap et al., 2015;</ref><ref type="bibr" target="#b45">Silver et al., 2016;</ref><ref type="bibr">2017)</ref>. One of the empirical findings is that neural networks, trained by firstorder methods from random initialization, have a remarkable ability of fitting training data <ref type="bibr" target="#b57">(Zhang et al., 2017)</ref>.</p><p>From a capacity perspective, the ability to fit training data may not be surprising: modern neural networks are always heavily over-parameterized -they have (much) more parameters than the total number of training samples. Thus, there exists parameter choices to achieve zero training error as long as data does not degenerate.</p><p>Yet, from an optimization perspective, the fact that randomly initialized first-order methods can find optimal solutions on the training data is quite non-trivial: neural networks are often equipped with the ReLU activation, making the training objective not only non-convex, but even non-smooth. Even the general convergence for finding approximate critical points of a non-convex, non-smooth function is not fully understood <ref type="bibr" target="#b11">(Burke et al., 2005)</ref>, and appears to be a challenging question on its own. This is in direct contrast to practice, in which ReLU networks trained by stochastic gradient descent (SGD) from random initialization almost never face the problem of non-smoothness or non-convexity, and can converge to even a global minimal over the training set quite easily. This was demonstrated by <ref type="bibr" target="#b24">Goodfellow et al. (2015)</ref> using experiments for a variety of network architectures, and a theoretical justification remains missing to explain this phenomenon.</p><p>Recently, there are quite a few papers trying to understand the success of neural networks from optimization perspective. Many of them focus on the case when the inputs are random Gaussian, and work only for two-layer neural networks <ref type="bibr" target="#b10">(Brutzkus &amp; Globerson, 2017;</ref><ref type="bibr" target="#b18">Du et al., 2018b;</ref><ref type="bibr" target="#b21">Ge et al., 2017;</ref><ref type="bibr" target="#b35">Li &amp; Yuan, 2017;</ref><ref type="bibr" target="#b42">Panigrahy et al., 2018;</ref><ref type="bibr" target="#b49">Soltanolkotabi, 2017;</ref><ref type="bibr" target="#b54">Tian, 2017;</ref><ref type="bibr" target="#b59">Zhong et al., 2017a;</ref><ref type="bibr">b)</ref>.</p><p>In <ref type="bibr" target="#b34">Li &amp; Liang (2018)</ref>, it was shown that for a two-layer network with ReLU activation, SGD finds nearly-global optimal (say, 99% classification accuracy) solutions on the training data, as long as the network is over-parameterized, meaning that when the number of neurons is polynomi-ally large comparing to the input size. Moreover, if the data is sufficiently structured (say, coming from mixtures of separable distributions), this perfect accuracy extends also to test data. As a separate note, over-parameterization is suggested as the possible key to avoid bad local minima by <ref type="bibr" target="#b43">Safran &amp; Shamir (2018)</ref> even for two-layer networks.</p><p>There are also results that go beyond two-layer neural networks but with limitations. Some consider deep linear neural networks without any activation functions <ref type="bibr" target="#b6">(Arora et al., 2018a;</ref><ref type="bibr" target="#b8">Bartlett et al., 2018;</ref><ref type="bibr" target="#b26">Hardt &amp; Ma, 2017;</ref><ref type="bibr" target="#b31">Kawaguchi, 2016)</ref>. The result of <ref type="bibr" target="#b13">Daniely (2017)</ref> applies to multi-layer neural network with ReLU activation, but is about the convex training process only with respect to the last layer. Daniely worked in a parameter regime where the weight changes of all layers except the last one make negligible contribution to the final output (and they form the so-called conjugate kernel). The result of <ref type="bibr" target="#b51">Soudry &amp; Carmon (2016)</ref> shows that under over-parameterization and under random input perturbation, there is bad local minima for multi-layer neural networks. Their work did not show any provable convergence rate.</p><p>In this paper, we study the following fundamental question</p><p>Can DNN be trained close to zero training error efficiently under mild assumptions?</p><p>If so, can the running time depend only polynomially in the number of layers?</p><p>Motivation. In 2012 AlexNet <ref type="bibr" target="#b33">(Krizhevsky et al., 2012)</ref> was born with 5 convolutional layers. Since then, the common trend in the deep learning community is to build network architectures that go deeper. In 2014, <ref type="bibr" target="#b48">Simonyan &amp; Zisserman (2014)</ref> proposed a VGG network with 19 layers. Later, <ref type="bibr" target="#b53">Szegedy et al. (2015)</ref> proposed GoogleNet with 22 layers. In practice, we cannot make the network deeper by naively stacking layers together due to the so-called vanishing / exploding gradient issues. For this reason, in 2015, <ref type="bibr" target="#b30">He et al. (2016)</ref> proposed an ingenious deep network structure called Deep Residual Network (ResNet), with the capability of handling at least 152 layers. For more overview and variants of ResNet, we refer the readers to <ref type="bibr" target="#b20">(Fung, 2017)</ref>.</p><p>Compared to the practical neural networks that go much deeper, the existing theory has been mostly around twolayer (thus one-hidden-layer) networks even just for the training process alone. It is natural to ask if we can theoretically understand how the training process has worked for multi-layer neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Result</head><p>In this paper, we extend the over-parameterization theory to multi-layer neural networks. We show that overparameterized neural networks can indeed be trained by regular first-order methods to global minima (e.g. zero training error), as as long as the dataset is non-degenerate. We say that the dataset is non-degenerate if the data points are distinct. This is a minimal requirement since a dataset {(x 1 , y 1 ), (x 2 , y 2 )} with the same input x 1 = x 2 and different labels y 1 = y 2 can not be trained to zero error. We denote by δ the minimum (relative) distance between two training data points, and by n the number of samples in the training dataset. Now, consider an L-layer fully-connected feedforward neural network, each layer consisting of m neurons equipped with ReLU activation. We show that,</p><p>• As long as m ≥ poly(n, L, δ −1 ), starting from random Gaussian initialized weights, gradient descent (GD) and stochastic gradient descent (SGD) find εerror global minimum in 2 regression using at most T = poly(n, L, δ −1 ) log 1 ε iterations. This is a linear convergence rate.</p><p>• Using the same network, if the task is multi-label classification, then GD and SGD find an 100% accuracy classifier on the training set in T = poly(n, L, δ −1 ) iterations.</p><p>• Our result also applies to other Lipschitz-smooth loss functions, and some other network architectures including convolutional neural networks (CNNs) and residual networks (ResNet). Remark. This paper does not cover the the generalization of over-parameterized neural networks to the test data. We refer interested readers to some practical evidence <ref type="bibr" target="#b52">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b56">Zagoruyko &amp; Komodakis, 2016</ref>) that deeper (and wider) neural networks actually generalize better. As for theoretical results, over-parameterized neural networks provably generalize at least for two-layer networks <ref type="bibr" target="#b3">(Allen-Zhu et al., 2018a;</ref><ref type="bibr" target="#b34">Li &amp; Liang, 2018)</ref> and for three-layer networks <ref type="bibr" target="#b3">(Allen-Zhu et al., 2018a)</ref>. 1   A concurrent but different result. We acknowledge a concurrent work of <ref type="bibr" target="#b17">Du et al. (2018a)</ref> which has a similar abstract to this paper, but is different from us in many aspects. Since we noticed many readers cannot tell the two results apart, we compare them carefully below. <ref type="bibr" target="#b17">Du et al. (2018a)</ref> has two main results, one for fully-connected networks and the other for residual networks (ResNet).</p><p>For fully-connected networks, they only proved the training time is no more than exponential in the number of layers, leading to a claim of the form "ResNet has an advantage because ResNet is polynomial-time but fully-connected net-1 If data is "well-structured" two-layer over-parameterized neural networks can learn it using SGD with polynomially many samples <ref type="bibr" target="#b34">(Li &amp; Liang, 2018)</ref>. If data is produced by some unknown two-layer (resp. three-layer) neural network, then twolayer (resp. three-layer) neural networks can also provably learn it using SGD and polynomially many samples <ref type="bibr" target="#b3">(Allen-Zhu et al., 2018a)</ref>.</p><p>work is (possibly) exponential-time." As we prove in this paper, fully-connected networks do have polynomial training time, so their logic behind this claim is ungrounded.</p><p>For residual networks, their training time scales polynomial in 1 λ0 , a parameter that depends on the minimal singular value of a complicated, L-times recursively-defined kernel matrix. It is not clear whether 1 λ0 is small or even polynomial from their original writing. In their version 2, they have sketched a possible proof to bound 1 λ0 in the special case of residual networks.</p><p>Their result is different from us in many other aspects. Their result only applies to the (significantly simpler<ref type="foot" target="#foot_0">2</ref> ) smooth activation functions and thus cannot apply to the state-of-the-art ReLU activation. Their ResNet requires the value of weight initialization to be a function polynomial in λ (which is our δ); this can heavily depend on the input data. Their result only applies to gradient descent but not to SGD. Their result only applies to 2 loss but not others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Other Related Works</head><p>Li &amp; Liang (2018) originally proved their result for the cross-entropy loss. Later, the "training accuracy" (not the testing accuracy) part of <ref type="bibr" target="#b34">(Li &amp; Liang, 2018)</ref> was extended to the 2 loss <ref type="bibr" target="#b19">(Du et al., 2018c)</ref>.</p><p>Linear networks without activation functions are important subjects on its own. Besides the already cited references <ref type="bibr" target="#b6">(Arora et al., 2018a;</ref><ref type="bibr" target="#b8">Bartlett et al., 2018;</ref><ref type="bibr" target="#b26">Hardt &amp; Ma, 2017;</ref><ref type="bibr" target="#b31">Kawaguchi, 2016)</ref>, there are a number of works that study linear dynamical systems, which can be viewed as the linear version of recurrent neural networks or reinforcement learning. Recent works in this line of research include <ref type="bibr" target="#b0">(Alaeddini et al., 2018;</ref><ref type="bibr" target="#b7">Arora et al., 2018b;</ref><ref type="bibr" target="#b15">Dean et al., 2017;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b27">Hardt et al., 2018;</ref><ref type="bibr" target="#b28">Hazan et al., 2017;</ref><ref type="bibr">2018;</ref><ref type="bibr">Marecek &amp; Tchrakian, 2018;</ref><ref type="bibr" target="#b41">Oymak &amp; Ozay, 2018;</ref><ref type="bibr" target="#b47">Simchowitz et al., 2018)</ref>.</p><p>There is sequence of work about one-hidden-layer (multiple neurons) CNN <ref type="bibr" target="#b10">(Brutzkus &amp; Globerson, 2017;</ref><ref type="bibr" target="#b18">Du et al., 2018b;</ref><ref type="bibr" target="#b23">Goel et al., 2018;</ref><ref type="bibr" target="#b40">Oymak, 2018;</ref><ref type="bibr" target="#b59">Zhong et al., 2017a)</ref>. Whether the patches overlap or not plays a crucial role in analyzing algorithms for such CNN. One category of the results have required the patches to be disjoint <ref type="bibr" target="#b10">(Brutzkus &amp; Globerson, 2017;</ref><ref type="bibr" target="#b18">Du et al., 2018b;</ref><ref type="bibr" target="#b59">Zhong et al., 2017a)</ref>. The other category <ref type="bibr" target="#b23">(Goel et al., 2018;</ref><ref type="bibr" target="#b40">Oymak, 2018)</ref> have figured out a weaker assumption or even removed that patch-disjoint assumption. On input data distribution, most relied on inputs being Gaussian <ref type="bibr" target="#b10">(Brutzkus &amp; Globerson, 2017;</ref><ref type="bibr" target="#b18">Du et al., 2018b;</ref><ref type="bibr" target="#b40">Oymak, 2018;</ref><ref type="bibr" target="#b59">Zhong et al., 2017a)</ref>, and some assumed inputs to be symmet-rically distributed with identity covariance and boundedness <ref type="bibr" target="#b23">(Goel et al., 2018)</ref>.</p><p>As for ResNet, <ref type="bibr" target="#b35">Li &amp; Yuan (2017)</ref> proved that SGD learns one-hidden-layer residual neural networks under Gaussian input assumption. The techniques in <ref type="bibr" target="#b59">(Zhong et al., 2017a;</ref><ref type="bibr">b)</ref> can also be generalized to one-hidden-layer ResNet under the Gaussian input assumption; they can show that GD starting from good initialization point (via tensor initialization) learns ResNet. <ref type="bibr" target="#b26">Hardt &amp; Ma (2017)</ref> deep linear residual networks have no spurious local optima.</p><p>If no assumption is allowed, neural networks have been shown hard in several different perspectives. Thirty years ago, <ref type="bibr" target="#b9">Blum &amp; Rivest (1993)</ref> first proved that learning the neural network is NP-complete. Stronger hardness results have been proved over the last decade <ref type="bibr" target="#b12">(Daniely, 2016;</ref><ref type="bibr" target="#b14">Daniely &amp; Shalev-Shwartz, 2016;</ref><ref type="bibr" target="#b22">Goel et al., 2017;</ref><ref type="bibr" target="#b32">Klivans &amp; Sherstov, 2009;</ref><ref type="bibr" target="#b37">Livni et al., 2014;</ref><ref type="bibr" target="#b38">Manurangsi &amp; Reichman, 2018;</ref><ref type="bibr" target="#b50">Song et al., 2017)</ref>.</p><p>An over-parameterized RNN theory. For experts in DNN theory, one may view this present paper as a deeplysimplified version of the recurrent neural network (RNN) paper <ref type="bibr" target="#b4">(Allen-Zhu et al., 2018b)</ref> by the same set of authors. A recurrent neural network executed on input sequences with time horizon L is very similar to a feedforward neural network with L layers. The main difference is that in feedforward neural networks, weight matrices are different across layers, and thus independently randomly initialized; in contrast, in RNN, the same weight matrix is applied across the entire time horizon so we do not have fresh new randomness for proofs that involve in induction.</p><p>So, the over-parameterized convergence theory of DNN is much simpler than that of RNN.</p><p>We write this DNN result as a separate paper because: (1) not all the readers can easily notice that DNN is easier to study than RNN;</p><p>(2) we believe the convergence of DNN is important on its own; (3) the proof in this paper is much simpler (30 vs 80 pages) and could reach out to a wider audience; (4) the simplicity of this paper allows us to tighten parameters in some non-trivial ways; and (5) the simplicity of this paper allows us to also study convolutional networks, residual networks, as well as different loss functions (all of them were missing from <ref type="bibr" target="#b4">(Allen-Zhu et al., 2018b)</ref>).</p><p>We also note that the techniques of this paper can be combined with <ref type="bibr" target="#b4">(Allen-Zhu et al., 2018b)</ref> to show the convergence of over-parameterized deep RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We use N (µ, σ) to denote the Gaussian distribution of mean µ and variance σ; and B(m, 1 2 ) to denote the binomial distribution with m trials and 1/2 success rate. We use v to denote Euclidean norms of vectors v, and M 2 , M F to denote spectral and Frobenius norms of matrices M. For a tuple</p><formula xml:id="formula_0">− → W = (W 1 , . . . , W L ) of matrices, we let − → W 2 = max ∈[L] W 2 and − → W F = ( L =1 W 2 F ) 1/2</formula><p>. We use φ(x) = max{0, x} to denote the ReLU function, and extend it to vectors v ∈ R m by letting φ(v) = (φ(v 1 ), . . . , φ(v m )). We use 1 event to denote the indicator function for event.</p><p>The training data consist of vector pairs {(x i , y * i )} i∈ <ref type="bibr">[n]</ref> , where each x i ∈ R d is the feature vector and y * i is the label of the i-th training sample. We assume without loss of generality that data are normalized so that x i = 1 and its last coordinate 3 We make the following separable assumption on the training data (motivated by <ref type="bibr" target="#b34">(Li &amp; Liang, 2018)</ref>):</p><formula xml:id="formula_1">(x i ) d = 1 √ 2 .</formula><p>Assumption 2.1. For every pair i, j ∈ [n], we have</p><formula xml:id="formula_2">x i − x j ≥ δ.</formula><p>To present the simplest possible proof, the main body of this paper only focuses on depth-L feedforward fullyconnected neural networks with an 2 -regression task. Therefore, each y * i ∈ R d is a target vector for the regression task. We explain how to extend it to more general settings in Section 5 and the Appendix. For notational simplicity, we assume all the hidden layers have the same number of neurons, and our results trivially generalize to each layer having different number of neurons. Specifically, we focus on the following network</p><formula xml:id="formula_3">g i,0 = Ax i h i,0 = φ(Ax i ) for i ∈ [n] g i, = W h i, −1 h i, = φ(W h i, −1 ) for i ∈ [n], ∈ [L] y i = Bh i,L for i ∈ [n]</formula><p>where A ∈ R m×d is the weight matrix for the input layer, W ∈ R m×m is the weight matrix for the -th hidden layer, and B ∈ R d×m is the weight matrix for the output layer.</p><p>For notational convenience in the proofs, we may also use h i,−1 to denote x i and W 0 to denote A.</p><p>Definition 2.2 (diagonal sign matrix). For each i ∈ [n] and ∈ {0, 1, . . . , L}, we denote by D i, the diagonal sign matrix where</p><formula xml:id="formula_4">(D i, ) k,k = 1 (W h i, −1 ) k ≥0 for each k ∈ [m].</formula><p>As a result, we have We make the following standard choices of random initialization:</p><formula xml:id="formula_5">h i, = D i, W h i, −1 = D i, g i,<label>and</label></formula><formula xml:id="formula_6">(D i, ) k,k = 1 (g i, ) k ≥0 .</formula><p>Definition 2.3. We say that − → W = (W 1 , . . . , W L ), A and B are at random initialization if</p><formula xml:id="formula_7">• [W ] i,j ∼ N (0, 2 m ) for every i, j ∈ [m] and ∈ [L]; • A i,j ∼ N (0, 2 m ) for every (i, j) ∈ [m] × [d]; and • B i,j ∼ N (0, 1 d ) for every (i, j) ∈ [d] × [m]</formula><p>. Assumption 2.4. Throughout this paper we assume m ≥ Ω poly(n, L, δ −1 ) • d for some sufficiently large polynomial. To present the simplest proof, we did not try to improve such polynomial factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Objective and Gradient</head><p>Our regression objective is</p><formula xml:id="formula_8">F ( − → W) := n i=1 F i ( − → W)</formula><p>where</p><formula xml:id="formula_9">F i ( − → W) := 1 2 Bh i,L − y * i 2 for each i ∈ [n]</formula><p>We also denote by loss i := Bh i,L − y * i the loss vector for sample i. For simplicity, we only focus on training − → W in this paper and thus leave A and B at random initialization. Our techniques can be extended to the case when A, B and − → W are jointly trained.</p><formula xml:id="formula_10">Definition 2.5. For each ∈ {1, 2, • • • , L}, we define Back i, := BD i,L W L • • • D i, W ∈ R d×m and for = L + 1, we define Back i, = B ∈ R d×m .</formula><p>Using this notation, one can calculate the gradient of F ( − → W) as follows.</p><p>Fact 2.6. The gradient with respect to the k-th row of</p><formula xml:id="formula_11">W ∈ R m×m is ∇ [W ] k F ( − → W) = n i=1 (Back i, +1 loss i ) k • h i, −1 • 1 [W ] k ,h i, −1 ≥0</formula><p>The gradient with respect to W is</p><formula xml:id="formula_12">∇ W F ( − → W) = n i=1 D i, (Back i, +1 loss i )h i, −1 We denote by ∇F ( − → W) = ∇ W1 F ( − → W), . . . , ∇ W L F ( − → W) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Results and Techniques</head><p>To present our result in the simplest possible way, we choose to mainly focus on fully-connected L-layer neural networks with the 2 regression loss. We shall extend it to more general settings (such as convolutional and residual networks and other losses) in Section 5. Our main results can be stated as follows:</p><p>Theorem 1 (gradient descent). Suppose m ≥ Ω poly(n, L, δ −1 ) • d . Starting from random initializa-tion, with probability at least 1 − e −Ω(log 2 m) , gradient descent with learning rate η = Θ dδ poly(n,L)•m finds a point</p><formula xml:id="formula_13">F ( − → W) ≤ ε in T = Θ poly(n, L) δ 2 • log ε −1 iterations.</formula><p>This is known as the linear convergence rate because ε drops exponentially fast in T . We have not tried to improve the polynomial factors in m and T , and are aware of several ways to improve these factors (but at the expense of complicating the proof). We note that d is the data input dimension and our result is independent of d.</p><p>Remark. In our version 1, for simplicity, we also put a log </p><formula xml:id="formula_14">F ( − → W) ≤ ε in T = Θ poly(n, L) • log 2 m δ 2 b • log ε −1 iterations.</formula><p>This is again a linear convergence rate because T ∝ log 1 ε . The reason for the additional log 2 m factor comparing to Theorem 1 is because we have a 1 − e −Ω(log 2 m) high confidence bound. Remark. For experts in optimization theory, one may immediately question the accuracy of Theorem 2, because SGD is known to converge at a slower rate T ∝ 1 poly(ε) even for convex functions. There is no contradiction here. Imaging a strongly convex function f (x) = n i=1 f i (x) that has a common minimizer x * ∈ arg min x {f i (x)} for every i ∈ [n], then SGD is known to converge in a linear convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Technical Theorems</head><p>The main difficulty of this paper is to prove the following two technical theorems. The first one is about the gradient bounds for points that are sufficiently close to the random initialization:</p><p>Theorem 3 (no critical point). With probability ≥ 1 − e −Ω(m/poly(n,L,δ −1 )) over randomness − → W (0) , A, B, it satisfies for every ∈ [L], every i ∈ [n], and every</p><formula xml:id="formula_15">− → W with − → W − − → W (0) 2 ≤ 1 poly(n,L,δ −1 ) , ∇F ( − → W) 2 F ≤ O F ( − → W) × Lnm d ∇F ( − → W) 2 F ≥ Ω F ( − → W) × δm dn 2 .</formula><p>Most notably, the second property of Theorem 3 says that as long as the objective is large, the gradient norm is also large. (See also Figure <ref type="figure" target="#fig_2">1</ref>.) This means, when we are sufficiently close to the random initialization, there is no saddle point or critical point of any order. This gives us hope to find global minima of the objective F ( − → W).</p><p>Unfortunately, Theorem 3 itself is enough. Even if we follow the negative gradient direction of F ( − → W), how can we guarantee that the objective truly decreases? Classically in optimization theory, one relies on the smoothness property (e.g. Lipscthiz smoothness <ref type="bibr" target="#b39">(Nesterov, 2004)</ref>) to derive such objective-decrease guarantee. Unfortunately, smoothness property at least requires the objective to be twice differentiable, but ReLU activation is not.</p><p>To deal with this issue, we prove the following "semismoothness" property of the objective.</p><p>Theorem 4 (semi-smoothness).</p><p>With probability at least 1 − e −Ω(m/poly(L,log m)) over the randomness of − → W (0) , A, B, we have :</p><formula xml:id="formula_16">for every − → W ∈ (R m×m ) L with − → W − − → W (0) 2 ≤ 1 poly(L, log m) ,</formula><p>and for every</p><formula xml:id="formula_17">− → W ∈ (R m×m ) L with − → W 2 ≤ 1 poly(L, log m) ,</formula><p>the following inequality holds</p><formula xml:id="formula_18">F ( − → W + − → W ) ≤ F ( − → W) + ∇F ( − → W), − → W + O nL 2 m d − → W 2 2 + poly(L) √ nm log m √ d • − → W 2 F ( − → W) 1/2</formula><p>Quite different from classical smoothness, we still have a first-order term − → W 2 on the right hand side, but classical smoothness only has a second-order term − → W 2 2 . As one can see in our final proofs, as m goes larger (so when we over-parameterize), the effect of the first-order term becomes smaller and smaller comparing to the second-order term. This brings Theorem 4 closer and closer, but still not identical, to the classical Lipschitz smoothness.</p><p>The derivation of our main Theorem 1 and 2 from technical Theorem 3 and 4 is quite straightforward, and can be found in Section F and G. Remark. In our proofs, we show that GD and SGD can converge fast enough and thus the weights stay close to random initialization by spectral norm bound 1 poly(n,L,δ −1 ) . (This ensures Theorem 3 and 4 both apply.) This bound seems extremely small, but in fact, is large enough to to- Observation. As far as minimizing objective is concerned, the (negative) gradient direction sufficiently decreases the training objective. This is consistent with our main findings Theorem 3 and 4. Using second-order information gives little help.</p><p>Remark 1. Gradient norm does not tend to zero because cross-entropy loss is not strongly convex (see Section 5).</p><p>Remark 2. The task is CIFAR10 (for CIFAR100 or CIFAR10 with noisy label, see Figure <ref type="figure">2</ref> through 7 in appendix). for m being large.</p><p>In practice, we acknowledge that one often goes beyond this theory-predicted spectral-norm boundary. However, quite interestingly, we still observe Theorem 3 and 4 happen in practice at least for image classification tasks. In Figure <ref type="figure" target="#fig_2">1</ref>, we show the typical landscape near a point − → W on the SGD training trajectory. The gradient is sufficiently large and going in its direction can indeed decrease the objective; in contrast, though the objective is non-convex, the negative curvature of its "Hessian" is not significant comparing to gradient. From Figure <ref type="figure" target="#fig_2">1</ref> we also see that the objective function is sufficiently smooth (at least in the two interested dimensions that we plot).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Main Techniques</head><p>Proof to Theorem 3 and 4 consist of the following steps.</p><p>Step 1: properties at random initialization. Let − → W = − → W (0) be at random initialization and h i, and D i, be de-fined with respect to − → W. We first show that forward propagation neither explode or vanish. That is,</p><formula xml:id="formula_19">h i, ≈ 1 for all i ∈ [n] and ∈ [L].</formula><p>This is basically because for a fixed y, we have Wy 2 is around 2, and if its signs are sufficiently random, then ReLU activation kills half of the norm, that is φ(Wy) ≈ 1. Then applying induction finishes the proof.</p><p>Analyzing forward propagation is not enough. We also need spectral norm bounds on the backward matrix</p><formula xml:id="formula_20">BD i,L W L • • • D i,a W a 2 ≤ O( m/d) ,</formula><p>and on the intermediate matrix</p><formula xml:id="formula_21">D i,a W a • • • D i,b W b 2 ≤ O( √ L) for every a, b ∈ [L]</formula><p>. Note that if one naively bounds the spectral norm by induction, then D i,a W a 2 ≈ 2 and it will exponentially blow up! Our careful analysis ensures that even when L layers are stacked together, there is no exponential blow up in L.</p><p>The final lemma in this step proves that, as long as</p><formula xml:id="formula_22">x i − x j ≥ δ, then h i, − h j, ≥ Ω(δ) for each layer ∈ [L].</formula><p>This can be proved by a careful induction. Details are in Section A.</p><p>Step 2: stability after adversarial perturbation. We show that for every − → W that is "close" to initialization, meaning W − W (0) 2 ≤ ω for every and for some ω ≤ 1 poly(L) , then (a) the number of sign changes</p><formula xml:id="formula_23">D i, − D (0) i, 0 is at most O(mω 2/3 L), and (b) the perturbation amount h i, − h (0) i, ≤ O(ωL 5/2 ).</formula><p>We call this "forward stability", and it is the most technical proof of this paper. Remark. Intuitively, both "(a) implies (b)" and "(b) implies (a)" are not hard to prove. If the number of sign changes is bounded in all layers, then h i, and h (0) i, cannot be too far away by applying matrix concentration; and reversely, if h i, is not far from h (0) i, in all layers, then the number of sign changes per layer must be small. Unfortunately, one cannot apply such derivation with induction, because constants will blow up exponentially in the number of layers.</p><p>Remark. In the final proof, − → W is a point obtained by GD/SGD starting from − → W (0) , and thus − → W may depend on the randomness of − → W (0) . Since we cannot control how such randomness correlates, we argue for the above two properties against all possible − → W.</p><p>Another main result in this step is to show that the backward matrix BD i,L W L • • • D i,a W a does not change by more than O(ω 1/3 L 2 m/d) in spectral norm. Recall that in the Step 1 we shown that this matrix is of spectral norm O( m/d); thus as long as ω 1/3 L 2 1, this change is somewhat negligible. Details are in Section B.</p><p>Step 3: gradient bound. The hard part of Theorem 3 is to show gradient lower bound. For this purpose, recall from Fact 2.6 that each sample i ∈ [n] contributes to the full gradient matrix by D i, (Back i, +1 loss i )h i, −1 , where the backward matrix is applied to a loss vector loss i . To show this is large, intuitively, one wishes to show (Back i, +1 loss i ) and h i, −1 are both vectors with large Euclidean norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thanks to</head><p>Step 1 and 2, this is not hard for a single sample</p><formula xml:id="formula_24">i ∈ [n]. For instance, h (0) i, −1 ≈ 1 by Step 1 and we know h i, −1 − h (0) i, −1 ≤ o(1) from</formula><p>Step 2. One can also argue for Back i, +1 loss i but this is a bit harder. Indeed, when moving from random initialization − → W (0) to − → W, the loss vector loss i can change completely. Fortunately, loss i ∈ R d is a low-dimensional vector, so one can calculate Back i, +1 u for every fixed u and then apply ε-net.</p><p>Finally, how to combine the above argument with multiple samples i ∈ [n]? These matrices are clearly not independent and may (in principle) sum up to zero. To deal with this, we use h i, − h j,</p><p>≥ Ω(δ) from Step 1. In other words, even if the contribution matrix D i, (Back i, +1 loss i )h i, −1 with respect to one sample i is fixed, the contribution matrix with respect to other samples j ∈ [n] \ {i} are still sufficiently random. Thus, the final gradient matrix will still be large. This idea comes from the prior work <ref type="bibr" target="#b34">(Li &amp; Liang, 2018)</ref>, and helps us prove Theorem 3. Details in Appendix C and D.</p><p>Step 4: smoothness. In order to prove Theorem 4, one needs to argue, if we are currently at − → W and perturb it by − → W , then how much does the objective change in second and higher order terms. This is different from our stability theory in Step 2, because Step 2 is regarding having a perturbation on − → W (0) ; in contrast, in Theorem 4 we need a (small) perturbation − → W on top of − → W, which may already be a point perturbed from − → W 0) . Nevertheless, we still manage to show that, if hi, is calculated on</p><formula xml:id="formula_26">− → W and h i, is calculated on − → W + − → W , then h i, − hi, ≤ O(L 1.5 ) W 2 .</formula><p>This, along with other properties to prove, ensures semi-smoothness. This explains Theorem 4 and details are in Section E.</p><p>Remark. In other words, the amount of changes to each hidden layer (i.e., h i, − hi, ) is proportional to the amount of perturbation W 2 . This may sound familiar to some readers: a ReLU function is Lipschitz continuous |φ(a) − φ(b)| ≤ |a − b|, and composing Lipschitz functions still yield Lipschitz functions. What is perhaps surprising here is that this "composition" does not create exponential blowup in the Lipschitz continuity parameter, as long as the amount of over-parameterization is sufficient and − → W is close to initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Notable Extensions</head><p>Our Step 1 through Step 4 in Section 4 in fact give rise to a general plan for proving the training convergence of any neural network (at least with respect to the ReLU activation). Thus, it is expected that it can be generalized to many other settings. Not only we can have different number of neurons each layer, our theorems can be extended at least in the following three major directions. <ref type="foot" target="#foot_1">4</ref>Different loss functions. There is absolutely no need to restrict only to 2 regression loss. We prove in Appendix H that, for any Lipschitz-smooth loss function f : Theorem 5 (arbitrary loss). From random initialization, with probability at least 1 − e −Ω(log 2 m) , gradient descent with appropriate learning rate satisfy the following.</p><p>• If f is nonconvex but σ-gradient dominant (a.k.a.</p><p>Polyak-Łojasiewicz), GD finds ε-error minimizer in<ref type="foot" target="#foot_2">5</ref> </p><formula xml:id="formula_27">T = O poly(n,L) σδ 2 • log 1 ε iterations as long as m ≥ Ω poly(n, L, δ −1 ) • dσ −2 . • If f is convex, then GD finds ε-error minimizer in T = O poly(n,L) δ 2 • 1 ε iterations as long as m ≥ Ω poly(n, L, δ −1 ) • d log ε −1 . • If f is non-convex, then SGD finds a point with ∇f ≤ ε in at most 6 T = O poly(n,L) δ 2 • 1 ε 2 iterations as long as m ≥ Ω poly(n, L, δ −1 ) • dε −1 .</formula><p>• If f is cross-entropy for multi-label classification, then GD attains 100% training accuracy in at most<ref type="foot" target="#foot_4">7</ref> .</p><formula xml:id="formula_28">T = O poly(n,L) δ 2 iterations as long as m ≥ Ω poly(n, L, δ −1 ) • d .</formula><p>We remark here that the 2 loss is 1-gradient dominant so it falls into the above general Theorem 5. One can also derive similar bounds for (mini-batch) SGD so we do not repeat the statements here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional neural networks (CNN).</head><p>There are lots of different ways to design CNN and each of them may require somewhat different proofs. In Appendix I, we study the case when A, W 1 , . . . , W L−1 are convolutional while W L and B are fully connected. We assume for notational simplicity that each hidden layer has d points each with m channels. (In vision tasks, a point is a pixel). In the most general setting, these values d and m can vary across layers. We prove the following theorem:</p><p>Theorem 6 (CNN). As long as m ≥ Ω poly(n, L, d, δ −1 )• d , with high probability, GD and SGD find an ε-error solution for 2 regression in</p><formula xml:id="formula_29">T = O poly(n, L, d) δ 2 • log ε −1 iterations for CNN.</formula><p>Of course, one can replace 2 loss with other loss functions in Theorem 5 to get different types of convergence rates. We do not repeat them here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual neural networks (ResNet).</head><p>There are lots of different ways to design ResNet and each of them may require somewhat different proofs. In symbols, between two layers, one may study h = φ(h −1 + Wh −1 ), h = φ(h −1 + W 2 φ(W 1 h −1 )), or even h = φ(h −1 + W 3 φ(W 2 φ(W 1 h −1 ))). Since the main purpose here is to illustrate the generality of our techniques but not to attack each specific setting, in Appendix J, we choose to consider the simplest residual setting h = φ(h −1 + Wh −1 ) (that was also studied for instance by theoretical work <ref type="bibr" target="#b26">(Hardt &amp; Ma, 2017)</ref>). With appropriately chosen random initialization, we prove the following theorem:</p><p>Theorem 7 (ResNet). As long as m ≥ Ω poly(n, L, δ −1 )• d , with high probability, GD and SGD find an ε-error solution for 2 regression in</p><formula xml:id="formula_30">T = O poly(n, L) δ 2 • log ε −1 iterations for ResNet.</formula><p>Of course, one can replace 2 loss with other loss functions in Theorem 5 to get different types of convergence rates. We do not repeat them here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we demonstrate for state-of-the-art network architectures such as fully-connected neural networks, convolutional networks (CNN), or residual networks (ResNet), assuming there are n training samples without duplication, as long as the number of parameters is polynomial in n and L, first-order methods such as GD/SGD can find global optima of the training objective efficiently, that is, with running time only polynomially dependent on the total number of parameters of the network.</p><p>Figure <ref type="figure" target="#fig_2">1</ref> illustrates our main technical contribution. With the help of over-parameterization, near the GD/SGD training trajectory, there is no local minima and the objective is semi-smooth. The former means as long as the training objective is large, the objective gradient is also large. The latter means simply following the (opposite) gradient direction can sufficiently decrease the objective. They two together means GD/SGD finds global minima on overparameterized feedforward neural networks.</p><p>There are plenty of open directions following our work, especially how to extend our result to other types of deep learning tasks and/or proving generalization. There is already generalization theory <ref type="bibr" target="#b3">(Allen-Zhu et al., 2018a)</ref> for over-parameterized three-layer neural networks, so can we go any deeper?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label></label><figDesc>Without loss of generality, one can re-scale and assume xi ≤ 1/ √ 2 for every i ∈ [n]. Again, without loss of generality, one can pad each xi by an additional coordinate to ensure xi = 1/ √ 2. Finally, without loss of generality, one can pad each xi by an additional coordinate 1 √ 2 to ensure xi = 1. This last coordinate 1 √ 2 is equivalent to introducing a (random) bias term, because A( y √ 2 , 1 √ 2 ) = A √ 2 (y, 0)+b where b ∼ N (0, 1 m I). In our proofs, the specific constant 1 √ 2 does not matter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2 (1/ε) factor in the amount of over-parameterization m in Theorem 1. Since some readers have raised concerns regarding this Du et al. (2018a), we have now removed it at the expense of changing half a line of the proof. Theorem 2 (SGD). Suppose b ∈ [n] and m ≥ Ω poly(n,L,δ −1 )•d b . Starting from random initialization, with probability at least 1 − e −Ω(log 2 m) , SGD with learning rate η = Θ( bδd poly(n,L)m log 2 m ) and mini-batch size b finds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Landscapes of the CIFAR10 image-classification training objective F (W ) near points W = Wt on the SGD training trajectory. The x and y axes represent the gradient direction ∇F (Wt) and the most negatively curved direction of the Hessian after smoothing (approximately found by Oja's method (Allen-Zhu &amp; Li, 2017; 2018)). The z axis represents the objective value.</figDesc><graphic url="image-6.png" coords="6,380.45,247.35,136.73,78.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Remark 3 .</head><label>3</label><figDesc>Architecture is ResNet with 32 layers (for VGG19 or ResNet-110, see Figure 2 through 7 in appendix).Remark 4. The six plots correspond to epoch 5, 40, 90, 120, 130 and 160. We start with learning rate 0.1, and decrease it to 0.01 at epoch 81, and to 0.001 at epoch 122. SGD with momentum 0.9 is used. The training code is unchanged from (Yang, 2018) and we only write new code for plotting such landscapes. tally change the outputs and fit the training data, because weights are randomly initialized (per entry) at around 1 √ m</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">For instance, we have to establish a semi-smoothness theorem for deep ReLU networks (see Theorem 4). If instead the activation function is Lipscthiz smooth, and if one does not care about exponential blow up in the number of layers L, then the network is automatically 2 O(L) -Lipschitz smooth.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">In principle, each such proof may require a careful rewriting of the main body of this paper. We choose to sketch only the proof difference (in the appendix) in order to keep this paper short. If there is sufficient interest from the readers, we can consider adding the full proofs in the future revision of this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">Note that the loss function when combined with the neural network together f (Bhi,L) is not gradient dominant. Therefore, one cannot apply classical theory on gradient dominant functions to derive our same result.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">Again, this cannot be derived from classical theory of finding approximate saddle points for non-convex functions, because weights − → W with small ∇f (Bhi,L) is a very different (usually much harder) task comparing to having small gradient with respect to − → W for the entire composite function f (Bhi,L</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">). 7 This is because attaining constant objective error ε = 1/4 for the cross-entropy loss suffices to imply perfect training accuracy.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Alaeddini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mesbahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mesbahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06611</idno>
		<title level="m">Linear model regression on time-series data: Nonasymptotic error bounds and applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.01722" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neon2: Finding Local Minima via First-Order Oracles</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.06673" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Full version</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning and Generalization in Overparameterized Neural Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04918</idno>
		<imprint>
			<date type="published" when="2018-11">November 2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">Going Beyond Two Layers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the convergence rate of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12065</idno>
		<imprint>
			<date type="published" when="2018-10">October 2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A convergence analysis of gradient descent for deep linear neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02281</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards provable control for unknown linear dynamical systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient descent with identity initialization efficiently learns positive definite linear transformations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="520" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training a 3-node neural network is np-complete</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning: From theory to applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989. 1993</date>
			<biblScope unit="page" from="9" to="28" />
		</imprint>
	</monogr>
	<note>A preliminary version of this paper was</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Globally optimal gradient descent for a convnet with gaussian inputs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.07966" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A robust gradient sampling algorithm for nonsmooth, nonconvex optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Overton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="751" to="779" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complexity theoretic limitations on learning halfspaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-eighth annual ACM symposium on Theory of Computing (STOC)</title>
				<meeting>the forty-eighth annual ACM symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SGD learns the conjugate kernel class of the network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2422" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Complexity theoretic limitations on learning dnfs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="815" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the sample complexity of the linear quadratic regulator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Matni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01688</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Safely learning to control the constrained linear quadratic regulator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Matni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10121</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03804</idno>
		<title level="m">Gradient descent finds global minima of deep neural networks</title>
				<imprint>
			<date type="published" when="2018-11">November 2018a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient descent learns one-hidden-layer CNN: don&apos;t be afraid of spurious local minima</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1712.00779" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradient Descent Provably Optimizes Over-parameterized Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018c</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An overview of resnet and its variants</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fung</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning one-hidden-layer neural networks with landscape design</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.00501" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reliably learning the ReLU in polynomial time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning one convolutional layer with overlapping patches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02547</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Qualitatively characterizing neural network optimization problems</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>.-R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Identity matters in deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.04231" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient descent learns linear dynamical systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning linear dynamical systems via spectral filtering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6702" to="6712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral filtering for general linear dynamical systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NINPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cryptographic hardness for learning intersections of halfspaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Sherstov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convergence analysis of two-layer neural networks with ReLU activation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.09886" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the computational efficiency of training neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Livni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="855" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Manurangsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reichman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04207</idno>
		<idno>arXiv:1808.01181</idno>
		<title level="m">Marecek, J. and Tchrakian, T. Robust spectral filtering and anomaly detection</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The computational complexity of training ReLU(s)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Introductory Lectures on Convex Programming Volume: A Basic course, volume I</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
	<note>ISBN 1402075537</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01223</idno>
		<title level="m">Learning compact neural networks with regularization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Non-asymptotic identification of LTI systems from a single trajectory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ozay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05722</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convergence results for neural networks via electrodynamics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITCS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spurious local minima are common in two-layer ReLU neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1712.08968" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A variant of azuma&apos;s inequality for martingales with subgaussian tails</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<idno>prints, abs/1110.2392</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning without mixing: Towards a sharp analysis of linear system identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08334</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning ReLUs via gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<idno>CoRR, abs/1705.04591</idno>
		<ptr target="http://arxiv.org/abs/1705.04591" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On the complexity of learning neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5514" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">No bad local minima: Data independent training error guarantees for multilayer neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08361</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1703.00560" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://github.com/bearpaw/pytorch-classification.Ac-cessed" />
		<title level="m">Classification on CIFAR-10/100 and Ima-geNet with PyTorch</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2018" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning long term dependencies via Fourier recurrent units</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06585</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning nonoverlapping convolutional neural networks with multiple kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03440</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recovery guarantees for one-hidden-layer neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03175</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
