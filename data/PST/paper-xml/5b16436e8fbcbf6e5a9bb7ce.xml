<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Model for Category Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-03-23">March 23, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shanxi</region>
									<country>R.P China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Pan</surname></persName>
							<email>quanpan@nwpu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shanxi</region>
									<country>R.P China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85281</postCode>
									<settlement>Tempe, Arizona</settlement>
									<country>the United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
							<email>yangtao107@nwpu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<region>Shanxi</region>
									<country>R.P China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Generative</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Generative Model for Category Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-23">March 23, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">5D3736597093CB49E68F6ECAD4259F3F</idno>
					<idno type="DOI">10.1016/j.ins.2018.03.050</idno>
					<note type="submission">Received date: 30 September 2017 Revised date: 20 March 2018 Accepted date: 22 March 2018 Preprint submitted to Journal of Information Science</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Category Sentence Generation</term>
					<term>Generative Adversarial Networks</term>
					<term>Generative Models</term>
					<term>Supervised Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The neural network model has been the fulcrum of the so-called AI revolution.</p><p>Although very powerful for pattern-recognition tasks, however, the model has two main drawbacks: it tends to overfit when the training dataset is small, and it is unable to accurately capture category information when the class number is large. In this paper, we combine reinforcement learning, generative adversarial networks, and recurrent neural networks to build a new model, termed category sentence generative adversarial network (CS-GAN). Not only the proposed model is able to generate category sentences that enlarge the original dataset, but also it helps improve its generalization capability during supervised training. We evaluate the performance of CS-GAN for the task of sentiment analysis.</p><p>Quantitative evaluation exhibits the accuracy improvement in polarity detection on a small dataset with high category information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of many existing machine learning and data mining algorithms relies on large amounts of labeled data. For example, one important reason that convolutional neural networks (CNNs) have become so popular is the emergence of large-scale datasets such as ImageNet, which contains 14,197,122 manually-5 labeled images <ref type="bibr" target="#b32">[33]</ref>. The majority of existing classifiers cannot perform as expected when the size of the training dataset is small. Constructing a large labeled dataset, however, is time-consuming and sometimes it requires domain knowledge. Thus, there is a gap between the importance of having a large training dataset and the difficulty in obtaining such data. <ref type="bibr" target="#b9">10</ref> Generative models, which can generate realistic data samples, appear to be a promising tool for augmenting data size to bridge such a gap. The essential idea of generative models is to approximate the underlying data distribution by training a model to fit the training data. With the learned data distribution, generative models can generate observable data values. Thus, a massive amount 15 of labeled data can be generated by training a generative model from a small amount of labeled data, which can be used for training the classifiers. Various generative models have been proposed in the literature, such as latent Dirichlet distribution <ref type="bibr" target="#b2">[3]</ref>, restricted Boltzmann machines <ref type="bibr" target="#b12">[14]</ref>, and generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref>, which use the adversarial training idea for generating 20 more realistic data samples. Among the existing generative models, GANs are attracting increasing attention. The core idea of GAN is to play a min-max game between a discriminator and a generator, i.e., adversarial training. The discriminator tries to differentiate between real samples and artificial samples (constructed by the generator) while the generator tries to create realistic sam-25 ples that can fool the discriminator (i.e., make the discriminator believe that the generated samples are real). GANs have shown an extremely powerful ability to generate artificial images and facilitated many applications. For example, an image generator based on GANs can create super-resolution [20] images from their low-resolution counterparts and an interactive image generator [50] can</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>generate realistic images from some sketches or do the auto painting <ref type="bibr" target="#b22">[24]</ref> with the help of GAN.</p><p>Because of the astonishing power of GAN in generating realistic images, its adoption in the context of natural language processing (NLP) for generating sentences is attracting increasing attention <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b15">17]</ref>. For example, Zhang 35 et al. <ref type="bibr" target="#b48">[49]</ref> and Semeniuta et al. <ref type="bibr" target="#b35">[36]</ref> used GANs for text data generation and achieved state-of-the-art results. Dialogue-GAN proposed in <ref type="bibr" target="#b21">[23]</ref> demonstrated the ability of GAN to generate realistic dialogues. However, existing works on text generation mainly focus on generating unlabeled sentences, which are not helpful for data augmentation to train better classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>40</head><p>In this paper, we study the novel problem of generating labeled sentences with GAN for data augmentation. Because sentences are sequential data, recurrent neural networks (RNNs) are always used during generation. Also, the generator can be an agent whose target is to predict the next character based on current characters, which can be considered as a reinforcement learning (RL) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we illustrate related works for the models we use (namely: RNN, GAN, and RL) and for sentence generation and sentiment analysis (the focus and context of this paper, respectively). 70</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Recurrent Neural Networks</head><p>Because of its recurrent structure, RNN is good for sequence data processing.</p><p>There is no constraint for the sequence length when applying this model, and the hidden unit is updated at every time-step. One of the early RNN models was BiRNN <ref type="bibr" target="#b34">[35]</ref>, which splits the neurons of regular RNN into two directions: 75 one for the forward computation and one for backward computation. Today, the most successful RNN model is the long short-term memory (LSTM) <ref type="bibr" target="#b14">[16]</ref> network, where the gates in each neuron help the model predict the sequence data better based on contextual tokens. Many more models based on LSTM have been proposed recently, e.g., bidirectional LSTM [12], gated recurrent neu-80 ral tensor network (GRNTN) <ref type="bibr" target="#b41">[42]</ref> etc. These works not only help the sequence data generation but also make the model more flexible when faced with the variety of sequence data.</p><p>Many works employed LSTM for sentence generation, either directly <ref type="bibr" target="#b39">[40]</ref> or as an embedded module <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref>. Some works <ref type="bibr" target="#b8">[9]</ref> use LSTM for machine 85 translation on the basis of sentence generation, other works deploy this model for end-to-end speech recognition based on sequence generation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b37">38]</ref>. All of these models leverage the so-called teacher-forcing algorithm <ref type="bibr" target="#b44">[45]</ref>, which teaches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T the generation using near future tokens. This algorithm was later improved by Alex et al. <ref type="bibr" target="#b17">[19]</ref>, who introduced the professor-forcing algorithm, which out-90 performs teacher-forcing methods in text generation by using dynamic RNNs and by teaching the generator a large range of existing tokens. Prior information, e.g., sentence sentiment, can be added to this model during sequence data generation, which makes the generation more flexible. Some works added category information, which aids category sentence generation, e.g., <ref type="bibr" target="#b9">[10]</ref> com-95 bined conditional LSTM and category information for sentence classification in semi-supervised learning. Our work also employs LSTM and prior category information for sentence generation but our goal is to use the generated category sentence to improve the generalization of supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generative Adversarial Networks 100</head><p>Recently, GAN <ref type="bibr" target="#b10">[11]</ref> has become very popular in the context of computing vision. Many models are based on GAN to generate images from a predefined distribution. In such models, the discriminator has the goal to distinguish between artificial images (created by the generator) and real ones. The zero-sum game between the generator and discriminator helps improve their respective 105 abilities step by step. Because GAN is a bit unstable when training the network, some methods were proposed to avoid collapse during training, e.g., WGAN <ref type="bibr" target="#b0">[1]</ref>,</p><p>LossSensitiveGAN <ref type="bibr" target="#b31">[32]</ref>, Improved GAN <ref type="bibr" target="#b33">[34]</ref>. Some works proposed to integrate extra information into GAN, e.g., Info-GAN <ref type="bibr" target="#b7">[8]</ref>, Cat-GAN <ref type="bibr" target="#b38">[39]</ref>, and other works <ref type="bibr" target="#b38">[39]</ref> used GAN in semi-supervised learning, in which GAN generates sam-110 ples for training the classifier.</p><p>There are some obstacles in applying GAN to NLP <ref type="bibr" target="#b10">[11]</ref>, e.g., the discrete space of words that cannot be differentiated in mathematics. To this end, works like Seq-GAN <ref type="bibr" target="#b47">[48]</ref> and Dialogue-GAN <ref type="bibr" target="#b21">[23]</ref> applied RL for text generation by using softmax over continue values for character selection. Controllable text gen-115 eration <ref type="bibr" target="#b15">[17]</ref> applies the variable auto-encoder (VAE) together with controllable information to generate category sentences. Zhang et al. <ref type="bibr" target="#b48">[49]</ref> and Semeniuta et al. <ref type="bibr" target="#b35">[36]</ref> used GANs for text data generation and achieved state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Reinforcement Learning</head><p>It is natural to consider sentence generation as the decision-making process of 120 an agent that takes actions (next character selection) based on a goal-oriented policy aimed at achieving best long-term rewards. There are several models of RL <ref type="bibr" target="#b40">[41]</ref>, some of which were applied to sentence generation, e.g., actor-critic algorithm <ref type="bibr" target="#b1">[2]</ref> and deep q-network <ref type="bibr" target="#b11">[13]</ref>. In SeqGAN <ref type="bibr" target="#b47">[48]</ref>, in particular, the Monte Carlo method is used to search for next tokens. This is also applied to dialogue 125 generation <ref type="bibr" target="#b21">[23]</ref> and neural network decoder with specific properties <ref type="bibr" target="#b20">[22]</ref>.</p><p>Recently, researchers have been looking for relations between RL and GAN <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">15]</ref>. In particular, Ho et al. <ref type="bibr" target="#b13">[15]</ref> found a connection between GAN and inverse RL <ref type="bibr" target="#b27">[29]</ref>, believing that a transformation can be made between the generative adversarial imitation learning and reverse RL using the entropy-regularized term. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Sentence Generation</head><p>Sentence generation consists in producing natural language from a computational representation of information. There are some seminal works on generating good sentences using GANs (Figure <ref type="figure" target="#fig_3">1</ref>). Most of them consider sentence generation as a process of character prediction and use RNN for feature ex-135 traction from time series data <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>. There are also some works treating sentence generation as the encoder-decoder problem, which aim to minimize the loss between the source data and the target data. Recently, VAE achieved state-of-art results in sentence generation <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b35">36]</ref>. Because all of these models are used for generating the sentence directly, we 140 can put them into the same class which is represented by the spiny round with number one (Figure <ref type="figure" target="#fig_3">1</ref>). The information stream in those models is represented by the red arrow line nearby. Besides the generator (RNN, VAE, etc.), which produces synthetic sentences from the known distribution z, a discriminator is introduced to evaluate the generated data (and, hence, help the generator per-145 form better). A zero-sum game is played by these two roles, which improves the quality of the generation step by step under the framework of GAN <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b15">17]</ref>. Thus, models with generator and discriminator can be set as the second class Sentence generation can also be treated as a decision-making process, which is sorted as the third class, where an agent selects characters from a dataset based on a policy that leads to best long-term rewards <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b47">48]</ref>. This model 160 information stream is represented by the green arrow lines in Figure <ref type="figure" target="#fig_3">1</ref>. </p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Sentiment Analysis</head><p>In recent years, sentiment analysis has become increasingly popular for processing social media data on online communities, blogs, wikis, microblogging platforms, and other online collaborative media. Sentiment analysis is a branch 165 of affective computing research <ref type="bibr" target="#b30">[31]</ref> that aims to classify text -but sometimes also audio and video -into either positive or negative -but sometimes also neutral <ref type="bibr" target="#b5">[6]</ref>. Most of the literature is on English language but recently an increasing number of publications is tackling the multilinguality issue <ref type="bibr" target="#b23">[25]</ref>.</p><p>While most works approach it as a simple categorization problem, senti-170 ment analysis is actually a suitcase research problem <ref type="bibr" target="#b3">[4]</ref> that requires tackling many NLP tasks, including word polarity disambiguation <ref type="bibr" target="#b45">[46]</ref>, concept extraction <ref type="bibr" target="#b4">[5]</ref>, subjectivity detection <ref type="bibr" target="#b6">[7]</ref>, personality recognition <ref type="bibr" target="#b25">[27]</ref>, and aspect extraction <ref type="bibr" target="#b24">[26]</ref>.</p><p>Sentiment analysis has raised growing interest both within the scientific com-175 munity, leading to many exciting open challenges, as well as in the business world, due to the remarkable benefits to be had from marketing and financial forecasting <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>The common way to achieve data augmentation is to generate labeled sen-180 tences which can capture the true data distribution. In order to capture the feature of the existing data distribution, we divide the generation process in two steps: adding category information into the model and forcing the model to generate category sentences accordingly. In this section, we outline the basic sentence generation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>185</head><p>It is difficult to generate natural sentences when the dictionary volume is large; because of the large searching space, selecting the next token is timeconsuming and precision-compromising. To limit the action space (dictionary volume), the model is built at the character level. RNN, described in next</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula><p>section, is used as the basic sentence generator. Then, we describe how we 190 employed GAN and RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recurrent Neural Networks</head><p>The most common way for sentence generation is using RNN, which has achieved impressive results <ref type="bibr" target="#b39">[40]</ref>. All of these methods are teacher-forcing models <ref type="bibr" target="#b44">[45]</ref> and they predict the next token in a stream of text via supervised 195 learning. There are different compositions about the input and output numbers in RNN which enable it to be designed flexibly according to different applications. In this paper, we apply RNN as the character predictor with one output and sequence of input and make the best prediction of p(x t+1 |y t ), where x t+1</p><p>is the predicted character and y t is the current state. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative Adversarial Networks</head><p>To the best of our knowledge, there are very few works on text analysis using GANs. In this paper, we take advantage of GAN by applying RNN as the generator and the generated sentence is scored by the discriminator. A recent work <ref type="bibr" target="#b15">[17]</ref> uses VAE to solve the problem of the high variance in model 205 training, and add the controllable information in the VAE. Unlike that work, we encode the text stream during the last layer of the RNN and we generate the sentence from the original data directly. After generation, the real sentences and the generated sentences are fed into the discriminator separately. Similar to vanilla GAN models, it is a zero-game theory between the generator and the 210 discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reinforcement Learning</head><p>Inspired by the model of SeqGAN <ref type="bibr" target="#b47">[48]</ref>, sentence generation can also be regarded as a game-playing process, in which the agent chooses next character based on the current state to achieve long-term rewards while the discrimina-215 tor aims to achieve immediate rewards. The main challenge lies in the policy gradient updating which has to be performed after the sentence is generated,</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>because we can only get the reward from the discriminator who gives the score over the whole sentence. SeqGAN <ref type="bibr" target="#b47">[48]</ref> addresses this by using Monte Carlo searching with rollout technique to get the reward from the generated tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>220</head><p>The drawback of this method is that it is time-consuming. Because the action space is reduced, in this work the sentence generation time is shortened at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CS-GAN Model</head><p>In this section, the proposed model (CS-GAN) is introduced. Recently, 225 models like CAT-GAN <ref type="bibr" target="#b38">[39]</ref>, Info-GAN <ref type="bibr" target="#b7">[8]</ref> have been trying to integrate category information into the generated data. Some models join the label as the input <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">17]</ref>, some others regard the label as the target that needs to be predicted <ref type="bibr" target="#b7">[8]</ref>.</p><p>To make the sentence generation controllable, the label information is used as the input. Inspired by the work of Hu and Yang et al. <ref type="bibr" target="#b15">[17]</ref>, the controllable 230 information c and the sentence distribution z are concatenated together to be the prior information. In our framework, there are two parts which are generator and descriptors respectively playing the min-max game. As we have described before, RNNs and RL are applied in the generator. As to the descriptor which contains classifier and discriminator, and these two parts are for the labeled 235 synthetic sentences generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generator</head><p>To avoid gradient vanish, in this paper we use LSTM as a generator. We use a classifier to ensure that the generated sentence contains the label information.</p><p>As we have described earlier, the category information is added at each generating step. The prior category vector concatenates word embedding at each time-step, which is widely applied in <ref type="bibr" target="#b36">[37]</ref>. Together with the latent variable z, the generator that using the LSTM can be depicted by the following equations:</p><formula xml:id="formula_3">f t = σ(W f [x t ; z; c] + U f h t-1 + b f ) (1) A C C E P T E D M A N U S C R I P T i t = σ(W i [x t ; z; c] + U i h t-1 + b i )<label>(2)</label></formula><formula xml:id="formula_4">o t = σ(W o [x t ; z; c] + U o h t-1 + b o )<label>(3)</label></formula><formula xml:id="formula_5">c t = f t ⊗ c t-1 + i t ⊗ σ(W c [x t ; z; c] + U c h t-1 + b c )<label>(4)</label></formula><formula xml:id="formula_6">h t = o t ⊗ relu(c t )<label>(5)</label></formula><p>The above equations are the same as the ones of vanilla LSTM models, except 240 for the concatenation term of current word embedding x t , latent variable z, and the controllable information c.</p><p>Based on the generated tokens, the agent (the generator) takes action a (the token set) and then the descriptors (discriminator/classifier) return rewards about the current status. The structure of CS-GAN is shown in Figure <ref type="figure" target="#fig_5">2</ref>. with the case in SeqGAN <ref type="bibr" target="#b47">[48]</ref>, there is also no the immediate reward after the action. To address that problem, the rollout technique is deployed, and the generator parameter is G γ which could be same with the G θg during the rollout process. Then the rollout sentences are fed into the descriptors to get the rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>255</head><p>Generally, the generator tries to gain the maximum expect reward as follows:</p><formula xml:id="formula_7">J(θ g ) = E[R T |w 0 , c, θ g ] = dg∈W G θg ( dg |w 0 , c)Q G θg &lt;D θ d ,D θc &gt; (w 0 , dg ),<label>(6)</label></formula><p>where R T is the reward of the rollout sentence, w 0 is the start state, c is the controllable information, θ g , θ d , θ c are the parameters of the generator, discriminator and the classifier respectively, dg is the generated token, D θg is the probability of a sentence is real, D θ d is the probability of the sentence in the right category, and G θg is the generated results based on the latent variable (w 0 , c)</p><p>through RNN-LSTM which can be depicted as follows:</p><formula xml:id="formula_8">G θg ( dg |w 0 , c) = p G ( dg |w 0 , z, c) = t p( dt g |d &lt;t g , z, c),<label>(7)</label></formula><p>where current state w 0 = d &lt;t g is the generated tokens before t, Q G θg &lt;D θ d ,D θc &gt; (w 0 , dg ) is the action-value function during the selection process. Unlike SeqGAN, CS-GAN contains two roles in the status judgments, which help the generation to be more adaptive. Meanwhile, the model is applied at character level, which restrains the size of the action space to a limited number. To ensure the existence of the gradient in the sequence deciding process, softmax function is used when mapping the real values to the tokens, so we have</p><formula xml:id="formula_9">dt ∼ sof tmax(o t /τ ),<label>(8)</label></formula><p>where o t is the logistic vector from G θg ( dt</p><formula xml:id="formula_10">g , |w 0 , c)Q G θg</formula><p>&lt;D θ d ,D θc &gt; (w 0 , dg ), and τ is the temperature which is normally 1. As mentioned earlier, the reward comes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>from the discriminator and classifier simultaneously. To balance these two descriptors when estimating the action-value function, one way is to resemble F-measure:</p><formula xml:id="formula_11">Q G θg &lt;D θ d ,D θc &gt; (w 0 , dg ) = 2D θ d D θc D θ d + D θc<label>(9)</label></formula><p>where the last layer of the discriminator and the classifier is the softmax function, which normalizes these two output values equally.</p><p>According to Equation <ref type="formula" target="#formula_7">6</ref>, the update in the generator parameters with respect to θ g can be derived as:</p><formula xml:id="formula_12">θ g ← θ g + α t θg J(θ g ),<label>(10)</label></formula><p>where α t denotes the learning rate at step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Descriptors</head><p>For discriminator and classifier, CNNs are utilized. In particular, the structure of both classifier and discriminator are the same as in <ref type="bibr" target="#b16">[18]</ref>. The classifier not only distinguishes sentence categories, but also leads the category sentence generation. Thus, we have the following equations for the classifier optimization:</p><formula xml:id="formula_13">L C = Loss(&lt; C(d r ; θ c ), s r &gt;) + Loss(&lt; C(G(z; θ g ); θ c ), ϕ(c) &gt;)<label>(11)</label></formula><p>where C(•) is the logistic loss of the classifier, ϕ is a linear function of the 260 controllable label generation, θ c and θ g are the parameters of the classifier and the RNN model. The first term in the right hand side is the loss from the true sentence, and the second term is the loss from the generated sentence.</p><p>As to the discriminator, following the characteristic of GANs, the discriminator can be described as follows:</p><formula xml:id="formula_14">L D = H E dg ∼(c,z) + H E dr ∼data H E dg ∼(c,z) = E dg∼(c,z) [-log(1 -D(G(ϕ(c), z, θ g )), θ d )] H E dr ∼data = E dr∼data [-log(D(s r , d r , θ d ))]<label>(12)</label></formula><p>where d g and d r are the generated tokens and real tokens respectively, c is the controllable information, s r is the real data label, θ g and θ d are the parameters of As the descriptors are strong, they can distinguish the generated data easily, which leads to poor behavior of the generator. To balance the two descriptors and the generator, we have to pre-train a sound generator at first using an 270 RNN-based model. During the training, the uniform distribution is adopted to generate the first character, and then the sentence is generated token by token. This model is depicted in Algorithm 1. To avoid high discrimination results based on poor generation, the discriminator will stop updating when the discriminator accuracy Dis acc is higher than 0.9. Update parameters of generator in Equation <ref type="formula" target="#formula_12">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>end for 14:</p><p>for d-steps do 15:</p><p>if Dis acc &lt; 0.9 then 16:</p><p>Update the parameters of discriminator with the generated sentences and real sentences using Equation 12. 17:</p><formula xml:id="formula_15">end if 18:</formula><p>for classifier-epochs do 19:</p><p>Train the classifier with generated labeled sentences and the real labeled sentences using Equation <ref type="formula" target="#formula_13">11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>It is hard to make a good evaluation for text generation because there is no objective way to assess whether an artificial sentence is more plausible or realistic than another. To this end, some works like SeqGAN <ref type="bibr" target="#b47">[48]</ref> use BLEU score, which is designed to evaluate machine translation quality. Zhang et al. <ref type="bibr" target="#b48">[49]</ref> use</p><formula xml:id="formula_16">280</formula><p>Jensen-Shannon divergence between two distributions, while works in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b47">48]</ref> apply the negative log-likelihood (NLL) to measure the prediction. Because one task of CS-GAN is to generate text, NLL is used as the one measurement in this work. Next, to validate the proposed model for category sentence generation, sentiment analysis on a different dataset is performed, where the polarity 285 classification accuracy is the main measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We extract the sub-datasets from Amazon review dataset <ref type="bibr" target="#b26">[28]</ref>, Yelp review dataset 1 , Stanford sentiment tree bank dataset (SST) 2 , NEWS dataset that crawled from NYTimes, Reuters and USAToday <ref type="bibr" target="#b42">[43]</ref>, and Emotion dataset 3 290 which is provided by CrowdFlower 4 . To make comparisons and the evaluate the model over small datasets, a small sub-dataset is selected from those datasets, respectively (except for the sub-datasets from SST).</p><p>In the Amazon review dataset, there are two categories and the maximum length is no more than 120 characters, the small sub-dataset which is named  <ref type="table" target="#tab_2">3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Compared Methods</head><p>To validate the different parts contributing to category sentence generation, we unstack the components of CS-GAN by removing one item at the time.</p><p>In the descriptor, the CNN structure is taken from <ref type="bibr" target="#b16">[18]</ref>, which is a one-layer</p><formula xml:id="formula_17">A C C E P T E D M A N U S C R I P T</formula><p>convolution on top of the character vectors, and max-pooling is applied to get 315 a dense vector for the final fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">CS-GAN without RL</head><p>To test the contribution of RL, we remove it from CS-GAN to check how results change in absence of RL. Without RL, the generator deciding the next token mainly depends on the current status. All of the roles in CS-GAN are reminded, except for the process of Monte Carlo search and the updating policy, so the rule of updating the generator parameters changes as follows:</p><formula xml:id="formula_18">J(θ g ) = dg∈W G θg ( dg |w 0 , c)<label>(13)</label></formula><p>As in the original CS-GAN, the discriminator measures the generated sentence by grading the score to the whole sentence, while the classifier and the discriminator are all the descriptive roles whose functions are to map signals to 320 features.</p><p>Similarly, the classifier and the discriminator are trained according to Equation 11 and Equation <ref type="formula" target="#formula_14">12</ref>, respectively. This model is shown in Figure <ref type="figure">3</ref>(a) and the algorithm is depicted in Algorithm 2. for g-steps do 6:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E</head><p>Generate a sequence d 1:t ∼ G θg 7:</p><p>Update parameters of generator in Equation <ref type="formula" target="#formula_18">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>end for 9:</p><p>for d-steps do 10:</p><p>if Dis acc &lt; 0.9 then 11:</p><p>Update the parameters of discriminator with the generated sentences and real sentences using Equation <ref type="formula" target="#formula_14">12</ref>.</p><p>12:</p><formula xml:id="formula_19">end if 13:</formula><p>for classifier-epochs do 14:</p><p>Train the classifier with generated labeled sentences and the real labeled sentences using Equation <ref type="formula" target="#formula_13">11</ref>.  for g-steps do 6:</p><p>Generate a sequence d 1:t ∼ G θg 7:</p><p>Update parameters of generator in Equation <ref type="formula" target="#formula_18">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>end for 9:</p><p>for classifier-epochs do 10:</p><p>Train the classifier with generated labeled sentences and the real labeled sentences using Equation <ref type="formula" target="#formula_13">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>end for 12: until model convergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>330</head><p>The performance of CS-GAN in sentence generation is described in 5.3.1, CS-GAN's sentence feature capturing is illustrated in 5.3.2, and category sentence in supervised learning generalization is depicted 5.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Text Generation</head><p>To validate the proposed model in text generation, NLL is used as the mea-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Feature Representation</head><p>In order to examine how well the features of the generated sentences have been captured, feature consistency among the synthetic and real sentences is tested <ref type="bibr" target="#b48">[49]</ref>. We extract the features from the top layer of the classifier, which is represented as a 1024-dimensional vector f . Then, we use empirical expectation  <ref type="table" target="#tab_1">2</ref>. From Table <ref type="table" target="#tab_1">2</ref>, we can see that CS-GAN captures the category information.</p><p>Though the generated sentences are short, they are related to their label information. To qualitatively analyze the performance of the generalization with different generation numbers in supervised learning, the number of the generated sentence is set as follows N generated = βN labeled , where β is generation 370 ratio which is set to 0.025, 0.5, 1.0 respectively. Here N labeled denotes the number of labeled training sentences, and N generated is the number of the generated sentences. The evaluation results are depicted in Figure <ref type="figure" target="#fig_17">6</ref>.</p><p>From the figure, we can see that when β is small (i.e., when there are few generated labeled sentences in the training process), the classifier will overfit To further test the effectiveness of the model in supervised learning, the 380 datasets Amazon-5000, Amazon-30000, NEWS-15000, and Emotion-15000 are used, and the generation ratio β is set to 1.0. The classification results show in Table <ref type="table" target="#tab_2">3</ref>. Except for the first row (which is the supervised learning by CNN), all rows are the result of CS-GAN and the compared models that are related to CS-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>385</head><p>The table shows that in the smaller original labeled dataset with two categories, like Amazon-5000, CS-GAN achieves the best accuracy, then comes to CS-GAN without RL, the last one is CS-GAN without RL &amp; GAN. It is just reversed in the case of Amazon-30000, and the difference in those models is slight. From this point, we can see that CS-GAN performs well with a small 390 labeled dataset with little category information. When there is a large labeled dataset with little category information, the little category information will lead to a perfect discriminator, which decreases the generated sentence quality and, hence, produces a worse data augmentation. That is why CS-GAN does not perform well on Amazon-3000. In the multi-categories datasets, however, the model 395 performs better than the compared models in the large sub-datasets, which is reflected in the results of NEWS-15000 and Emotions-15000. In those cases, the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T large category information makes a less perfect discriminator, which improves the quality of the generated sentences and, hence, leads to a better data augmentation. Thus, when category information is little, CS-GAN performs well 400 with a small labeled dataset in supervised learning (which remedies the lack of the data), but when the original dataset is large, the strength of CS-GAN is weakened. In any case, the proposed model doubled the sentences number which helps the model achieve the best results when category information is high.</p><p>405 In the previous experiments, the maximum sentence length of the datasets is fixed. Therefore, to test the performance of the proposed model in different maximum length sentences, the datasets extracted from SST and Yelp on the basis of the sentence length are used. The results of the classification are shown in Figure <ref type="figure" target="#fig_19">7</ref>. We can see that, in general, the shorter the sentence, the better the 410 results. When sentence length increases, average accuracy is lower but the full CS-GAN always outperforms its downgraded versions in both datasets. This is mainly caused by the fact that the sentences generated by CS-GAN are short, as it can be seen in Table <ref type="table" target="#tab_1">2</ref>. This is a common sign in sequential generative models <ref type="bibr" target="#b19">[21]</ref>: the optimization for the likelihood objective function prefers giving  in supervised learning on the basis of the generated sentences, especially in the multi-categories datasets. However, this advantage can be weakened by a large amount of data when there is little category information (two classes). We also validated the proposed model on the task of sentiment analysis, where CS-GAN showed the best performance with different sentence lengths, especially for a 430 small labeled dataset with short sentence length.</p><p>In the future, we plan to further explore sentence feature extraction and model a disentangled representation based on different properties using the generative models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>45 process.•</head><label>45</label><figDesc>Hence, in this paper, an ensemble of RNNs and RL is applied. In particular, we aim to tackle two challenges: (1) generating realistic sentences with GAN, given the discrete nature of text; and (2) incorporating category information in GAN to generate labeled synthetic data. To this end, we propose a novel framework termed category sentence-generative adversarial networks 50 (CS-GAN), which not only can expand any given dataset by means of GANs and RL, but also can learn sentence structure directly with RNNs. Experiments show the effectiveness of the proposed model in the context of sentiment analysis, especially in the case of large category information. The main contributions of this work are as follows: 55 We study a new and important problem of labeled sentence generation, which can be used to help train better classifiers; • We propose a new framework CS-GAN, which stacks GAN, RNN and RL together for better sentence generation; • We conduct extensive experiments to demonstrate the proposed frame-60 A C C E P T E D M A N U S C R I P T work can generate more realistic sentences with labels. The remainder of the paper is organized as follows: Section 2 illustrates the literature of both models and tasks related to the research work; Section 3 introduces preliminaries of the proposed model; Section 4 describes CS-GAN in detail; Section 5 validates the effectiveness of the proposed model; finally, 65 Section 6 offers concluding remarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>130</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>in which the information stream is represented by the blue arrow lines nearby in Figure1. Despite the astonishing success of GANs in image generation, gen-150 erating sentences and documents using GANs is seldom studied and remains a challenging problem. The main difficulty of generating texts using GANs lies in the discrete nature of texts, which limits the differential propagation in GANs.Unlike image pixels (which are represented as the real number within a certain range), in fact, words or tokens of documents are discrete and are usually rep-155 resented as one-hot coding. This can be solved temporarily using the softmax function during token selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sentence generation models in previous works. The different color arrow lines stand for the information streams in different text generation models, and the spiny round with number denotes the class number. Spiny round 1 together with the red arrow stands for the first class, spiny round 2 together with blue arrows denotes the second class, and spiny round 3 together with the green arrows is the third class. w k is the tokens in the sentence, z is the prior distribution in GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>200</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structure of CS-GAN: c is the structured category information, z is the input distribution, dg is the output sentence from the generator, sr and dr are the real label and sentence respectively, the dash arrows stand for the constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>S C R I P Tthe generator and the discriminator, ϕ is the linear function, z is the distribution of the generated data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 W = d 1:t 1 : 2 : 3 : γ ← θ g 4 : 5 :: repeat 7 : 8 :</head><label>11234578</label><figDesc>The algorithm of CS-GAN Require: generator G θg ; roll-out G γ ;discriminator D θ d ; Classifier D θc ; Dataset Initialize the parameters of G θg , D θ d and D θc Use the positive samples W to pre-train the generator Generate the negative samples using G γ for training D θ d and D θc Pre-train discriminator and classifier 6for g-steps do Generate a sequence d 1:t ∼ G θg</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>γ ← θ g 23 :</head><label>23</label><figDesc>until model convergence A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>295</head><label></label><figDesc>Amazon-5000 contains 5000 training sentences, 2000 development sentences, and 2000 test sentences, and the large sub-dataset Amazon-30000 has 30000 training sentences, 10000 development sentences, and 10000 test sentences. As to the sub-dataset from the NEWS and Emotions, the maximum sentence length is 154 and 150 respectively, and NEWS-15000 together with Emotions-15000 have 300 1 http://yelp.com/dataset 2 http://nlp.stanford.edu/sentiment/treebank.html 3 http://data.world/crowdflower/sentiment-analysis-in-text 4 http://crowdflower.com A C C E P T E D M A N U S C R I P T the same size for the training set, development set and the test set. The suffixes of "Amazon", "NEWS" and "Emotions" represent the size of the training set. For the validation of the sentence length in sentiment analysis, we use the sub-datasets from Yelp and SST. Unlike the dataset mentioned before, we extract four sub-datasets by the sentence length from SST and Yelp respectively, 305 which are Yelp-200, Yelp-500, Yelp-800, Yelp-1000 and SST-120, SST-100, SST-80, SST-60 respectively (suffixes denoting the maximum length of a sentence at character level). The details of those sub-datasets are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>310</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Algorithm 2 1 : 2 : 3 :</head><label>2123</label><figDesc>The algorithm of CS-GAN without RL Require: generator G θg ;discriminator D θ d ; Classifier D θc ; Dataset W = d 1:t Initialize the parameters of G θg , D θ d and D θc Use real sentences W to pre-train the generator Pre-train the discriminator and classifier with real labeled sentences 4: repeat 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>17 :</head><label>17</label><figDesc>until model convergence 5.2.2. CS-GAN without RL &amp; GAN 325 Next, we remove GAN from CS-GAN without RL. This model is depicted in Figure 3(b), which can be treated as the RNN model. The left roles are the generator and the classifier, and the discriminator is removed along with GAN.The algorithm is depicted in Algorithm 3. The structure of CS-GAN without RL, where dg is the output sentence from the generator, sr and dr are the real label and sentence respectively. The dash arrows stand for the constraints. The structure of CS-GAN without RL &amp; GAN, where dg is the output tokens from the RNN, sr and dr are the real label and tokens respectively. The dash arrows stand for the constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 3 :Algorithm 3 1 : 2 : 3 :</head><label>33123</label><figDesc>Figure 3: The structures of the compared models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The results of NLL from model CS-GAN, CS-GAN without RL and CS-GAN without RL &amp; GAN in text data generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>340</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>345Figure 5 :</head><label>5</label><figDesc>Figure 5: Left: the scatter plot of feature expectations from real sentences against the synthetic sentences generated by CS-GAN, CS-GAN without RL and CS-GAN without RL &amp; GAN, respectively. Right: The scatter plot of feature covariances from the real sentences against the synthetic sentences that generated by CS-GAN, CS-GAN without RL and CS-GAN without RL &amp; GAN, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>375easily.Figure 6 :</head><label>6</label><figDesc>Figure 6: The results of the losses from CS-GAN using different numbers of generated data as training data, and β is the generation ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 7 :420</head><label>7</label><figDesc>Figure 7: The accuracy of the sentiment analysis in the sub-datasets of SST and Yelp using the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Alex Graves, Santiago Fernández, and Jürgen Schmidhuber. Bidirectional lstm networks for improved phoneme classification and recognition. Ar-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The details of the sub-datasets.</figDesc><table><row><cell>Dataset Name</cell><cell cols="2">Classes Train</cell><cell>Dev</cell><cell>Test</cell><cell>Max Len</cell></row><row><cell>Amazon-5000</cell><cell>2</cell><cell>5000</cell><cell>2000</cell><cell>2000</cell><cell>120</cell></row><row><cell>Amazon-30000</cell><cell>2</cell><cell cols="3">30000 10000 10000</cell><cell>120</cell></row><row><cell>NEWS-15000</cell><cell>7</cell><cell cols="2">15000 5000</cell><cell>5000</cell><cell>154</cell></row><row><cell>Emotion-15000</cell><cell>13</cell><cell cols="2">15000 5000</cell><cell>5000</cell><cell>150</cell></row><row><cell>Yelp-1000</cell><cell>5</cell><cell cols="2">15000 5000</cell><cell>5000</cell><cell>1000</cell></row><row><cell>Yelp-800</cell><cell>5</cell><cell cols="2">15000 5000</cell><cell>5000</cell><cell>800</cell></row><row><cell>Yelp-500</cell><cell>5</cell><cell cols="2">15000 5000</cell><cell>5000</cell><cell>500</cell></row><row><cell>Yelp-200</cell><cell>5</cell><cell cols="2">15000 5000</cell><cell>5000</cell><cell>200</cell></row><row><cell>SST-120</cell><cell>2</cell><cell>4440</cell><cell>553</cell><cell>1176</cell><cell>120</cell></row><row><cell>SST-100</cell><cell>2</cell><cell>3475</cell><cell>421</cell><cell>906</cell><cell>100</cell></row><row><cell>SST-80</cell><cell>2</cell><cell>2452</cell><cell>291</cell><cell>634</cell><cell>80</cell></row><row><cell>SST-60</cell><cell>2</cell><cell>1556</cell><cell>167</cell><cell>397</cell><cell>60</cell></row><row><cell cols="6">All of these experiments are running on the GTX-TITAN graphics card</cell></row><row><cell cols="4">which is equipped with a 12GB GPU memory.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The examples of the generated sentences.</figDesc><table><row><cell></cell><cell>POSITIVE</cell><cell>NEGATIVE</cell></row><row><cell>SST</cell><cell>This is really lead movie .</cell><cell>As film and day waste .</cell></row><row><cell></cell><cell>You take the one better .</cell><cell>This movie is mess .</cell></row><row><cell></cell><cell>LOVE</cell><cell>EMPTY</cell></row><row><cell></cell><cell>love the warm</cell><cell>is telling a story?</cell></row><row><cell></cell><cell>Oh thank you my dear</cell><cell>read the book boring</cell></row><row><cell></cell><cell>RELIEF</cell><cell>ANGER</cell></row><row><cell></cell><cell>read the book</cell><cell>I hate the hat</cell></row><row><cell></cell><cell>Thanks</cell><cell>jest the boy</cell></row><row><cell></cell><cell>SURPRISE</cell><cell>NEUTRAL</cell></row><row><cell>Emotion</cell><cell>weather today is a gift to us</cell><cell>Have a tea please.</cell></row><row><cell></cell><cell>I hate that happend</cell><cell>so the heat is here</cell></row><row><cell></cell><cell>HAPPINESS</cell><cell>SADNESS</cell></row><row><cell></cell><cell>Next Page</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The classification results on the different size dataset.</figDesc><table><row><cell>Model Name</cell><cell>Amazon-5000</cell><cell>Amazon-30000</cell></row><row><cell>CNN</cell><cell>84.83%</cell><cell>89.55%</cell></row><row><cell>CS-GAN w/o RL&amp;GAN</cell><cell>85.60%</cell><cell>89.67%</cell></row><row><cell>CS-GAN w/o RL</cell><cell>86.18%</cell><cell>89.54%</cell></row><row><cell>CS-GAN</cell><cell>86.43%</cell><cell>89.34%</cell></row><row><cell>Model Name</cell><cell cols="2">Emotion-15000 NEWS-15000</cell></row><row><cell>CNN</cell><cell>40.75%</cell><cell>72.08%</cell></row><row><cell>CS-GAN w/o RL&amp;GAN</cell><cell>39.32%</cell><cell>72.31%</cell></row><row><cell>CS-GAN w/o RL</cell><cell>40.14%</cell><cell>72.09%</cell></row><row><cell>CS-GAN</cell><cell>41.52%</cell><cell>74.33%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>surement over the data of Amazon-5000. Results are shown in Figure4.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Young Scientists Fund of the National Natural Science Foundation of China [Grant No.61402373] and Aeronautical Science Foundation of China Key Laboratory Project [Grant No.20155553036].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>the "safe" response when predicting the phrase. Hence, it is hard for those variants to get a good sentence generation when faced with a long sentence, and that is why the classification results decrease in all of the three variants with the increase of sentence length. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">445</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentiment analysis is a big suitcase</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SenticNet 5: Discovering conceptual primitives for sentiment analysis by means of context embeddings</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distinguishing between facts and opinions for sentiment analysis: Survey and 455 challenges</title>
		<author>
			<persName><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Welsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian network based extreme learning machine for subjectivity detection</title>
		<author>
			<persName><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edoardo</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolfo</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1780" to="1797" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and 460 Pieter Abbeel</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning 465 phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Warde-470</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">tificial Neural Networks: Formal Models and Their Applications-ICANN 2005</title>
		<imprint>
			<date type="published" when="2005">2014. 2005</date>
			<biblScope unit="page" from="753" to="753" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.09202</idno>
		<title level="m">Generating text with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann ma-480 chines</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Momentum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">926</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neu-485 ral computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00955</idno>
		<title level="m">Controllable text generation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-490 guage Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Lan-490 guage Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Alex M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alias Parth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio ; Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016. 2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
	<note>Professor forcing: A new algorithm for training recurrent networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A 500 diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to decode for future success</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06549</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">505 Adversarial learning for neural dialogue generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06547</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Auto-painter: Cartoon image generation from sketch by using conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01908</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: From formal to informal and scarce resource languages</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Siaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimond</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chiong</surname></persName>
		</author>
		<author>
			<persName><surname>Cornforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="527" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive 515 LSTM</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Alexander Gelbukh, and Erik Cambria. Deep learning-based document modeling for personality detection from text</title>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: 520 understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Connecting generative adversarial networks and actor-critic methods</title>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01945</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06264</idno>
		<title level="m">Loss-sensitive generative adversarial networks on lipschitz densities</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>International 535</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural net-540 works</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldip K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02390</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A hierarchical latent 550 variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Springenberg</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gated recurrent neural tensor network</title>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirna</forename><surname>Adriani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Classification of short 565 texts by deploying topical annotations</title>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Vitale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="376" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<title level="m">Stefan Ultes, and Steve Young. A networkbased end-to-end trainable task-oriented dialogue system. arXiv preprint 570</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Word polarity 575 disambiguation using bayesian model and opinion-level features</title>
		<author>
			<persName><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="380" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Natural language based financial forecasting: A survey</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Welsch</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-017-9588-9</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Seqgan: sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05473</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Jun-Yan Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
	<note>NIPS workshop on Adversarial Training</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
