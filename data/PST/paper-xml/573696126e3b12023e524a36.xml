<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PrivTree: A Differentially Private Algorithm for Hierarchical Decompositions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
							<email>xkxiao@ntu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PrivTree: A Differentially Private Algorithm for Hierarchical Decompositions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">09590765B3F4F068D20BFE3CD7EF06C6</idno>
					<idno type="DOI">10.1145/2882903.2882928</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Differential privacy; hierarchical decompositions</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a set D of tuples defined on a domain Ω, we study differentially private algorithms for constructing a histogram over Ω to approximate the tuple distribution in D. Existing solutions for the problem mostly adopt a hierarchical decomposition approach, which recursively splits Ω into sub-domains and computes a noisy tuple count for each sub-domain, until all noisy counts are below a certain threshold. This approach, however, requires that we (i) impose a limit h on the recursion depth in the splitting of Ω and (ii) set the noise in each count to be proportional to h. The choice of h is a serious dilemma: a small h makes the resulting histogram too coarse-grained, while a large h leads to excessive noise in the tuple counts used in deciding whether sub-domains should be split. Furthermore, h cannot be directly tuned based on D; otherwise, the choice of h itself reveals private information and violates differential privacy.</p><p>To remedy the deficiency of existing solutions, we present PrivTree, a histogram construction algorithm that adopts hierarchical decomposition but completely eliminates the dependency on a pre-defined h. The core of PrivTree is a novel mechanism that (i) exploits a new analysis on the Laplace distribution and (ii) enables us to use only a constant amount of noise in deciding whether a sub-domain should be split, without worrying about the recursion depth of splitting. We demonstrate the application of PrivTree in modelling spatial data, and show that it can be extended to handle sequence data (where the decision in sub-domain splitting is not based on tuple counts but a more sophisticated measure). Our experiments on a variety of real datasets show that PrivTree considerably outperforms the states of the art in terms of data utility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Releasing sensitive data while preserving privacy is a problem that has attracted considerable attention in recent years. The stateof-the-art paradigm for addressing the problem is differential privacy <ref type="bibr" target="#b16">[16]</ref>, which requires that the data released reveals little infor-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD <ref type="bibr">'16</ref>, June 26-July 01, 2016, San Francisco, CA, USA c 2016 ACM. ISBN 978-1-4503-3531-7/16/06. . . $15.00 DOI: http://dx.doi.org/10.1145/2882903.2882928 mation about whether any particular individual is present or absent from the data. To fulfill such a requirement, a typical approach adopted by the existing solutions is to publish a noisy version of the data in place of the original one.</p><p>In this paper, we consider a fundamental problem that is frequently encountered in differentially private data publishing: Given a set D of tuples defined over a domain Ω, we aim to decompose Ω into a set S of sub-domains and publish a noisy count of the tuples contained in each sub-domain, such that S and the noisy counts approximate the tuple distribution in D as accurately as possible. Applications of the problem include:</p><p>• Private modelling of spatial data <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b41">41]</ref> often requires generating a multi-dimensional histogram of the input data.</p><p>• For differentially private data mining (e.g., k-means <ref type="bibr" target="#b48">[48]</ref> and regression analysis <ref type="bibr" target="#b29">[29]</ref>), one of the general approaches is to first coarsen the input data and inject noise into it, and then use the modified data to derive mining results.</p><p>• Existing algorithms for sequence data publishing <ref type="bibr" target="#b7">[7]</ref> require identifying frequent patterns (e.g., prefixes) in a given set D of sequences. This is equivalent to asking for a decomposition of the sequence domain Ω into a set of disjoint sub-domains, such that (i) each sub-domain includes all sequences in D containing a particular pattern, and (ii) the number of sequences included in each sub-domain is larger than a given threshold.</p><p>To address the above decomposition problem, the prior art mostly adopts a hierarchical approach, which (i) recursively splits Ω into sub-domains and computes a noisy tuple count for each of them, and (ii) stops splitting a sub-domain when its noisy count is smaller than a threshold. This approach, albeit intuitive, requires a pre-defined limit h on the maximum depth of recursion when splitting Ω. The reason is that, to ensure differential privacy, the amount of noise injected in each tuple count has to be proportional to the maximum recursion depth, and hence, h must be fixed in advance so that the algorithm can decide the correct noise amount to use.</p><p>Nevertheless, the choice of h is a serious dilemma: for the algorithm to produce fine-grained sub-domains of Ω, h cannot be small; yet, increasing h would lead to noisier tuple counts, and thus more errors in deciding whether a sub-domain should be split. As a consequence, no choice of h could result in an accurate approximation of the input data. Furthermore, we cannot tune h directly on the input dataset; otherwise, the choice of h itself reveals private information and violates differential privacy. To mitigate these issues, existing work relies on heuristics to select an appropriate value of h, and to generate fine-grained decompositions even when h is small. As we show in our experiments, however, those heuris-tics are rather ineffective when the input data follows a skewed distribution (which is often the case in practice).</p><p>Contributions. Motivated by the limitations of existing solutions, we present PrivTree, an algorithm for the decomposition problem that adopts the hierarchical approach but completely eliminates the dependency on a pre-defined h. In particular, PrivTree requires only a constant amount of noise in deciding whether a sub-domain should be split, which enables it to generate fine-grained decompositions without worrying about the recursion depth. Such a surprising improvement is obtained with a novel mechanism for differential privacy that exploits a non-trivial analysis on the Laplace noise <ref type="bibr" target="#b17">[17]</ref> to derive an extremely tight privacy bound. Its central insight is that, in the context of hierarchical decomposition, it is possible to publish a sequence S of 0/1 values using O(1) noise, regardless of the sensitivity of S <ref type="bibr" target="#b17">[17]</ref>. In contrast, the standard Laplace mechanism <ref type="bibr" target="#b17">[17]</ref> requires that noise amount must be proportional to S's sensitivity.</p><p>To demonstrate the applications of PrivTree, we apply it to the private modeling of spatial data, and present a non-trivial extension to tackle sequence data, for which we adopt an advanced Markov model and utilize a sophisticated measure (instead of tuple counts) to decide whether a sub-domain should be split. We experimentally evaluate our algorithms on a variety of real data, and show that they considerably outperform the states of the art in terms of data utility.</p><p>In addition, we present an in-depth analysis on the connection between PrivTree and the support vector technique (SVT) <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28]</ref>, a technique widely adopted for data publishing under differential privacy. We show that there exists a variant of SVT <ref type="bibr" target="#b28">[28]</ref> that could have been used to implement PrivTree, if its privacy guarantees are as claimed in previous work <ref type="bibr" target="#b28">[28]</ref>. Nevertheless, we prove that the SVT variant <ref type="bibr" target="#b28">[28]</ref> does not satisfy differential privacy, which makes it inapplicable in our context.</p><p>In summary, we make the following contributions in this paper:</p><p>1. We propose PrivTree, a differentially private algorithm for hierarchical decomposition that eliminates the dependency on a pre-defined threshold of the recursion depth.</p><p>2. We present applications of PrivTree in modeling spatial and sequence data. (Sections 3 and 4)</p><p>3. We analyze the connection between PrivTree and the SVT, and point out a misclaim about the latter in <ref type="bibr" target="#b28">[28]</ref>. (Section 5)</p><p>4. We conduct extensive experiments to demonstrate the superiority of PrivTree over the states of the art. (Section 6)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>In this section, we introduce the concepts behind differential privacy <ref type="bibr" target="#b16">[16]</ref>, and define the problem of spatial decomposition <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b46">46]</ref>, which we will address with our PrivTree algorithm in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Differential Privacy</head><p>Let D be a sensitive dataset with n tuples, and A be a data publishing algorithm that takes D as input and releases a set of information A(D). Differential privacy requires that A(D) should be insensitive to the presence or absence of any particular tuple in D, so that an adversary cannot infer much private information from A(D). More formally, differential privacy is defined based on the concept of neighboring datasets, as shown in the following. DEFINITION 2.1 (NEIGHBORING DATASETS <ref type="bibr" target="#b16">[16]</ref>). Two datasets are neighboring if one of them can be obtained by inserting a tuple into the other. DEFINITION 2.2 (ε-DIFFERENTIAL PRIVACY <ref type="bibr" target="#b16">[16]</ref>). An algorithm A satisfies ε-differential privacy if, for any two neighboring datasets D and D and for any possible output O of A,</p><formula xml:id="formula_0">ln Pr [A(D) = O] Pr [A(D ) = O] ≤ ε,</formula><p>where Pr[•] denotes the probability of an event.</p><p>There exist several mechanisms <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b38">38]</ref> for achieving differential privacy, among which the most fundamental one is the Laplace mechanism. Specifically, the Laplace mechanism considers a function f that takes D as input and outputs a vector of real numbers, and it aims to release f (D) with differential privacy. To achieve this objective, it adds i.i.d. noise into each value in f (D), such that the noise η follows a Laplace distribution with the following probability density function:</p><formula xml:id="formula_1">Pr[η = x] = 1 2λ e -|x|/λ . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>We denote the above distribution as Lap(λ), and refer to λ as the scale (since the standard deviation of Lap(λ) is proportional to λ). Dwork et al. <ref type="bibr" target="#b17">[17]</ref> prove that the Laplace mechanism achieves (S(f )/λ)-differential privacy, where S(f ) is the sensitivity of f defined as follows:</p><p>DEFINITION 2.3 (SENSITIVITY <ref type="bibr" target="#b17">[17]</ref>). Let f be a function that maps a dataset D into a vector of real numbers. The global sensitivity of f is defined as</p><formula xml:id="formula_3">S(f ) = max D,D f (D) -f (D ) 1 ,</formula><p>where D and D are any two neighboring datasets, and • 1 denotes the L1 norm.</p><p>Intuitively, S(f ) measures the maximum possible change in f 's output when we insert or remove one arbitrary tuple in f 's input.</p><p>An important property of differential private algorithms is that their composition also ensures differential privacy: LEMMA 2.1 (COMPOSITION RULE <ref type="bibr" target="#b37">[37]</ref>). Let A1, . . . , A k be k algorithms, such that Ai satisfies εi-differential privacy (i ∈ <ref type="bibr">[1, k]</ref>). Then, the sequential composition (A1, . . . , At) satisfies ( k i=1 εi)-differential privacy. This lemma is particularly useful in proving that an algorithm ensures differential privacy: we can first decompose the algorithm into a few sequential components, and then analyze each component separately; after that, we can apply Lemma 2.1 to establish the overall privacy guarantee of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatial Decompositions</head><p>Let D be a set of data points in a multi-dimensional space Ω. A spatial decomposition <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b46">46]</ref> of D consists of a tree-structured decomposition of Ω into its sub-domains, along with a partitioning of the data points among the leaves of the decomposition tree. For example, Figure <ref type="figure" target="#fig_0">1</ref> illustrates a spatial decomposition of a twodimensional dataset D that contains 12 data points. The decomposition tree has 9 nodes, namely, v1, v2, . . . , v9, each of which is associated with a sub-domain of Ω (denoted as "dom" and visualized as a black rectangle in Figure <ref type="figure" target="#fig_0">1</ref>). We refer to each sub-domain as a region. The root of the tree, v1, corresponds to a region that covers the entire Ω; this region is recursively divided into four equal-size sub-regions in the lower levels of the tree, until each leaf node contains a sufficiently small number of data points.</p><p>The spatial decomposition in Figure <ref type="figure" target="#fig_0">1</ref> is referred to as a quadtree <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b46">46]</ref>, and is widely adopted in spatial databases for efficient query processing. In particular, suppose that we are to use the quadtree to answer range count queries, i.e., queries that ask for the number of data points contained in a rectangle q. In that case, we can pre-compute, for each node v in the quadtree, the number of data points contained in v's region. Then, we can answer any range count query q with a top-down traversal from the root node of the quadtree. Specifically, at the beginning of the traversal, we initialize the query answer as ans = 0. After that, for each node v that we traverse, we examine v's region dom(v), and differentiate four cases:</p><p>1. If dom(v) is disjoint from q, we ignore v;</p><p>2. If dom(v) is fully contained in q, we increase ans by the point count pre-computed for v;</p><p>3. If dom(v) partially intersects q and v is not a leaf node, then we visit every child of v with a region not disjoint from q;</p><p>4. If dom(v) partially intersects q and v is a leaf node, then we inspect the data points in dom(v), and add to ans the number of points contained in q.</p><p>After the traversal terminates, we return ans as the result. For instance, consider a range count query q that corresponds to the dashed-line rectangle in Figure <ref type="figure" target="#fig_0">1</ref>. To answer q, we only need to examine four nodes, namely, v1, v4, v5, v9; the other nodes are all ignored since their regions are disjoint from q.</p><p>The efficiency of quadtrees results from its adaptiveness to the underlying distribution, i.e., it grows deep into the dense regions of Ω where there are a large number of data points (e.g., the region of v4 in Figure <ref type="figure" target="#fig_0">1</ref>), and it ignores those regions that are sparse (e.g., the regions of v2, v3, v5). Such adaptiveness has motivated existing work <ref type="bibr" target="#b12">[12]</ref> to utilize quadtrees for generating private synopses of spatial data. Specifically, the technique in <ref type="bibr" target="#b12">[12]</ref> first applies a differentially private algorithm to generate a quadtree, and then employs the Laplace mechanism to inject noise into the point count of each node. The quadtree and the noisy counts can then be used to answer any range-count query q using the top-down traversal algorithm mentioned above, with two minor modifications. First, whenever we visit a node v whose region is fully contained in q, we add the noisy count associated with v (instead of the exact count) to the query answer ans. Second, if v is a leaf node whose region dom(v) partially intersects q, then we multiply the noisy count of v by |q ∩ dom(v)| |dom(v)| before adding it to ans, where |•| denotes the area of a region. That is, given only the noisy count of v, we estimate the number of data points in dom(v) that are contained in q, by assuming that the points follow a uniform distribution. The rationale of this approach is that, given the adaptiveness of the quadtree, each leaf node v should cover a region where the data distribution is not highly skewed; otherwise, v should contain a dense sub-region, in which case the quadtree construction algorithm should have further split v (instead of making v a leaf node). This makes it relatively accurate to adopt a uniform assumption when estimating the contribution of v to the answer of q. In Section 3, we will present a more detailed analysis of the above quadtree approach, and then use it to motivate our PrivTree algorithm. the privacy risk of a node v (see Equation ( <ref type="formula" target="#formula_10">5</ref>))</p><formula xml:id="formula_4">ρ (v)</formula><p>an upper bound of ρ (see Equation ( <ref type="formula" target="#formula_16">7</ref>)) β the fanout of the spatial decomposition tree δ the decaying factor used by PrivTree I the set of distinct items in a given set D of sequences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRIVATE SPATIAL DECOMPOSITIONS</head><p>This section presents our solution for constructing private spatial decompositions. We first revisit the private quadtree approach (in Section 2.2) and discuss its limitations; after that, we elaborate our PrivTree algorithm, analyze its guarantees, and discuss its extensions. Table <ref type="table" target="#tab_0">1</ref> shows the notations that we frequently use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Private Quadtrees Revisited</head><p>Algorithm 1 presents a generic version of the private quadtree approach mentioned in Section 2.2. The algorithm takes as input four parameters: (i) a set D of spatial points defined over a multidimensional domain Ω, (ii) the scale λ of the Laplace noise to be used in the construction of the quadtree, (iii) the threshold θ used to decide whether a quadtree node should be split, and (iv) the threshold h on the maximum height of the decomposition tree. The output of the algorithm is a quadtree T where each node v comes with two pieces of information: the sub-domain of Ω corresponding to v (denoted as dom(v)), and a noisy version of the point count in dom(v) (denoted as ĉ(v)). We define the depth of v as the hop distance between v and the root of T , and denote it as depth(v).</p><p>The algorithm starts by creating the root note v1 of T , after which it sets dom(v1) = Ω and marks v1 as unvisited (Lines 1-2). The subsequent part of the algorithm consists of a number of iterations (Lines 3-9). In each iteration, we examine if there is an unvisited node v in T . If such v exists, we mark v as visited, and employ the Laplace mechanism to generate a noisy version ĉ(v) of the number of points contained in dom(v) (Lines 4-6). After that, we split v if the following two conditions simultaneously hold. First, ĉ(v) &gt; θ, i.e., dom(v) is likely to contain a sufficiently large number of points. Second, the height of the tree is smaller than h, which, as we discuss shortly, ensures that the noisy counts generated by the algorithm would not violate differential privacy. If both of the above conditions are met, then we generate v's children and insert them into T as unvisited nodes (Lines 7-9); otherwise, v becomes a leaf node of T . When all of the nodes in T become visited, the algorithm terminates and returns T .</p><p>Algorithm 1: SimpleTree (D, λ, θ, h) 1 initialize a quadtree T with a root node v 1 ; 2 set dom(v 1 ) = Ω, and mark v 1 as unvisited; 3 while there exists an unvisited node v do Privacy and Utility Analysis. Algorithm 1 ensures ε-differential privacy if λ ≥ h/ε. To understand this, suppose that we insert an arbitrary point t into D. Then, T has only h nodes whose exact point counts are affected by the insertion of t, i.e., the h nodes whose sub-domains contain t. In addition, the point count of those nodes should change by one after t's insertion. This indicates that the sensitivity (see Definition 2.3) of all point counts in T equals h, and hence, adding i.i.d. Laplace noise of scale λ ≥ h/ε into the counts would achieve ε-differential privacy.</p><p>As we mention in Section 1, however, requiring λ ≥ h/ε makes it rather difficult for Algorithm 1 to generate high-quality quadtrees. Specifically, if we set h to a small value, the resulting quadtree T would not adapt well to the data distribution in D, due to the restriction on the tree height; meanwhile, increasing h would also increase the amount of noise in each ĉ(v), which makes Algorithm 1 more error-prone in deciding whether a node should be split, thus degrading the quality of T . In other words, any choice of h inevitably leads to inferior data utility. Furthermore, we cannot directly tune h by (i) testing the performance of Algorithm 1 on D under different settings of h, and then (ii) selecting the one that yields the best result. The reason is that such a tuning process violates differential privacy: when we change the input data from D to a neighboring dataset D , the tuning process may select a different h, in which case Algorithm 1 would use different noise scales for D and D , invalidating its privacy guarantee.</p><p>To alleviate the above issue, existing work <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b48">48]</ref> resorts to heuristics to choose h without violating differential privacy, and to enhance the performance of Algorithm 1, e.g., by avoiding the generation of noisy counts for certain levels of the decomposition tree (so that h can be reduced), and by exploiting correlations among the noisy counts to improve their accuracy <ref type="bibr" target="#b25">[25]</ref>. However, none of those heuristics is able to thoroughly address the limitations of Algorithm 1. As we shown in our experiments in Section 6, existing approaches tend to provide inferior data utility, especially when the input data follows a skewed distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rationale Behind Our Solution</head><p>To remedy the deficiency of Algorithm 1, we aim to eliminate the requirement that λ ≥ h/ε, and make λ a constant instead. This would not only resolve the dilemma in choosing h, but also unleash the potential of quadtrees as the tree height is no longer restricted. Towards this end, we first make a simple observation: after we finish constructing the quadtree T in Algorithm 1, we could remove all noisy counts associated with the intermediate nodes, and release only the noisy counts for the leaf nodes as well as the sub-domains of all nodes. The released tree, denoted as T , could still be used for query processing, since we can re-generate an alternative count for each intermediate node v in T by summing up the published noisy counts of the leaf nodes under v. Intuitively, T reveals less information than T does, and hence, we might use less noise in T to achieve ε-differential privacy.</p><p>However, the above intuition does not hold in general, as T and T require the same amount of noise to enforce the same privacy guarantee. To explain, consider two neighboring datasets D and D , such that D is obtained by inserting a point t into D . Let v1, v2, . . . , v h be the h nodes in T whose sub-domains contain t. Then, these h nodes should form a path from the root of T to a leaf node. Furthermore, for any vi, the point count of vi is decreased by one when we change the input dataset from D to D . We use c(vi) to denote vi's exact point count on D.</p><p>Without loss of generality, assume that v h is a leaf node (i.e., v1, . . . , v h-1 are all intermediate nodes). Let Pr[D → T ] (resp. Pr[D → T ]) denote the probability that we obtain T from D (resp. D ) given fixed λ, θ, and h. Then,</p><formula xml:id="formula_5">ln Pr[D → T ] Pr[D → T ] = h-1 i=1 ln Pr[c(v i ) + Lap(λ) &gt; θ] Pr[c(v i ) -1 + Lap(λ) &gt; θ] + ln Pr[c(v h ) + Lap(λ) = ĉ(v h )] Pr[c(v h ) -1 + Lap(λ) = ĉ(v h )]</formula><p>.</p><p>By Equation ( <ref type="formula" target="#formula_1">1</ref>), for any</p><formula xml:id="formula_6">c(v h ) &lt; ĉ(v h ), ln Pr[c(v h ) + Lap(λ) = ĉ(v h )] Pr[c(v h ) -1 + Lap(λ) = ĉ(v h )] = 1 λ . (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>In addition, for any vi</p><formula xml:id="formula_8">(i ∈ [1, h -1]) with c(vi) ≤ θ, ln Pr[c(v i ) + Lap(λ) &gt; θ] Pr[c(v i ) -1 + Lap(λ) &gt; θ] = 1 λ .<label>(3)</label></formula><p>Therefore, when c(vi) ≤ θ holds for every vi</p><formula xml:id="formula_9">(i ∈ [1, h -1]), ln Pr[D → T ] Pr[D → T ] = h λ .<label>(4)</label></formula><p>By Definition 2.2, this indicates that λ must be at least h/ε to ensure that T achieves ε-differential privacy. In summary, T is no better than T in terms of the amount of noise required, because of the negative result in Equations ( <ref type="formula" target="#formula_6">2</ref>) and (3). In other words, in the worst case, releasing the boolean result of c(vi) + Lap(λ) &gt; θ incurs the same privacy cost as releasing c(vi) + Lap(λ) directly. That said, if c(vi) &gt; θ for some vi, then Equation (3) does not hold, in which case T could entail a smaller privacy cost than T does. To illustrate this, we denote the l.h.s. of Equation (3) as a function ρ of c(vi), i.e.,</p><formula xml:id="formula_10">ρ(x) = ln Pr[x + Lap(λ) &gt; θ] Pr[x -1 + Lap(λ) &gt; θ] ,<label>(5)</label></formula><p>and we plot ρ in Figure <ref type="figure" target="#fig_2">2</ref>. (Note that the y-axis of Figure <ref type="figure" target="#fig_2">2</ref> is in a logarithmic scale.) Observe that, when</p><formula xml:id="formula_11">x = c(v) ≥ θ + 1, ρ(x)</formula><p>decreases exponentially with the increase of x. This indicates that ln</p><formula xml:id="formula_12">Pr[D→T ] Pr[D →T ] could be much smaller than h/λ, if c(vi) ≥ θ + 1 holds for all i ∈ [1, h -1]. For example, if c h-1 ≥ θ + 1 and c(vi) -c(vi+1) is at least a constant for all i ∈ [1, h -2], then h-1 i=1 ρ c(v i ) = Θ 1 λ , (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>due to the exponential decrease of ρ(vi). In that case, we have</p><formula xml:id="formula_14">ln Pr[D→T ] Pr[D →T ] = Θ (1/λ) instead of ln Pr[D→T ] Pr[D →T ]</formula><p>= h/λ, which would enable us to set λ as a constant independent of h.</p><p>The above analysis leads to an interesting question: can we ensure that Equation ( <ref type="formula" target="#formula_12">6</ref>) holds for any input dataset? In Section 3.3, we will give an affirmative answer to this question. The basic idea of our method is to add a bias term to each c(vi), so that c(vi)c(vi+1</p><formula xml:id="formula_15">) (i ∈ [1, h -2]</formula><p>) is larger than a constant of choice. In addition, the bias term is independent of the input data, which guarantees that its usage does not leak any private information. The derivation of the bias term requires a careful analysis of ρ(x). To simplify our analysis, we devise a simple upper bound of ρ(x):</p><formula xml:id="formula_16">LEMMA 3.1. Let ρ be a function such that ρ (x) = 1/λ, if x &lt; θ + 1 1 λ exp θ+1-x λ , otherwise<label>(7)</label></formula><p>Then, ρ(x) ≤ ρ (x) for any x.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows ρ with a dashed line. Observe that it closely captures the exponential decrease of ρ when c(v) ≥ θ + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The PrivTree Algorithm</head><p>Algorithm 2 presents our PrivTree technique for private spatial decomposition. As with Algorithm 1, PrivTree asks for a spatial dataset D, the scale λ of Laplace noise to be used, and a threshold θ for deciding whether a node should be split. However, it does not request a threshold h on the maximum tree height; instead, it requires a positive number δ, the usage of which will be clarified shortly. The output of PrivTree is a quadtree T , with the point count associated with each node removed. That is, T reveals the subdomain of each node v, but conceals all information about c(v). In Section 3.4, we will explain how we obtain the point count of each node, as well as our choices of θ and δ.</p><p>In a nutshell, PrivTree is similar to Algorithm 1 in that it also (i) generates T by recursively splitting a root node v1 whose region dom(v1) covers the whole data space Ω, and (ii) decides whether a node v should be split based on a noisy point count of v. However, the method for obtaining noisy counts marks the crucial difference between the two algorithms. Specifically, given a node v, PrivTree does not generate its noisy count by directly adding Laplace noise to c(v). Instead, PrivTree first computes a biased count b</p><formula xml:id="formula_17">(v) = c(v) -depth(v) • δ, and checks if it is smaller than θ -δ; if it is, then PrivTree increases it to θ -δ. In other words, b(v) = max θ -δ, c(v) -depth(v) • δ . (<label>8</label></formula><formula xml:id="formula_18">)</formula><p>After that, PrivTree produces a noisy count b(v) = b(v) + Lap(λ), and splits v if b(v) is larger than the given threshold θ. Notice that PrivTree does not restrict the height of T , as the decision to split any node v solely depends on b(v).</p><p>Privacy Analysis. Consider any quadtree T output by PrivTree, and any two neighboring datasets D and D , such that D is obtained by inserting a point t into D . In what follows, we show that setting</p><formula xml:id="formula_19">λ = Θ(1/ε) is sufficient for ε-differential privacy, i.e., -ε ≤ ln Pr[D → T ] Pr[D → T ] ≤ ε. (<label>9</label></formula><formula xml:id="formula_20">)</formula><p>The proof for the first inequality in Equation ( <ref type="formula" target="#formula_19">9</ref> </p><formula xml:id="formula_21">(v) = c(v) -depth(v) • δ; 6 adjust b(v) if it is excessively small: b(v) = max {b(v), θ -δ}; 7 compute a noisy version of b(v): b(v) = b(v) + Lap(λ); 8 if b(v) &gt; θ then 9</formula><p>split v, and add its children to T ;</p><p>10 mark the children of v as unvisited;</p><p>11 return T with all point counts removed t. Note that those nodes should form a path from the root of T to a leaf. Let k be the length of the path, and vi be i-th node in the path, with v1 denoting the root of T . We have c(vi</p><formula xml:id="formula_22">) = c (vi) + 1 and b(vi) = b (vi) + 1, if b(vi) ≥ θ -δ + 1 b (vi), otherwise<label>(10)</label></formula><p>Then, by Equation ( <ref type="formula" target="#formula_1">1</ref>),</p><formula xml:id="formula_23">ln Pr[D → T ] Pr[D → T ] = k-1 i=1 ln Pr[b(v i ) + Lap(λ) &gt; θ] Pr[b (v i ) + Lap(λ) &gt; θ] + ln Pr[b(v k ) + Lap(λ) ≤ θ] Pr[b (v k ) + Lap(λ) ≤ θ] ≥ 0 - 1 λ = - 1 λ .</formula><p>This indicates that λ ≥ 1/ε ensures the first inequality in Equation <ref type="bibr">(9)</ref>. Next, we prove the second inequality in Equation ( <ref type="formula" target="#formula_19">9</ref>) by analyz-</p><formula xml:id="formula_24">ing ln Pr[b(v i )+Lap(λ)&gt;θ] Pr[b (v i )+Lap(λ)&gt;θ]</formula><p>, which we refer to as the privacy cost of vi. The high-level idea of our proof is as follows. First, due to the way that we generate biased counts, each node vi's bias count b(vi) is at least a constant δ smaller than that of its parent vi-1, as long as b(vi) ≥ θ + 1. Based on this observation and Lemma 3.1, we show that all nodes vi with b(vi) ≥ θ + 1 incur a total privacy cost of Θ(1/ε). After that, we prove that the total privacy cost of the remaining nodes is also Θ(1/ε).</p><p>By the definition of v1, . . . , v k , we have c(vi) ≥ c(vi+1) and <ref type="bibr" target="#b8">(8)</ref>. Without loss of generality, assume that there exists</p><formula xml:id="formula_25">depth(vi) = depth(vi+1) -1 for any i ∈ [1, k -1]. This indicates that b(vi) ≥ b(vi+1) ≥ θ -δ, due to Equation</formula><formula xml:id="formula_26">m ∈ [1, k -1], such that b(vm) ≥ θ -δ + 1 and b(vm+1) = θ -δ. Then, b(vi-1) ≥ b(vi) + δ ≥ θ + 1, if i ∈ [2, m] b(vi) = θ -δ, otherwise<label>(11)</label></formula><p>Combining Equations ( <ref type="formula" target="#formula_22">10</ref>) and ( <ref type="formula" target="#formula_26">11</ref>), we have b</p><formula xml:id="formula_27">(vi) = b(vi) when i &gt; m, and b (vi) = b(vi) -1 otherwise. Therefore, ln Pr[D → T ] Pr[D → T ] = k-1 i=1 ln Pr[b(v i ) + Lap(λ) &gt; θ] Pr[b (v i ) + Lap(λ) &gt; θ] + ln Pr[b(v k ) + Lap(λ) ≤ θ] Pr[b (v k ) + Lap(λ) ≤ θ] ≤ k-1 i=1 ln Pr[b(v i ) + Lap(λ) &gt; θ] Pr[b (v i ) + Lap(λ) &gt; θ] = m i=1 ln Pr[b(v i ) + Lap(λ) &gt; θ] Pr[b(v i ) -1 + Lap(λ) &gt; θ] = m i=1 ρ b(v i ) ,</formula><p>where ρ(•) is as defined in Equation <ref type="bibr" target="#b5">(5)</ref>. By Lemma 3.1 and Equation <ref type="bibr" target="#b11">(11)</ref>,</p><formula xml:id="formula_28">m i=1 ρ b(v i ) ≤ m i=1 ρ b(v i ) = ρ b(vm) + m-1 i=1 1 λ exp θ + 1 -b(v i ) λ ≤ 1 λ + 1 λ • 1 1 -exp(-δ/λ) = 1 λ • 2e δ/λ -1 e δ/λ -1 .</formula><p>Therefore, if we set δ = γ • λ, where γ is a constant, then</p><formula xml:id="formula_29">ln Pr[D → T ] Pr[D → T ] = m i=1 ρ b(v i ) ≤ 1 λ • 2e γ -1 e γ -1 = Θ 1 λ .</formula><p>Summing up the above analysis, we have the following theorem:</p><formula xml:id="formula_30">THEOREM 3.1. PrivTree satisfies ε-differential privacy if λ ≥ 2e γ -1 e γ -1 • 1 ε and δ = γ • λ for some γ &gt; 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Noisy Counts and Parameterization</head><p>Generation of Noisy Counts. Recall that PrivTree outputs a quadtree with the point count for each node removed. However, if a quadtree with noisy counts is needed, we can easily obtain it by adding a postprocessing step to PrivTree. In particular, given a dataset D, we first invoke PrivTree to produce a ε 2 -differentially private quadtree T . After that, for each leaf node v of T , we publish a noisy version of v's point count using Laplace noise of scale 2/ε. It can be verified that this postprocessing step satisfies ε 2differential privacy. Then, by Lemma 2.1, the generation of T and the noisy counts as a whole achieves ε-differential privacy. Finally, we compute a noisy count for each intermediate node v in T , by taking the sum of the noisy counts of all leaf nodes under v.</p><p>Choice of δ. As shown in Theorem 3.1, when δ = γ • λ, PrivTree needs to use a noise scale λ ≥ 2e γ -1 e γ -1 • 1 ε to achieve ε-differential privacy. Intuitively, the choice of δ is a balancing act between the amount of bias and the amount of noise in each biased noisy count b(v) used by PrivTree. In particular, if δ is small with respect to λ, then the bias term in each b(v) is small, but the noise amount in b(v) would need to be large, since 2e γ -1 e γ -1 • 1 ε increases when γ = δ/λ decreases. In contrast, if δ is large with respect to λ, then each b(v) would have small noise but a large bias.</p><p>That said, we observe that there is a more important factor in choosing δ. To explain, consider a node v with a biased count b(v) = θδ. Ideally, we would like PrivTree to avoid splitting such a node v, as its point count is likely to be small. Nevertheless, if b(v) + Lap(λ) &gt; θ, then PrivTree would split v and insert its children into the quadtree. In turn, each of v's children also has a certain probability to be split, and so on. If δ is excessively small, then each offspring of v has a relatively large splitting probability, in which case the splitting process may not converge, i.e., PrivTree may keep generating offsprings of v and does not terminate.</p><p>To address the above issue, we set δ in such a way to ensure that if b(v) = θδ, then in expectation, only 2 nodes would be generated from the subtree under v (including v itself). Specifically, we set δ = λ • ln β, where β denote the fanout of T , i.e., the number of children that each intermediate node in T has. (For example, β = 4 if T is a two-dimensional quadtree.) By Equation <ref type="bibr" target="#b1">(1)</ref>, this setting of δ guarantees that any node v with b(v) = θδ has 1 2β probability to be split. Formally, we have the following lemma: LEMMA 3.2. Let T be the output of PrivTree given δ = λ•ln β, and T * be the output of PrivTree when it sets b(v) = c(v) for each node v (i.e., no noise or bias is introduced in the split decisions). Then,</p><formula xml:id="formula_31">E[|T |] ≤ 2 • |T * | whenever |T * | &gt; 1, where | • | denotes the number of nodes in a tree.</formula><p>The above setting of δ leads to the following corollary:</p><formula xml:id="formula_32">COROLLARY 1. PrivTree satisfies ε-differential privacy if λ ≥ 2β-1 β-1 • 1 ε and δ = λ • ln β, where β is the fanout of T .</formula><p>Choice of θ. Intuitively, the threshold θ serves the purpose of ensuring that each leaf node v of T contains a sufficiently large point count c(v), so that if we choose to output a noisy version of c(v), it would not overwhelmed by the Laplace noise injected. The choice of θ, however, is complicated by the fact that PrivTree adds a negative bias to the point count of a node when it decides whether or not to split the node. In particular, due to the negative bias, even θ = 0 could ensure that a node v with b(v) &gt; θ has a sufficient large point count. Therefore, we use θ = 0 in our implementation of PrivTree, and we observe that it leads to reasonably good results in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Extensions</head><p>Although we have presented PrivTree in the context of spatial decomposition, we note that it could be extended in several different aspects for other applications. First, the decomposition tree used by PrivTree does not have to be a quadtree, but can be any other tree structure instead. For example, suppose that we are given a multi-dimensional dataset D containing both numeric and categorical attributes, and that each categorical attribute has a taxonomy. Then, we can still apply PrivTree on D to generate a private synopsis of D, by splitting each numeric dimension of D according to a binary tree and each categorical dimension based on its taxonomy.</p><p>Second, when PrivTree decides whether or not a node v should be split, the decision does not have to be based on the count of tuples contained in dom(v), but can also be based on any other score function μ(v) that is monotonic, i.e., μ(v) ≤ μ(u) whenever v is a child node of another node u. The rationale is that, as long as μ is monotonic, we can add a bias to the score of each node v to ensure that it is at least a constant smaller that the score of v's parent. Then, we can apply Lemma 3.1 to show that PrivTree guarantees differential privacy, given that λ is properly set based on the sensitivity of the score function μ. In Section 4, we will apply this idea to extend PrivTree for private modeling of sequence data.</p><p>Finally, although the privacy analysis of PrivTree (in Section 3.3) assumes that the presence or absence of a tuple t only affects one leaf node and its ancestors, it can be extended to the case when multiples leaf nodes and their ancestors are impacted. In particular, if at most x leaf nodes can be affected, then we can apply PrivTree with the noise scale λ enlarged x times. The intuition is that each affected leaf node, along its ancestors, incurs one unit of privacy cost, which in turn requires one unit of noise to mitigate; as such, when there are x affected leaf nodes, we need x units of noise for sufficient privacy protection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PRIVATE MARKOV MODELS</head><p>This section presents an extension of PrivTree for constructing Markov models on sequence data. We first introduce the basic concepts of sequences and Markov models in Section 4.1. After that, we elaborate our PrivTree extension in Section 4.2, and compare it with existing solutions in Section 4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sequence Data and Markov Models</head><p>Given a finite alphabet I, a sequence s of length l over I is an</p><formula xml:id="formula_33">ordered list x1x2 • • • x l , where each xi (i ∈ [1, l]) is a symbol in I.</formula><p>For convenience, we abuse notation and write</p><formula xml:id="formula_34">s = $x1x2 • • • x l &amp;,</formula><p>where $ and &amp; are two special symbols that mark the beginning and the end of a sequence, respectively. Sequences are frequently used to represent user behavioral data, such as trajectories, web navigation traces, and product purchasing histories.</p><p>Markov models are a type of stochastic models commonly used to characterize sequence data. They assume the Markov property <ref type="bibr" target="#b44">[44]</ref>, i.e., a symbol x in a sequence s is decided by a few symbols that immediately proceeds x in s, but not any others. A Markov model over D is often represented as prediction suffix tree (PST) <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b44">44]</ref>, where each node v is associated with a predictor string dom(v), as well as a prediction histogram hist(v). In particular, dom(v) consists of symbols in I ∪ {$}, while hist(v) contains a count for each symbol x in I ∪ {&amp;}. The count, denoted as hist(v)[x], is computed as follows. We first inspect all occurrences of dom(v) in the sequences in D, and count the number y of occurrences when dom(v) is immediately followed by the symbol x; after that, we set hist(v)[x] = y. In other words, hist(v)[x] indicates how often an appearance of dom(v) would immediately lead to an appearance of x in a sequence.</p><p>For example, Figure <ref type="figure" target="#fig_4">3</ref> illustrates a PST constructed over a set D containing four sequences s1, s2, s3, s4, over an alphabet I = {A, B}. The node v6 has a predictor string dom(v6) = AA, and prediction histogram hist(v6) that contains a count for each element in {A, B, &amp;}. The counts in the histogram sum up to 3, since the string AA appears 3 times in D, i.e., once in s3 and twice in s4. In addition, hist(v6)[A] = 1 because, among the 3 occurrences of AA, only one is immediately followed by A (i.e., the first occurrence of AA in s4). The root node v1 has an empty predictor string dom(v1) = ∅, and its prediction histogram counts the occurrences of each individual symbol in I ∪ {&amp;}, e.g., hist(v1)[A] = 6 since the symbol A appears 6 times in total in D.</p><p>The nodes in a PST are organized in such a way that each node v has |I| + 1 children. Furthermore, for each child v of v, dom(v ) is obtained by adding a symbol in I ∪ {$} in the beginning of dom(v). That is, dom(v) is a suffix of dom(v ). For example, in the PST in Figure <ref type="figure" target="#fig_4">3</ref>, v3 is a parent of v5; accordingly, dom(v5) = $A is obtained by adding the symbol $ to the beginning of dom(v3) = A. The intuition here is that (i) each node v in a PST provides a way to predict the "next symbol" in a sequence based on a "predicate" dom(v), and (ii) when we split v, each child node would have a longer "predicate" that provides a more specific predication.</p><p>A PST T can be used to support a wide range of queries, such as estimating the number of times that a query string sq appears in the sequences in D. Specifically, given sq = x1x2 . . . x l , we first inspect the root node v1's prediction histogram hist(v1), and then initialize a temporary answer ans = hist(v1) <ref type="bibr">[x1]</ref>. After that, we examine xi (i ∈ [2, l]) in ascending order of i. For each xi, we consider the length-(i -1) prefix of sq, i.e., s * i = x1x2 . . . xi-1. We identify the node v in T whose predictor string is the longest suffix of s * i . Then, we compute the sum of the counts in v's prediction histogram hist(v), referred to as the magnitude of the histogram and denoted as hist(v) 1. After that, we set</p><formula xml:id="formula_35">ans = ans • hist(v)[xi] hist(v) 1 ,<label>(12)</label></formula><p>i.e., we multiple ans by the probability that the "next symbol" equals xi, as predicted by hist(v). When all xi (i ∈ <ref type="bibr">[1, l]</ref>) are examined, we return ans as the query answer.</p><p>For example, consider a query sequence sq = AB on the PST in Figure <ref type="figure" target="#fig_4">3</ref>. We first visit the root node v1, and initialize ans = hist(v1)[A] = 6. After that, we consider the length-1 prefix of sq, i.e., s * 2 = A. We identify v3 as the node whose predictor string is the longest suffix of s * 2 , and we set ans = ans</p><formula xml:id="formula_36">• hist(v 3 )[B]</formula><p>hist(v 3 ) 1 = 3. Finally, we return ans = 3 as the answer.</p><p>In addition to the aforementioned query type, we can also utilize a PST T to generate a synthetic sequence dataset, by sampling sequences from T one by one. Specifically, to generate a sequence, we start from an initial sequence s0 = $ and insert symbols into s0 iteratively. In the i-th iteration (i ≥ 1), we inspect the sequence si-1, and identify the node v in T whose predictor string is the longest suffix of si-1. Then, we sample a symbol xi from the symbol distribution represented by hist(v), i.e.,</p><formula xml:id="formula_37">Pr[xi = x] = hist(v)[x] hist(v) 1 .</formula><p>After that, we insert xi to the end of si-1, and denote the resulting sequence as si. If xi happens to be &amp;, then we return si as the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extension of PrivTree</head><p>To construct a PST T on a sequence dataset D, we can start from a root node v1 with a predictor string dom(v1) = ∅, and then recursively split v1. This motivates us to adopt PrivTree for the generation of differentially private PSTs. However, we can no longer use a node v's noisy count c(v) to decide whether v should be split, since c(v) is undefined on a PST. Instead, as discussed in Section 3.5, we can redefine c(v) as a score function that measures the suitability of v for splitting. In the non-private setting, existing work <ref type="bibr" target="#b44">[44]</ref> typically avoids splitting a node v if any of the following conditions is satisfied: C1. dom(v) starts with $. In this case, no more symbol can be added to the beginning of dom(v); thus, v cannot be split.</p><p>C2. The magnitude of hist(v) is small. The rationale is that, when hist(v) 1 is small, further splitting v results in child nodes v whose prediction histograms hist(v ) have even smaller magnitudes. In that case, the symbol distribution captured by hist(v ) is obtained from a tiny sample set of sequences, which leads to poor prediction accuracy.</p><p>C3. The entropy<ref type="foot" target="#foot_0">1</ref> of hist(v) is small. This is because when hist(v) has a small entropy, there is little uncertainty in the symbol prediction given by hist(v); as such, there is little benefit in splitting v. (See v4 in Figure <ref type="figure" target="#fig_4">3</ref> for an example.)</p><p>Suppose that we are to adopt the above conditions into PrivTree.</p><p>Condition C1 can be straightforwardly applied, since it only depends on dom(v) and does not rely on D, i.e., it does not leak private information. In contrast, conditions C2 and C3 cannot be directly adopted since the counts in hist(v) depend on D. To address this issue, we aim to design a score function c(•) for PrivTree with the following two properties:</p><p>P1. c(•) is monotonic, i.e., c(v) ≤ c(u) for any node v and its parent u. This, as discussed in Section 3.5, is required to ensure that PrivTree satisfies differential privacy.</p><p>P2. If a node v's prediction histogram has a small magnitude or a small entropy, then c(v) tends to be small. This is motivated by conditions C2 and C3 mentioned above.</p><p>Our construction of c(•) is based on the following observation: if a prediction histogram has a small entropy, it often has one symbol count that dominates the others, because a small entropy implies that the distribution of symbols in the histogram is skewed. (v4 in Figure <ref type="figure" target="#fig_4">3</ref> shows an example.) Motivated by this, we define c(v) as</p><formula xml:id="formula_38">c(v) = hist(v) 1 -max x∈I∪{&amp;} hist(v)[x],<label>(13)</label></formula><p>i.e., c(v) equals the magnitude of hist(v) minus the largest count in hist(v). The intuition is that if the magnitude of hist(v) is small, then c(v) must be small, regardless of the largest count in hist(v); on the other hand, if the entropy of hist(v) is small, then the largest count in hist(v) tends to be close to the magnitude of hist(v) (since the count often dominates all other counts in hist(v)), which results in a small c(v) as well. Thus, c(v) fulfills property P2. The following lemma show that c(v) also satisfies property P1.</p><p>LEMMA 4.1. c(•) is a monotonic function.</p><p>In summary, we can construct a private PST on a sequence dataset D using PrivTree (i.e., Algorithm 2), with three minor changes. First, in Line 1 of Algorithm 2, T is a PST with a fanout |I|+1 (instead of a quadtree), and v1 is a PST node with a predictor string dom(v1) = ∅ and a prediction histogram hist(v1). Second, in Line 5, c(v) is as defined in Equation <ref type="bibr" target="#b13">(13)</ref>. Third, in Line 11, we return T after removing the biased score b(v) and the prediction histogram hist(v) of each node v.</p><p>After we obtained the PST T (without prediction histograms) from the modified PrivTree, we can postprocess T to recover the prediction histograms. Specifically, for each leaf node v in T , we derive the prediction histogram hist(v) from D, and then compute a noisy version of hist(v), denoted as hist(v), by adding Laplace noise into each histogram count. After that, for any non-leaf node v , we construct a noisy prediction hist(v ), such that for any symbol x ∈ I ∪ {&amp;},</p><formula xml:id="formula_39">hist(v )[x] = v is a leaf node under v hist(v)[x].</formula><p>Finally, if any noisy histogram in T has a negative count, we reset the count to zero. This is to ensure that each histogram represents a distribution of symbols.</p><p>Privacy Analysis and Parameterization. To analyze the privacy guarantee of the modified PrivTree, we first introduce an assumption that is also adopted in prior work <ref type="bibr" target="#b6">[6]</ref> on sequence data publication under differential privacy: we assume that the length of each sequence in D, when taking into account &amp; but not $, is at most l , where l is a known constant. To explain why this assumption is needed, consider that we insert an infinite sequence s into D to obtain a neighboring dataset D . In that case, the insertion of s incurs unbounded changes in the histogram counts of the PST, which makes it impossible to achieve differential privacy. In general, if l is unknown, we may choose an appropriate l and truncate any sequences s that is excessively long<ref type="foot" target="#foot_1">2</ref> . Specifically, if s = $x1x2 . . . x l &amp;, then we truncate it to s = $x1x2 . . . x l , i.e., s becomes an open-ended sequence. Note that the removal of &amp; from s does not affect the construction of the PST.</p><p>Given the above assumption, we prove the privacy guarantee of the modified PrivTree as follows. β-1 • l ε and δ = λ • ln β. In addition, we prove that the postprocessing of PrivTree's output (i.e., adding Laplace noise to the histogram counts of the leaf nodes) also achieves ε-differential privacy. THEOREM 4.2. Postprocessing PrivTree's output with Laplace noise of scale λ achieves ε-differential privacy, when λ ≥ l ε . Finally, we clarify how we set θ and divide the privacy budget ε between PrivTree and its postprocessing step. First, we set θ = 0, following our analysis in Section 3.4. Second, we set the noise scale in PrivTree and its postprocessing step, such that PrivTree achieves ε β -differential privacy and the postprocessing procedure ensures ε•(β-1) β -differential privacy. To explain, recall that in PrivTree, we inject Laplace noise into each node v's score c(v), which equals the sum of β -1 counts in v's prediction histogram hist(v) (i.e., all counts except the largest one). Meanwhile, in the postprocessing step, we add Laplace noise to each count y in the prediction histograms of T 's leaf nodes. Intuitively, c(v) is roughly β -1 times more resilient to noise than y. Therefore, we set the privacy budget for the postprocessing step to be β -1 times the budget for PrivTree, so as to balance the relative accuracy of c(v) and y after noise injection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Previous Work</head><p>There exist two differentially private methods <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref> for modeling sequence data, and they both utilize hierarchical decompositions for model construction. However, they considerably differ from PrivTree in three aspects. First, they model sequences based on their prefixes <ref type="bibr" target="#b7">[7]</ref> or n-grams <ref type="bibr" target="#b6">[6]</ref>, while PrivTree is based on a PST representation of the variable length Markov chain model <ref type="bibr" target="#b44">[44]</ref>. Second, their algorithms for hierarchical decompositions are similar in spirit to Algorithm 1, due to which they also require a predefined threshold h on the maximum height of the decomposition tree. Consequently, they suffer from similar deficiencies to those of Algorithm 1, i.e., they cannot generate accurate models because of the dependency on h. Third, when constructing a decomposition tree, the methods in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref> decide whether a node be split based only on a count associated with the node, whereas PrivTree adopts a more advanced strategy that takes into account three conditions commonly considered in the non-private setting. The above differences make PrivTree an effective approach for modeling sequence data, as we demonstrate in our experiments in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONNECTIONS TO SVT</head><p>In this section, we investigate the connection between PrivTree and the sparse vector techniques (SVTs) <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28]</ref>, which are a type of differentially private algorithms widely adopted in the literature. They take as input a sequence of queries and a threshold θ, and output either a set of queries whose results are likely to be larger than θ <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b28">28]</ref>, or a noisy version of the answers for such a query set <ref type="bibr" target="#b21">[21]</ref>. Intuitively, SVTs are similar in spirit to PrivTree since they both aim to identify some elements in a set (e.g., a node set or a query set) with "scores" above a given threshold. Motivated by this, in the following, we examine whether SVTs can be adopted for the hierarchical decomposition problem. Among the three existing variants of SVTs <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b28">28]</ref> that satisfy ε-differential privacy 3 , we will focus on a variant dubbed the binary SVT, as it is most relevant to our problem. Interested readers are referred to the technical report of this paper for discussions on the other two variants.</p><p>Algorithm 3 presents a generic version of the binary SVT. Its input includes (i) a dataset D, (ii) a sequence Q = {q1, q2, . . .} of queries such that each qi has sensitivity 1, (iii) a threshold θ, and (iv) a noise scale λ. Its output is a sequence of binary variables {o1, o2, . . .}, such that oi = 1 indicates that the result of qi is larger than θ, and oi = 0 indicates otherwise. The algorithm is fairly simple. It first computes a noisy threshold θ = θ + Lap(λ), and then, for each query qi in the sequence, it generates a noisy query answer qi(D) using Laplace noise of scale λ (Line 1-3). If qi(D) &gt; θ, then algorithm outputs oi = 1; otherwise, the algorithm outputs oi = 0 (Line 4-7). Previous work <ref type="bibr" target="#b28">[28]</ref> makes the following claim about the privacy assurance of the algorithm:</p><formula xml:id="formula_40">CLAIM 1. Algorithm 3 ensures ε-differential privacy if λ ≥ 2 ε .</formula><p>In other words, the noise scale required by the algorithm is Θ( 1 ε ) and independent of the number of queries.</p><p>If Claim 1 holds, Algorithm 3 could yield highly competitive solutions for the problems that we consider. For example, consider the spatial decomposition problem studied in Section 3. Given a threshold θ and a set D of spatial points in a multi-dimensional space Ω, we first initialize (i) a quadtree T containing only a root node v1 with dom(v1) = Ω, and (ii) a query sequence Q = {c(v1)}, i.e., Q contains only one query that asks the number of points in dom(v1) (we will dynamically append queries to Q during the construction of the quadtree). After the initialization, we invoke the binary SVT to inspect each query in Q one by one; if the binary SVT outputs 1 for a query c(v), then we split the node v in T , and append a query c(v ) to the end of Q for each child node v of v. When all queries in Q are inspected, we return the quadtree T obtained. By Claim 1, T ensures ε-differential privacy, as long as the binary SVT uses Laplace noise of scale λ ≥ 2 ε when generating the noisy versions of c(vi). In contrast, PrivTree requires injecting Laplace noise of scale λ ≥ 2β-1 β-1 • 1 ε &gt; 2 ε , which indicates that the solution based on the binary SVT is more favorable.</p><p>Unfortunately, we show that Claim 1 does not hold: in the worst case, Algorithm 3 requires λ = Ω( k ε ) to achieve ε-differential privacy, where k denotes the number of queries. 3 There exist other variants of SVT that satisfy a relaxed version of ε-differential privacy <ref type="bibr" target="#b23">[23]</ref>. We do not consider those variants. LEMMA 5.1. There exists a sequence Q of k count queries for which Algorithm 3 violates ε-differential privacy if λ ≤ k 4ε .</p><p>Lemma 5.1 invalidates all solutions based on the binary SVT, including those in previous work <ref type="bibr">[9,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b34">34]</ref>. In concurrent work <ref type="bibr" target="#b11">[11]</ref>, Chen and Machanavajjhala present a similar analysis on the binary SVT, and also come to the conclusion that it is not differentially private. In the full version of this paper<ref type="foot" target="#foot_2">4</ref> , we discuss the other two variants of SVT <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21]</ref>, and show that one of them <ref type="bibr" target="#b21">[21]</ref> also violates differential privacy, while the other <ref type="bibr" target="#b18">[18]</ref> does not yield a competitive solution for our problem (even after we improve it with an optimization that leads to better data utility).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>This section evaluates PrivTree against the states of the art on differentially private modelling of spatial and sequence data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiments on Spatial Data</head><p>Datasets. We make use of four real spatial datasets shown in Table <ref type="table" target="#tab_1">2</ref>: road <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b41">41]</ref>, where each point represents the latitude and longitude of a road junction in the states of Washington and New Mexico; Gowalla <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b48">48]</ref>, which contains check-in locations shared by users on a location-based social networking website; NYC<ref type="foot" target="#foot_3">5</ref> and Beijing<ref type="foot" target="#foot_4">6</ref> , which are 4-dimensional datasets that record the pickup and drop-off locations of NYC and Beijing taxis, respectively. Figure <ref type="figure">4</ref> visualizes the points in road and gowalla, as well as the pickup locations in NYC and Beijing. Observe that the data distribution in road (resp. NYC) is more skewed than that in Gowalla (resp. Beijing).</p><p>Methods. We compare PrivTree against five state-of-the-art methods: UG <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b48">48]</ref>, AG <ref type="bibr" target="#b41">[41]</ref>, Hierarchy <ref type="bibr" target="#b42">[42]</ref>, DAWA <ref type="bibr" target="#b30">[30]</ref>, and Privelet * <ref type="bibr" target="#b50">[50]</ref>. UG partitions the data domain into m d grid cells of equal size, and releases a noisy count for each cell, with m = (nε/10) 2/(d+2) <ref type="bibr" target="#b48">[48]</ref>. AG is an improved version of UG that is specifically designed for two-dimensional data. It first employs a coarsened version of UG to produce a set of grid cells; after that, for each cell whose noisy count is above a threshold, AG further splits it into smaller cells and releases their noisy counts. Hierarchy utilizes a multi-level decomposition tree to generate spatial histograms, with the tree height and fanout heuristically chosen to minimize the mean squared error in answering range count queries. DAWA requires as input a workload of range count queries, and it employs the matrix mechanism <ref type="bibr" target="#b31">[31]</ref> to generate a histogram that is optimized for the given workload. Privelet * publishes multidimensional datasets by utilizing the Haar wavelet transformation to reduce the errors of range count queries.</p><p>DAWA and Privelet * both require that the input data should have a discrete domain. Following <ref type="bibr" target="#b30">[30]</ref>, we discretize the domain of each dataset into a uniform grid with 2 20 cells before feeding it to DAWA and Privelet * . The other parameters of each method (e.g., the height and fanout of the decomposition tree, and the grid granularity) are set as suggested in the original papers. For PrivTree, we set its fanout to 4 (resp. 16) for two-dimensional (resp. fourdimensional) datasets, which is standard for quadtrees.</p><p>We adopt the implementations of DAWA and Privelet * provided by their respective authors, and we implement all other methods in C++. All experiments are conducted on a windows/linux machine with a 2.4GHz CPU and 16GB main memory.   Tasks. We apply each method to create private synopses of every dataset, and we evaluate the quality of each decomposition by the accuracy of its answers to range count queries. In particular, we construct three query sets on each dataset: small, medium, and large, each of which contains 10, 000 randomly generated range count queries. Each query in the small, medium, and large set has a region that cover [0.01%, 0.1%), [0.1%, 1%), and [1%, 10%) of the data domain, respectively. Following prior work <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b41">41]</ref>, we measure accuracy of an answer q(D) to a query q by its relative error, defined as</p><formula xml:id="formula_41">RE (q(D)) = |q(D) -q(D)| max {q(D), Δ} .</formula><p>where Δ is a smoothing factor set to 0.1% of the dataset cardinality n <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b50">50]</ref>. We repeat each experiment 100 times, and report the average relative error of each method for each query set. For DAWA (which is query-dependent), we allow it to generate a synopsis for each query set separately, based on a sample set of 500 queries.</p><p>Results. Figure <ref type="figure" target="#fig_8">5</ref> illustrates the average relative error of each method on each dataset as a function of the privacy budget ε. On road, PrivTree significantly outperforms UG, AG, Hierarchy, and Privelet * , regardless of the query set used and the value of ε. In particular, on the large query set, the average relative error of PrivTree is at most 1 4 (resp. 1  10 ) of the error of AG (resp. UG and Hierarchy). This demonstrates the effectiveness of PrivTree in approximating the distribution of the input data. Meanwhile, AG is superior to UG and Hierarchy in all cases, which is consistent with the results in previous work <ref type="bibr" target="#b41">[41]</ref>. DAWA is the only method that comes close to PrivTree, but its relative error is never smaller than that of PrivTree, and is 2 to 3 times higher than the latter for the small and medium query sets on road (resp. small query set on Gowalla) when ε ≥ 0.8. Furthermore, we note that DAWA is given a sample query set in advance to optimize its query performance, whereas PrivTree is not given such an advantage.</p><p>On Gowalla, PrivTree still consistently achieves the best results, but the performance gaps between PrivTree and the other methods are reduced. The reason is that the data distribution in Gowalla is less skewed than that of road (see Figure <ref type="figure">4</ref>), which makes Gowalla easier to dealt with for all methods. DAWA incurs relatively small errors in all cases, but is noticeably inferior to PrivTree on the small and medium query sets.</p><p>On NYC and Beijing, we omit AG and Hierarchy since (i) AG is only applicable on two-dimensional data, and (ii) when applied on a four-dimensional dataset, Hierarchy produces a decomposition tree with at least 2.18 billion leaf nodes <ref type="bibr" target="#b42">[42]</ref>, which cannot fit in the main memory of our machine. As shown in Figures <ref type="figure" target="#fig_8">5g-5l</ref>, PrivTree consistently outperforms all other methods by a large margin on the highly skewed NYC, because its tree construction mechanism enables it to effectively adapt to the skewness of the data, by growing the tree tall (resp. short) in the dense (resp. sparse) regions of the data. On the other hand, on the less skewed Beijing, the accuracies of UG and DAWA are considerably improved. Nevertheless, PrivTree still incurs smaller query errors in all settings. One may notice that the error of DAWA on NYC only decreases around 2 times when ε increases from 0.05 to 1.6. We find that it is caused by a "private partitioning" step of DAWA <ref type="bibr" target="#b30">[30]</ref>, as well as the discretization of data domain Ω that it requires.</p><p>In summary, PrivTree provides better data utility than all baselines, especially when the input dataset follows a skewed distribution. This makes PrivTree a more favorable approach for releasing spatial data under differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments on Sequence Data</head><p>Datasets. We use two real sequence datasets: mooc<ref type="foot" target="#foot_5">7</ref> and msnbc <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b6">6]</ref>. mooc contains 80, 362 learners' behavior sequences on a MOOC platform, and the behaviors are divided into seven categories: working on assignments, watching videos, accessing other course objects, accessing the course wiki, accessing the course forum, navigating to other part of course, and closing the web page. msnbc consists of 989, 818 sequences of URL categories, each of which corresponds to a user's browsing history during a 24-hour period on msnbc.com. Table <ref type="table" target="#tab_2">3</ref> shows the key statistics of mooc and msnbc. Note that the total number |I| of symbols in mooc (resp. msnbc) is not excessively large. Otherwise (e.g., when |I| &gt; 1000), the domain of the sequence data would be extremely Tasks. We consider two analytical tasks on each sequence dataset D. The first task is to identify the top-k frequent strings in D, i.e., the k strings that appear the largest number of times in the sequences in D. This task is an important primitive in sequence data mining <ref type="bibr" target="#b20">[20]</ref>, and is also considered in existing work <ref type="bibr" target="#b6">[6]</ref> on sequence data publishing. Following previous work <ref type="bibr" target="#b6">[6]</ref>, we measure the precision of the top-k strings returned by differentially private algorithm, i.e.,</p><formula xml:id="formula_42">precision = |K(D) ∩ A(D)| k ,</formula><p>where K(D) is the exact set of top-k frequent strings in D, and A(D) is the set returned by algorithm A.</p><p>The second task is to approximate the distribution of sequence lengths in D. In particular, we apply PrivTree and other existing methods to generate synthetic sequence data from D. Then, we compare the distribution of sequence lengths in the synthetic data with that in D, and we measure their total variation distance <ref type="bibr" target="#b13">[13]</ref>, i.e., half of the L1 distance between the two probability distributions. For each task, we repeat each experiment 100 times and report the average measurements.</p><p>Methods. For the task of top-k frequent string mining, we compare PrivTree against two differentially private techniques: N-gram <ref type="bibr" target="#b6">[6]</ref> and EM <ref type="bibr" target="#b38">[38]</ref>. In particular, N-gram is the state-of-the-art solution for sequence data publishing, and it is based on a variable-length ngram model. N-gram requires a pre-defined threshold nmax on the maximum length of n-grams; we set nmax = 5, as suggested in <ref type="bibr" target="#b6">[6]</ref>. Meanwhile, EM is a standard application of the exponential mechanism <ref type="bibr" target="#b38">[38]</ref> in our context. It first initializes a set R that contains |I| string of length 1, each of which consists of a unique symbol in I. After that, it invokes the exponential mechanism k times. In each invocation, it selects the most frequent string r from R with differential privacy, and then replaces r in R with |I| strings, each of which is obtained by adding a symbol to the end of r. The k strings obtained are then returned as the result. For the task of approximating sequence length distributions, we omit EM since it is inapplicable.</p><p>Note that PrivTree, N-gram, and EM all require that the maximum sequence length in the input data is bounded by a constant l that is not excessively large (see Section 4.2 for a discussion on the necessity of l ). Following previous work <ref type="bibr" target="#b6">[6]</ref>, we set l to be roughly the 95% quantile of the sequence lengths in the input data, i.e., only around 5% sequences are truncated (see Table <ref type="table" target="#tab_2">3</ref>). To illustrate the effects of truncation, we also include in our experiments a baseline approach dubbed Truncate. This approach directly answers all queries on the truncated dataset, without any privacy assurance.</p><p>Results. Figure <ref type="figure" target="#fig_9">6</ref> shows the precision of each method (for topk string mining) as a function of the privacy budget ε. The precision of Truncate remains unchanged for all ε, since it does not enforce differential privacy. Among the differentially private methods, PrivTree consistently outperforms N-gram and EM, in most cases by a large margin. Furthermore, in Figure <ref type="figure" target="#fig_9">6d</ref>-6f, PrivTree has an even higher precision than Truncate when ε ≥ 0.8. The reason is that the Markov model adopted by PrivTree is able to recover some information that is lost due to the truncation of sequences. For example, suppose that the string aa appears in 5 sequences in a dataset D and, in each appearance, it is immediately followed by a symbol b. Assume that one of those 5 sequences (denoted as s) is truncated, and its suffix aab becomes aa after the truncation. In that case, the Markov model can be used able to accurately recover the truncated symbol of s, because, based on the truncated data, it would predict that the "next symbol" after aa is always b. Intuitively, such recovering of information is more effective when the amount of noise in the Markov model is small, which explains why PrivTree outperforms Truncate only when ε is large. In contrast, N-gram never outperforms Truncate, and its precision is lower than that of PrivTree by more than 10% in most settings. In addition, EM yields unattractive precision in almost all cases. Its accuracy degrades with the increases of k, since a larger k requires it to inject more noise into the selection procession of top-k frequent strings.</p><p>In the last set of experiments, we evaluate the accuracy of the sequence length distribution in the synthetic sequence data generated by each method. Figure <ref type="figure" target="#fig_11">7</ref> illustrates the total variation distance of each sequence length distribution. Observe that PrivTree incurs a small error comparable to that of Truncate, especially when ε ≥ 0.2. In contrast, N-gram entails an enormous error in all cases. Based on the results in Figures <ref type="figure" target="#fig_9">6</ref> and<ref type="figure" target="#fig_11">7</ref>, we conclude that PrivTree is a more preferable solution than N-gram to modeling sequential data under ε-differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ADDITIONAL RELATED WORK</head><p>In Sections 3.1, 4.3, and 6, we have introduced the states-of-   the-art solutions <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b50">50]</ref> for publishing spatial and sequence data under differential privacy. Besides those solutions, there are a few other methods for private modeling of spatial and sequence data. In particular, Xiao et al. <ref type="bibr" target="#b51">[51]</ref> present a spatial decomposition algorithm based on the k-d tree <ref type="bibr" target="#b4">[4]</ref>. It first imposes a uniform grid over the data domain, and then construct a private k-d tree over the cells in the grid. This method, however, is shown to be inferior to the UG and AG methods tested in our experiments, in terms of data utility <ref type="bibr" target="#b41">[41]</ref>. Chen et al. <ref type="bibr" target="#b7">[7]</ref> consider the publication of sequence data under differential privacy, and propose an algorithm that releases a prefix tree of sequences to support count queries and frequent string mining. Nevertheless, subsequent work by Chen et al. <ref type="bibr" target="#b6">[6]</ref> shows that the prefix-based method is considerably outperformed by the N-gram approach in our experiment.</p><p>In addition, there is a long line of research on processing aggregate queries in a differentially private manner. Specifically, Barak et al. <ref type="bibr" target="#b2">[2]</ref> investigate the publication of marginals (i.e., projections of a dataset on subsets of its dimensions), and propose a solution based on the Fourier transform. Ding et al. <ref type="bibr" target="#b15">[15]</ref> publish multiple data cubes with both privacy and consistency guarantees. The matrix mechanism of Li and Miklau <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref> and follow-up approaches <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53]</ref> take into account a query workload, and aim to release a version of the data that maximizes the overall accuracy of the workload. DAWA <ref type="bibr" target="#b30">[30]</ref> is the most advanced method among these approaches, but as shown in our experiments, it is outperformed by PrivTree in terms of the relative errors of range count queries on spatial data.</p><p>Moreover, there exists extensive work that addresses numerous other tasks under differential privacy, such as regressions <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b57">57]</ref>, clusterings <ref type="bibr" target="#b48">[48]</ref>, decision trees <ref type="bibr" target="#b19">[19]</ref>, recommendation systems <ref type="bibr" target="#b37">[37]</ref>, time-series data analysis <ref type="bibr" target="#b43">[43]</ref>, combinatorial optimizations <ref type="bibr" target="#b49">[49]</ref>, frequent itemset mining <ref type="bibr" target="#b35">[35]</ref>, and graph queries <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b56">56]</ref>. Finally, recent research has also studied the adoption of differential privacy in various systems <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b45">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUDING REMARKS</head><p>In this paper, we study the problem of hierarchical decomposition under differential privacy, and address the central dilemma of choosing the maximum height h of the decomposition tree. We show that the constraint on h can be removed by introducing a carefully controlled bias in deciding when a node should be split. Based on this result, we propose PrivTree, a general approach for hierarchical decomposition on private data, and we showcase its applications on spatial and sequence data release. Our experimental results demonstrate that PrivTree significantly outperforms the states of the art in terms of data utility. For future work, we plan to extend the idea behind PrivTree to other problems that are based on a lattice-model instead of a tree-model, such as frequent itemset mining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. PROOFS</head><p>Proof of Lemma 3.1. By Equations ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_10">5</ref>), for any x,</p><formula xml:id="formula_43">ρ(x) = ln ⎛ ⎝ +∞ θ-x 1 2λ exp -|y| λ dy +∞ θ+1-x 1 2λ exp -|y| λ dy ⎞ ⎠ ≤ 1 λ . Recall that ρ (x) = 1/λ when c(v) &lt; θ + 1. Therefore, ρ(x) ≤ ρ (x) holds if x &lt; θ + 1. Now consider that x ≥ θ + 1. In that case, ρ(x) = ln 1 -1 2 exp θ-x λ 1 -1 2 exp θ+1-x λ .</formula><p>For convenience, we let α = 1 2 exp θ+1-x λ , and define a function f of α as follows:</p><formula xml:id="formula_44">f (α) = ρ(x) - 1 λ exp θ + 1 -x λ = ln 1 -αe -1/λ 1 -α - 2α λ .</formula><p>Observe that α ∈ (0, 1/2] whenever x ≥ θ + 1. Therefore, we can prove the lemma by showing that f (α) ≤ 0 for all α ∈ (0, 1/2].</p><p>For this purpose, we first compute the second derivative of f with respect to α:</p><formula xml:id="formula_45">f (α) = (e 1/λ -1) • e 1/λ + 1 -2α (1 -α) 2 • (e 1/λ -α) 2 .</formula><p>Given that e 1/λ -1 &gt; 0 and e 1/λ + 1 -2α ≥ e 1/λ &gt; 0, we have f (α) &gt; 0. This indicates that max α∈(0,1/2] f (α) = max{f (0), f(1/2)}. Meanwhile, f (0) = 0, and</p><formula xml:id="formula_46">f (1/2) = ln(2 -e -1/λ ) - 1 λ = ln e 1/λ -e 1/2λ -e -1/2λ 2 - 1 λ &lt; ln(e 1/λ ) - 1 λ = 0.</formula><p>Therefore, max α∈(0,1/2] f (α) ≤ 0, which proves the lemma.</p><p>Proof of Theorem 3.1. The theorem directly follows from the privacy analysis in Section 3.3.</p><p>Proof of Lemma 3.2. Let V be the set of all possible nodes in a quadtree built on D. We divide the nodes in V into three subsets: (i) the set V1 of nodes that appear as non-leaf nodes in T * , (ii) the set V2 of the nodes that appear as leaves in T * , and (iii) the set V3 of the nodes that do not appear in T * . Let g(S) be the expected number of nodes in a set S that appear in T . Then, we have</p><formula xml:id="formula_47">E[|T |] = g(V1) + g(V2) + g(V3) ≤ |V1| + |V2| + g(V3) = |T * | + g(V3).</formula><p>Therefore, the lemma can be proved by showing g(V3) ≤ |T * |.</p><p>Observe that each node in V3 must be the descendant of a node in V2, i.e., a node that appears as a leaf in T * . Therefore, we can divide the nodes in V3 into |V2| subsets, such that all nodes in the same subset are descendants of the same node in V2. Consider any such subset S that corresponds to a node v ∈ V2. Given that v appears as a leaf in T * , we have c(v) ≤ θ. In addition, since</p><formula xml:id="formula_48">|T * | &gt; 1, depth(v) ≥ 1 holds. Therefore, c(v) -depth(v) • δ ≤ c(v) -δ ≤ θ -δ.</formula><p>By Equation ( <ref type="formula" target="#formula_17">8</ref>), we have b(v) = θδ. Furthermore, for any v ∈ S, we have c(v ) ≤ c(v), which also leads to b(v ) = θδ. Therefore, v and v have the same probability ps to be split. Given δ = λ • ln β, we have</p><formula xml:id="formula_49">ps = Pr[θ -δ + Lap(λ) &gt; θ] = Pr[Lap(λ) &gt; δ] = ∞ λ•ln β 1 2λ exp - |y| λ dy = 1 2β .</formula><p>Assume that v appears in T . Then, given that v is split with </p><formula xml:id="formula_50">(S) = +∞ i=1 β i • 1 2β i = +∞ i=1 1 2 i = 1.</formula><p>Since each S uniquely corresponds to a node in V2, we have</p><formula xml:id="formula_51">g(V 3) = |V2| • g(S) = |V2| ≤ |T * |.</formula><p>Therefore, the lemma is proved.</p><p>Proof of Corollary 1. The corollary follows from Theorem 3.1 when γ = ln β.</p><p>Proof of Lemma 4.1. Let u and v be two nodes in T , such that u is the parent of v. Then, hist(v)[x] ≤ hist(u)[x] holds for every symbol x ∈ I ∪ {&amp;}. Let xv (resp. xu) be the symbol that has the largest count in hist(v) (resp. hist(u)). We have</p><formula xml:id="formula_52">c(v) = hist(v) 1 -hist(v)[xv] ≤ hist(v) 1 -hist(v)[xu] = x =xu hist(v)[x] ≤ x =xu hist(u)[x] = hist(u) 1 -hist(u)[xu] = c(u).</formula><p>Therefore, c(•) is monotonic. In the following, we will prove that for any i ∈ [1, l] and any output T of the modified PrivTree,</p><formula xml:id="formula_53">- ε l ≤ ln Pr[Di → T ] Pr[Di-1 → T ] ≤ ε l , (<label>14</label></formula><formula xml:id="formula_54">)</formula><p>where Pr[Di → T ] denotes the probability that PrivTree outputs T given Di. This would prove the theorem because, given that</p><formula xml:id="formula_55">l ≤ l , ln Pr[D → T ] Pr[D → T ] = l i=1 ln Pr[Di → T ] Pr[Di-1 → T ] ∈ [-ε, ε].</formula><p>Observe that Di can be obtained by appending a symbol xi to the end of the sequence si-1 in Di-1. Therefore, when we change the input data from Di-1 to Di, the only changes in the PST are the histogram counts that xi contributes to. Observe that if xi contributes to the prediction histogram hist(v) of a node v, then dom(v) must be a suffix of si-1, and the only possible change in hist(v) is that hist(v) <ref type="bibr">[xi]</ref> would be increased by one. Then, by the definition of the PST, all of those nodes v should form a path from the root of the PST to a leaf. In addition, by Equation ( <ref type="formula" target="#formula_38">13</ref>), the score c(v) of each of those nodes v is changed by at most one. In that case, we can prove Equation ( <ref type="formula" target="#formula_53">14</ref>) by reusing the analysis in the proof of Theorem 3.1.</p><p>To explain, recall that the correctness of Theorem 3.1 only replies on two conditions. First, the score c(v) of each node v is monotonic. Second, when we change the input data, all of the nodes affected should form a path from the root of the decomposition tree to a leaf, and the score of each of those nodes should change by at most one. Notice that all three conditions are satisfied when we change the input of the modified PrivTree from Di-1 to Di. Combining this with the fact that the modified PrivTree uses a noise scale that is l times that of Algorithm 2, it can be verified that Equation ( <ref type="formula" target="#formula_53">14</ref>) holds. Therefore, the theorem is proved. In what follows, we prove the lemma by showing that Equation <ref type="bibr" target="#b15">(15)</ref> does not hold when λ ≤ k/4ε. Recall that Algorithm 3 generates a noisy threshold θ, and outputs 1 for a query q only when its noisy answer q(D) is larger than θ. Therefore, Therefore, Pr[D 1 →E] Pr[D 3 →E] &gt; e 2ε when λ ≤ k/4ε, which proves the lemma.</p><formula xml:id="formula_56">Pr[D 1 → E] Pr[D 3 → E]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ADDITIONAL EXPERIMENTS</head><p>In this section, we evaluate the computation efficiency of PrivTree, and the impact of β (i.e., tree fanout) on the accuracy of PrivTree. Table <ref type="table">B</ref> shows the processing time of PrivTree on each dataset (averaged over 100 runs), with ε varying from 0.05 to 1.6. The running time of PrivTree on road and msnbc are larger than that on the other datasets, since road and msnbc are larger in size than the others. In addition, the computation cost of PrivTree increases with ε. To understand this, recall that when PrivTree decides whether or not to split a node v, it first subtracts a bias term depth(v) • δ from the score of v, and then injects noise into the biased score, after which it splits v if the noisy score is larger than the threshold θ. As δ is inversely proportional to ε (see Corollary 1), the bias term increases when ε decreases, in which case the noisy score of v is less likely to be larger than θ. Therefore, when ε is small, PrivTree has lower probabilities to split nodes, which leads to a small running time.</p><p>Previously, in Section 6.1, we evaluate the query accuracy of PrivTree on spatial data with its fanout β set to 2 d , where d is the dataset dimensionality. In that case, whenever PrivTree splits     <ref type="figure" target="#fig_2">2</ref> illustrates the results of the same experiments when β varies. In particular, when we set β = 2 i with i &lt; d, PrivTree would split the dimensions of each node in a round robin fashion, with i dimensions being bisected each time. Observe that, in general, the query error of PrivTree slightly increases when β decreases. This is mainly due to the bias term depth(v)•δ that PrivTree subtracts from the score c(v) of each node v, when it decides whether v should be split. Specifically, a decreased β increases the height of PrivTree's decomposition tree, in which case the nodes towards the leaf level of the tree would be given a larger bias term. In turn, the increased bias term renders it more difficult for PrivTree to correctly decide whether a node should be split, thus degrading the quality of PrivTree's output. Nevertheless, on a few settings on NYC and Beijing, β = 2 d/2 entails smaller errors than β = 2 d . The reason is that, when β is large, any incorrect decisions made by PrivTree in node splitting would have a more pronounced negative effect, e.g., a quadtree node with a small count would be divided into a larger number of child nodes, each of which would have an even smaller count that is likely to be overwhelmed by the noise subsequently added. The increased number of noise-dominated nodes would then lead to less accurate query answers, which explains why β = 2 d/2 sometimes outperforms β = 2 d . That said, the overall result in Figure <ref type="figure" target="#fig_16">8</ref> indicates that β = 2 d is still a preferable choice for PrivTree.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of a spatial decomposition tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 mark v as visited; 5 compute</head><label>45</label><figDesc>the number c(v) of points in D that are contained in dom(v); 6 compute a noisy version of c(v): ĉ(v) = c(v) + Lap(λ); 7 if ĉ(v) &gt; θ and depth(v) &lt; h -1 then 8 split v, and add its children to T ; 9 mark the children of v as unvisited; 10 return T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of ρ(x) and ρ (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 mark v as visited; 5 compute a biased</head><label>45</label><figDesc>) is relatively straightforward. For any node v in T , let c(v) be v's point count on D, and b(v) be the biased version of c(v) generated from Equation<ref type="bibr" target="#b8">(8)</ref>. Let c (v) and b (v) be the counterparts of c(v) and b(v), respectively, given D as the input. Then, we have c(v) = c (v) for all nodes v in T , except for the nodes whose sub-domains contain Algorithm 2: PrivTree (D, λ, θ, δ) 1 initialize a quadtree T with a root node v 1 ; 2 set dom(v 1 ) = Ω, and mark v 1 as unvisited; 3 while there exists an unvisited node v do point count for v with decaying factor δ: b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of a prediction suffix tree (PST).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>THEOREM 4 . 1 .</head><label>41</label><figDesc>Let β = |I|+1. The modified PrivTree ensures ε-differential privacy when λ ≥ 2β-1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 3 :</head><label>3</label><figDesc>BinarySVT (D, Q = {q1, q2, . . .}, θ, λ) 1 compute a noisy version of θ: θ = θ + Lap (λ); 2 for i = 1, 2, . . . do 3 compute a noisy version of q i (D): qi (D) = q i (D) + Lap (λ); 4 if qi (D) &gt; θ then 5 output o i = 1 and continue;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(c) road -large queries.(f) Gowalla -large queries.(i) NYC -large queries.(l) Beijing -large queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results of range count queries on spatial datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of top-k frequent string mining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Errors of sequence length distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Proof of Theorem 4 . 1 .</head><label>41</label><figDesc>Let D and D be two neighboring datasets, such that D is obtained by inserting a sequence s into D . Assume that s = $x1 . . . x l , where xi ∈ I ∪ {&amp;} for i ∈ [1, l]. To facilitate our proof, we define l datasets D1, D2, . . . , D l , such that Di = D ∪ {si} and si = $x1x2 . . . xi is the length-i prefix of s ended at symbol xi. Observe that D l = D. For convenience, we define D0 = D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Proof of Theorem 4 . 2 .</head><label>42</label><figDesc>Let T be the output of the modified PrivTree. Let D and D be two neighboring datasets, such that D is obtained by inserting a sequence s into D . Assume that s = $x1 . . . x l , where xi ∈ I ∪ {&amp;} for i ∈[1, l]. Observe that each symbol xi in s contributes to the prediction histograms of the nodes whose predictor strings dom(•) are suffixes of $x1 . . . xi-1. By the definition of PSTs, these nodes form a path from the root of T to a leaf. This indicates that each xi gets counted in the histogram of one leaf node only. Taking into account l ≤ l , it follows that the sensitivity of releasing the histogram counts of all leaf nodes in T is l . By the property of the Laplace mechanism, the postprocessing step ensures ε-differential privacy if λ ≥ l ε .Proof of Lemma 5.1. Consider three datasets D1 = {a, b}, D2 = {a, b, b}, and D3 = {b, b}, where each tuple is either a or b. Observe that D1 is a neighboring dataset of D2, while D2 is a neighboring dataset of D3. Let qa (resp. q b ) be a query that asks for the number of a (resp. b) in a dataset. Let Q be a sequence of k queries, such that first k/2 queries are all qa, and the remaining k/2 queries are all q b . Suppose that we invoke Algorithm 3 on D1, D2, D3, respectively, with Q, a noise scale λ, and a threshold θ = 1. Let E be the event that Algorithm 3 outputs 1 for the first k/2 queries in Q, and 0 for the remaining k/2 queries. In addition, let Pr[D → E] denote the probability that E occurs when the input dataset is D. If Algorithm 3 satisfies ε-differential privacy, thenPr[D1 → E] Pr[D2 → E] ≤ e ε ,Pr[D2 → E] Pr[D3 → E] ≤ e ε . This indicates that Pr[D 1 → E] Pr[D3 → E] = Pr[D1 → E] Pr[D2 → E] • Pr[D2 → E] Pr[D3 → E] ≤ e 2ε . (15)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 dx 1 λ 1 λ 1 λ</head><label>2111</label><figDesc>Pr[ θ = x] • Pr[ qa(D1) &gt; x] • Pr[ qb (D 1 ) ≤ x] Pr[ θ = x] • Pr[ qa(D3) &gt; x] • Pr[ qb (D 3 ) ≤ x] Pr[ θ = x] • Pr[Lap(λ) &gt; x -1] • Pr[Lap(λ) ≤ x -1] Pr[ θ = x] • Pr[Lap(λ) &gt; x] • Pr[Lap(λ) ≤ x -2] k Consider any x ∈ (-∞, +∞). If x &gt; 1, then Pr[Lap(λ) &gt; x -• Pr[Lap(λ) &gt; x],andPr[Lap(λ) ≤ x -1] &gt; Pr[Lap(λ) ≤ x -2]. This leads to Pr[Lap(λ) &gt; x -1] • Pr[Lap(λ) ≤ x -1] ≥ e • Pr[Lap(λ) &gt; x] • Pr[Lap(λ) ≤ x -2] (16) Meanwhile, if x ≤ 1, then Pr[Lap(λ) ≤ x -• Pr[Lap(λ) ≤ x -2],andPr[Lap(λ) &gt; x -1] &gt; Pr[Lap(λ) &gt; x].In that case, Equation (16) still holds.Given that Equation (16) holds for all x ∈ (-∞, ∞), we havePr[D 1 → E] Pr[D 3 → E] = ∞ -∞ Pr[ θ = x] • Pr[Lap(λ) &gt; x -1] • Pr[Lap(λ) ≤ x -1] Pr[ θ = x] • Pr[Lap(λ) &gt; x] • Pr[Lap(λ) ≤ x -2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(c) road -large queries.(f) Gowalla -large queries.(i) NYC -large queries.(l) Beijing -large queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Impact of fanout on PrivTree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table of notations</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>n, d</cell><cell>the cardinality and dimensionality of the input dataset D</cell></row><row><cell>Lap(λ)</cell><cell>a random variable following the Laplace distribution</cell></row><row><cell></cell><cell>with 0 mean and λ scale</cell></row><row><cell>dom(v)</cell><cell>the sub-domain of a node v</cell></row><row><cell>depth(v)</cell><cell>the hop distance from a node v to the root of the tree</cell></row><row><cell>θ</cell><cell>the threshold used to decide if a node should be split</cell></row><row><cell cols="2">c(v), ĉ(v) the point count of a node v, and its noisy version</cell></row><row><cell cols="2">b(v), b(v) the biased count of a node v, and its noisy version</cell></row><row><cell>ρ(v)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Characteristics of spatial datasets. Description road 2 1 , 634, 165 Coordinates of road intersections in the states of Washington and New Mexico Gowalla 2 107, 091 Check-in locations shared by users of a location-based social networking website NYC 4 9 8 , 013 Pickup and drop-off locations of NYC taxis Beijing 4 3 0 , 000 Pickup and drop-off locations of Beijing taxis</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) road</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Gowalla</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(c) NYC -pickup</cell><cell>(d) Beijing -pickup</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Figure 4: Visualization of datasets</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PrivTree</cell><cell></cell><cell>UG</cell><cell>AG</cell><cell></cell><cell cols="2">DAWA</cell><cell>Hierarchy</cell><cell>Privelet</cell></row><row><cell cols="2">3%</cell><cell cols="3">average relative error</cell><cell></cell><cell></cell><cell>20%</cell><cell cols="3">average relative error</cell><cell></cell><cell></cell><cell>100%</cell><cell cols="2">average relative error</cell><cell>40%</cell><cell>average relative error</cell></row><row><cell cols="2">2% 2.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell>25% 30% 35%</cell></row><row><cell cols="2">1.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell></row><row><cell cols="2">0.5% 1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1%</cell><cell></cell><cell></cell><cell>5% 10% 15%</cell></row><row><cell cols="4">0% 0.05 0.1</cell><cell>0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell cols="3">0% 0.05 0.1</cell><cell>0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell cols="2">0.1% 0.05 0.1</cell><cell>0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell>0% 0.05 0.1</cell><cell>0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell></row><row><cell></cell><cell></cell><cell cols="4">(a) road -small queries.</cell><cell></cell><cell></cell><cell cols="4">(d) Gowalla -small queries.</cell><cell></cell><cell></cell><cell cols="3">(g) NYC -small queries.</cell><cell>(j) Beijing -small queries.</cell></row><row><cell cols="2">3%</cell><cell cols="3">average relative error</cell><cell></cell><cell></cell><cell>40%</cell><cell cols="3">average relative error</cell><cell></cell><cell></cell><cell>1000%</cell><cell cols="2">average relative error</cell><cell>120%</cell><cell>average relative error</cell></row><row><cell cols="2">2% 2.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25% 30% 35%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell>80% 100%</cell></row><row><cell cols="2">1.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60%</cell></row><row><cell cols="2">0.5% 1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5% 10% 15%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell>20% 40%</cell></row><row><cell cols="4">0% 0.05 0.1</cell><cell>0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell cols="2">0% 0.05 0.1</cell><cell cols="2">0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell cols="4">1% 0.05 0.1 0.2 0.4 0.8 1.6 privacy budget ε</cell><cell>0% 0.05 0.1</cell><cell>0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell></row><row><cell></cell><cell></cell><cell cols="4">(b) road -medium queries.</cell><cell></cell><cell cols="6">(e) Gowalla -medium queries.</cell><cell cols="4">(h) NYC -medium queries.</cell><cell>(k) Beijing -medium queries.</cell></row><row><cell>10%</cell><cell cols="4">average relative error</cell><cell></cell><cell></cell><cell>40%</cell><cell cols="3">average relative error</cell><cell></cell><cell></cell><cell>1000%</cell><cell cols="2">average relative error</cell><cell>30%</cell><cell>average relative error</cell></row><row><cell>6% 8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>25% 30% 35%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell>20% 25%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15%</cell></row><row><cell>2% 4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5% 10% 15%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell>5% 10%</cell></row><row><cell cols="3">0% 0.05 0.1</cell><cell cols="2">0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell cols="2">0% 0.05 0.1</cell><cell cols="2">0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell cols="4">1% 0.05 0.1 0.2 0.4 0.8 1.6 privacy budget ε</cell><cell>0% 0.05 0.1</cell><cell>0.2 privacy budget ε 0.4</cell><cell>0.8</cell><cell>1.6</cell></row></table><note><p><p>Name</p>Dimensionality d Cardinality n</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Characteristics of sequence datasets.</figDesc><table><row><cell>Name</cell><cell cols="3">|I| Cardinality Avg. sequence length</cell><cell cols="2">Description l</cell><cell># of sequences with length &gt; l</cell></row><row><cell>mooc</cell><cell>7</cell><cell>8 0 , 362</cell><cell>13.46</cell><cell>Users' behavior sequences on a MOOC website</cell><cell>50</cell><cell>3,653</cell></row><row><cell>msnbc</cell><cell>17</cell><cell>989, 818</cell><cell cols="2">4.75 Users' web navigation histories on a news portal</cell><cell>20</cell><cell>31,606</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1 2β probability, each child of v has 1 2β probability to appear in T . Therefore, in expectation, the number of v's children that appear in T should equal β • 1 2β = 1 2 . In general, for any i ≥ 1, there exist β i nodes v ∈ S with depth(v )depth(v) = i, and each such v has ( 1 2β ) i probability to appear in T . Hence, the expected number of nodes in S that appear in T is g</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Running time of PrivTree (seconds). the sub-domain dom(v) of v is divided into 2 d parts by bisecting all dimensions of dom(v). Figure</figDesc><table><row><cell cols="7">Dataset ε = 0.05 ε = 0.1 ε = 0.2 ε = 0.4 ε = 0.8 ε = 1.6</cell></row><row><cell>road</cell><cell>0.97</cell><cell>1.15</cell><cell>1.35</cell><cell>1.61</cell><cell>1.93</cell><cell>2.52</cell></row><row><cell cols="2">Gowalla 0.044</cell><cell cols="4">0.055 0.073 0.093 0.12</cell><cell>0.17</cell></row><row><cell>NYC</cell><cell>0.032</cell><cell cols="4">0.040 0.051 0.072 0.10</cell><cell>0.15</cell></row><row><cell cols="7">Beijing 0.0085 0.012 0.013 0.019 0.030 0.047</cell></row><row><cell>mooc</cell><cell>0.22</cell><cell>0.26</cell><cell>0.30</cell><cell>0.35</cell><cell>0.41</cell><cell>0.46</cell></row><row><cell>msnbc</cell><cell>1.73</cell><cell>2.03</cell><cell>2.21</cell><cell>2.50</cell><cell>2.72</cell><cell>3.05</cell></row><row><cell>a node v,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here we treat hist(v) as a probability distribution.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Such l can be chosen by first identifying the 90% or 95% quantile of the sequence lengths in D, and then computing a differentially private version of the quantile<ref type="bibr" target="#b54">[54]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://arxiv.org/abs/1601.03229</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://publish.illinois.edu/dbwork/open-data/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://research.microsoft.com/apps/pubs/?id=152883</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://www.kddcup2015.com/ sparse, in which case it is enormously difficult to publish useful information under differential privacy.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Xiaokui Xiao was supported by grant ARC19/14 from MOE, Singapore and gifts from MSRA and AT&amp;T.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(f) msnbc -top200.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Privacy, accuracy, and consistency too: a holistic solution to contingency table release</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On prediction using variable order markov models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Begleiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="385" to="421" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1069" to="1109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Differentially private sequential data publication via variable-length n-grams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Acs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castelluccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="638" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differentially private transit data publication: a case study on the montreal transportation system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Sossou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="213" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards statistical queries over distributed private user data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reznichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Differentially private high-dimensional data publication via sampling-based inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recursive mechanism: Towards node differential privacy and unrestricted joins</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="653" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the privacy properties of variants on the sparse vector technique</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<idno>CoRR, abs/1508.07306</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differentially private spatial decompositions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to nonparametric estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Kreveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Overmars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">C</forename><surname>Schwarzkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational geometry</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differentially private data cubes: optimizing noise sources and consistency</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<title level="m">Differential privacy. In ICALP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TCC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data mining with differential privacy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<title level="m">Data mining: concepts and techniques: concepts and techniques</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A study of privacy and fairness in sensitive data analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple and practical algorithm for differentially private data release</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multiplicative weights mechanism for privacy-preserving data analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the geometry of differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosting the accuracy of differentially private histograms through consistency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1021" to="1032" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Private analysis of graph structure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raskhodnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yaroslavtsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TODS</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Private convex optimization for empirical risk minimization with applications to high-dimensional regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="25" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Top-k frequent itemsets via differentially private fp-trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Differentially private m-estimators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A data-and workload-aware algorithm for range queries under differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="341" to="352" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing linear counting queries under differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="123" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An adaptive mechanism for accurate query answering under differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="514" to="525" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimal error of query sets under the differentially-private matrix mechanism</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Differentially private histogram publication for dynamic datasets: an adaptive sampling approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Privbasis: Frequent itemset mining with differential privacy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qardaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1340" to="1351" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exponential random graph estimation under differential privacy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="921" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differentially private recommender systems: Building privacy into the netflix prize contenders</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mechanism design via differential privacy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Djoin: Differentially private join queries over distributed databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haeberlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="149" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Smooth sensitivity and sampling in private data analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raskhodnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Differentially private grids for geospatial data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qardaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="757" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding hierarchical methods for differentially private histograms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qardaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1954" to="1965" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Differentially private aggregation of distributed time-series with transformation and encryption</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="735" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The power of amnesia: Learning probabilistic automata with variable memory length</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="117" to="149" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Airavat: Security and privacy for mapreduce</title>
		<author>
			<persName><forename type="first">I</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T V</forename><surname>Setty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kilzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Foundations of multidimensional and metric data structures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Privacy-preserving statistical estimation with optimal convergence rate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05998</idno>
		<title level="m">Differentially private k-means clustering</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Differentially private combinatorial optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SODA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Differential privacy via wavelet transforms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1214" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Differentially private data release through multidimensional partitioning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Secure Data Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="150" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Accurate and efficient private release of datacubes and contingency tables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yaroslavtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="745" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Low-rank mechanism: Optimizing batch queries under differential privacy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1352" to="1363" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On differentially private frequent itemset mining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Privbayes: Private data release via bayesian networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1423" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Private release of graph statistics using ladder functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="731" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PrivGene: differentially private model fitting using genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
