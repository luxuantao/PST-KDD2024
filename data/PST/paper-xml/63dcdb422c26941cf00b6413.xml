<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2023 AUTOTRANSFER: AUTOML WITH KNOWLEDGE TRANSFER -AN APPLICATION TO GRAPH NEURAL NETWORKS</title>
				<funder>
					<orgName type="full">Toshiba</orgName>
				</funder>
				<funder ref="#_P5Cj5R9">
					<orgName type="full">Stanford Data Science Initiative</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder ref="#_Q8t4cFS">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_pPxFykU">
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_SA3PSuc">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_sgzZsHf">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">NEC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-14">14 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
							<email>kaidicao@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
							<email>jiaxuan@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaju</forename><surname>Liu</surname></persName>
							<email>jiajuliu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2023 AUTOTRANSFER: AUTOML WITH KNOWLEDGE TRANSFER -AN APPLICATION TO GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-14">14 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.07669v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AutoML has demonstrated remarkable success in finding an effective neural architecture for a given machine learning task defined by a specific dataset and an evaluation metric. However, most present AutoML techniques consider each task independently from scratch, which requires exploring many architectures, leading to high computational costs. Here we propose AUTOTRANSFER, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest. Our key innovation includes a task-model bank that captures the model performance over a diverse set of GNN architectures and tasks, and a computationally efficient task embedding that can accurately measure the similarity among different tasks. Based on the task-model bank and the task embeddings, we estimate the design priors of desirable models of the novel task, by aggregating a similarity-weighted sum of the top-K design distributions on tasks that are similar to the task of interest. The computed design priors can be used with any AutoML search algorithm. We evaluate AUTOTRANSFER on six datasets in the graph machine learning domain. Experiments demonstrate that (i) our proposed task embedding can be computed efficiently, and that tasks with similar embeddings have similar best-performing architectures; (ii) AUTOTRANSFER significantly improves search efficiency with the transferred design priors, reducing the number of explored architectures by an order of magnitude. Finally, we release GNN-BANK-101, a large-scale dataset of detailed GNN training information of 120,000 task-model combinations to facilitate and inspire future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks are highly modular, requiring many design decisions to be made regarding network architecture and hyperparameters. These design decisions form a search space that is nonconvex and costly even for experts to optimize over, especially when the optimization must be repeated from scratch for each new use case. Automated machine learning (AutoML) is an active research area that aims to reduce the human effort required for architecture design that usually covers hyperparameter optimization and neural architecture search. AutoML has demonstrated success <ref type="bibr" target="#b47">(Zoph and Le, 2016;</ref><ref type="bibr" target="#b29">Pham et al., 2018;</ref><ref type="bibr" target="#b48">Zoph et al., 2018;</ref><ref type="bibr" target="#b5">Cai et al., 2018;</ref><ref type="bibr" target="#b13">He et al., 2018;</ref><ref type="bibr" target="#b10">Guo et al., 2020;</ref><ref type="bibr" target="#b8">Erickson et al., 2020;</ref><ref type="bibr" target="#b21">LeDell and Poirier, 2020)</ref> in many application domains.</p><p>Finding a reasonably good model for a new learning task<ref type="foot" target="#foot_0">1</ref> in a computationally efficient manner is crucial for making deep learning accessible to domain experts with diverse backgrounds. Efficient AutoML is especially important in domains where the best architectures/hyperparameters are highly sensitive to the task. A notable example is the domain of graph learning<ref type="foot" target="#foot_1">2</ref> . First, graph learning methods receive input data composed of a variety of data types and optimize over tasks that span an equally diverse set of domains and modalities such as recommendation <ref type="bibr" target="#b39">(Ying et al., 2018;</ref><ref type="bibr" target="#b12">He et al., 2020)</ref>, physical simulation <ref type="bibr" target="#b33">(Sanchez-Gonzalez et al., 2020;</ref><ref type="bibr" target="#b28">Pfaff et al., 2020)</ref>, and bioinformatics <ref type="bibr" target="#b46">(Zitnik et al., 2018)</ref>. This differs from computer vision and natural language processing where the input data has a predefined, fixed structure that can be shared across different neural architectures. Second, neural networks that operate on graphs come with a rich set of design choices and a large set of parameters to explore. However, unlike other domains where a few pre-trained architectures such as ResNet <ref type="bibr" target="#b11">(He et al., 2016)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> dominate the benchmarks, it has been shown that the best graph neural network (GNN) design is highly task-dependent <ref type="bibr" target="#b40">(You et al., 2020)</ref>.</p><p>Although AutoML as a research domain is evolving fast, existing AutoML solutions have massive computational overhead when finding a good model for a new learning task is the goal. Most present AutoML techniques consider each task independently and in isolation, therefore they require redoing the search from scratch for each new task. This approach ignores the potentially valuable architectural design knowledge obtained from the previous tasks, and inevitably leads to a high computational cost. The issue is especially significant in the graph learning domain <ref type="bibr" target="#b9">Gao et al. (2019)</ref>; <ref type="bibr" target="#b45">Zhou et al. (2019)</ref>, due to the challenges of diverse task types and the huge design space that are discussed above.</p><p>Here we propose AUTOTRANSFER<ref type="foot" target="#foot_2">3</ref> , an AutoML solution that drastically improves AutoML architecture search by transferring previous architectural design knowledge to the task of interest. Our key innovation is to introduce a task-model bank that stores the performance of a diverse set of GNN architectures and tasks to guide the search algorithm. To enable knowledge transfer, we define a task embedding space such that tasks close in the embedding space have similar corresponding top-performing architectures. The challenge here is that the task embedding needs to capture the performance rankings of different architectures on different datasets, while being efficient to compute. Our innovation here is to embed a task by using the condition number of its Fisher Information Matrix of various randomly initialized models and also a learning scheme with an empirical generalization guarantee. This way we implicitly capture the properties of the learning task, while being orders of magnitudes faster (within seconds). We then estimate the design prior of desirable models for the new task, by aggregating design distributions on tasks that are close to the task of interest. Finally, we initiate a hyperparameter search algorithm with the task-informed design prior computed.</p><p>We evaluate AUTOTRANSFER on six datasets, including both node classification and graph classification tasks. We show that our proposed task embeddings can be computed efficiently and the distance measured between tasks correlates highly (0.43 Kendall correlation) with model performance rankings. Furthermore, we present AUTOTRANSFER significantly improves search efficiency when using the transferred design prior. AUTOTRANSFER reduces the number of explored architectures needed to reach a target accuracy by an order of magnitude compared to SOTA. Finally, we release GNN-BANK-101-the first large-scale database containing detailed performance records for 120,000 task-model combinations which were trained with 16,128 GPU hours-to facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we summarize the related work on AutoML regarding its applications on GNNs, the common search algorithms, and pioneering work regarding transfer learning and task embeddings.</p><p>AutoML for GNNs. Neural architecture search (NAS), a unique and popular form of AutoML for deep learning, can be divided into two categories: multi-trial NAS and one-shot NAS. During multi-trial NAS, each sampled architecture is trained separately. GraphNAS <ref type="bibr" target="#b10">(Gao et al., 2020)</ref> and Auto-GNN <ref type="bibr" target="#b45">(Zhou et al., 2019)</ref> are typical multi-trial NAS algorithms on GNNs which adopt an RNN controller that learns to suggest better sets of configurations through reinforcement learning. One-shot NAS (e.g., <ref type="bibr" target="#b25">(Liu et al., 2018;</ref><ref type="bibr" target="#b30">Qin et al., 2021;</ref><ref type="bibr" target="#b24">Li et al., 2021)</ref>) involves encapsulating the entire model space in one super-model, training the super-model once, and then iteratively sampling sub-models from the super-model to find the best one. In addition, there is work that explicitly studies fine-grained design choices such as data augmentation <ref type="bibr" target="#b41">(You et al., 2021)</ref>, message passing layer type <ref type="bibr" target="#b6">(Cai et al., 2021;</ref><ref type="bibr" target="#b7">Ding et al., 2021;</ref><ref type="bibr">Zhao et al., 2021)</ref>, and graph pooling <ref type="bibr" target="#b35">(Wei et al., 2021)</ref>. Notably, AUTOTRANSFER is the first AutoML solution for GNNs that efficiently transfer design knowledge across tasks. HPO Algorithms. Hyperparameter Optimization (HPO) algorithms search for the optimal model hyperparameters by iteratively suggesting a set of hyperparameters and evaluating their performance. Random search samples hyperparameters from the search space with equal probability. Despite not  learning from previous trials, random search is commonly used for its simplicity and is much more efficient than grid search <ref type="bibr" target="#b1">(Bergstra and Bengio, 2012)</ref>. The TPE algorithm <ref type="bibr" target="#b2">(Bergstra et al., 2011)</ref> builds a probabilistic model of task performance over the hyperparameter space and uses the results of past trials to choose the most promising next configuration to train, which the TPE algorithm defines as maximizing the Expected Improvement value <ref type="bibr" target="#b18">(Jones, 2001)</ref>. Evolutionary algorithms <ref type="bibr" target="#b31">(Real et al., 2017;</ref><ref type="bibr" target="#b16">Jaderberg et al., 2017)</ref> train multiple models in parallel and replace poorly performing models with "mutated" copies of the current best models. AUTOTRANSFER is a general AutoML solution and can be applied in combination with any of these HPO algorithms.</p><p>Transfer Learning in AutoML. <ref type="bibr" target="#b36">Wong et al. (2018)</ref> proposed to transfer knowledge across tasks by reloading the controller of reinforcement learning search algorithms. However, this method assumes that the search space on different tasks starts with the same learned prior. Unlike AUTOTRANSFER, it cannot address the core challenge in GNN AutoML: the best GNN design is highly task-specific. GraphGym <ref type="bibr" target="#b40">(You et al., 2020)</ref> attempts to transfer the best architecture design directly with a metric space that measures task similarity. GraphGym <ref type="bibr" target="#b40">(You et al., 2020)</ref> computes task similarity by training a set of 12 "anchor models" to convergence which is computationally expensive. In contrast, AUTOTRANSFER designs light-weight task embeddings requiring minimal computations overhead. Additionally, <ref type="bibr" target="#b43">Zhao and Bilen (2021)</ref>; <ref type="bibr" target="#b24">Li et al. (2021)</ref> proposes to conduct architecture search on a proxy subset of the whole dataset and later transfer the best searched architecture on the full dataset. <ref type="bibr" target="#b17">Jeong et al. (2021)</ref> studies a similar setting in vision domain.</p><p>Task Embedding. There is prior research trying to quantify task embeddings and similarities. Similar to GraphGym, Taskonomy <ref type="bibr" target="#b42">(Zamir et al., 2018)</ref> estimates the task affinity matrix by summarizing final losses/evaluation metrics using an Analytic Hierarchy Process <ref type="bibr" target="#b32">(Saaty, 1987)</ref>. From a different perspective, Task2Vec <ref type="bibr" target="#b0">(Achille et al., 2019)</ref> generates task embeddings for a given task using the Fisher Information Matrix associated with a pre-trained probe network. This probe network is shared across tasks and allows Task2Vec to estimate the Fisher Information Matrix of different image datasets. Le et al. ( <ref type="formula">2022</ref>) extends a similar idea to neural architecture search. The aforementioned task embeddings cannot be directly applied to GNNs as the inputs do not align across datasets. AUTOTRANSFER avoids the bottleneck by using asymptotic statistics of the Fisher Information Matrix with randomly initiated weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION AND PRELIMINARIES</head><p>We first introduce formal definitions of data structures relevant to AUTOTRANSFER. For each training attempt on a task T (i) , we can record its model architecture M j , hyperparameters H j , and corresponding value of loss l j , i.e., (M j , H j , l j ). We propose to maintain a task-model bank to facilitate knowledge transfer to future novel tasks.</p><p>Definition 2 (Task-Model Bank) A task-model bank B is defined as a collection of tasks, each with multiple training attempts, in the form of</p><formula xml:id="formula_0">B = {(T (i) , {(M (i) j , H<label>(i) j , l (i)</label></formula><formula xml:id="formula_1">j )})}.</formula><p>AutoML with Knowledge Transfer. Suppose we have a task-model bank B. Given a novel task T (n) which has not been seen before, our goal is to quickly find a model that works reasonably well on the novel task by utilizing knowledge from the task-model bank.</p><p>In this paper, we focus on AutoML for graph learning tasks, though our developed technique is general and can be applied to other domains. We define the input graph as G = {V, E}, where V is the node set and E ? V ? V is the edge set. Furthermore, let y denote its output labels, which can be node-level, edge-level or graph-level. A GNN parameterized by weights ? outputs a posterior distribution P(G, y, ?) for label predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED SOLUTION: AUTOTRANSFER</head><p>In this section, we introduce the proposed AUTOTRANSFER solution. AUTOTRANSFER uses the task embedding space as a tool to understand the relevance of previous architectural designs to the target task. The designed task embedding captures the performance rankings of different architectures on different tasks while also being efficient to compute. We first introduce a theoretically motivated solution to extract a scale-invariant performance representation of each task-model pair. We use these representations to construct task features and further learn task embeddings. These embeddings form the task embedding space that we finally use during the AutoML search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BASICS OF THE FISHER INFORMATION MATRIX (FIM)</head><p>Given a GNN defined above, its Fisher Information Matrix (FIM) F is defined as</p><formula xml:id="formula_2">F = E G,y [? ? log P(G, y, ?) ? ? log P(G, y, ?) ].</formula><p>which formally is the expected covariance of the scores with respect to the model parameters. There are two popular geometric views for the FIM. First, the FIM is an upper bound of the Hessian and coincides with the Hessian if the gradient is 0. Thus, the FIM characterizes the local landscape of the loss function near the global minimum. Second, similar to the Hessian, the FIM models the loss landscape with respect not to the input space, but to the parameter space. In the information geometry view, if we add a small perturbation to the parameter space, we have KL(P(G, y, ?) P(G, y, ? + d?)) = d? F d?.</p><p>where KL(?, ?) stands for Kullback-Leibler divergence. It means that the parameter space of a model forms a Riemannian manifold and the FIM works as its Riemannian metric. The FIM thus allows us to quantify the importance of a model's weights in a way that is applicable to different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FIM-BASED TASK FEATURES</head><p>Scale-invariant Representation of Task-Model Pairs. We aim to find a scale-invariant representation for each task-model pair which will form the basis for constructing task features. The major challenge in using the FIM to represent GNN performance is that graph datasets do not have a universal, fixed input structure, so it is infeasible to find a single pre-trained model and extract its FIM. However, training multiple networks poses a problem as the FIMs computed for different networks are not directly comparable. We choose to use multiple networks but additionally propose to use asymptotic statistics of the FIM associated with randomly initialized weights. The theoretical justification for the relationship between the asymptotic statistics of the FIM and the trainability of neural networks was studied in <ref type="bibr" target="#b19">(Karakida et al., 2019;</ref><ref type="bibr" target="#b27">Pennington and Worah, 2018)</ref> to which we refer the readers. We hypothesize that such a measure of trainability encodes loss landscapes and generalization ability and thus correlates with final model performance on the task. Another issue that relates to input structures of graph datasets is that different models have different number of parameters. Despite some specially designed architectures, e.g., <ref type="bibr" target="#b22">(Lee et al., 2019;</ref><ref type="bibr" target="#b26">Ma et al., 2019)</ref>, most GNN architecture design can be represented as a sequence of pre-processing layers, message passing layers, and post-processing layers. Pre-process layers and post-process layers are Multilayer Perceptron (MLP) layers, of which the dimensions vary across different tasks due to different input/output structures. Message passing layers are commonly regarded as the key design for GNNs and the number of weight parameters can remain the same across tasks. In this light, we only consider the FIM with respect to the parameters in message passing layers so that the number of parameters considered stays the same for all datasets. We note that such formulation has its limitations, in the sense that it cannot cover all the GNN designs in the literature. We leave potential extensions with better coverage for future work. We further approximate the FIM by only considering the diagonal entries, which implicitly neglects the correlations between parameters. We note that this is common practice when analyzing the FIMs of deep neural networks, as the full FIM is massive (quadratic in the number of parameters) and infeasible to compute even on modern hardware. Similar to <ref type="bibr" target="#b27">Pennington and Worah (2018)</ref>, we consider the first two moments of FIM</p><formula xml:id="formula_3">m 1 = 1 n tr[F ] and m 2 = 1 n tr[F 2 ]<label>(1)</label></formula><p>and use ? = m 2 /m 2 1 as the scale-invariant representation. The computed ? is lower bounded by 1 and captures how concentrated the spectrum is. A small ? indicates the loss landscape is flat, and its corresponding model design enjoys fast first-order optimization and potentially better generalization. To encode label space information into each task, we propose to train only the last linear layer of each model on a given task, which can be done efficiently. The parameters in other layers are frozen after being randomly initialized. We take the average over R initializations to estimate the average ?.</p><p>Constructing Task Features. We denote task features as measures extracted from each task that characterize its important traits. The design of task features should reflect our final objective: to use these features to identify similar tasks and transfer the best design distributions. Thus, we select U model designs as anchor models and concatenate the scale-invariant representations ?u of each design as task features. To retain only the relative ranking among anchor model designs, we normalize the concatenated feature vector to a scale of 1. We let z f denote the normalized task feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">FROM TASK FEATURES TO TASK EMBEDDINGS</head><p>The task feature z f introduced above can be regarded as a means of feature engineering. We construct the feature vector with domain knowledge, but there is no guarantee it functions as anticipated. We thus propose to learn a projection function g(?) : R U ? R D that maps task feature z f to final task embedding z e = g(z f ). We do not have any pointwise supervision that can be used as the training objective. Instead, we consider the metric space defined by GraphGym. The distance function in GraphGym -computed using the Kendall rank correlation between performance rankings of anchor models trained on the two compared tasks -correlates nicely with our desired knowledge transfer objective. It is not meaningful to enforce that task embeddings mimic GraphGym's exact metric space, as GraphGym's metric space can still contain noise, or does not fully align with the transfer objective. We consider a surrogate loss that enforces only the rank order among tasks. To illustrate, let us consider tasks T (i) , T (j) , T (k) and their corresponding task embeddings, z</p><formula xml:id="formula_4">(i) e , z (j) e , z<label>(k)</label></formula><p>e . Note that z e is normalized to 1 so z</p><formula xml:id="formula_5">(i) e z (j)</formula><p>e measures the cosine similarity between tasks T (i) and T (j) . Let d g (?, ?) denote the distance estimated by GraphGym. We want to enforce</p><formula xml:id="formula_6">z (i) e z (j) e &gt; z (i) e z (k) e if d g (T (i) , T (j) ) &lt; d g (T (i) , T (k) ).</formula><p>To achieve this, we use the margin ranking loss as our surrogate supervised objective function:</p><formula xml:id="formula_7">L r (z (i) e , z (j) e , z (k) e , y) = max(0, -y ? (z (i) e z (j) e -z (i) e z (k) e ) + margin). (<label>2</label></formula><formula xml:id="formula_8">)</formula><p>Here if d g (T (i) , T (j) ) &lt; d g (T (i) , T (k) ), then we have its corresponding label y = 1, and y = -1 otherwise. Our final task embedding space is then a FIM-based metric space with cosine distance function, where the distance is defined as d e (T (i) , T (j) ) = 1 -z</p><formula xml:id="formula_9">(i) e z (j)</formula><p>e . Please refer to the detailed training pipeline at Algorithm 2 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">AUTOML SEARCH ALGORITHM WITH TASK EMBEDDINGS</head><p>To transfer knowledge to a novel task, a na?ve idea would be to directly carry over the best model configuration from the closest task in the bank. However, even a high Kendall rank correlation between model performance rankings of two tasks T (i) , T (j) does not guarantee the best model configuration in task T (i) will also achieve the best performance on task T (j) . In addition, since task similarities are subject to noise, this na?ve solution may struggle when there exist multiple reference tasks that are all highly similar.</p><p>To make the knowledge transfer more robust to such failure cases, we introduce the notion of design distributions that depend on top performing model designs and propose to transfer design distributions rather than the best design configurations. Formally, consider a task T (i) in the taskmodel bank B, associated with its trials {(M</p><formula xml:id="formula_10">(i) j , H (i) j , l (i) j )}.</formula><p>We can summarize its designs as a list of configurations C = {c 1 , . . . , c W }, such that all potential combinations of model architectures M and hyperparameters H fall under the Cartesian product of the configurations. For example, c 1 could be the instantiation of aggregation layers, and c 2 could be the start learning rate. We then define design distributions as random variables c 1 , c 2 , . . . , c W , each corresponding to a hyperparameter. Each random variable c is defined as the frequency distribution of the design choices used in the top K trials. We multiply all distributions for the individual configurations {c 1 , . . . , c W } to approximate the overall task's design distribution P(C|T (i) ) = w P(c w |T (i) ).</p><p>During inference, given a novel task T (n) , we select a close task subset S by thresholding task embedding distances, i.e., S = {T (i) |d e (T (n) , T (i) ) ? d thres }. We then derive the transferred design prior P t (C|T (n) ) of the novel task by weighting design distributions from the close task subset S.</p><formula xml:id="formula_11">P t (C|T (n) ) = T (i) ?S 1 de(T (n) ,T (i) ) P(C|T (i) ) T (i) ?S 1 de(T (n) ,T (i) )</formula><p>.</p><p>(3)</p><p>The inferred design prior for the novel task can then be used to guide various search algorithms.</p><p>The most natural choice for a few trial regime is random search. Rather than sampling each design configuration following a uniform distribution, we propose to sample from the task-informed design prior P t (C|T (n) ). Please refer to Appendix A to check how we augment other search algorithms.</p><p>For AUTOTRANSFER, we can preprocess the task-model bank</p><formula xml:id="formula_12">B into B p = {(D (i) , L (i) (?)), z (i)</formula><p>e , P(C|T (i) )} as our pipeline only requires using task embedding z (i) e and design distribution P(C|T (i) ) rather than detailed training trials. A detailed search pipeline is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Summary of AUTOTRANSFER search pipeline</head><p>Require: A processed task-model bank ?u ?mean(a</p><formula xml:id="formula_13">B p = {(D (i) , L (i) (?)), z (i) e , P(C|T (i) )}, a novel task T (n) = D (n) , L (n) (?) , U anchor models M 1 , ..., M U , R</formula><formula xml:id="formula_14">(1) u , a<label>(2)</label></formula><p>u , ..., a</p><formula xml:id="formula_15">(V ) u ) 8: end for 9: z (n) f ? -concat(? 1 , ?2 , ..., ?U ) 10: z (n) e ? -g(z (n) f ) 11: Select close task subset S ? -{T (i) |1 -z (n) e z (i)</formula><p>e ? d thres } 12: Get design prior P t (C|T (n) ) by aggregating subset S following Eq. 3 13: Start a HPO search algorithm with the task-informed design prior P t (C|T (n) )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTAL SETUP</head><p>Task-Model Bank: GNN-BANK-101.</p><p>To facilitate AutoML research with knowledge transfer, we collected GNN-BANK-101 as the first large-scale graph database that records reproducible design configurations and detailed training performance on a variety of tasks. Specifically, GNN-BANK-101 currently includes six tasks for node classification (AmazonComputers <ref type="bibr" target="#b34">(Shchur et al., 2018)</ref>, AmazonPhoto <ref type="bibr" target="#b34">(Shchur et al., 2018)</ref>, CiteSeer <ref type="bibr" target="#b38">(Yang et al., 2016)</ref>, CoauthorCS <ref type="bibr" target="#b34">(Shchur et al., 2018)</ref>, CoauthorPhysics <ref type="bibr" target="#b34">(Shchur et al., 2018)</ref>, Cora <ref type="bibr" target="#b38">(Yang et al., 2016)</ref>) and six tasks for graph classification (PROTEINS <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>, BZR <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>, COX2 <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>, DD <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>, ENZYMES <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>, IMDB-M <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>). Our design space follows <ref type="bibr" target="#b40">(You et al., 2020)</ref>, and we extend the design space to include various commonly adopted graph convolution and activation layers. We extensively run 10,000 different models for each task, leading to 120,000 total task-model combinations, and record all training information including train/val/test loss.</p><p>Benchmark Datasets. We benchmark AUTOTRANSFER on six different datasets following prior work <ref type="bibr" target="#b30">(Qin et al., 2021)</ref>. Our datasets include three standard node classification datasets (Coauthor-Physics <ref type="bibr" target="#b34">(Shchur et al., 2018)</ref>, CoraFull <ref type="bibr" target="#b3">(Bojchevski and G?nnemann, 2017)</ref> and OGB-Arxiv <ref type="bibr" target="#b14">(Hu et al., 2020)</ref>), as well as three standard benchmark graph classification datasets, (COX2 <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>, IMDB-M <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref> and PROTEINS <ref type="bibr" target="#b15">(Ivanov et al., 2019)</ref>). CoauthorPhysics and CoraFull are transductive node classification datasets, so we randomly assign nodes into train/valid/test sets following a 50%:25%:25% split <ref type="bibr" target="#b30">(Qin et al., 2021)</ref>. We randomly split graphs following a 80%:10%:10% split for the three graph classification datasets <ref type="bibr" target="#b30">(Qin et al., 2021)</ref>. We follow the default train/valid/test split for the OGB-Arxiv dataset <ref type="bibr" target="#b14">(Hu et al., 2020)</ref>. To make sure there is no information leakage, we temporarily remove all records related to the task from our task-model bank if the dataset we benchmark was collected in the task-model bank.</p><p>Baselines. We compare our methods with the state-of-the-art approaches for GNN AutoML. We use GCN and GAT with default architectures following their original implementation as baselines. For multi-trial NAS methods, we consider GraphNAS <ref type="bibr" target="#b10">(Gao et al., 2020)</ref>. For one-shot NAS methods, we include DARTS <ref type="bibr" target="#b25">(Liu et al., 2018)</ref> and GASSO <ref type="bibr" target="#b30">(Qin et al., 2021)</ref>. GASSO is designed for transductive settings, so we omit it for graph classification benchmarks. We further provide results of HPO algorithms based on our proposed search space as baselines: Random, Evolution, TPE <ref type="bibr" target="#b2">(Bergstra et al., 2011)</ref> and HyperBand <ref type="bibr" target="#b23">(Li et al., 2017)</ref>.</p><p>We by default allow searching 30 trials maximum for all the algorithms, i.e., an algorithm can train 30 different models and collect the model with the best accuracy. We use the default setting for one-shot To better understand the sample efficiency of AUTOTRANSFER, we plot the best test accuracy found at each trial in Figure <ref type="figure">3</ref> for OGB-Arxiv and TU-PROTEINS datasets. We notice that the advanced search algorithms (Evolution and TPE) have no advantages over random search at the few-trial regime since the amount of prior search data is not yet sufficient to infer potentially better design configurations. On the contrary, by sampling from the transferred design prior, AUTOTRANSFER achieves significantly better average test accuracy in the first few trials. The best test accuracy at trial 3 of AUTOTRANSFER surpasses its counterpart at trial 10 for every other method.</p><p>Figure <ref type="figure">3</ref>: Performance comparisons in the few-trial regime. At trial t, we plot the best test accuracy among all models searched from trial 1 to trial t. AUTOTRANSFER can reduce the number of trials needed to search by an order of magnitude (see also Table <ref type="table" target="#tab_3">4</ref> in Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ANALYSIS OF TASK EMBEDDINGS</head><p>Qualitative analysis of task features. To examine the quality of the proposed task features, we visualize the proposed task similarity matrix (Figure <ref type="figure" target="#fig_3">4</ref> (b)) along with the task similarity matrix (Figure <ref type="figure" target="#fig_3">4</ref> (a)) proposed in GraphGym. We show that our proposed task similarity matrix captures similar patterns as GraphGym's task similarity matrix while being computed much more efficiently by omitting training. We notice that the same type of tasks, i.e., node classification and graph classification, share more similarities within each group. As a sanity check, we examined that the closest task in the bank with respect to CoraFull is Cora. The top 3 closest tasks for OGB-Arxiv are AmazonComputers, AmazonPhoto, and CoauthorPhysics, all of which are node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization of projection function g(?).</head><p>To show the proposed projection function g(?) can generate task embeddings that can generalize to novel tasks, we conduct leave-one-out cross validation with all tasks in our task-model bank. Concretely, for each task considered as a novel task T (n) , we use the rest of the tasks, along with their distance metric d g (?, ?) estimated by GraphGym's exact but computationally expensive metric space, to train the projection function g(?). We calculate Kendall rank correlation over task similarities for Task Feature (without g(?)) and Task Embedding (with g(?)) against the exact task similarities. The average rank correlation and the standard deviation over ten runs is shown on Figure <ref type="figure" target="#fig_3">4</ref> (c). We find that with the proposed g(?), our task embeddings indeed correlate better with the exact task similarities, and therefore, generalize better to novel tasks.</p><p>Ablation study on alternative task space design. To demonstrate the superiority of the proposed task embedding, we further compare it with alternative task features. Following prior work <ref type="bibr" target="#b9">(Yang et al., 2019)</ref>, we use the normalized losses over the first 10 steps as the task feature. The results on OGB-Arxiv are shown in Table <ref type="table" target="#tab_2">2</ref>. Compared to AUTOTRANSFER's task embedding, the task feature induced by normalized losses has a lower ranking correlation with the exact metric and yields worse performance. Table <ref type="table" target="#tab_2">2</ref> further justifies the efficacy of using the Kendall rank correlation as the metric for task embedding quality, as higher Kendall rank correlation leads to better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we study how to improve AutoML search efficiency by transferring existing architectural design knowledge to novel tasks of interest. We introduce a task-model bank that captures the performance over a diverse set of GNN architectures and tasks. We also introduce a computationally efficient task embedding that can accurately measure the similarity between different tasks. We release GNN-BANK-101, a large-scale database that records detailed GNN training information of 120,000 task-model combinations. We hope this work can facilitate and inspire future research in efficient AutoML to make deep learning more accessible to a general audience.</p><p>Algorithm 2 Training Pipeline for the projection function g(?)</p><p>Require: Task features {z Sample T (i) , T (j) , T (k)   3: e , y) in Eq. 2 6: end for Social Impact. Our long-term goal is to provide a seamless GNN infrastructure that simplifies the training and deployment of ML models on structured data. Efficient and robust AutoML algorithms are crucial to making deep learning more accessible to people who are interested but lack deep learning expertise as well as those who lack the massive computational budget AutoML traditionally requires. We believe this paper is an important step to provide AI tools to a broader population and thus allow for AI to help enhance human productivity. The datasets we used for experiments are among the most widely-used benchmarks, which should not contain any undesirable bias. Besides, training/test losses and accuracies are highly summarized statistics that we believe should not incur potential privacy issues.</p><formula xml:id="formula_16">z (i) e , z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL RESULTS</head><p>Search efficiency. We summarize the average number of trials needed to surpass the average best accuracy found by TPE with 30 trials in Table <ref type="table" target="#tab_3">4</ref>. We show that AUTOTRANSFER reduces the number of explored architectures by an order of magnitude. Ablation study on number of anchor models and task embedding design. We empirically demonstrate how the number of anchor models affect the rank correlation in Table <ref type="table" target="#tab_4">5</ref>. While 3 anchor models are not enough to capture the task distance, we found that 9 and 12 have a satisfactory trade-off between capturing task distance and computational efficiency. Furthermore, we empirically in Table <ref type="table">6</ref> demonstrate that the learned task embedding space is superior to the proposed task feature space in terms of correlation as well as final search performance. Visualizing model designs. We visualized the transferred design distributions and ground truth design distributions on the TU-PROTEINS dataset in Figure <ref type="figure">5</ref>, as well as Coauthor-Physics dataset in Figure <ref type="figure">6</ref>. We could observe that the transferred design distributions have a positive correlation on most of the design choices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of AUTOTRANSFER. Left: We introduce GNN-BANK-101, a large database containing a diverse set of GNN architectures and hyperparameters applied to different tasks, along with their training/evaluation statistics. Middle: We introduce a task embedding space, where each point corresponds to a different task. Tasks close in the embedding space have similar corresponding top-performing models. Right: Given a new task of interest, we guide the AutoML search by referencing the design distributions of the most similar tasks in the task embedding space.</figDesc><graphic url="image-1.png" coords="3,294.48,106.35,108.60,106.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pipeline for extracting task embeddings. Left: To efficiently embed a task, we first extract task features by concatenating features measured from R randomly initialized anchor models. Then, we introduce a projection function g(?) with learned weights to transform the task features into task embeddings. Right: Training objective for optimizing g(?) with triplet supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>specifies the number of repeats. 1: for u = 1 to U do 2: for r = 1 to R do 3: Initialize weights ? for anchor model M u randomly 4: Estimate FIM F ? -E D [? ? log P(M u , y, ?) ? ? log P(M u , y, ?) ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) GraphGym's task similarity between all pairs of tasks (computed from the Kendall rank correlation between performance rankings of models trained on the two compared tasks), a higher value represents a higher similarity. (b) The proposed task similarity computed by computing the dot-product between extracted task features. (c) The Kendall rank correlation of similarity rankings of the other tasks with respect to the central task between the proposed method and GraphGym.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>f</head><label></label><figDesc>|T (i) } extracted for each task from the task-model bank. Distance measure d g (?, ?) estimated in GraphGym. 1: for each iteration do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1 if d g (T (i) , T (j) ) &lt; d g (T (i) , T (k) ) else -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Task-Model Bank (historical performance records) Task Embedding Space Design Distributions</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Agg</cell><cell></cell></row><row><cell>Task</cell><cell>Agg</cell><cell>Dim</cell><cell cols="2">Epoch ...</cell><cell>Val_loss</cell><cell>Dim</cell><cell></cell></row><row><cell>Cora</cell><cell>sum</cell><cell>64</cell><cell>80</cell><cell>...</cell><cell>0.22</cell><cell>Epoch</cell><cell></cell></row><row><cell>Cora TU-DD</cell><cell>mean sum</cell><cell>128 64</cell><cell>200 200</cell><cell>... ...</cell><cell>0.26 0.46</cell><cell>of interest Novel Task</cell><cell>...</cell></row><row><cell cols="2">TU-DD mean</cell><cell>128</cell><cell>200</cell><cell>...</cell><cell>0.86</cell><cell>Agg</cell><cell></cell></row><row><cell>TU-DD Arxiv</cell><cell>max mean</cell><cell>256 128</cell><cell>800 200</cell><cell>... ...</cell><cell>0.52 0.68</cell><cell>Dim</cell><cell></cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Epoch</cell><cell>...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons of AUTOTRANSFER and other baselines. We report the average test accuracy and the standard deviation over ten runs. With only 3 trials AUTOTRANSFER already outperform most SOTA baselines with 30 trials. DARTS and GASSO), as they only train a super-model once and can efficiently evaluate different architectures. We are mostly interested in studying the few-trial regime where most advanced search algorithms degrade to random search. Thus we additionally include a random search (3 trials) baseline where we pick the best model out of only 3 trials.5.2 EXPERIMENTS ON SEARCH EFFICIENCYWe evaluate AUTOTRANSFER by reporting the average best test accuracy among all trials considered over ten runs of each algorithm in Table1. The test accuracy collected for each trial is selected at the epoch with the best validation accuracy. By comparing results from random search (3 trials) and AUTOTRANSFER (3 trials), we show that our transferred task-informed design prior significantly improves test accuracy in the few-trial regime, and can be very useful in environments that are where computationally constrained. Even if we increase the number of search trials to 30, AUTOTRANSFER still demonstrates non-trivial improvement compared with TPE, indicating that our proposed pipeline has advantages even when computational resources are abundant. Notably, with only 3 search trials, AUTOTRANSFER surpasses most of the baselines, even those that use 30 trials.</figDesc><table><row><cell></cell><cell></cell><cell>Node</cell><cell></cell><cell></cell><cell>Graph</cell><cell></cell></row><row><cell>Method</cell><cell>Physics</cell><cell>CoraFull</cell><cell>OGB-Arxiv</cell><cell>COX2</cell><cell>IMDB</cell><cell>PROTEINS</cell></row><row><cell>GCN (30 trials)</cell><cell>95.88?0.16</cell><cell>67.12?0.52</cell><cell>70.46?0.18</cell><cell>79.23?2.19</cell><cell>50.40?3.02</cell><cell>74.84?2.82</cell></row><row><cell>GAT (30 trials)</cell><cell>95.71?0.24</cell><cell>65.92?0.68</cell><cell>68.82?0.32</cell><cell>81.56?4.17</cell><cell>49.67?4.30</cell><cell>75.30?3.72</cell></row><row><cell>GraphNAS (30 trials)</cell><cell>92.77?0.84</cell><cell>63.13?3.28</cell><cell>65.90?2.64</cell><cell>77.73?1.40</cell><cell>46.93?3.94</cell><cell>72.51?3.36</cell></row><row><cell>DARTS</cell><cell>95.28?1.67</cell><cell>67.59?2.85</cell><cell>69.02?1.18</cell><cell>79.82?3.15</cell><cell>50.26?4.08</cell><cell>75.04?3.81</cell></row><row><cell>GASSO 4</cell><cell>96.38</cell><cell>68.89</cell><cell>70.52</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Random (3 trials)</cell><cell>95.16?0.55</cell><cell>61.24?4.04</cell><cell>67.92?1.92</cell><cell>76.88?3.17</cell><cell>45.79?4.39</cell><cell>72.47?2.57</cell></row><row><cell>TPE (30 trials)</cell><cell>96.41?0.36</cell><cell>66.37?1.73</cell><cell>71.35?0.44</cell><cell>82.27?2.01</cell><cell>50.33?4.00</cell><cell>79.46?1.28</cell></row><row><cell>HyperBand (30 trials)</cell><cell>96.56?0.30</cell><cell>67.75?1.24</cell><cell>71.60?0.36</cell><cell>82.21?1.79</cell><cell>50.86?3.45</cell><cell>79.32?1.16</cell></row><row><cell>AUTOTRANSFER (3 trials)</cell><cell>96.64?0.42</cell><cell>69.27?0.76</cell><cell>71.42?0.39</cell><cell>82.13?1.59</cell><cell>52.33?2.13</cell><cell>77.81?2.19</cell></row><row><cell>AUTOTRANSFER (30 trials)</cell><cell>96.91?0.27</cell><cell>70.05?0.42</cell><cell>72.21?0.27</cell><cell>86.52?1.58</cell><cell>54.93?1.23</cell><cell>81.25?1.17</cell></row><row><cell>NAS algorithms (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the alternative task space design versus AUTOTRANSFER's task embedding. We report the average test accuracy and the standard deviation OGB-Arxiv over ten runs.</figDesc><table><row><cell></cell><cell cols="2">Kendall rank correlation Test accuracy</cell></row><row><cell>Alternative: Normalized Loss</cell><cell>-0.07?0.43</cell><cell>68.13?1.27</cell></row><row><cell>AUTOTRANSFER's Task Feature</cell><cell>0.18?0.30</cell><cell>70.67?0.52</cell></row><row><cell>AUTOTRANSFER's Task Embedding</cell><cell>0.43?0.22</cell><cell>71.42?0.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Average number of search trials needed to surpass the average best result found by TPE with 30 trials</figDesc><table><row><cell></cell><cell></cell><cell>Node</cell><cell></cell><cell></cell><cell>Graph</cell><cell></cell></row><row><cell></cell><cell>Physics</cell><cell>CoraFull</cell><cell>OGB-Arxiv</cell><cell>COX2</cell><cell>IMDB</cell><cell>PROTEINS</cell></row><row><cell>Num. of Trials</cell><cell>3</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>6</cell></row><row><cell>Accuracy</cell><cell cols="2">96.64?0.42 67.85?1.31</cell><cell>71.42?0.39</cell><cell cols="3">82.96 ? 1.75 52.33?2.13 80.21?1.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Average Kendall rank correlation of similarity rankings of the other tasks with respect to the central task between the proposed method and GraphGym.</figDesc><table><row><cell>Num. of anchor models</cell><cell>3</cell><cell>6</cell><cell>9</cell><cell>12</cell></row><row><cell>Task Feature</cell><cell cols="4">0.03?0.34 0.11?0.36 0.16?0.34 0.18?0.30</cell></row><row><cell>Task Embedding</cell><cell cols="4">0.12?0.28 0.26?0.30 0.36?0.24 0.43?0.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this paper, we refer to a task as a given dataset with an evaluation metric/loss, e.g., cross-entropy loss on node classification on the Cora dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We focus on the graph learning domain in this paper. AUTOTRANSFER can be generalized to other domains.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Source code is available at https://github.com/snap-stanford/AutoTransfer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Results come from the original paper<ref type="bibr" target="#b30">(Qin et al., 2021)</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We thank <rs type="person">Xiang Lisa Li</rs>, <rs type="person">Hongyu Ren</rs>, <rs type="person">Yingxin Wu</rs> for discussions and for providing feedback on our manuscript. We also gratefully acknowledge the support of <rs type="funder">DARPA</rs> under Nos. <rs type="grantNumber">HR00112190039</rs> (TAMI), N660011924033 (MCS); ARO under Nos. <rs type="grantNumber">W911NF-16-1-0342</rs> (MURI), <rs type="grantNumber">W911NF-16-1-0171</rs> (DURIP); <rs type="funder">NSF</rs> under Nos. <rs type="grantNumber">OAC-1835598</rs> (CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), <rs type="funder">NIH</rs> under No. <rs type="grantNumber">3U54HG010426-04S1</rs> (HuBMAP), <rs type="funder">Stanford Data Science Initiative</rs>, <rs type="institution">Wu Tsai Neurosciences Institute, Amazon, Docomo</rs>, <rs type="institution">GSK</rs>, <rs type="institution">Hitachi</rs>, <rs type="funder">Intel</rs>, <rs type="person">JPMorgan Chase</rs>, <rs type="person">Juniper Networks</rs>, <rs type="institution">KDDI</rs>, <rs type="funder">NEC</rs>, and <rs type="funder">Toshiba</rs>. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pPxFykU">
					<idno type="grant-number">HR00112190039</idno>
				</org>
				<org type="funding" xml:id="_sgzZsHf">
					<idno type="grant-number">W911NF-16-1-0342</idno>
				</org>
				<org type="funding" xml:id="_SA3PSuc">
					<idno type="grant-number">W911NF-16-1-0171</idno>
				</org>
				<org type="funding" xml:id="_Q8t4cFS">
					<idno type="grant-number">OAC-1835598</idno>
				</org>
				<org type="funding" xml:id="_P5Cj5R9">
					<idno type="grant-number">3U54HG010426-04S1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL IMPLEMENTATION DETAILS</head><p>Runtime analysis. We have empirically shown that AUTOTRANSFER can significantly improve search efficiency by reducing the number of trials needed to achieve reasonably good accuracy. The only overhead we introduced is the procedure of estimating task embeddings. Since we use a randomly initialized architecture, extracting each task feature only requires at most one forward and a few backward passes from a single minibatch of data. The wall-clock time depends on the size of the network and data structure. In our experiments, it typically takes a few seconds on an NVIDIA T4 GPU. We repeat the task feature extraction process 5 times for each anchor model, and for a total of 12 anchor models. Thus, the wall-clock time of the overhead for computing task embedding of novel task is within a few minutes. We note the length of this process is generally comparable to one trial of training on a small-sized dataset, and the time saved is much more significant for large-scale datasets.</p><p>Details for task-model-bank training. Our GNN model specifications are summarized in Table <ref type="table">3</ref>. Our code base was developed based on GraphGym <ref type="bibr" target="#b40">(You et al., 2020)</ref>. For all the training trials, We use the Adam optimizer and cosine learning rate scheduler (annealed to 0, no restarting). We use L2 regularization with a weight decay of 5e-4. We record losses and accuracies for training, validation and test splits every 20 epochs.</p><p>Training details for AUTOTRANSFER. We summarize the training procedure for the projection function g(?) in Algorithm 2. We set U = 12 and R = 5 throughout the paper. We use the same set of anchor model designs as those in GraphGym. We use a two-layer MLP with a hidden dimension of 16 to parameterize the projection function g(?). We use the Adam optimizer with a learning rate of 5e-3. We use margin = 0.1 and train the network for 1000 iterations with a batch size of 128. We adopt K = 16 when selecting the top K trials that are used to summarize design distributions.</p><p>Details for adapting TPE, evolution algorithms. We hereby illustrate how to compose the search algorithms with the transferred design priors. TPE is a Bayesian hyperparameter optimization method, meaning that it is initialized with a prior distribution to model the search space and updates the prior as it evaluates hyperparemeter configurations and records their performance. We replace this prior distribution with the task-informed design priors. As evolutionary algorithms generally initialize a large population and iteratively prune and mutate existing networks, we replace the random network initialization with the task-informed design priors. As we mainly focus on the few trial search regime, we set the fully random warm-up trials to 5 for both TPE and evolution algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL DISCUSSION</head><p>Limitations. In principle, AUTOTRANSFER leverages the correlation between model performance rankings among tasks to efficiently construct model priors. Thus, it is less effective if the novel task has large task distances with respect to all tasks in the task-model bank. In practice, users can continuously add additional search trials to the bank. As the bank size grows, it will be less likely that a novel task has a low correlation with all tasks in the bank.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task2vec: Task embedding for meta-learning</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6430" to="6439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bal?zs</forename><surname>K?gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03815</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking graph neural architecture search from message-passing</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6657" to="6666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diffmg: Differentiable meta graph search for heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="279" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autogluon-tabular: Robust and accurate automl for structured data</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shirkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Larroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Automated Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Graphnas: Graph neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09981</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu ; Zichao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/195</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2020/195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Christian</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">7 2020. 2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding isomorphism bias in graph data sets</title>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Sviridov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Population based training of neural networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09846</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geon Park, Eunyoung Hyung, Jinheon Baek, and Sung Ju Hwang. Taskadaptive neural network search with meta-contrastive learning</title>
		<author>
			<persName><forename type="first">Wonyong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayeon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21310" to="21324" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A taxonomy of global optimization methods based on response surfaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of global optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="345" to="383" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal statistics of fisher information in deep neural networks: Mean field approach</title>
		<author>
			<persName><forename type="first">Ryo</forename><surname>Karakida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun-Ichi</forename><surname>Amari</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1032" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fisher task distance and its application in neural architecture search</title>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Cat P Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Tarokh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="47235" to="47249" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">H2o automl: Scalable automatic machine learning</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Ledell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Automated Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6765" to="6816" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One-shot graph neural architecture search with dynamic search space</title>
		<author>
			<persName><forename type="first">Yanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zean</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8510" to="8517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with eigenpooling</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The spectrum of the fisher information matrix of a singlehidden-layer neural network</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Worah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning mesh-based simulation with graph networks</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph differentiable architecture search with structure learning</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Leon Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The analytic hierarchy process-what it is and how it is used</title>
		<author>
			<persName><forename type="first">Roseanna</forename><forename type="middle">W</forename><surname>Saaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical modelling</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-5</biblScope>
			<biblScope unit="page" from="161" to="176" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pooling architecture search for graph classification</title>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transfer learning with neural automl</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Oboe: Collaborative filtering for automl model selection</title>
		<author>
			<persName><forename type="first">Chengrun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dae</forename><forename type="middle">Won</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Udell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17009" to="17021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12121" to="12132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dataset condensation with differentiable siamese augmentation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12674" to="12685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06608</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
