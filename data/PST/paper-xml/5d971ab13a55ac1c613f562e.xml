<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Robustness of ReRAM-based Spiking Neural Network Accelerator with Stochastic Spike-timing-dependent-plasticity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-11">11 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xueyuan</forename><surname>She</surname></persName>
							<email>xshe@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Long</surname></persName>
							<email>yunlong@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
							<email>saibal.mukhopadhyay@ece.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Robustness of ReRAM-based Spiking Neural Network Accelerator with Stochastic Spike-timing-dependent-plasticity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-11">11 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1909.05401v1[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ReRAM</term>
					<term>spiking neural network</term>
					<term>spike-timingdependent-plasticity(STDP)</term>
					<term>process-in-memory(PIM)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spike-timing-dependent-plasticity (STDP) is an unsupervised learning algorithm for spiking neural network (SNN), which promises to achieve deeper understanding of human brain and more powerful artificial intelligence. While conventional computing system fails to simulate SNN efficiently, process-inmemory (PIM) based on devices such as ReRAM can be used in designing fast and efficient STDP based SNN accelerators, as it operates in high resemblance with biological neural network. However, the real-life implementation of such design still suffers from impact of input noise and device variation. In this work, we present a novel stochastic STDP algorithm that uses spiking frequency information to dynamically adjust synaptic behavior. The algorithm is tested in pattern recognition task with noisy input and shows accuracy improvement over deterministic STDP. In addition, we show that the new algorithm can be used for designing a robust ReRAM based SNN accelerator that has strong resilience to device variation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Spiking neural network (SNN) is a neuromorphic computing paradigm that mimics the behavior of animal nervous systems at the level of neurons and synapses. The development of SNN promotes deeper understanding of cognition system, and at the same time serves as a potential approach to achieving artificial neural networks (ANN) that are as efficient as the human brain, which outperforms state-of-the-art ANNs with only fraction of their power consumption <ref type="bibr" target="#b0">[1]</ref>. Originating from the synaptic modulation rule first studied by Hebb <ref type="bibr" target="#b1">[2]</ref>, which has been the foundation of research in learning and memory ever since 1949, spike-timing-dependent-plasticity or STDP <ref type="bibr" target="#b2">[3]</ref>, is now a widely adopted weight update algorithm in SNN. The temporal relationship between spiking events in the network as determined by STDP makes it possible for SNN to achieve learning with unlabeled input data, i.e. learning is unsupervised. In fact, SNN with STDP learning rule has been used in various computer vision related machine learning tasks <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> and shows accuracy results comparable with supervised neural network designs <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref>.</p><p>Conventional SNN accelerators suffer from the "memory wall" of von Neumann architecture, which hinders the development of systems with higher performance as memory access speed becomes the bottleneck. One promising solution to this problem, process-in-memory, or PIM, has been actively studied in the design of novel neural network accelerators. The integration of memory and computation supports potential breakthrough of the bandwidth barrier, and can fully take advantage of the parallel operations in SNN. Resistive random access memory (ReRAM), a type of memory device that supports non-volatile modification of resistance and exhibits behavior similar to biological synapses <ref type="bibr" target="#b8">[9]</ref>, is an ideal candidate for such PIM SNN accelerators. ReRAM also has the advantage of high read-and-wirte speed as well as simple structure <ref type="bibr" target="#b9">[10]</ref>. In addition, the non-volatile nature of ReRAM has the benefit of better energy efficiency as no power is need to maintain the information stored in memory.</p><p>Resistance of ReRAM is modified by voltage/current signal which changes the conducting mirco-structure between two electrodes. With controlled pulse width and amplitude of spike signal, ReRAM devices can be modified to continuous states of resistance <ref type="bibr" target="#b10">[11]</ref>, enabling analog representation of synapse conductance. Based on ReRAM, PIM design with a simple cell structure, in which only one ReRAM device is needed per synapse, can be achieved. Such design requires much less components compared to conventional CMOS implementation of SNN <ref type="bibr" target="#b11">[12]</ref>. For instance, in Indiveri's work the circuit design for SNN with STDP based on CMOS needs about 30 transistor per synapse <ref type="bibr" target="#b12">[13]</ref>.</p><p>Meanwhile, existing designs of ReRAM based SNN accelerators <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b14">[15]</ref> still face challenge caused by the stochastic nature in both the manufacturing process and operation of ReRAM. It is reported in Querlioz's work <ref type="bibr" target="#b4">[5]</ref> that device variation of ReRAM significantly degrades learning accuracy for MNIST dateset <ref type="bibr" target="#b15">[16]</ref>. The reason is that device variation leads to difficulties in achieving reliable STDP learning behavior as synapse conductance is stored in term of resistance of ReRAM, and each individual ReRAM device shows different resistance modulation characteristics. Beside the negative impact of device variation, noise is another factor that adversely affects learning in real life neural network applications. The event based system of SNN is sensitive to temporal and spatial distortion of spikes, and input data used in the learning process of SNN can contain noisy signal from sources such as the data collection process. In this work we propose a novel STDP learning rule and use it to implement an algorithmic approach to designing a robust ReRAM based PIM accelerator for SNN. This paper makes three key contributions:</p><p>• We propose a frequency-dependent (FD) stochastic STDP algorithm based on learning rules observed in neurophysiological experiments, which can be easily integrated into most STDP based SNNs. • Compared to determinstic STDP, the proposed design is able to achieve better accuracy when learning under noisy input conditions. The improvement can be observed across different noise types as well as a wide range of noise levels. • The proposed algorithm shows strong resilience to the impact of device variation in ReRAM based networks, while deterministic STDP suffers from accuracy drop under the same circumstances. In the following sections, we first discuss the fundamental theory and algorithm of SNN in section II, then present the PIM structure considered in this work in section III. In section IV we demonstrate results from learning the MNIST dataset and compare the performance of networks based on deterministic STDP and FD stochastic STDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SPIKING NEURAL NETWORK MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spiking Neuron Model</head><p>There are different models that are developed to capture the firing pattern of real biological neurons. Long <ref type="bibr" target="#b5">[6]</ref> shows that with parameter tuning it is possible the achieve similar spiking frequency in different neuron models, such as Hodkin-Huxley and integrate-and-fire (LIF), when operating under a specific range of input current. This indicates that for an event based network such as the one presented in this work, which uses spikes rather than membrane potential of neurons to encode information, a mathematically less complex model can be used to achieve same level of performance as the more complex ones. To optimize speed and power consumption of our network design, we choose to use LIF model in this work. The model is described by:</p><formula xml:id="formula_0">dv/dt = a + bv + cI (1) v = v reset , if v &gt; v threshold<label>(2)</label></formula><p>In the equation, a, b and c are decided based on the specific network settings. I is the sum of current signal from all synapses that connects to neuron m. I is evaluated by:</p><formula xml:id="formula_1">I m = N n=0 g n,m v pren<label>(3)</label></formula><p>Here g n,m is the conductance of the synapse connecting neuron n and m. And v pren is the voltage signal resulting from spike of neuron n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Synapse Model</head><p>In SNN, two neurons connected by a synapse are called pre-synaptic neuron and post-synaptic neuron. When the presynaptic neuron spikes, current signal is sent through the synapse to the post-synaptic neuron. Conductance of the Learning is achieved through modulating the conductance following an algorithm developed from the learning rule by Hebb <ref type="bibr" target="#b1">[2]</ref> and the temporal synaptic modification rule observed in hippocampus <ref type="bibr" target="#b16">[17]</ref>. A more detailed theoretical work by Gerstner explains learning from the perspective of spatial-temporal patterns of spikes <ref type="bibr" target="#b17">[18]</ref> and the algorithm is later named spike-timing-dependent-plasticity (STDP). Since then multiple experimental evidence of STDP has been observed <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>, making STDP a biologically plausible algorithm that is suitable for the purpose of this work.</p><p>With STDP learning rule integrated, the network is able to extract the causality between spikes of two connected neurons from their temporal relationship. As a result, the SNN can perform conductance update without using label of the input data. More specifically, there are two operations of STDP: long-term potentiation (LTP) and long-term depression (LTD). LTP is triggered when post-synaptic neuron spikes closely after a pre-synaptic neuron spike, indicating a causal relationship between the two events, and the conductance of the synapse is increased. On the other hand, when a postsynaptic neuron spikes before pre-synaptic spike arrives or without receiving a pre-synaptic spike at all, the synapse goes through LTD which decreases its conductance.</p><p>We choose to use STDP model presented by Querlioz <ref type="bibr" target="#b4">[5]</ref>, as it matches experimental measurement of memresistive devices <ref type="bibr" target="#b8">[9]</ref> [23] and has been tested in neural network application <ref type="bibr" target="#b4">[5]</ref>. The model is described by the following equations:</p><formula xml:id="formula_2">∆G p = α p e −βp(G−Gmin)/(Gmax−Gmin)<label>(4)</label></formula><formula xml:id="formula_3">∆G d = α d e −β d (Gmax−G)/(Gmax−Gmin) (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>∆G p is the magnitude of LTP actions, and ∆G d is the magnitude of LTD actions. α p , α d , β p , β d , G max and G min are parameters that are tuned based on other network configurations such as input matrix size, input spiking frequency and voltage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stochastic Behavior of Synapses</head><p>In stochastic SNN, the potentiation and depression of synapses are non-deterministic, and the probability of the two actions depends on the temporal relationship between presynaptic and post-synaptic spikes. We consider the algorithm presented in <ref type="bibr" target="#b23">[24]</ref> by Srinivasan, as probabilities are determined by: P pot = γ pot e (−∆t/(τpot)) (6)</p><formula xml:id="formula_5">P dep = γ dep e (∆t/(τ dep ))<label>(7)</label></formula><p>In the functions above, τ dep and τ pot are time constant parameters. ∆t is determined by subtracting the arrival time of the pre-synapse spike from that of the post-synapse spike (t post −t pre ), as shown in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>. Probability of potentiation P pot is higher with smaller ∆t, which indicates a stronger causal relationship. The probability of depression P dep is higher when ∆t is larger. γ pot and γ dep controls the peak value of probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proposed Stochastic STDP Model</head><p>Srinivasan's algorithm captures the exponential dependence on timing of synaptic behavior as observed in biological experiments <ref type="bibr" target="#b2">[3]</ref>. However it falls short to resolve the associative potentiation issue in STDP, which is directly related to the theoretical construct of the Hebbian synapse <ref type="bibr" target="#b1">[2]</ref>. Associativity in biological synaptic behavior is first reported by Levy <ref type="bibr" target="#b24">[25]</ref> and is proved to be an important role in the forming of classical conditioning of nervous systems <ref type="bibr">[26] [27]</ref>. As shown in Fig. <ref type="figure">2</ref> (a), associativity is a temporal specificity such that when a strong (in case of our SNN model, more frequent) input and a weak (less frequent) input into one neuron induce a postsynaptic spike, a following conductance modulation process is triggered. For timing of spikes received by the synapse t 1 , t 2 and t 3 , as shown in Fig. <ref type="figure">2 (b</ref>), ∆t = t 3 − t 2 for the weak input is smaller than ∆t = t 3 − t 1 for the strong input. The occurrence of the post-synaptic spike in this case has more correlation to the strong input, whereas the coincidental timing of spikes from the weak input presents an "illusive" causal relationship. Experimental result from Levy <ref type="bibr" target="#b27">[28]</ref> shows that in hippocampus, if the weak input spike arrives before (by as much as 20 ms) or at the same time with the strong input spike, LTP of the synapse transmitting the weak input spike is induced. If the weak input spike arrives after the strong input spike, LTD is induced. In this way the nervous system detects the "illusive" event and react properly.</p><p>In the context of STDP based SNNs, associativity can cause erroneous conductance modulation if unaccounted for. Therefore, we propose a frequency-dependent (FD) stochastic STDP that dynamically adjust the width of LTP/LTD window based on input signal frequency. The algorithm is now described by:</p><formula xml:id="formula_6">P pot = γ pot e (−∆t/(τpot(1+Φpot)))<label>(8)</label></formula><formula xml:id="formula_7">P dep = γ dep e (∆t/(τ dep (1+Φ dep )))<label>(9)</label></formula><formula xml:id="formula_8">Φ dep = φ dep f − f min f max − f min (<label>10</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">Φ pot = φ pot f − f min f max − f min (11)</formula><p>Shown in Fig. <ref type="figure">2 (c</ref>) are probability curves of this algorithm. When input spike originates from a weak input, the probability declines faster than that from a strong input. As a result, spike arriving time for weak input needs to be much closer to the post-synaptic spike to have the same probability of inducing LTP, i.e. the window for LTP is narrower for weak input. The same rule applies to LTD behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Network Architecture</head><p>In this work we use an SNN architecture as shown in Fig. <ref type="figure">3</ref>. This architecture is designed for pattern recognition tasks and consists of three layers. First is the input layer. Each neuron in this layer corresponds to one pixel in the input image. During learning process, the 8-bit pixel intensity from the input data is converted into spiking frequency over a range from f min to f max , as shown in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>, and the relationship is direct proportional. Each input unit keeps track of the corresponding spiking frequency and during learning period t learn of one image, constantly sends excitatory spike signals to the next layer at such frequency. The input layer connects to the spiking neuron layer (second layer) in an all-toall fashion (fully connected). The inhibition layer (third layer) has the same dimension as the second layer. For a neuron in the third layer at location &lt; i, j &gt;, there is only one input connection to it which is from the neuron at the same location, i.e. &lt; i, j &gt;, in the second layer, and its output connects to all second layer neurons except for &lt; i, j &gt;.</p><p>As a result, when one neuron in the second layer spikes, it sends excitatory signal to the one corresponding neuron in the third layer. The inhibitory neuron has low threshold that it activates immediately after receiving one excitatory signal, and then sends inhibitory signal to all other neurons in the second layer for a period of time t inh . Membrane potential of neurons that receive the inhibitory signal is decreased by a value of v inh , and can not spike during t inh . With the inhibition layer implemented, the network achieves a winner-take-all principle throughout the spiking neuron layer, preventing multiple neurons from learning the same pattern. STDP is applied to connections between the first and second layer. The conductance matrix of synapses connected to a single spiking neuron forms a learned template that contains features of one pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL OF HARDWARE ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ReRAM based SNN Accelerator Design</head><p>ReRAM is a two terminal device where the resistive switching layer (e.g. HfOx, NiO, TiO 2 , Al 2 O 3 , or their combinations) sandwitched between the top and bottom electrodes as shown in Fig. <ref type="figure" target="#fig_2">4 (c)</ref>. Device resistance can be modulated by applying set/reset voltage with different directions. In general, ReRAM can be defined as devices that exhibit a voltagecurrent characteristic <ref type="bibr" target="#b28">[29]</ref> as shown in Fig. <ref type="figure" target="#fig_2">4 (a)</ref>. Pinched at 0 Volt, the curve shows resistance variation of the ReRAM device under different injection current. Specifically, the set process happens when a positive voltage is applied on the top electrode which causes the oxygen ion migration, leaving oxygen vacancy to form a conductive filament (CF). On the other hand, device is reset by applying a reversed voltage which causes the recombination of oxygen ion and vacancy. The formation and rupture of conductive filament determines the device resistance, and thus, different data storage status.</p><p>The non-volatile memory and synapse like resistance modulation process of ReRAM make it an ideal candidate for STDP based SNN circuit. We consider the crossbar structure that has been explored in several ReRAM based SNN hardware implementations <ref type="bibr" target="#b4">[5]</ref> [14] <ref type="bibr" target="#b14">[15]</ref> to be a good paradigm of designing SNN accelerators. As shown in Fig. <ref type="figure" target="#fig_2">4</ref> (b), in this crossbar array wordlines (WL) and bitlines (BL) are connected by ReRAM at intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mapping SNN into ReRAM Accelerator</head><p>Based on the SNN architecture shown in Fig. <ref type="figure">3</ref>, an ReRAM crossbar PIM accelerator can be constructed accordingly for pattern recognition tasks. In our simulation that uses MNIST dataset, which has an image dimension of 28X28, a total of 784 input neurons are connected to WL. While with more spiking neurons the network is able to learn more distinct templates and therefore achieve better classification accuracy, in this work we consider a balanced option between area and performance, and use a second layer with 1000 spiking neuron. It is possible to use ReRAM as binary device by separating the high resistance state and low resistance state. However it would require more devices for each synapse to achieve enough bit width. Here we choose to utilize the continuous resistance states of ReRAM so that only one analog device is need per synapse. With ReRAM placed at each intersection of the crossbar structure, an all-to-all connection is achieved between input neuron and spiking neuron. The result network is shown in Fig. <ref type="figure" target="#fig_3">5</ref>   of input spikes and backpropagated spikes used in this design, as demonstrated in Fig. <ref type="figure" target="#fig_3">5</ref> (b), has been shown in <ref type="bibr" target="#b4">[5]</ref> to create resistance modulation behavior on ReRAM as described by ( <ref type="formula" target="#formula_2">4</ref>) and ( <ref type="formula" target="#formula_3">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Device Variation in ReRAM</head><p>Recent study of ReRAM shows that device variations is one of its intrinsic properties <ref type="bibr" target="#b29">[30]</ref>. Such variations is a result of the unique resistance modulation operation of ReRAM. In different resistance states of ReRAM distinct conduction mechanisms dominate. At low resistance state, the conduction is similar to metallic conductors as CF constitutes of dense oxygen vacancies. On the other hand, at high resistance state, gaps form between vacancies and electrons move in hopping motion as described by <ref type="bibr" target="#b30">[31]</ref>. In case of ultra-high resistance state, connections between CF and electrode are broken and electrons need to overcome large tunneling barrier during conduction.</p><p>The resistance modulation process thus changes the fundamental conduction mechanism of an ReRAM device across resistance states, and its randomness originates from a few different sources. First, in order to change device resistance, current impulses are injected through the ReRAM in a process called electric formation <ref type="bibr" target="#b31">[32]</ref>. During electric formation oxygen vacancies generation and redistribution happens randomly in the crystal structure, as a result of random walk <ref type="bibr" target="#b32">[33]</ref>. Those vacancies constitute CFs that are in stochastic geometry and have different resistance. In addition to that, electric formation also creates different gaps between vacancies that affect electron hopping conductivity. Similarly, it leads to varying tunneling distance between tip of CF and device terminals <ref type="bibr" target="#b33">[34]</ref> in ultra-high reistance state. Therefore, a fixed current impulse can produce different resistance modulation among ReRAM devices and even across operation cycles on the same device. Those variations lead to a statistical distribution of device resistance rather than a specific value. Lin and Li finds in their work that ReRAM device variation can be characterized by a normal or log-normal distribution <ref type="bibr" target="#b34">[35]</ref>  <ref type="bibr" target="#b35">[36]</ref>.</p><p>Prior works have demonstrated that the intrinsic device variation can be a major concern in terms of computing accuracy <ref type="bibr" target="#b36">[37]</ref>. In this paper, we test the proposed FD stochastic STDP algorithm for enhancing the system robustness for device variation. When simulating the SNN, we model such variation by applying parametrized Gaussian noise to the conductance value after each resistance modification process. Specifically, the distribution of an expected G is:</p><formula xml:id="formula_11">X G ∼ N (G, σ 2 dv = (γ dv G) 2 ) (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>The distribution is centered at the expected value, with standard deviation proportional to the expected G. And γ dv is a parameter used to control variation level.</p><p>IV. RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Settings</head><p>In order to test the SNN performance under noisy input condition, noise is added to the original image data prior to the learning process. As shown in Fig. <ref type="figure" target="#fig_4">6</ref>     </p><note type="other">Inference Noise Inference Noise</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning with Noisy Input Data</head><p>For the two types of noise tested, FD stochastic STDP based network shows better performance in almost all test conditions as shown in Fig. <ref type="figure" target="#fig_6">7 (a)</ref>. Accuracy gain at each learning noise level is shown in Fig. <ref type="figure" target="#fig_6">7</ref> (b) as discrete points and average accuracy gain over all inference noise conditions is shown as a line. It can be observed that accuracy improvement over network based on deterministic STDP is more prominent when learning input is more noisy, e.g. with non-noisy input learning the average of all test cases is 2.9% higher for FD stochastic STDP, while at 0 dB noise the improvement rises to 8.1%. A similar trend exists for salt-and-pepper noise, as average accuracy improvement increases from 2.7% at non-noisy input to 8.6% at 35% noise. It is also worth noting that networks which learn noisy input dataset show better performance at classifying test images with noise than those with no noise, and maximum accuracy result of a specific inference noise level comes from networks that receives similarly noisy input for learning. This is illustrated in the heatmaps from Fig. <ref type="figure" target="#fig_6">7</ref> (e). Each heatmap shows accuracy in one simulation setting (e.g. FD Stochastic STDP learning input with AWGN noise). The x-axis represents inference noise and y-axis represents test noise; higher accuracy is displayed in green and lower in red. The diagonal positions of each heatmap have higher accuracy than neighbor positions.</p><p>Visualization of conductance matrix after learning input with AWGN noise is shown in Fig. <ref type="figure" target="#fig_6">7 (c</ref>). At 5 dB input noise both deterministic and FD stochastic STDP are able to learn patterns with distinct features of different digits. Fig. <ref type="figure" target="#fig_6">7 (d)</ref> shows the distribution of conductance of all 786,000 synapses in the network, with deterministic STDP and FD stochastic STDP both showing distribution resembling normal distribution. Indeed, when looking at the extreme case where SNR decreases to -10 dB, learning abilities of two network configurations are negatively affected. However, with FD stochastic STDP, the network can extract more features from the noisy input, and achieve around 20% higher classification accuracy. This performance difference can also be reflected in the distribution of conductance in the network as shown in Fig. <ref type="figure" target="#fig_6">7 (d</ref>). The distribution of deterministic STDP, which has a wide and flat region, is less ideal than that of FD stochastic STDP which is closer to normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network with ReRAM Device Variation</head><p>In order to understand quantitatively how ReRAM device variation affects learning ability of SNN, we test networks with a range of variations and compare accuracy of the each setting. In this part, no input images noise is used, and device variation during inference has the same distribution as that for learning. Fig. <ref type="figure" target="#fig_7">8 (a)</ref> shows change in the result accuracy when ReRAM devices are at different levels of variation. Deterministic STDP and FD stochastic STDP both experience accuracy drop. However, accuracy of deterministic STDP drops around 3 times larger than FD stochastic STDP from no variation to a value of σ dv = 0.02, and degrade faster with increasing device variation. On the other hand, FD stochastic STDP is more robust to device variation. It has the capability to adapt to a wide range of randomness in the resistance modulation process and achieves above 80% accuracy with up-to σ dv = 0.1, at which deterministic STDP fails to provide meaningful accuracy.</p><p>FD stochastic STDP provides accuracy improvement as it is able to filter out modulation activities induced from spike pairs that have low causality, or from associative synaptic events, therefore statistically keeps the learned pattern from being disrupted by random modulation values. Apparently, such filtering can maintain a promising level of effectiveness under impact of an extended range of variations until the randomness becomes too high. Fig. <ref type="figure" target="#fig_7">8 (b)</ref> shows example patterns learned by networks with different device variation. Patterns from FD stochastic STDP network experience minor degradation from σ dv = 0.04 to σ dv = 0.12, while patterns from deterministic STDP network lose their distinct features rapidly as device variation increases. Fig. <ref type="figure" target="#fig_7">8</ref> (c) shows conductance distribution of deterministic STDP (left) and FD stochastic STDP based network with device variation of σ dv = 0.08.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning with Device Variation and Noisy Input</head><p>In this test, we investigate performance of the proposed algorithm when device variation and input noise are present at the same time. Accuracy is tested with images that have the same type and level of noise as the ones used in learning, and device variation during inference has the same distribution as that for learning. The result is shown in Table <ref type="table" target="#tab_1">I</ref>, with each column showing accuracy under certain device variation σ dv . FD stochastic STDP outperforms deterministic STDP in all test conditions, and achieves greater accuracy gain at higher device variation. For both algorithm, compared to when no noise is present in the input, accuracy drops slower with device variation when input is noisy. Also, comparing results from one specific device variation above zero, maxmium accuracy occurs when input is noisy, instead of when no noise is applied. This behavior shows that when learning noisy input the network experiences increase in the robustness, and smooth out the degradation of accuracy when device variation is present in STDP learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We present FD stochastic STDP as an algorithmic development that serves several purposes. It attempts to addresses the associativity behavior in nervous systems, which has been absent in conventional STDP algorithm used in SNN. It provides a more biologically plausible learning rule while at the same time achieves better performance. FD stochastic STDP also has the benefit of easy adoption and high efficiency as it uses the frequency information readily available in spiketiming based SNN. We show that FD stochastic STDP based SNN can achieve better accuracy when learning noisy input data compared to network based on deterministic STDP. It also exhibits higher robustness to randomness in conductance modulation resulting from device variation in ReRAM based SNN accelerators, thus provides an solution to designing reliable ReRAM based PIM accelerator for SNN algorithmically.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: (a) In STDP learning rule, when pre-synaptic spikes arrive at a synapse after post-synaptic spikes, ∆t is smaller than zero. (b) Conversion of input image to spike trains: darker pixels have higher frequency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: (a) The associative LTP behavior: convergent of two input (strong and weak) and induction of post-synaptic spike. (b) Spiking timing analysis of associative LTP. (c) Probability curves of the proposed frequency-dependent stochastic STDP with different input spike frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: (a) ReRAM voltage-current characteristic with the "pinched at zero" curve. (b) ReRAM crossbar array. (c) Probability density function (PDF) of ReRAM resistance distribution at low resistance state and high resistance state (top); the change of conductive filaments geometry during resistance modulation of ReRAM (bottom): as gaps forms between oxygen vacancies, resistance of ReRAM increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: (a) Crossbar array design of SNN circuit with ReRAM as synapses. (b) Waveform of input spike (signal from input neuron) and backpropagated spike (signal from spiking neuron).</figDesc><graphic url="image-20.png" coords="5,296.96,376.43,256.02,235.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: (a) Examples of different level of AWGN and salt-and-pepper noises used in this work. (b) Program flow of this SNN simulator.</figDesc><graphic url="image-23.png" coords="5,258.99,419.06,255.71,256.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: (a) Accuracy of learning input with AWGN noise (left) and with salt-and-pepper noise (right). (b) Accuracy gain of FD stochastic STDP over deterministic STDP for AWGN noise (left) and salt-and-pepper noise (right); red dots represents results from different inference noise levels and blue line tracks average accuracy gain over tests on all inference noise levels. (c) Visualization of patterns learned from 5 dB and -10 dB input dataset by two network configurations. (d) Distribution of synapse conductance for (i) deterministic STDP learning 5 dB noise input, (ii) FD stochastic STDP learning 5 dB noise input, (iii) deterministic STDP learning -10 dB noise of input, and (iv) FD stochastic STDP learning -10 dB noise input. (e) Heat maps of classification accuracy of (i) deterministic STDP learning input with AWGN noise, (ii) FD stochastic STDP learning input with AWGN noise, (iii) deterministic STDP learning input with salt-and-pepper noise and (iv) FD stochastic STDP learning input with salt-and-pepper noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: (a) Accuracy of the network with different standard deviation of device variation. (b) Examples of patterns learned by networks with no device variation (No DV) and different standard deviation of device variation. (c) Distribution of synapse conductance for deterministic STDP (left) and FD stochastic STDP (right) at σ dv = 0.08.</figDesc><graphic url="image-149.png" coords="7,58.42,256.32,128.78,63.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Accuracy results (%) for learning noisy input (top: deterministic STDP, bottom: FD stochastic STDP) and with device variation from σ dv = 0.04 to σ dv = 0.14 .</figDesc><table><row><cell></cell><cell cols="2">No DV 0.04 0.08</cell><cell>0.1</cell><cell>0.12 0.14</cell></row><row><cell>no noise</cell><cell>95.2</cell><cell cols="2">74.7 45.6 14.6 10.3 10.5</cell></row><row><cell>10 dB</cell><cell>89.5</cell><cell cols="2">79.0 56.0 19.1 13.9 11.7</cell></row><row><cell>5 dB</cell><cell>78.3</cell><cell cols="2">78.1 62.9 23.9 17.0 12.5</cell></row><row><cell>0 dB</cell><cell>71.6</cell><cell cols="2">69.6 60.8 25.4 22.7 14.5</cell></row><row><cell>5%</cell><cell>92.1</cell><cell cols="2">83.4 57.3 19.3 14.0 12.3</cell></row><row><cell>20%</cell><cell>76.4</cell><cell cols="2">74.2 52.0 22.9 16.7 13.2</cell></row><row><cell>35%</cell><cell>68.3</cell><cell cols="2">66.5 47.6 15.3 14.8 13.9</cell></row><row><cell></cell><cell cols="2">No DV 0.04 0.08</cell><cell>0.1</cell><cell>0.12 0.14</cell></row><row><cell>No noise</cell><cell>97.7</cell><cell cols="2">84.4 81.6 79.1 67.9 37.0</cell></row><row><cell>10 dB</cell><cell>95.9</cell><cell cols="2">87.4 83.4 80.4 76.8 43.3</cell></row><row><cell>5 dB</cell><cell>85.8</cell><cell cols="2">85.3 84.0 81.5 78.3 46.7</cell></row><row><cell>0 dB</cell><cell>78.8</cell><cell cols="2">76.8 74.6 76.7 77.2 51.6</cell></row><row><cell>5%</cell><cell>97.2</cell><cell cols="2">88.9 85.6 81.1 74.6 42.5</cell></row><row><cell>20%</cell><cell>78.1</cell><cell cols="2">77.6 76.7 78.1 75.7 49.3</cell></row><row><cell>35%</cell><cell>73.2</cell><cell cols="2">71.8 71.3 69.8 68.6 49.5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported in parts by Office of Naval Research Young Investigator Program, National Science Foundation, and Semiconductor Research Corporation nCORE Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Brain and high metabolic rate organ mass: Contributions to resting energy expenditure beyond fat-free mass</title>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">E</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Clinical Nutrition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Organization of Behavior; A Neuropsychological Theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Hebb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Attneave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synaptic Modification by Correlated Activity: Hebb&apos;s Postulate Revisited</title>
		<author>
			<persName><forename type="first">Guo-Qiang</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu-Ming</forename><surname>Poo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning of digit recognition using spike-timing-dependent plasticity</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2015-08">August. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Immunity to device variations in a spiking neural network with memristive nanodevices</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Querlioz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Dollfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Gamrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="288" to="295" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accelerating biophysical neural network simulation with region of interest based approximation</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueyuan</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automation Test in Europe Conference Exhibition (DATE)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel</forename><forename type="middle">Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nanoscale memristor device as synapse in neuromorphic systems</title>
		<author>
			<persName><forename type="first">Sung</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idongesit</forename><surname>Ebong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhavitavya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinaki</forename><surname>Bhadviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nano Letters</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An 8 mb multi-layered cross-point ReRAM macro with 443 MB/s write throughput</title>
		<author>
			<persName><forename type="first">Akifumi</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryotaro</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuuichirou</forename><surname>Ikeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memristor Resistance Modulation for Analog Applications</title>
		<author>
			<persName><forename type="first">Tsung-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><forename type="middle">H</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Electron Device Letters</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Realizing biological spiking network models in a configurable wafer-scale hardware system</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Fieres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karlheinz</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
				<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A VLSI array of low-power spiking neurons and bistable synapses with spiketiming dependent plasticity</title>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Chicca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Douglas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A spiking neuromorphic design with resistive crossbar</title>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beiye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Design Automation Conference on -DAC &apos;15</title>
				<meeting>the 52nd Annual Design Automation Conference on -DAC &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Plasticity in memristive devices for spiking neural networks</title>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Saïghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">G</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2323" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Longlasting potentiation of synaptic transmission in the dentate area of the unanaesthetized rabbit following stimulation of the perforant path</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Bliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Gardnermedwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Why spikes? Hebbian learning and retrieval of time-resolved excitation patterns</title>
		<author>
			<persName><forename type="first">Wulfram</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Ritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leo Van Hemmen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Lübke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Frotscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Sakmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Longterm synaptic plasticity between pairs of individual CA3 pyramidal cells in rat hippocampal slice cultures</title>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Debanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Beat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">M</forename><surname>Gähwiler</surname></persName>
		</author>
		<author>
			<persName><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synaptic plasticity in a cerebellum-like structure depends on temporal order</title>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiko</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsty</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A synaptically controlled, associative signal for Hebbian plasticity in hippocampal neurons</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analog memory and spike-timing-dependent plasticity characteristics of a nanoscale titanium oxide bilayer resistive switching device</title>
		<author>
			<persName><forename type="first">Kyungah</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Insung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjae</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Magnetic Tunnel Junction Based Long-Term Short-Term Stochastic Synapse for a Spiking Neural Network with On-Chip STDP Learning</title>
		<author>
			<persName><forename type="first">Gopalakrishnan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhronil</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synapses as associative memory elements in the hippocampal formation</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oswald</forename><surname>Steward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Classical conditioning in a simple withdrawal reflex in Aplysia californica</title>
		<author>
			<persName><forename type="first">T J</forename><surname>Carew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E T</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A cellular mechanism of classical conditioning in Aplysia: Activity-dependent amplification of presynaptic facilitation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Carew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Kandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal contiguity requirements for longterm associative potentiation/depression in the hippocampus</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Steward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="791" to="797" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">If its pinched its a memristor</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semiconductor Science and Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">104001</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Overview of Current Compliance Effect on Reliability of Nano Scaled Metal Oxide Resistive Random Access Memory Device</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Napolean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Electronic processes in non-crystalline materials</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nevill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Analytic Modeling for Nanoscale Resistive Filament Variation in ReRAM with Stochastic Differential Equation</title>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Eriguchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random walk on the infinite cluster of the percolation model</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Grimmett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kesten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probability Theory and Related Fields</title>
				<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the stochastic nature of resistive switching in metal oxide RRAM: Physical modeling, Monte Carlo simulation, and experimental characterization</title>
		<author>
			<persName><forename type="first">Shimeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximeng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S Philip</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical Digest -International Electron Devices Meeting, IEDM</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DL-RSIM: A Simulation Framework to Enable Reliable ReRAM-based Accelerators for Deep Learning</title>
		<author>
			<persName><forename type="first">Meng-Yao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ting</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design, ICCAD &apos;18</title>
				<meeting>the International Conference on Computer-Aided Design, ICCAD &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Variation-Aware, Reliability-Emphasized Design and Optimization of RRAM Using SPICE Model</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H P</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Design of Reliable DNN Accelerator with Un-reliable ReRAM</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automation Test in Europe Conference Exhibition (DATE)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fast and Low-Precision Learning in GPU-Accelerated Spiking Neural Network</title>
		<author>
			<persName><forename type="first">Xueyuan</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><surname>Design</surname></persName>
		</author>
		<title level="m">Automation Test in Europe Conference Exhibition (DATE)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
