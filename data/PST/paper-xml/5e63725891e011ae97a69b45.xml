<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ViSiL: Fine-grained Spatio-Temporal Video Similarity Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-20">20 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giorgos</forename><surname>Kordopatis-Zilos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Information Technologies Institute</orgName>
								<orgName type="institution" key="instit2">CERTH</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Mary University of London</orgName>
								<address>
									<addrLine>Mile End road</addrLine>
									<postCode>E1 4NS</postCode>
									<settlement>London</settlement>
									<region>Queen</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Information Technologies Institute</orgName>
								<orgName type="institution" key="instit2">CERTH</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
							<email>i.patras@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Mary University of London</orgName>
								<address>
									<addrLine>Mile End road</addrLine>
									<postCode>E1 4NS</postCode>
									<settlement>London</settlement>
									<region>Queen</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Kompatsiaris</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Information Technologies Institute</orgName>
								<orgName type="institution" key="instit2">CERTH</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ViSiL: Fine-grained Spatio-Temporal Video Similarity Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-20">20 Aug 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1908.07410v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we introduce ViSiL, a Video Similarity Learning architecture that considers fine-grained Spatio-Temporal relations between pairs of videos -such relations are typically lost in previous video retrieval approaches that embed the whole frame or even the whole video into a vector descriptor before the similarity estimation. By contrast, our Convolutional Neural Network (CNN)-based approach is trained to calculate video-to-video similarity from refined frame-to-frame similarity matrices, so as to consider both intra-and inter-frame relations. In the proposed method, pairwise frame similarity is estimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on regional CNN frame features -this avoids feature aggregation before the similarity calculation between frames. Subsequently, the similarity matrix between all video frames is fed to a four-layer CNN, and then summarized using Chamfer Similarity (CS) into a video-to-video similarity score -this avoids feature aggregation before the similarity calculation between videos and captures the temporal similarity patterns between matching frame sequences. We train the proposed network using a triplet loss scheme and evaluate it on five public benchmark datasets on four different video retrieval problems where we demonstrate large improvements in comparison to the state of the art. The implementation of ViSiL is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the popularity of Internet-based video sharing services, the volume of video content on the Web has reached unprecedented scales. For instance, YouTube reports almost two billion users and more than one billion hours of video viewed per day <ref type="foot" target="#foot_1">2</ref> . As a result, content-based video retrieval, which is an essential component in applications such as video filtering, recommendation, copyright protec- tion and verification, becomes increasingly challenging.</p><p>In this paper, we address the problem of similarity estimation between pairs of videos, an issue that is central to several video retrieval systems. A straightforward approach to this is to aggregate/pool frame-level features into a single video-level representation on which subsequently one can calculate a similarity measure. Such video-level representations include global vectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>, hash codes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref> and Bag-of-Words (BoW) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. However, this disregards the spatial and the temporal structure of the visual similarity, as aggregation of features is influenced by clutter and irrelevant content. Other approaches attempt to take into account the temporal sequence of frames in the similarity computation, e.g., by using Dynamic Programming <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>, Temporal Networks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17]</ref> and Temporal Hough Voting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. Another line of research considers spatio-temporal video representation and matching based on Recurrent Neural Networks (RNN) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> or in the Fourier domain <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2]</ref>. Such approaches may achieve high performance in certain tasks such as video alignment or copy detection, but not in more general retrieval tasks.</p><p>A promising direction is exploiting better the spatial and temporal structure of videos in the similarity calculation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. However, recent approaches either focused on the spatial processing of frames and completely disregarded temporal information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>, or considered global frame representations (essentially discarding spatial information) and then considered the temporal alignment among such frame representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref>. In this paper, we propose ViSiL, a video similarity learning network that considers both the spatial (intra-frame) and temporal (inter-frame) structure of the visual similarity. We first introduce a frameto-frame similarity that employs Tensor Dot (TD) product and Chamfer Similarity (CS) on region-level frame Convolutional Neural Network (CNN) features weighted with an attention mechanism. This leads to a frame-to-frame similarity function that takes into consideration region-to-region pairwise similarities, instead of calculating the similarity of frame-level embeddings where the regional details are lost. Then, we calculate the matrix with the similarity scores between each pair of frames between the two videos and use it as input to a four-layer CNN, that is followed by a Chamfer Similarity (i.e., a mean-max filter) at its final layer. By doing so, we learn the temporal structure of the frame-level similarity of relevant videos, such as the presence of diagonal structures in Figure <ref type="figure" target="#fig_0">1</ref>, and suppress spurious pairwise frame similarities that might occur. We evaluate ViSiL on several video retrieval problems, namely Near-Duplicate Video Retrieval (NDVR), Finegrained Incident and Event-based Video Retrieval (FIVR, EVR), and Action Video Retrieval (AVR) using public benchmark datasets, where in all cases, often by a large margin, it outperforms the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video retrieval approaches can be roughly classified into three categories <ref type="bibr" target="#b24">[25]</ref>, namely, methods that calculate similarity using global video representations, methods that account for similarities between individual video frames and methods that employ spatio-temporal video representations.</p><p>Methods in the first category extract a global video vector and use dot product or Euclidean distance to compute similarity between videos. Goa et al. <ref type="bibr" target="#b10">[11]</ref> extracted a video imprint for the entire video based on a feature alignment procedure that exploits the temporal correlations and removes feature redundancies across frames. Kordopatis et al. created visual codebooks for features extracted from intermediate CNN layers <ref type="bibr" target="#b19">[20]</ref> and employed Deep Metric Learning (DML) to train a network using a triplet loss scheme to learn an embedding that minimizes the distance between related videos and maximizes it between irrelevant ones <ref type="bibr" target="#b20">[21]</ref>. A popular direction is the generation of a hash code for the entire video combined with Hamming distance. Liong et al. <ref type="bibr" target="#b22">[23]</ref> employed a CNN architecture to learn binary codes for the entire video and trained it end-to-end based on the pair-wise distance of the generated codes and video class labels. Song et al. <ref type="bibr" target="#b30">[31]</ref> built a self-supervised video hashing system, able to capture the temporal relation between frames using an encoder-decoder scheme. These methods are typically outperformed by the ones of the other two categories.</p><p>Methods in the second category typically extract framelevel features to apply frame-to-frame similarity calculation and then aggregate them into video-level similarities. Tan et al. <ref type="bibr" target="#b31">[32]</ref> proposed a graph-based Temporal Network (TN) structure generated through keypoint frame matching, which is used for the detection of the longest shared path between two compared videos. Several recent works have employed modifications of this approach for the problem of partial-copy detection, combining it with global CNN features <ref type="bibr" target="#b16">[17]</ref> and a CNN+RNN architecture <ref type="bibr" target="#b13">[14]</ref>. Additionally, other approaches employ Temporal Hough Voting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> to align matched frames by means of a temporal Hough transform. These are often outperformed by TN in several related problems. Another popular solution is based on Dynamic Programming (DP) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. Such works calculate the similarity matrix between all frame pairs, and then extract the diagonal blocks with the largest similarity. To increase flexibility, they also allow limited horizontal and vertical movements. Chou et al. <ref type="bibr" target="#b6">[7]</ref> and Liu et al. <ref type="bibr" target="#b23">[24]</ref> combined DP with BoW matching to measure frame similarities. However, the proposed solutions are not capable of capturing a large variety of temporal similarity patterns due to their rigid aggregation approach. By contrast, ViSiL, which belongs to this category of methods, learns the similarity patterns in the CNN subnet that operates on the similarity matrix between the frame pairs. Methods in the third category extract spatio-temporal representations based on frame-level features and use them to calculate video similarity. A popular direction is to use the Fourier transform in a way that accounts for the temporal structure of video similarity. Revaud et al. <ref type="bibr" target="#b27">[28]</ref> proposed the Circulant Temporal Encoding (CTE) that encodes the frame features in a spatio-temporal representation with Fourier transform and thus compares videos in the frequency domain. Poullot et al. <ref type="bibr" target="#b25">[26]</ref> introduced the Temporal Matching Kernel (TMK) that encodes sequences of frames with periodic kernels that take into account the frame descriptor and timestamp. Baraldi et al. <ref type="bibr" target="#b1">[2]</ref> built a deep learning layer component based on TMK and set up a training process to learn the feature transform coefficients using a triplet loss that takes into account both the video similarity score and the temporal alignment. However, the previous methods rely on global frame representations, which disregard the spatial structure of similarity. Finally, Feng et al. <ref type="bibr" target="#b9">[10]</ref> developed an approach based on cross gated bilinear matching for video re-localization. They employed C3D features <ref type="bibr" target="#b33">[34]</ref> and built a multi-layer recurrent architecture that matches videos through attention weight-Figure <ref type="figure">2</ref>. Overview of the training scheme of the proposed architecture. A triplet of an anchor, positive and negative videos is provided to a CNN to extract regional features that are PCA whitened and weighted based on an attention mechanism. Then the Tensor Dot product is calculated for the anchor-positive and anchor-negative pairs followed by Chamfer Similarity to generate frame-to-frame similarity matrices. The output matrices are passed to a CNN to capture temporal relations between videos and calculate video-to-video similarity by applying Chamfer Similarity on the output. The network is trained with the triplet loss function. The double arrows indicate shared weights.</p><p>ing and factorized bilinear matching to locate related video parts. However, even though this approach performs well on video matching problems, it was found to be inapplicable for video retrieval tasks as will be shown in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Tensor Dot (TD): Having two tensors A ∈ R N1×N2×K and B ∈ R K×M1×M2 , their TD (also known as tensor contraction) is given by summing the two tensors over specific axes. Following the notation in <ref type="bibr" target="#b35">[36]</ref>, TD of two tensors is</p><formula xml:id="formula_0">C = A • (i,j) B<label>(1)</label></formula><p>where C ∈ R N1×N2×M1×M2 is the TD of the tensors, and i and j indicate the axes over which the tensors are summed.</p><p>In the given example i and j can only be 3 and 1 respectively, since they are the only ones of the same size (K).</p><p>Chamfer Similarity (CS): This is the similarity counterpart of Chamfer Distance <ref type="bibr" target="#b2">[3]</ref>. Considering two sets of items x and y with total number of N and M items respectively and their similarity matrix S ∈ R N ×M , CS is calculated as the average similarity of the most similar item in set y for each item in set x. This is formulated in Equation <ref type="formula" target="#formula_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS(x, y)</head><formula xml:id="formula_1">= 1 N N i=1 max j∈[1,M ] S(i, j)<label>(2)</label></formula><p>Note that CS is not symmetric, i.e. CS(x, y) = CS(y, x), however, that a symmetric variant SCS can be defined as, SCS(x, y) = (CS(x, y) + CS(y, x))/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ViSiL description</head><p>Figure <ref type="figure">2</ref> illustrates the proposed approach. We first extract features from the intermediate convolution layers of a CNN architecture by applying region pooling on the feature maps. These are further PCA-whitened and weighted based on an attention mechanism (section 4.1). Additionally, a similarity function based on TD and CS is devised to accurately compute the similarity between frames (section 4.2). A similarity matrix comprising all pairwise frame similarities is then fed to a CNN to train a video-level similarity model (section 4.3). This is trained with a triplet loss scheme (section 4.4) based on selected and automatically generated triplets from a training dataset (section 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature extraction</head><p>Given an input video frame, we apply Regional Maximum Activation of Convolution (R-MAC) <ref type="bibr" target="#b32">[33]</ref> on the activations of the intermediate convolutional layers <ref type="bibr" target="#b19">[20]</ref> given a specific granularity level L N , N ∈ {1, 2, 3, ...}. Given a CNN architecture with a total number of K convolutional layers, this process generates K feature maps M k ∈ R N ×N ×C k (k = 1, ..., K), where C k is the number of channels of the k th convolution layer. All extracted feature maps have the same resolution (N × N ) and are concatenated into a frame representation M ∈ R N ×N ×C , where C = C 1 + ... + C K . We also apply 2 -normalization on the channel axis of the feature maps, before and after concatenation. This feature extraction process is denoted as L N -iMAC. The extracted frame features retain the spatial information of frames at different granularities. We then employ PCA on the extracted frame descriptors to perform whitening and/or dimensionality reduction as in <ref type="bibr" target="#b14">[15]</ref>.</p><p>2 -normalization on the extracted frame descriptors result in all region vectors being equally considered in the similarity calculation. For instance, this would mean that a completely dark region would have the same impact on similarity with a region depicting a subject of interest. To avoid this issue, we weight the frame regions based on their saliency via a visual attention mechanism over region vectors inspired by methods from different research fields, i.e. document classification <ref type="bibr" target="#b36">[37]</ref>. To successfully adapt it to the needs of video retrieval, we build the following attention mechanism: given a frame representation M with region vector r ij :</p><formula xml:id="formula_2">M(i, j, •) ∈ R C , where i ∈ [1, N ], j ∈ [1, N ],</formula><p>we introduce a visual context unit vector u and use it to measure the importance of each region vector. To this end, we calculate the dot product between every r ij region vector, with the internal context vector u to derive the weight scores α ij . Since all vectors are unit norm, α ij will be in the range [−1, 1]. To retain region vectors' direction and change their norm, we divide the weight scores α ij by 2 and add 0.5 in order to be in range [0, 1]. Equation 3 formulates the weighting process.</p><formula xml:id="formula_3">α ij = u r ij , s.t. u = 1 r ij = (α ij /2 + 0.5)r ij<label>(3)</label></formula><p>All functions in the weighting process are differentiable; therefore u is learned through the training process. Unlike the common practice in the literature, we do not apply any normalization function on the calculated weights (e.g. softmax or division by sum) because we want to weight each vector independently. Also, we empirically found that, unlike other works, using a hidden layer in the attention module has negative effect on the system's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Frame-to-frame similarity</head><p>Given two video frames d, b, we apply CS on their region feature maps to calculate their similarity (Figure <ref type="figure" target="#fig_1">3</ref>). First, the regional feature maps <ref type="table">1</ref>. Architecture of the proposed network for video similarity learning. For the calculation of the output size, we assume that two videos with total number of X and Y frames are provided. decomposed into their region vectors d ij , b kl ∈ R C . Then, the dot product between every pair of region vectors is calculated, creating the similarity matrix of the two frames, and CS is applied on the similarity matrix to compute the frame-to-frame similarity.</p><formula xml:id="formula_4">M d , M b ∈ R N ×N ×C are Type Kernel size Output size Activ. / stride Conv 3×3 / 1 X × Y × 32 ReLU M-Pool 2×2 / 2 X/2 ×Y /2 × 32 - Conv 3×3 / 1 X/2 ×Y /2 × 64 ReLU M-Pool 2×2 / 2 X/4 ×Y /4 × 64 - Conv 3×3 / 1 X/4 ×Y /4 × 128 ReLU Conv 1×1 / 1 X/4 ×Y /4 × 1 - Table</formula><formula xml:id="formula_5">CS f (d, b) = 1 N 2 N i,j=1 max k,l∈[1,N ] d ij b kl<label>(4)</label></formula><p>This process leverages the geometric information captured by region vectors and provides some degree of spatial invariance. More specifically, the CNN extracts features that correspond to mid-level visual structures, such as object parts, and combined with CS, that by design disregards the global structure of the region-to-region matrix, constitutes a robust similarity calculation process against spatial transformations, e.g. spatial shift. This presents a trade-off between the preservation of the frame structure and invariance to spatial transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Video-to-video similarity</head><p>To apply frame-to-frame similarity on two videos q, p with X and Y frames respectively, we apply TD combined with CS on the corresponding video tensors Q and P and derive the frame-to-frame similarity matrix S qp f ∈ R X×Y . This is formulated in Equation <ref type="formula" target="#formula_6">5</ref>.</p><formula xml:id="formula_6">S qp f = 1 N 2 N 2 i=1 max j∈[1,N 2 ] Q • (3,1) P (•, i, j, •)<label>(5)</label></formula><p>where the TD axes indicate the channel dimension of the corresponding video tensors. In that way, we apply Equation 4 on every frame pair.</p><p>To calculate the similarity between two videos, the generated similarity matrix S qp f derived from the previous process is provided to a CNN network. The network is capable of learning robust patterns of within-video similarities at segment level. Table <ref type="table">1</ref> displays the architecture of the CNN architecture of the proposed ViSiL framework.</p><p>To calculate the final video similarity, we apply the hard tanh activation function on the values of the network output, which clips values within range [−1, 1]. Then, we apply CS to derive a single value as in Equation <ref type="formula" target="#formula_7">6</ref>.</p><formula xml:id="formula_7">CS v (q, p) = 1 X X i=1 max j∈[1,Y ] Htanh(S qp v (i, j))<label>(6)</label></formula><p>where S qp v ∈ R X ×Y is the output of the CNN network, and Htanh indicates the element-wise hard tanh function. The output of the network has to be bounded in order to accordingly set the margin in Equation <ref type="formula">7</ref>.</p><p>Similar to the frame-to-frame similarity calculation, this process is a trade-off between respecting video-level structure and being invariant to some temporal differences. As a result, different temporal similarity structures in the frameto-frame similarity matrix can be captured, e.g. strong diagonals or diagonal parts (i.e. contained sequences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Loss function</head><p>The target video similarity score CS v (q, p) should be higher for relevant videos and lower for irrelevant ones. To train our network we organize our video collection in video triplets (v, v + , v − ), where v, v + , v − stand for an anchor, a positive (i.e. relevant), and a negative (i.e. irrelevant) video respectively. To force the network to assign higher similarity scores to positive video pairs and lower to negative ones, we use the 'triplet loss', that is</p><formula xml:id="formula_8">L tr = max{0, CS v (v, v − ) − CS v (v, v + ) + γ} (7)</formula><p>where γ is a margin parameter.</p><p>In addition, we define a similarity regularization function that penalizes high values in the input of hard tanh that would lead to saturated outputs. This is an effective mechanism to drive the network to generate output matrices S v with values in the range [−1, 1], which is the clipping range of hard tanh. To calculate the regularization loss, we simply sum all values in the output similarity matrices that fall outside the clipping range (Equation <ref type="formula" target="#formula_9">8</ref>).</p><formula xml:id="formula_9">L reg = X i=1 Y j=1 | max{0, S qp v (i, j) − 1}|+ +| min{0, S qp v (i, j) + 1}|<label>(8)</label></formula><p>Finally, the total loss function is given in Equation <ref type="formula" target="#formula_10">9</ref>.</p><formula xml:id="formula_10">L = L tr + r * L reg (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>where r is a regularization hyperparameter that tunes the contribution of the similarity regularization to the total loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training ViSiL</head><p>Training the ViSiL architecture requires a training dataset with ground truth annotations at segment level. Using such annotations, we extract video pairs with related visual content to serve as anchor-positive pairs during training. Additionally, we artificially generate positive videos by applying a number of transformations on arbitrary videos. We consider three categories of transformation: (i) colour, including conversion to grayscale, brightness, contrast, hue, and saturation adjustment, (ii) geometric, including horizontal or vertical flip, crop, rotation, resize and rescale, and (iii) temporal, including slow motion, fast forward, frame insertion, video pause or reversion. During training, one transformation from each category is randomly selected and applied on the selected video.</p><p>We construct two video pools that consist of positive pairs. For each positive pair we then generate hard triplets, i.e. construct negative videos (hard negatives) with similarity to the anchor that is greater than the one between the anchor and positive videos. In what follows, we use a BoW approach <ref type="bibr" target="#b19">[20]</ref> to calculate similarities between videos.</p><p>The first pool derives from the annotated videos in the training dataset. Two videos with at least five second overlap constitute a positive pair. Let s be the similarity of the corresponding video segments. Videos with similarity (BoW-based <ref type="bibr" target="#b19">[20]</ref>) larger than s with either of the segments in the positive pair, constitute hard negatives. The second pool derives from arbitrary videos from the training dataset that are used to artificially generate positive pairs. Videos that are similar with the initial videos (similarity &gt; 0.1) are considered hard negatives. To avoid potential near-duplicates, we exclude videos with similarity &gt; 0.5 from the hard negative sets.</p><p>At each training epoch, we sample T triplets from each video pool. Due to GPU memory limitations, we do not feed the entire videos to the network. Instead, we select a random video snippet with total size of W frames from each video in the triplet, assuring that there are at least five seconds overlap between the anchor and the positive videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation setup</head><p>The proposed approach is evaluated on four retrieval tasks, namely Near-Duplicate Video Retrieval (NDVR), Fine-grained Incident Video Retrieval (FIVR), Event Video Retrieval (EVR), and Action Video Retrieval (AVR). In all cases, we report the mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>VCDB <ref type="bibr" target="#b15">[16]</ref> is used as the training dataset to generate triplets for training our models. It consists of 528 videos with 9,000 pairs of copied segments in the core dataset, and also a subset of 100,000 distractor videos.</p><p>CC WEB VIDEO <ref type="bibr" target="#b34">[35]</ref> simulates the NDVR problem. It consists of 24 query sets and 13,129 videos. We found several quality issues with the annotations, e.g. numerous positives mislabeled as negatives. Hence, we provide results on a 'cleaned' version of the annotations. We also use two evaluation settings, one measuring performance only on the query sets, and a second on the entire dataset.</p><p>FIVR-200K is used for the FIVR task <ref type="bibr" target="#b18">[19]</ref>. It consists of 225,960 videos and 100 queries. It includes three different retrieval tasks: a) the Duplicate Scene Video Retrieval (DSVR), b) the Complementary Scene Video Retrieval (CSVR), and c) the Incident Scene Video Retrieval (ISVR). For quick comparison of the different variants, we use FIVR-5K, a subset of FIVR-200K by selecting the 50 most difficult queries in the DSVR task (using <ref type="bibr" target="#b19">[20]</ref> to measure difficulty), and for each one randomly picking the 30% of annotated videos per label category.</p><p>EVVE <ref type="bibr" target="#b27">[28]</ref> was designed for the EVR problem. It consists of 2,375 videos, and 620 queries. However, we managed to download and process only 1897 videos and 503 queries (≈80% of the initial dataset) due to the unavailability of the remaining ones.</p><p>Finally, ActivityNet <ref type="bibr" target="#b3">[4]</ref>, reorganized based on <ref type="bibr" target="#b9">[10]</ref>, is used for the AVR task. It consists of 3,791 training, 444 validation and 494 test videos. The annotations contain the exact video segments that correspond to specific actions. For evaluation, we consider any pair of videos with at least one common label as related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>We extract one frame per second for each video. For all retrieval problems except for AVR, we are using the feature extraction scheme of Section 4.1 based on ResNet-50 <ref type="bibr" target="#b12">[13]</ref>, but for efficiency purposes only extract intermediate features from the output maps of the four residual blocks. Additionally, the PCA for the whitening layer is learned from 1M region vectors sampled from videos in VCDB. For AVR, we extract features from the last 3D convolutional layer of the I3D architecture <ref type="bibr" target="#b5">[6]</ref> by max-pooling on the spatial dimensions. We also tested I3D features for the other retrieval problems, but without any significant improvements.</p><p>For training, we feed the network with only one video triplet at a time due to GPU memory limitations. We employ Adam optimization <ref type="bibr" target="#b17">[18]</ref> with learning rate l = 10 −5 . For each epoch, T =1000 triplets are selected per pool. The model is trained for 100 epochs, i.e. 200K iterations, and the best network is selected based on mean Average Precision (mAP) on a validation set. Other parameters are set to γ = 0.5, r = 0.1 and W = 64. The weights of the feature extraction CNN and whitening layer remain fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we first compare the proposed frame-toframe similarity calculation scheme with several global features with dot product similarity (Section 6.1). We also provide an ablation study to evaluate the proposed approach under different configurations (Section 6.2). Finally, we compare the "full" proposed approach (denoted as ViSiL v ) with the best performing methods in the state-of-the-art (to the best of our knowledge) in each problem (Section 6.3). We have re-implemented two popular approaches that employ similarity calculation on frame-level representations, i.e. DP <ref type="bibr" target="#b6">[7]</ref> and TN <ref type="bibr" target="#b31">[32]</ref>. However, both of them were originally proposed in combination with hand-crafted features, which is an outdated practice. Hence, we combine them with the proposed feature extraction scheme and our frame-to-frame similarity calculation. We also implemented a naive adaptation of the publicly available Video re-localization (VReL) method <ref type="bibr" target="#b9">[10]</ref> to a retrieval setting, where we rank videos based on the probability of the predicted segment (Equation <ref type="formula">12</ref>in the original paper).</p><formula xml:id="formula_12">Features MAC [33] SPoC [1] R-MAC [33] GeM [12] iMAC [20] L 2 -iMAC L 2 -iMAC L 3 -iMAC L 3 -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Frame-to-frame similarity comparison</head><p>This section presents a comparison on FIVR-5K of the proposed feature extraction scheme against several global pooling schemes proposed in the literature. Dot product is used for similarity calculation. Video-level similarity for all runs is calculated with the application of the raw CS on the generated similarity matrices. The benchmarked feature extraction methods include the Maximum Activations of Convolutions (MAC) <ref type="bibr" target="#b32">[33]</ref>, Sum-Pooled Convolutional features (SPoC) <ref type="bibr" target="#b0">[1]</ref>, Regional Maximum Activation of Convolutions (R-MAC) <ref type="bibr" target="#b32">[33]</ref>, Generalized Mean (GeM) pooling <ref type="bibr" target="#b26">[27]</ref> (with initial p = 3 (cf. Table <ref type="table">1</ref> in <ref type="bibr" target="#b26">[27]</ref>) and intermediate Maximum Activation of Convolutions (iMAC) <ref type="bibr" target="#b19">[20]</ref>, which is equivalent to the proposed feature extraction for N = 1. Additionally, we evaluate the proposed scheme with region levels L N , N = 2, 3, and with two different region vector sizes for each region level. We use PCA to reduce region vectors' size, without applying whitening.</p><p>Table <ref type="table" target="#tab_0">2</ref> presents the results of the comparison on FIVR-5K. The proposed scheme with N = 3 (L 3 -iMAC) achieves the best results on all evaluation tasks by a large margin. Furthermore, it is noteworthy that the reduced features achieve competitive performance especially compared with the global descriptors of similar dimensionality. Hence, in settings where there is insufficient storage space, the reduced ViSiL features offer an excellent trade-off between retrieval performance and storage cost. We also tried to combine the proposed scheme with other pooling schemes, e.g. GeM pooling, but this had no noteworthy impact on the system's performance. Next, we will consider the best performing scheme (L 3 -iMAC without dimensionality reduction) as the base frame-to-frame similarity scheme ViSiL f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation study</head><p>We first evaluate the impact of each individual module of the architecture on the retrieval performance of ViSiL. Table <ref type="table" target="#tab_1">3</ref> presents the results of four runs with different configuration settings on FIVR-5K. The attention mechanism in the third run is trained using the main training process. The addition of each component offers additional boost to the performance of the system. The biggest improvement for the DSVR and CSVR tasks, 0.024 and 0.021 of mAP respectively, is due to employing a CNN model for refined video-level similarity calculation in ViSiL v . Also, considerable gains on the ISVR task (0.018 mAP) are due to the application of the attention mechanism. We also report results when the Symmetric Chamfer Distance (SCS) is used for both frame-to-frame and video-to-video similarity calculation (ViSiL sym ). Apparently, the non symmetric version of the CS works significantly better in this problem. Additionally, we evaluate the impact of the similarity regularization loss L reg of Equation <ref type="formula" target="#formula_9">8</ref>. This appears to have notable impact on the retrieval performance of the system. The mAP increases for all three tasks reaching an improvement of more than 0.02 mAP on DSVR and ISVR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>L reg DSVR CSVR ISVR 0.859 0.842 0.756 0.880 0.869 0.777 Table <ref type="table">4</ref>. Impact of similarity regularization on the performance of the proposed method on FIVR-5K.</p><p>In the supplementary material we assess the performance of similarity functions other than CS, the impact of different values of hyperparameters γ, W and r, and the computational complexity of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison against state-of-the-art 6.3.1 Near-duplicate video retrieval</head><p>We first compare the performance of ViSiL against state-ofthe-art approaches on several versions of CC WEB VIDEO <ref type="bibr" target="#b34">[35]</ref>. The proposed approach is compared with the publicly available implementation of Deep Metric Learning (DML) <ref type="bibr" target="#b20">[21]</ref>, the Circulant Temporal Encoding (CTE) <ref type="bibr" target="#b27">[28]</ref> (we report the results of the original paper) and our two reimplementations based on Dynamic Programming (DP) <ref type="bibr" target="#b6">[7]</ref> and Temporal Networks (TN) <ref type="bibr" target="#b31">[32]</ref>. The ViSiL v approach achieves the best performance compared to all competing systems in all cases except in the case where the original annotations are used (where CTE performs best). In that case, there were several erroneous annotations as explained above. When tested on the 'cleaned' version, ViSiL achieves almost perfect results in both evaluation settings. Moreover, it is noteworthy that our re-implementations of the state-of-the-art methods lead to considerably better results than the ones reported in the original papers, meaning that direct comparison with the originally reported results would be much more favourable for ViSiL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Fine-grained incident video retrieval</head><p>Here, we evaluate the performance of ViSiL against the state-of-the-art approaches on FIVR-200K <ref type="bibr" target="#b18">[19]</ref>. We compare with the best performing method reported in the original paper, i.e. Layer Bag-of-Words (LBoW) <ref type="bibr" target="#b19">[20]</ref> implemented with iMAC features from VGG <ref type="bibr" target="#b28">[29]</ref> and our two re-implementations of DP <ref type="bibr" target="#b6">[7]</ref> and TN <ref type="bibr" target="#b31">[32]</ref>. Furthermore, we tested our adaptation of VReL <ref type="bibr" target="#b9">[10]</ref>, but with no success (neither when training on VCDB nor on ActivityNet). As shown in Table <ref type="table" target="#tab_4">6</ref>, ViSiL v outperforms all competing systems, including DP and TN. Its performance is considerably higher on the DSVR task achieving almost 0.9 mAP. When conducting manual inspection of the erroneous results, we came across some interesting cases (among the top ranked irrelevant videos), which should actually be considered as positive results but were not labelled as such (Figure <ref type="figure" target="#fig_2">4</ref>).  <ref type="table">7</ref>. mAP comparison of three ViSiL setups with the LAMV <ref type="bibr" target="#b1">[2]</ref> on EVVE. The ordering of events is the same as in <ref type="bibr" target="#b27">[28]</ref>. Our results are reported on a subset of the videos (≈80% of the original dataset) due to unavailability of the full original dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Event video retrieval</head><p>For EVR, we compare ViSiL with the state-of-the-art approach Learning to Align and Match Videos (LAMV) <ref type="bibr" target="#b1">[2]</ref>. ViSiL performs well on the EVR problem, even without applying any query expansion technique, i.e. Average Query Expansion (AQE) <ref type="bibr" target="#b8">[9]</ref>. As shown in Table <ref type="table">7</ref>, ViSiL v achieves the best results on the majority of the events in the dataset. However, due to the fact that some of the videos are no longer available, we report results on the currently available ones that account for ≈80% of the original EVVE dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Action video retrieval</head><p>We also assess the performance of the proposed approach on ActivityNet <ref type="bibr" target="#b3">[4]</ref> reorganized based on <ref type="bibr" target="#b9">[10]</ref>. We compare with the publicly available DML approach <ref type="bibr" target="#b20">[21]</ref>, our re-implementations of DP <ref type="bibr" target="#b6">[7]</ref> and TN <ref type="bibr" target="#b31">[32]</ref>, and the adapted VReL <ref type="bibr" target="#b9">[10]</ref>. For all runs, we extracted features from I3D <ref type="bibr" target="#b5">[6]</ref>. The proposed approach with the symmetric similarity calculation ViSiL sym outperforms all other approaches by a considerable margin (0.035 mAP) to the second best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>DSVR CSVR ISVR LBoW <ref type="bibr" target="#b19">[20]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we proposed a network that learns to compute similarity between pairs of videos. The key contributions of ViSiL are a) a frame-to-frame similarity computation scheme that captures similarities at regional level and b) a supervised video-to-video similarity computation scheme that analyzes the frame-to-frame similarity matrix to robustly establish high similarities between video segments of the compared videos. Combined, they lead to a video similarity computation method that is accounting for both the fine-grained spatial and temporal aspects of video similarity. The proposed method has been applied to a number of content-based video retrieval problems, where it improved the state-of-art consistently and, in several cases, by a large margin. For future work, we plan to investigate ways of reducing the computational complexity and apply the proposed scheme for the corresponding detection problems (e.g. video copy detection, re-localization).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Depiction of the frame-to-frame similarity matrix and the CNN output of the ViSiL approach for two video pair examples: relevant videos that contain footage from the same incident (top), unrelated videos with spurious visual similarities (bottom).</figDesc><graphic url="image-1.png" coords="1,309.52,227.72,232.45,153.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of frame-level similarity calculation between two video frames. In this example, the frames are near duplicates.</figDesc><graphic url="image-3.png" coords="4,61.94,72.31,212.60,114.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Examples of challenging cases of related videos that were mistakenly not labelled as positives in FIVR-200K.</figDesc><graphic url="image-4.png" coords="8,52.02,180.23,232.44,216.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="3,69.60,72.01,453.53,190.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>mAP comparison of proposed feature extraction and similarity calculation against state-of-the-art feature descriptors with dot product for similarity calculation on FIVR-5K. Video similarity is computed based on CS on the derived similarity matrix.</figDesc><table><row><cell></cell><cell>Dims.</cell><cell cols="3">DSVR CSVR ISVR</cell></row><row><cell></cell><cell>2048</cell><cell>0.747</cell><cell>0.730</cell><cell>0.684</cell></row><row><cell></cell><cell>2048</cell><cell>0.735</cell><cell>0.722</cell><cell>0.669</cell></row><row><cell></cell><cell>2048</cell><cell>0.777</cell><cell>0.764</cell><cell>0.707</cell></row><row><cell></cell><cell>2048</cell><cell>0.776</cell><cell>0.768</cell><cell>0.711</cell></row><row><cell></cell><cell>3840</cell><cell>0.755</cell><cell>0.749</cell><cell>0.689</cell></row><row><cell></cell><cell>4x3840</cell><cell>0.814</cell><cell>0.810</cell><cell>0.738</cell></row><row><cell></cell><cell>4x512</cell><cell>0.804</cell><cell>0.802</cell><cell>0.727</cell></row><row><cell></cell><cell>9x3840</cell><cell>0.838</cell><cell>0.832</cell><cell>0.739</cell></row><row><cell>iMAC</cell><cell>9x256</cell><cell>0.823</cell><cell>0.818</cell><cell>0.738</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies on FIVR-5K. W and A stand for whitening and attention mechanism respectively.</figDesc><table><row><cell></cell><cell cols="3">DSVR CSVR ISVR</cell></row><row><cell>ViSiL f</cell><cell>0.838</cell><cell>0.832</cell><cell>0.739</cell></row><row><cell>ViSiL f +W</cell><cell>0.844</cell><cell>0.837</cell><cell>0.750</cell></row><row><cell cols="2">ViSiL f +W+A 0.856</cell><cell>0.848</cell><cell>0.768</cell></row><row><cell>ViSiL sym</cell><cell>0.830</cell><cell>0.823</cell><cell>0.731</cell></row><row><cell>ViSiL v</cell><cell>0.880</cell><cell>0.869</cell><cell>0.777</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>cc web cc web * cc web c cc web * mAP of three ViSiL setups and SoA methods on four different versions of CC WEB VIDEO. (</figDesc><table><row><cell>c</cell></row></table><note>* ) denotes evaluation on the entire dataset, and subscript c that the cleaned version of the annotations was used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.383 0.158 0.461 0.387 0.277 0.247 0.138 0.222 0.273 0.273 0.908 0.691 0.837 0.500 0.126 0.588 0.455 0.343 0.267 0.142 0.230 0.293 0.216 0.950 0.776 0.889 0.570 0.169 0.432 0.345 0.393 0.297 0.181 0.479 0.564 0.369 0.885 0.799 0.864 0.704 0.357 0.440 0.363 0.295 0.370 0.214 0.577 0.389 0.266 0.943 0.702 0.918 0.724 0.227 0.446 0.390 0.405 0.308 0.223 0.604 0.578 0.399 0.916 0.855 Table</figDesc><table><row><cell>Method</cell><cell>mAP</cell><cell>per event class</cell></row><row><cell>LAMV[2]</cell><cell>0.536</cell><cell>0.715 0</cell></row><row><cell>LAMV+QE [2]</cell><cell>0.587</cell><cell></cell></row><row><cell>ViSiL f</cell><cell>0.589</cell><cell></cell></row><row><cell>ViSiL sym</cell><cell>0.610</cell><cell></cell></row><row><cell>ViSiL v</cell><cell>0.631</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>mAP comparison of three ViSiL setups and state-of-theart methods on the three tasks of FIVR-200K.</figDesc><table><row><cell></cell><cell>0.710</cell><cell>0.675</cell><cell>0.572</cell></row><row><cell>DP [7]</cell><cell>0.775</cell><cell>0.740</cell><cell>0.632</cell></row><row><cell>TN [32]</cell><cell>0.724</cell><cell>0.699</cell><cell>0.589</cell></row><row><cell>ViSiL f</cell><cell>0.843</cell><cell>0.797</cell><cell>0.660</cell></row><row><cell>ViSiL sym</cell><cell>0.833</cell><cell>0.792</cell><cell>0.654</cell></row><row><cell>ViSiL v</cell><cell>0.892</cell><cell>0.841</cell><cell>0.702</cell></row><row><cell cols="2">Method DML [21] 0.705 mAP VReL [10] 0.209 DP [7] 0.621 TN [32] 0.648</cell><cell cols="2">Method ViSiL f ViSiL sym 0.745 mAP 0.652 ViSiL v 0.710</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8</head><label>8</label><figDesc></figDesc><table /><note>. mAP comparison of three ViSiL setups and four publicly available retrieval methods on ActivityNet based on the reorganization from<ref type="bibr" target="#b9">[10]</ref>.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/MKLab-ITI/visil</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://www.youtube.com/yt/about/press/, accessed 21 March 2019</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work is supported by the WeVerify H2020 project, partially funded by the EU under contract numbers 825297. The work of Ioannis Patras has been supported by EPSRC under grant No. EP/R026424/1. GKZ also thanks LazyProgrammer for the amazing DL courses.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary materials A. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Different similarity calculation functions</head><p>In this section, we compare the impact of different functions, other than CS, on the frame-to-frame (F2F) and video-to-video (V2V) similarity calculation. In general, CS can be considered to be equivalent to a Max-Pooling (MP) function followed by Average-Pooling (AP). A different combination could be the application of two AP functions. Table <ref type="table">9</ref> illustrates the results for different combinations of the core similarity functions of the proposed system on FIVR-5K. It is evident that the use of two AP functions for V2V does not work at all. The run with the two AP for F2F and CS for V2V achieves competitive mAP, but still lower than the run with CS in both functions as proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F2F</head><p>V2V  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Impact of hyperparameter values</head><p>In this section, we compare the impact of different values of hyperparameter γ, r and W , on the performance of the proposed system. As default values, we use the values reported in the original paper, i.e. γ = 0.5, r = 0.1 and W = 64, and change one at a time.</p><p>We first assess the impact of the margin parameter γ on the retrieval performance of the proposed approach. Figure <ref type="figure">5</ref>(a) illustrates the performance of the method trained with different margins on the three tasks of FIVR-5K. Regarding the DSVR task, one may notice that that the performance of the model improves as the margin parameter increases. However, this is not the case for the ISVR task. The approach reports high performance (mAP greater than 0.775) for small values of γ, i.e. within range [0.25, 0.5], but performance drops as γ increases.</p><p>Additionally, we assess the impact of the regularization parameter r on the retrieval performance of the proposed approach. Figure <ref type="figure">5</ref>(b) illustrates the performance of the method trained with different regularization parameters on the three tasks of FIVR-5K. On DSVR and CSVR tasks, the proposed approach achieves the best results for r = 1.0 with considerable margin from the second best, approxi-mately 0.003 mAP. However, on the ISVR task, the performance significantly dropped in comparison to the default value (r = 0.1). For values lower than the default, the proposed approach does not report competitive results on any evaluation task.</p><p>Finally, we assess the impact of the size of video snippet W on the retrieval performance of the proposed approach. Figure <ref type="figure">5</ref>(c) depicts the mAP of the method with different values of W on the three tasks of FIVR-5K dataset. Regarding the DSVR and CSVR tasks, it is evident that the larger the size of video snippets W the better the performance of the proposed methods. The run with W = 96 yields the best results on both tasks with 0.880 and 0.870 mAP, respectively. However, the system's performance on the ISVR task is independent of the size of video snippets used for training, since all runs report approximately the same mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Computational complexity</head><p>In this section, we compare the computational complexity of different setups of the proposed approach. The proposed method can be split in two distinct processes, an offline and an online. The offline process comprises the feature extraction from video frames, whereas the online one the similarity calculation between two videos.</p><p>In Table <ref type="table">10</ref>, we compare the MAC and iMAC runs (cf. Table <ref type="table">2</ref> of the paper) with the ViSiL f and ViSiL v in terms of execution time and performance. In that way, we assess the trade-off between the performance gain from the introduction of each component of the method, and the associated computational cost. The average length of videos in FIVR-5K is 103 seconds. All the experiments were executed on a machine with an Intel i7-4770K CPU and a GTX1070 GPU.</p><p>For the offline process, all runs need approximately the same time to extract frame features. The use of intermediate convolutional layer does not slow down the feature extraction process, since both MAC and iMAC needs 950 ms for feature extraction. The extraction of regional vectors (ViSiL f ) has minor impact on the speed, approximately 1% increase of the total extraction time. Also, the application of whitening and attention-based weighting does not significantly increases the extraction time; ViSiL v needs 80 ms more than ViSiL f per video.</p><p>Regarding the online process, the complexity of calculating the frame-to-frame similarity matrix between videos of M frames each, is O(M 2 N 2 ), where N is the number of regions per frame. This is to be compared to O(M 2 ) of frame-to-frame methods such as iMAC (where N = 1). Based on our experiments, the MAC and iMAC runs need less than 2.5 ms to calculate video similarity. The computation of the proposed frame-to-frame similarity matrix increases the execution time by 3.7 ms, which is more than a 150% increase (comparing iMAC and ViSiL f ). Finally, in  <ref type="table">10</ref>. mAP and execution time comparison of four versions of the proposed approach on FIVR-5K. The execution time of the offline process refers to the average feature extraction time per video. The execution time of the online process refers to the average time for the calculation of video similarity of video pairs.</p><p>ViSiL v , the second-stage CNN on the frame-to-frame similarity matrix takes 40% of the execution time, and further increasing it approximately by 3.5 ms but for a significant performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Examples</head><p>This section presents some visual examples of the outputs of the system components.</p><p>Figure <ref type="figure">6</ref> illustrates three visual examples of video frames coloured based on the attention weights of their regions vectors. Apparently, the proposed attention mechanism weights the frame regions independently based on their saliency. It assigns high weight values on the informationrich regions (e.g. the concert stage, the Mandalay Bay building); whereas, it assigns low values on regions that contain no meaningful object (e.g. solid dark regions).</p><p>Additionally, Figure <ref type="figure">7</ref> illustrates examples of the input frame-to-frame similarity matrix, the network output and the calculated video similarity of two compared videos for three video categories. The network is able to extract temporal patterns from the input frame-to-frame similarity matrices (e.g. strong diagonals, consistent parts with high sim-ilarity) and suppress the noisy (i.e. small inconsistent parts with high similarity values), in order to calculate the final video-to-video similarity precisely. Also, sampled frames from the compared videos are depicted for the better understanding of the different video relation types.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aggregating local deep features for image retrieval</title>
		<author>
			<persName><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LAMV: Learning to align and match videos with kernelized temporal layers</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7804" to="7813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Parametric correspondence and chamfer matching: Two new techniques for image matching</title>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">M</forename><surname>Harry G Barrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>SRI AI Center</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Million-scale near-duplicate video retrieval system</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Multimedia</title>
				<meeting>the 19th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="837" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Patternbased near-duplicate video retrieval and localization on webscale videos</title>
		<author>
			<persName><forename type="first">Chien-Li</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua-Tsung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suh-Yin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="395" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image-based approach to video copy detection with spatiotemporal post-filtering</title>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stable hyper-pooling and query expansion for event detection</title>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1825" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video re-localization</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ER3: A unified framework for event retrieval, recognition and recounting</title>
		<author>
			<persName><forename type="first">Zhanning</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2253" to="2262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised t-distributed video hashing and its deep hashing extension</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Y</forename><surname>Goulermas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5531" to="5544" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning spatial-temporal features for video copy detection by the combination of cnn and rnn</title>
		<author>
			<persName><forename type="first">Yaocong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="21" to="29" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Negative evidences and cooccurences in image retrieval: The benefit of pca and whitening</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="774" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VCDB: a large-scale database for partial copy detection in videos</title>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="357" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Partial copy detection in videos: A benchmark and an evaluation of popular methods</title>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="42" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04094</idno>
		<title level="m">Symeon Papadopoulos, Ioannis Patras, and Ioannis Kompatsiaris. FIVR: Fine-grained Incident Video Retrieval</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Near-duplicate video retrieval by aggregating intermediate cnn layers</title>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Multimedia Modeling</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Near-duplicate video retrieval with deep metric learning</title>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Kordopatis-Zilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">IR feature embedded bof indexing method for near-duplicate video retrieval</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanlin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangfeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congjun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep video hashing</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Venice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yap-Peng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1209" to="1219" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An image-based near-duplicate video retrieval and localization using improved edit distance</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Multimedia Tools and Applications</publisher>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="24435" to="24456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Near-duplicate video retrieval: Current research and future trends</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><forename type="middle">Wah</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal matching kernel with explicit feature maps</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Poullot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Tsukatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin'ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
				<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finetuning CNN image retrieval with no human annotation</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Event retrieval in large video collections with circulant temporal encoding</title>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2459" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiple feature hashing for real-time large scale near-duplicate video retrieval</title>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Tao Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Multimedia</title>
				<meeting>the 19th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised video hashing with hierarchical binary auto-encoder</title>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3210" to="3221" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable detection of partial near-duplicate videos by visual-temporal consistency</title>
		<author>
			<persName><forename type="first">Hung-Khoon</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM international conference on Multimedia</title>
				<meeting>the 17th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05879</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical elimination of near-duplicates from web video search</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international conference on Multimedia</title>
				<meeting>the 15th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="218" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
				<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
