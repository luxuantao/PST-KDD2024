<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolution of the Samsung Exynos CPU Microarchitecture Industrial Product</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brian</forename><surname>Grayson</surname></persName>
							<email>bgrayson@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeff</forename><surname>Rupley</surname></persName>
							<email>jrupley@austin.rr.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gerald</forename><forename type="middle">?</forename><surname>Zuraski</surname><genName>Jr</genName></persName>
							<email>gzuraskijr@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Quinnell</surname></persName>
							<email>eric.quinnell@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
							<email>djimenez@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tarun</forename><surname>Nakra</surname></persName>
							<email>tarun.nakra@amd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Kitchin</surname></persName>
							<email>pkitchin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Hensley</surname></persName>
							<email>ryan.hensley@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edward</forename><surname>Brekelbaum</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vikas</forename><surname>Sinha</surname></persName>
							<email>sinhavk@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ankit</forename><surname>Ghiya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">?</forename><surname>Amd</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Goodix</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evolution of the Samsung Exynos CPU Microarchitecture Industrial Product</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>microprocessor</term>
					<term>superscalar</term>
					<term>branch prediction</term>
					<term>prefetching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Samsung Exynos family of cores are highperformance "big" processors developed at the Samsung Austin Research &amp; Design Center (SARC) starting in late 2011. This paper discusses selected aspects of the microarchitecture of these cores -specifically perceptron-based branch prediction, Spectre v2 security enhancements, micro-operation cache algorithms, prefetcher advancements, and memory latency optimizations. Each micro-architecture item evolved over time, both as part of continuous yearly improvement, and in reaction to changing mobile workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Samsung started development of the Exynos family of cores beginning in late 2011, with the first generation core ("M1") supporting the newly introduced ARM v8 64-bit architecture <ref type="bibr" target="#b0">[1]</ref>. Several generations of the Exynos M-series CPUs <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>, here referred to as M1 through M6, are found most commonly in the Samsung Exynos-based Galaxy S7 through S20 smart phones, and are implemented in a variety of process nodes, from 14nm to 7nm. The cores support the ARMv8 Instruction Set Architecture <ref type="bibr" target="#b3">[4]</ref>, both AArch32 and AArch64 variants. The cores are superscalar out-of-order designs capable of up to 2.9GHz, and employ up to three levels of cache hierarchy in a multi-node cluster. Each Exynos M-series CPU cluster is complemented by an ARM Cortex-A series cluster in a big/little configuration in generations one through three, and in a big/medium/little configuration in subsequent generations.</p><p>Among the numerous details from this effort, this paper selectively covers:</p><p>? Yearly evolution of a productized core microarchitecture (M1 through M5) and future work (M6); ? Adaptations of the microarchitecture due to changes in mobile workloads;</p><p>This paper is part of the Industry Track of ISCA 2020's program. All of the authors were employed by Samsung at SARC while working on the cores described here. The authors express gratitude to Samsung for supporting the publication of this paper.</p><p>? Deep technical details within the microarchitecture. The remainder of this paper discusses several aspects of front-end microarchitecture (including branch prediction microarchitecture, security mitigations, and instruction supply) as well as details on the memory subsystem, in particular with regards to prefetching and DRAM latency optimization. The overall generational impact of these and other changes is presented in a cross-workload view of IPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>Simulation results shown here as well as the internal microarchitectural tuning work are based on a trace-driven cycleaccurate performance model that reflects all six of the implementations in this paper, rather than a silicon comparison, because M5 and M6 silicon were not available during the creation of this paper.</p><p>The workload is comprised of 4,026 traces, gathered from multiple CPU-based suites such as SPEC CPU2000 and SPEC CPU2006; web suites including Speedometer, Octane, BBench, and SunSpider; mobile suites such as AnTuTu and Geekbench; and popular mobile games and applications. Sim-Point <ref type="bibr" target="#b4">[5]</ref> and related techniques are used to reduce the simulation run time for most workloads, with a warmup of 10M instructions and a detailed simulation of the subsequent 100M instructions. Note that the same set of workloads is utilized here across all of the generations; some of the more recent workloads did not exist during the development of M1, and some earlier workloads may have become somewhat obsolete by M6, but keeping the workload suite constant allows a fair cross-generational comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MICROARCHITECTURAL OVERVIEW</head><p>Several of the key microarchitectural features of the M1 through M6 cores are shown in Table <ref type="table">I</ref>. Although the cores were productized at different frequencies, performance results in this paper come from simulations where all cores were run at 2.6GHz, so that per-cycle comparisons (IPC, load latencies) are valid to compare. Selected portions of the microarchitecture that are not covered elsewhere will be briefly discussed in this section.</p><p>For data translation operations, M3 and later cores contain a fast "level 1.5 Data TLB" to provide additional capacity at much lower latency than the much-larger L2 TLB.</p><p>Note that, according to the table, there were no significant resource changes from M1 to M2. However, there were several efficiency improvements, including a number of deeper queues not shown in Table <ref type="table">I</ref>, that resulted in the M2 speedups shown later in this paper.</p><p>Both the integer and floating-point register files utilize the physical-register-file (PRF) approach for register renaming, and M3 and newer cores implement zero-cycle integer registerregister moves via rename remapping and reference-counting.</p><p>The M4 core and beyond have a "load-load cascading" feature, where a load can forward its result to a subsequent load a cycle earlier than usual, giving the first load an effective latency of 3 cycles.</p><p>In general, most resources are increased in size in succeeding generations, but there are also several points where sizes were reduced, due to evolving tradeoffs. Two examples are M3's reduction in L2 size due to the change from shared to private L2 as well as the addition of an L3, and M4's reduction in L3 size due to changing from a 4-core cluster to a 2-core cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BRANCH PREDICTION</head><p>The Samsung dynamic branch prediction research is rooted in the Scaled Hashed Perceptron (SHP) approach <ref type="bibr" target="#b5">[6]</ref> [7] <ref type="bibr" target="#b7">[8]</ref> [9] <ref type="bibr" target="#b9">[10]</ref>, advancing the state-of-the-art perceptron predictor over multiple generations. The prediction hardware uses a Branch Target Buffer (BTB) approach across both a smaller, 0-bubble TAKEN micro-BTB (?BTB) with a local-history hashed perceptron (LHP) and a larger 1-2 bubble TAKEN main-BTB (mBTB) with a full SHP. The hardware retains learned information in a Level-2 BTB (L2BTB) and has a virtual address-based BTB (vBTB) in cases of dense branch lines that spill the normal BTB capacity. Function returns are predicted with a Return-Address Stack (RAS) with standard mechanisms to repair multiple speculative pushes and pops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Initial direction</head><p>The initial branch predictor design had two performance goals: use a state-of the-art conditional predictor to reduce MPKI, and support up to two predictions per clock for the common case of a leading NOT-TAKEN branch. The latter avoids incurring an extra cycle (or more) for many of the NOT-TAKEN branches. For the workloads discussed here, the lead branch is TAKEN in 60% of all cases, the second paired branch TAKEN 24% of all cases, and two sequential NOT-TAKEN 16% of all cases.</p><p>The first generation SHP consists of eight tables of 1,024 weights each, in sign/magnitude representation, along with a "local BIAS" weight kept in the BTB entry for each branch.</p><p>Each of the SHP weight tables is indexed using an XOR hash <ref type="bibr" target="#b10">[11]</ref> composed of three items:</p><p>1) A hash of the global history (GHIST) <ref type="bibr" target="#b11">[12]</ref> pattern in a given interval for that table. The GHIST records the outcome of a conditional branch as one bit. 2) A hash of the path history (PHIST) <ref type="bibr" target="#b12">[13]</ref> in a given interval for that table. The PHIST records three bits, bits two through four, of each branch address encountered. 3) A hash of the program counter (PC) for the branch being predicted. The GHIST and PHIST intervals were determined empirically using a stochastic search algorithm, taking into consideration both the diminishing returns of longer GHIST (shown in Figure <ref type="figure" target="#fig_0">1</ref> using the publicly-available CBP5 <ref type="bibr" target="#b13">[14]</ref> workloads), and the cost of maintaining GHIST state. M1 utilized a 165-bit GHIST length, and an 80-bit PHIST length. To compute an SHP prediction, the signed BIAS weight in the BTB entry is doubled <ref type="bibr" target="#b7">[8]</ref>, and added to the sum of the signed weights read out from each of the 8 tables according to the hashed history intervals. If the resulting sum is at least 0, then the branch is predicted TAKEN; otherwise, it is predicted not taken.</p><p>The predictor is updated on a misprediction, or on a correct prediction where the absolute value of the sum fails to exceed a threshold trained using the threshold training algorithm from the O-GEHL predictor <ref type="bibr" target="#b14">[15]</ref>. To update the SHP predictor, the counters used during prediction are incremented if the branch was taken, or decremented otherwise, saturating at the maximum or minimum counter values. Always-TAKEN branches -both unconditional branches, and conditional branches that have always been taken so far -do not update the SHP weight tables, to reduce the impact of aliasing <ref type="bibr" target="#b15">[16]</ref>.</p><p>The main BTBs are organized into 8 sequential discovered branches per 128B cacheline as seen in Figure <ref type="figure" target="#fig_1">2</ref>, based on the gross average of 5 instructions per branch. As mentioned above, additional dense branches exceeding the first 8 spill to a virtual-indexed vBTB shown in the right of Figure <ref type="figure" target="#fig_1">2</ref> at an additional access latency cost.</p><p>The indirect branch predictor is based on the virtual program counter (VPC) predictor <ref type="bibr" target="#b16">[17]</ref>. VPC breaks up an indirect prediction into a sequence of conditional predictions of "virtual  PCs" that each consult SHP, with each unique target up to a design-specified maximum "chain" stored in the BTB at the program order of the indirect branch. Figure <ref type="figure" target="#fig_2">3</ref> shows the VPC algorithm with a maximum of 16 targets per indirect branch, several of which are stored in the shared vBTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. First refinement during M1 design</head><p>During early discussions, the two-bubble penalty on TAKEN branches was clearly identified as limiting in certain scenarios, such as tight loops or code with small basic blocks and predictable branches. The baseline predictor is therefore augmented with a micro-BTB (?BTB) that has zero-bubble throughput, but limited capacity. This predictor is graph-based <ref type="bibr" target="#b17">[18]</ref> specifically using an algorithm to first filter and identify common branches with common roots or "seeds" and then learn both TAKEN and NOT-TAKEN edges into a "graph" across several iterations, as seen in the example in Figure <ref type="figure" target="#fig_3">4</ref>. Difficult-to-predict branch nodes are augmented with use of a local-history hashed perceptron (LHP). When a small kernel is confirmed as both fully fitting within the ?BTB and predictable by the ?BTB, the ?BTB will "lock" and drive the pipe at 0 bubble throughput until a misprediction, with predictions checked by the mBTB and SHP. Extremely highly confident predictions will further clock gate the mBTB for large power savings, disabling the SHP completely.</p><p>The above description completes an overview of the branch prediction hardware in the M1 first-generation core. The M2 core made no significant changes to branch prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. M3 Branch Prediction: Throughput</head><p>For the M3 implementation, the rest of the core was undergoing significant widening of the pipe (4-wide to 6-wide throughout) as well as more than doubling of the out-of-order window. To maintain the ability to feed a wider and more capable microarchitecture, the branch predictor needed to be improved.</p><p>To reduce average branch turnaround, the ?BTB graph size doubled, but reduced the area increase by only adding entries that could be used exclusively to store unconditional branches. For workloads that could not fit within the ?BTB, the concept of an early always-taken redirect was added for the mBTB: Always-taken branches will redirect a cycle earlier. Such branches are called 1AT branches, short for "1-bubble always-TAKEN" branches.</p><p>M3 also made several changes to further reduce MPKI in the predictor: doubling of SHP rows (reduces aliasing for conditional branches); and doubling of L2BTB capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. M4 Branch Prediction Changes: Focus on larger workloads</head><p>The M4 generation predictor refinement focused on better support for larger working set sizes. The L2BTB size was doubled again, making this generation capable of holding four times as many branches as the first generation. In addition, L2BTB fills into the BTB had their latency slightly reduced, and their bandwidth improved by 2x, emphasizing the importance of real-use-case code. This capacity and latency bandwidth change significantly improved performance for mobile workloads like BBench by 2.8% in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. M5 Branch Prediction: Efficiency improvements</head><p>For the M5 generation, the branch predictor research focus was on improving the predictor in terms of several different efficiencies.</p><p>Some sections of code are very branch-sparse, sometimes with entire BTB lines devoid of any actual discovered branches. Thus the design added state to track "Empty Line Optimizations" (skip an entire line devoid of branches) to reduce both the latency and power of looking up uninteresting addresses.</p><p>The previous generation had already accelerated always-TAKEN branches via a one-bubble redirect. In this generation, this idea was extended in two different ways: reducing the bubbles to zero via replication, and extending coverage to often-TAKEN branches. The replication is done via copying the targets of always -TAKEN and often-TAKEN branches into their predecessor branches' mBTB information, thus providing zero-bubble always-TAKEN (ZAT) and zero-bubble often-TAKEN (ZOT) prediction capability. Thus, on an mBTB lookup for a branch, it provided both its own target, and if that target location led next to a candidate always/often-TAKEN branch, the target of that subsequent branch. This increased storage cost, but greatly improved taken-branch throughput, as shown in the example in Figure <ref type="figure" target="#fig_4">5</ref>. With a new zero-bubble structure in the mBTB, the ?BTB and this ZAT/ZOT predictor can sometimes contend. The main predictor has no startup penalty but uses full mBTB and SHP resources, while the ?BTB has a two-cycle startup penalty, but can chain zero-bubble predictions without lead branches and could also save mBTB and SHP power on tight kernels. A heuristic arbiter is used to intelligently select which zerobubble predictor to use.</p><p>Given the growing capabilities of the one-bubble predictor, the M5 design was able to decrease the area for the ?BTB by reducing the number of entries, and having the ZAT/ZOT predictor participate more. This resulted in a better area efficiency for a given amount of performance.</p><p>The SHP increased from 8 tables to 16 tables of 2,048 8bit weights in sign magnitude representation, reducing MPKI from both alias reductions as well as increasing the number of unique tables. The GHIST length was also increased by 25%, and the table hash intervals rebalanced for this longer GHIST.</p><p>Finally, one performance problem was the effective refill time after a mispredict. If the correct-path contained several small basic blocks connected with taken branches, it could take many cycles to refill the pipe, which was observed in some of our key web workloads, among others. This is shown in Figure <ref type="figure" target="#fig_5">6</ref>, where we assume basic block A has 5 instructions, basic block B has 6, and basic block C has 3, and all end with a taken branch. Once a mispredict is signaled, the branch prediction pipe starts fetching at the correct target, A, and in subsequent cycles predicts sequential fall-through with a fetch width of 6, at addresses A+6 and A+12. (After a mispredict, the ?BTB is disabled until the next "seed" branch.) However, when the branch prediction pipe reaches the third stage, it sees that A has a predicted-taken branch to B. The core squashes the speculative prediction lookups for A+6 and A+12, and restarts the branch prediction pipe for address B. In this example, assume A's basic block contained 5 instructions. If B also has a small basic block and a taken branch, it again requires three cycles to fetch an additional 6 instructions. All told, in this example, it requires 9 cycles from the mispredict redirect to fetch 14 instructions, and the downstream core will likely become fetch-starved. In order to help mitigate this long effective refill time after a mispredict, the M5 core added a block called the Mispredict Recovery Buffer (MRB). After identifying lowconfidence branches <ref type="bibr" target="#b18">[19]</ref>, it records the highest probability sequence of the next three fetch addresses. On a matching mispredict redirect, the predicted fetch addresses can be read sequentially out of the MRB in consecutive cycles as shown in Figure <ref type="figure">7</ref>, eliminating the usual branch prediction delay, and in this example allowing 14 instructions to be provided in only 5 cycles. Note that in the third stage, the MRB-predicted target address is checked against the newly predicted target from the branch predictor, and if they agree, no correction is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. M6 Branch Prediction: Indirect capacity improvements</head><p>For the sixth generation, the greatest adjustment was to respond to changes in popular languages and programming styles in the market. The first of these changes was to increase the size of the mBTB by 50%, due to larger working set sizes.</p><p>Second, JavaScript's increased use put more pressure on indirect targets, allocating in some cases hundreds of unique indirect targets for a given indirect branch via function calls. Unfortunately, the VPC algorithm requires O(n) cycles to train Fig. <ref type="figure">7</ref>. Mispredict Recovery Buffer (MRB) providing the highest probability basic block addresses after an identified low-confidence mispredicted branch <ref type="bibr" target="#b19">[20]</ref> and predict all the virtual branches for an n-target indirect branch. It also consumes much of the vBTB for such manytarget indirect branches.</p><p>A solution to the large target count problem is to dedicate unique storage for branches with very large target possibilities. As a trade-off, the VPC algorithm is retained since the accuracy of SHP+VPC+hash-table lookups still proves superior to a pure hash-table lookup for small numbers of targets. Additionally, large dedicated storage takes a few cycles to access, so doing a limited-length VPC in parallel with the launch of the hash-table lookup proved to be superior in throughput and performance. This hybrid approach is shown in Figure <ref type="figure">8</ref>, and reduced end-to-end prediction latency compared to the full-VPC approach. Fig. <ref type="figure">8</ref>. VPC reduction to 5 targets followed by an Indirect Hash Performance modeling showed that accessing the hashtable lookup indirect target table with the standard SHP GHIST/PHIST/PC hash did not perform well, as the precursor conditional branches do not highly correlate with the indirect targets. A different hash is used for this table, based on the history of recent indirect branch targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Overall impact</head><p>From M1 through M6, the total bit budget for branch prediction increased greatly, in part due to the challenges of predicting web workloads with their much larger working set. Table <ref type="table">II</ref> shows the total bit budget for the SHP, L1, and L2 branch predictor structures. The L2BTB uses a slower denser macro as part of a latency/area tradeoff. With all of the above changes, the predictor was able to go from an average mispredicts-per-thousand-instructions (MPKI) of 3.62 for a set of several thousand workload slices on the first implementation, to an MPKI of 2.54 for the latest implementation. This is shown graphically in Figure <ref type="figure" target="#fig_6">9</ref>, where the breadth of the impact can be seen more clearly.</p><p>On the left side of the graph, many workloads are highly predictable, and are unaffected by further improvements. In the middle of the graph are the interesting workloads, like most of SPECint and Geekbench, where better predictor schemes and more resources have a noticeable impact on reducing MPKI. On the right are the workloads with very hard to predict branches The graph has the Y-axis clipped to highlight the bulk of the workloads which have the characteristic of an MPKI under 20, but even the clipped highly-unpredictable workloads on M1 are improved by ~20% on subsequent generations. Overall, these changes reduced SPECint2006 MPKI by 25.6% from M1 to M6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BRANCH PREDICTION SECURITY</head><p>During the evolution of the microarchitectures discussed here, several security vulnerabilities, including Spectre <ref type="bibr" target="#b20">[21]</ref>, became concerning. Several features were added to mitigate security holes. In this paper's discussion, the threat model is based on a fully-trustworthy operating system (and hypervisor if present), but untrusted userland programs, and that userland programs can create arbitrary code, whether from having full access, or from ROP/widget programming.</p><p>This paper only discusses features used to harden indirect and return stack predictions. Simple options such as erasing all branch prediction state on a context change may be necessary in some context transitions, but come at the cost of having to retrain when going back to the original context. Separating storage per context or tagging entries by context come at a significant area cost. The compromise solution discussed next provides improved security with minimal performance, timing, and area impact.</p><p>The new front-end mechanism hashes per-context state and scrambles the learned instruction address targets stored in a branch predictor's branch-target buffers (BTB) or returnaddress-stack (RAS). A mixture of software-and hardwarecontrolled entropy sources are used to generate the hash key (CONTEXT HASH) for a process. The hashing of these stored instruction address targets will require the same exact numbers to perfectly un-hash and un-scramble the predicted taken target before directing the program address of the CPU. If a different context is used to read the structures, the learned target may be predicted taken, but will jump to an unknown/unpredictable address and a later mispredict recovery will be required. The computation of CONTEXT HASH is shown in Figure <ref type="figure" target="#fig_7">10</ref>.</p><p>The CONTEXT HASH register is not software accessible, and contains the hash used for target encryption/decryption. Its value is calculated with several inputs, including:</p><p>? A software entropy source selected according to the user, kernel, hypervisor, or firmware level implemented as SCXTNUM ELx as part of the security feature CSV2 (Cache Speculation Variant 2) described in ARM v8.5 <ref type="bibr" target="#b3">[4]</ref>. ? A hardware entropy source, again selected according to the user, kernel, hypervisor, or firmware level. ? Another hardware entropy source selected according to the security state. ? An entropy source from the combination of ASID (process ID), VMID (virtual machine ID), security state, and privilege level.</p><p>Note that the CONTEXT HASH computation is performed completely in hardware, with no software visibility to intermediate values, even to the hypervisor. The computation of the CONTEXT HASH register also includes rounds of entropy diffusion <ref type="bibr" target="#b22">[23]</ref> -specifically a deterministic, reversible non-linear transformation to average per-bit randomness. The small number of cycles required for the hardware to perform multiple levels of hashing and iterative entropy spreading have minimal performance impact, given the latency of a context switch.</p><p>Within a particular processor context, CONTEXT HASH is used as a very fast stream cipher to XOR with the indirect branch or return targets being stored to the BTB or RAS, as in Figure <ref type="figure" target="#fig_0">11</ref>. Such a simple cipher can be inserted into the RAS and BTB lookups without much impact to the timing paths, as compared to a more rigorous encryption/decryption scheme. Fig. <ref type="figure" target="#fig_0">11</ref>. Indirect/RAS Target Encryption <ref type="bibr" target="#b21">[22]</ref> To protect against a basic plaintext attack, a simple substitution cipher or bit reversal can further obfuscate the actual stored address. When the branch predictor is trained and ready to predict jump targets from these common structures, the hardware will use the program's CONTEXT HASH to perfectly invert and translate out the correct prediction target.</p><p>This approach provides protection against both crosstraining and replay attacks. If one attacker process tries to cross-train an indirect predictor with a target, it will be encrypted with CONTEXT HASH attack into the predictor structures, and then read out and decoded in the victim's execution with the differing CONTEXT HASH victim , which will result in the victim beginning execution at a different address than the intended attack target. With regard to a replay attack, if one process is able to infer mappings from a plaintext target to a CONTEXT HASH-encrypted target, this mapping will change on future executions, which will have a different process context (process ID, etc.).</p><p>In addition, the operating system can intentionally periodically alter the CONTEXT HASH for a process or all processes (by changing one of the SW ENTROPY * LVL inputs, for example), and, at the expense of indirect mispredicts and retraining, provide protection against undesired cross training during the lifetime of a process, similar to the concept in CEASER <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MICRO-OPERATION CACHE (UOC)</head><p>M1 through M4 implementations fetched and decoded instructions as is common in a modern out-of-order pipeline: fetch instructions from the instruction cache, buffer them in the instruction queue, and generate micro-ops (?ops) through the decoder before feeding them to later pipe stages. As the design moved from supplying 4 instructions/?ops per cycle in M1, to 6 per cycle in M3 (with future ambitions to grow to 8 per cycle), fetch and decode power was a significant concern.</p><p>The M5 implementation added a micro-operation cache <ref type="bibr">[25] [26]</ref> as an alternative ?op supply path, primarily to save fetch and decode power on repeatable kernels. The UOC can hold up to 384 ?ops, and provides up to 6 ?ops per cycle to subsequent stages. The view of instructions by both the front-end and by the UOC is shown in Figure <ref type="figure" target="#fig_8">12</ref>. The front end operates in one of three different UOC modes: FilterMode, where the ?BTB predictor determines predictability and size of a code segment; BuildMode, where the UOC is allocating appropriate basic blocks; and FetchMode, where the instruction cache and decode logic are disabled, and instruction supply is solely handled by the UOC. Each mode is shown in the flowchart of Figure <ref type="figure" target="#fig_9">13</ref>, and will be described in more detail in the next paragraphs. The FilterMode is designed to avoid using the UOC Build-Mode when it would not be profitable in terms of both power and performance. In FilterMode, the ?BTB predictor checks several conditions to ensure the current code segment is highly predictable, and will also fit within both the ?BTB and UOC finite resources. Once these conditions are met, the mechanism switches into BuildMode.</p><p>To support BuildMode, the ?BTB predictor is augmented with "built" bits in each branch's prediction entry, tracking whether or not the target's basic block has already been seen in the UOC and back-propagated to the predictor. Initially, all of these "built" bits are clear. On a prediction lookup, the #BuildTimer is incremented, and the "built" bit is checked:</p><p>? If the "built" bit is clear, #BuildEdge is incremented, and the basic block is marked for allocation in the UOC. Next, the UOC tags are also checked ; if the basic block is present, information is propagated back to the ?BTB predictor to update the "built" bit. This back-propagation avoids the latency of a tag check at prediction time, at the expense of an additional "build" request that will be squashed by the UOC. ? If the "built" bit is set, #FetchEdge is incremented. When the ratio between #FetchEdge and #BuildEdge reaches a threshold, and the #BuildTimer has not timed out, it indicates that the code segment should now be mostly or completely UOC hits, and the front end shifts into FetchMode.</p><p>In FetchMode, the instruction cache and decode logic are disabled, and the ?BTB predictions feed through the UAQ into the UOC to supply instructions to the rest of the machine. As long as the ?BTB remains accurate, the mBTB is also disabled, saving additional power. While in FetchMode, the front end continues to examine the "built" bits, and updates #BuildEdge if the bit is clear, and #FetchEdge if the bit is set. If the ratio of #BuildEdge to #FetchEdge reaches a different threshold, it indicates that the current code is not hitting sufficiently in the UOC, and the front end shifts back into FilterMode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. L1 DATA PREFETCHING</head><p>Hardware prefetching into the L1 Cache allows data to be fetched from memory early enough to hide the memory latency from the program. Prefetching is performed based on information from demand loads, and can be limited by the size of the cache and maximum outstanding misses. Capacity of this cache grew from 32KB in M1, to 64KB in M3, to 128KB in M6. Outstanding misses grew from 8 in M1, to 12 in M3, to 32 in M4, and 40 in M6. The significant increase in misses in M4 was due to transitioning from a fill buffer approach to a data-less memory address buffer (MAB) approach that held fill data only in the data cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm</head><p>The L1 prefetcher detects strided patterns with multiple components (e.g. +1?3, +2?1, meaning a stride of 1 repeated 3 times, followed by a stride of two occuring only once). It operates on the virtual address space and prefetches are permitted to cross page boundaries. This approach allows large prefetch degrees (how many outstanding prefetches are allowed to be outstanding at a time) to help cover memory latency and solve prefetcher timeliness issues. This design also inherently acts as a simple TLB prefetcher to preload the translation of next pages prior to demand accesses to those pages.</p><p>The prefetcher trains on cache-misses to effectively use load pipe bandwidth. To avoid noisy behavior and improve pattern detection, out-of-order addresses generated from multiple load pipes are reordered back into program order using a ROB-like structure <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b27">[28]</ref>. To reduce the size of this re-order buffer, an address filter is used to deallocate duplicate entries to the same cache line. This further helps the training unit to see unique addresses.</p><p>The prefetcher training unit can train on multiple streams simultaneously and detect multi-strides, similar to <ref type="bibr" target="#b28">[29]</ref>. If the same multi-stride pattern is repeated, it can detect a pattern. For example, consider this access pattern: A; A+2; A+4; A+9; A+11; A+13; A+18 and so on The above load stream has a stride of +2, +2, +5, +2, +2, +5. The prefetch engine locks onto a pattern of +2?2, +5?1 and generates prefetches A+20, A+22, A+27 and so on. The load stream from the re-order buffer is further used to confirm prefetch generation using a confirmation queue. Generated prefetch addresses get enqueued into the confirmation queue and subsequent demand memory accesses will match against the confirmation queue. A confidence scheme is used based on prefetch confirmations and memory sub-system latency to scale prefetcher degree (see next section for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Degree and One-pass/Two-pass</head><p>In order to cover the latency to main memory, the required degree can be very large (over 50). This creates two problems: lack of miss buffers, and excess prefetches for short-lived patterns. These excess prefetches waste power, bandwidth and cache capacity.</p><p>A new adaptive, dynamic degree <ref type="bibr" target="#b29">[30]</ref> mechanism avoids excessive prefetches. Prefetches are grouped into windows, with the window size equal to the current degree. A newly created stream starts with a low degree. After some number of confirmations within the window, the degree will be increased. If there are too few confirmations in the window, the degree is decreased. In addition, if the demand stream overtakes the prefetch stream, the prefetch issue logic will skip ahead of the demand stream, avoiding redundant late prefetches.</p><p>To reduce pressure on L1 miss buffers, a "two pass mode" scheme <ref type="bibr" target="#b30">[31]</ref> is used, starting with M1, as shown in Figure <ref type="figure" target="#fig_10">14</ref>. Note that M3 and beyond added an L3 between the L2 and interconnect. When a prefetch is issued the first time, it will not allocate an L1 miss buffer, but instead be sent as a fill request into the L2 (1). The prefetch address will also be inserted (2) into a queue, which will be held until sufficient L1 miss buffers are available. The L2 cache will issue the request to the interconnect (3) and eventually receive a response (4). When a free L1 miss buffer is available, an L1 miss buffer will allocate (5) and an L1 fill request will occur (6 and 7).</p><p>This scheme is suboptimal for cases where the working set fits in the L2 (since every first pass prefetch to the L2 would hit). To counteract this, the mechanism also tracks the number of first pass prefetch hits in the L2, and if they reach a certain watermark, it will switch into "one pass mode". In this mode, shown on the right in Figure <ref type="figure" target="#fig_10">14</ref>, only step 2 is initially performed. When there are available miss buffers (which may be immediately), steps 5, 6, and 7 are performed, saving both power and L2 bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Memory Streaming Prefetcher</head><p>The multi-strided prefetcher is excellent at capturing regular accesses to memory. However, programs which traverse a linked-list or other certain types of data structures are not covered at all. To attack these cases, in M3 an additional L1 prefetch engine is added -a spatial memory stream (SMS) prefetcher <ref type="bibr">[32] [33]</ref>. This engine tracks a primary load (the first miss to a region), and attaches associated accesses to it (any misses with a different PC). When the primary load PC appears again, prefetches for the associated loads will be generated based off the remembered offsets.</p><p>Also, this SMS algorithm tracks confidence for each associated demand load. Only associated loads with high confidence are prefetched, to filter out the ones that will appear transiently along with the primary load. In addition, when confidence drops to a lower level, the mechanism will only issue the first pass (L2) prefetch.</p><p>With two different prefetch engines, a scheme is required to avoid duplicate prefetches. Given that a trained multi-stride engine can prefetch further ahead of the demand stream than </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Integrated Confirmation Queues</head><p>To cover memory latency and allow multiple load streams to operate simultaneously, the size of the confirmation queue needs to be considerably large. Also, at the start of pattern detection, the prefetch generation can lag behind the demand stream. Since confirmations are based on issued prefetches, this can result in few confirmations. As prefetch degree is a function of confirmations, this mechanism can keep the degree in a low state and prefetches may never get ahead of the demand stream.</p><p>To combat the above issues, the M3 core introduced an integrated confirmation scheme <ref type="bibr" target="#b33">[34]</ref> to replace the confirmation queue. The new mechanism keeps the last confirmed address, and uses the locked prefetch pattern to generate the next N confirmation addresses into a queue, where N is much less than the stream's degree. This is the same logic as the prefetch generation logic, but acts independently. This confirmation scheme reduces the confirmation structure size considerably and also allows confirmations even when the prefetch engine has not yet generated prefetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. LARGE CACHE MANAGEMENT AND PREFETCHERS</head><p>Over six generations, the large cache (L2/L3) hierarchy evolved as shown in Table <ref type="table">III</ref> below. M3 added a larger but more complex three-level hierarchy, which helps balance latency versus capacity objectives with the increasing working set needs of modern workloads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Co-ordinated Cache Hierarchy Management</head><p>To avoid data duplication, the outer cache levels (L3) are made exclusive to the inner caches (L1 and L2). Conventional exclusive caches do not keep track of data reuse since cache lines are swapped back to inner-level caches. The L2 cache tracks both frequency of hits within the L2, as well as subsequent re-allocation from the L3 cache. Upon L2 castout, these details are used to intelligently choose to either allocate the castouts into the L3 in an elevated replacement state, or an ordinary replacement state, or avoid allocation altogether.</p><p>This bidirectional coordination allows the caches to preserve useful data in the wake of transient streams, or when the application working set exceeds the total cache capacity. Amongst the hardware overhead, each cache tag stores some metadata to indicate the reuse or dead behavior of the associated lines, and for different transaction types (prefetch vs. demand). This meta-data is passed through request or response channels between the cache levels. There are some cases that needed to be filtered out from being marked as reuse, such as the second pass prefetch of two-pass prefetching. More information can be found in the related patent <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Buddy Cache Prefetcher</head><p>The L2 cache tags are sectored at a 128B granule for a default data line size of 64B. This sectoring reduces the tag area and allows a lower latency for tag lookups. Starting in M4, a simple "Buddy" prefetcher is added that, for every demand miss, generates a prefetch for its 64B neighbor (buddy) sector. Due to the tag sectoring, this prefetching does not cause any cache pollution, since the buddy sector will stay invalid in absence of buddy prefetching. There can be an impact on DRAM bandwidth though, if the buddy prefetches are never accessed. To alleviate that issue, a filter is added to the Buddy prefetcher to track the patterns of demand accesses. In the case where access patterns are observed to almost always skip the neighboring sector, the buddy prefetching is disabled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Standalone Cache Prefetcher</head><p>Starting in M5, a standalone prefetcher is added to prefetch into the lower level caches beyond the L1s. This prefetcher observes a global view of both the instruction and data accesses at the lower cache level, to detect stream patterns <ref type="bibr" target="#b35">[36]</ref>. Both demand accesses and core-initiated prefetches are used for its training. Including the core prefetches improves their timeliness when they are eventually filled into the L1. Among the challenges of the standalone prefetcher is the outof-orderness of program accesses observed at the lower-level cache that can pollute the patterns being trained. Another challenge is the fact that it operates on physical addresses, which limits its span to a single page. Similarly, hits in the L1 can filter the access stream seen by the lower levels of cache, making it difficult to isolate the real program access stream. For those reasons, the standalone prefetcher uses an algorithm to handle long, complex streams with larger training structures, and techniques to reuse learnings across 4KB physical page crossings. The standalone prefetcher also employs a twolevel adaptive scheme to maintain high accuracy of issued prefetches as explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adaptive Prefetching</head><p>The standalone prefetcher is built with an adaptive scheme shown in Figure <ref type="figure" target="#fig_11">15</ref> with two modes of prefetching: low confidence mode and high confidence mode. In low confidence mode, "phantom" prefetches are generated for confidence tracking purposes into a prefetch filter, but not issued to the memory system or issued very conservatively. The confidence is tracked through demand accesses matching entries in the prefetch filter. When confidence increases beyond a threshold, high confidence mode is entered.</p><p>In high confidence mode, prefetches are issued aggressively out to the memory system. The high-confidence mode relies on cache meta-data to track the prefetcher's confidence. This meta-data includes bits associated with the tags to mark whether a line was prefetched, and if it was accessed by a demand request. If the confidence reduces below a threshold, the prefetcher transitions back to the low confidence mode. Hence, the prefetcher accuracy is continuously monitored to detect transitions between application phases that are prefetcher friendly and phases that are difficult to prefetch or when the out-of-orderness makes it hard to detect the correct patterns. The Exynos mobile processor designs contain three different voltage/frequency domains along the core's path to main memory: the core domain, an interconnect domain, and a memory controller domain. This provides flexibility with regards to power usage in different mobile scenarios: GPUheavy, core-heavy, etc. However, this requires four on-die asynchronous crossings (two outbound, two inbound), as well as several blocks' worth of buffering, along that path. Over several generations, the Exynos designs add several features to reduce the DRAM latency through multiple levels of the cache hierarchy and interconnect. These features include data fast path bypass, read speculation, and early page activate.</p><p>The CPU memory system on M4 supports an additional dedicated data fast path bypass directly from DRAM to the CPU cluster. The data bypasses multiple levels of cache return path and interconnect queuing stages. Also a single direct asynchronous crossing from the memory controller to the CPU bypasses the interconnect domain's two asynchronous crossings.</p><p>To further optimize latency, the CPU memory system on M5 supports speculative cache lookup bypass for latency critical reads. The read requests are classified as "latency critical" based on various heuristics from the CPU (e.g. demand load miss, instruction cache miss, table walk requests etc.) as well as a history-based cache miss predictor. Such reads speculatively issue to the coherent interconnect in parallel to checking the tags of the levels of cache. The coherent interconnect contains a snoop filter directory that is normally looked up in the path to memory access for coherency management. The speculative read feature utilizes the directory lookup <ref type="bibr" target="#b36">[37]</ref> to further predict with high probability whether the requested cache line may be present in the bypassed lower levels of cache. If yes, then it cancels the speculative request by informing the requester. This cancel mechanism avoids penalizing memory bandwidth and power on unnecessary accesses, acting as a second-chance "corrector predictor" in case the cache miss prediction from the first predictor is wrong.</p><p>The M5 CPU memory system contains another feature to reduce memory latency. For latency critical reads, a dedicated sideband interface sends an early page activate command to the memory controller to speculatively open a new DRAM page. This interface, like the fast data path mechanism described above, also bypasses two asynchronous crossings with one. The page activation command is a hint the memory controller may ignore under heavy load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. OVERALL IMPACT ON AVERAGE LOAD LATENCY</head><p>The net impact of all of the generational changes on average load latency is shown in Figure <ref type="figure" target="#fig_12">16</ref>. These include the cache size, prefetching, replacement algorithms, and DRAM latency changes discussed in the above sections. Note that the 3-cycle cascading load latency feature is clearly visible on the left of the graph for workloads that hit in the DL1 cache. It also partially contributes to lower DL1 cache hit latencies for many other workloads that occasionally miss. There are many other changes not discussed in this paper that also contributed to lower average load latency.</p><p>Overall, these changes reduce average load latency from 14.9 cycles in M1 to 8.3 cycles in M6, as shown in Table <ref type="table" target="#tab_3">IV</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XI. CONCLUSION</head><p>This paper discusses the evolution of a branch predictor implementation, security protections, prefetching improvements, and main memory latency reductions. These changes, as well as dozens of other features throughout the microarchitecture, resulted in substantial year-on-year frequency-neutral performance improvements.</p><p>Throughout the generations of the Exynos cores, the focus was to improve performance across all types of workloads, as shown in Figure <ref type="figure" target="#fig_0">17:</ref> ? Low-IPC workloads were greatly improved by more sophisticated, coordinated prefetching, as well as cache replacement/victimization optimizations. ? Medium-IPC workloads benefited from MPKI reduction, cache improvements, additional resources, and other performance tweaks. ? High-IPC workloads were capped by M1's 4-wide design.</p><p>M3 and beyond were augmented in terms of width and associated resources needed to achieve 6 IPC for workloads capable of that level of parallelism. The average IPC across these workloads for M1 is 1.06, while the average IPC for M6 is 2.71, which showcases a compounded growth rate of 20.6% frequency-neutral IPC improvement every year.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Average MPKI over all CBP5 traces of an eight-table, 1K weight SHP vs adding GHIST range bits to the hash</figDesc><graphic url="image-1.png" coords="2,311.51,258.88,252.00,121.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>aFig. 2 .</head><label>2</label><figDesc>Fig. 2. Main and Virtual BTB branch "chains"</figDesc><graphic url="image-2.png" coords="3,48.49,462.76,251.99,93.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Main and Virtual BTB indirect VPC chain</figDesc><graphic url="image-3.png" coords="3,311.51,462.76,252.00,110.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Learned branch "graph" as used by the ?BTB<ref type="bibr" target="#b17">[18]</ref> </figDesc><graphic url="image-4.png" coords="4,48.49,75.18,252.00,135.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Increase in branch throughput by reducing bubbles for Always Taken (AT) and zero-often-taken (ZOT) predictions. The replication scheme allows X to specify a redirect to both A, and to B, the target of the next branch after address A.</figDesc><graphic url="image-5.png" coords="4,311.51,351.32,251.99,122.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. An illustration of the slow effective refill time after a mispredict to a series of small basic blocks</figDesc><graphic url="image-8.png" coords="5,311.51,415.82,252.00,247.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. MPKIs across 4,026 workload slices. Y-axis is clipped at 20 MPKI for clarity. Note that M2, which had no substantial branch prediction change over M1, is not shown in this graph.</figDesc><graphic url="image-9.png" coords="6,311.51,50.54,252.00,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Computation of CONTEXT HASH encryption/decryption register [22]. This computation is only performed during a context switch, and takes only a few cycles.</figDesc><graphic url="image-10.png" coords="7,48.49,50.54,252.01,239.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Instruction-based and ?op-based views</figDesc><graphic url="image-12.png" coords="7,311.51,557.65,251.99,94.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Modes of UOC operation</figDesc><graphic url="image-13.png" coords="8,48.49,121.35,252.00,220.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. One-pass/two-pass prefetching scheme for M1/M2.</figDesc><graphic url="image-14.png" coords="9,311.51,50.54,252.01,249.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Adaptive Prefetcher State Transitions</figDesc><graphic url="image-15.png" coords="10,311.51,462.47,252.00,149.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Evolution of Average Load Latency</figDesc><graphic url="image-16.png" coords="11,311.51,50.54,252.00,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="6">GENERATIONAL AVERAGE LOAD LATENCIES</cell></row><row><cell></cell><cell>M1</cell><cell>M2</cell><cell>M3</cell><cell>M4</cell><cell>M5</cell><cell>M6</cell></row><row><cell>Avg. load lat.</cell><cell cols="2">14.9 13.8</cell><cell>12.8</cell><cell>11.1</cell><cell>9.5</cell><cell>8.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The work described here represents the combined effort of hundreds of engineers over eight years of innovation and hard work in order to meet all of the schedule, performance, timing, and power goals, for five annual generations of productized silicon and a sixth completed design. The authors are deeply Fig. 17. IPCs across 4026 workload slices, by generation indebted to Samsung for supporting the publication of this paper, and to <rs type="person">Keith Hawkins</rs>, <rs type="person">Mike Goddard</rs>, and <rs type="person">Brad Burgess</rs> for their leadership of SARC during these designs. The authors also thank the anonymous reviewers for their comments and suggestions on improvements.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Samsung Exynos M1 Processor</title>
		<author>
			<persName><forename type="first">B</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
		<title level="m">Samsung Exynos M3 Processor</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>IEEE Hot Chips 30</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zuraski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Samsung M3 Processor</title>
		<imprint>
			<date type="published" when="2019-04">March-April 2019</date>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ARM Architectural Reference Manual ARMv8, for Armv8-A architectural profile</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ARM Incorporated</publisher>
		</imprint>
	</monogr>
	<note>Revision E.a</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using SimPoint for Accurate and Efficient Simulation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic Branch Prediction with Perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast Path-Based Neural Branch Prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Microarchitecture (MICRO-36)</title>
		<meeting>the 36th Annual International Symposium on Microarchitecture (MICRO-36)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Optimized Scaled Neural Branch Predictor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 29th International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Strided Sampling Hashed Perceptron Predictor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Championship Branch Prediction (CBP-4)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Merging Path and Gshare Indexing in Perceptron Branch Prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Architecture and Code Optimization</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Combining Branch Predictors. TN-36</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Digital Equipment Corporation, Western Research Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two-level Adaptive Branch Prediction</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-24</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-24</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th ACM/IEEE International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic Path-based Branch Correlation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-28</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th Annual International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">CBP-5 Kit</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>in 5th Championship Branch Prediction Workshop (CBP-5</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analysis of the O-GEometric History Length Branch Predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Symposium on Computer Architecture (ISCA-32)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving Branch Prediction Accuracy by Reducing Pattern History Table Interference</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Parallel Architectures and Compilation Technique (PACT)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High Performance Zero Bubble Conditional Branch Prediction using Micro Branch Target Buffer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zuraski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">United States Patent</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">200</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Assigning Confidence to Conditional Branch Predictions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jacobson</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-29</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-29</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
			<affiliation>
				<orgName type="collaboration">MICRO-29</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mispredict Recovery Apparatus and Method for Branch and Fetch Pipelines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jumani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tkaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Quinnell</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Patent Pending</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spectre Attacks: Exploiting Speculative Execution</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mangard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Prescher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yarom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Secure Branch Predictor with Context-Specific Learned Instruction Target Address Encryption</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tkaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Quinnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Barakat</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Patent Pending</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A Mathematical Theory of Cryptography</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno>MM 45-110-02</idno>
		<imprint>
			<date type="published" when="1945-09-01">September 1, 1945</date>
		</imprint>
	</monogr>
	<note type="report_type">Bell System Technical Memo</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CEASER: mitigating conflict-based cache attacks via encrypted-address and remapping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-51)</title>
		<meeting>the 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-51)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA</title>
		<author>
			<persName><forename type="first">B</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Orenstien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Almog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Micro-operation Cache Using Predictive Allocation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Quinnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tkaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S S</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Patent Pending</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implementation of Precise Interrupts in Pipelined Processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pleszkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Annual International Symposium on Computer Architecture (ISCA-12)</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Address Re-ordering Mechanism for Efficient Pre-fetch Training in an Out-of order Processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">United States Patent</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">851</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective stream-based and execution-based data prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iacobovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International Conference on Supercomputing (ICS-18)</title>
		<meeting>the 18th Annual International Conference on Supercomputing (ICS-18)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive Mechanism to tune the Degree of Pre-fetches Streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">United States Patent</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">491</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pre-fetch Chaining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chinnakonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">United States Patent</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">361</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatial Memory Streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Symposium on Computer Architecture (ISCA-33)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">System and Method for Spatial Memory Streaming Training</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">United States Patent</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Integrated Confirmation Queues</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">United States Patent</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">320</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Coordinated Cache Management Policy for an Exclusive Cache Hierarchy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Silvera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Patent patent pending</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 49th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Torres</surname></persName>
		</author>
		<title level="m">Parallel with Cache Level Search, Leveraging Interconnect Directory</title>
		<imprint/>
	</monogr>
	<note>Speculative DRAM Read. Patent Pending</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
