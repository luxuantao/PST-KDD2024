<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Discrete Particle Swarm Algorithm Applied to Attribute Selection in a Bioinformatics Data Set</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Elon</forename><forename type="middle">S</forename><surname>Correa</surname></persName>
							<email>e.s.correa@kent.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computing Laboratory</orgName>
								<orgName type="institution">University of Kent Canterbury</orgName>
								<address>
									<postCode>CT2 7NF</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
							<email>a.a.freitas@kent.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Computing Laboratory</orgName>
								<orgName type="institution">University of Kent Canterbury</orgName>
								<address>
									<postCode>CT2 7NF</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><forename type="middle">G</forename><surname>Johnson</surname></persName>
							<email>c.g.johnson@kent.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Computing Laboratory</orgName>
								<orgName type="institution">University of Kent Canterbury</orgName>
								<address>
									<postCode>CT2 7NF</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A New Discrete Particle Swarm Algorithm Applied to Attribute Selection in a Bioinformatics Data Set</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">17ED9A564D098A12559412A1325BF0A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.6 [Computing Methodologies]: Artificial Intelligence-Learning</term>
					<term>induction Algorithms</term>
					<term>performance Particle swarm</term>
					<term>optimization</term>
					<term>Data Mining</term>
					<term>attribute selection</term>
					<term>Naive Bayes classifier</term>
					<term>bioinformatics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many data mining applications involve the task of building a model for predictive classification. The goal of such a model is to classify examples (records or data instances) into classes or categories of the same type. The use of variables (attributes) not related to the classes can reduce the accuracy and reliability of a classification or prediction model. Superfluous variables can also increase the costs of building a model -particularly on large data sets. We propose a discrete Particle Swarm Optimization (PSO) algorithm designed for attribute selection. The proposed algorithm deals with discrete variables, and its population of candidate solutions contains particles of different sizes. The performance of this algorithm is compared with the performance of a standard binary PSO algorithm on the task of selecting attributes in a bioinformatics data set. The criteria used for comparison are: (1) maximizing predictive accuracy; and</p><p>(2) finding the smallest subset of attributes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Most of the particle swarm algorithms present in the literature deal only with continuous variables <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b10">10]</ref>. This is a significant limitation because many optimization problems are set in a space featuring discrete variables. Typical examples include problems which require the ordering or arranging of discrete variables, such as scheduling or routing problems <ref type="bibr" target="#b17">[17]</ref>. Therefore, the design of particle swarm algorithms that deal with discrete variables is pertinent to this field of study.</p><p>We propose a discrete Particle Swarm Optimization (PSO) algorithm applied to attribute selection in Data Mining. We shall refer to this algorithm as the discrete Particle Swarm Optimization (DPSO) algorithm. The DPSO deals with discrete variables, and its population of candidate solutions contains particles of different sizes. Although the algorithm has been specifically designed for an attribute selection task, it is by no means limited to this kind of application. The DPSO algorithm may potentially be applied to other discrete optimization problems, such as facility location problems <ref type="bibr" target="#b2">[2]</ref>, with a few minor modifications.</p><p>Many data mining applications involve the task of building a model for predictive classification. The goal of such a model is to classify examples (records or data instances) into classes or categories of the same type. The use of variables (attributes) not related to the classes can reduce the accuracy and reliability of a classification or prediction model. Superfluous variables can also increase the costs of building a model -particularly on large data sets. The objective of attribute selection is to simplify a data set by reducing its dimensionality and identifying relevant underlying attributes without sacrificing predictive accuracy. By doing that, it also reduces redundancy in the information provided by the selected attributes. For a review of the attribute selection task using genetic algorithms see <ref type="bibr" target="#b4">[4]</ref>.</p><p>The DPSO algorithm was designed to tackle the data mining task of attribute selection. It differs from other traditional PSO algorithms because its particles do not represent points inside an n-dimensional Euclidean space (continuous case) or lattice (binary case) as in the standard PSO algorithms <ref type="bibr" target="#b9">[9]</ref>. Instead, they represent a combination of selected attributes.</p><p>The paper is organized as follows. Section 2 briefly introduces PSO algorithms. Section 3 describes the standard binary PSO algorithm. Section 4 introduces the DPSO algorithm proposed in this paper for the task of attribute selection. Section 5 reports computational experiments, and describes the postsynaptic data set -the data set used in our experiments. It also includes a brief discussion of the results obtained. Section 6 presents conclusions of the work and points out future research directions. The next subsection specifies the notation used throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Notation</head><p>A lowercase letter, e.g., x, denotes a random variable. An uppercase letter with an arrow over the letter, e.g., -→ X , denotes a vector of random variables.</p><p>-→ X = (x1, x2, ..., xn) denotes an n-dimensional vector of random variables. Abusing the mathematical notation, we use -→ X = {x1, x2, ..., xn} (note the braces "{}") to represent a vector of random variables which is also a set of indices.</p><p>-→ X = {x1, x2, ..., xn} is a set of indices in the mathematical sense of set. That is, there are no duplicated indices and there is no ordering among the indices x1, x2, ..., xn. Given a candidate solution, say -→ X (i), the symbol f ( -→ X (i)), called the fitness function, represents a measurement of how well the solution -→ X (i) solves the target problem. Subsection 5.2 describes how the measurement f ( -→ X (i)) is computed in the present work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A BRIEF INTRODUCTION TO PARTICLE SWARM OPTIMIZATION</head><p>Particle Swarm Optimization (PSO) comprises a set of search techniques, inspired by the behavior of natural swarms, for solving optimization problems <ref type="bibr" target="#b9">[9]</ref>. In PSO a potential solution to a problem is represented by a particle -→ X (i) = (x (i,1) , x (i,2) , ..., x (i,n) ) in an n-dimensional search space. The coordinates x (i,d) of these particles have a rate of change (velocity) v (i,d) , d = 1, 2, ..., n. Every particle keeps a record of the best position that it has ever visited. Such a record is called the particle's previous best position and denoted by -→ B (i). The global best position attained by any particle so far is also recorded and stored in a particle denoted by -→ G . An iteration comprises evaluation of each particle, then stochastic adjustment of v (i,d) in the direction of particle -→ X (i)'s previous best position and the previous best position of any particle in the neighborhood <ref type="bibr">[8]</ref>. There is much variety in the neighborhood topology used in PSO, but quite often gbest or lbest topologies are used. In the gbest topology every particle has only the global best particle -→ G as its neighbor. In the lbest topology, usually, each particle has a number of other particles to its right and left as neighbors. For a review of the neighborhood topologies used in PSO the reader is referred to <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>.</p><p>Generally speaking, the set of rules that govern PSO are: evaluate, compare and imitate. The evaluation phase measures how well each particle (candidate solution) solves the problem at hand. The comparison phase identifies the best particles. The imitation phase produces new particles based on some of the best particles previously found. These three phases are repeated until a given stopping criterion is met. The objective is to find the particle that best solves the target problem.</p><p>Important concepts in PSO are velocity and neighborhood topology. Each particle, -→ X (i), is associated with a velocity vector. This velocity vector is updated at every generation. The updated velocity vector is then used to generate a new particle -→ X (i). The neighborhood topology defines how other particles in the swarm, such as -→ B (i) and -→ G , interact with -→ X (i) to modify its respective velocity vector and, consequently, its position as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE STANDARD BINARY PSO ALGORITHM</head><p>The standard binary version of the PSO algorithm <ref type="bibr" target="#b9">[9]</ref> works as follows. Potential solutions (particles) to the target problem are encoded as fixed length binary strings; i.e., -→ X (i) = (x (i,1) , x (i,2) , ..., x (i,n) ), where x (i,j) ∈ {0, 1}, i = 1, 2,..., N and j = 1, 2, ..., n. Given a list of attributes A = (A1, A2, ..., An), the first element of -→ X (i), from the left to the right hand side, corresponds to the first attribute "A1", the second to the second attribute "A2", and so forth. A value of 0 on the site associated to an attribute signifies that the respective attribute is not selected. A value of 1 means that it is selected. For example, given the list of attributes A = (A1, A2, A3, A4, A5) and N = 4, a swarm could look like this:</p><formula xml:id="formula_0">- → X (1) = (0, 1, 1, 0, 1) - → X (2) = (1, 0, 0, 1, 0) - → X (3) = (0, 1, 0, 1, 1) - → X (4) = (1, 1, 1, 0, 0)</formula><p>In this example, particle -→ X (1) = (0, 1, 1, 0, 1) represents a candidate solution where attributes A2, A3 and A5 are the only attributes selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The initial population for the standard binary PSO algorithm</head><p>For the initial population, N binary strings of length n are randomly generated. Each particle -→ X (i) is independently generated as follows. For every position x (i,d) of -→ X (i) a uniform random number ϕ is drawn on the interval (0, 1). If ϕ &lt; 0.5, then x (i,d) = 1, otherwise x (i,d) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Updating the records</head><p>At the beginning, the previous best position of -→ X (i), denoted by -→ B (i), is empty. Therefore, once the initial particle </p><formula xml:id="formula_1">- → X (i) is generated, - → B (i) is set to - → B (i) = - → X (i). After that, every time that - → X (i) is updated, - → B (i) is also updated if f ( - → X (i)) is better than f ( - → B (i)). Otherwise, - → B (i)</formula><formula xml:id="formula_2">→ B (i) previously computed. After that, - → G is updated if the fittest f ( - → B (i)) in the swarm is better than f ( - → G (i)). And, in that case, f ( - → G (i)) is set to f ( - → G (i)) = fittest f ( - → B (i)). Otherwise, - →</formula><p>G remains as it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Updating the velocities for the standard binary PSO algorithm</head><p>Every particle i is associated to a unique vector of velocities V (i) = (v (i,1) , v (i,2) , ..., v (i,n) ). The elements v (i,d) in V (i) determine the rate of change of each respective coordinate <ref type="bibr" target="#b1">(1)</ref> where w (0 &lt; w &lt; 1), called the inertia weight, is a constant value chosen by the user. Equation 1 is a standard equation used in PSO algorithms to update the velocities <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b14">14]</ref></p><formula xml:id="formula_3">x (i,d) in - → X (i), d = 1, 2, ..., n. Each element v (i,d) ∈ V (i) is updated according to the equation: v (i,d) = w v (i,d) + ϕ1(b (i,d) -x (i,d) ) + ϕ2(g (d) -x (i,d) ),</formula><formula xml:id="formula_4">. Note that x (i,d) is the d th component of - → X (i); b (i,d) is the d th component of - → B (i); g (d) is the d th component of - →</formula><p>G and d = 1, 2, ..., n. The factors ϕ1 and ϕ2 are uniform random numbers independently generated in the interval (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sampling new particles for the standard binary PSO algorithm</head><p>New particles are then sampled as follows. For each particle i and each dimension d, the value of the new coordinate x (i,d) ∈ -→ X (i) can be either 0 or 1. The decision of whether x (i,d) will be 0 or 1 is based on its respective velocity v (i,d) ∈ V (i) and is given by the following equation:</p><formula xml:id="formula_5">x (i,d) =  1, if(rand &lt; S(v (i,d) )) 0, otherwise;<label>(2)</label></formula><p>where 0 ≤ rand ≤ 1 is a uniform random number and</p><formula xml:id="formula_6">S(v (i,d) ) = 1 1 + exp(-v (i,d) )</formula><p>is the sigmoid function. Equation 2 is a standard equation used to sample new particles in the binary PSO algorithm <ref type="bibr" target="#b9">[9]</ref>. Note that the lower the value of v (i,d) the more likely the value of x (i,d) will be 0. By contrast, the higher the value of v (i,d) the more likely the value of x (i,d) will be 1. The next section presents the DPSO algorithm prosed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE PROPOSED DISCRETE PSO ALGORITHM</head><p>This algorithm deals with discrete variables (attributes) and its population of candidate solutions contains particles of different sizes. Potential solutions to the optimization problem at hand are represented by a swarm of particles. There are N particles in a swarm. The length of each particle may vary from 1 to n, where n is the number of attributes of the problem. Each particle -→ X (i) keeps a record of the best position it has ever attained. This information is stored in a separated particle labeled as -→ B (i). The swarm also keeps a record of the global best position ever attained by any particle in the swarm. This information is also stored in a separated particle labeled</p><formula xml:id="formula_7">- → G . Note that - → G is equal to the best - → B (i) present in the swarm.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding of the particles for the proposed DPSO algorithm</head><p>Each attribute is identified by a unique positive integer number, or index. These numbers, indices, vary from 1 to n. A particle is a subset of non-ordered indices without repetition, e.g., -→ X (i) = {2, 4, 18, 1}. For example, given the list of attributes (A1, A2, A3, A4, A5) and N = 4, a swarm could look like this:</p><formula xml:id="formula_8">- → X (1) = {4, 1, 2} - → X (2) = {5} - → X (3) = {2, 1} - → X (4) = {1, 3, 2, 5, 4}</formula><p>Note that in the particle -→ X (1) = {4, 1, 2}, 4 corresponds to attribute A4, 1 to A1 and 2 to A2. Therefore, -→ X (1) = {4, 1, 2} represents a candidate solution where the attributes A4, A1, and A2 have been selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The initial population for the proposed DPSO algorithm</head><p>The initial population of particles is generated as follows. At the beginning, an integer random number determines the number of attribute indices, or length, that a particle will have. This number is uniformly generated between 1 and n, inclusive, individually for every single particle. Particles bear their randomly chosen size throughout the execution of the algorithm. Once the dimensionality is known, the actual particle is generated with as many attribute indices as the previously chosen random number that corresponds to its size. For instance, if the uniform random number k ∈ {1, 2, 3, ..., n} that determines the length of a particle is drawn as k = 2, a particle that contains exactly 2 attribute indices is generated. Those indices are also randomly chosen from the sequence I = {1, 2, 3, ..., n}, one at a time, and without replacement. It means that there will be no repeated indices on the configuration of any single particle. Algorithm 1 shows a pseudocode for the encoding of a discrete particle. Note that the particles are completely generated one-by-one. After the initial population is generated, Algorithm 1 Encoding of a discrete particle Require: i, j, , n, N , -→ X (i), I = {1, 2, ..., n} and 1 ≤ rand ≤ n, where rand is a uniform random number, rand ∈ {1, 2, ..., n}.</p><formula xml:id="formula_9">1: for i = 1 to N 2: = rand 3: - → X (i) = ∅ 4: I = {1, 2, ..., n} 5:</formula><p>for j = 1 to 6:</p><p>Randomly select an attribute (index) from I 7:</p><p>Insert the selected attribute (index) in -→ X (i) 8:</p><formula xml:id="formula_10">I = I - - → X (i), (Recall that - → X (i) is also a set.) 9:</formula><p>end for 10: end for the information in -→ B (i) and -→ G is then updated exactly as described in Subsection 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Velocities = proportional likelihoods</head><p>The DPSO algorithm does not use a vector of velocities as the standard PSO algorithm does. It works with proportional likelihoods instead. Arguably, the notion of proportional likelihood used in the DPSO algorithm and the notion of velocity used in the standard PSO are somewhat similar. Every particle is associated with a 2-by-n array of proportional likelihoods, where 2 is the number of rows in this array and n is the number of columns. A generic proportional likelihood array looks like this:</p><formula xml:id="formula_11">V (i) = " proportional likelihood row attribute index row</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>«</head><p>. Each of the n elements in the first row of V (i) represents the proportional likelihood that an attribute be selected. The second row of V (i) shows the indices of the attributes associated with the respective proportional likelihoods. There is a one-to-one correspondence between the columns of this array and the attributes of the problem domain. At the beginning, all elements in the first row of V (i) are set to 1, e.g.,</p><formula xml:id="formula_12">V (i) = " 1 1 1 1 1 1 2 3 4 5 « .</formula><p>After the initial population of particles is generated, this array is always updated before a new configuration for the particle associated to it is generated. The updating process is based on -→ X (i), -→ B (i) and -→ G and works as follows. In addition to -→ X (i), -→ B (i) and -→ G , three constant updating factors, namely, α, β and γ, are also used to update the proportional likelihoods v (i,d) . These factors determine the strength of the contribution of -→ X (i), -→ B (i) and -→ G to the adjustment of every coordinate v (i,d) ∈ V (i). Note that α, β and γ are parameters chosen by the user. The contribution of these parameters to the updating of v (i,d) is as follows. All indices present in -→ X (i) have their correspondent proportional likelihood increased by α. In addition to that, all indices present in -→ B (i) have their correspondent proportional likelihood increased by β. The same for -→ G for which the proportional likelihoods are increased by γ. For instance, given n = 5, α = 0.10, β = 0.12, γ = 0.14,</p><formula xml:id="formula_13">- → X (i) = {2, 3, 4}, - → B (i) = {3, 5, 2}, - → G = {5, 2}</formula><p>and</p><formula xml:id="formula_14">V (i) = " 1 1 1 1 1 1 2 3 4 5</formula><p>« , the updated V (i) would be:</p><formula xml:id="formula_15">V (i) = " 1 1 + α + β + γ 1 + α + β 1 + α 1 + β + γ 1 2 3 4 5 « . Note that index 1 is not present in - → X (i), - → B (i) or - → G .</formula><p>Therefore, the proportional likelihood of attribute 1 in V (i) remains as it is. This new updated array replaces the old one and will be used to generate a new configuration to the particle associated to it as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sampling new particles for the proposed DPSO algorithm</head><p>The proportional likelihood array V (i) is then used to sample a new instance of particle -→ X (i) -that is, the particle associated to it. First, every element of the first row of the array V (i) is multiplied by a uniform random number between 0 and 1. A new random number is drawn for every single multiplication performed. To illustrate, suppose that</p><formula xml:id="formula_16">V (i) = " 1 1.36 1.22 1.1 1.26 1 2 3 4<label>5</label></formula><p>« . The multiplied proportional likelihood array would be:</p><formula xml:id="formula_17">V (i) = " 1 × ϕ1 1.36 × ϕ2 1.22 × ϕ3 1.1 × ϕ4 1.26 × ϕ5 1 2 3 4 5 « ,</formula><p>where ϕ1, ..., ϕ5 are uniform random numbers independently drawn on the interval (0, 1). Suppose that the multiplied array V (i) looks like this:</p><formula xml:id="formula_18">V (i) = " 0.11 0.86 0.57 0.62 1.09 1 2 3 4<label>5</label></formula><formula xml:id="formula_19">«</formula><p>. The new particle is then defined by ranking the columns in V (i) by the values in its first row. That is, the elements in the first row of the array are ranked in a decreasing order of value and the indices of the attributes (in the second row of V (i)) follow their respective proportional likelihoods. For example, ranking </p><formula xml:id="formula_20">V (i) = " 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>«</head><p>. After ranking the array V (i), the first k indices (in the second row of V (i)), from left to right, are selected to compose the new particle.</p><p>The constant k represents the length of the particle -→ X (i), the particle associated to the ranked array V (i). Thus, if particle i, the particle associated to the array</p><formula xml:id="formula_21">V (i) = "</formula><p>1.09 0.86 0.62 0.57 0.11 5 2 4 3 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>«</head><p>, has length 3, the first 3 indices from the second row of V (i) would be selected to compose the particle. Based on the array V (i) given above, if k = 3 (that is, -→ X (i) = {*, *, *}) the indices (attributes) 5, 2 and 4 would be selected to compose the new particle, i.e., -→ X (i) = {5, 2, 4}. Note that indices that have a higher proportional likelihood are, on average, more likely to be selected.</p><p>The updating of -→</p><formula xml:id="formula_22">X (i), - → B (i) and - → G is identical to what is described in Subsection 3.2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we report and discuss computational experiments. The quality of a candidate solution (fitness) is computed by the well-known Naive Bayes classifier <ref type="bibr" target="#b11">[11]</ref>. The Naive Bayes classifier uses a probabilistic approach to assign each example (record) of the data set to a possible class. In our application, it assigns a record (protein) of the data set to one of the classes, negative or positive. A Naive Bayes classifier assumes that all attributes are probabilistic independent of one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Postsynaptic data set</head><p>This section presents the bioinformatics data set used in the present work for attribute selection. A synapse is a connection between two neurons: presynaptic and postsynaptic. The first is usually the sender of a "signal", such as the release of chemicals, while the second is the receiver. A postsynaptic receptor is a sensor on the surface of a neuron. It captures messenger molecules from the nervous system, neurotransmitters, and thereby functions in transmitting information from one neuron to another <ref type="bibr" target="#b15">[15]</ref>. The data set used in this paper is called the postsynaptic data set. It has been recently created and mined for the first time in <ref type="bibr" target="#b13">[13]</ref>. The data set contains 4303 records of proteins. These proteins belong to either one of the following two classes: positive or negative. Proteins that belong to the positive class have postsynaptic activity. Proteins that belong to the negative class do not have postsynaptic activity. From the 4303 proteins on the data set, 260 belong to the positive class and 4043 to the negative class.</p><p>This data set is a particularly interesting case study for evaluating the proposed DPSO algorithm for two reasons. First, postsynaptic proteins are involved in the nervous system. Predicting postsynaptic activity is potentially useful for understanding several diseases of the nervous system. Second, this data set has many attributes what makes the attribute selection task challenging. More precisely, each protein has 443 PROSITE patterns, or attributes.</p><p>PROSITE is a database of protein families and domains. It is based on the observation that, while there is a huge number of different proteins, most of them can be grouped, on the basis of similarities in their sequences, into a limited number of families (a protein consists of a sequence of amino acids). PROSITE patterns are small regions within a protein that present a high sequence similarity when compared to other proteins. In our data set the absence of a given PROSITE pattern is indicated by a value of 0 for the attribute corresponding to that PROSITE pattern. The presence of it is indicated by a value of 1 for that same attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Methodology</head><p>The fitness function f ( -→ X (i)) of a any particle i is computed as follows. f ( -→ X (i)) is equal to the predictive accuracy achieved by a Naive Bayes classifier on the postsynaptic data set and using only the attributes present in -→ X (i). The objective is to find the smallest subset of attributes (PROSITE patterns) with which it is possible to classify the proteins on the data set as belonging to one of the classes (positive or negative) with an acceptable accuracy. We define the accuracy as acceptable if it is equal to or better than the one obtained by the classification performed considering all the 443 original attributes. Note that this is a naive and particular definition of acceptable accuracy. We chose this definition because it suits the purpose of our experiments -to compare the performance of the standard binary PSO and the DPSO algorithms in the postsynaptic data set. As a rule, the definition of acceptable accuracy is problem dependent and should take into account prior knowledge of the target problem -when available. In fact, in many real-world applications, minimizing the number of selected attributes while maximizing classification accuracy are conflicting tasks.</p><p>The measurement of f ( -→ X (i)) in this paper follows what in Data Mining is called a wrapper approach. The wrapper approach searches for an optimal attribute subset tailored to a particular algorithm, such as the Naive Bayes classifier. For more information on wrapper and other attribute selection approaches see <ref type="bibr" target="#b18">[18]</ref>.</p><p>The computational experiments involved a 10-fold crossvalidation method <ref type="bibr" target="#b18">[18]</ref>. First, the 4303 records in the postsynaptic data set were divided into 10 almost equally sized folds. There are three folds containing 431 records each one and seven folds containing 430 records each one. The folds were randomly generated but under the following regulation. The proportion of positive and negative classes in every single fold must be similar to the one found in the original data set containing all the 4303 records. This is known as stratified cross-validation. Each of the 10 folds is used once as test set and the remaining of the data set is used as training set. Out of the 9 folds in the training set, one is reserved to be used as a validation set. The Naive Bayes classifier uses the remaining 8 folds to compute the probabilities required to classify new examples. Once those probabilities have been computed, the Naive Bayes classifier classifies the examples in the validation set. The accuracy of this classification on the validation set is the value of the fitness function f ( -→ X (i)). After the run of the PSO algorithm is completed, the 9 folds are merged into a full training set. The Naive Bayes classifier is then trained again on this full training set (9 merged folds), and the probabilities computed in this final, full training set are used to classify examples in the test set (the 10th reserved fold), which was never accessed during the run of the PSO algorithm.</p><p>In each of the 10 iterations of the cross-validation procedure, the predictive accuracy of the classification is assessed by 3 different methods:</p><p>(1) Using all the 443 original attributes: all possible attributes are used by the Naive Bayes classifier.</p><p>(2) Standard binary PSO algorithm: only the attributes selected by the best particle found by the binary PSO algorithm are used by the Naive Bayes classifier.</p><p>(3) Proposed DPSO algorithm: only the attributes selected by the best particle found by the DPSO algorithm are used by the Naive Bayes classifier.</p><p>As the standard binary PSO and the DPSO algorithms are stochastic algorithms, 30 independent runs for each algorithm were performed for every single fold. The results obtained, averaged over 30 runs, are reported in Table <ref type="table">1</ref>. Since the Naive Bayes classifier is deterministic, only one run is performed for the classification using all the 443 original attributes. The average number of attributes selected by the attribute selection algorithms has always been rounded to the nearest integer. The population size used for both algorithms is 200 and the search stops after 20,000 fitness evaluations (or 100 iterations). The binary PSO algorithm uses a inertia weight value of 0.8 (i.e., w = 0.8). The choice of the value of this parameter was based on the work presented in <ref type="bibr" target="#b16">[16]</ref>. Other choices of parameter values were α = 0.10, β = 0.12 and γ = 0.14. These values were empirically determined in our preliminary experiments; but we make no claim that these are optimal values. Parameter optimization is a topic for future research. The measurement of the predictive accuracy rate of a model should be a reliable estimate of how well that model classifies the test examples (unseen during the training phase) on the target problem. In Data Mining, typically, the equation:</p><formula xml:id="formula_23">Standard accuracy rate = T P + T N T P + F P + F N + T N<label>(3)</label></formula><p>is used to assess the accuracy rate of a classifier (see the definition of T P , T N , etc. below). Nevertheless, if the class distribution is highly unbalanced, which is the case with the postsynaptic data set, Equation 3 is an ineffective way of measuring the accuracy rate of a model. For instance, on a data set in which 90% of the examples belong to the positive class and 10% to the negative class, it would be easy to maximize Equation 3 by simply predicting always the majority class. Therefore, on our experiments we use a more demanding measurement for the accuracy rate of a classification model. It has also been used before in <ref type="bibr" target="#b13">[13]</ref>, the paper in which the postsynaptic data set was used for the first time. This measurement is given by the equation:</p><formula xml:id="formula_24">Predictive accuracy rate = T P R × T N R,<label>(4)</label></formula><p>where, T P R = T P T P + F N and T N R = T N T N + F P .</p><p>Note that if any of the quantities T P R or T N R is zero, the value returned by Equation 4 is also zero. Also note that T P (true positives) is the number of records correctly classified as positive class and F P (false positives) is the number of records incorrectly classified as positive class. T N (true negatives) is the number of records correctly classified as negative class and F N (false negatives) is the number of records incorrectly classified as negative class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Analyzing the predictive accuracy values (T P R × T N R) shown in Table <ref type="table">1</ref> we see that, on average, the binary PSO and the DPSO algorithms obtained a higher predictive accuracy value than the classification performed using all the 443 original attributes. The only exception being for fold number 7. The results obtained for the average predictive accuracy values suggest that the predictive accuracy of the Table <ref type="table">1</ref>: The postsynaptic data set. Results of the classification performed by a Naive Bayes classifier using: (1) all possible attributes; (2) only the attributes given by the best solution found by the binary PSO algorithm and (3) only the attributes given by the best solution found by the DPSO algorithm. The results shown for the binary PSO and DPSO algorithms are averaged over 30 independent runs. The average number of attributes selected has always been rounded to the nearest integer.</p><p>Using all the 443 original attributes Binary PSO algorithm Proposed DPSO algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fold</head><p>No. of instances T P R T N R classification of the proteins on the data set analyzed improves when using not all but only a small subset of relevant attributes to perform the classification.</p><formula xml:id="formula_25">T P R × T N R T P R T N R T P R × T N R No. of attributes selected T P R T N R T P R × T N R No. of</formula><p>To assess the performance of the binary PSO algorithm and the DPSO algorithm we have considered two criteria: (1) maximizing predictive accuracy; and (2) finding the smallest subset of attributes. Comparing the first criterion, accuracy, we note that the DPSO algorithm did slightly better than the binary PSO algorithm. However, the difference is negligible (the average predictive accuracy value for the binary PSO algorithm was equal to 0.73, whereas for the DPSO algorithm it was equal to 0.74). The discriminating factor between the performance of these algorithms seems to be on the second criteria, finding the smallest subset of attributes. For all the 10 folds the DPSO algorithm selected a smaller subset of attributes. The difference between the average number of attributes selected by the binary PSO algorithm, which was equal to 27.10, and by the DPSO algorithm, which was equal to 12.70, clearly indicates that the second algorithm tends to finding a smaller subset of attributes than the first one does. One of the reasons for that seems to be the way in which the initial population of particles is generated for each algorithm. For the binary PSO algorithm the average number of attributes selected (which is the number of ones in the particle's configuration) on the particles of the initial population follows a binomial distribution <ref type="bibr" target="#b3">[3]</ref>. The binomial distribution is given by:</p><formula xml:id="formula_26">P (k|n) = n! k!(n -k)! p k (1 -p) n-k ,<label>(5)</label></formula><p>where P (k|n) represents the probability of obtaining exactly k successes out of n Bernoulli trials. Translating it to the binary PSO algorithm, n represents the total number of attributes (n = 443) of the problem and k the number of selected attributes (the number of ones in the particle's configuration). The result of each Bernoulli trial is true with probability p and false with probability q = (1p) <ref type="bibr" target="#b12">[12]</ref>. Note that for the initial particles generated by the binary PSO algorithm, the probability that an attribute be selected (suc-cess) is p = 0.5 and the probability of it not be selected is q = (1 -0.5) = 0.5. Therefore, the probability of a particle with k attributes being generated is equal to:</p><formula xml:id="formula_27">P (k|443) = 443! k!(443 -k)! 0.5 k (1 -0.5) 443-k = 443! k!(443 -k)! 0.5 443 .<label>(6)</label></formula><p>From Equation <ref type="formula" target="#formula_27">6</ref>we conclude that particles that have nearly 221 attributes selected (k ≈ 221) are more likely to be generated than particles with any other number of selected attributes. The more the value k distances from 221 towards 1 or towards 443, the less likely a particle with length equal to k will be generated. Therefore, the length of the particles in the initial population of the binary PSO algorithm will be, on average, concentrated around n 2 . Recall that in our application n 2 ≈ 221. We carried out an experiment that seems</p><p>to corroborate what has been said. We generated an initial population of particles for the problem addressed in this paper exactly as described in Subsection 3.1. Recall that in this case n = 443 and 1 ≤ k ≤ 443. This population contains 10,000 randomly generated particles. We then recorded the length of every particle generated. Figure <ref type="figure" target="#fig_0">1</ref> shows the number of occurrences of every particle's size (from 1 to 443) observed on those 10,000 randomly generated particles. For the DPSO algorithm the average number of attributes selected on the particles of the initial population is given by the equation:</p><formula xml:id="formula_28">P (k) = 1 n , ∀ k ∈ {1, 2, ..., n},<label>(7)</label></formula><p>where n represents the total number of attributes (n = 443) of the problem and k the number of selected attributes, or the length of the particle. From Equation <ref type="formula" target="#formula_28">7</ref>we conclude that the probability of a particle of length k being generated is equal to 1 n for all k ∈ {1, 2, ..., n}. Therefore, the length of the particles in the initial population of the DPSO algorithm tends to be uniformly distributed between 1 and n. We also generated an initial population of particles for the problem addressed in this paper exactly as described in Section 4. This population contains 10,000 randomly generated particles. We then recorded the length of every particle generated. Figure <ref type="figure" target="#fig_2">2</ref> shows the number of occurrences of every particle's size (from 1 to 443) observed on those 10,000 randomly generated particles. The fact that the DPSO algorithm has many more small sized particles in the initial population, by comparison with the standard binary PSO algorithm, seems to help the former to obtain smaller sets of selected attributes than the latter -as shown in the results of Table <ref type="table">1</ref>. Another trend observed in the results was  the high frequency of some attributes on the best particles found at each run of the attribute selection algorithms. We recorded the best particle found by each algorithm (i.e., the binary PSO and the DPSO algorithms) on each of the 30 runs and for every one of the 10 folds. We then computed the frequency of the attributes selected on the 10 × 30 = 300 best particles found by the standard binary PSO algorithm. The same was done for the proposed DPSO algorithm. The following 10 attributes have been selected, by both algorithms, in more than 85% of their respective 300 best particles found: A134, A162, A186, A320, A321, A333, A342, A351, A352 and A353. The names of the PROSITE patterns that correspond to these attributes are shown in Table <ref type="table" target="#tab_3">2</ref>. The information was obtained from the web site of the European Bioinformatics Institute, UniProtKB/Swiss-Prot. Address: http://www.ebi.ac.uk/swissprot/. Neurotransmitter-gated ion-channel A353/ps00237</p><p>Rhodopsin-like GPCR superfamily This information may be useful for a biologist. It suggests that those 10 PROSITE patterns are the most relevant ones for the classification of the proteins in the given data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>The computational results indicate that the use of variables apparently unrelated to the class attribute tends to reduce the accuracy and reliability of a classification model on the postsynaptic data set. Using fewer attributes, the binary PSO and the DPSO algorithms obtained, on average, a better predictive accuracy than the classification performed using all the 443 original attributes. The overall results also indicate that, in the data set used in this paper, the DPSO algorithm performed as well as or better than the binary PSO algorithm. These two algorithms obtained a similar level of predictive accuracy. However, the DPSO algorithm clearly tends to find smaller subsets of attributes than the standard binary PSO algorithm does -as shown in Table <ref type="table">1</ref>. Perhaps, a partial reason for this difference is the way in which the initial population is generated for each algorithm. For the standard binary PSO algorithm the average number of attributes in the particles at the initial population is roughly n 2 , where n is the number of attributes of the data set being mined. For the DPSO algorithm the length of the particles in the initial population tends to be uniformly distributed between 1 and n.</p><p>In future research we intend to investigate to what extent the apparent advantage in the performance of the DPSO algorithm over the binary PSO algorithm is because of the way in which the initial populations are generated. This investigation will require the application of the algorithms to a variety of different test problems. We also intend to improve the DPSO algorithm by allowing the particles to vary in length during the execution of the algorithm. Perhaps, a mutation-like operator can be implemented so that the length of a particle may increase or decrease at random. Another idea is to generated the initial population such that it contains one instance of particle of every possible size for a particle from 1 to n, where n is the number of variables of the target problem. Parameter optimization is also a topic for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Binary PSO algorithm: an initial population generated at random. The population contains 10,000 particles which were generated as described in Subsection 3.1 for n = 443 and 1 ≤ k ≤ 443.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>attributes (length of the particle) k Number of occurrences observed in 10,000 random samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DPSO algorithm: an initial population generated at random.The population contains 10,000 particles which were generated as described in section 4 for n = 443 and 1 ≤ k ≤ 443.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>PROSITE patterns selected in more than 85% of the runs performed by the attribute selection algorithms.</figDesc><table><row><cell>Attribute/</cell><cell></cell></row><row><cell>PROSITE</cell><cell>Name</cell></row><row><cell>pattern ID</cell><cell></cell></row><row><cell>A134/ps00086</cell><cell>Cytochrome P450</cell></row><row><cell cols="2">A162/ps00904 Protein prenyltransferase, alpha subunit</cell></row><row><cell>A186/ps00856</cell><cell>Guanylate kinase</cell></row><row><cell>A320/ps00713</cell><cell>Sodium: dicarboxylate symporter</cell></row><row><cell>A321/ps00714</cell><cell>Sodium: dicarboxylate symporter</cell></row><row><cell>A333/ps00405</cell><cell>43 kDa postsynaptic protein</cell></row><row><cell>A342/ps00410</cell><cell>Dynamin</cell></row><row><cell>A351/ps00232</cell><cell>Cadherin</cell></row><row><cell>A352/ps00236</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>Thanks to Gisele L. Pappa for kindly providing us with the postsynaptic data set.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-swarm optimization in dynamic environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3005</biblScope>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using a genetic algorithm for solving a capacity p-median problem</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carnieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Algorithms</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="373" to="388" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pisani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Purves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics. W. W. Norton &amp; Company</title>
		<imprint>
			<date type="published" when="1997-09">September 1997</date>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Data Mining and Knowledge Discovery with Evolutionary Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-10">October 2002</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hierarchical particle swarm optimizer for dynamic optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Janson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Middendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evoworkshops 2004: 1st European Workshop on Evolutionary Algorithms in Stochastic and Dynamic Environments</title>
		<meeting><address><addrLine>Coimbra, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="513" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A particle swarm optimisation approach in the construction of optimal risky portfolios</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd IASTED International Multi-Conference on Applied Informatics</title>
		<meeting>the 23rd IASTED International Multi-Conference on Applied Informatics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
	<note>Artificial intelligence and applications</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Small worlds and mega-minds: effects of neighborhood topology on particle swarm performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Congress of Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Angeline</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Schoenauer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zalzala</surname></persName>
		</editor>
		<meeting>the Congress of Evolutionary Computation<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1931" to="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discrete binary version of the particle swarm algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 Conference on Systems, Man, and Cybernetics</title>
		<meeting>the 1997 Conference on Systems, Man, and Cybernetics<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="4104" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Swarm Intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extending particle optimisers with self-organized criticality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Løvbjerg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Congress on Evolutionary Computation CEC2002</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Fogel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>El-Sharkawi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Greenwood</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Iba</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Marrow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Shackleton</surname></persName>
		</editor>
		<meeting>the 2002 Congress on Evolutionary Computation CEC2002</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1588" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Probability, Random Variables and Stochastic Processes With Errata Sheet</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Pillai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-12">December 2001</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting post-synaptic activity in proteins with data mining</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Pappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="19" to="25" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring extended particle swarms: a genetic programming approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Chio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Langdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO&apos;05: Proceedings of the 2005 Conference on Genetic and Evolutionary Computation</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nerve Endings: the Discovery of the Synapse</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rapport</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
			<publisher>W. W. Norton &amp; Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parameter selection in particle swarm optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EP&apos;98: Proceedings of the 7th International Conference on Evolutionary Programming</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithms for the vehicle routing and scheduling problems with time window constraints</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="265" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
