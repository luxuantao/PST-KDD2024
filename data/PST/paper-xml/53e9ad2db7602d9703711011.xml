<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bursty Feature Representation for Clustering Text Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Block N4, Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuiyu</forename><surname>Chang</surname></persName>
							<email>askychang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Block N4, Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Block N4, Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<email>jzhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Block N4, Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bursty Feature Representation for Clustering Text Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C472238093176355792E6F72450FFBB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text representation plays a crucial role in classical text mining, where the primary focus was on static text. Nevertheless, well-studied static text representations including TFIDF are not optimized for non-stationary streams of information such as news, discussion board messages, and blogs. We therefore introduce a new temporal representation for text streams based on bursty features. Our bursty text representation differs significantly from traditional schemes in that it 1) dynamically represents documents over time, 2) amplifies a feature in proportional to its burstiness at any point in time, and 3) is topic independent. Our bursty text representation model was evaluated against a classical bagof-words text representation on the task of clustering TDT3 topical text streams. It was shown to consistently yield more cohesive clusters in terms of cluster purity and cluster/class entropies. This new temporal bursty text representation can be extended to most text mining tasks involving a temporal dimension, such as modeling of online blog pages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The performance of a text mining solution often depends critically on the underlying text representation. Modern information sources including news, discussion boards, chat messages, and blogs manifest themselves as text streams/sequences of chronologically ordered documents. With the additional temporal dimension in text streams, classical document representations that have been optimized for static text mining problems (e.g., text clustering and text classification) are poised for a major revamp.</p><p>The classical text representation, known as the bagof-words or Vector Space Model (VSM) <ref type="bibr" target="#b11">[12]</ref>, represents a document as a vector; with each element denoting the weight/importance associated with a feature in the document. Popular weighting methods include binary, term frequency (TF), and term frequencyinverse document frequency (TFIDF). The feature space typically includes raw words, word stems, phrases, extracted named entities, etc.</p><p>The static VSM is not designed to meaningfully model a transition from one semantic context to another over time, not to mention representing evolving trends in text streams. Specifically, a feature in static VSM could refer to completely different topics at various points in time, e.g., most occurrences of the word feature "war" in news collections circa 1998 refer to "the war between NATO and the Serbs", whereas the same word feature found in news collections circa 2002 mostly refers to the "war between US and Iraq" or "war on terrorism". Grouping words into phrases (e.g., n-grams) as new features and assigning part-ofspeech tags to each words can improve the semantic meaning somewhat, but neither approach takes into consideration the time dimension of text streams.</p><p>An up and coming topic is usually accompanied by a sharp rise in the reporting frequency of some distinctive features, known as "bursty features". These bursty features could be used to more accurately portray the semantics of an evolving topic. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the effectiveness of using top bursty features to represent two separate topics. Had we used the usual feature selection and weighting scheme, the word features "Gingrich" and "Newt" frequent in both related but different topics would turn up almost equally important for representing documents of these two topics. We therefore propose a new text stream representation model, called bursty feature representation, which can emulate sophisticated temporal and topical behaviour via bursty features, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. In our model, a burst corresponds to a phenomenon in which a large amount of text content about a particu-Downloaded 09/04/15 to 221.232.232.96. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php lar topic is generated over a relatively short time period. Our model varies the bursty features' weights of documents with respect to its publication date. Unlike static document representations, this model dynamically represents a document over time, i.e., a document representation is fully dependent on its publication date. Such a dynamic representation is particularly suitable for modeling text streams, especially for information sources grouped by topics. To the best of our knowledge, we are unaware of any similar prior work.</p><p>The remainder of this paper is organized as follows. Section 2 describes related work. In Section 3, we cover the background knowledge of document representation along with the motivation for our bursty feature representation. In Section 4, we describe how to identify bursty features, and subsequently define our self-boosting model for bursty feature representation of text streams. Experimental clustering results are presented in Section 5. Finally, conclusions are drawn and future research directions are identified in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This work is largely inspired by three broad and overlapping research fields.</p><p>First, a significant number of recent work has modeled a topic in text streams as a "burst of activities" <ref type="bibr" target="#b6">[7]</ref>. Many practical applications have been developed as a result of research in this field. Babcock et al. <ref type="bibr" target="#b0">[1]</ref> present a query operator scheduling strategy to minimize runtime memory usage during bursty time periods. Kumar et al. <ref type="bibr" target="#b8">[9]</ref> applied Kleinberg's algorithm <ref type="bibr" target="#b6">[7]</ref> to identify bursty communities in a Weblog graph. Mei et al. <ref type="bibr" target="#b10">[11]</ref> further summarized the evolutionary thematic patterns of a text stream by identifying bursty behavior. Our approach shares some common ideas as the work of Fung et al. <ref type="bibr" target="#b5">[6]</ref>, who clustered bursty features and organized them into different bursty events. However, our work is distinct from <ref type="bibr" target="#b5">[6]</ref> in two aspects: 1) we use bursty features combined with static features to completely represent each and every document in the text stream, and 2) as a proof-of-concept, we cluster actual documents instead of words, i.e., we find groups of topical bursty documents instead of groups of topical bursty features.</p><p>Second, our work is motivated by research in the field of Topic Detection and Tracking (TDT). In TDT, a large amount of research has previously been conducted on identifying emerging topical trends, i.e., detecting new events <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b15">17]</ref> and tracking topics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. However, the vast majority of TDT research does not differentiate trivial topics from bursty topics. Neither do they utilize the time interval information of text streams. For example, the online event detection model proposed by Kumaran et al. <ref type="bibr" target="#b7">[8]</ref> simply used time to obtain the document arrival order. In this paper, we emphasize the importance of time by incorporating it directly into the document representation. Our grouping of similar documents in time and content into corresponding bursty topics can be viewed a new kind of topic tracking and identification model in TDT.</p><p>The third inspiration of this work comes from tackling the "curse of dimensionality" problem <ref type="bibr" target="#b3">[4]</ref> while processing the hundreds of thousands of unique features in text mining. Documents in very high-dimensional space are almost equally far away from one another, making classical similarity measures like Euclidean distances less discriminative. Thus, an important preprocessing step in most text mining tasks is to reduce the dimensionality or feature space. By selecting only bursty features, as will be shown later, we are in fact reducing the dimensionality of the problem space. Another interesting work by Yu et al. <ref type="bibr" target="#b16">[18]</ref> studied the correlation between features with respect to class concepts, with the goal of removing redundant features. This is different from our work which models the dynamic association of word features and documents to topics (classes) utilizing the additional temporal dimension. A detailed survey of text feature selection can be found in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Terminologies</head><p>Let D be a corpus of text streams with N documents and T be the time period spanned by D. Let F be the complete static VSM features space with |F | = M . <ref type="figure" target="#fig_1">2</ref>, static VSM creates a document vector from a raw text document in two steps: <ref type="bibr" target="#b0">(1)</ref> applying text preprocessing such as stopword removal and stemming, and (2) assigning weights to text features as vector elements. In static VSM, each document d i is represented by a vector of M feature weights,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Static VSM As shown in Figure</head><formula xml:id="formula_0">d i = [d i1 , d i2 , . . . , d iM ] T ,</formula><p>where d ij is the weight of the j-th feature for document d i and T denotes the matrix transpose. The weight of a feature determines how much it contributes to the overall document vector d i .</p><p>A document representation typically involves:</p><p>• Feature definition: determine the type of features Downloaded 09/04/15 to 221.232.232.96. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php to be derived from a document, e.g., term feature type, title term feature type.</p><p>• Feature selection/transformation: select and transform features, e.g., both rarely used and overly common term features may be discarded.</p><p>• Feature weighting: assign weight to a feature type based on a given formula, e.g., binary, TF, and TFIDF.</p><p>3.2 Motivation for Bursty Topic Representation Bursty topics can be emphasized by considering only certain time windows and bursty features. For example, the word feature "hurricane" from topic "Hurricane Mitch" shown in Figure <ref type="figure" target="#fig_2">3</ref>  total of 401 documents contain the feature "hurricane", among which 97 are outside of the "Hurricane Mitch" topic. If "hurricane" were the only feature discriminating this topic from the rest, the corresponding precision would be (401 -97)/401 = 75.81%. From Figure <ref type="figure" target="#fig_2">3</ref>, if we only take into account the bursty period of word feature "hurricane" lasting from 24-Oct to 16-Nov, only 10 out of the 260 documents containing the word feature "hurricane" are off-topic, thereby yielding a 20% improvement in precision at (260 -10)/260 = 96.15%! The above simple example illustrates the benefit of using a single bursty feature restricted to certain bursty time periods over the whole life span of a topic. If more bursty periods and their associated bursty features are identified, they could collectively improve the overall distinctiveness of each topic.</p><p>Such discriminative features with corresponding bursty life spans are called "bursty features" in this paper. To find bursty features, the burstiness of every word feature in the corpus with respect to all topics will have to be computed. In this example, we would need to determine a set of features for the "Hurricane Mitch" topic, and their corresponding bursty life spans. This is a challenging problem because the i-th document in a text stream now has a dynamic vector representation d i (t) that depends on time stamp t. In the next section, we shall present our proposed bursty feature representation to this challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bursty Feature Representation</head><p>We now describe our bursty feature representation that combines burstiness with static feature weights. Representing a document with bursty features involves two major steps: (1) identifying bursty features, and (2) representing documents using bursty features/weights, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. In Figure <ref type="figure" target="#fig_3">4</ref>, a document is assigned bursty weights depending on its time stamp t. The same raw document may have different bursty feature representations at two different time points t i = t j .</p><p>In Section 4.1, we will first describe how bursty features can be identified. This is followed by the bursty feature representations in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bursty Feature Identification</head><p>Bursty feature identification from text streams have recently been investigated by a number of researchers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">19]</ref>. Since the goal of this paper is to utilize bursty features and not to develop a new bursty feature identification algorithm, we simply adopt Kleinberg's <ref type="bibr" target="#b6">[7]</ref> 2-state finite automaton model to identify bursty features, as shown in Figure <ref type="figure" target="#fig_4">5</ref>. There are two states q 0 and q 1 in the finite automaton model A of Figure <ref type="figure" target="#fig_4">5</ref>. For every feature f in a text stream, when A is in state q 0 at time point t, it has a low emission rate β 0 = |R d |/T , where |R d | is the size of all relevant documents containing f over the whole Downloaded 09/04/15 to 221.232.232.96. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php time range T of the text stream. When A is in state q 1 at time t, the rate is increased to</p><formula xml:id="formula_1">β 1 = s • |R d |/T , where β 1 &gt; β 0 because s &gt; 1.</formula><p>In other words, we have defined two emission states for each feature f : one with the average document frequency over the whole time duration, and another one with a higher document frequency. The larger the number of documents containing f at time point t, the higher the likelihood of f being identified as a bursty feature at t.</p><p>We compute the burstiness of each feature f over all topics and over the full time period of the text stream, i.e., retrospective analysis. Some features may not be bursty at all (zero burst), while others may induce multiple bursty periods. The formal definition of a "bursty feature" is given below. Definition 4.1. (bursty feature) If a feature f i has at least one burst, it is a bursty feature with bursty weight w i and bursty period p i . In Kleinberg's algorithm, the bursty weight is defined as the cost improvement incurred by assigning state q 1 over time period p i rather than state q 0 . Figure <ref type="figure" target="#fig_6">6</ref> plots an example of the bursty feature "impeachment" with two bursts over the entire text stream. The first burst persists from 3-Oct to 10-Oct, 1998 (8 days) with a bursty weight of 13.6775, whereas the second and larger burst spans 16 days from 7-Dec to 22-Dec, 1998 with a bursty weight of 96.0530.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bursty Feature Representation</head><p>In practice, due to the sparsity of bursty features and the relatively short period under consideration, a bursty-feature-only representation frequently degenerates into a zero-vector. To understand this phenomenon, just consider any document that either has no bursty features or whose bursty features happen to be all dormant at time t. Clearly, its corresponding bursty feature representation becomes a zero vector, making any similarity comparisons to it meaningless. Therefore, to overcome the zero-vector problem with the pure bursty feature representation, we propose a self-boosting representation that falls back to the static VSM vector in the worst case as follows.</p><p>Let B denote the bursty feature space and B ⊆ F . Let F P ij denote the static feature weight (i.e., binary weighting) of f j in document d i .</p><p>Definition 4.2. (Bursty Feature Representation) A document d i (t) at time t has a bursty feature representation in the form</p><formula xml:id="formula_2">d i (t) = [d i1 (t), d i2 (t), . . . , d iM (t)] T , where d ij (t) = F P ij + δw j , if f j ∈ B ∧ t ∈ p j , F P ij , otherwise,</formula><p>where δ &gt; 0 is the burst coefficient.</p><p>Here, the role of δ is to combine the sufficiency properties of the static VSM feature space with the discriminative and accuracy properties of bursty features. In other words, bursty features are enhanced or boosted by a factor of δw j . Non-bursty documents will simply fall back to their static feature representation in the bursty feature space as illustrated in Figure <ref type="figure" target="#fig_7">7</ref>. The optimal δ can be determined experimentally via cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments 5.1 Dataset and Experimental Setup</head><p>The TDT3 [14] dataset includes news articles collected during the three month period of October through December 1998. Among these, 37,526 English articles originated from 8 English sources, and 13,657 Chinese articles came from 3 Chinese sources. We extracted all on-topic English news articles as TDT3-Eng, which contains 8,458 articles covering 116 topics.</p><p>After stopword removal, 125,468 distinct features remained in TDT3-Eng. Among these, 2,160 distinct bursty features with 2,646 bursts were identified as bursty feature space (set B) using the 2-state automaton model described in Section 4.1. We independently selected another 2,160 features (set F ) using the document frequency thresholding technique <ref type="bibr" target="#b13">[15]</ref>.</p><p>For a fair comparison, only bursty features in F ∩ B are used in our bursty feature representation. Finally we have 1,244 distinct bursty features (|F ∩ B| = 1, 244) with 1,695 bursts, averaging 1.36 bursts per bursty feature. Downloaded 09/04/15 to 221.232.232.96. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php 5.2 Baseline Representation We compare our bursty feature representation to the static binary VSM. Had we used TFIDF, some rare features (with low DF) would be emphasized, and the boosting effect of our bursty feature representation would not be clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metrics</head><p>Assume that K clusters are generated for dataset D. Let |k j | Ci denote the number of documents from topic C i assigned to cluster k j . Similarly, let |C i | kj denote the number of documents from cluster k j originating from class C i . We evaluate our clustering results using the known data class labels as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Cluster Purity</head><p>The purity of cluster k j is defined by</p><formula xml:id="formula_3">purity(k j ) = 1 |k j | max i (|k j | Ci ).</formula><p>The overall purity of a clustering solution is expressed as a weighted sum of individual cluster purities</p><formula xml:id="formula_4">cluster purity = K j=1 |k j | |D| purity(k j ) = 1 |D| K j=1 max i |k j | Ci .</formula><p>In general, the larger the purity value the better the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Cluster Entropy</head><p>Cluster entropy measures the diversity of a cluster k j , and is defined as</p><formula xml:id="formula_5">entropy(k j ) = - i |k j | Ci |k j | log |k j | Ci |k j | .</formula><p>The total entropy of a cluster solution is</p><formula xml:id="formula_6">cluster entropy = K j=1 |k j | |D| entropy(k j ).</formula><p>A good clustering algorithm should have low cluster entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Class Entropy</head><p>Both cluster purity and entropy measure the homogeneity of a cluster, but neither of them measures the recall of each topic. Thus, we introduce class entropy as follow:</p><formula xml:id="formula_7">entropy(C i ) = - j |C i | kj |C i | log |C i | kj |C i | .</formula><p>The total class entropy of a cluster solution is</p><formula xml:id="formula_8">class entropy = K i=1 |C i | |D| entropy(C i ).</formula><p>Ideally, we want the class entropy to be as small as possible.</p><p>5.4 Clustering TDT3-Eng We applied K-means (K = 116) clustering to TDT3-Eng, which comprises 116 topics. Since bursty features are identified based on TDT3-Eng itself, the burst coefficient δ is set to 1 to simulate the circumstance in which both static features (in [0, 1]) and normalized bursty features (in [0, 1]) contribute equally to the representation. Table <ref type="table">1</ref> lists the 3 evaluation metrics averaged over 10 clustering runs for the binary VSM and bursty feature representations. The metrics are also plotted in Figure <ref type="figure" target="#fig_8">8</ref>, which shows the mean, spread (standard deviation) in each direction, and range. From Table <ref type="table">Table</ref>  1, we see that bursty features resulted in clusters with on average 10.06% and 6.81% lower cluster and class entropies, respectively, and 6.93% higher cluster purity. Figure <ref type="figure" target="#fig_8">8</ref> further highlights that bursty feature representation generated more consistent and stable clustering solutions with lower variance and better results in all three metrics.</p><p>The results are very encouraging considering that 1) many of the topics in TDT3-Eng are small (with just a few documents) and non-bursty, and 2) there is a fair amount of overlap in bursty feature space between the various topics.  1. In this work, we have only benchmarked the bursty feature representation against a binary VSM. We would like to compare it with other well-known text representations, especially TFIDF.</p><p>2. We have so far only shown the superiority of our bursty feature representation for clustering. We would like to evaluate its suitability for other tasks such as classification and regression.</p><p>3. Since we are using Kleinberg's model, we are in fact applying retrospective burst analysis to text streams. In online applications, an alternative online burst analysis model has to be developed in order to analyze and estimate the burstiness of new documents/words as and when they arrive.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Frequent features of two topics (bursty features shown in bold).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Creating a static VSM model from raw text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Traffic overlap between a bursty topic and its feature "hurricane".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An overview of bursty feature representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A 2-state finite automaton model for identifying bursty features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>-98 21-Oct-98 10-Nov-98 30-Nov-98 20-Dec-98 time bursty weight</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of a bursty feature "impeachment".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Practical bursty feature representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Averaged clustering results for TDT3-Eng over 10 runs, showing the mean (end points of line joining the two box plots), spread (box), and range (vertical line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1: Averaged clustering results for TDT3-Eng over 10 runs.</figDesc><table><row><cell>representation</cell><cell>cluster</cell><cell>cluster</cell><cell>class</cell></row><row><cell></cell><cell>purity</cell><cell cols="2">entropy entropy</cell></row><row><cell>binary VSM</cell><cell>0.5750</cell><cell>0.5682</cell><cell>0.8553</cell></row><row><cell cols="2">bursty feature 0.6149</cell><cell cols="2">0.5110 0.7971</cell></row><row><cell cols="4">Improvement 6.93% 10.06% 6.81%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>6</head><label></label><figDesc>Conclusions and Future WorkIn this paper, we introduced a new bursty feature representation for highlighting the temporally important features in text streams. The model builds on the classical VSM model and adds an adjustable weight to features considered bursty. With this bursty feature representation, a document may have very different bursty feature representations at different points in time, i.e., the bursty document vector is dynamic and dependent on time. This captures the importance of bursty words at various periods in time. Experimental results on the TDT3 dataset confirmed and quantified the improvements brought about by our bursty feature representation over a binary VSM model.As discussed in the paper, many challenges await our future work. Downloaded 09/04/15 to 221.232.232.96. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>0.63</cell><cell>clus ter purity</cell><cell>0.60</cell><cell>clus ter entropy</cell><cell>0.90</cell><cell>clas s entropy</cell></row><row><cell>0.62</cell><cell></cell><cell>0.58</cell><cell></cell><cell>0.88</cell><cell></cell></row><row><cell>0.55 0.56 0.57 0.58 0.59 0.60 0.61</cell><cell></cell><cell>0.48 0.50 0.52 0.54 0.56</cell><cell></cell><cell>0.76 0.78 0.80 0.82 0.84 0.86</cell><cell></cell></row><row><cell>0.54</cell><cell></cell><cell>0.46</cell><cell></cell><cell>0.74</cell><cell></cell></row><row><cell>0.53</cell><cell></cell><cell>0.44</cell><cell></cell><cell>0.72</cell><cell></cell></row><row><cell></cell><cell>binary VSM</cell><cell>burs ty feature</cell><cell>binary VSM</cell><cell>burs ty feature</cell><cell>binary VSM</cell><cell>burs ty feature</cell></row><row><cell></cell><cell>repres entation</cell><cell>repres entation</cell><cell>repres entation</cell><cell>repres entation</cell><cell>repres entation</cell><cell>repres entation</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Copyright © by SIAM. Unauthorized reproduction of this article is prohibited</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Chain: operator scheduling for memory minimization in data stream systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A system for New Event Detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farahat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lexical Chains for Topic Tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, National University of Dublin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An algorithm for approximate closestpoint queries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AChf SCG</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="160" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised and supervised clustering for topic tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="310" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parameter free bursty events detection in text streams</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P C</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bursty and hierarchical structure in streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text classification and named entities for new event detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On the Bursty Evolution of Blogspace</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="159" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward Integrating Feature Selection Algorithms for Classification and Clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering evolutionary theme patterns from text: an exploration of temporal text mining</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="198" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<title level="m">Term-weighting approaches in automatic text retrieval, Information Processing and Management</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining semantic and syntactic document classifiers to improve first story detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="424" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Study of Retrospective and On-Line Event Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topicconditioned Novelty Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="688" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="856" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient elastic burst detection in data streams</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Redistribution subject to SIAM license or copyright</title>
		<idno>Downloaded 09/04/15 to 221.232.232.96</idno>
		<ptr target="http://www.siam.org/journals/ojsa.php" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
