<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Personalization using Embeddings for Search Ranking at Airbnb</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
							<email>mihajlo.grbovic@airbnb.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>Inc. San Francisco</addrLine>
									<settlement>Airbnb</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haibin</forename><surname>Cheng</surname></persName>
							<email>haibin.cheng@airbnb.com</email>
							<affiliation key="aff1">
								<address>
									<addrLine>Inc. San Francisco</addrLine>
									<settlement>Airbnb</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-time Personalization using Embeddings for Search Ranking at Airbnb</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3219885</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems ? Content ranking</term>
					<term>Web log analysis</term>
					<term>Personalization</term>
					<term>Query representation</term>
					<term>Document representation</term>
					<term>Search Ranking</term>
					<term>User Modeling</term>
					<term>Personalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search Ranking and Recommendations are fundamental problems of crucial interest to major Internet companies, including web search engines, content publishing websites and marketplaces. However, despite sharing some common characteristics a one-size-fitsall solution does not exist in this space. Given a large difference in content that needs to be ranked, personalized and recommended, each marketplace has a somewhat unique challenge. Correspondingly, at Airbnb, a short-term rental marketplace, search and recommendation problems are quite unique, being a two-sided marketplace in which one needs to optimize for host and guest preferences, in a world where a user rarely consumes the same item twice and one listing can accept only one guest for a certain set of dates. In this paper we describe Listing and User Embedding techniques we developed and deployed for purposes of Real-time Personalization in Search Ranking and Similar Listing Recommendations, two channels that drive 99% of conversions. The embedding models were specifically tailored for Airbnb marketplace, and are able to capture guest's short-term and long-term interests, delivering effective home listing recommendations. We conducted rigorous offline testing of the embedding models, followed by successful online tests before fully deploying them into production.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>During last decade Search architectures, which were typically based on classic Information Retrieval, have seen an increased presence of Machine Learning in its various components <ref type="bibr" target="#b1">[2]</ref>, especially in Search Ranking which often has challenging objectives depending on the type of content that is being searched over. The main reason behind this trend is the rise in the amount of search data that can be collected and analyzed. The large amounts of collected data open up possibilities for using Machine Learning to personalize search results for a particular user based on previous searches and recommend similar content to recently consumed one.</p><p>The objective of any search algorithm can vary depending on the platform at hand. While some platforms aim at increasing website engagement (e.g. clicks and time spent on news articles that are being searched), others aim at maximizing conversions (e.g. purchases of goods or services that are being searched over), and in the case of two sided marketplaces we often need to optimize the search results for both sides of the marketplace, i.e. sellers and buyers. The two sided marketplaces have emerged as a viable business model in many real world applications. In particular, we have moved from the social network paradigm to a network with two distinct types of participants representing supply and demand. Example industries include accommodation (Airbnb), ride sharing (Uber, Lyft), online shops (Etsy), etc. Arguably, content discovery and search ranking for these types of marketplaces need to satisfy both supply and demand sides of the ecosystem in order to grow and prosper.</p><p>In the case of Airbnb, there is a clear need to optimize search results for both hosts and guests, meaning that given an input query with location and trip dates we need to rank high listings whose location, price, style, reviews, etc. are appealing to the guest and, at the same time, are a good match in terms of host preferences for trip duration and lead days. Furthermore, we need to detect listings that would likely reject the guest due to bad reviews, pets, length of stay, group size or any other factor, and rank these listings lower.</p><p>To achieve this we resort to using Learning to Rank. Specifically, we formulate the problem as pairwise regression with positive utilities for bookings and negative utilities for rejections, which we optimize using a modified version of Lambda Rank <ref type="bibr" target="#b3">[4]</ref> model that jointly optimizes ranking for both sides of the marketplace.</p><p>Since guests typically conduct multiple searches before booking, i.e. click on more than one listing and contact more than one host during their search session, we can use these in-session signals, i.e. clicks, host contacts, etc. for Real-time Personalization where the aim is to show to the guest more of the listings similar to the ones we think they liked since staring the search session. At the same time we can use the negative signal, e.g. skips of high ranked listings, to show to the guest less of the listings similar to the ones we think they did not like. To be able to calculate similarities between listings that guest interacted with and candidate listings that need to be ranked we propose to use listing embeddings, low-dimensional vector representations learned from search sessions. We leverage these similarities to create personalization features for our Search Ranking Model and to power our Similar Listing Recommendations, the two platforms that drive 99% of bookings at Airbnb.</p><p>In addition to Real-time Personalization using immediate user actions, such as clicks, that can be used as proxy signal for shortterm user interest, we introduce another type of embeddings trained on bookings to be able to capture user's long-term interest. Due to the nature of travel business, where users travel 1-2 times per year on average, bookings are a sparse signal, with a long tail of users with a single booking. To tackle this we propose to train embeddings at a level of user type, instead of a particular user id, where type is determined using many-to-one rule-based mapping that leverages known user attributes. At the same time we learn listing type embeddings in the same vector space as user type embeddings. This enables us to calculate similarities between user type embedding of the user who is conducting a search and listing type embeddings of candidate listings that need to be ranked.</p><p>Compared to previously published work on embeddings for personalization on the Web, novel contributions of this paper are:</p><p>? Real-time Personalization -Most of the previous work on personalization and item recommendations using embeddings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> is deployed to production by forming tables of user-item and item-item recommendations offline, and then reading from them at the time of recommendation. We implemented a solution where embeddings of items that user most recently interacted with are combined in an online manner to calculate similarities to items that need to be ranked. For short-term interest personalization we trained listing embeddings using more than 800 million search clicks sessions, resulting in high quality listing representations. We used extensive offline and online evaluation on real search traffic which showed that adding embedding features to the ranking model resulted in significant booking gain. In addition to the search ranking algorithm, listing embeddings were successfully tested and launched for similar listing recommendations where they outperformed the existing algorithm click-through rate (CTR) by 20%.</p><p>For long-term interest personalization we trained user type and listing type embeddings using sequences of booked listings by 50 million users. Both user and listing type embeddings were learned in the same vector space, such that we can calculate similarities between user type and listing types of listings that need to be ranked. The similarity was used as an additional feature for search ranking model and was also successfully tested and launched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In a number of Natural Language Processing (NLP) applications classic methods for language modeling that represent words as highdimensional, sparse vectors have been replaced by Neural Language models that learn word embeddings, i.e. low-dimensional representations of words, through the use of neural networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref>. The networks are trained by directly taking into account the word order and their co-occurrence, based on the assumption that words frequently appearing together in the sentences also share more statistical dependence. With the development of highly scalable continuous bag-of-words (CBOW) and skip-gram (SG) language models for word representation learning <ref type="bibr" target="#b16">[17]</ref>, the embedding models have been shown to obtain state-of-the-art performance on many traditional language tasks after training on large text data.</p><p>More recently, the concept of embeddings has been extended beyond word representations to other applications outside of NLP domain. Researchers from the Web Search, E-commerce and Marketplace domains have quickly realized that just like one can train word embeddings by treating a sequence of words in a sentence as context, same can be done for training embeddings of user actions, e.g. items that were clicked or purchased <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, queries and ads that were clicked <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, by treating sequence of user actions as context. Ever since, we have seen embeddings being leveraged for various types of recommendations on the Web, including music recommendations <ref type="bibr" target="#b25">[26]</ref>, job search <ref type="bibr" target="#b12">[13]</ref>, app recommendations <ref type="bibr" target="#b20">[21]</ref>, movie recommendations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>, etc. Furthermore, it has been shown that items which user interacted with can be leveraged to directly lean user embeddings in the same feature space as item embeddings, such that direct user-item recommendations can be made <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Alternative approach, specifically useful for cold-start recommendations, is to still to use text embeddings (e.g. ones publicly available at https://code.google.com/p/word2vec) and leverage item and/or user meta data (e.g. title and description) to compute their embeddings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>. Finally, similar extensions of embedding approaches have been proposed for Social Network analysis, where random walks on graphs can be used to learn embeddings of nodes in graph structure <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Embedding approaches have had a major impact in both academia and industry circles. Recent industry conference publications and talks show that they have been successfully deployed in various personalization, recommendation and ranking engines of major Web companies, such as Yahoo <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>, Etsy <ref type="bibr" target="#b0">[1]</ref>, Criteo <ref type="bibr" target="#b17">[18]</ref>, Linkedin <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, Tinder <ref type="bibr" target="#b15">[16]</ref>, Tumblr <ref type="bibr" target="#b9">[10]</ref>, Instacart <ref type="bibr" target="#b21">[22]</ref>, Facebook <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In the following we introduce the proposed methodology for the task of listing recommendations and listing ranking in search at Airbnb. We describe two distinct approaches, i.e. listing embeddings for short-term real-time personalization and user-type &amp; listing type embeddings for long term personalization, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Listing Embeddings</head><p>Let us assume we are given a set S of S click sessions obtained from N users, where each session s = (l 1 , . . . , l M ) ? S is defined as an uninterrupted sequence of M listing ids that were clicked by the user. A new session is started whenever there is a time gap of more than 30 minutes between two consecutive user clicks. Given this data set, the aim is to learn a d-dimensional real-valued representation v l i ? R d of each unique listing l i , such that similar listings lie nearby in the embedding space.</p><p>More formally, the objective of the model is to learn listing representations using the skip-gram model <ref type="bibr" target="#b16">[17]</ref> by maximizing the objective function L over the entire set S of search sessions, defined as follows</p><formula xml:id="formula_0">L = s ?S l i ?s -m ?j ?m,i 0 log P(l i+j |l i ) ,<label>(1)</label></formula><p>Probability P(l i+j |l i ) of observing a listing l i+j from the contextual neighborhood of clicked listing l i is defined using the soft-max</p><formula xml:id="formula_1">P(l i+j |l i ) = exp(v ? l i v ? l i +j ) | V | l =1 exp(v ? l i v ? l ) ,<label>(2)</label></formula><p>where v l and v ? l are the input and output vector representations of listing l, hyperparameter m is defined as a length of the relevant forward looking and backward looking context (neighborhood) for a clicked listing, and V is a vocabulary defined as a set of unique listings ids in the data set. From (1) and <ref type="bibr" target="#b1">(2)</ref> we see that the proposed approach models temporal context of listing click sequences, where listings with similar contexts (i.e., with similar neighboring listings in search sessions) will have similar representations.</p><p>Time required to compute gradient ?L of the objective function in (1) is proportional to the vocabulary size |V |, which for large vocabularies, e.g. several millions listing ids, is an infeasible task. As an alternative we used negative sampling approach proposed in <ref type="bibr" target="#b16">[17]</ref>, which significantly reduces computational complexity. Negative sampling can be formulated as follows. We generate a set D p of positive pairs (l, c) of clicked listings l and their contexts c (i.e., clicks on other listings by the same user that happened before and after click on listing l within a window of length m), and a set D n of negative pairs (l, c) of clicked listings and n randomly sampled listings from the entire vocabulary V. The optimization objective then becomes where parameters ? to be learned are v l and v c , l, c ? V. The optimization is done via stochastic gradient ascent.</p><formula xml:id="formula_2">argmax ? (l,c)? D p log 1 1 + e -v ? c v l + (l,c)? D n log 1 1 + e v ? c v l ,<label>(3)</label></formula><p>Booked Listing as Global Context. We can break down the click sessions set S into 1) booked sessions, i.e. click sessions that end with user booking a listing to stay at, and 2) exploratory sessions, i.e. click sessions that do not end with booking, i.e. users were just browsing. Both are useful from the standpoint of capturing contextual similarity, however booked sessions can be used to adapt the optimization such that at each step we predict not only the neighboring clicked listings but the eventually booked listing as well. This adaptation can be achieved by adding booked listing as global context, such that it will always be predicted no matter if it is within the context window or not. Consequently, for booked sessions the embedding update rule becomes</p><formula xml:id="formula_3">argmax ? (l,c )?Dp log 1 1 + e -v ? c v l + (l,c )?Dn log 1 1 + e v ? c v l + log 1 1 + e -v ? l b v l ,<label>(4)</label></formula><p>where v l b is the embedding of the booked listing l b . For exploratory sessions the updates are still conducted by optimizing objective (3).</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a graphical representation of how listing embeddings are learned from booked sessions using a sliding window of size 2n + 1 that slides from the first clicked listing to the booked listing. At each step the embedding of the central listing v l is being updated such that it predicts the embeddings of the context listings v c from D p and the booked listing v l b . As the window slides some listings fall in and out of the context set, while the booked listing always remains within it as global context (dotted line).</p><p>Adapting Training for Congregated Search. Users of online travel booking sites typically search only within a single market, i.e. location they want to stay at. As a consequence, there is a high probability that D p contains listings from the same market. On the other hand, due to random sampling of negatives, it is very likely that D n contains mostly listings that are not from the same markets as listings in D p . At each step, for a given central listing l, the positive context mostly consist of listings from the same market as l, while the negative context mostly consists of listings that are not from the same market as l. We found that this imbalance leads to learning sub-optimal within-market similarities. To address this issue we propose to add a set of random negatives D m n , sampled from the market of the central listing l,</p><formula xml:id="formula_4">argmax ? (l,c )?Dp log 1 1 + e -v ? c v l + (l,c )?Dn log 1 1 + e v ? c v l + log 1 1 + e -v ? l b v l + (l,mn )?Dm n log 1 1 + e v ? mn v l . (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where parameters ? to be learned are v l and v c , l, c ? V. Upon listing creation the host is required to provide information about the listing, such as location, price, listing type, etc. We use the provided meta-data about the listing to find 3 geographically closest listings (within a 10 miles radius) that have embeddings, are of same listing type as the new listing (e.g. Private Room) and belong to the same price bucket as the new listing (e.g. $20 -$25 per night). Next, we calculate the mean vector using 3 embeddings of identified listings to form the new listing embedding. Using this technique we are able to cover more than 98% of new listings.</p><p>Examining Listing Embeddings. To evaluate what characteristics of listings were captured by embeddings we examine the d = 32 dimensional embeddings trained using (5) on 800 million click sessions. First, by performing k-means clustering on learned embeddings we evaluate if geographical similarity is encoded. Figure <ref type="figure">2</ref>, which shows resulting 100 clusters in California, confirms that listings from similar locations are clustered together. We found the clusters very useful for re-evaluating our definitions of travel markets. Next, we evaluate average cosine similarities between  <ref type="table" target="#tab_1">1</ref>) and between listings of different price ranges (Table <ref type="table" target="#tab_2">2</ref>). From those tables it can observed that cosine similarities between listings of same type and price ranges are much higher compared to similarities between listings of different types and price ranges. Therefore, we can conclude that those two listing characteristics are well encoded in the learned embeddings as well.</p><p>While some listing characteristics, such as price, do not need to be learned because they can be extracted from listing meta-data, other types of listing characteristics, such as architecture, style and feel are much harder to extract in form of listing features. To evaluate if these characteristics are captured by embeddings we can examine k-nearest neighbors of unique architecture listings in the listing embedding space. Figure <ref type="figure">3</ref> shows one such case where for a listing of unique architecture on the left, the most similar listings are of the same style and architecture. To be able to conduct fast and easy explorations in the listing embedding space we developed an internal Similarity Exploration Tool shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>Demonstration video of this tool, which is available online at https://youtu.be/1kJSAG91TrI, shows many more examples of embeddings being able to find similar listings of the same unique architecture, including houseboats, treehouses, castles, chalets, beachfront apartments, etc. Listing embeddings described in Section 3.1. that were trained using click sessions are very good at finding similarities between listings of the same market. As such, they are suitable for short-term, insession, personalization where the aim is to show to the user listings that are similar to the ones they clicked during the immanent search session. However, in addition to in-session personalization, based on signals that just happened within the same session, it would be useful to personalize search based on signals from user's longerterm history. For example, given a user who is currently searching for a listing in Los Angeles, and has made past bookings in New York and London, it would be useful to recommend listings that are similar to those previously booked ones.</p><p>While some cross-market similarities are captured in listing embeddings trained using clicks, a more principal way of learning such cross-market similarities would be to learn from sessions constructed of listings that a particular user booked over time. Specifically, let us assume we are given a set S b of booking sessions obtained from N users, where each booking session s b = (l b1 , . . . , l b M ) is defined as a sequence of listings booked by user j ordered in time. Attempting to learn embeddings v l id for each listin?_id using this type of data would be challenging in many ways:</p><p>? First, booking sessions data S b is much smaller than click sessions data S because bookings are less frequent events. ? Second, many users booked only a single listing in the past and we cannot learn from a session of length 1. ? Third, to learn a meaningful embedding for any entity from contextual information at least 5 -10 occurrences of that entity are needed in the data, and there are many listin?_ids on the platform that were booked less than 5 -10 times. ? Finally, long time intervals may pass between two consecutive bookings by the user, and in that time user preferences, such as price point, may change, e.g. due to career change.</p><p>To address these very common marketplace problems in practice, we propose to learn embeddings at a level of listin?_type instead of listin?_id. Given meta-data available for a certain listin?_id such as location, price, listing type, capacity, number of beds, etc., we use a  The mapping from listin?_id to a listin?_type is a many-to-one mapping, meaning that many listings will map into the same listin?_type.</p><p>To account for user ever-changing preferences over time we propose to learn user _type embeddings in the same vector space as listin?_type embeddings. The user _type is determined using a similar procedure we applied to listings, i.e. by leveraging metadata about user and their previous bookings, defined in Table <ref type="table" target="#tab_4">4</ref>. For example, for a user from San Francisco with MacBook laptop, English language settings, full profile with user photo, 83.4% average Guest 5 star rating from hosts, who has made 3 bookings in the past, where the average statistics of booked listings were $52.52 Price Per Night, $31.85 Price Per Night Per Guest, 2.33 Capacity, 8.24 Reviews and 76.1% Listing 5 star rating, the resulting user _type is SF _l? 1 _dt 1 _f p 1 _pp 1 _nb 1 _ppn 2 _pp? 3 _c 2 _nr 3 _l5s 3 _?5s 3 . When generating booking sessions for training embeddings we calculate the user _type up to the latest booking. For users who made their first booking user _type is calculated based on the first 5 rows from Table <ref type="table" target="#tab_4">4</ref> because at the time of booking we had no prior information about past bookings. This is convenient, because learned embeddings for user _types which are based on first 5 rows can be used for coldstart personalization for logged-out users and new users with no past bookings.</p><p>Training Procedure. To learn user _type and listin?_type embeddings in the same vector space we incorporate the user _type The objective that needs to be optimized can be defined similarly to <ref type="bibr" target="#b2">(3)</ref>, where instead of listing l, the center item that needs to be updated is either user _type (u t ) or listin?_type (l t ) depending on which one is caught in the sliding window. For example, to update the central item which is a user _type (u t ) we use</p><formula xml:id="formula_6">argmax ? (ut ,c )?D book log 1 1 + e -v ? c vu t + (ut ,c )?Dne? log 1 1 + e v ? c vu t ,<label>(6)</label></formula><p>where D book contains the user _type and listin?_type from recent user history, specifically user bookings from near past and near future with respect to central item's timestamp, while D ne? contains random user _type or listin?_type instances used as negatives.</p><p>Similarly, if the central item is a listin?_type (l t ) we optimize the following objective</p><formula xml:id="formula_7">argmax ? (lt ,c )?D book log 1 1 + e -v ? c v l t + (lt ,c )?Dne? log 1 1 + e v ? c v l t .<label>(7)</label></formula><p>Figure <ref type="figure" target="#fig_3">5a</ref> (on the left) shows a graphical representation of this model, where central item represents user _type (u t ) for which the updates are performed as in <ref type="bibr" target="#b5">(6)</ref>.</p><p>Since booking sessions by definition mostly contain listings from different markets, there is no need to sample additional negatives from same market as the booked listing, like we did in Session 3.1. to account for the congregated search in click sessions.</p><p>Explicit Negatives for Rejections. Unlike clicks that only reflect guest-side preferences, bookings reflect host-side preferences as well, as there exists an explicit feedback from the host, in form of accepting guest's request to book or rejecting guest's request to book. Some of the reasons for host rejections are bad guest star ratings, incomplete or empty guest profile, no profile picture, etc. These characteristics are part of user _type definition from Table <ref type="table" target="#tab_4">4</ref>.</p><p>Host rejections can be utilized during training to encode the host preference signal in the embedding space in addition to the guest preference signal. The whole purpose of incorporating the rejection signal is that some listin?_types are less sensitive to user _types with no bookings, incomplete profiles and less than average guest</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5: Recommendations based on type embeddings</head><p>User Type S F _l? 1 _dt 1 _f p 1 _pp 1 _nb 3 _ppn 5 _pp? 5 _c 4 _nr 3 _l 5s 3 _?5s 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Listing Type</head><p>Sim U S _l t 1 _pn 4 _p? 5 _r 5 _5s 4 _c 2 _b 1 _bd 3 _bt 3 _nu 3 (large, good reviews) 0.629 U S _l t 1 _pn 3 _p? 3 _r 5 _5s 2 _c 3 _b 1 _bd 2 _bt 2 _nu 3 (cheaper, bad reviews) 0.350 U S _l t 2 _pn 3 _p? 3 _r 5 _5s 4 _c 1 _b 1 _bd 2 _bt 2 _nu 3 (priv room, good reviews) 0.241 U S _l t 2 _pn 2 _p? 2 _r 5 _5s 2 _c 1 _b 1 _bd 2 _bt 2 _nu 3 (cheaper, bad reviews) 0.169 U S _l t 3 _pn 1 _p? 1 _r 5 _5s 3 _c 1 _b 1 _bd 2 _bt 2 _nu 3 (shared room, bad reviews) 0.121 star ratings than others, and we want the embeddings of those listin?_types and user _types to be closer in the vector space, such that recommendations based on embedding similarities would reduce future rejections in addition to maximizing booking chances.</p><p>We formulate the use of the rejections as explicit negatives in the following manner. In addition to sets D book and D ne? , we generate a set D r e j of pairs (u t , l t ) of user _type or listin?_type that were involved in a rejection event. As depicted in Figure <ref type="figure" target="#fig_3">5b</ref> (on the right), we specifically focus on the cases when host rejections (labeled with a minus sign) were followed by a successful booking (labeled with a plus sign) of another listing by the same user. The new optimization objective can then be formulated as</p><formula xml:id="formula_8">argmax ? (ut ,c )?D book log 1 1 + exp -v ? c vu t + (ut ,c )?Dne? log 1 1 + exp v ? c vu t + (ut ,lt )?D r e j ec t log 1 1 + exp v ? l t vu t . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>in case of updating the central item which is a user _type (u t ), and</p><formula xml:id="formula_10">argmax ? (lt ,c )?D book log 1 1 + exp -v ? c v l t + (lt ,c )?Dne? log 1 1 + exp v ? c v l t + (lt ,ut )?D r e j ec t log 1 1 + exp v ? u t v l t . (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>in case of updating the central item which is a listin?_type (l t ). Given learned embeddings for all user _types and listin?_types, we can recommend to the user the most relevant listings based on the cosine similarities between user's current user _type embedding and listin?_type embeddings of candidate listings. For example, in Table <ref type="table">5</ref> we show cosine similarities between user _type = SF _l? 1 _dt 1 _f p 1 _pp 1 _nb 3 _ppn 5 _pp? 5 _c 4 _nr 3 _l5s 3 _?5s 3 who typically books high quality, spacious listings with lots of good reviews and several different listin?_types in US. It can be observed that listing types that best match these user preferences, i.e. entire home, lots of good reviews, large and above average price, have high cosine similarity, while the ones that do not match user preferences, i.e. ones with less space, lower price and small number of reviews have low cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section we first cover the details of training Listing Embeddings and their Offline Evaluation. We then show Online Experiment Results of using Listing Embeddings for Similar Listing Recommendations on the Listing Page. Finally, we give background on our Search Ranking Model and describe how Listing Embeddings and Listing Type &amp; User Type Embeddings were used to implement features for Real-time Personalization in Search. Both applications of embeddings were successfully launched to production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Listing Embeddings</head><p>For training listing embeddings we created 800 million click sessions from search, by taking all searches from logged-in users, grouping them by user id and ordering clicks on listing ids in time. This was followed by splitting one large ordered list of listing ids into multiple ones based on 30 minute inactivity rule. Next, we removed accidental and short clicks, i.e. clicks for which user stayed on the listing page for less than 30 seconds, and kept only sessions consisting of 2 or more clicks. Finally, the sessions were anonymized by dropping the user id column. As mentioned before, click sessions consist of exploratory sessions &amp;. booked sessions (sequence of clicks that end with booking). In light of offline evaluation results we oversampled booked sessions by 5x in the training data, which resulted in the best performing listing embeddings.</p><p>Setting up Daily Training. We learn listing embeddings for 4.5 million Airbnb listings and our training data practicalities and parameters were tuned using offline evaluation techniques presented below. Our training data is updated daily in a sliding window manner over multiple months, by processing the latest day search sessions and adding them to the dataset and discarding the oldest day search sessions from the dataset. We train embeddings for each listin?_id, where we initialize vectors randomly before training (same random seed is used every time). We found that we get better offline performance if we re-train listing embeddings from scratch every day, instead of incrementally continuing training on existing vectors. The day-to-day vector differences do not cause discrepancies in our models because in our applications we use the cosine similarity as the primary signal and not the actual vectors themselves. Even with vector changes over time, the connotations of cosine similarity measure and its ranges do not change.</p><p>Dimensionality of listing embeddings was set to d = 32, as we found that to be a good trade-off between offline performance and memory needed to store vectors in RAM memory of search machines for purposes of real-time similarity calculations. Context window size was set to m = 5, and we performed 10 iterations over the training data. To implement the congregated search change to the algorithm we modified the original word2vec c code 1 . Training used MapReduce, where 300 mappers read data and a single reducer trains the model in a multi-threaded manner. End-to-end daily data generation and training pipeline is implemented using Airflow 2 , which is Airbnb's open-sourced scheduling platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Offline Evaluation of Listing Embeddings</head><p>To be able to make quick decisions regarding different ideas on optimization function, training data construction, hyperparameters, etc, we needed a way to quickly compare different embeddings.</p><p>One way to evaluate trained embeddings is to test how good they are in recommending listings that user would book, based on the most recent user click. More specifically, let us assume we are given the most recently clicked listing and listing candidates that need to be ranked, which contain the listing that user eventually booked. By calculating cosine similarities between embeddings of clicked listing and candidate listings we can rank the candidates and observe the rank position of the booked listing.  For purposes of evaluation we use a large number of such search, click and booking events, where rankings were already assigned by our Search Ranking model. In Figure <ref type="figure" target="#fig_5">6</ref> we show results of offline evaluation in which we compared several of d = 32 embeddings with regards to how they rank the booked listing based on clicks that precede it. Rankings of booked listing are averaged for each click leading to the booking, going as far back as 17 clicks before the booking to the Last click before the booking. Lower values mean higher ranking. Embedding versions that we compared were 1) d32: trained using (3), 2) d32 book: trained with bookings as global context (4) and 3) d32 book + neg: trained with bookings as global context and explicit negatives from same market <ref type="bibr" target="#b4">(5)</ref>.</p><p>It can be observed that Search Ranking model gets better with more clicks as it uses memorization features. It can also be observed that re-ranking listings based on embedding similarity would be useful, especially in early stages of the search funnel. Finally, we can conclude that d32 book + neg outperforms the other two embedding versions. The same type of graphs were used to make decisions regarding hyperparameters, data construction, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Similar Listings using Embeddings</head><p>Every Airbnb home listing page<ref type="foot" target="#foot_0">3</ref> contains Similar Listings carousel which recommends listings that are similar to it and available for the same set of dates. At the time of our test, the existing algorithm for Similar Listings carousel was calling the main Search Ranking model for the same location as the given listing followed by filtering on availability, price range and listing type of the given listing.</p><p>We conducted an A/B test where we compared the existing similar listings algorithm to an embedding-based solution, in which similar listings were produced by finding the k-nearest neighbors in listing embedding space. Given learned listing embeddings, similar listings for a given listing l were found by calculating cosine similarity between its vector v l and vectors v j of all listings from the same market that are available for the same set of dates (if check-in and check-out dates are set). The K listings with the highest similarity were retrieved as similar listings. The calculations were performed online and happen in parallel using our sharded architecture, where parts of embeddings are stored on each of the search machines.</p><p>The A/B test showed that embedding-based solution lead to a 21% increase in Similar Listing carousel CTR (23% in cases when listing page had entered dates and 20% in cases of dateless pages) and 4.9% increase in guests who find the listing they end up booking in Similar Listing carousel. In light of these results we deployed the embedding-based Similar Listings to production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Real time personalization in Search</head><p>Ranking using Embeddings Background. To formally describe our Search Ranking Model, let us assume we are given training data about each search D s = (x i , y i ), i = 1...K, where K is the number of listings returned by search, x i is a vector containing features of the i-th listing result and y i ? {0, 0.01, 0.25, 1, -0.4} is the label assigned to the i-th listing result. To assign the label to a particular listing from the search result we wait for 1 week after search happened to observe the final outcome, which can be y i = 1 if listing was booked, y i = 0.25 if listing host was contacted by the guest but booking did not happen, y = -0.4 if listing host rejected the guest, y i = 0.01 is listing was clicked and y i = 0 if listing was just viewed but not clicked. After that 1 week wait the set D s is also shortened to keep only search results up to the last result user clicked on K c ? K. Finally, to form data D = N s=1 D s we only keep D s sets which contain at least one booking label. Every time we train a new ranking model we use the most recent 30 days of data.</p><p>Feature vector x i for the i-th listing result consists of listing features, user features, query features and cross-features. Listing features are features associated with the listing itself, such as price per night, listing type, number of rooms, rejection rate, etc. Query features are features associated with the issued query, such as number of guests, length of stay, lead days, etc. User features are features associated with the user who is conducting the search, such as average booked price, guest rating, etc. Cross-features are features derived from two or more of these feature sources: listing, user, query. Examples of such features are query listing distance: distance between query location and listing location, capacity fit: difference between query number of guests and listing capacity, price difference: difference between listing price and average price of user's historical bookings, rejection probability: probability that host will reject these query parameters, click percentage: real-time memorization feature that tracks what percentage of user's clicks were on that particular listing, etc. The model uses approximately 100 features. For conciseness we will not list all of them.</p><p>Next, we formulate the problem as pairwise regression with search labels as utilities and use data D to train a Gradient Boosting Decision Trees (GBDT) model, using package 4 that was modified to support Lambda Rank. When evaluating different models offline, we use NDCG, a standard ranking metric, on hold-out set of search sessions, i.e. 80% of D for training and 20% for testing.</p><p>Finally, once the model is trained it is used for online scoring of listings in search. The signals needed to calculate feature vectors x i for each listing returned by search query q performed by user u are all calculated in an online manner and scoring happens in parallel using our sharded architecture. Given all the scores, the listings are shown to the user in a descending order of predicted utility.  Next, we introduced several user short-term history sets, that hold user actions from last 2 weeks, which are updated in real-time as new user actions happen. The logic was implemented using using Kafka<ref type="foot" target="#foot_1">5</ref> . Specifically, for each user_id we collect and maintain (regularly update) the following sets of listing ids:</p><p>(1) H c : clicked listing_ids -listings that user clicked on in last 2 weeks. (2) H lc : long-clicked listing_ids -listing that user clicked and stayed on the listing page for longer than 60 sec. (3) H s : skipped listing_ids -listings that user skipped in favor of a click on a lower positioned listing (4) H w : wishlisted listing_ids -listings that user added to a wishlist in last 2 weeks. (5) H i : inquired listing_ids -listings that user contacted in last 2 weeks but did not book. (6) H b : booked listing_ids -listings that user booked in last 2 weeks.</p><p>We further split each of the short-term history sets H * into subsets that contain listings from the same market. For example, if user had clicked on listings from New York and Los Angeles, their set H c would be further split into H c (NY ) and H c (LA).</p><p>Finally, we define the embedding features which utilize the defined sets and the listing embeddings to produce a score for each candidate listing. The features are summarized in Table <ref type="table" target="#tab_5">6</ref>.</p><p>In the following we describe how EmbClickSim feature is computed using H c . The rest of the features from top rows of Table <ref type="table" target="#tab_5">6</ref> are computed in the same manner using their corresponding user short-term history set H * .</p><p>To compute EmbClickSim for candidate listing l i we need to compute cosine similarity between its listing embedding v l i and embeddings of listings in H c . We do so by first computing H c market-level centroid embeddings. To illustrate, let us assume H c contains 5 listings from NY and 3 listings from LA. This would entail computing two market-level centroid embeddings, one for NY and one for LA, by averaging embeddings of listing ids from each of the markets. Finally, EmbClickSim is calculated as maximum out of More generally EmbClickSim can be expressed as</p><formula xml:id="formula_12">EmbClickSim(l i , H c ) = max m ?M cos(v l i , l h ?m,l h ?H c v l h ),<label>(10)</label></formula><p>where M is the set of markets user had clicks in.</p><p>In addition to similarity to all user clicks, we added a feature that measures similarity to the latest long click, EmbLastLongClickSim. For a candidate listing l i it is calculated by finding the cosine similarity between its embedding v l i and the embedding of the latest long clicked listing l l ast from H lc ,</p><formula xml:id="formula_13">EmbLastLon?ClickSim(l i , H lc ) = cos(v l i , v l l as t ).<label>(11)</label></formula><p>User-type &amp; Listing-type Embedding Features. We follow similar procedure to introduce features based on user type and listing type embeddings. We trained embeddings for 500K user types and 500K listing types using 50 million user booking sessions. Embeddings were d = 32 dimensional and were trained using a sliding window of m = 5 over booking sessions. The user type and listing type embeddings were loaded to search machines memory, such that we can compute the type similarities online.</p><p>To compute the UserTypeListingTypeSim feature for candidate listing l i we simply look-up its current listing type l t as well as current user type u t of the user who is conducting the search and calculate cosine similarity between their embeddings,</p><formula xml:id="formula_14">U serTypeListin?TypeSim(u t , l t ) = cos(v u t , v l t ).<label>(12)</label></formula><p>All features from Table <ref type="table" target="#tab_5">6</ref> were logged for 30 days so they could be added to search ranking training set D. The coverage of features, meaning the proportion of D which had particular feature populated, are reported in Table <ref type="table" target="#tab_6">7</ref>. As expected, it can be observed that features based on user clicks and skips have the highest coverage. Finally, we trained a new GBDT Search Ranking model with embedding features added. Feature importances for embedding features (ranking among 104 features) are shown in Table <ref type="table" target="#tab_6">7</ref>. Top ranking features are similarity to listings user clicked on (EmbClick-Sim: ranked 5th overall) and similarity to listings user skipped (EmbSkipSim: ranked 8th overall). Five embedding features ranked among the top 20 features. As expected, long-term feature UserType-ListingTypeSim which used all past user bookings ranked better than short-term feature EmbBookSim which takes into account only bookings from last 2 weeks. This also shows that recommendations To evaluate if the model learned to use the features as we intended, we plot the partial dependency plots for 3 embedding features: EmbClickSim, EmbSkipSim and UserTypeListTypeSim. These plots show what would happen to listing's ranking score if we fix values of all but a single feature (the one we are examining). On the left subgraph it can be seen that large values of EmbClickSim, which convey that listing is similar to the listings user recently click on, lead to a higher model score. The middle subgraph shows that large values of EmbSkipSim, which indicate that listing is similar to the listings user skipped, lead to a lower model score. Finally, the right subgraph shows that large values of UserTypeListingTypeSim, which indicate that user type is similar to listing type, lead to a higher model score as expected.</p><p>Online Experiment Results Summary. We conducted both offline and online experiments (A/B test). First, we compared two search ranking models trained on the same data with and without embedding features. In Table <ref type="table" target="#tab_7">8</ref> we summarize the results in terms of DCU (Discounted Cumulative Utility) per each utility (impression, click, rejection and booking) and overall NDCU (Normalized Discounted Cumulative Utility). It can be observed that adding embedding features resulted in 2.27% lift in NDCU, where booking DCU increased by 2.58%, meaning that booked listings were ranked higher in the hold-out set, without any hit on rejections (DCU -0.4 was flat), meaning that rejected listings did not rank any higher than in the model without embedding features. Observations from Table <ref type="table" target="#tab_7">8</ref>, plus the fact that embedding features ranked high in GBDT feature importances (Table <ref type="table" target="#tab_6">7</ref>) and the finding that features behavior matches what we intuitively expected (Figure <ref type="figure" target="#fig_7">7</ref>) was enough to make a decision to proceed to an online experiment. In the online experiment we saw a statistically significant booking gain and embedding features were launched to production. Several months later we conducted a back test in which we attempted to remove the embedding features, and it resulted in negative bookings, which was another indicator that the real-time embedding features are effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed a novel method for real-time personalization in Search Ranking at Airbnb. The method learns low-dimensional representations of home listings and users based on contextual co-occurrence in user click and booking sessions. To better leverage available search contexts, we incorporate concepts such as global context and explicit negative signals into the training procedure. We evaluated the proposed method in Similar Listing Recommendations and Search Ranking. After successful test on live search traffic both embedding applications were deployed to production.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Skip-gram model for Listing Embeddings</figDesc><graphic url="image-1.png" coords="3,317.96,83.69,247.15,93.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: California Listing Embedding Clusters</figDesc><graphic url="image-2.png" coords="4,68.00,83.69,211.85,231.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Embeddings Evaluation Tool</figDesc><graphic url="image-4.png" coords="4,317.96,83.69,242.10,304.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Listing Type and User Type Skip-gram model</figDesc><graphic url="image-5.png" coords="6,53.80,83.68,242.11,114.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1 https://code.google.com/p/word2vec 2 http://airbnb.io/projects/airflow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Offline evaluation of Listing Embeddings</figDesc><graphic url="image-6.png" coords="7,327.11,83.69,221.93,145.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>4 https://github.com/yarny/gbdt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Partial Dependency Plots for EmbClickSim, EmbSkipSim and UserTypeListTypeSim</figDesc><graphic url="image-7.png" coords="9,53.80,83.69,504.38,115.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Cosine similarities between different Listing Types</figDesc><table><row><cell>Room Type</cell><cell cols="3">Entire Home Private Room Shared Room</cell></row><row><cell>Entire Home</cell><cell>0.895</cell><cell>0.875</cell><cell>0.848</cell></row><row><cell>Private Room</cell><cell></cell><cell>0.901</cell><cell>0.865</cell></row><row><cell>Shared Room</cell><cell></cell><cell></cell><cell>0.896</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Cosine similarities between different Price Ranges</figDesc><table><row><cell>Price Range</cell><cell cols="5">&lt;$30 $30-$60 $60-$90 $90-$120 $120+</cell></row><row><cell>&lt;$30</cell><cell>0.916</cell><cell>0.887</cell><cell>0.882</cell><cell>0.871</cell><cell>0.854</cell></row><row><cell>$30-$60</cell><cell></cell><cell>0.906</cell><cell>0.889</cell><cell>0.876</cell><cell>0.865</cell></row><row><cell>$60-$90</cell><cell></cell><cell></cell><cell>0.902</cell><cell>0.883</cell><cell>0.880</cell></row><row><cell>$90-$120</cell><cell></cell><cell></cell><cell></cell><cell>0.898</cell><cell>0.890</cell></row><row><cell>$120+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.909</cell></row><row><cell cols="6">3.2 User-type &amp; Listing-type Embeddings</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Mappings of listing meta data to listing type buckets</figDesc><table><row><cell>Buckets</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>Country</cell><cell>US</cell><cell>CA</cell><cell>GB</cell><cell>FR</cell><cell>MX</cell><cell>AU</cell><cell>ES</cell><cell>...</cell></row><row><cell>Listing Type</cell><cell>Ent</cell><cell>Priv</cell><cell>Share</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>$ per Night</cell><cell>&lt;40</cell><cell>40-55</cell><cell>56-69</cell><cell>70-83</cell><cell>84-100</cell><cell>101-129</cell><cell>130-189</cell><cell>190+</cell></row><row><cell>$ per Guest</cell><cell>&lt;21</cell><cell>21-27</cell><cell>28-34</cell><cell>35-42</cell><cell>43-52</cell><cell>53-75</cell><cell>76+</cell><cell></cell></row><row><cell>Num Reviews</cell><cell>0</cell><cell>1</cell><cell>2-5</cell><cell>6-10</cell><cell>11-35</cell><cell>35+</cell><cell></cell><cell></cell></row><row><cell>Listing 5 Star %</cell><cell>0-40</cell><cell>41-60</cell><cell>61-90</cell><cell>90+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Capacity</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6+</cell><cell></cell><cell></cell></row><row><cell>Num Beds</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Num Bedrooms</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Num Bathroom</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>New Guest Acc %</cell><cell>&lt;60</cell><cell>61-90</cell><cell>&gt;91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mappings of user meta data to user type buckets S_lt 1 _pn 3 _p? 3 _r 3 _5s 4 _c 2 _b 1 _bd 2 _bt 2 _nu 3 . Buckets are determined in a data-driven manner to maximize for coverage in each listin?_type bucket.</figDesc><table><row><cell>Buckets</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>Market</cell><cell>SF</cell><cell>NYC</cell><cell>LA</cell><cell>HK</cell><cell>PHL</cell><cell>AUS</cell><cell>LV</cell><cell>...</cell></row><row><cell>Language</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>jp</cell><cell>ru</cell><cell>ko</cell><cell>de</cell><cell>...</cell></row><row><cell>Device Type</cell><cell>Mac</cell><cell>Msft</cell><cell>Andr</cell><cell>Ipad</cell><cell>Tablet</cell><cell>Iphone</cell><cell>...</cell><cell></cell></row><row><cell>Full Profile</cell><cell>Yes</cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Profile Photo</cell><cell>Yes</cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Num Bookings</cell><cell>0</cell><cell>1</cell><cell>2-7</cell><cell>8+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>$ per Night</cell><cell>&lt;40</cell><cell>40-55</cell><cell>56-69</cell><cell>70-83</cell><cell>84-100</cell><cell>101-129</cell><cell>130-189</cell><cell>190+</cell></row><row><cell>$ per Guest</cell><cell>&lt;21</cell><cell>21-27</cell><cell>28-34</cell><cell>35-42</cell><cell>43-52</cell><cell>53-75</cell><cell>76+</cell><cell></cell></row><row><cell>Capacity</cell><cell>&lt;2</cell><cell>2-2.6</cell><cell>2.7-3</cell><cell>3.1-4</cell><cell>4.1-6</cell><cell>6.1+</cell><cell></cell><cell></cell></row><row><cell>Num Reviews</cell><cell>&lt;1</cell><cell>1-3.5</cell><cell>3.6-10</cell><cell>&gt; 10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Listing 5 Star %</cell><cell>0-40</cell><cell>41-60</cell><cell>61-90</cell><cell>90+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Guest 5 Star %</cell><cell>0-40</cell><cell>41-60</cell><cell>61-90</cell><cell>90+</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>rule-based mapping defined in Table</p>3</p>to determine its listin?_type. For example, an Entire Home listing from U S that has a 2 person capacity, 1 bed, 1 bedroom &amp; 1 bathroom, with Average Price Per Night of $60.8, Average Price Per Night Per Guest of $29.3, 5 reviews, all 5 stars, and 100% New Guest Accept Rate would map into listin?_type = U</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Embedding Features for Search Ranking The first step in adding embedding features to our Search Ranking Model was to load the 4.5 million embeddings into our search backend such that they can be accessed in real-time for feature calculation and model scoring.</figDesc><table><row><cell>Feature Name</cell><cell>Description</cell></row><row><cell>EmbClickSim</cell><cell>similarity to clicked listings in H c</cell></row><row><cell>EmbSkipSim</cell><cell>similarity to skipped listings H s</cell></row><row><cell>EmbLongClickSim</cell><cell>similarity to long clicked listings H lc</cell></row><row><cell>EmbWishlistSim</cell><cell>similarity to wishlisted listings H w</cell></row><row><cell>EmbInqSim</cell><cell>similarity to contacted listings H i</cell></row><row><cell>EmbBookSim</cell><cell>similarity to booked listing H b</cell></row><row><cell>EmbLastLongClickSim</cell><cell>similarity to last long clicked listing</cell></row><row><cell cols="2">UserTypeListingTypeSim user type and listing type similarity</cell></row><row><cell cols="2">Listing Embedding Features.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Embedding Features Coverage and Importances</figDesc><table><row><cell>Feature Name</cell><cell cols="2">Coverage Feature Importance</cell></row><row><cell>EmbClickSim</cell><cell>76.16%</cell><cell>5/104</cell></row><row><cell>EmbSkipSim</cell><cell>78.64%</cell><cell>8/104</cell></row><row><cell>EmbLongClickSim</cell><cell>51.05%</cell><cell>20/104</cell></row><row><cell>EmbWishlistSim</cell><cell>36.50%</cell><cell>47/104</cell></row><row><cell>EmbInqSim</cell><cell>20.61%</cell><cell>12/104</cell></row><row><cell>EmbBookSim</cell><cell>8.06%</cell><cell>46/104</cell></row><row><cell>EmbLastLongClickSim</cell><cell>48.28%</cell><cell>11/104</cell></row><row><cell cols="2">UserTypeListingTypeSim 86.11%</cell><cell>22/104</cell></row><row><cell cols="3">based on past bookings are better with embeddings that are trained</cell></row><row><cell cols="3">using historical booking sessions instead of click sessions.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Offline Experiment Results</figDesc><table><row><cell>Metrics</cell><cell>Percentage Lift</cell></row><row><cell cols="2">DCU -0.4 (rejections) +0.31%</cell></row><row><cell>DCU 0.01 (clicks)</cell><cell>+1.48%</cell></row><row><cell>DCU 0.25 (contacts)</cell><cell>+1.95%</cell></row><row><cell>DCU 1 (bookings)</cell><cell>+2.58%</cell></row><row><cell>NDCU</cell><cell>+2.27%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://www.airbnb.com/rooms/433392</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>https://kafka.apache.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We would like to thank the entire <rs type="institution">Airbnb Search Ranking Team</rs> for their contributions to the project, especially <rs type="person">Qing Zhang</rs> and <rs type="person">Lynn Yang</rs>. We would also like to thank <rs type="person">Phillippe Siclait</rs> and <rs type="person">Matt Jones</rs> for creating the Embedding Evaluation Tool. The summary of this paper was published in Airbnb's Medium Blog 6 .</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An Ensemble-based Approach to Click-Through Rate Prediction for Promoted Listings at Etsy</title>
		<author>
			<persName><forename type="first">Kamelia</forename><surname>Aryafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01377</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern information retrieval</title>
		<imprint>
			<biblScope unit="volume">463</biblScope>
			<date type="published" when="1999">1999</date>
			<publisher>ACM press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Item2vec: neural item embedding for collaborative filtering</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2007">2011. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Joint Text Embedding for Personalized Content-based Recommendation</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01084</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields with distributed user embeddings for ad targeting</title>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical neural language models for joint representation of streaming documents and their content</title>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
		<ptr target="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-" />
	</analytic>
	<monogr>
		<title level="m">recommendations-and-real-time-personalization-in-search-601172f7603e Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Proceedings of the 24th International 6</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable semantic matching of queries to ads in sponsored search advertising</title>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2016</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-and content-aware embeddings for query rewriting in sponsored search</title>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2015</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gender and interest targeting for sponsored post advertising at tumblr</title>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1819" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">E-commerce in your inbox: Product recommendations at scale</title>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaikit</forename><surname>Savla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bhagwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Personalized Job Recommendation System at LinkedIn: Practical Challenges and Lessons Learned</title>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
		<meeting>the Eleventh ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="346" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kula</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08439</idno>
		<title level="m">Metadata embeddings for user and item cold-start recommendations</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Learning for Personalized Search and Recommender Systems</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://www.slideshare.net/BenjaminLe4/deep-learning-for-personalized-search-and-recommender-systems" />
	</analytic>
	<monogr>
		<title level="m">Slideshare</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Personalized Recommendations at Tinder: The TinVec Approach</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://www.slideshare.net/SessionsEvents/dr-steve-liu-chief-scientist-tinder-at-mlconf-sf-2017" />
	</analytic>
	<monogr>
		<title level="m">Slideshare</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nedelec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Smirnova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavian</forename><surname>Vasile</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07625</idno>
		<title level="m">Specializing Joint Representations for the task of Product Recommendation</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Embedding-based news recommendation for millions of users</title>
		<author>
			<persName><forename type="first">Shumpei</forename><surname>Okura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Tajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1933" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Smartphone app categorization for interest targeting in advertising marketplace</title>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daneo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiankai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiji</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="93" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learned Embeddings for Search at Instacart</title>
		<author>
			<persName><forename type="first">Sharath</forename><surname>Rao</surname></persName>
		</author>
		<ptr target="https://www.slideshare.net/SharathRao6/learned-embeddings-for-search-and-discovery-at-instacart" />
	</analytic>
	<monogr>
		<title level="m">Slideshare</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language Modelling for Collaborative Filtering: Application to Job Applicant Matching</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Gonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Caillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mich?le</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Tools with Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling User Activities on the Web using Paragraph Vector</title>
		<author>
			<persName><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayato</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Tajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="125" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning music embedding with metadata for context aware recommendation</title>
		<author>
			<persName><forename type="first">Dongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiguang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2016 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlinear latent factorization by embedding multiple user interests</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03856</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">StarSpace: Embed All The Things! arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ranking relevance in yahoo search</title>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Dawei Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mianwei</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chikashi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Nobata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD</title>
		<meeting>the 22nd ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
