<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangtao</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luoqing</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Life Fellow, IEEE</roleName><forename type="first">Yuan</forename><forename type="middle">Yan</forename><surname>Tang</surname></persName>
							<email>yytang@umac.mo</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Statistics</orgName>
								<orgName type="laboratory">Hubei Key Laboratory of Applied Mathemat-ics</orgName>
								<orgName type="institution">Hubei University</orgName>
								<address>
									<postCode>430062</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Science and Technology</orgName>
								<orgName type="institution">University of Macau</orgName>
								<address>
									<postCode>999078</postCode>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Normal University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Mathematics and Statistics</orgName>
								<orgName type="laboratory">Hubei Key Laboratory of Applied Mathematics</orgName>
								<orgName type="institution">Hubei University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">ulty of Science and Technology</orgName>
								<orgName type="institution">University of Macau</orgName>
								<address>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">an Honorary Profes-sor with Chongqing University</orgName>
								<address>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong, respec</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B6263F2185706CBE21B682261A3665A4</idno>
					<idno type="DOI">10.1109/TNNLS.2018.2874432</idno>
					<note type="submission">received October 26, 2017; revised March 26, 2018 and September 30, 2018; accepted October 3, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Classification</term>
					<term>hyperspectral image (HSI)</term>
					<term>inhomogeneous pixels</term>
					<term>joint sparse representation (JSR)</term>
					<term>maximum likelihood estimation (MLE)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A joint sparse representation (JSR) method has shown superior performance for the classification of hyperspectral images (HSIs). However, it is prone to be affected by outliers in the HSI spatial neighborhood. In order to improve the robustness of JSR, we propose a maximum likelihood estimation (MLE)-based JSR (MLEJSR) model, which replaces the traditional quadratic loss function with an MLE-like estimator for measuring the joint approximation error. The MLE-like estimator is actually a function of coding residuals. Given some priors on the coding residuals, the MLEJSR model can be easily converted to an iteratively reweighted JSR problem. Choosing a reasonable weight function, the effect of inhomogeneous neighboring pixels or outliers can be dramatically reduced. We provide a theoretical analysis of MLEJSR from the viewpoint of recovery error and evaluate its empirical performance on three public hyperspectral data sets. Both the theoretical and experimental results demonstrate the effectiveness of our proposed MLEJSR method, especially in the case of large noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>form several subspaces, and each testing sample can be linearly and sparsely represented by these training samples and finally is fallen into the subspace that has the minimum approximation error <ref type="bibr" target="#b18">[18]</ref>. The earliest SRC methods for the HSI classification just used the spectral information. Chen et al. <ref type="bibr" target="#b19">[19]</ref> introduced SRC for the HSI classification. Haq et al. <ref type="bibr" target="#b20">[20]</ref> proposed a fast SRC method, where the 1 -minimization problem was solved by a fast homotopy algorithm. In order to embed class-discriminative information into the dictionary of SRC, Wang et al. <ref type="bibr" target="#b21">[21]</ref> proposed a learning vector quantization-based SRC method, which designed a hinge loss function-based optimization problem for dictionary learning. Exploiting the correlation between testing and training samples, Cui and Prasad <ref type="bibr" target="#b22">[22]</ref> designed a class-dependent SRC model for the classification of HSIs.</p><p>Exploiting the rich spatial information of HSI, SRC was dramatically improved and different kinds of spatialspectral-based SRC methods were proposed <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b25">[25]</ref>- <ref type="bibr" target="#b39">[39]</ref>. Song et al. <ref type="bibr" target="#b25">[25]</ref> integrated both the spatial and spectral information to form extended multiattribute profiles (EMAPs) and performed SRC on EMAPs. EMAPs offered structural information of HSI, so the SRC on EMAPs largely improved the original SRC. Sun et al. <ref type="bibr" target="#b26">[26]</ref> given a low-rank group prior on the representation coefficients and proposed a new SRC method. Utilizing the correlations among spatial neighboring pixels, a joint sparse representation (JSR) model was proposed <ref type="bibr" target="#b19">[19]</ref>. JSR generalized the SRC by combining the sparse representation of each neighboring pixel together based on the joint sparsity assumption that all neighboring pixels have a common sparsity pattern. Because spatial neighborhood information was used to enhance the sparse representation of each testing pixel, the JSR model has shown superior performance and drawn increasing attention.</p><p>The underlying assumption of the JSR model is that neighboring pixels are similar and equally contribute to the sparse representation of the testing pixel. However, this assumption is not always true especially when a testing pixel is located on the boundary of an object. In this case, the spatial neighborhood of the testing pixel usually contains different inhomogeneous pixels, such as background or pixels from other classes. In order to reduce the effect of inhomogeneous neighboring pixels, Zou et al. <ref type="bibr" target="#b27">[27]</ref> replaced the original fixed squared neighborhoods in the JSR with adaptive local regions, which were generated by a presegmentation map on the HSI.</p><p>The pixels in a local region are usually similar. Similarly, Fu et al. <ref type="bibr" target="#b28">[28]</ref> designed shape-adaptive local smooth regions for the JSR classification. Fang et al. <ref type="bibr" target="#b29">[29]</ref> employed an oversegmentation method to divide the image into many superpixels and proposed a superpixel-based discriminative JSR model. Rather than designing adaptive local regions or superpixels, Zhang et al. <ref type="bibr" target="#b30">[30]</ref> predefined a weight function to reduce the effect of inhomogeneous neighboring pixels and proposed a nonlocal weighted JSR (WJSR) classification method. By fusing multiple features, a superpixel-level multitask JSR method <ref type="bibr" target="#b31">[31]</ref> and a multiscale adaptive JSR method <ref type="bibr" target="#b32">[32]</ref> were proposed. Considering that the dictionary in the original JSR model only used the spectral information of training samples, Soltani-Farani et al. <ref type="bibr" target="#b33">[33]</ref> incorporated spatial-contextual information into the dictionary learning and proposed a spatial-aware dictionary-based JSR method. Exploiting the smoothness and similarity of sparse representation coefficients, Tang et al. <ref type="bibr" target="#b34">[34]</ref> proposed a manifold-based JSR method that imposed a smoothness constraint on the sparse representation coefficients, and a regularized JSR method <ref type="bibr" target="#b35">[35]</ref> that forced the similarity of sparse coefficients of similar neighboring pixels.</p><p>It should be noted that all the aforementioned JSR algorithms are based on the least-squares (LS) fitting criterion. Naturally, there is a key issue: whether the LS-based objective function is effective enough to reflect the fitting precision, especially when there exist inhomogeneous pixels in the spatial neighborhood. Because the LS model aims to minimize the sum of quadratic loss of samples, its fitting result would bias to the samples with large erroneous (outliers or noisy samples). In this sense, the LS criterion is usually inappropriate for handling the data with noise or outliers <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>. In order to alleviate the impact of noise or outliers, many methods have been proposed to improve the LS loss function in the SRC model. <ref type="bibr">Wright and Ma [42]</ref> replace the 2 -norm loss function with an 1 -norm one, which is less sensitive to outliers. Except for the 1 -norm loss, there are other robust estimators, such as the Huber loss and M-estimators. Based on the maximum likelihood estimation (MLE) principle, if the fitting residual follows Gaussian or Laplacian distribution, the corresponding loss function has just the 2 -norm or 1 -norm form <ref type="bibr" target="#b43">[43]</ref>. However, for real HSI data, there are usually different types of noise in the spectral bands and inhomogeneous pixels in the spatial neighborhood <ref type="bibr" target="#b44">[44]</ref>. The complexity of noise distribution and the existence of inhomogeneous neighboring pixels may make the conventional 2 -norm or 1 -norm-based fidelity term invalid. Rather than defining the loss function as a definite norm, Yang et al. <ref type="bibr" target="#b43">[43]</ref> designed the objective function as a function of the fitting residuals based on the MLE principle and generated a robust sparse coding (RSC) model.</p><p>Based on the aforementioned techniques, the robustness of SRC can be improved to a certain extent. However, the robustness of the JSR model is almost untouched. To robust the JSR, an MLE-based JSR (MLEJSR) model is proposed for the HSI classification. We first change the objective function of JSR from the matrix norm form to the residual vector norm form and then perform an MLE-like estimator on the residual vector. Given some priors on the coding residuals, the MLEJSR model is easily converted to an iteratively reweighted JSR problem, where the weights associated with the coding residuals can be used to reduce the effect of inhomogeneous neighboring pixels.</p><p>The rest of this paper is organized as follows. Section II describes the JSR and our proposed MLEJSR models. The experimental results and analysis are provided in Section III. Finally, Section IV gives the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MAXIMUM LIKELIHOOD ESTIMATION-BASED JSR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Joint Sparse Representation Classification</head><p>Assume that all training samples form a B × N dictionary X = [x 1 , x 2 , . . . , x N ], and all neighboring pixels of a testing pixel z form a B × T neighborhood matrix Z = [z 1 , z 2 , . . . , z T ] (z 1 z). Based on the joint sparsity prior, Z has a representation as follows:</p><formula xml:id="formula_0">Z = [z 1 , z 2 , . . . , z T ] = [Xα 1 , Xα 2 , . . . , Xα T ] = XS (1)</formula><p>where S = [α 1 , α 2 , . . . , α T ] is an N × T sparsity matrix, which is resolved by</p><formula xml:id="formula_1">Ŝ = arg min S XS -Z 2 F , s.t. S row,0 ≤ K (2)</formula><p>where S row,0 counts the numbers of nonzero rows of S, and K is a parameter related to the sparsity level. The optimization problem (2) can be solved by the simultaneous orthogonal matching pursuit (SOMP) algorithm. Finally, each test pixel z is classified as the class with the minimal approximation error</p><formula xml:id="formula_2">Class(z) = arg min c=1,...,C r c (Z)<label>(3)</label></formula><p>where r c (Z) = Z -X c Ŝc F is the approximation error of the cth class, and X c and Ŝc represent the training pixels and sparse coefficients corresponding to the cth class, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Maximum Likelihood Estimation-Based Joint Sparse Representation</head><p>In the JSR model (2), the objective function is an LS metric</p><formula xml:id="formula_3">L(S) = Z -XS 2 F . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>From the viewpoint of Bayesian, the LS method actually requires the fitted residual E = Z -XS that follows a normal distribution. However, in practice, this may be not true, especially when the data contain outliers (abnormal data or noise). In the presence of outliers, the LS model (4) will fail <ref type="bibr" target="#b41">[41]</ref>. For a testing pixel of HSI, there may exist inhomogeneous pixels in its spatial neighborhood, such as background or pixels from other classes, especially when the testing pixel is located around the boundary of an object. This can be seen from Fig. <ref type="figure" target="#fig_0">1</ref>, where the spatial neighborhood of a boundary pixel usually contains different types of inhomogeneous pixels. These inhomogeneous pixels will seriously affect the LS-based JSR model and can be regarded as outliers. To reduce their effects and obtain a robust model, an MLEJSR model is proposed.</p><p>Inspired by the robust regression theory <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b45">[45]</ref>, the LS objective function in (4) can be designed as a function of the fitting residuals. Based on the MLE principle, it can obtain an optimal parameter estimator that best explains the observed data.</p><p>We first transfer the matrix norm form (4) to the vector norm form by processing each neighboring pixel z t separately, as follows:</p><formula xml:id="formula_5">L(S) = T t =1 z t -(XS) t 2 2 = T t =1 z t -Xα t 2 2 . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Denote e = [e 1 , . . . , e T ] T as the error residual with e t = z t -Xα t 2 , then (5) can be rewritten as</p><formula xml:id="formula_7">L(e) = e 2 2 = T t =1 e 2 t .<label>(6)</label></formula><p>Assume that e 1 , e 2 , . . . , e T are independently and identically distributed (i.i.d.) random variables and drawn from the same distribution function f θ (e t ), where θ denotes the distribution parameter. The likelihood is</p><formula xml:id="formula_8">L θ (e 1 , e 2 , . . . , e T ) = T t =1 f θ (e t ).<label>(7)</label></formula><p>The maximum likelihood estimation is equivalent to minimizing the following objective function:</p><formula xml:id="formula_9">-lnL θ = T t =1 ρ θ (e t ) (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where ρ θ (e t ) = -ln f θ (e t ).</p><p>Replacing the LS objective function with an MLE-like estimator and considering the sparsity constraint on S, an MLE-based JSR (MLEJSR) model is proposed as follows:</p><formula xml:id="formula_11">min S L(S, θ ) = T t =1 ρ θ (e t ) = T t =1 ρ θ (z t -(XS) t 2 ) s.t. S row,0 ≤ K . (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>In ( <ref type="formula" target="#formula_11">9</ref>), the loss function ρ θ or the error distribution f θ is unknown. In order to solve this problem, some basic prior assumptions are provided as follows: f θ (e i ) is symmetric, and</p><formula xml:id="formula_13">f θ (e i ) &lt; f θ (e j ) if |e i | &gt; |e j |.</formula><p>Based on these assumptions, we can deduce some properties on ρ θ (e i ):</p><formula xml:id="formula_14">ρ θ (0) is the global minimal of ρ θ ; ρ θ (-e i ) = ρ θ (e i ); and ρ θ (e i ) &gt; ρ θ (e j ) if |e i | &gt; |e j |. Without loss of generality, we set ρ θ (0) = 0. Denote F θ (e) = T t =1 ρ θ (e t ).</formula><p>Based on the first-order Taylor expansion around e 0 , F θ (e) can be approximated as</p><formula xml:id="formula_15">Fθ (e) = F θ (e 0 ) + (e -e 0 ) T F θ (e 0 ) + 1 2 (e -e 0 ) T U(e -e 0 ) (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where F θ (e 0 ) is the first-order derivation of F θ (e) at e 0 . Because the error residuals e i are i.i.d., the mixed derivatives (∂ 2 F θ /∂e i ∂e j ) = 0 for e i = e j , and the Hessian matrix U is a diagonal matrix.</p><p>Based on the assumption ρ θ (0) = 0, F θ (e) attains its minimum value 0 at e = 0. As an approximation to F θ (e), Fθ (e) should also reach its minimum at e = 0. By a simple derivation operation on <ref type="bibr" target="#b9">(10)</ref>, it gets</p><formula xml:id="formula_17">F θ (e) = F θ (e 0 ) + U(e -e 0 ). Let F θ (0) = 0, it can easily derive U i,i = u θ (e 0,i ) = ρ θ (e 0,i ) e 0,i<label>(11)</label></formula><p>where e 0,t is the tth element of e 0 . Because ρ θ (e i ) and e i have the same sign, U i,i is nonnegative. Merging similar terms, Fθ (e) in ( <ref type="formula" target="#formula_15">10</ref>) can be rewritten as</p><formula xml:id="formula_18">Fθ (e) = (1/2)U 1/2 e 2</formula><p>2 + b, where b is related to e 0 . Based on the above analysis, the MLEJSR model in ( <ref type="formula" target="#formula_11">9</ref>) is now approximated by min</p><formula xml:id="formula_19">S,U L(S, U) = U 1/2 e 2 2 = T t =1 U t,t e 2 t = T t =1 U t,t z t -Xα t 2 2 = (Z -XS)U 1/2 2 F s.t. S row,0 ≤ K (12)</formula><p>where the diagonal element U t,t [i.e., u θ (e t )] can be regarded as MLE weight for the neighboring pixel z t . The determination of the distribution ρ θ in the model ( <ref type="formula" target="#formula_11">9</ref>) is now transformed into the determination of the MLE weight U, which can be computed based on <ref type="bibr" target="#b10">(11)</ref>. The loss function in the current model has a general weighted LS form and can be generalized to the commonly used losses, such as an M-estimator, 1 -norm-based loss, Huber loss, Welsch loss, and correntropy-based loss <ref type="bibr" target="#b46">[46]</ref>, by designating a specific weight.</p><p>Here, we set the MLE weight as the logistic function <ref type="bibr" target="#b47">[47]</ref> </p><formula xml:id="formula_20">u θ (e i ) = exp μδ -μe 2 i 1 + exp μδ -μe 2 i (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>where μ and δ are the positive scalars. In particular, when δ → +∞, then u θ → 1 and the MLEJSR model ( <ref type="formula">12</ref>) is reduced to the original JSR model in <ref type="bibr" target="#b1">(2)</ref>. If μ = 2 and δ → 0, then u θ (e i ) ≈ (1/2(1 + e 2 i )) for small errors. This weight corresponds to the weight in the L 2,1 -norm-based loss. If μ = 1/σ 2 and δ → 0, then u θ (e i ) ≈ (1/1 + exp(e 2 i /σ 2 )) and the corresponding loss function is very close to the correntropybased loss.  We set δ as the pth percentile of error vector e 2 and show the MLE weights in Fig. <ref type="figure" target="#fig_1">2</ref>. It is clear that the MLE weight function is bounded in the interval [0, 1], and parameter μ controls the degree of decay, and δ (or p) controls the location of demarcation point (e 2 i = δ) <ref type="bibr" target="#b43">[43]</ref>. Adjusting these two parameters, the noise or outliers with large errors will automatically be assigned small weight values.</p><p>With <ref type="bibr" target="#b10">(11)</ref>, <ref type="bibr" target="#b13">(13)</ref>, and ρ θ (0) = 0, by a simple integral operation, we can obtain the MLE objective function</p><formula xml:id="formula_22">ρ θ (e i ) = e i 0 ρ θ (e i )de i = e i 0 e i u θ (e i )de i = e i 0 e i exp(μδ -μe 2 i ) 1 + exp μδ -μe 2 i de i = - 1 2μ ln 1 + exp μδ -μe 2 i e i 0 = - 1 2μ ln 1 + exp μδ -μe 2 i 1 + exp(μδ) . (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>We show the MLE objective function in Fig. <ref type="figure" target="#fig_2">3</ref> and compare it with the LS loss y = e 2 i . Compared with the global LS loss function, MLE objective function is a local metric, which is controlled by the parameters μ and p. If the LS loss function is used, the fitting residual will quadratically grow and become uncontrollable for outliers, which usually generate large reconstruction residuals. However, in the case of MLE loss function, the objective function values of outliers can be truncated by choosing appropriate parameters. In detail, p controls the position of the truncation point and μ determines the degree of truncation. By choosing a large μ, the MLE weights of outliers almost can be truncated to a constant, so these outliers have less influence on the model. Using the MLE loss function to replace the LS loss may be more effective in real situations, especially when outliers exist.</p><p>The optimization of the MLEJSR model ( <ref type="formula">12</ref>) is relatively easy. In the following, we will transform the optimization problem <ref type="bibr" target="#b12">(12)</ref> into an iteratively reweighted JSR problem.</p><p>Initializing the weight matrix U as the identity matrix: U = I, <ref type="bibr" target="#b12">(12)</ref> reverses back to the original JSR model. Therefore, we can employ the SOMP algorithm to resolve the sparsity matrix S. Then, error residual e i in <ref type="bibr" target="#b6">(7)</ref> and hence the weight u in ( <ref type="formula" target="#formula_20">13</ref>) can be obtained.</p><p>When the weight matrix U is obtained, denoting Z = ZU</p><formula xml:id="formula_24">1 2</formula><p>and S = SU (1/2) , then the model ( <ref type="formula">12</ref>) is now changed to</p><formula xml:id="formula_25">min S Z -X S 2 F , s.t. S row,0 ≤ K . (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>The only difference to <ref type="bibr" target="#b12">(12)</ref> is that S row,0 is replaced by S row,0 because the position of sparse rows does not change by multiplying a positive-definite matrix U to S. The model ( <ref type="formula" target="#formula_25">15</ref>) is actually a WJSR model, which only needs to weight the neighborhood matrix Z. By alternatively updating u and S, it can get the desirable solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Theoretical Analysis of MLEJSR</head><p>After a few iterations, MLEJSR algorithm will converge, as shown in Proposition 1.</p><p>Proposition 1: Denote S (m) and U (m) as the solutions at the mth iteration, then the sequence {L(S (m) , U (m) )} converges.</p><p>Proof 1: In two successive alternative iteration processes, we can get L(S (m+1) , U (m+1) )≤L(S (m) , U (m+1) )≤L(S (m) , U (m) ) <ref type="bibr" target="#b16">(16)</ref> where the second inequality is true because U (m+1) is the solution of minimizing L(S (m) , U), and the first one due to S (m+1) is the minimizer of L(S, U (m+1) ). Thus, the sequence {L(S (m) , U (m) ), m = 1, 2, . . .} is nonincreasing. It is clear that L(S, U) is lower bounded. Therefore, the sequence L(S (m) , U (m) ) is convergence. Now, we analyze the recovery error of JSR and MLEJSR. Assume that the real observation model is</p><formula xml:id="formula_27">Z = XS 0 + E (<label>17</label></formula><formula xml:id="formula_28">)</formula><p>where E is the noise. Denote the support set (nonzero rows) of S 0 as 0 = {i : S 0 (i, :) = 0}, and c 0 = {1, 2, . . . , N}/ 0 . Let X 0 be the submatrix of X, which is composed of the columns of X indexed by 0 . Then, the model ( <ref type="formula" target="#formula_27">17</ref>) can be rewritten as</p><formula xml:id="formula_29">Z = X 0 S 0 | 0 + E. (<label>18</label></formula><formula xml:id="formula_30">)</formula><p>Assume that the SOMP algorithm can find the solution of JSR problem (identify the correct support set) in the K th step, then</p><formula xml:id="formula_31">S SOMP K = arg min supp(S)⊂ 0 Z -XS 2 F (19) and K = 0 . It is easy to see S SOMP K | c 0 = 0 and S SOMP K | 0 = X T 0 X 0 -1 X T 0 Z = S 0 | 0 + X † 0 E (20)</formula><p>where X † 0 denotes the pseudoinverse of X 0 , and the second equality is derived based on <ref type="bibr" target="#b17">(17)</ref>. The recovery error of JSR is</p><formula xml:id="formula_32">R SOMP K = S SOMP K -S 0 2 F = X † 0 E 2 F . (<label>21</label></formula><formula xml:id="formula_33">)</formula><p>From ( <ref type="formula">12</ref>) or <ref type="bibr" target="#b15">(15)</ref>, it can be seen that the proposed MLEJSR is an iteratively reweighted JSR algorithm. In the outer loop, i.e., fixing S, it updates the weight matrix U such that the effect of inhomogeneous neighboring pixels is weakened. In the inner loop with a fixed U, it solves the support set of the sparse matrix S. In each inner loop, MLEJSR is reduced to a WJSR problem and can be solved by a weighted SOMP algorithm</p><formula xml:id="formula_34">S WSOMP K = arg min supp(S)⊂ 0 (Z -XS)U 1/2 2 F . (<label>22</label></formula><formula xml:id="formula_35">)</formula><p>The model ( <ref type="formula" target="#formula_34">22</ref>) is equivalent to</p><formula xml:id="formula_36">S WSOMP K = arg min supp( S)⊂ 0 ZU 1/2 -X S 2 F (<label>23</label></formula><formula xml:id="formula_37">)</formula><p>where</p><formula xml:id="formula_38">S WSOMP K = S WSOMP K U 1/2 .</formula><p>The recovery error is</p><formula xml:id="formula_39">R WSOMP K = S WSOMP K -S 0 2 F = X † 0 EU 1/2 2 F . (<label>24</label></formula><p>) Assume that there are M outer iterations, and the total recovery error of MLEJSR is</p><formula xml:id="formula_40">R WSOMP = X † 0 E(U (M) ) 1/2 2 F (<label>25</label></formula><formula xml:id="formula_41">)</formula><p>where</p><formula xml:id="formula_42">U (M) = U (M-1) U (M-2) • • • U (0) .</formula><p>From <ref type="bibr" target="#b21">(21)</ref>, the recovery error of SOMP or JSR is determined by the noise E. Considering a special case that only the neighboring pixel z 1 has a large impulsive noise n 1 = z 1 -(XS) 1 while all other noise n i (i = 1) has small absolute value e i = n i 2 &lt; κ, then</p><formula xml:id="formula_43">R SOMP K = X † 0 E 2 F = K k=1 T t =1 a T k n t 2 = K k=1 (a T k n 1 ) 2 + K k=1 T t =2 a T k n t 2 ≥ K c 1 n 1 2 2 + K k=1 T t =2 a T k n t 2 (<label>26</label></formula><formula xml:id="formula_44">)</formula><p>where c 1 = min k=1,...,K a k 2 cos 2 θ k , a T k is the kth row of X † 0 , and n t is the tth column of E. It is clear that R SOMP K is controlled by the large noise n 1 . If e 1 = n 1 2 → +∞, then R SOMP K → +∞. However, for MLEJSR, the recovery error satisfies</p><formula xml:id="formula_45">R WSOMP = X † 0 EU 1/2 2 F ≤ X † 0 2 F EU 1/2 2 F = c 0 T t =1 u t n t 2 2 = c 0 T t =1 u t e 2 t (<label>27</label></formula><formula xml:id="formula_46">)</formula><p>where c 0 = X † 0 2 F . It is clear that the recovery error of MLEJSR can be adjusted according to the weight u t . If the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MLEJSR-Based Classifier</head><p>Input:</p><formula xml:id="formula_47">Dictionary X = [x 1 , x 2 , . . . , x N ], parameter K . For a test pixel z, construct Z = [z 1 , z 2 , . . . , z T ] 1 Initialize Z (0) = Z, U (0) = I, set m = 1.</formula><p>Run the following steps until convergence. (a) compute the weighted neighborhood matrix:</p><formula xml:id="formula_48">Z (m) = Z (m-1) U (m-1)</formula><p>(b) solve the sparse coefficient matrix:</p><formula xml:id="formula_49">S (m) = arg min S Z (m) -XS 2 F , s.t. S row,0 ≤ K (c) calculate the error: e (m) t = (Z (m) -XS (m) ) t 2 = z (m) t -Xα (m) t 2</formula><p>(d) estimate MLE weights:</p><formula xml:id="formula_50">u (m) θ (e t ) = exp μ (m) δ (m) -μ (m) (e (m) t ) 2 1 + exp μ (m) δ (m) -μ (m) (e (m) t ) 2 (e) update U (m) = diag u (m) θ (e 1 ), . . . , u (m) θ (e T ) (f) m ← m + 1 2</formula><p>Compute the approximation error of each class:</p><formula xml:id="formula_51">r c (z) = Z (m) -X c (S (m) ) c 2 F</formula><p>3 Classify the testing pixel z:</p><formula xml:id="formula_52">Class(z) = arg min c=1,...,C r c (z)</formula><p>end For Output: The labels of all testing pixels measurements with large noise are assigned small weights, the recovery error of MLEJSR can be dramatically reduced.</p><p>Recall that the weight corresponding to the neighboring pixel z t is</p><formula xml:id="formula_53">u t = exp μδ -μe 2 t 1 + exp μδ -μe 2 t = 1 1 + exp(-μδ)exp μe 2 t . (28)</formula><p>For large noise n 1 , the corresponding weight u 1 is very small, and u 1 e 2 1 tends to 0 when e 1 → +∞, i.e., u 1 e 2 1 &lt; ε 1 . Meanwhile, for the pixels with small noise, their weights are relatively large and very close to 1 if e i → 0 (i = 1). Hence</p><formula xml:id="formula_54">R WSOMP ≤ c 0 u 1 e 2 1 + T t =2 u t e 2 t ≤ c 0 (ε 1 + (T -1)κ 2 ). (<label>29</label></formula><formula xml:id="formula_55">)</formula><p>From the above analysis, we can see that the recovery error of MLEJSR is much smaller than that of JSR, and the effect of large noise can be controlled according to an iterative reweighting strategy in the MLEJSR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MLEJSR-Based Classifier</head><p>The detail process of the MLEJSR-based classifier is shown in Algorithm 1.</p><p>The proposed MLEJSR can be regarded as an iteratively reweighted JSR, where the MLE weight computed from the fitting residuals can be used to weaken the effect of outliers. In detail, inhomogeneous neighboring pixel z t with a large reconstruction error e t will be given a low weight value u θ (e t ).</p><p>In the MLEJSR, the parameters δ and μ in the weight function need to be estimated in each iteration. From Fig. <ref type="figure" target="#fig_2">3</ref>, δ determines the position of the truncation point. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, if e 2 i &gt; δ, the weight u θ (e i ) &lt; 0.5. Therefore, we can choose an appropriate δ to control the effect of outliers. The more outliers, the δ, and vice versa. If there is no outlier in the data, we can set δ = ∞ such that all samples have equal weight value 1.</p><p>In general, the number of homogeneous or inhomogeneous neighboring pixels for each testing pixel is different. Therefore, in the JSR classification, it would be better to design δ i for each testing pixel separately. For this purpose, we first determine the number of homogeneous pixels in the spatial neighborhood of each testing pixel. A threshold value γ to indicate the local homogeneous patch can be computed as <ref type="bibr" target="#b48">[48]</ref> γ = median{d i j , 1 ≤ i &lt; j ≤ C} <ref type="bibr" target="#b30">(30)</ref> where d i j = x i -x j 2 , and xi is the mean vector of training samples in the i th class. Then, the number of homogeneous pixels n i for each testing pixel z i is counted as the number of neighboring pixels z ik , whose distance to the central testing pixel z i is smaller than γ , i.e., z ikz i 2 &lt; γ . The potential frequency of homogeneous pixels in the spatial neighborhood of the testing pixel z i is: f i = n i /T , and the probability can be set as:</p><formula xml:id="formula_56">p i = 1/(1 + be -f i ) with b = 0.5.</formula><p>In the experiments, we set δ i for the testing pixel z i as the (100 p i )th percentile of error vector e 2 . Parameter μ determines the decay rate of an MLE weight value, which is set as μ = c/δ i with c = 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>Three benchmark HSI scenes are used.   2) Salinas: This scene has the size of 512 × 217 pixels and 204 spectral bands. The data contain 16 ground-truth classes. The false color composite image and the ground-truth map are shown in Fig. <ref type="figure" target="#fig_5">5</ref>.</p><p>3) Kennedy Space Center: The image scene contains 512 × 614 pixels and 176 spectral bands. The data contain 13 ground-truth classes. The false color composite image and the ground-truth map are shown in Fig. <ref type="figure" target="#fig_6">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Setting</head><p>We compare our proposed MLEJSR with the following classification methods <ref type="bibr" target="#b37">[37]</ref>: SVM, SVM with composite kernel (SVM-CK) <ref type="bibr" target="#b13">[13]</ref>, SRC <ref type="bibr" target="#b18">[18]</ref>, RSC <ref type="bibr" target="#b43">[43]</ref>, OMP <ref type="bibr" target="#b49">[49]</ref>, JSR <ref type="bibr" target="#b19">[19]</ref>, nonlocal WJSR <ref type="bibr" target="#b30">[30]</ref>, and adaptive SOMP (ASOMP) <ref type="bibr" target="#b27">[27]</ref>. The class accuracy (CA), overall accuracy (OA), and κ coefficient on the test set are used to evaluate different algorithms.</p><p>JSR, WJSR, and MLEJSR have two common parameters: neighborhood size T and sparsity level K . For fair comparison, we set them as the same as follows: K = 30 and T = 81 for Indian Pines and Salinas and K = 20 and T = 81 for KSC. MLEJSR employs an alternative iteration strategy to solve the MLE weight and sparse coefficient matrix. The number of iterations is set as 5 for the three data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification Results</head><p>We randomly draw 10%, 5%, and 5% labeled samples from each class to form the training set for three data sets, respectively <ref type="bibr" target="#b19">[19]</ref>. All of the rest labeled samples make up the testing set. The experiment is randomly run 10 times, and the averaged results of 10 runs are recorded. The class for the lack of representative atoms in the dictionary. 2) Compared with the original JSR and WJSR methods, MLEJSR obtains higher classification accuracies on the subclasses, such as subclasses of "Corn" and "Soybean" of Indian Pines, subclasses of "Fallow," "Lettuce," and "Vinyard" of Salinas, and subclasses of "Cabbage" and "Marsh" of KSC. 3) From the classification maps, MLEJSR improves the existing JSR methods not only on the large homogeneous regions but also on the boundary regions. By weighting neighboring pixels, MLEJSR largely eliminates the "salt &amp; pepper" noise in the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Investigation on the Effect of MLE Weight</head><p>Here, we analyze the reason why the proposed MLEJSR can improve the original JSR. We investigate the whole  classification process of JSR and MLEJSR. Given a testing pixel A, we can extract all its spatial neighbors in a 9 × 9 window centered at A. These pixels are from Classes 0, 2, 10, and 11, as shown in Fig. <ref type="figure" target="#fig_9">9(a)</ref>. In the JSR model, the sparse coefficient matrix S and representation residual e 2 t of neighboring pixels can be easily obtained by the SOMP algorithm, which are shown in Fig. <ref type="figure" target="#fig_9">9(b)</ref> and<ref type="figure">(c</ref>). Most sparse representation coefficients come from Classes 2, 10, and 11, as shown in the red ellipses in Fig. <ref type="figure" target="#fig_9">9(b)</ref>. This coincides with the fact that neighboring pixels mainly come from these classes and hence have different sparsity patterns. The reconstruction residual of each class is shown in Fig. <ref type="figure" target="#fig_9">9(d)</ref>. It is clear that the central testing pixel A belonging to Class 10 is now wrongly classified to Class 2 by the JSR. The basic assumption in the JSR is that spatial neighboring pixels share a common sparsity pattern. Here, this assumption is false, so JSR is fail. It should also be noted that the spectral characteristics of Class 2 and Class 10 are very similar, so it is very difficult to discriminate these two classes.</p><p>In Fig. <ref type="figure" target="#fig_9">9</ref>(c), six neighboring pixels corresponding to index 7 from Class 2, indexes 24, 32, and 37 from Class 10, and indexes 66 and 71 from Class 11 have very small representation residuals e 2 t . Based on these residuals, MLE weights can be computed, as shown in Fig. <ref type="figure" target="#fig_9">9</ref>(g). By weighting neighboring pixels using the computed MLE weights, the weighted neighboring pixels are shown in Fig. <ref type="figure" target="#fig_9">9</ref>(e). Because only six pixels have large weights, most of the neighboring pixels are restrained. This also leads to the changes of sparse representation coefficients in Fig. <ref type="figure" target="#fig_9">9</ref>(f), where several large coefficients are kept while other small coefficients are almost erased. The remaining coefficients just correspond to three sparsity patterns of neighboring pixels. Finally, when using the representor of each class to reconstruct the neighborhood pixel matrix, Class 10 has the minimal reconstruction residual, as shown in Fig. <ref type="figure" target="#fig_9">9</ref>(h). Based on the reconstruction residuals, MLEJSR correctly classifies the testing pixel A.</p><p>From the whole process, we can see that MLEJSR can effectively restrain interrupt pixels (background, inhomogeneous pixels, noise, or outliers), which helps to eliminate false representation coefficients in the JSR model. As a result, the sparse representation and classification in the MLEJSR are  relatively more accurate than JSR, especially when the spatial neighborhood has multiple sparsity patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Investigation on Noise in Spatial Neighborhood</head><p>Here, we show the classification OAs of JSR algorithms when the neighboring pixels are noised. We consider four types of noise as follows: Gaussian distribution N (0, s), uniform distribution U(0, s), exponential distribution E(s), and chi-square distribution s • χ 2 (3). We can regard s as the amplitude of noise in each distribution. For the small noise cases with s = 0.001 or the number of noisy neighboring pixel T e = 1, we set δ as the 95-th percentile of error vector e 2 and c = 2, for simplicity. For other heavy noise cases, we set δ as the 60-th percentile of error vector and c = 20. All the data are normalized to have unit 2 -norm before the classification.</p><p>We add different types of noise to the spatial neighborhood of Indian Pines data set, respectively. We first fix s = 0.01 and change the numbers of noisy neighboring pixels T e in the set {1, 5, 10, 20, 30, 40, 50, 60}. The classification OAs of JSR, WJSR, and MLEJSR for different noise distributions are shown in Fig. <ref type="figure" target="#fig_10">10</ref>. The results on different noise   distributions are similar. JSR almost fails even if only one pixel is noised. WJSR improves JSR at a certain extent by weighting neighboring pixels. However, it still suffers from the impact of noise and declines sharply as the increase of noisy neighboring pixels. Our proposed MLEJSR shows little changes and keeps the OA over 90% when T e is smaller than 40. It should be noted that the total number of neighboring pixels is 81 for a 9×9 spatial window. Even half of pixels in the neighborhood are noised, and the results of MLEJSR are also satisfactory. In the extreme case with 60 noisy neighboring pixels, MLE-JSR still provides OAs over 80%. However, JSR and WJSR completely fall down and their results are even worse than a random guess. It demonstrates that MLEJSR is much effective in the case of both small noise and large noise.</p><p>In the second experiment, we fix T e = 20 and change the amplitude of noise s in the set {0.001, 0.01, 0.1, 1}. Table <ref type="table" target="#tab_3">IV</ref> shows the classification results of three JSR methods. The results of the original JSR is extremely bad in the case of heavy noise. When s gets larger, the OAs of WJSR decrease dramatically at first and then reach steady state when s is larger than 0.01. Notwithstanding, the classification performance is poor. In contrast, our proposed MLEJSR provides consistently good results in all the cases.</p><p>Taking uniform distribution as an example, we show classification OAs of different algorithms as a function of s and T e in Fig. <ref type="figure" target="#fig_11">11</ref>. It is clear that JSR and WJSR can work only when the noise is very small (s = 0.001) or not exist. Otherwise, their performance will decline rapidly as the increase of noise. Although MLEJSR also shows slightly performance degradation when the degree of noise is increased, its results are acceptable in the case of heavy noise. Therefore, the MLEJSR method is more robust than LS-based JSR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Investigation on Noisy Spectral Bands</head><p>The original Indian Pines data have 220 spectral bands. Due to water absorption, 20 bands were deleted and the rest 200 bands were used for the classification. Now, we show the classification OA of JSR, WJSR, and MLEJSR as the changes of spectral band number on Indian Pines, i.e., <ref type="bibr" target="#b40">40,</ref><ref type="bibr">80,</ref><ref type="bibr">120,</ref><ref type="bibr">160,</ref><ref type="bibr">200</ref> (choosing from 200 corrected bands in ascending order), and 220 (all bands). Fig. <ref type="figure" target="#fig_12">12</ref> shows the OAs of three JSR algorithms. When the number of bands is smaller than 80, JSR and WJSR show bad results. However, MLEJSR shows satisfactory results consistently.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Investigation on Training Samples</head><p>To investigate the consistency and stability of JSR methods, we show the OAs of JSR algorithms on Indian Pines when the size of the training set changes. Here, the ratio of training samples per class is set as 2%, 4%, 6%, 8%, and 10%, respectively. The classification overall accuracies of three JSR methods are shown in Fig. <ref type="figure" target="#fig_13">13</ref>, where MLEJSR generates much better results than JSR and WJSR, especially when the number of training samples is very limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Investigation on the Model Parameter</head><p>We investigate the effect of the decay parameter on the performance of the MLEJSR model. In our MLEJSR, the decay parameter μ is set as: μ = c/δ i with a preestimated δ i for each testing pixel. Therefore, μ is only determined by an empirical parameter c. We show the OAs of MLEJSR at different decay parameters c on Indian Pines in Fig. <ref type="figure" target="#fig_14">14</ref>. It can be seen that the proposed MLEJSR algorithm is not very sensitive to the decay parameter c and shows good results when c is larger than 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>An MLEJSR method has been proposed for the HSI classification in this paper. In MLEJSR, the loss function has been designed as an MLE-like estimator, which is a function of coding residuals. Given some prior assumptions on the coding residuals, the MLEJSR model can be easily converted to an iteratively reweighted JSR problem, where the weights can be used to weaken the impact of outliers. Experimental results have shown the effectiveness and robustness of the proposed MLEJSR algorithm, especially when there exist inhomogeneous pixels in the spatial neighborhood.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Some representative spatial neighborhoods in the Indian Pines are marked in different colors. The spatial neighborhood contains pixels from only one class (black), one class and background (purple), two classes (red), two classes and background (green), and three classes and background (white).</figDesc><graphic coords="3,90.59,58.85,167.66,132.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. MLE weight function with different parameters μ and p. The location of demarcation points satisfying e 2 i = δ is the intersections A, B, C, and D on the horizontal black line u = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison between MLE and LS objective functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) False color composite image and (b) ground-truth map of Indian Pines.</figDesc><graphic coords="6,149.75,58.49,80.30,108.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 1 )</head><label>1</label><figDesc>Indian Pines: The image scene has the size of 145 × 145 pixels and 200 spectral bands. It contains 16 classes and the number of samples in each class is unbalanced. The false color composite image and the ground-truth map are shown in Fig. 4. 1 http://www.ehu.eus/ccwintco/index.php?titlex=Hyperspectral_Remote_ Sensing_Scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) False color composite image and (b) ground-truth map of Salinas.</figDesc><graphic coords="6,409.91,216.05,80.54,108.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) False color composite image and (b) ground-truth map of KSC.</figDesc><graphic coords="6,318.83,216.41,80.66,107.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Classification maps on Indian Pines. (a) Ground truth. (b) SVM. (c) SVM-CK. (d) SRC. (e) RSC. (f) OMP. (g) JSR. (h) WJSR. (i) ASOMP. (j) MLEJSR.</figDesc><graphic coords="8,89.99,361.97,75.74,79.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Classification maps on Salinas. (a) Ground truth. (b) SVM. (c) SVM-CK. (d) SRC. (e) RSC. (f) OMP. (g) JSR. (h) WJSR. (i) ASOMP. (j) MLEJSR.</figDesc><graphic coords="9,75.83,336.89,100.70,78.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison between JSR and MLEJSR. (a) Central testing pixel A from Class 10 and its spatial neighbors in a 9 × 9 window from Class 0 (background, the second and sixth columns), Class 2 (the first column), Class 10 (the third to fifth columns), and Class 11 (the seventh to ninth columns). (b) Sparse coefficient matrix of JSR. (c) Representation residual e 2 t of JSR. (d) Reconstruction residual of each class of JSR. (e) Weighted neighborhood pixels. (f) Sparse coefficient matrix of MLEJSR. (g) MLE weight. (h) Reconstruction residual of each class of MLEJSR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. OA the number of noisy neighboring pixels (s = 0.01). The noise distributions are (a) Gaussian, (b) uniform, (c) exponential, and (d) chi-square.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. OA versus the number of noisy neighboring pixels under uniform distribution noise for different algorithms. (a) JSR. (b) WJSR. (c) MLEJSR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. OA versus the band number on Indian Pines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. OA versus the numbers of training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. OA versus the decay parameter c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>RESULTS ON THE TESTING SET OF INDIAN PINES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>RESULTS ON THE TESTING SET OF SALINAS statistics, i.e., CA, OA, and κ, for three data sets are shown in Tables I-III. Figs. 7 and 8 show the classification maps generated by different classification algorithms on Indian Pines and Salinas data sets, respectively.Based on these results, we can see the following.</figDesc><table><row><cell>1) MLEJSR achieves the best results and improves the OA</cell></row><row><cell>of JSR by about 2% on three data sets. Based on the</cell></row><row><cell>CAs, MLEJSR tends to generate higher OAs for large</cell></row><row><cell>classes, such as Classes 2-4 and 10-12 of Indian Pines,</cell></row><row><cell>Classes 8-10 and 15 of Salinas, and Classes 10-13 of</cell></row></table><note><p>KSC. If a class has plenty of samples, the approximation and corresponding representation residuals are more accurate. This helps to produce a reliable MLE weight estimation and hence MLEJSR classification result. For the classes with extremely limited training samples, such as Classes 7 and 9 of Indian Pines and Class 7 of KSC, MLEJSR shows poor performance may be because MLE representation residuals of these classes are inaccurate</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>RESULTS ON THE TESTING SET OF KSC</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV OA</head><label>IV</label><figDesc>VERSUS THE NOISE AMPLITUDE IN THE CASE OF T e = 20</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Prof. D. Landgrebe providing the Indian Pines data set, Dr. J. Anthony Gualtieri for providing Salinas data set, and Prof. M. Crawford for providing the KSC data set. They would also like to thank the associate editor and anonymous reviewers for their insightful comments and suggestions, which have greatly improved this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Natural Science Foundation of China under Grant 61871177 and Grant 11771130. where he became a Full Professor in 1994. His current research interests include approximation theory, wavelet analysis, learning theory, neural networks, signal processing, and pattern recognition. Dr. Li is a Managing Editor of the International Journal on Wavelets, Multiresolution, and Information Processing. Yuan Yan Tang (S'88-M'88-SM'96-F'04-LF'16) is currently a Chair Professor with the Fac-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On combining multiple features for hyperspectral remote sensing image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="879" to="893" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances in spectral-spatial classification of hyperspectral images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Tilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="652" to="675" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An SVM ensemble approach combining spectral, structural, and semantic features for the classification of highresolution remotely sensed imagery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="257" to="272" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly learning the hybrid CRF and MLR model for simultaneous denoising and classification of hyperspectral imagery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1319" to="1334" />
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Band selection using improved sparse subspace clustering for hyperspectral imagery classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2784" to="2797" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for remote sensing data: A technical tutorial on the state of the Art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Salient band selection for hyperspectral image classification via manifold ranking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1279" to="1289" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A sparse and lowrank near-isometric linear embedding method for feature extraction in hyperspectral imagery classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4032" to="4046" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonparametric coupled Bayesian dictionary and classifier learning for hyperspectral classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4038" to="4050" />
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recent advances in techniques for hyperspectral image processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<idno>pp. S110-S122</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral remote sensing images with support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Region-kernel-based support vector machines for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4810" to="4824" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Composite kernels for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez-Chova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muñoz-Marí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vila-Francés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Calpe-Maravilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="97" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extreme learning machine with composite kernels for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observat. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2351" to="2360" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ideal regularized composite kernel for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1563" to="1574" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification and feature extraction for remote sensing images from urban areas based on morphological transformations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pesaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1940" to="1949" />
			<date type="published" when="2003-09">Sep. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive Bayesian contextual classification based on Markov random fields</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2454" to="2463" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification using dictionary-based sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3973" to="3985" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast and robust sparse approach for hyperspectral data classification using a few labeled samples</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">S</forename><surname>Haq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2287" to="2302" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial-spectral classification of hyperspectral images using discriminative dictionary designed by learning vector quantization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4808" to="4822" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Class-dependent sparse representation classifier for robust hyperspectral image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2683" to="2695" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A hybrid sparsity and distance-based discrimination detector for hyperspectral images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1704" to="1717" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-paced joint sparse representation for the classification of hyperspectral images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2865102</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Remotely sensed image classification using sparse representations of morphological attribute profiles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5122" to="5136" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structured priors for sparse-representation-based hyperspectral image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1235" to="1239" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral urban data using adaptive simultaneous orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">85099</biblScope>
			<date type="published" when="2014-07">Jul. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification via shape-adaptive joint sparse representation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="556" to="567" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral images with a superpixel-based discriminative sparse model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4186" to="4201" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A nonlocal weighted joint sparse representation classification method for hyperspectral imagery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observat. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2056" to="2065" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint collaborative representation with multitask learning for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5923" to="5936" />
			<date type="published" when="2014-09">Sep. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral-spatial hyperspectral image classification via multiscale adaptive sparse representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7738" to="7749" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial-aware dictionary learning for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Soltani-Farani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Rabiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hosseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="527" to="541" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Manifold-based sparse representation for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7606" to="7618" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification based on regularized sparse representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2174" to="2182" />
			<date type="published" when="2014-06">Jun. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification with robust sparse representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="641" to="645" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nearest regularized joint sparse representation for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="424" to="428" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust joint sparse representation based on maximum correntropy criterion for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7152" to="7164" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification via multitask joint sparse representation and stepwise MRF optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2966" to="2977" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust parameter estimation in computer vision</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="537" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Correntropy: Properties and applications in non-Gaussian signal processing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Pokharel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5286" to="5298" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dense error correction via 1 -minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3540" to="3560" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust sparse coding for face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Total-variation-regularized low-rank matrix factorization for hyperspectral image restoration</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="178" to="188" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust regression: Asymptotics, conjectures and Monte Carlo</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="799" to="821" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximum correntropy criterion for robust face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1561" to="1576" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modified logistic regression: An approximation to SVM and its applications in large-scale text categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive nonlocal spatial-spectral kernel for hyperspectral imagery classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4086" to="4101" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">His current research interests include machine learning and hyperspectral image processing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He is currently an Associate Professor with the Faculty of Mathematics and Statistics</title>
		<meeting><address><addrLine>Wuhan, China; Beijing, China; Macau, China; Starkville, MS, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">Dec. 2007. 2005 and 2008. 2011. 2017 to 2018</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="4655" to="4666" />
		</imprint>
		<respStmt>
			<orgName>University of Macau ; Department of Electrical and Computer Engineering, Mississippi State University ; Hubei University</orgName>
		</respStmt>
	</monogr>
	<note>Signal recovery from random measurements via orthogonal matching pursuit</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
