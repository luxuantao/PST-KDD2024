<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Multimodal Learning for Affective Analysis and Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lei</forename><surname>Pang</surname></persName>
							<email>leipang3-c@my.cityu.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Shiai</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
							<email>cwngo@cs.cityu.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="laboratory">Multimedia Communications Research Laboratory (MCRLab)</orgName>
								<orgName type="institution">University of Ottawa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Multimodal Learning for Affective Analysis and Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9F871FB8B47C04D2BF577052BCB506C9</idno>
					<idno type="DOI">10.1109/TMM.2015.2482228</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2015.2482228, IEEE Transactions on Multimedia This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2015.2482228, IEEE Transactions on Multimedia This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2015.2482228, IEEE Transactions on Multimedia This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2015.2482228, IEEE Transactions on Multimedia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social media has been a convenient platform for opinion voicing through posting messages, ranging from tweeting a short text to uploading a media file and any combinations of them. Understanding the perceived emotions inherently underlying these user-generated-contents (UGC) could bring light to emerging applications such as advertising and media analytics. Existing research efforts on affective computation are mostly dedicated to single media, either text captions or visual contents. Few attempts are paid for combined analysis of multiple media, despite that emotion can be viewed as an expression of multimodal experience. In this paper, we explore the learning of highly non-linear relationships that exist among low-level features across different modalities for emotion prediction. Using Deep Bolzmann Machine (DBM), a joint density model over the space of multimodal inputs, including visual, auditory and textual modalities, is developed. The model is trained directly using UGC data without any labeling efforts. While the model learns a joint representation over multimodal inputs, training samples in absence of certain modalities can also be leveraged. More importantly, the joint representation enables emotionoriented cross-modal retrieval, for example, retrieval of videos using text query "crazy cat". The model does not restrict the types of input and output, and hence in principle, emotion prediction and retrieval on any combinations of media are feasible. Extensive experiments on web videos and images show that the learnt joint representation could be very compact and be complementary to hand-crafted features, leading to performance improvement in both emotion classification and cross-modal retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In contrast, emotion classification in the lower two videos with similar visual appearances is performed using the strong clues in the textual captions and auditory information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms</head><p>Emotion analysis, Multimodal learning, Deep Boltzmann Machine, Cross modal retrieval I. INTRODUCTION Social media is an opinion-rich knowledge source including plenty of timely user-generated-contents (UGC) with different media types. Automatically understanding the emotional status of users from their uploaded multimedia contents is in high demands for many applications <ref type="bibr" target="#b0">[1]</ref>. For example, when searching information about a resort, the retrieved images or videos can be ranked based on their emotions to provide implicit comments. In addition, when asking opinion-related questions about hot events, providing emotion tags for retrieved videos helps users more quickly understand the sentiment of public's view. This function can also be used by governments to better understand people's reactions towards their new policies.</p><p>The existing works on affective analysis of UGC data are mostly devoted to single media <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b9">[10]</ref>.</p><p>For example, linguistic <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref> and semantic <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> features are both adopted for text-based analysis.</p><p>However, inferring perceived emotion signals underlying short messages that are usually sparse in textual description is not easy. Figure <ref type="figure" target="#fig_0">1</ref> (1st row) shows two images of "sadness" and "happiness" emotions respectively. Nevertheless, no obvious emotion clues are observed by merely reading their text captions, which play the roles of referring visual content (roller coaster) rather than emotion in the images. In other words, the captions convey semantic meanings while the actual emotion signals are buried inside the images. Apparently, visual content such as color contrast and tone provide more vivid clues to reveal the underlying emotions for this example. The second row of Figure <ref type="figure" target="#fig_0">1</ref> shows a counter example, where the emotions are subject to user perceptions by looking at the snapshots of dogs sampled from two different videos. By turning off the visual signals and only listening to audio effects, the woofing and laughing sounds of dogs already convey a strong sense of emotions. In this example, the moods underlying videos are also somewhat captured by the words "dirty", "hates" and "loves" in the captions.</p><p>Due to these limitations, there are several works studying the fusion of multimodal features, including multi-kernel fusion <ref type="bibr" target="#b0">[1]</ref>, conditional random field <ref type="bibr" target="#b10">[11]</ref> and Bayesian rules <ref type="bibr" target="#b11">[12]</ref>. These works are based on the standard early or late fusion strategies <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>, despite the employment of different machine learning models. The "shallow" way of combining different features is also questionable in principle, given the diverse representations and correlational structures among different modalities. For example,</p><p>it remains an open problem on the right way of combining raw pixels and audio waveform, for joint understanding of phonemes and visemes (lip pose and motion), in speech recognition <ref type="bibr" target="#b14">[15]</ref>. In brief, the highly non-linear relationships existing between different modalities, particularly, are often overlooked by the existing approaches.</p><p>Emotion is also correlated with surrounding context, specifically the objects, sounds and scenes in images or videos. For example, a video with "a man listening to bird chirping in the garden" casts an enjoy mood useful for emotion prediction. SentiBank <ref type="bibr" target="#b5">[6]</ref> is one such recent effort that explicitly identifies 3,244 adjective noun pairs (ANPs) for learning large-scale emotion-oriented concept classifiers. Examples of ANPs are "amazing flowers", "awesome view" and "shy smile". Nevertheless, due to the open nature of how nouns and adjectives are combined, SentiBank is hard to be generalized to cover the possible ANPs, not even mentioning the daunting efforts required in labeling of training examples for training ANP classifiers free of sample noise.</p><p>In this paper, we propose a more generalized framework for unsupervised feature learning based on Deep Boltzmann machine (DBM) <ref type="bibr" target="#b15">[16]</ref>, aiming to learn features coupling emotion and semantic signals buried in multimodal signals. As we consider eight types of wildly different features in terms of statistical properties, deep-based learning is preferable for inferring non-linear relationships among features within and across modalities. A joint embedded space shared by multimodal signals is expected to capture such relationships with semantic and emotion contexts. As studied in other works <ref type="bibr" target="#b16">[17]</ref>, a joint space learnt by in multimodal settings. Furthermore, the joint space also enlightens cross-modal retrieval, where for example, either text-to-video or video-to-text search can be performed under our model.</p><p>The main contribution of this paper is the proposal of a deep multimodal learning platform that enables a more generalized way of learning features coupled with emotions and semantics. Empirical studies also demonstrate the feasibility of employing such features for multi-modal affective classification and retrieval. The remaining of this paper is organized as follows. Section II presents related works.</p><p>Section III presents the deep network architecture, followed by sections IV and V describing the joint space representation and network learning respectively. Section VI and VII presents experimental results on affective analysis and retrieval respectively, and finally Section VIII concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Affective computation has been extensively studied in the last decades, and many methods are proposed for handling various media types including textual documents <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>, images <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>, music <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> and movies <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Two widely investigated tasks are emotion detection and sentiment analysis. Both of them are standard classification problems with different state spaces. Usually emotion detection is defined on several discrete emotions, such as anger, sadness, joy etc., while sentiment analysis aims at categorizing data into positive or negative. Since the adopted techniques of these two tasks are quite similar, we will not differentiate them in this section. Previous efforts are summarized mainly based on the modality of the data they are working on.</p><p>For textual data, lexicon-based approach using a set of pre-defined emotional words or icons has been proved to be an effective way. In <ref type="bibr" target="#b1">[2]</ref>, they propose to predict the sentiment of tweets by using the emoticons (e.g., positive emoticon ":)" and negative one ":=(") and acronyms (e.g., lol (laugh out loudly), gr8 (great) and rotf (rolling on the floor)). A partial tree kernel is adopted to combine the emoticons, acronyms and Part-of-Speech (POS) tags. In <ref type="bibr" target="#b3">[4]</ref>, three lexicon emotion dictionaries and POS tags are leveraged to extract linguistic features from the textual documents. In <ref type="bibr" target="#b2">[3]</ref>, a semantic feature is proposed to address the sparsity of microbloggings. The non-appeared entities are inferred using a pre-defined hierarchical entity structure. For example, "iPad" and "iPhone" indicate the appearance of "Product/Apple". Furthermore, the latent sentiment topics are extracted and the associated sentiment tweets are used to augment the original feature space. In <ref type="bibr" target="#b4">[5]</ref>, a set of sentimental aspects, such as opinion strength, emotion and polarity indicators, are combined as meta-level features for boosting the sentiment classification on Twitter messages.</p><p>Affective analysis of images adopts a similar framework with general concept detection. In SentiBank <ref type="bibr" target="#b5">[6]</ref>, a set of visual concept classifiers, which are strongly related to emotions and sentiments, are trained based on unlabeled Web images. Then, a SVM classifier is built upon the output scores of these concept classifiers. The performance of SentiBank is recently improved by using deep convolution neural network (CNN) <ref type="bibr" target="#b6">[7]</ref>. Nevertheless, the utility of SentiBank is limited by the number and kind of concepts (or ANPs). Due to the fact that ANPs are visually emotional concepts, selection of right samples for classifier training could be subjective. In addition to the semantic level features, a set of low-level features, such as color-histogram and visual aesthetics, are also adopted in <ref type="bibr" target="#b7">[8]</ref>. The combined features are then fed into a multi-task regression model for emotion prediction. In <ref type="bibr" target="#b20">[21]</ref>, hand-crafted features derived from principles-of-art such as balance and harmony are proposed for recognition of image emotion. In <ref type="bibr" target="#b9">[10]</ref>,</p><p>the deep CNN is directly used for training sentiment classifiers rather than using a mid-level consisting of some general concepts. Since Web images are weakly labelled, the system progressively select a subset of the training instances with relatively distinct sentiment labels to reduce the impact of noisy training instances.</p><p>For emotional analysis of music, various hand-crafted features corresponding to different aspects (e.g., melody, timbre and rhythm) of music are proposed. In <ref type="bibr" target="#b18">[19]</ref>, the early fused features are characterized by cosine radial basis function (RBF). In <ref type="bibr" target="#b21">[22]</ref>, a ListNet layer is added on top of the RBF layer for ranking the music in valence and arousal in Cartesian coordinates. Besides hand-crafted features, the authors in <ref type="bibr" target="#b19">[20]</ref> adopt deep belief networks (DBN) on the Discrete Fourier Transforms (DFTs) of music signals.</p><p>Then, SVM classifiers are trained on the latent features from hidden layers.</p><p>In the video domain, most research efforts are dedicated to movies. In <ref type="bibr" target="#b13">[14]</ref>, a large emotional dataset, which contains about 9,800 movie clips, is constructed. SVM classifiers are trained on different lowlevel features, such as audio features, complexity and color harmony. Then, late fusion is employed to combine the classifiers. In <ref type="bibr" target="#b12">[13]</ref>, a set of features are proposed based on psychology and cinematography for affective understanding in movies. Early fusion is adopted to combine the extracted features. Other fusion strategies on auditory and visual modalities are studied in <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, a hierarchical architecture is proposed for predicting both emotion intensity and emotion types. CRF is adopted to model the temporal information in the video sequence. In addition to movies, a large-scale Web video dataset for emotion analysis is recently proposed in <ref type="bibr" target="#b0">[1]</ref>, where a simplified multi-kernel SVM is adopted to combine the features from different modalities.</p><p>Different from those works, the approach proposed in this paper is a fully generative model, which defines a joint representation for various features extracted in different modalities. More importantly, the joint representation conveying information from multiple modalities can still be generated when some modalities are missing, which means that our model does not restrict to the media types of user generated contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP NETWORK DESIGN</head><p>Figure <ref type="figure">2</ref> shows the proposed network architecture, which is composed of three different pathways respectively for visual, auditory and textual modalities. Each pathway is formed by stacking multiple</p><p>Restricted Boltzmann Machines (RBM), aiming to learn several layers of increasingly complex representations of individual modality. Similar to <ref type="bibr" target="#b22">[23]</ref>, we adopt Deep Boltzmann Machine (DBM) <ref type="bibr" target="#b15">[16]</ref> in our multimodal learning framework. Different from other deep networks for extracting feature, such as Deep Belief Networks (DBN) <ref type="bibr" target="#b23">[24]</ref> and denoising Autoencoders (dA) <ref type="bibr" target="#b24">[25]</ref>, DBM is a fully generative model which can be utilized for extracting features from data with certain missing modalities. Additionally, besides the bottom-up information propagation in DBN and dA, a top-down feedback is also incorporated in DBM, which makes the DBM more stable on missing or noisy inputs such as weakly labelled data on the Web. The pathways eventually meet and the sophisticated non-linear relationships among three modalities are jointly learned. The final joint representation can be viewed as a shared embedded space, where the features with very different statistical properties from different modalities can be represented in an unified way.</p><p>The proposed architecture is more generalized and powerful in terms of scale and learning capacity.</p><p>In visual pathway, the low-level features amount to 20,651 dimensions, resulting in large number of parameters to be trained if connecting them directly to the hidden layer. Instead, we design a separate pathway for each low-level feature, which requires less parameters and hence more flexible and efficient to train. This advantage makes our system more scalable to handling higher dimensional features, rather than features of 3,875 dimensions used in <ref type="bibr" target="#b22">[23]</ref>. We further consider learning the separated pathways in visual modality in parallel. The computational cost can be further reduced. Furthermore, we generate a compact representation which represents the common feature and preserves the unique characteristic of each visual feature. In this way, it will not overwhelm other modalities because of high dimensionality during joint representation learning. Auditory and textual pathway do not suffer from this problem.</p><p>However, the proposed structure can be easily extended for other modalities.</p><p>It is worth noticing that the high-level semantics in visual and auditory modalities can be represented in the final joint representation, by considering the correlations between them and textual inputs during training. Since our model is fully generative, the semantics of input data without textual modality can also be extracted. Other semantic features for visual and auditory data (e.g., SentiBank <ref type="bibr" target="#b5">[6]</ref>, Classemes <ref type="bibr" target="#b25">[26]</ref> and Objectbank <ref type="bibr" target="#b26">[27]</ref>) basically adopt the shallow learning models, which learn the local patterns extracted from the data. These methods suffer from information loss <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and is sensitive to the diverse appearance of input data. In contrast, our model has the capability of mining generative representations from the raw data, which has been proved to be more powerful <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MULTIMODAL JOINT REPRESENTATION</head><p>Our network is built upon RBMs. A standard RBM has two binary-valued layers, i.e., visible layer (denoted as v) and hidden layer (denoted as h). The probability distribution over the inputs v is defined as</p><formula xml:id="formula_0">P (v) = 1 Z ∑ h exp(-E(v, h)).<label>(1)</label></formula><p>E(v, h) is the free energy between v and h, given by</p><formula xml:id="formula_1">E(v, h) = - ∑ ij v i W ij h j - ∑ i a i v i - ∑ j b j h j , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where W is the weight of link connecting two layers, a and b are the bias weights for v, h respectively.</p><p>The feature learning problem is elegantly stated to maximize the probability in Equation <ref type="formula" target="#formula_0">1</ref>or to minimize the free energy in Equation <ref type="formula" target="#formula_1">2</ref>. The standard RBM can only handle binary-valued inputs. Other generalized RBMs include Gaussian RBM <ref type="bibr" target="#b30">[31]</ref> designed for modeling real-valued inputs and Replicated Softmax <ref type="bibr" target="#b31">[32]</ref> for modeling sparse word count vectors. Next, we describe different pathways and their joint representation. Each pathway consists of a stack of RBMs selected according to the property of input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visual Pathway</head><p>The visual input consists of five complementary low-level features widely used in previous works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>. As shown in Figure <ref type="figure">2</ref>, each feature is modeled with a separate two-layer DBM. Let K = {d, g, o, l, s} denote the set of five features, respectively as DenseSIFT <ref type="bibr" target="#b32">[33]</ref>, GIST <ref type="bibr" target="#b33">[34]</ref>, HOG <ref type="bibr" target="#b34">[35]</ref>, LBP <ref type="bibr" target="#b35">[36]</ref> and SSIM <ref type="bibr" target="#b36">[37]</ref>. Furthermore, let</p><formula xml:id="formula_3">v V = {v k }, h 1 V = {h (1k) } and h 2 V = {h (2k)</formula><p>} as the sets of real-valued inputs, first and second hidden layers respectively, where k ∈ K. For example, v d refers to the visible layer for DenseSIFT. In addition, the joint layer in visual pathway (the layer in red in Figure <ref type="figure">2</ref> is denoted as h (v)   The connections between v k and h (1k) are modeled with Gaussian RBM <ref type="bibr" target="#b30">[31]</ref> and the connections between h (1k) and h (2k) are modeled with standard binary RBM. Hence, the probability distribution over the real-valued input v k is given by where Z(θ k ) is the partition function and the free energy E is defined as</p><formula xml:id="formula_4">E(v k , h (1k) , h (2k) ; θ k ) = ∑ i (v k i -b k i ) 2 2(δ k i ) 2 - ∑ ij v k i δ k i W (1k) ij h (1k) j - ∑ jl h (1k) j W (2k) jl h (2k) j (4)</formula><p>where θ k = {a, b, W (1k) , W (2k) } are the model parameters. Note that for brevity, the bias terms a on the hidden layers are omitted. To generate the joint representation over these five low-level features, we combine the five DBM models by adding an additional layer h (v) on top of them. Then, the joint density distribution over the five features v V is given by</p><formula xml:id="formula_5">P (v V ; θ v ) = ∑ h 2 V ,h (v) P (h 2 V , h (v) )( ∏ k∈K ( ∑ h (1k) P (v k , h (1k) | h (2k) ))),<label>(5)</label></formula><p>The density distribution</p><formula xml:id="formula_6">P (h 2 V , h (v)</formula><p>) in Equation <ref type="formula" target="#formula_5">5</ref>is given by</p><formula xml:id="formula_7">P (h 2 V , h (v) ) = 1 Z(θ v ) ∏ k∈K exp( ∑ pq W (3k) h (2k) p h (v) q )<label>(6)</label></formula><p>The joint distribution 2k) in Equation <ref type="formula" target="#formula_5">5</ref>can be easily inferred from Equation <ref type="formula">3</ref>as</p><formula xml:id="formula_8">P (v k , h (1k) | h (2k) ) of v k and h (1k) over h (</formula><formula xml:id="formula_9">P (v k , h (1k) | h (2k) ) = 1 Z(θ k ) exp(- ∑ i (v k i -b k i ) 2 2δ 2 i + ∑ ij v k i δ i W (1k) ij h (1k) j + ∑ jp W (2k) jp h (1k) j h (2k) p )<label>(7)</label></formula><p>Until now, all the probability distributions in Equation 5 are provided and the probability distribution over the whole set of input features v V in visual pathway can be easily inferred by subscribing these equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Auditory Pathway</head><p>The input features adopted in auditory pathway are MFCC <ref type="bibr" target="#b37">[38]</ref> and Audio-Six (i.e., Energy Entropy, Signal Energy, Zero Crossing Rate, Spectral Rolloff, Spectral Centroid, and Spectral Flux) <ref type="bibr" target="#b0">[1]</ref>. The Audio-Six descriptor, which can capture different aspects of an audio signal, is expected to be complementary to the MFCC. Since the dimension of Audio-Six is only six, we directly concatenate the MFCC feature with Audio-Six rather than separating them into two sub-pathways as the design in visual pathway. The correlation between these two features can be learned by the deep architecture of DBM <ref type="bibr" target="#b22">[23]</ref>. Let v a denote the real-valued auditory features and h (1a) and h (2a) represent the first and second hidden layers respectively. Similar to Equation 3, the DBM is constructed by stacking one Gaussian RBM and one standard binary RBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Textual Pathway</head><p>Different from the visual and auditory modalities, the inputs of the textual pathway are discrete values (i.e., count of words). Thus, we use Replicated Softmax <ref type="bibr" target="#b31">[32]</ref> to model the distribution over the word count vectors. Let v t as a visible unit denoting the associated metadata (i.e., title and description) of a video t, and v t k denotes the count of the k th word in a pre-defined dictionary containing K words. The first and second hidden layers are h (1t) and h (2t) . Then, the probability of generating v t by the text-specific two-layer DBM is given by</p><formula xml:id="formula_10">P (v t ; θ t ) = 1 Z(θ t ) ∑ h (1t) ,h (2t) exp( ∑ jk W (1t) kj h (1t) j v t k ) + ∑ jl W (2t) jl h (1t) j h (2t) l + N ∑ j b (1t) j h (1t) j )<label>(8)</label></formula><p>Note that the bias term of the first hidden layer h (1t) is scaled up by the length of the document. This scaling is important for allowing hidden units to behave sensibly when dealing with documents of different lengths. As stated in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b31">[32]</ref>, without the bias scaling, the scale of the weights would be optimized to fit to the average document length. This would induce that the longer documents tend to saturate the units and shorter ones may be ambiguous on activating the hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Joint Representation</head><p>To combine the learned representations of DBMs for the three modalities, an additional layer is added on top of the three pathways, which is annotated as "Joint Representation" in Figure <ref type="figure">2</ref>. We denote this layer as h (J) . We further use v = {v V , v a , v t } to represent all the visible inputs. The final joint density distribution over multi-model inputs can be written as</p><formula xml:id="formula_11">P (v; θ) = ∑ h (v) ,h (2a) ,h (2t) ,h (J) P (h (v) , h (2a) , h (2t) , h (J) ) ( ∑ h 1 V ,h 2 V P (v V , h 1 V , h 2 V | h (v) ))( ∑ h (1a) P (v a , h (1a) | h (2a) )) ( ∑ h (1t) P (v t , h (1t) | h (2t) ))<label>(9)</label></formula><p>By subscribing equations 5 and 8 into above equation, the probability distribution over the multiple inputs formulated by the proposed network can be easily inferred. We do not show the detail formula here due to the space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. NETWORKING LEARNING AND INFERENCING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Approximate Network Learning</head><p>The learning of our proposed model is not trivial due to multiple layers of hidden units and multiple modalities. Inspired by <ref type="bibr" target="#b22">[23]</ref>, we split the learning process into two stages. First, each RBM component of the proposed multimodal DBM is pretrained by using the greedy layerwise pretraining strategy <ref type="bibr" target="#b15">[16]</ref>. In this stage, the time cost for exactly computing the derivatives of the probability distributions with respect to parameters increases exponentially with the number of units in the network. Thus, we adopt 1-step contrastive divergence (CD 1 ), an approximate learning method, to learn the parameters. In CD k algorithm, the training sample and its reconstruction is used to approximate the direction of the change for the parameters. In practice, CD 1 is widely used for RBM training, since good approximation of the changing direction is already obtained when k = 1. Note that, CD 1 actually performs poorly in approximating the size of the change in parameters. However, it is accurate enough for learning a RBM to provide hidden features for a high-level RBM training <ref type="bibr" target="#b38">[39]</ref>. This is because CD 1 retains most of the information in the inputs.</p><p>As discussed in <ref type="bibr" target="#b38">[39]</ref>, CD 1 is still far from optimal to be used for learning a joint-density model.</p><p>Therefore, in the joint learning stage, we adopt a more radical departure from CD 1 , named as "persistent contrastive divergence" (PCD) <ref type="bibr" target="#b39">[40]</ref>. In contrast to initialize each alternating Gibbs Markov chain at a training sample, the states of a number of persistent chains or "fantasy particles" are tracked in PCD.</p><p>Each persistent chain has its hidden and visible states, which are generated by running mean-field updates with Gibbs sampling for one or a few times after each weight is updated. Then the derivative of the probability distribution is approximated by the difference between the pairwise statistics measured on a mini-batch of data and the persistent chains. Since the weight-updates repel each chain from its current state by raising the energy of that state, the persistent chains mix surprisingly fast <ref type="bibr" target="#b40">[41]</ref>. Furthermore, PCD also learns significantly better models than CD 1 or even CD 10 as reported in <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Joint Representation Inferring</head><p>The representation learnt by the proposed model is a set of distributions over layers conditioned on their adjacent layers. If all the modalities are present, the element h</p><formula xml:id="formula_12">(J)</formula><p>p in joint representation is inferred </p><formula xml:id="formula_13">p(h (J) p = 1 | h (v) , h (2a) , h (2t) ) = g( ∑ q W (Jv) qp h (v) q + ∑ l W (Ja) lp h (2a) l + ∑ m W (Jt) mp h (2t) m )<label>(10)</label></formula><p>where g(x) = 1/(1 + exp(-x)) and bias term is omitted from presentation. For full details of inference at each hidden layer, please refer to Appendix.</p><p>There are two ways to generate the joint representation if some modalities are not available. First we can directly generate the joint representation based on the existing modalities only and leave the missing ones out. For example, if text is missing, the joint representation will be computed by p(h</p><formula xml:id="formula_14">(J) p = 1 | h (v) , h (2a) ).</formula><p>The second way is to infer the missing modalities by alternating Gibbs sampling. Meanwhile, the joint representation is updated with the generated data of missing modalities. For example, assuming that the textual modality is missing, the observed visual modality v V and auditory modality v a are clamped at the inputs and all hidden units are initialized randomly. Alternating Gibbs sampling is used to draw</p><formula xml:id="formula_15">samples from P (v t | v V , v a )</formula><p>by updating each hidden layer given the states of the adjacent layers. As reported in <ref type="bibr" target="#b22">[23]</ref>, the second method achieves better performance than the first one, which indicates that the mutlimodal DBM can generate meaningful representations of the missing modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussions</head><p>While the proposed architecture follows the principle <ref type="bibr" target="#b22">[23]</ref>, the main novelty comes from the design of multiple visual pathways. Despite that the architecture may appear more complicated than <ref type="bibr" target="#b22">[23]</ref> at first glance, the design indeed simplifies <ref type="bibr" target="#b22">[23]</ref> by significantly reducing the number of hyper parameters.</p><p>With reference to Table I that lists the number of neurons in each layer, the network contains 99,690,496 hyper parameters. While this number is terribly high, it requires only 29% of that parameters in <ref type="bibr" target="#b22">[23]</ref>,</p><p>for converting the input features from 20,651 to 2,048 dimensions. Furthermore, the design accelerates learning by allowing parallel training of parameters on each pathway. The locally connected hidden units in the pathways also speed up the PCD learning at the second stage. In our experiments, by using Tesla-K20 GPU, network learning completes in about one week with around 1 million training examples. Given the same amount of time, <ref type="bibr" target="#b22">[23]</ref> is only able to train a network with input features of 3,875 dimensions.</p><p>In short, our proposed network is more scalable in learning and effective in testing (see Section VI) than <ref type="bibr" target="#b22">[23]</ref>.</p><p>Overfitting becomes an issue with large number of hyper parameters to be learnt in the network.</p><p>As stated in <ref type="bibr" target="#b38">[39]</ref>, assuming that each image contains 1,000 pixels, using </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Learning</head><p>We constructed two datasets, E-Flickr and E-YouTube, for DBM learning. The images in E-Flickr are crawled from Flickr by using the 3244 ANPs used in SentiBank <ref type="bibr" target="#b5">[6]</ref> as keywards. On average, there are 250 images being retrieved for each ANP. The number of images per ANP is kept to about the same so as not to bias any ANP during DBM learning. All these images, along with their metadata (title, descriptions and tags), are included in E-Flickr. Similarly for E-YouTube, ANPs keywords are issued to YouTube for crawling videos. For each ANP, only top-100 ranked videos are considered, considering  The set of features extracted from the datasets are summarized in Table <ref type="table" target="#tab_1">I</ref>. For each video, keyframes are sampled at the rate of one frame per second. Five different visual features, as listed in Table <ref type="table" target="#tab_1">I</ref>, are respectively extracted from the keyframes and then averagely pooled to form feature vectors. Audio features are extracted over every 32ms time-window of audio frames, with 50% overlap between two adjacent windows. Similar as visual features, these features are averagely pooled across time-windows.</p><formula xml:id="formula_16">h (2d) h (1g) h (2g) h (1o) h (2o) h (1l) h (2l) h (1s) h (2s) h (v) h (1a) h (2a) h (J)</formula><p>Among the set of audio-visual features, DenseSIFT, HOG, SSIM and MFCC are further quantized into bag-of-words representation. We followed the same settings as <ref type="bibr" target="#b0">[1]</ref>, and the dimensions of different features are listed in the second column of Table <ref type="table" target="#tab_1">I</ref>. As for textual features, a total of 1,447,612 distinct words are extracted from E-Flickr and E-YouTube after stopword removal and lemmatization using CoreNLP <ref type="bibr" target="#b41">[42]</ref>.</p><p>In the experiments, only words with document frequency larger than 800 are kept. Eventually, textual feature is in 4,314 dimensions, with an average of 13 words per image and 8 words per video.</p><p>Note that video-level metadata only describes a fraction of video keyframes, and furthermore, feature pooling could possible introduce noise. In contrast, image-level metadata provides relatively more specific description of image content and emotion. For this consideration, the learning of DBM is started from using image samples followed by video sample. Audio-pathway is left out during pre-training using E-Flickr images, but turned on when E-YouTube videos are involved for fine-tuning. During training, each dimension of visual and auditory features is mean-centered and normalized to unit variance to avoid the instability problem <ref type="bibr" target="#b38">[39]</ref>. In addition, to avoid running separate Markov chains for each word count to get sufficient statistics for modeling distribution, all word count vectors are scaled so that they sum to 5 <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this section, we evaluate the performance of the joint representation learnt using our multimodel DBM on affective analysis. We name our model as E-MDBM since the architecture has been enhanced</p><p>with more features and modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Video Emotion Detection</head><p>The "VideoEmotion" dataset consists of 1,101 videos which are manually labeled with eight emotional categories. Following <ref type="bibr" target="#b0">[1]</ref>, the results are evaluated by accuracy. As the textual information of the videos is not provided, we only have visual and auditory modalities in this dataset.</p><p>Effect of exploring multimodal relations. We first evaluate the capability of proposed model in learning non-linear relations among different modalities. The input to the textual pathway is missing and initialized to zeros. As described in Section V-B, the model is allowed to update the state of the textual input layer when performing mean-field update by alternating Gibbs sampling. In this experiment, we run the meanfield update for 5 times <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The final joint representations (up layer) are drawn from P (h</p><formula xml:id="formula_17">(J) |v V , v a )</formula><p>and used for learning a logistic regression model. For comparison, classifiers using the same training data are learned with the representations extracted from different hidden layers in Figure <ref type="figure">2</ref>.</p><p>We adopt the same settings for train-test splits in <ref type="bibr" target="#b0">[1]</ref>. Ten train-test splits are generated, each using 2/3 of the data for training and 1/3 for testing. Table <ref type="table" target="#tab_2">II</ref> shows the prediction results. We can see that the joint representation h (J) achieves the best overall performance. It improves the accuracy over the joint visual representation h (v) and the representation from second auditory hidden layer h (2a) by 8.02% and 41.26%</p><p>respectively. Although single modality may perform slightly better than the joint representation for some emotions, the performance is not consistent. For example, auditory feature is better to recognize "Disgust", but it performs poorly for emotion "Fear". This is because that "Fear" is not apparently conveyed in the auditory signal. However, visual feature achieves the best result on "Fear". Another interesting observation is that the performance using second hidden layer of each pathway is generally better than that of the first hidden layer. As mentioned in Section III, the E-MDBM model is a fully generative model. The neurons of hidden layers will receive messages from both lower layers and higher layers. By using this top-down feedback, the higher hidden layers can deal with the impact from ambiguous inputs, and thus are more robust. In addition, the joint representation (h (v) ) on visual pathway leads to 2.5% improvement over the best performance achieved in single visual feature. This indicates that the proposed structure can preserve the capability of learning correlation between different visual features, meanwhile reduce the complexity of the model learning comparing to <ref type="bibr" target="#b22">[23]</ref>.</p><p>Figure <ref type="figure">3a</ref> further shows the confusion matrix based on the joint representation h (J) . Most categories are confused with the category "Surprise", where similar observation is also noted in <ref type="bibr" target="#b0">[1]</ref>. Second, the category "anticipation" is confused particularly by "Fear" and "Surprise". As shown in Table <ref type="table" target="#tab_2">II</ref>, almost all features perform poorly on this category. We attribute this unsatisfactory performance to the fact that neither audio nor visual can concretely describe the emotion of "anticipation", for example, in a sport event. Facial expression seems to be the dominant cue in conveying "anticipation". This is probably the reason that LBP, often being applied for face recognition, performs comparatively good for this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of missing modality.</head><p>There is no textual modality in this dataset. We further evaluate the impact of missing certain modality by using either only visual feature or auditory feature as input, which are named as E-MDBM-V and E-MDBM-A respectively. The input of missing modality is initialized with zero and updated in the same way with the missing textual modality. The results are showed in Figure <ref type="figure" target="#fig_4">4</ref>. We also show the result of h (J) in Table <ref type="table" target="#tab_2">II</ref>, which is named as E-MDBM-VA for consistency.</p><p>Additionally, the performance of SentiBank from <ref type="bibr" target="#b0">[1]</ref> and art features (AF) from <ref type="bibr" target="#b20">[21]</ref> are also shown here for comparison. In SentiBank, logistic regression model is trained on the scores of the 1,200 ANP classifiers. For AF, 10 features are proposed in <ref type="bibr" target="#b20">[21]</ref> for representing 6 artistic principles (i.e., balance, emphasis, harmony, variety, gradation, and movement) and logistic regression model is learnt on these features. Although our model is able to fill in the missing modalities and integrate the information into the final joint representation, E-MDBM-VA using both visual and auditory inputs exhibits the best performance. This is not surprising since the filling of missing modalities generated some noises comparing to the real data. However, the accuracy of E-MDBM-V is approaching that of E-MDBM-VA.</p><p>For category "surprise", E-MDBM-V is even better. This indicates that the missing data is somehow recovered using this model. In addition, only visual features are used in SentiBank, AF and E-MDBM-V.</p><p>Our model outperforms SentiBank and AF by 7.7% and 5.6% in accuracy respectively. This is because that our model embeds the information from all the three modalities during unsupervised model training. Thus the correlations between visual modality and other two modalities can be jointly represented, especially We can also observe that the auditory information seems less effective comparing to visual information.</p><p>However, it works better in "Disgust" and "Sadness" categories, where the visual information cannot provide enough clues for emotion detection. For instance, there is a video showing a cat, which is annotated as "Disgust". The visual appearance actually conveys no emotional information. On the other hand, the background music, which is very sharp and noisy, actually makes people feel uncomfortable and disgust. The same situation exists in the "Sadness" videos. There are only several common objects shown in the video, such as faces and people hugs, whereas, woeful music is used as background. In short, missing of modality will degrade the performance even using our proposed model. However, the joint representation can somehow capture the correlations between different modalities, and is a good compensation when certain modality is not available.</p><p>Comparison with state-of-the-arts. We compare our model with the simplified version in <ref type="bibr" target="#b22">[23]</ref>. For fair comparison, we extend the model in <ref type="bibr" target="#b22">[23]</ref> to handle three modalities by adding a new pathway for textual input. This model is named as MDBM. Same training data and settings described in Section VI-A are employed for learning MDBM. We also show the results reported in <ref type="bibr" target="#b0">[1]</ref> here. These results can be considered as the state-of-the-arts as they are produced through fine tuning on the dataset. In <ref type="bibr" target="#b0">[1]</ref>, various auditory (Au.) features, visual (V.) features, attribute (At.) features, and their combinations are evaluated.</p><p>Table <ref type="table" target="#tab_4">III</ref> shows the best one in each kind of the feature. Furthermore, the combinations of our joint representation and other features are also included. In addition, the performance based on the art features (AF) <ref type="bibr" target="#b20">[21]</ref> is also reported.</p><p>We can see that E-MDBM consistently outperforms MDBM either evaluated individually or combined with other features. Specifically, E-MDBM leads to 9% performance improvement over MDBM. This indicates that our design can better preserve the unique property of each visual feature during the learning in visual pathway by splitting the architecture into several sub-pathways, each of which corresponds to one feature. In contrast, all the features are concatenated into one feature vector in MDBM, where some visual features may be overwhelmed during the learning. However, V.+Au. performs better than E-MDBM. This is probably because the pre-training of our model is performed on Flickr images, while V.+Au. is tuned on the Web videos in VideoEmotion dataset. The domain gap may influence the learned joint representation.</p><p>As stated in <ref type="bibr" target="#b0">[1]</ref>, this may also cause the performance degradation of attribute features (e.g., SentiBank) which are extracted using concept classifiers learned on Web images. Despite the domain gap, E-MDBM consistently achieves better performance than SentiBank, either when being utilized individually or fused with other features. While not performing better than hand-crafted features, E-MDBM can complement these features well. When average lately fused with features in <ref type="bibr" target="#b0">[1]</ref>, an improvement of 11.09% is attained.</p><p>Further fusion with attribute features (i.e., SentiBank, Classemes and ObjectBank), an accuracy of 0.511 is attained. The degree of improvement introduced by E-MDBM is greater than that can be offered by MDBM. Figure <ref type="figure">3b</ref> shows the confusion matrix based on fusion results of E-MDBM. Compared with Figure <ref type="figure">3a</ref>, four categories, especially "Trust", become less confused by "Surprise" after fusion.</p><p>Nevertheless, the performance for "Anticipation" remains poor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sentiment Analysis on Twitter Messages</head><p>To avoid the impact of domain shift, we further conduct experiments on "ImageTweets" dataset <ref type="bibr" target="#b5">[6]</ref>,</p><p>which includes 596 text-image Twitter messages. In this dataset, only textual and visual modality are available. These messages are manually assigned to either positive or negative sentiment based on affection expressed in the text-image pairs.</p><p>We compare our model with several state-of-the-art methods used in <ref type="bibr" target="#b5">[6]</ref>. Besides the early fused low-level visual features ("Visual") and attribute feature ("SentiBank") extracted from images, a lexiconbased approach ("Lexicon") used on textual analysis is also selected as baseline. The text information is represented using the sentiment scores of words obtained from SentiStrength <ref type="bibr" target="#b42">[43]</ref>. The art features (AF) <ref type="bibr" target="#b20">[21]</ref> is also adopted as one baseline. The classifiers for textual feature and visual features are Naive Bayes classifier and logistic regression model respectively. MDBM <ref type="bibr" target="#b22">[23]</ref> is utilized on this dataset with visual+textual input. For our proposed E-MDBM, we consider three features E-MDBM-V, E-MDBM-T amd E-MDBM-VT corresponding to visual input, textual input and visual+textual input respectively. For fair comparison, logistic regression model is used upon these joint representations.</p><p>In <ref type="bibr" target="#b5">[6]</ref>, the dataset is equally split into five subsets. In our experiment, each classifier is trained on four subsets and tested on the other. This process is repeated five times. Figure <ref type="figure" target="#fig_5">5</ref> shows the average results. We first consider the case of single modality input. We can see that Lexicon performs much worse than other methods. This is not surprising since Twitter messages are usually sparse and lack of emotion signals.</p><p>In comparison, with the same input, our joint representation E-MDBM-T achieves 43.49% improvement over Lexicon. Again, this demonstrates that the common space embedded in the E-MDBM preserves the correlations from multiple modalities with different emotional signals. Thus the sparsity problem can be addressed using the information from other modalities by mapping from textual input to the joint space.</p><p>For the visual modality, different from the results in Table <ref type="table" target="#tab_4">III</ref>, SentiBank and E-MDBM-V perform much better than low level visual features (Visual) on image dataset. This is because that both our model and attribute feature represent the semantics in the images, which can narrow down the gap between low-level features and high level human perceptions. In addition, AF achieves a similar performance to SentiBank, indicating that the Tweet images also partially follow the artistic principles. Again, E-MDBM-V improves over SentiBank and AF by 4.14% and 3.11% in accuracy respectively. We then compare the performances of different methods using multiple modalities. For comparison, we also show the result of lately fused Lexicon and SentiBank (SentiBank+Lexicon) features. We can see that E-MDBM-VT exhibits the best results, which leads to 8% and 5% improvements over SentiBank+Lexicon and MDBM respectively.</p><p>Finally, similar to the conclusion made on VideoEmotion dataset, the performances of E-MDBM-V and E-MDBM-T are worse than that of E-MDBM-VT, which also indicates that there are some noises in the generated missing modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENT: RETRIEVAL</head><p>This section experiments the feasibility of the proposed model for video retrieval (Section VII-A) and cross-media retrieval (Section VII-B). A new dataset including 1,139 reference videos was constructed YouTube. With this dataset, we can perform Web video retrieval using different types of queries. In specific, we randomly select 50 videos covering all the eight emotions in <ref type="bibr" target="#b0">[1]</ref> as queries. In some of the queries, the content and emotion in different modalities are not always aligned. Figure <ref type="figure">6</ref> shows some examples of queries. The selected 23 ANPs include 18 emotion words<ref type="foot" target="#foot_2">1</ref> and 16 general concept words<ref type="foot" target="#foot_3">2</ref> .</p><p>We first annotate all the test and query videos for the 34 words. A video is considered to be relevant to the query if they share at least one emotion category or concept. In other words, two videos can be semantically or emotionally relevant. In this way, we can generate ground-truth for each query. Note that as emotion words are extracted from ANPs, their categories are inferred from which the ANPs belong to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Retrieval</head><p>Three sets of experiments are conducted by using the text, video and multimodal (text+video) queries.</p><p>For each query, a joint representation is extracted using the proposed E-MDBM. Note that there are missing modalities for text or video queries. For the reference videos, we assume that all the three modalities are available when extracting the joint representations. The baselines used in the experiments depend on the modality of queries. For text query, we compute Jaccard coefficient between the word count vector of a query and the surrounding text of a reference video. For video query, we select two baseline methods that utilize visual and auditory information by fusing low level visual feature and SentiBank respectively with auditory feature. For multimodel query, the two baselines are further augmented using textual feature. In this way, for a given type of query, the compared methods leverage same set of input features.</p><p>Table <ref type="table" target="#tab_5">IV</ref> shows the results of video retrieval on eight different categories of emotions in terms of mean average precision (MAP). E-MDBM achieves consistently better performances than the baselines in all three types of queries. As baseline methods consider only matching the modalities of same type during similarity measure, reference videos with the searched content or emotion exists in a modality different from the type of query cannot be retrieved. For example, although the top-left video in Figure <ref type="figure">6</ref> shows a "joy" emotion with a dog befriends a cat, text-only query is misled by the word "disable". For this particular example, the performance is poor even for multimodal query with late fusion strategy. Similarly for the two queries shown in the second row of Figure <ref type="figure">6</ref>, where there are mismatches between the concepts in captions and video content. Late fusion of multiple modalities helps little for these example queries.</p><p>In contrast, by non-linearly projecting all the reference videos into a joint space, E-MDBM has generated features that inherently capture the complex relationship among different modalities of videos. Hence, cross-modal matching between queries and reference videos are implicitly performed during retrieval.</p><p>For the examples in Figure <ref type="figure">6</ref>, E-MDBM shows improvement in large margin, for example, the AP for the top-left query achieved by V.+Au.+Te. and SentiBank+Au.+Te. are 0.193 and 0.247 resepctively. In contrast, our joint representation exhibits much better AP with 0.394. Finally, it is worth mentioning that using E-MDBM, each video is represented as a feature in 4,096 dimensions, in contrast to 28,965 dimensions of hand-crafted features. The compact feature representation will greatly save time and space in video retrieval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-Modal Retrieval</head><p>We further evaluate our method on cross-modal retrieval. Different from the experiment in VII-A, where the feature extracted for database videos adopts all the three modalities, cross-modal retrieval assume that query and the candidate data have different form of media types. Many applications can benefit from the cross-modal retrieval. For example, given a video without text description, the retrieved textual documents are helpful for automatically understanding the videos. Comparing to traditional semantic concept detection, which aims at recognizing some general concepts from visual appearance or auditory signals, the generated description using cross-modal retrieval would be more comprehensive.</p><p>Using the same dataset as Section VII-A, we generate four different versions of datasets, where each with one or two modalities of reference videos being purposely omitted. The four datasets are named as DB-X, with X-only modality being generated by the joint representation, where X includes T (text), V (visual), A (audio) and VA (visual-audio) modalities. During experiments, E-MDBM-X is experimented to search on a dataset without modality X. For example E-MDBM-T (text-only query) is searched against the dataset DB-V where the text and audio signals of all videos are ignored. Table <ref type="table" target="#tab_6">V</ref> summarizes the results of cross-modal retrieval for 50 queries. As can be observed, the performances are very encouraging.</p><p>For example, E-MDBM-T can achieve MAPs of 0.366 and 0.371 on DB-V and DB-A respectively when metadata of reference videos are ignored. These results are better than text-based query using word count (MAP=0.353) when the metadata are involved in similarity comparison, as shown in Table <ref type="table" target="#tab_5">IV</ref>. Using E-MDBM-V (visual-only query) against DB-T can attain MAP of 0.437, which is fairly impressive given that only the metadata of reference videos are kept for similarity measure. The result is close to SentiBank (AP=0.446), which compares similarity with reference videos directly based on visual modality.</p><p>We further detail the results for each emotion category in Table <ref type="table" target="#tab_7">VI</ref>, where every column corresponds to a different cross-modal retrieval scenario. For example, T→V+A means the use of text-based query for searching against videos with only visual and audio modalities. Interestingly, query-by-visual often exhibits better performance even though the visual modality of reference videos is not considered. In addition, visual seems to be correlated better with text than audio modality, resulting in better performance in 7 out of eight categories for V→T. The only exception is the category "Trust". This is because many videos expressing the trust emotion in this dataset are about trust test with laughing and cheering sounds.</p><p>Comparing Table <ref type="table" target="#tab_7">VI</ref>  Particularly, we propose a multi-pathway DBM architecture dealing with low-level features of various types and more than twenty-thousand dimensions, which is not previously attempted to the best of our knowledge. The major advantage of this model is on capturing the non-linear and complex correlations among different modalities in a joint space. The model enjoys peculiarities such as learning is unsupervised and can cope with samples of missing modalities.</p><p>Compared with hand-crafted features, our model generates much more compact features and allows natural cross-modal matching beyond late or early fusion. As demonstrated on ImageTweets datasets, the features generated by mapping single-modality samples (text or visual) into the joint space consistently outperform hand-crafted features in sentiment classification. In addition, we show the complementary between deep and hand-crafted features for emotion prediction on VideoEmotion dataset. Among the eight categories of emotion, nevertheless, the categories "anticipation" and "surprise" remain difficult either with learnt or hand-tuned features. For video retrieval, our model shows favorable performances, convincingly outperforms hand-crafted features over different types of queries. Encouraging results are also obtained when applying the deep features for cross-modal retrieval, which is not possible for handcrafted features. Compared to SentiBank, our model has the edge of not limiting to a predefined set of vocabularies. Hence, the learning is fully generative and the model is more expressive, as we show in the experiments that our model is able to perform better than SentiBank in both classification and retrieval tasks. Finally, our model also consistently outperforms the early version of MDBM <ref type="bibr" target="#b22">[23]</ref> in all the experiments conducted in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>The conditional distributions over different layers, using "DenseSIFT" and "Textual" pathway as example, is inferred as following )</p><formula xml:id="formula_18">v d i | h (1d) ∼ N (δ i ∑ j W (1d) ij h (1d) j + b d i , δ 2 i ) p(h (1d) j = 1 | v d , h (2d) ) = g( ∑ i W (1d) ij v d i δ i + ∑ l W (2d) jl h (2d) l ) p(h (2d) l = 1 | h (1d) , h (v) ) = g( ∑ j W (2d) jl h (1d) j + ∑ q W (vd) lq h (v) q ) p(h (v) q = 1 | h 2 V , h<label>(</label></formula><formula xml:id="formula_19">p(v t ik = 1 | h (1t) ) = exp( ∑ j h (1t) j W (1t) jk + b t k ) ∑ K q=1 exp( ∑ j h (1t) j W (1t) jq + b t k ) p(h (1t) j = 1 | v t , h (2t) ) = g( K ∑ k=1 W (1t) kj v t k + ∑ l W (2t) jl h (2t) l + N b (1t) j )</formula><p>p(h</p><formula xml:id="formula_20">(2t) l = 1 | h (1t) , h (J) ) = g( ∑ j W (2t) jl h (1t) j + ∑ p W (Jt) lp h (J) p )<label>(11)</label></formula><p>where g(x) = 1/(1 + exp(-x)) is the logistic function. When inferring the distributions, the observed modalities are clamped at the inputs and Gibbs sampling is performed for updating the states of each layer. As mentioned in Section V-B, mean-field update is adopted for state updating. Since each hidden layer is influenced by its higher and lower layers, alternating sampling is conducted for update all the necessary states to approximate the distribution.   proposed in <ref type="bibr" target="#b22">[23]</ref>. Meanwhile, SentiBank represents the classifiers trained on the scores of the concepts classifers in <ref type="bibr" target="#b5">[6]</ref>. Lexicon represents the Naive Bayes classifiers trained based on SentiStrength <ref type="bibr" target="#b42">[43]</ref>. AF represents the classifiers trained on the art features proposed in <ref type="bibr" target="#b20">[21]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Examples of emotional images and videos with their associated tags and titles from Twitter and YouTube. The left image in the first row describes the destruction caused by Hurricane Sandy and the right one describes the Manta roller coaster in the seaworld Orlando. The textual descriptions for these two images are almost the same and the implicit emotions can only be predicted by the visual information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a k-step Markov chain is initialized with the training sample. The stochastic reconstruction of the training sample from Markov chain by Gibbs sampling has a decreased free energy. Hence, this reconstruction can be approximately treated as the distribution generated by the RBM model. The contrast between</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>10,000 training examples to learn weights of a million parameters in one RBM is quite reasonable. In the proposed network, the largest RBM has 6, 300 × 2, 048 or around 1.3 millions of parameters. Using 20,000 training samples, for example, is practically feasible for learning this RBM. Since RBMs in the network are learnt in parallel, the chance of overfitting shall not be high even with only around 20,000 training samples. In our case, there are close to a million of training images and videos (see Section VI-A), and we did not observe tendency of overfitting when learning the parameters. VI. EXPERIMENT: AFFECTIVE ANALYSIS This section starts by introducing model training with unlabeled images and videos sampled from social media websites (Section VI-A). Two sets of experiments are conducted for affective analysis (Section VI-B and Section VI-C), respectively emotion prediction on YouTube videos and sentiment classification on Twitter images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>J) ) = g(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Prediction accuracies. SentiBank is the attribute feature proposed in [6]. E-MDBM-V and E-MDBM-A represent the joint representation generated by our proposed E-MDBM using only the visual or auditory signals as inputs. Similarly, E-MDBM-VA indicates the joint representation using both visual and auditory modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Prediction accuracies on the ImageTweets. E-MDBM-V and E-MDBM-T are the classifiers trained on the joint representation generated by using only the visual or textual information through E-MDBM. E-MDBM-VT is trained on the joint representations over both visual and textual modalities. MDBM represents the joint representations over both visual and textual modalities but based on the architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>The number of neurons in each layer of our enhanced multimodal DBM (E-MDBM).v, h 1 , h 2 , h(v) and h (J) represent the visible layers, first hidden layers, second hidden layers, joint representation layer over visual features and joint representation layer over visual, auditory and textual modalities.</figDesc><table><row><cell>Features</cell><cell>v</cell><cell>h 1</cell><cell>h 2</cell><cell>h (v)</cell><cell>h (J)</cell></row><row><cell>DenseSIFT</cell><cell cols="3">6,300 2,048 1,024</cell><cell></cell><cell></cell></row><row><cell>GIST</cell><cell>512</cell><cell cols="2">1,024 1,024</cell><cell></cell><cell></cell></row><row><cell>HOG2x2</cell><cell cols="3">6,300 2,048 1,024</cell><cell>2,048</cell><cell></cell></row><row><cell>LBP</cell><cell cols="3">1,239 2,048 1,024</cell><cell></cell><cell>4,096</cell></row><row><cell>SSIM</cell><cell cols="3">6,300 2,048 1,024</cell><cell></cell><cell></cell></row><row><cell>MFCC+AudioSix</cell><cell cols="3">4,000 2,048 1,024</cell><cell>-</cell><cell></cell></row><row><cell cols="4">Word Count Vector 4,314 2,048 1,024</cell><cell>-</cell><cell></cell></row><row><cell>by Gibbs sampling as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Prediction accuracies for each emotion category of VideoEmotion obtained by applying logistic regression to representations learned at different hidden layers. The highest accuracy of each category is highlighted.</figDesc><table><row><cell>Category</cell><cell>h (1d)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Prediction accuracies of the state-of-the-arts on VideoEmotion<ref type="bibr" target="#b0">[1]</ref>. The notations V., Au., and At. represent visual, auditory and attribute features respectively.</figDesc><table><row><cell>Category</cell><cell>SentiBank</cell><cell>AF</cell><cell cols="3">MDBM E-MDBM V.+Au.</cell><cell>V.+Au.</cell><cell>V.+Au.</cell><cell>V.+Au.+At.</cell><cell>V.+Au.+At.</cell><cell>V.+Au.+At.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+MDBM</cell><cell>+E-MDBM</cell><cell></cell><cell>+MDBM</cell><cell>+E-MDBM</cell></row><row><cell>Anger</cell><cell>0.345</cell><cell>0.388</cell><cell>0.294</cell><cell>0.303</cell><cell>0.549</cell><cell>0.527</cell><cell>0.515</cell><cell>0.527</cell><cell>0.509</cell><cell>0.485</cell></row><row><cell>Anticipation</cell><cell>0.082</cell><cell>0.031</cell><cell>0.067</cell><cell>0</cell><cell>0.028</cell><cell>0.067</cell><cell>0.030</cell><cell>0.067</cell><cell>0.034</cell><cell>0</cell></row><row><cell>Disgust</cell><cell>0.237</cell><cell>0.298</cell><cell>0.267</cell><cell>0.282</cell><cell>0.399</cell><cell>0.381</cell><cell>0.308</cell><cell>0.438</cell><cell>0.399</cell><cell>0.538</cell></row><row><cell>Fear</cell><cell>0.414</cell><cell>0.435</cell><cell>0.395</cell><cell>0.509</cell><cell>0.396</cell><cell>0.484</cell><cell>0.648</cell><cell>0.471</cell><cell>0.545</cell><cell>0.527</cell></row><row><cell>Joy</cell><cell>0.398</cell><cell>0.396</cell><cell>0.442</cell><cell>0.508</cell><cell>0.480</cell><cell>0.557</cell><cell>0.567</cell><cell>0.484</cell><cell>0.590</cell><cell>0.542</cell></row><row><cell>Sadness</cell><cell>0.106</cell><cell>0.063</cell><cell>0.217</cell><cell>0.265</cell><cell>0.289</cell><cell>0.274</cell><cell>0.229</cell><cell>0.208</cell><cell>0.217</cell><cell>0.324</cell></row><row><cell>Surprise</cell><cell>0.735</cell><cell>0.688</cell><cell>0.666</cell><cell>0.675</cell><cell>0.746</cell><cell>0.802</cell><cell>0.861</cell><cell>0.767</cell><cell>0.828</cell><cell>0.787</cell></row><row><cell>Trust</cell><cell>0.052</cell><cell>0.061</cell><cell>0.136</cell><cell>0.156</cell><cell>0.311</cell><cell>0.327</cell><cell>0.250</cell><cell>0.287</cell><cell>0.312</cell><cell>0.438</cell></row><row><cell>Overall</cell><cell>0.352</cell><cell>0.359</cell><cell>0.371</cell><cell>0.404</cell><cell>0.451</cell><cell>0.484</cell><cell>0.501</cell><cell>0.463</cell><cell>0.499</cell><cell>0.511</cell></row></table><note><p>the textual modality which helps to explore the semantics in the videos. Comparing to SentiBank, where the extracted semantics are limited to 1,200 predefined ANPs, our model trained on wild Web data is expected to capture more complex semantics. AF, which comprises hand-tuned features dedicated for art photos, still performs reasonably well on the web video domain. This basically gives clue to the correlation between art-based features and emotions. Interestingly, AF even performs slightly better than SentiBank that predefines the set of ANPs for emotion description. E-MDBM-V, which learns features directly from examples while considering multi-modality correlation, has better capacity in dealing with diversities in user-generated videos compared to SentiBank and AF.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Mean average precision@20 of text-based, video-based and multimodal query for retrieving emotional videos.</figDesc><table><row><cell>Category</cell><cell cols="8">Text-based Query Word Count E-MDBM-T V.+Au. SentiBank+Au. E-MDBM-VA V.+Au.+Te. SentiBank+Au.+Te. E-MDBM-VAT Video-based Query Multimodal Query</cell></row><row><cell>Anger</cell><cell>0.222</cell><cell>0.254</cell><cell>0.345</cell><cell>0.340</cell><cell>0.489</cell><cell>0.340</cell><cell>0.404</cell><cell>0.398</cell></row><row><cell>Anticipation</cell><cell>0.202</cell><cell>0.234</cell><cell>0.437</cell><cell>0.384</cell><cell>0.393</cell><cell>0.384</cell><cell>0.393</cell><cell>0.634</cell></row><row><cell>Disgust</cell><cell>0.389</cell><cell>0.411</cell><cell>0.446</cell><cell>0.492</cell><cell>0.481</cell><cell>0.496</cell><cell>0.492</cell><cell>0.465</cell></row><row><cell>Fear</cell><cell>0.417</cell><cell>0.356</cell><cell>0.423</cell><cell>0.436</cell><cell>0.487</cell><cell>0.436</cell><cell>0.507</cell><cell>0.464</cell></row><row><cell>Joy</cell><cell>0.401</cell><cell>0.521</cell><cell>0.490</cell><cell>0.528</cell><cell>0.584</cell><cell>0.530</cell><cell>0.584</cell><cell>0.673</cell></row><row><cell>Sadness</cell><cell>0.337</cell><cell>0.337</cell><cell>0.492</cell><cell>0.422</cell><cell>0.410</cell><cell>0.422</cell><cell>0.410</cell><cell>0.400</cell></row><row><cell>Surprise</cell><cell>0.508</cell><cell>0.579</cell><cell>0.554</cell><cell>0.640</cell><cell>0.585</cell><cell>0.641</cell><cell>0.588</cell><cell>0.572</cell></row><row><cell>Trust</cell><cell>0.333</cell><cell>0.333</cell><cell>0.400</cell><cell>0.443</cell><cell>0.400</cell><cell>0.443</cell><cell>0.400</cell><cell>0.467</cell></row><row><cell>Overall</cell><cell>0.353</cell><cell>0.402</cell><cell>0.450</cell><cell>0.470</cell><cell>0.489</cell><cell>0.472</cell><cell>0.492</cell><cell>0.533</cell></row><row><cell cols="8">using 23 ANPs in [6] as queries. Both videos and their surrounding metadata are downloaded from</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Cross-modal retrieval: Mean average precision @ 20 on four different types of queries against four different versions of datasets.</figDesc><table><row><cell>Modality</cell><cell cols="4">DB-T DB-V DB-A DB-VA</cell></row><row><cell>E-MDBM-T</cell><cell>-</cell><cell>0.366</cell><cell>0.371</cell><cell>0.368</cell></row><row><cell>E-MDBM-V</cell><cell>0.437</cell><cell>-</cell><cell>0.358</cell><cell>-</cell></row><row><cell>E-MDBM-A</cell><cell cols="2">0.351 0.365</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">E-MDBM-VA 0.430</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI :</head><label>VI</label><figDesc>Cross-modal retrieval: Mean average precision@20 for eight emotion categories under different scenarios. The best performance for each category is highlighted.</figDesc><table><row><cell>Category</cell><cell cols="5">T→V+A T→V T→A A→T A→V V→A V→T V+A→T</cell></row><row><cell>Anger</cell><cell>0.222</cell><cell>0.224 0.280 0.233 0.223</cell><cell>0.253</cell><cell>0.376</cell><cell>0.376</cell></row><row><cell>Anticipation</cell><cell>0.205</cell><cell>0.200 0.218 0.200 0.200</cell><cell>0.202</cell><cell>0.246</cell><cell>0.200</cell></row><row><cell>Disgust</cell><cell>0.383</cell><cell>0.384 0.394 0.390 0.383</cell><cell>0.385</cell><cell>0.475</cell><cell>0.473</cell></row><row><cell>Fear</cell><cell>0.374</cell><cell>0.356 0.473 0.401 0.356</cell><cell>0.345</cell><cell>0.667</cell><cell>0.656</cell></row><row><cell>Joy</cell><cell>0.475</cell><cell>0.472 0.411 0.386 0.471</cell><cell cols="2">0.394 0.456</cell><cell>0.450</cell></row><row><cell>Sadness</cell><cell>0.333</cell><cell>0.333 0.333 0.333 0.333</cell><cell>0.337</cell><cell>0.380</cell><cell>0.372</cell></row><row><cell>Surprise</cell><cell>0.500</cell><cell>0.500 0.504 0.519 0.500</cell><cell cols="2">0.504 0.560</cell><cell>0.566</cell></row><row><cell>Trust</cell><cell>0.333</cell><cell>0.333 0.333 0.333 0.333</cell><cell cols="2">0.444 0.333</cell><cell>0.333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>with TableIV, which can be considered as the up-bound of cross model retrieval, there is still a performance gap. However, considering that we are matching the data from different modalities with different statistic properties, basically the results in Table V can demonstrate the ability of our method for modeling the joint representations.</figDesc><table /><note><p><p>VIII. CONCLUSION AND FUTURE WORK</p>We have presented a deep model for learning multimodal signals coupled with emotions and semantics.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>September 23, 2015   DRAFT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTION ON MULTIMEDIA, VOL. ,NO. , 2014  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>List of emotion words: cold, broken, fantastic, curious, classic, fat, crazy, lazy, creepy, haunted, clear, bright, natural, heavy, dirty, adorable, shiny, tasty</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>List of concept: morning, chair, fence, architecture, bird, spider, cat, tree, castle, moon, spring, rain, dog, star, food, body</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work described in this paper was supported by two grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (CityU 11210514, CityU 120213), and a grant from the National Hi-Tech Research and Development Program (863 Program) of China under Grant 2014AA015102. The Tesla K40 used for this research was</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting emotions in user-generated videos</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentiment analysis of twitter data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vovsha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Languages in Social Media</title>
		<meeting>the Workshop on Languages in Social Media</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Workshop of Making Sense of Microposts co-located with WWW</title>
		<author>
			<persName><forename type="first">H</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note>Alleviating data sparsity for twitter sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kouloumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
		<title level="m">Twitter sentiment analysis: The good the bad and the omg!&quot; in ICWSM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining strengths, emotions and polarities for boosting twitter sentiment analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poblete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
		<meeting>the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1410.8586</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting viewer perceived emotions in animated gifs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust image sentiment analysis using progressively trained and domain transferred deep networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical affective content analysis in arousal and valence dimensions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="2140" to="2150" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Determination of emotional content of video clips by low-level audiovisual features</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Affective understanding in film</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A large video database for computational models of induced emotion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Baveye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Bettinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dellandreá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chamaret</surname></persName>
		</author>
		<editor>ACII</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with correspondence autoencoder</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aggregate features and adaboost for music classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="473" to="484" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ranking-based emotion recognition for music organization and retrieval</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="762" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning features from music audio with deep belief networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Society for Music Information Retrieval Conference (ISMIR)</title>
		<meeting>the 11th International Society for Music Information Retrieval Conference (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring principles-of-art features for image emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online reranking via ordinal informative concepts for context fusion in concept detection and video search</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1880" to="1890" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2949" to="2980" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient object category recognition using classemes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Measuring invariances in deep networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition 2007 (CVPR&apos;07)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Yaafe, an easy to use and efficient audio feature extraction software</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Essid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<editor>ISMIR</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines using approximations to the likelihood gradient</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using Fast Weights to Improve Persistent Contrastive Divergence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on Machine learning</title>
		<meeting>the 26th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P14/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sentiment strength detection in short informal text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2544" to="2558" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
