<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Nonstationarity for Performance Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Stewart</surname></persName>
							<email>stewart@cs.rochester.edu</email>
							<affiliation key="aff0">
								<orgName type="department">U. Rochester CS Dept</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Terence</forename><surname>Kelly</surname></persName>
							<email>kterence@hpl.hp.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Hewlett-Packard Labs</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Zhang</surname></persName>
							<email>alex.zhang@hp.com</email>
							<affiliation key="aff2">
								<address>
									<settlement>Hewlett-Packard Labs</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Nonstationarity for Performance Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6D7E5F4299A648E217FC253CCCF4CF83</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Experimentation</term>
					<term>Management</term>
					<term>Measurement</term>
					<term>Performance enterprise</term>
					<term>internet services</term>
					<term>LAR regression</term>
					<term>mutli-tier</term>
					<term>noninvasive</term>
					<term>nonstationarity</term>
					<term>performance prediction</term>
					<term>realistic workloads</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real production applications ranging from enterprise applications to large e-commerce sites share a crucial but seldom-noted characteristic: The relative frequencies of transaction types in their workloads are nonstationary, i.e., the transaction mix changes over time.</p><p>Accurately predicting application-level performance in businesscritical production applications is an increasingly important problem. However, transaction mix nonstationarity casts doubt on the practical usefulness of prediction methods that ignore this phenomenon.</p><p>This paper demonstrates that transaction mix nonstationarity enables a new approach to predicting application-level performance as a function of transaction mix. We exploit nonstationarity to circumvent the need for invasive instrumentation and controlled benchmarking during model calibration; our approach relies solely on lightweight passive measurements that are routinely collected in today's production environments. We evaluate predictive accuracy on two real business-critical production applications. The accuracy of our response time predictions ranges from 10% to 16% on these applications, and our models generalize well to workloads very different from those used for calibration.</p><p>We apply our technique to the challenging problem of predicting the impact of application consolidation on transaction response times. We calibrate models of two testbed applications running on dedicated machines, then use the models to predict their performance when they run together on a shared machine and serve very different workloads. Our predictions are accurate to within 4% to 14%. Existing approaches to consolidation decision support predict post-consolidation resource utilizations. Our method allows application-level performance to guide consolidation decisions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Modern distributed applications continue to grow in scale and complexity. Distributed enterprise applications are furthermore assuming a growing role in business-critical operations. Understanding the performance of such applications is consequently increasingly difficult yet increasingly important due to their economic value. This paper considers the problem of performance prediction in distributed applications: Given forecasts of future application workload, we seek to predict application-level response times. A good solution to this problem will enable operators to explore a wide range of important "what-if" scenarios, e.g., "How will response times change if the the number visitors at my Web site doubles and the buy:browse ratio increases by 50%?" We do not address the complementary problem of workload forecasting, but we show that if accurate workload forecasts are available they can be mapped directly to accurate performance predictions.</p><p>The workloads of the real production applications that we seek to model share a crucial but seldom-noted characteristic: the transaction mixes in these workloads are highly nonstationary in the sense that the relative frequencies of transaction types vary considerably over time. This is a problem for most conventional performance models, which implicitly assume that transaction mix is stationary, because the system resource demands of different transaction types are usually very different in real applications.</p><p>Our approach leverages earlier work that focused on retrospectively explaining performance in terms of transaction mix <ref type="bibr" target="#b23">[23]</ref>. We incorporate queueing-theoretic extensions into the earlier technique to obtain a method suitable for prospectively predicting future performance as a function of transaction mix. One novel feature of our approach is that whereas performance models in prior systems literature include a scalar measure of workload intensity, we describe workload using a transaction-mix vector. Another novel feature is that we exploit transaction mix nonstationarity to circumvent the need for invasive instrumentation and controlled benchmarking during model calibration.</p><p>Our approach is practical for real production systems and can be applied to a wide range of applications. Our models are calibrated using purely passive measurements that are routinely collected in today's real production applications. Furthermore, they work well under a wide range of workload conditions and a wide variety of application architectures, including locally distributed multi-tier E-commerce applications and globally-distributed highavailability enterprise applications.</p><p>We compare our proposed method with several alternatives, evaluating their ability to predict response times in two very different real production applications: the Web shopping site of a major retailer and a business-critical internal enterprise application. Our method accurately predicts response times for both applications.</p><p>Furthermore our performance models generalize well to regions of workload space very different from those present in the calibration data. We demonstrate that transaction mix models achieve substantially greater accuracy than similar models that employ scalar measures of workload intensity.</p><p>Finally, we apply our method to the challenging problem of predicting response times in applications that are consolidated onto a shared infrastructure, subject to a severe handicap: we must calibrate our models using only lightweight passive observations of the applications running on dedicated machines prior to consolidation. We evaluate our performance predictions in consolidated environments using a testbed of benchmark applications, since real production applications were unavailable for experimentation. Our predictions are remarkably accurate according to two measures that penalize inaccuracy in very different ways. The current state of the art in consolidation decision support both in practice and in the research literature predicts the resource utilization effects of consolidation. We present a practical way to incorporate application-level performance into consolidation decision-making.</p><p>The remainder of this paper is organized as follows: Section 2 describes the prevalence of transaction mix nonstationarity in realworld workloads, the problems it poses for many conventional performance models, and the opportunities it creates that we exploit. Section 3 presents our approach to performance prediction, defines our main accuracy measure, and describes an accuracy-maximizing model calibration procedure. Section 4 describes the applications used in our tests and presents empirical results on the accuracy of our predictions. Section 5 applies our models to the challenging problem of predicting the performance of applications that are consolidated onto a shared infrastructure. Section 6 reviews related work, and Section 7 concludes with a discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TRANSACTION MIX NONSTATIONAR-ITY IN REAL WORKLOADS</head><p>It is well known that the volume of demand in production applications naturally fluctuates on several time scales (e.g., daily and weekly cycles). Similarly, there is little reason for the transaction mix of real applications to remain constant over time. In this section, we describe transaction mix nonstationarity in two real production applications (Section 4.1 describes the applications themselves in detail). An investigation into the factors that influence nonstationarity in real applications is orthogonal to our goal of performance prediction, so we leave it for future work.</p><p>Figures <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_1">2</ref> illustrate time variations in transaction mix. The first is a scatterplot of the relative frequencies of the two most common transaction types of the "VDR" application in 5-minute time windows. Note that nearly every possible combination is present (the upper right corner of the plot must be empty because the sum of the two fractions cannot exceed 1). Figure <ref type="figure" target="#fig_1">2</ref> is a time series of the fraction of "ACME" transactions that are of type "add-tocart" in 5-minute windows. It shows that this fraction varies over two orders of magnitude during a four-day period (note that the vertical scale is logarithmic). The transaction mix nonstationarity evident in these figures is not an artifact of 5-minute time windows; it remains when we aggregate measurements into much longer intervals. Figure <ref type="figure" target="#fig_3">4</ref> shows the fraction of VDR transactions due to the most common transaction type in hour-long windows over a period of several days; the fraction ranges from less than 5% to over 50%. Plots using longer aggregation intervals are qualitatively similar.</p><p>One implication of transaction mix nonstationarity is that the full spectrum of workloads for which we must predict performance may not be available during model calibration. Performance models must therefore generalize well to workloads that are very different from those used for calibration. Furthermore, a convincing validation of a performance prediction method requires nonstationary workloads, because stationary workloads differ qualitatively from real-world workloads.</p><p>Synthetic workload generators used in benchmarking and systems research typically employ first-order Markov models to determine the sequence of transactions submitted by client emulators; examples include the standard TPC-W workload generator <ref type="bibr" target="#b46">[47]</ref> and the RUBiS workload generator <ref type="bibr">[38]</ref>. This approach cannot yield the kind of markedly nonstationary workloads that we observe in real production applications, because the long-term relative state occupancy probabilities of first-order Markov processes are stationary <ref type="bibr" target="#b42">[43]</ref>. Figure <ref type="figure" target="#fig_4">5</ref> shows the relative fractions of the two most common transaction types in the workload generated by the default RU-BiS generator during a 5-hour run, in 5-minute windows. Nearly all of the 60+ data points lie on top of one another. Plots of different transaction type pairs aggregated into different time windows are qualitatively similar.</p><p>What are the implications of nonstationarity for performance modeling? We define a scalar performance model as one that ignores transaction mix in workload and instead considers only a scalar measure of workload intensity, e.g., arrival rate. Nonstationarity clearly poses serious problems for scalar performance models. For example, consider an application whose workload consists of equal numbers of two transaction types: type A, which places heavy demands on system resources, and type B, which has light demands. Suppose that we want to predict the application's performance if the total number of transactions increases by 50%. Scalar models may work well if the relative proportion of the two transaction types remains equal. However such models are unlikely to yield accurate predictions if the transaction mix changes: Performance will differ dramatically if the number of type-A transactions doubles and the number of type-B remains constant, or vice versa. Of course, evaluations of scalar performance models using firstorder Markov workload generators will not expose this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stationary test workloads mask the deficiencies of scalar performance models.</head><p>This paper employs transaction mix models that predict applicationlevel performance based on transaction counts by type. These models have a number of attractive features: they are "semantically clear" in the sense that their free parameters have intuitive interpretations; they yield accurate performance predictions under a wide range of circumstances; and the computational procedures used for model calibration are fairly straightforward. However it is nonstationarity that makes our approach particularly practical, because nonstationarity allows us to calibrate our models using only lightweight passive measurements that are collected in today's real production environments. We describe the opportunities that nonstationarity creates for calibration in greater detail in Section 3.5, after we describe our performance models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TRANSACTION MIX MODELS</head><p>This section describes our transaction mix performance models and several variants and alternatives against which we shall later compare them. All models have the same general high-level form:</p><formula xml:id="formula_0">P = F a ( W )<label>(1)</label></formula><p>where P is a scalar summary of application performance, F specifies the functional form of our models, a is a vector of calibrated parameter values, and W is a vector of workload characteristics. This section explains the development of our approach. Section 3.1 justifies our basic assumptions in terms of the measured properties     of real applications. Section 3.2 presents our performance models. Section 3.4 defines the accuracy measure that we seek to optimize, and Section 3.5 explains how we calibrate our models to maximize accuracy according to this measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Assumptions</head><p>We begin with the following observations about modern distributed applications:</p><p>1. Workload consists of request-reply transactions. 2. Transactions occur in a small number of types (e.g., "log in," "browse," "add-to-cart," "checkout" for an E-commerce site). 3. Transaction types strongly influence system resource demands (e.g., "checkout" transactions at an E-commerce site require more CPU than browsing). 4. Resources are adequately provisioned or over-provisioned in business-critical production applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Transaction mix is nonstationary.</head><p>The first two observations apply to every commercially-important distributed production application that we have encountered. The third property arises because transaction types often determine the run-time code path through application logic, which in turn strongly influences resource service demands. The fourth property, adequate resource provisioning, is a fundamental requirement of capacity planning in business-critical applications. By design, allocated capacity is generous relative to offered workload; heavy load and overload represent serious failures of the capacity planning process. Fortunately such failures are rare because capacity planning for intra-enterprise applications can often exploit good estimates of the total user population and anticipated usage patterns.</p><p>Even in server-consolidation scenarios where elevating resource utilization is an explicit goal, practitioners are advised to keep peak utilizations of resources such as CPU below 70% <ref type="bibr" target="#b14">[14]</ref>. In practice, enterprise system operators are typically even more cautious than this conservative guideline. Figure <ref type="figure" target="#fig_2">3</ref> shows cumulative distributions of resource utilizations encountered by arriving transactions in the two distributed production applications used in our investigation, "ACME" and "VDR." Transactions arriving at these two very different applications operated by two different firms rarely find utilization at any resource in excess of 35%; utilization greater than 50% is almost never encountered. This implies that while queueing times at resources such as CPUs and disks should not be ignored, service times will often account for much of overall transaction response times. Another implication is that non-queueing congestion effects associated with very heavy load, e.g., cache interference, are likely to be rare in practice.</p><p>Together with our first three assumptions, Figure <ref type="figure" target="#fig_2">3</ref> suggests a radically simple performance model that accounts for transaction service times but ignores queueing entirely. Our previous work reports that such a model works surprisingly well in practice. Specifically, the sum of response times across all transactions within a specified time interval is well explained by transaction mix alone <ref type="bibr" target="#b23">[23]</ref>. However, Figure <ref type="figure" target="#fig_2">3</ref> also suggests that waiting times are sometimes non-negligible, and our approach in this paper models waiting times.</p><p>In summary, we observe that transaction mix alone is a powerful performance predictor; it will be the W of Equation 1. We have also seen that queueing can be non-negligible, and our models will explicitly account for waiting times in addition to service times. Our performance measure P will be aggregate transaction response time within short time windows (e.g., 5-minute intervals); this can easily be converted to average response time because we know the number of transactions within each window. After specifying the form of our models F and defining our measures of model accuracy, we describe how to obtain accuracy-maximizing parameters a by exploiting naturally-occurring nonstationarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models</head><p>This section develops a series of three performance models of increasing sophistication and breadth of applicability. The Basic model of Section 3.2.1 takes into account transaction mix alone; it is taken from previous work <ref type="bibr" target="#b23">[23]</ref>. Section 3.2.2 extends the Basic model to explicitly incorporate queueing delays. The Extended model does not conform to the template of Equation 1, however, because its inputs include resource utilizations as well as transaction mix. While the Extended model may offer improved accuracy when used to retrospectively explain performance, it cannot be used to predict performance given workload forecasts alone. The Composite model of Section 3.2.3 corrects this deficiency by modeling resource utilizations in terms of transaction mix and incorporating the utilizations thus obtained into the Extended model. Finally, our empirical evaluations will include variants of the Basic, Extended, and Composite models that use only a scalar measure of workload intensity rather than a vector describing transaction mix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Basic Model</head><p>We divide time into short non-overlapping intervals, e.g., 5 minutes. For interval i let N i j denote the number of transactions of type j that began during the interval and let T i j denote the sum of their response times. Our Basic model has the form</p><formula xml:id="formula_1">y i = ∑ j T i j = ∑ j α j N i j (2)</formula><p>where y i is the sum of all transaction response times during interval i. Note that no intercept term is present in Equation <ref type="formula">2</ref>, i.e., we constrain the model to pass through the origin: Aggregate response time must be zero for intervals with no transactions. Values of model parameters α j are obtained through model calibration; let a j denote these calibrated values. Intuitively, calibrated parameters a j represent typical service times for the various transaction types, summed over all service and delay centers on the transaction's execution path.</p><p>For given model parameters a j and observed transaction mix N i j at time i, let ŷi = F a ( N i ) = ∑ j a j N i j <ref type="bibr" target="#b3">(3)</ref> denote the fitted value of the model at time i. If the N i j represent past workload, ŷi can be interpreted as the model's guess of what aggregate response time should have been during interval i.</p><p>If instead the given transaction mix is a forecast of future workload, the fitted value represents the model's performance prediction. Note that since the total number of transactions within an interval is known-it is simply ∑ j N i j -one can convert a fitted value ŷi representing aggregate response time into an average response time.</p><p>Our Basic model can be thought of as an open queueing network containing a single service station with an infinite number of servers. Waiting cannot occur in such a system, and Equation <ref type="formula">2</ref>does not explicitly model waiting times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Extended Model</head><p>We extend the Basic model of Equation 2 by adding terms representing waiting times, as follows:</p><formula xml:id="formula_2">y i = n ∑ j=1 α j N i j + ∑ r 1 λ i • U 2 ir 1 -U ir • n ∑ j=1 N i j .<label>(4)</label></formula><p>The rightmost term represents waiting times in an M/M/1 queue, with one queue per resource; U ir denotes the utilization of resource r during interval i. The naïve approach of adding utilizations as simple linear terms has no basis in queueing theory, but we shall compare our approach with this alternative (see the discussion of  To derive Equation <ref type="formula" target="#formula_2">4</ref>, we note that the term 1</p><formula xml:id="formula_3">λ i • U 2 ir</formula><p>1-U ir represents the average waiting time (per transaction at resource r over interval i) in an M/M/1 queueing model, where λ i is the arrival rate of transactions of all types in interval i. We multiply this term by the number of transactions of all types in interval i to obtain the sum of waiting times, to agree with the left-hand-side of the equation. Realizing that λ i = ∑ n j=1 N i j /L where L is the interval length in seconds, one can further simplify the sum-of-waiting-time term</p><formula xml:id="formula_4">to ∑ r L • U 2 ir 1-U ir .</formula><p>Finally, since the sum-of-waiting-time term on the right-hand-side of Equation <ref type="formula" target="#formula_2">4</ref>does not involve any unknown parameters α j , one can regress z i ≡ y i -∑ r L • U 2 ir 1-U ir against the transaction mix ∑ n j=1 α j N i j . Figure <ref type="figure" target="#fig_5">6</ref> depicts our Extended model as an open queueing network consisting of a single-server station for each resource (e.g., CPU, disk, network) at each tier (e.g., application server, database server). Although the figure shows only two tiers with three resources each, the model can accommodate additional tiers (e.g., for a Web server) and additional resources. The Extended model captures the distributed aspect of an application by explicitly including network queueing effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Composite Model</head><p>Because it relies on resource utilizations, the Extended model of Equation 4 cannot be used to predict performance based on transaction mix N i j alone. Our Composite model overcomes this difficulty by estimating utilizations as weighted sums of transaction counts:</p><formula xml:id="formula_5">U ir = β 0r + ∑ j β jr N i j (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where β jr represents the service demand of transaction type j on resource r. The total service demand placed on the resource is the sum of service demands of all transaction types. As with the Basic response time model of Section 3.2.1, we obtain for each resource r parameters b jr corresponding to the β jr during model calibration. Unlike the model of Equation <ref type="formula">2</ref>, however, we include an intercept term β 0r in our utilization models, because real system resources are not entirely idle even in the complete absence of application workload. Equation 5 generalizes the familiar Utilization Law; specifically, it reduces to the Utilization Law in the special case of one transaction type and no intercept term. Once we have obtained utilization estimates Ûir from a calibrated utilization model (Equation <ref type="formula" target="#formula_5">5</ref>), we substitute these into a calibrated Extended model to obtain a Composite model of aggregate response time as a function of transaction mix N i j . In rare cases where Ûir &lt; 0 or Ûir ≥ 1, we correct the utilization estimate to zero or 1 -ε, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Scalar Models</head><p>Recent queueing models of distributed application performance rely on a scalar measure of workload intensity that ignores transaction types <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref>. What additional predictive power do we obtain by using a transaction-mix vector? To address this question, our empirical evaluations will compare our models with Scalar variants that use only the total number of transactions in each time interval. For example, the Scalar variant of the Basic model is</p><formula xml:id="formula_7">y i = ∑ j T i j = αN i (6)</formula><p>where N i = ∑ j N i j is the total number of transactions that occurred during time interval i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Our approach contains a number of simplifications that deserve mention. We account for waiting times at each resource using an expression for a single-server queue, whereas many real production applications run on systems with multiple CPUs and disks at each tier. Previous work made similar simplifying assumptions <ref type="bibr" target="#b40">[41]</ref>, and we find that in practice this approach works well. Our model assumes an open network in which requests exit after service. A closed network model would require us to model client "think times," as in some previous models of distributed applications <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Our queueing networks implicitly assume that transactions do not recirculate among resources; our models aggregate all service times from all of a transaction's visits to a resource, rather than explicitly modeling visits separately. More sophisticated queueing models take into account recirculation among service stations <ref type="bibr" target="#b20">[20]</ref>, but such models require detailed information about how transactions move among resources, which is often not available in practice. Tools to gather this information exist as research prototypes, e.g., Magpie <ref type="bibr" target="#b7">[7]</ref>, but few real production systems are currently instrumented to measure fine-grained transaction resource visits. We have designed our models to require for calibration only data that is routinely collected on today's real production applications.</p><p>Our use of open queuing network models is in part motivated by practical considerations: It is often difficult in practice to obtain for real production applications the client session information required to calibrate closed models, and therefore the use of an open model facilitates more thorough empirical validation than would be possible if we employed a closed model. However there are good reasons for preferring open models for their own sake, e.g., they are relatively simple. More importantly, open models are more appropriate if transient overloads are possible because open models do not inherently restrict the number of concurrent transactions in a system. See Schroeder et al. for a detailed discussion of the implications of open versus closed models <ref type="bibr" target="#b39">[40]</ref>.</p><p>For all of the production and testbed applications considered in this paper, transaction types are given-they can easily be inferred by, e.g., inspecting the request URL. In our experience such transaction type information is sufficient for our method and is readily available in practice. Our approach remains applicable if transaction types are not given as long as a classifier can be constructed that maps transactions to types that reflect their system resource service demands. Transaction type classification is an orthogonal problem to our modeling interests and has been the subject of extensive research (see Section 6.1).</p><p>We have implicitly assumed that an application's set of transaction types is fixed and the relationship between transaction type and resource demands is stable. This is not a restrictive assumption in practice because the time required to re-calibrate new performance models is short compared to the time scales on which application logic and transaction structure changes. In our experience with real production applications in the enterprise, changes to application structure and configuration are normally rare; stakeholders in business-critical applications do not undertake such modifications lightly or frequently. Even if transaction types or their resource demands change completely, it takes only a day or two to gather suf-ficient data to calibrate completely new performance models. Less drastic changes are easier to handle: Model calibration itself takes less than a second, and therefore continuous re-calibration (e.g., at the conclusion of every 5-minute measurement interval) can be used to track gradual drift in the workload/performance relationship.</p><p>A final simplification is that our models ignore interaction effects across transaction types and implicitly assume that queueing is the only manifestation of congestion. However queueing does not describe certain kinds of resource contention, e.g., cache interference. "Checkout" transactions, for instance, may require more CPU service time during heavy browsing if the latter reduces processor cache hit rates for the former. Our models do not account for such effects. The question of whether our simplifying assumptions are oversimplifications is ultimately an empirical one, which we address in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Accuracy Measures</head><p>If y i is the actual measured aggregate response time during interval i and ŷi is the fitted value obtained from a calibrated performance model, let e i = y iŷi denote the residual (model error) at time i. We measure model accuracy in terms of intuitive functions of the residuals. We cannot use the conventional coefficient of multiple determination R 2 to assess model accuracy; it is not meaningful because Equation 2 and Equation 4 lack intercept terms <ref type="bibr">[33, p. 163]</ref>.</p><p>Our overall figure of merit, normalized aggregate error, generalizes the familiar, intuitive concept of absolute percent error:</p><formula xml:id="formula_8">normalized aggregate error ≡ ∑ i |e i | ∑ i y i (7)</formula><p>Consider, for example, a single (y, ŷ) pair: if y = 100 and ŷ = 105, then normalized aggregate error is 0.05, indicating that the model's prediction is off by 5%. We say that model parameters for Equation 2 or Equation 4 are optimal if they minimize error as defined by Equation <ref type="formula">7</ref>.</p><p>In addition to our overall figure of merit we shall also report the distribution of normalized residuals |e i |/y i , scatterplots of (y, ŷ) pairs, and order statistics on the normalized residuals. Each of these measures offers different insight into model accuracy and penalizes inaccuracy in a different way. For example, the distribution of |e i |/y i penalizes even small residuals if the corresponding measurements are small, whereas Equation 7 penalizes residuals whose magnitude is large even if |e i | is small relative to the corresponding y i . A good model is accurate according to both measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Calibration</head><p>The input to calibration is a data set consisting of aggregate response times y i and transaction mixes N i = (N i1 , N i2 , . . .). For our Extended and Composite models we furthermore require resource utilizations U ir . These inputs correspond to readily available and purely passive measurements of applications and their underlying system resources. A good rule of thumb is that model calibration requires roughly ten times as many measurement intervals i as transaction types <ref type="bibr" target="#b33">[33]</ref>. If measurements are taken at 5-minute intervals, a few days suffice to collect enough data to calibrate our models of the production applications that we study.</p><p>The output of calibration is a set of calibrated parameter values a j corresponding to the α j parameters of the Basic and Extended models and, for a Composite model, calibrated parameter values b jr corresponding to the β j parameters of the utilization model (Equation <ref type="formula" target="#formula_5">5</ref>).</p><p>The goal of calibration is to compute parameters that maximize model accuracy. The denominator in Equation 7 is a constant, so to achieve optimal accuracy a calibrated model must minimize the numerator, i.e., the sum of absolute residuals. This is a special case of linear programming, for which specialized variants of the simplex algorithm have been developed; we use the algorithm of Barrodale &amp; Roberts <ref type="bibr">[8]</ref>. The algorithm yields model parameters that optimize retrospective explanatory accuracy with respect to the data used for calibration. This exercise is sometimes known as least absolute residuals (LAR) regression. Ordinary least squares (OLS) regression minimizes the sum of squared residuals, and it can be shown that a model with OLS parameters can have arbitrarily worse accuracy than an optimal model according to the measure of Equation <ref type="formula">7</ref>. In practice we find that LAR-calibrated models are more accurate than their OLS-calibrated counterparts.</p><p>Another advantage of LAR is that it is robust, i.e., it resists the influence of extreme values in the calibration data set. By contrast, OLS is far more sensitive to distortion by outliers. A wide variety of robust regression procedures are available; several are variants of OLS and LAR <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b51">52]</ref>. We prefer plain-vanilla LAR because it guarantees optimal retrospective accuracy, because it is conceptually simple and easy to explain, and because it involves no tunable parameters. The only disadvantage of LAR is that numerical solvers are not as widely available. However, as reported previously, the accuracy gain over OLS outweighs the inconvenience of LAR <ref type="bibr" target="#b23">[23]</ref>. A final advantage of using linear programming for model calibration is that it is easy to add additional constraints, e.g., on the values of parameters. Extensions of elementary statistical techniques such as least-squares regression can sometimes achieve similar capabilities, but in our experience they do not offer the generality, convenience, and flexibility of linear programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">The Role of Nonstationarity</head><p>Model calibration in our approach exploits variations in transaction volume, transaction mix, and resource utilization in the calibration data-i.e., the kind of nonstationarity found in real workloads. At the other extreme, it is easy to show that lightweight passive measurements of response times and utilizations collected under perfectly stationary workload cannot be used for model calibration. The essential difficulty is that the optimization problem that the regression procedure seeks to solve lacks a unique solution. Typical implementations of OLS regression, for example, fail in such cases because they attempt to invert a singular (non-invertible) matrix.</p><p>A simple example conveys intuition for the insurmountable problems created by perfectly stationary transaction mix. Consider the following utilization and transaction mix measurements: These data cannot be used to calibrate a utilization model (Equation 5) regardless of the calibration procedure used, and regardless of the utilization measurements u i . The problem is that the transaction count information is essentially the same in each row because the counts in rows 2 through 4 are multiples of those in row 1 and the A:B ratio is everywhere the same-i.e., the transaction mix is stationary. This makes it impossible to determine whether A is a heavyweight transaction and B is lightweight or vice versa. The situation is the same if the data include aggregate response times rather than utilization measurements and our goal is to calibrate a Basic model (Equation <ref type="formula">2</ref>) or an Extended model (Equation <ref type="formula" target="#formula_2">4</ref>).</p><p>If a first-order Markov model generates workload for calibration and if measurement intervals contain a reasonably large number of requests, the result will almost certainly be nearly-stationary workload. This in turn causes multicollinearity, a regression pathology that arises when predictor variables are mutually correlated. The net effect is that predictive accuracy will suffer regardless of the regression procedure used. On the other hand, our empirical results show that naturally-occurring workloads have sufficient transaction mix nonstationarity to allow us to calibrate very accurate models using passive measurements of utilizations and response times.</p><p>In summary, the nonstationarity of real workloads makes possible our lightweight model calibration approach, which relies on passive measurements and requires no invasive system or application instrumentation or controlled benchmarking. Nonstationarity allows model calibration to substitute data analysis for invasive measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VALIDATION</head><p>We calibrate our models and evaluate their retrospective explanatory accuracy on the first half of each validation data set. We then apply the calibrated models to the transaction mix in each time interval i of the second half to obtain fitted values ŷi . Finally, we compare these ŷi with observed y i to evaluate prospective prediction accuracy. This section first describes the two real production data sets used in our evaluation and an additional data set collected in a lab environment, then presents our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>Table <ref type="table" target="#tab_2">1</ref> summarizes our data sets. The first two were collected on distributed production applications serving real customers and real enterprise users, respectively. Together these data sets severely challenge the ability of our method to generalize along several dimensions. They differ in terms of the nature of the application, the extent of geographic distribution, and workload. Their workloads exhibit the nonstationarities described in Section 2. Due to space limitations we provide an abbreviated description of the data sets here; additional information on all three data sets is available in <ref type="bibr" target="#b24">[24]</ref>.</p><p>Pronounced workload periodicity is present in all of our data sets. Referring to the light data series and right-hand vertical axis in Figure <ref type="figure" target="#fig_3">4</ref>, we see that workload ranges from under 100 to roughly 7,300 transactions per hour. The ACME data set shows a similar daily cycle with comparably wide variation. Other periodicities and nonstationarities are present in all of our data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">ACME: DotCom-Era Web Shopping</head><p>The "ACME" data set is taken from the busiest of seven servers comprising a major US retailer's Web shopping site. This server accounts for roughly 38% of all ACME transactions during the measurement period. ACME was typical of large E-commerce sites circa 2000. For confidentiality reasons, the researchers who studied the ACME site were not permitted to disclose the details of its hardware and software infrastructure. However, an extensive workload characterization is available <ref type="bibr" target="#b3">[3]</ref>. The ACME data set includes measurements of transaction response times and system resource utilizations collected at the application server tier; it does not include requests for static Web pages. Each transaction is furthermore tagged as a cache hit or a miss, and we treat hits and misses of the same transaction type as two different types.  All of the app servers ran BEA WebLogic. We have less detailed information about hosts at the database tier, but we know that they are similar in number and specifications to those at the app server tier and that they ran Oracle 9i. VDR operators and system architects have told us that VDR transactions are relatively "heavyweight" in the sense that they place substantial demands on system resources. The VDR data set includes both transaction records and system resource utilization measurements collected at both application server and database server tiers, derived respectively from application-level logs and Open-View Performance Agent (OVPA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">VDR: Modern Enterprise Application</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">PetStore: Sample Application</head><p>The PetStore data set was collected on a testbed application serving a highly variable and highly nonstationary workload. The total request rate and the ratios of transaction type frequencies vary sinusoidally, and the total rate increases over time so that the peaks represent transient overloads ("flash crowds") during which offered workload exceeds the system capacity. Our models implicitly assume that load does not exceed system capacity, so this data set provides an extraordinary challenge to our approach. The PetStore data set and the environment in which it was collected are described in detail in earlier publications <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b24">24]</ref>; due to space limitations we do not repeat these descriptions here. The PetStore data set was generated long before the present investigation began and therefore was not (consciously or otherwise) tailored to the methodology proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We calibrate our models on the first half of each data set, and also measure their retrospective explanatory accuracy on the first half. We then evaluate the predictive accuracy of the calibrated models using the transaction mixes in the second half of each data set. Transaction mix nonstationarity therefore implies that predic- tive evaluations will involve workloads different than those used in calibration. Nonstationarities provide a challenging and credible evaluation of the extent to which models generalize beyond the calibration data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Retrospective Explanation</head><p>Table <ref type="table" target="#tab_3">2</ref> summarizes the explanatory accuracy of eight models on our three data sets according to our overall figure of merit, normalized aggregate error (Equation <ref type="formula">7</ref>). The table includes both the standard transaction mix model (TMIX) versions of our Basic and Extended models as well as Scalar variants that ignore transaction types and use only the total number of transactions. Both OLS and LAR regression are used to calibrate each model variant.</p><p>First, the transaction mix models consistently outperform their scalar counterparts by a wide margin for the production data sets: 15% vs. 10% error for VDR and 20% vs. 13% error for ACME regardless of the calibration procedure. Even for the deliberately overloaded PetStore, which violates flow balance, the Extended transaction mix model calibrated with LAR has only a 22% error. We achieve substantially greater accuracy by exploiting transaction mix.</p><p>Second, the Extended model, which accounts for queueing, outperforms the Basic model, which does not. Our Basic model achieves nearly the same accuracy for both production applications as the Extended model. This is what we expect because queueing is deliberately minimized in production applications (see Figure <ref type="figure" target="#fig_2">3</ref> and the discussion in Section 3.1). For the heavily-loaded PetStore data, in which queueing is a first-order effect, the Extended model performs substantially better than the Basic model (22% vs. 32% when LAR is used).</p><p>Third, normalized aggregate error is lower when LAR regression is used. Outliers (extreme data values) are present in both data sets and are particularly numerous and large in the PetStore data. LAR is a robust estimation procedure and is less sensitive to outliers than OLS, and therefore yields more accurate models according to our accuracy measure.</p><p>We also evaluated model variants that include an intercept term in Equations 2 and 4. Such models are "wrong" from a queueingtheoretic perspective because they imply nonzero aggregate response times even when no transactions occur. However an intercept can increase retrospective accuracy but cannot reduce it, so we might be tempted to include one if we are willing to trade "sanity" for accuracy. We found that the benefits of including an intercept are very limited, and that the principled models with no intercept are nearly as accurate.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Application: Performance Anomaly Detection</head><p>Our previous work explains how accurate retrospective explanatory performance models can be useful <ref type="bibr" target="#b23">[23]</ref>. The most obvious application is performance anomaly detection, i.e., identifying when performance is surprising, given workload. Knowing whether workload explains performance can guide our choice of diagnostic tools: Ordinary overload might recommend bottleneck analysis, whereas degraded performance not explained by workload might suggest a fault in application logic or configuration.</p><p>We have shown that a real performance bug episode in a real distributed production application appears as a prominent performance anomaly. Figure <ref type="figure" target="#fig_8">8</ref> shows a scatterplot of (y i , ŷi ) pairs generated by a Basic model of the "FT" application during a period when application operators reported episodes of a performance bug due to a misconfiguration in a concurrency parameter. One episode corresponds to the prominent clusters of points in the lower right corner of the figure. FT is a globally-distributed enterprise application that resembles VDR in several respects. See <ref type="bibr" target="#b23">[23]</ref> for details on the FT application and on this case.</p><p>Model calibration takes under one second for large data sets, so our method can be used to detect anomalies in real time by simply recomputing a new model at the conclusion of each time interval (e.g., every 5 minutes) using a large moving window of historical data (e.g., from the previous week or month). The data point corresponding to the most recent interval may then be deemed anomalous if the overall accuracy of the model is good but the most recent performance observation y i disagrees substantially with the model's fitted value ŷi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Prospective Prediction</head><p>Table <ref type="table" target="#tab_4">3</ref> summarizes predictive accuracy results for our Basic and Composite models calibrated using LAR regression; the table also includes Scalar variants of both models. We do not present results for OLS calibration because they do not alter the qualitative conclusions we drew from Table <ref type="table" target="#tab_3">2</ref>: LAR works better, sometimes by a substantial margin.</p><p>In addition to normalized aggregate error, Table <ref type="table" target="#tab_4">3</ref> shows the alternative accuracy measure discussed in Section 3.4: the median of the distribution of normalized absolute residuals |e i |/y i . The two measures differ in how they penalize inaccuracy. Normalized aggregate error severely punishes even a single large residual but may "forgive" many small residuals, even those where |y iŷi | is large in relation to y i . Our other accuracy measure, median |e i |/y i , has the opposite tendency: It forgives a large residual if the corresponding y i is also large, but it penalizes us if the residual is often large in relation to y i .</p><p>Our prospective prediction results are consistent with our retrospective explanation results: First, transaction mix models (TMIX) outperform their Scalar counterparts by a very large margin for both real production applications by both accuracy measures. Even the Basic TMIX achieves normalized aggregate error under 15% for both real production data sets, and its individual performance predictions ŷi are within 14% of the true value y i half of the time. The Basic model leaves little room for improvement when applied to real production applications; as we would expect, the Composite model offers relatively modest accuracy improvements for the real production applications, which are intentionally very lightly loaded (Figure <ref type="figure" target="#fig_2">3</ref>). Queueing delays are likely to be small in relation to service times in such situations, so we gain relatively little by modeling queueing. However for the deliberately overloaded PetStore the situation is very different, as we would expect: The Composite model consistently achieves far better accuracy by both of our accuracy measures.</p><p>The relative benefits of incorporating transaction mix and queuing in the testbed application and the production applications are consistent with our expectations: PetStore is heavily loaded and has few transaction types; as we expect, the greatest increase in accuracy occurs when we move from the Basic model to the Composite model. The production applications are lightly loaded and have many transaction types; not surprisingly, the greatest increase in accuracy occurs when we move from a Scalar model to a TMIX model.</p><p>The combined benefits of accounting for both queueing and transaction mix in production applications are striking. Compared with the TMIX/Composite models, the Scalar/Basic models have substantially worse normalized aggregate error: 33% greater error for VDR and 34% greater error for ACME. The differences are even larger when we consider median |e i |/y i : 52% greater error for VDR and 40% greater error for ACME.</p><p>Figures 9, 10, and 11 illustrate both retrospective and predictive accuracy for a Composite model of the VDR application calibrated with LAR regression. In all cases, retrospective fitted values or residuals are shown in blue and prospective fitted values are shown in red. Figure <ref type="figure" target="#fig_9">9</ref> presents a time series of observed aggregate response times y i overlaid on fitted values ŷi . Overall, the latter track the former quite closely. Residuals are not markedly larger for prospective performance prediction than for retrospective performance explanation. For the VDR data set, our model generalizes well from historical data used for calibration to future workload data used for prediction.</p><p>Figure <ref type="figure" target="#fig_0">10</ref> shows a scatterplot of (y i , ŷi ) pairs. The three straight diagonal lines in the figure are the y = x diagonal, indicating perfect prediction, flanked by y = 2x and y = x/2. Although observed values y i range over three orders of magnitude, fitted values ŷi almost always agree to within a factor of two, and are usually within 15% of the true y i value. Finally, Figure <ref type="figure" target="#fig_0">11</ref> shows the full distributions of |e i |/y i for both explanatory and predictive models. Our Composite model is highly accurate for retrospectively explaining performance, and the figure shows that residuals remain remark- Figure <ref type="figure" target="#fig_0">10</ref>: Scatterplot of (y i , ŷi ) pairs. Figure <ref type="figure" target="#fig_0">11</ref> ably small when the model is used to predict performance based on transaction mix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Generalizing to New Conditions</head><p>We performed an additional experiment to evaluate the ability of our Composite model to predict performance in a real application under workload conditions very different from those of the calibration data. We sorted the VDR data set in ascending order on application server CPU utilization. The first half of the resulting re-ordered data set contains time intervals during which CPU utilization on the app server was very low. The second half contains time intervals during which utilization was much higher. Figure <ref type="figure" target="#fig_10">12</ref> shows the distributions of CPU utilizations in both halves of the re-ordered VDR data set.</p><p>We calibrated a Composite model on the first half of the reordered VDR data, when load was very light (CPU utilization between 4% and 18%). We then evaluated the model's predictive accuracy on the second half, when load was much heavier (utilization between 18% and 45%). The model's predictive accuracy was remarkably high under this challenging test: normalized aggregate error ∑ i |e i |/ ∑ i y i was under 9.7%; most predictions ŷi were within 8% of the true value y i . Furthermore, the model's predictive accuracy did not vary with load: Figure <ref type="figure" target="#fig_11">13</ref> shows a scatterplot of error per time interval |e i |/y i versus CPU utilization on the app server. The figure shows that accuracy is not noticeably worse under high utilization (i.e., there is no upward trend to the right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Modeling Utilization</head><p>A nonspecialist in queueing theory might wonder why we do not simply incorporate resource utilizations into our performance model by adding linear U r terms rather than the mysterious U 2 /(1-U) terms of Equation <ref type="formula" target="#formula_2">4</ref>. Table <ref type="table" target="#tab_0">4</ref> compares the predictive accuracy of our Basic and Composite models with a model that incorporates utilization in the naïve way. We see that the naïve approach sometimes improves upon our Basic model. However the "correct" approach of our Composite model yields still better accuracy. Several similar cases not reported here tend toward the same conclusion: Embellishing the Basic model in haphazard ways sometimes offers modest advantages, but amendments with sound theoretical justifications (as in our Extended and Composite models) yield better results overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PERFORMANCE-AWARE APPLICATION CONSOLIDATION</head><p>Enterprise systems that comprise multiple applications often execute each sub-application on separate machines to isolate the effects of software faults and workload spikes. Compared with such machine-granularity application boundaries, application consolidation (i.e., executing multiple applications on one machine) has several advantages including better resource utilization and lower management and maintenance overheads. However, workload fluctuations in consolidated environments can have complex effects on application-level performance that reduce the overall predictability of the system. In this section, we use our transaction mix model to predict application-level performance amidst contention for shared physical resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Consolidation Model</head><p>We predict system-wide performance in consolidated environments by combining the Composite transaction mix models of each target application running in isolation. We concatenate the transaction mix vectors and sum the resource utilizations in the Composite transaction mix models of the target applications. More formally, the system-wide sum of response times for two consolidated applications in interval i is:</p><formula xml:id="formula_9">ŷi = n ′ +n ′′ ∑ j=1 α j N i j + ∑ r 1 λ i (U ′ ir +U ′′ ir ) 2 1 -(U ′ ir +U ′′ ir ) • n ′ +n ′′ ∑ j=1 N i j</formula><p>where superscripts ( ′ and ′′ ) distinguish between the two applications. For brevity, we show a unified a j and N j which represent concatenation of each application's individual parameters. The variable n represents the number of transaction types, U r represents the utilization of resource r, and λ represents the post-consolidation arrival rate of both applications. Note that the unification of several Composite models can trivially be extended to handle the consolidation of more than two applications, though we do not provide empirical results on this more general case. We acknowledge that additive measures of resource utilization may not account for some additional costs of resource sharing (e.g., context switching). We have achieved accurate performance predictions without considering such effects, though their impact may become more significant as the degree of consolidation increases. Likewise, we assume an application's resource requirements do not decrease after consolidation, which can happen if the target applications interact or share data.</p><p>The resulting transaction mix model can be manipulated to derive performance predictions for each application under consolidation by considering the sum of the transaction types corresponding to a target application. Specifically, we extract the per-application  performance under consolidation as follows:</p><formula xml:id="formula_10">ŷ′ i = n ′ ∑ j=1 α j N i j + ∑ r 1 λ i (U ′ ir +U ′′ ir ) 2 1 -(U ′ ir +U ′′ ir ) • n ′ ∑ j=1 N i j . ŷ′′ i = n ′ +n ′′ ∑ j=n ′ +1 α j N i j + ∑ r 1 λ i (U ′ ir +U ′′ ir ) 2 1 -(U ′ ir +U ′′ ir ) • n ′ +n ′′ ∑ j=n ′ +1 N i j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Realistic Workload Generation</head><p>Not surprisingly, the ACME and VDR administrators did not allow us to perform consolidation experiments using their applications, so we built our own testbed from benchmark applications. Unfortunately, the workload generators shipped with our benchmark applications produced stationary workloads, which, as we have seen, differ qualitatively from workloads observed in the wild. We developed our own workload generator which applies realistic nonstationary workloads to our testbed by mimicking the transaction type frequencies in the traces of real applications (e.g., ACME and VDR). First, we separately ranked the transaction types in both our real and testbed applications according to their popularity in the real trace and workload generator probabilities respectively. We created a synthetic trace that imitates the the nonstationarity of real workloads by replacing each transaction type in the real trace with the transaction type with the same popularity rank in the testbed application. Finally, our workload generator also mimics the seasonality of real workloads by varying the request rate according to a sawtooth pattern. Henceforth references to a real application (i.e., ACME or VDR) preceded by "M-" will indicate a mimicked workload based on the named dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation</head><p>Our testbed consists of two benchmark applications. The Rice University Bidding System (RUBiS) <ref type="bibr">[38]</ref> is an online auction benchmark that consists of 22 transaction types that provide services such as browsing for items, placing bids, or viewing a user's information. The StockOnline <ref type="bibr" target="#b41">[42]</ref> trading system comprises six transaction types corresponding to buying, selling, viewing prices, viewing holdings, updating account information, and creating new users. Each application runs on top of the JBoss 4.0.2 application server <ref type="bibr" target="#b21">[21]</ref> and accesses MySQL <ref type="bibr" target="#b32">[32]</ref> as the back-end database. The application server and database run on separate machines. Resource consumption at the database is negligible in our testbed, so we focus on the consolidation of the application server tier.</p><p>Our experiments are run on a three-machine cluster. Each node consists of four 2.4-GHz CPUs with Intel Hyperthreading technology and 6 GB of main memory. We use the Linux 2.6.9 kernel distributed by Red Hat. All of the experiments mentioned in this section were run for five hours. Matching our observations of the ACME workload, each test was configured to exhibit eight complete sawtooth fluctuations in request rate. Unless otherwise noted, request rates were set for low CPU utilization (15-25%) when each application ran in isolation. Measurements were collected every 30 seconds using the SAR system monitor. Each respective testbed application was calibrated using the M-ACME imitation workload.</p><p>Table <ref type="table" target="#tab_6">5</ref> shows the accuracy of our consolidation predictions using the metrics discussed in Section 4.2.3. We show one consolidation scenario where the calibration and evaluation workloads are the same (M-ACME) and two scenarios in which the workload under consolidation differs from the calibration workload (M-ACME). Under the same calibration and evaluation workloads the aggregate error is 5.5% and 11% for RUBiS and Stock respectively. We also observe that the composition of transaction mix models for consolidation is robust to changes in workload; predicting performance on completely different workloads from the calibration set still yields aggregate errors within 9% and 14%. We note that the normalized errors for the StockOnline application are about twice that of RUBiS. Upon further investigation, we found that StockOnline is developed in a fashion such that transaction type does not provide much distinctive information about resource demands. In this sense, StockOnline violates one of our fundamental assumptions about applications, yet we are still able to report normalized aggregate errors of between 11% and 14%.</p><p>Figure <ref type="figure" target="#fig_12">14</ref> shows the cumulative distribution of absolute percent error when the RUBiS and StockOnline applications are consolidated and subjected to the M-VDR workloads. More than 97.6% and 74.2% of RUBiS and StockOnline predictions respectively are within 20% actual response time. The calibration and evaluation of StockOnline on different workloads represents a significant challenge for our model yet 98% of performance predictions are within 40% of the actual response time.</p><p>We also wish to demonstrate the robustness of our consolidation prediction under heavy load. We increased the workload intensity by ramping up utilizations such that CPU utilizations reached 70% on the application server. Figure <ref type="figure" target="#fig_13">15</ref> shows the cumulative distribution of the normalized residual error in our heavy-load consolidation scenario. We report normalized aggregate error of 12.8% and 11.7% for RUBiS and StockOnline respectively. While these results indicate the robustness of our consolidation model, we note the limitations of our queueing model: As we noted with the Pet-Store application, our predictions are not intended for systems in which a resource is frequently saturated. Consolidation experiments under such situations gave performance predictions with un-  acceptable (41%) normalized standard error; since this result is expected it is not shown here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>This section reviews literature on several topics related to our work: We begin with a survey of workload characterization studies that have identified regularities in modern application workloads and then consider workload generators used in research and in commercial benchmarking. We then review instrumentation tools and techniques that can be used to calibrate performance models, theoretical performance models themselves, and the application of such models to practical performance prediction problems. Finally, we summarize the state of the art in server consolidation research and practice. A review of literature on an important application of our modeling technique, performance anomaly detection, is available in <ref type="bibr" target="#b23">[23]</ref>. A review of literature on LAR regression is available in <ref type="bibr" target="#b24">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Workload Characterization</head><p>Previous research has characterized many aspects of workload in modern transactional applications, e.g., Web server workloads <ref type="bibr" target="#b4">[4]</ref> and Web user sessions <ref type="bibr" target="#b2">[2]</ref>. <ref type="bibr">Menasce et al.</ref> propose first-order Markov models of customer accesses at e-commerce sites <ref type="bibr" target="#b30">[30]</ref> and describe statistically self-similar arrival patterns at such sites <ref type="bibr" target="#b29">[29]</ref>. Some forms of nonstationarity have been observed in workloads, e.g., in Internet traffic <ref type="bibr" target="#b12">[12]</ref> and in the diurnal cycles of requests to Web sites <ref type="bibr" target="#b5">[5]</ref>. However to the best of our knowledge nonstationarity in the mix of application-level transaction types is not considered in previous research.</p><p>Many characterization studies lead directly to prescriptions for improved performance. <ref type="bibr">Breslau</ref>   Web cache removal policy is optimal <ref type="bibr" target="#b11">[11]</ref>. Arlitt &amp; Williamson find support for size-based Web cache removal policies in the size distributions of Web documents <ref type="bibr" target="#b5">[5]</ref>. Jung et al. characterize transient overload events ("flash crowds") and suggest ways for system defenses to distinguish them from denial-of-service attacks <ref type="bibr" target="#b22">[22]</ref>.</p><p>Clustering techniques are frequently employed to simplify workload for performance modeling <ref type="bibr" target="#b6">[6]</ref>. Often workload units are clustered according to their resource demands as in Magpie <ref type="bibr" target="#b7">[7]</ref> and in Esposito et al. <ref type="bibr" target="#b18">[18]</ref>, but occasionally clustering is applied to other aspects of workload, e.g., customers <ref type="bibr" target="#b50">[51]</ref>.</p><p>Our contribution is to recognize that in many modern applications transaction types effectively cluster workload elements according to their resource demands, and that nonstationarity in transaction mix presents an opportunity to calibrate performance models without invasive instrumentation or controlled benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Workload Generation</head><p>Commercial synthetic workload generators are used to evaluate the performance of live applications and systems <ref type="bibr" target="#b31">[31]</ref> and in standard benchmarks such as TPC-W <ref type="bibr" target="#b46">[47]</ref>. To the best of our knowledge, such tools generate workloads with stationary transaction mixes. Schroeder et al. survey workload generators used in research, emphasizing the distinction between open and closed generators <ref type="bibr" target="#b39">[40]</ref>. These tools allow users to configure arrival rate parameters but do not facilitate the generation of workloads with nonstationary transaction mixes. To the best of our knowledge, the only workload generator that does so is the SWAT tool of Krishnamurthy et al. <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>. SWAT employs sophisticated mathematical programming techniques to construct a mix of sessions that conforms to user-specified aggregate workload criteria; nonstationarity follows from the use of recorded sessions from a real production system. SWAT strives to provide both the fidelity of simple trace replay with the control and configurability of conventional workload generators.</p><p>Our contribution is to recognize that the stationary workloads produced by conventional workload generators make parameter estimation for performance modeling more difficult than naturallyoccurring nonstationary workloads. Indeed, our results show that controlled experiments using synthetic workload generators are not necessary to calibrate performance models. Accurate performance models may be calibrated using the kinds of lightweight passive measurements routinely collected in today's production systemstransaction logs and utilization logs.</p><p>We furthermore report that the simple expedient of replaying transaction logs from real production applications with the transaction types re-named to suit a testbed application (Section 5.2) yields good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Instrumentation &amp; Measurement</head><p>A wide range of commercial performance measurement tools are available. Sauers et al. provide a candid survey of the strengths and limitations of one vendor's products <ref type="bibr" target="#b38">[39]</ref>. Some tools provide insight into transaction execution paths and resource usage noninvasively-without application source code modificationsvia instrumented middleware <ref type="bibr" target="#b19">[19]</ref>. If source code is available, applications may be systematically instrumented to record more detailed transaction execution information, e.g., using the ARM instrumentation standard <ref type="bibr" target="#b44">[45]</ref>. Instrumentation to characterize transaction resource demands in greater detail and with lower overhead remains an active research area <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Our contribution is to recognize that transaction mix nonstationarity in real-world workloads enables us to use very lightweight measurements to characterize the resource demands of transaction types for the purpose of calibrating performance models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Queueing Theory</head><p>Queueing networks are the subject of a large theoretical literature; see Bolch et al. for a lengthy survey <ref type="bibr" target="#b10">[10]</ref>. Jain describes applications of queueing theory to computer system performance analysis <ref type="bibr" target="#b20">[20]</ref>. The approaches that Jain surveys differ from ours in several key respects: Jain emphasizes the design of controlled experiments for performance analysis, and an underlying assumption throughout much of the book is that systematic benchmarking is possible. Furthermore most of Jain's queueing network models assume far more detailed information about transaction behavior than is available in many practical situations. For instance, it is frequently assumed that the number of times a transaction visits various resources and the distribution of service times at each station can be measured directly. Our work proceeds from the assumption that lightweight passive measurements of transaction response times and resource utilizations are all that is available.</p><p>Operational analysis is a branch of queueing theory that attempts to avoid probabilistic assumptions about system workload (e.g., Poisson arrivals and exponentially-distributed service times) and rely solely upon measurable quantities <ref type="bibr" target="#b17">[17]</ref>; Little's Law and the Utilization Law are classic examples of operational laws. Mean Value Analysis (MVA) restricts attention to the averages (as opposed to the full distributions) of performance measures <ref type="bibr" target="#b35">[35]</ref>. Rolia &amp; Sevcik introduce a variant of MVA designed to accommodate software servers in addition to conventional hardware service stations <ref type="bibr" target="#b36">[36]</ref>. Like conventional MVA, this method pertains to closed queueing networks, whereas we employ open network models.</p><p>Generalizations of queueing-theoretic models and MVA address multiclass networks <ref type="bibr" target="#b9">[9]</ref>. Workload classes are often interpreted as categories such as "batch," "terminal," and "transaction," but classes can also be used to represent different transaction types. One problem with existing multiclass methods for our purposes is that most assume a closed network-our production traces do not include sufficient information about client sessions for us to employ a closed model. Another problem is that the computational cost of computing exact solutions to MVA using conventional algorithms increases rapidly with the number of classes. A more efficient algorithm has appeared recently <ref type="bibr" target="#b13">[13]</ref> but it is formidably complex and difficult to implement. Multiclass models with more than a handful of classes are rarely used in practice due to their complexity and the computational cost of computing solutions.</p><p>Our contribution is to introduce a computationally tractable and conceptually simple performance model for open networks that takes transaction mix into account, that models multiple service centers, that is easy to calibrate, and that yields accurate response time predictions for real production applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Applied Performance Prediction</head><p>This section reviews in depth two recent papers that apply queueing models to distributed applications, highlighting similarities and contrasts with respect to our work. We refer the reader to their excellent literature reviews for recent, broad, and thorough surveys of related work in this field <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Urgaonkar et al. model multi-tier Internet services as productform queueing networks and employ mean value analysis to compute average response times <ref type="bibr" target="#b47">[48]</ref>; in some respects this work is similar to that of Liu et al.  <ref type="bibr">[48, p. 294</ref>]. An important difference is that their method requires more diverse model parameter estimates than ours, including request visit ratios at each tier, service times at each tier, user think times, and certain other parameters related to congestion effects. Urgaonkar et al. report that their approach yields accurate average response time estimates for two sample applications (RUBiS and Rubbos) subjected to stationary synthetic workloads in a testbed environment; they do not report validation results on real production applications.</p><p>Stewart &amp; Shen present a performance model of distributed Internet applications based on "profiles" that summarize how application software components and their workloads place demands on underlying system resources <ref type="bibr" target="#b40">[41]</ref>. Their model also accounts for inter-component communications and component placement. This work shares some features in common with our approach. For instance, Stewart &amp; Shen account for waiting times at servers using an M/G/1 model; we employ a similar model in Equation <ref type="formula" target="#formula_2">4</ref>. They estimate the resource demands of components by fitting linear models to benchmark data. However, they describe workload by a constant scalar arrival rate, whereas we use a time-varying vector of per-type transaction counts. Stewart &amp; Shen report that their most sophisticated model variant predicts average response times to within 14%. Their validation uses testbed applications (RUBiS and StockOnline) and stationary synthetic workloads.</p><p>An important difference with respect to our work is that the method of Stewart &amp; Shen requires very extensive calibration: The resource consumption profile of each component must be estimated via controlled benchmark experiments, and inter-component communication overheads must also be measured. They place each profiled component on a dedicated machine during calibration and require at least one benchmark run per component. For their full model, O(N 2 ) benchmark runs are required to estimate pairwise inter-component communication costs <ref type="bibr">[41, p. 75</ref>]. We exploit nonstationarity to obtain similar performance profiles using only lightweight passive measurements of running production systems: The coefficients of our utilization model (Equation <ref type="formula" target="#formula_5">5</ref>) correspond closely to those in the "component resource profiles" of Stewart &amp; Shen (see Figure <ref type="figure" target="#fig_1">2</ref> and Tables <ref type="table" target="#tab_2">1</ref> and<ref type="table" target="#tab_3">2</ref> in <ref type="bibr" target="#b40">[41]</ref>. Another difference is that we do not require knowledge of internal application component structure; we use only externally-visible transaction types.</p><p>We emphasize two important differences between our evaluation experiments and those presented in Urgaonkar et al. and in Stewart &amp; Shen. First, as noted above, we have employed two real production traces for our evaluations; they have used only testbed applications. The workload of our applications is nonstationary in several key parameters, including both workload intensity and transaction mix. By contrast, Stewart &amp; Shen and Urgaonkar et al. employ synthetic workloads reminiscent of classic steady-state benchmarks both for model calibration and for evaluation. The transaction mixes in their synthetic workload (e.g., the buy:browse ratio in their synthetic e-commerce workloads) remain constant during both calibration and evaluation. We believe that our nonstationary workload yields a far more challenging and more realistic test of a performance model's generalizability and predictive accuracy.</p><p>Our empirical evaluations could not include comparisons with the methods of Stewart &amp; Shen and of Urgaonkar et al. for two reasons: First, the input-output behavior of the three models is sufficiently different to preclude a true apples-to-apples comparison. More importantly, the other two approaches require far more extensive calibration data than is available in our data sets (our current testbed at HP does not permit the same instrumentation as used in Stewart &amp; Shen; e.g., kernel modifications are not allowed). However we do compare our preferred approach with alternatives that, like the models of Stewart &amp; Shen and of Urgaonkar et al., employ a scalar measure of workload intensity (Section 3.2.4). We found that transaction mix models offer substantially higher accuracy than their Scalar counterparts (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Consolidation</head><p>The computing trends of the 1980s and 1990s led to decentralized IT infrastructures that can be more difficult to manage and less cost-effective than their centralized predecessors. Server consolidation attempts to increase resource utilization while reducing the costs of hardware, data center floor space, power, cooling, and administration. The resource cost benefits of consolidation alone are potentially attractive: Andrzejak et al. studied CPU utilization in six enterprise data centers containing roughly 1,000 CPUs and found that consolidation could reduce the number of CPUs needed during peaks by 53% and the mean number required by 79% <ref type="bibr" target="#b1">[1]</ref>.</p><p>Administrators know that systems can be overloaded if the sum of consolidated application resource demands is excessive. Practitioners are advised to rely on rough guidelines for total resource utilization, e.g., "avoid peak CPU utilization over 70%" <ref type="bibr" target="#b14">[14]</ref>. Commercial capacity planning decision support aids may employ more sophisticated time-series analysis of pre-consolidation historical data, but their suggestions are based on considerations of resource utilizations <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Recent research on consolidation decision support also bases recommendations on resource utilization. Rolia et al. analyze historical utilization data to provide statistical guarantees on post-consolidation utilization <ref type="bibr" target="#b37">[37]</ref>. Urgaonkar et al. profile applications on dedicated nodes to estimate resource demands and "pack" appli-cations to maximize revenue while controlling the potential for resource overload <ref type="bibr" target="#b48">[49]</ref>.</p><p>The state of the art in research and in practice is to base consolidation decisions on considerations of resource utilizations and either ignore application-level workload or model it as a scalar quantity. This is problematic because the relationship between applicationlevel performance and utilization is complex, and because both depend on transaction mix. Our contribution is a practical way to obtain accurate predictions of response times in transactional applications, thus allowing consolidation decisions to consider applicationlevel performance as well as system resource utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>The global geographic distribution, organizational decentralization, opaque component structures, and unprecedented scale of modern application architectures confound performance modeling in challenging new ways. Performance prediction in business-critical applications, however, remains an important problem due to the growing economic importance of these applications. This paper presents a practical, versatile, and accurate approach to predicting application-level response times in complex modern distributed applications. Our method exploits naturally-occurring workload nonstationarity to circumvent the need for invasive instrumentation or controlled benchmarking for model calibration. It relies solely on measurement data that is routinely collected in today's production environments. Our method can be adapted to a wide range of applications, and calibrated models generalize well to new regions of workload/performance space. It is novel in its use of transaction mix to predict performance, and we have shown that transaction mix is a far more powerful predictor of application performance under realistic conditions than scalar workload volume.</p><p>Our empirical results show that our method predicts response times in real production applications to within 16% by two very different accuracy measures. A model of a real production application calibrated under light load predicts performance under heavy load to within 10%. Our results show that if accurate workload forecasts are available, they can be mapped directly to accurate performance predictions. Furthermore we predict response times of consolidated applications to within 4% to 14% based on passive preconsolidation measurements, even when workload changes dramatically between calibration and evaluation. Whereas existing approaches to consolidation decision support consider only resource utilization, our approach enables application-level response times to guide consolidation decisions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fractions of VDR transactions, two most common types. Each point represents a different 5-minute interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fraction of ACME "add-tocart" transactions vs. time. Each point represents a different 5-minute interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CDFs of resource utilizations encountered by arriving transactions in ACME and VDR applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fraction of VDR transactions due to most common type (heavy line); total transaction volume (light line), 1-hr windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fraction of RUBiS transactions due to two most common types using default generator. The figure is not empty; note the tight cluster of points at coordinates (0.3, 0.3). This workload is highly stationary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Extended queueing model: One station per resource at each tier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7</head><label>7</label><figDesc>Figure 7 depicts the architecture of the globally-distributed VDR application. VDR is a high-availability business-critical internal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A performance anomaly in the "FT" application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Time series of y i and ŷi .Figure10: Scatterplot of (y i , ŷi ) pairs. Figure11: CDF of |e i |/y i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: CDFs of app server CPU utilization in both halves of re-ordered VDR data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Prediction error |e i |/y i versus app server CPU utilization in re-ordered VDR data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: CDF of absolute percent error (|e i |/y i ) under different calibration and evaluation workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: CDF of absolute percent error (|e i |/y i ) under heavy non-saturating workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc><ref type="bibr" target="#b28">[28]</ref>. The Urgaonkar et al. model assumptions differ from ours in several details. For instance, Urgaonkar et al. explicitly model concurrency limits whereas we do not. We assume an open queueing network whereas Urgaonkar et al. assume a closed network. We explicitly model distinct physical resources such as CPUs and disks whereas Urgaonkar et al. associate a single queue with each tier. The models differ in their assumptions about how requests recirculate among tiers; compare our Figure 6 with their Figure 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4 in</head><label>4</label><figDesc>Section 4.2.1).</figDesc><table><row><cell>App Server</cell><cell>DB Server</cell></row><row><cell>CPU</cell><cell>CPU</cell></row><row><cell>Network</cell><cell>Network</cell></row><row><cell>Disk</cell><cell>Disk</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Summary of production data sets.</head><label>1</label><figDesc>HP application serving both external customers and HP users on six continents. Its system architecture therefore incorporates redundancy and fail-over features both locally and globally. Regional hubs in Atlanta, Swindon, and Singapore respectively serve the Americas, Europe, and Asia regions. All application server hosts are HP 9000/800 servers running HP-UX B.11.11. The Americas region has two such hosts with 16 CPUs and 64 GB RAM each. The European region has three, with 16 CPUs and 32 GB RAM each, and the Asia region has two, with 12 CPUs and 20 GB RAM each.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Data</cell><cell cols="2">Production</cell><cell></cell><cell></cell><cell></cell><cell>Transactions:</cell><cell>Trans'ns/min</cell><cell>Resp time (sec)</cell><cell>Type of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Set</cell><cell></cell><cell cols="4">Dates Duration</cell><cell>Number Types</cell><cell>Mean Median Mean Median Application</cell></row><row><cell></cell><cell></cell><cell cols="2">ACME</cell><cell cols="6">July 2000 4.6 days 1,180,430</cell><cell>93</cell><cell>182.2</cell><cell>183.0 0.929</cell><cell>0.437</cell><cell>Web Retail Shopping</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VDR</cell><cell cols="5">Jan 2005 7.8 days</cell><cell>666,293</cell><cell>37</cell><cell>59.4</cell><cell>56.4 1.289</cell><cell>1.236</cell><cell>Business-critical Enterprise</cell></row><row><cell></cell><cell></cell><cell cols="8">PetStore April 2004 38 hours 4,920,642</cell><cell>10</cell><cell>2147.8</cell><cell>3163.8 0.096</cell><cell>0.040</cell><cell>Sample application</cell></row><row><cell></cell><cell cols="3">AMERICAS</cell><cell></cell><cell cols="3">EUROPE</cell><cell></cell><cell>ASIA</cell></row><row><cell></cell><cell>client</cell><cell>client</cell><cell>client</cell><cell></cell><cell>client</cell><cell>client</cell><cell>client</cell><cell></cell><cell>client</cell><cell>client</cell><cell>client</cell></row><row><cell>Managed App Server Provider</cell><cell cols="3">WAN Load balancer Primary DB App Srvr Backup DB App Srvr</cell><cell>DB failover</cell><cell cols="3">WAN Load balancer Primary DB App Srvr Backup DB App Srvr</cell><cell cols="2">DB failover</cell><cell>WAN Load balancer Primary DB App Srvr Backup DB App Srvr</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Oracle DB replication</cell><cell></cell><cell>WAN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">Figure 7: VDR application architecture.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : Retrospective explanatory accuracy: Normalized ag- gregate error</head><label>2</label><figDesc>∑ i |e i |/ ∑ i y i .</figDesc><table><row><cell cols="2">VDR</cell><cell cols="2">ACME</cell><cell cols="2">PetStore</cell></row><row><cell>Scalar</cell><cell>TMIX</cell><cell>Scalar</cell><cell>TMIX</cell><cell>Scalar</cell><cell>TMIX</cell></row><row><cell>Basic</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">OLS 0.1485 0.1002 0.2034 0.1308 0.4403 0.3605</cell></row><row><cell cols="6">LAR 0.1462 0.0940 0.2029 0.1281 0.3646 0.3230</cell></row><row><cell>Extended</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">OLS 0.1478 0.0997 0.2012 0.1213 0.2320 0.2897</cell></row><row><cell cols="6">LAR 0.1454 0.0936 0.2006 0.1185 0.1978 0.2221</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Prospective prediction accuracy: Normalized aggre- gate error</head><label>3</label><figDesc>∑ i |e i |/ ∑ i y i and median |e i |/y i .</figDesc><table><row><cell></cell><cell cols="2">VDR</cell><cell cols="2">ACME</cell><cell cols="2">PetStore</cell></row><row><cell></cell><cell>Scalar</cell><cell>TMIX</cell><cell>Scalar</cell><cell>TMIX</cell><cell>Scalar</cell><cell>TMIX</cell></row><row><cell>∑i |e i | ∑i y i</cell><cell cols="6">Basic 0.1621 0.1226 0.1749 0.1470 0.6528 0.6257</cell></row><row><cell cols="7">Composite 0.1606 0.1218 0.1723 0.1305 0.3702 0.4173</cell></row><row><cell cols="2">median</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">|e i |/y i</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">Basic 0.1418 0.0963 0.1724 0.1378 0.5247 0.5056</cell></row><row><cell cols="7">Composite 0.1420 0.0931 0.1664 0.1230 0.1708 0.2365</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : Incorporating utilization naïvely vs. correctly: Nor- malized aggregate error.</head><label>4</label><figDesc>: CDF of |e i |/y i .</figDesc><table><row><cell></cell><cell>VDR</cell><cell cols="2">ACME PetStore</cell></row><row><cell>Basic</cell><cell cols="2">0.1226 0.1470</cell><cell>0.6257</cell></row><row><cell>naïve U r</cell><cell cols="2">0.1221 0.2193</cell><cell>0.5794</cell></row><row><cell cols="3">Composite 0.1218 0.1305</cell><cell>0.4173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : Application-level performance prediction accuracy in consolidated environments. Predictions are based on measurements of each application in isolation. In all cases model calibration employed the M-ACME workload.</head><label>5</label><figDesc>∑i |e i | ∑i y i ) Absolute Residuals (|e i |/y i ) Error ( ∑i |e i | ∑i y i ) Absolute Residuals (|e i |/y i )</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RUBiS Accuracy</cell><cell>Stock Accuracy</cell></row><row><cell cols="6">Consolidated Consolidated Normalized</cell><cell>Median of</cell><cell>Normalized</cell><cell>Median of</cell></row><row><cell></cell><cell>RUBiS</cell><cell></cell><cell>Stock</cell><cell cols="2">Aggregate</cell><cell>Normalized</cell><cell>Aggregate</cell><cell>Normalized</cell></row><row><cell cols="5">Workload Error ( M-ACME Workload M-ACME 0.0549</cell><cell></cell><cell>0.0383</cell><cell>0.1064</cell><cell>0.1263</cell></row><row><cell cols="2">M-ACME</cell><cell></cell><cell>M-VDR</cell><cell>0.0724</cell><cell></cell><cell>0.0706</cell><cell>0.1391</cell><cell>0.1184</cell></row><row><cell cols="2">M-VDR</cell><cell></cell><cell>M-VDR</cell><cell>0.0810</cell><cell></cell><cell>0.0626</cell><cell>0.1349</cell><cell>0.1145</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cumulative Density Function (CDF)</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell cols="2">Rubis Stock</cell></row><row><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell cols="5">sumRespTime Absolute Percentage Error (APE): |Predict-Actual|/Actual</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>et al. show that the Zipf-like distribution of Web document access frequencies implies that a certain</figDesc><table><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cumulative Density Function (CDF)</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell>Rubis Stock</cell></row><row><cell></cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell cols="5">sumRespTime Absolute Percentage Error (APE): |Predict-Actual|/Actual</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Martin Arlitt supplied the ACME data set. Ira Cohen, Julie Symons, and the second author collected the FT and PetStore data sets for separate projects <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. We thank the operators of the ACME, FT, and VDR production systems for providing anonymized trace data. Hsiu-Khuern Tang answered questions on statistical matters. Arjun Nath provided valuable assistance to our testbed experiments. We are deeply grateful to Narayan Krishnan and Eric Wu for their extraordinarily assistance in setting up and administering the cluster we used for consolidation tests. We thank David Oppenheimer, Jerry Rolia, and Bhuvan Urgaonkar for many insightful discussions of performance modeling and its applications, and we thank Kim Keeton, Kai Shen, Zhikui Wang, Xiaoyun Zhu, Sharad Singhal, and the anonymous reviewers for reading drafts and offering many helpful suggestions. The first author acknowledges support from the U.S. National Science Foundation CAREER Award CCF-0448413.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bounding the resource savings of utility computing models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andrzejak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rolia</surname></persName>
		</author>
		<idno>HPL-2002-339</idno>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
		<respStmt>
			<orgName>HP Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Characterizing Web user sessions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000-09">Sept. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Characterizing the scalability of a large web-based shopping system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rolia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Internet Tech</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="69" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Web server workload characterization: The search for invariants</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Internet Web servers: Workload characterization and performance implications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Arlitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. on Networking</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="631" to="644" />
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Capacity planning for MVS computer systems</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Artis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="45" to="62" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using Magpie for request extraction and workload modelling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mortier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004-12">Dec. 2004</date>
			<biblScope unit="page" from="259" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An improved algorithm for discrete L1 linear approximations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Barrodale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="839" to="848" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open, closed, and mixed networks of queues with different classes of customers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baskett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Muntz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Palacios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="248" to="260" />
			<date type="published" when="1975-04">Apr. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Queueing Networks and Markov Chains</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bolch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>De Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Web caching and Zipf-like distributions: Evidence and implications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breslau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the nonstationarity of Internet traffic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<biblScope unit="page" from="102" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient algorithm for the exact analysis of multiclass queueing networks with large population sizes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Casale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Capacity Planning for Internet Services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cockcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Sun Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Correlating instrumentation data to system states: A building block for automated diagnosis and control</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Symons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Capturing, indexing, clustering, and retrieving system history</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Symons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The operational analysis of queueing network models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Denning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Buzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="261" />
			<date type="published" when="1978-09">Sept. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Workload characterization for trend analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mazzeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1981-07">July 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><surname>Hewlett-Packard</surname></persName>
		</author>
		<ptr target="http://h20229.www2.hp.com/products/tran/" />
		<title level="m">OpenView Transaction Analyzer</title>
		<imprint>
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Art of Computer Systems Performance Analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="http://www.jboss.com" />
		<title level="m">The JBoss J2EE Application Server</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Flash crowds and denial of service attacks: characterization and implications for CDNs and Web sites</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting performance anomalies in global applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX WORLDS</title>
		<imprint>
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Predicting performance in distributed enterprise applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<idno>HPL-2006-76</idno>
		<ptr target="http://www.hpl.hp.com/techreports/2006/HPL-2006-76.html" />
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
		<respStmt>
			<orgName>HP Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">SWAT: A tool for stress testing session-based Web applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
		<respStmt>
			<orgName>Computer Measurement Group Conf.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A synthetic workload generation technique for stress testing session-based systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="868" to="882" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Whole program paths</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling 3-tiered web applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<ptr target="http://www.cs.mcgill.ca/∼xueliu/publications/MASCOTS05Modeling.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. MASCOTS, Sept. 2005</title>
		<meeting>MASCOTS, Sept. 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">In search of invariants for e-business workloads</title>
		<author>
			<persName><forename type="first">D</forename><surname>Menasce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Riedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM E-Commerce Conf</title>
		<imprint>
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A methodology for workload characterization of e-commerce sites</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Menasce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A F</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM E-Commerce Conf</title>
		<imprint>
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Loadrunner load tester</title>
		<ptr target="http://www.mercury.com/us/products/performance-center/loadrunner/" />
	</analytic>
	<monogr>
		<title level="m">Mercury Interactive</title>
		<imprint>
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<ptr target="http://www.mysql.com" />
		<title level="m">MySQL database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Neter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Kutner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Nachtsheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>Applied Linear Statistical Models. Irwin, fourth edition</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toutenburg</surname></persName>
		</author>
		<title level="m">Linear Models: Least Squares and Alternatives</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mean value analysis of closed multichain queuing networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="322" />
			<date type="published" when="1980-04">Apr. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The method of layers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Sevcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="689" to="700" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Resource access management for a resource utility for commercial applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arlitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int&apos;l Sympos. on Integrated Network Mgmt. (IM)</title>
		<imprint>
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">HP-UX 11i Tuning and Performance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Sauers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ruemmler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Weygant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Open versus closed: A cautionary tale</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harchol-Balter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Performance modeling and system management for multi-component online services</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="http://forge.objectweb.org/projects/stock-online" />
		<title level="m">The StockOnline Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An Introduction to Markov Processes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Stroock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Server consolidation assessments with VMware CapacityPlanner</title>
		<author>
			<persName><forename type="first">R</forename><surname>Talaber</surname></persName>
		</author>
		<ptr target="http://download3.vmware.com/vmworld/2005/pac196.pdf" />
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<ptr target="http://www.opengroup.org/management/arm" />
		<title level="m">Application response measurement (ARM)</title>
		<imprint>
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
		<respStmt>
			<orgName>The Open Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stardust: Tracking activity in a distributed storage system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Strunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abd-El-Malek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMETRICS</title>
		<meeting>SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Transaction Processing Performance Council. TPC-W benchmark</title>
		<ptr target="http://www.tpc.org/tpcw/" />
		<imprint>
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An analytical model for multi-tier Internet services and its applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pacifici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spreitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tantawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="291" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Resource overbooking and application profiling in shared hosting platforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Server capacity planning and consolidation</title>
		<author>
			<persName><surname>Vmware</surname></persName>
		</author>
		<ptr target="http://www.vmware.com/news/releases/vacservices.html" />
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Characterizing customer groups for an e-commerce website</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Makaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM E-Commerce Conf</title>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Introduction to Robust Estimation and Hypothesis Testing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Wilcox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><surname>Id</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>wpm.tex,v 1.115 2007/04/10 23:23:37 kterence Exp $</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
