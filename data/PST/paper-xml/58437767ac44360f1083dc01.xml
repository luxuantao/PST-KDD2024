<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Micro Tells Macro: Predicting the Popularity of Micro-Videos via a Transductive Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
							<email>jingyuanchen91@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">‡ Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuemeng</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">‡ Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">‡ Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<email>xiangwang.nus@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">‡ Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">‡ Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">‡ Shandong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Micro Tells Macro: Predicting the Popularity of Micro-Videos via a Transductive Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">230215E99BD1521CA68232E37AD7A47B</idno>
					<idno type="DOI">10.1145/2964284.2964314</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Micro-Videos</term>
					<term>Popularity Prediction</term>
					<term>Multi-View Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Micro-videos, a new form of user generated contents (UGCs), are gaining increasing enthusiasm. Popular microvideos have enormous commercial potential in many ways, such as online marketing and brand tracking. In fact, the popularity prediction of traditional UGCs including tweets, web images, and long videos, has achieved good theoretical underpinnings and great practical success. However, little research has thus far been conducted to predict the popularity of the bite-sized videos. This task is nontrivial due to three reasons: 1) micro-videos are short in duration and of low quality; 2) they can be described by multiple heterogeneous channels, spanning from social, visual, acoustic to textual modalities; and 3) there are no available benchmark dataset and discriminant features that are suitable for this task. Towards this end, we present a transductive multi-modal learning model. The proposed model is designed to find the optimal latent common space, unifying and preserving information from different modalities, whereby micro-videos can be better represented. This latent space can be used to alleviate the information insufficiency problem caused by the brief nature of micro-videos. In addition, we built a benchmark dataset and extracted a rich set of popularity-oriented features to characterize the popular micro-videos. Extensive experiments have demonstrated the effectiveness of the proposed model. As a side contribution, we have released the dataset, codes and parameters to facilitate other researchers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The last couple of years have witnessed the unprecedented growth of smart mobile devices, enabling users to record life stories vividly with short videos and then instantly upload them to social media websites, such as Snapchat<ref type="foot" target="#foot_0">1</ref> and Vine<ref type="foot" target="#foot_1">2</ref> . Since micro-videos, acting more like 'live action photographs', are usually short in length, they need little bandwidth and hence gain tremendous user enthusiasm. The limits regarding the maximum length of micro-videos on Snapchat and Vine, are set as 10 and 6 seconds, respectively. Considering Vine as an example, its video length distribution over our collected 303, 242 Vine videos is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Micro-videos, representing a new form of user generated contents (UGCs), can be viewed, discussed and even reposted by users once they are uploaded, which leads to their rapid rise. It is reported that Vine hit 1.5 billion daily loops <ref type="foot" target="#foot_2">3</ref> in 2015 <ref type="foot" target="#foot_3">4</ref> and Snapchat hit 7 billion daily views in 2016 <ref type="foot" target="#foot_4">5</ref> . As a video messaging platform, it is hard to crawl micro-videos from Snapchat. Hence, we focus on Vine here, which can be gathered more easily for research.</p><p>Interestingly, among the tremendous volume of microvideos, some popular ones will be widely viewed and spread by users, while many only gain little attention. This phenomena is similar to many existing social media sites, such as Twitter <ref type="foot" target="#foot_5">6</ref> . For example, the micro-video about the explosion that interrupted during the France-Germany soccer match in 2015 has been successfully looped by over 330 million times. Obviously, if we can identify the hot and popular micro-videos in advance, it will benefit many applications, such as online marketing and network reservation. Regarding online marketing, the accurate early prediction of popular micro-videos can facilitate companies' planning of advertising campaigns and thus maximizing their revenues. For network service providers, they can timely reserve adequate distributed storage and bandwidth for popular ones, based on the prediction. Therefore, it is highly desirable to develop an effective scheme to accurately predict the popularity of micro-videos.</p><p>However, the popularity prediction of micro-videos is non-trivial due to the following challenges.</p><p>First of all, due to the short duration of micro-videos, each modality can only provide limited information, the socalled modality limitation. Fortunately, micro-videos always involve multiple modalities, namely, social, visual, acoustic and textual 7 modalities. In a sense, these modalities are corelated rather than independent and essentially characterize the same micro-videos. Therefore, the major challenge lies on how to effectively fuse micro-videos' heterogeneous clues from multiple modalities <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36]</ref>. The most naive strategies are early fusion and late fusion <ref type="bibr" target="#b27">[27]</ref>. They, however, fail to account for the relatedness among multiple modalities. Therefore, it is important to take modality relatedness into consideration. Secondly, due to certain external factors, such as camera shaking and lighting condition, some modalities of the micro-videos, such as visual or acoustic ones, may be of poor quality, which is another kind of modality limitation. Hence, learning directly from the original feature spaces of modalities, which was adopted by most multi-modal learning methods may be imprecise. Consequently, to improve the learning performance, how to compensate the noisy modality with reliable ones poses a crucial challenge for us. The last challenge we are facing is the lack of benchmark dataset to support our research. We found that both the contents and social influence of micro-video publishers would account for their popularity. As far as we know, the only available microvideo dataset <ref type="bibr" target="#b24">[24]</ref> does not contain the important textual and social modalities and is lack of popularity indicators as the ground truth, which makes it unsuitable for our research. It is thus necessary to build a comprehensive micro-video dataset and further extract a rich set of discriminant features to enhance the learning performance.</p><p>To address the aforementioned challenges, we present a novel Transductive Multi-modAL Learning approach, TMALL for short, to predicting the popularity of microvideos.</p><p>As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, we first crawl a representative micro-video dataset from Vine and develop a rich set of popularity-oriented features from multimodalities. We then perform multi-modal learning to predict the popularity of micro-videos, which seamlessly takes the modality relatedness and modality limitation into account by utilizing a common space shared by all modalities. We assume that there exists an optimal common space, which maintains the original intrinsic characteristics of micro-videos in the original spaces. In the light of this, all modalities are forced to be correlated. Meanwhile, microvideos with different popularity can be better separated in such optimal common space, as compared to that of each single modality. In a sense, we alleviate the modality limitation problem. Extensive experiments on this realworld dataset have well-validated our work.</p><p>Our main contributions can be summarized in threefold:</p><p>7 Micro-videos are usually associated with certain textual data, such as video descriptions given by the video owners.</p><p>• We approached the popularity prediction of microvideos by proposing a TMALL model, which is able to simultaneously model the modality relatedness and handle the modality limitations by introducing a common space shared among all modalities. Moreover, we have derived its closed-form solution rigorously. • We developed a rich set of popularity-oriented features from multiple modalities to comprehensively characterize the popular micro-videos. Apart from numerical results, we also provided several deep insights based on the experimental results. • We constructed a large-scale micro-video dataset, comprising of 303, 242 micro-videos, 98, 166 users and 120, 324 following relationships. We have released our compiled dataset, codes and parameters<ref type="foot" target="#foot_6">8</ref> to facilitate other researchers to repeat our experiments and verify their proposed approaches. The remainder of this paper is structured as follows. Section 2 reviews the related work. Data preparation is introduced in Section 3. Sections 4 and 5 detail the proposed TMALL model and the feature extraction, respectively. Section 6 presents the experimental results and analysis, followed by our concluding remarks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Popularity predication and multi-view learning are both related to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Popularity Prediction</head><p>Due to its enormous commercial potential, popularity prediction of UGCs has attracted great attention from both the industry and academia <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b11">11]</ref>. Hong et al. <ref type="bibr" target="#b12">[12]</ref> explored the popularity prediction of tweets, which is measured by the number of future retweets, by introducing a rich set of features, such as topological features, temporal features and meta features. Beyond the text popularity prediction, McParlane et al. <ref type="bibr" target="#b20">[20]</ref> focused on the popularity of images. They extracted some sophisticated features and cast the task of image popularity prediction as a problem of binary classification, where the given image would be classified as popular or not. Later, Cappallo et al. <ref type="bibr" target="#b5">[5]</ref> proposed a latent ranking method for the image popularity prediction solely based on the visual content. The proposed method was evaluated on several image datasets collected from micro-blogging and photo-sharing websites. Although huge success has been achieved by these approaches, limited efforts have thus far been dedicated to the problem of video popularity prediction, where multiple modalities coexist. Noting this gap, Trzcinski et al. <ref type="bibr" target="#b29">[29]</ref> shifted from images to videos, and studied the problem of video popularity prediction utilizing both the visual clues and the early popularity pattern of the video once it is released. However, this approach suffered from two limitations. First, the proposed approach can only work on videos that have been published over a certain period. Second, the authors only used the traditional machine learning method-Support Vector Regression (SVR), which failed to make full use of the relationship among modalities. As a complement, we aim to timely predict the popularity of a given micro-video even before it get published by proposing a novel multi-modal learning scheme. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-View Learning</head><p>To deal with data containing multiple modalities, multiview learning is a highly feasible paradigm. Multi-view learning is designed to improve the learning performance by introducing a function to model each view and jointly optimizing all functions. Existing work follows this line can be roughly classified into two categories: co-training and subspace learning. Co-training algorithms usually train separate learners on distinct views, which are then imposed to be consistent across views. Sindhwani et. al. <ref type="bibr" target="#b26">[26]</ref> introduced a co-regularization framework for multiview semi-supervised learning, as an extension of supervised regularization algorithms. Noticing that corruption may exist among different views, Christoudias et al.</p><p>[7] proposed an approach for multi-view learning taking the view disagreement into consideration. In contrast, subspace learning approaches hold the general assumption that different views are generated from a latent view. Chaudhuri et al. <ref type="bibr" target="#b6">[6]</ref> first employed canonical correlation analysis (CCA) to learn an efficient subspace, on which traditional machine learning algorithms can be applied. Gao et al. <ref type="bibr">[9]</ref> later introduced a novel multi-view subspace clustering method, which is able to simultaneously perform clustering on the subspace of each view and guarantee the consistency among multiple views by a common clustering structure.</p><p>Overall, compelling success has been achieved by multiview learning models on various problems, such as categorization <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b28">28]</ref>, clustering <ref type="bibr" target="#b6">[6,</ref><ref type="bibr">9]</ref> and multimedia retrieval <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>. However, to the best of our knowledge, limited efforts have been dedicated to applying multi-view learning in the context of micro-video popularity prediction, which is the major concern of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATA COLLECTION</head><p>This section details the dataset setup, which covers the crawling strategy, and ground truth construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Crawling Strategy</head><p>Our micro-video dataset was crawled from one of the most prominent micro-video sharing social networks, Vine. Beside the historical uploaded micro-videos, Vine also archives users' profiles and their social connections.</p><p>In particular, we first randomly selected 10 active Vine users from Rankzoo<ref type="foot" target="#foot_7">9</ref> , which provides the top 1, 000 active users on Vine, as the seed users. We then adopted the breadth-first crawling strategy to expand the seed users by crawling their followers. Considering that these seed users may have millions of followers, we practically only retained the first 1, 000 returned followers for each seed user to improve the crawling efficiency. After three layers of crawling, we harvested a densely connected user set consisting of 98, 166 users as well as 120, 324 following relationships among users. For each user, his/her brief profile was crawled, containing full name, description, location, follower count, followee count, like count, post count and loop count of all post videos. Besides, we also collected the timeline (the micro-video posting history, including the repostings from others.) of each user between July 1st and October 1st, 2015. Finally, we obtained 1.6 million video postings, including a total number of 303, 242 unique micro-videos with a total duration of 499.8 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ground Truth Construction</head><p>We employed four popularity-related indicators, namely, the number of comments (n comments), the number of likes (n likes), the number of reposts (n reposts), and the number of loops/views (n loops) to measure the popularity of microvideos. Figure <ref type="figure" target="#fig_2">3</ref> illustrates the proportion of micro-videos regarding each of the four indicators in our dataset. It is noticed that the distributions of them are different, and each of them measure one aspect of the popularity. In order to comprehensively and precisely measure the popularity of each micro-video, yi, we linearly fuse all the four indicators:</p><formula xml:id="formula_0">yi = (n reposts + n comments + n likes + n loops) 4 . (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OUR PROPOSED TMALL MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Notation</head><p>We first declare several notations. We employ bold capital letters (e.g., X) and bold lowercase letters (e.g., x) to denote matrices and vectors, respectively. We use non-bold letters (e.g., x) to represent scalars, and Greek letters (e.g., β) as parameters. If not clarified, all vectors are in column form.</p><p>Without loss of generality, suppose we have N labeled samples and M unlabeled samples with K 2 modalities. It is worth noting that the unlabeled samples also serve as testing samples. Z k stands for the number of features generated from the k-th modality. Then the k-th modality can be represented as X k ∈ R (N +M )×Z k . The popularity of all the videos are denoted by y = {y1, y2,</p><formula xml:id="formula_1">• • • , yN } T ∈ R N . Let f = {f1, f2, • • • , fN , fN+1, fN+2, • • • , fN+M } T ∈ R N +M</formula><p>stand for the predicted results regarding popularity for all samples, including the labeled and unlabeled ones. We aim to jointly learn the common space X0 ∈ R (N +M )×Z 0 shared by multiple modalities and the popularity for the M unlabeled micro-videos.</p><p>Our proposed model targets at reasoning from observed training micro-videos to testing ones. Such prediction belongs to transductive learning, in which both labeled samples as well as unlabeled samples are available for training. It hence obtains better performance. In contrast, inductive model is reasoning from observed training cases to general rules, which are then applied to the test cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Problem Formulation</head><p>It is apparent that different modalities may contribute distinctive and complementary information about microvideos. For example, textual modality gives us hints about the topics of the given micro-video; acoustic and visual modalities may respectively convey location and situation of micro-videos, and user modality demonstrates the influence of the micro-video publisher. These clues jointly contribute to the popularity of a micro-video. Obviously, due to the noise and information insufficiency of each modality, it may be suboptimal to conduct learning directly from each single modality separately. In contrast, we assume that there exists an optimal latent space, in which micro-videos can be better described. Moreover, the optimal latent space should maintain the original intrinsic characteristics conveyed by multi-modalities of the given micro-videos. Therefore, we penalize the disagreement of the normalized Laplacian matrix between the latent space and each modality. In particular, we formalize this assumption as follows. Let S k ∈ R (N +M )×(N +M ) be the similarity matrix 10 , which is computed by the Gaussian similarity function as follows,</p><formula xml:id="formula_2">S k (i, j) =      exp ( - x i k -x j k 2 2σ 2 k ) , if i ̸ = j; 0 , if i = j. (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where x i k and x j k are the micro-video pairs in the k-th modality space. Thereinto, the radius parameter σ k is simply set as the median of the Euclidean distances over all video pairs in the k-th modality. We then derive the corresponding normalized Laplacian matrix as follows,</p><formula xml:id="formula_4">L(S k ) = I -D -1 2 k S k D -1 2 k , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>10 To facilitate the illustration, k ranges from 0 to K.</p><p>where I is a (N + M ) × (N + M ) identity matrix and D k ∈ R (N +M )×(N +M ) is the diagonal degree matrix, whose (u, u)th entry is the sum of the u-th row of S k . Since S k (i, j) &gt; 0, we can derive that tr(L(S k )) &gt; 0. We thus can formulate the disagreement penalty between the latent space and the original modalities as,</p><formula xml:id="formula_6">K ∑ k=1 ∥ 1 tr(L(S0)) L(S0) - 1 tr(L(S k )) L(S k ) ∥ 2 F , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where tr(A) is the trace of matrix A and • F denotes the Frobenius norm of matrix. In addition, inspired by <ref type="bibr" target="#b32">[32]</ref>, considering that similar micro-videos attempt to have similar popularity in the latent common space, we adopt the following regularizer, 1 2</p><formula xml:id="formula_8">N +M ∑ m=1 N +M ∑ n=1 ( f (x m 0 ) √ D0(x m 0 ) - f (x n 0 ) √ D0(x n 0 ) ) 2 S0(m, n) = f T L(S0)f . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>Based upon these formulations, we can define the loss function that measures the empirical error on the training samples. As reported in <ref type="bibr" target="#b22">[22]</ref>, the squared loss usually yields good performance as other complex ones. We thus adopt the squared loss in our algorithm for simplicity and efficiency. In particular, since we do not have the labels for testing samples, we only consider the squared loss regarding the N unlabeled samples to guarantee the learning performance. We ultimately reach our objective function as, min</p><formula xml:id="formula_10">f ,L(S 0 ) N ∑ i=1 (yi -fi) 2 + µf T L(S0)f + λ K ∑ k=1 ∥ 1 tr(L(S0)) L(S0) - 1 tr(L(S k )) L(S k ) ∥ 2 F , (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where λ and µ are both nonnegative regularization parameters.</p><p>To be more specific, λ penalizes the disagreement among the latent space and modalities, and µ encourages that similar popularity will be assigned to similar micro-videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Alternative Optimization</head><p>To simplify the representation, we first define that,</p><formula xml:id="formula_12">       L = 1 tr(L(S0)) L(S0), Lk = 1 tr(L(S k )) L(S k ).<label>(7)</label></formula><p>Therefore, the objective function can be transformed to,</p><formula xml:id="formula_13">min f N ∑ i=1 (yi -fi) 2 + λ K ∑ k=1 L -Lk 2 F + µf T Lf , subject to tr(L(S0)) = 1.<label>(8)</label></formula><p>Furthermore, to optimize L more efficiently, inspired by the property that tr( Lk ) = 1, we let,</p><formula xml:id="formula_14">L(S0) = K ∑ k=1 β k Lk , subject to K ∑ k=1 β k = 1.<label>(9)</label></formula><p>Consequently, we have,</p><formula xml:id="formula_15">L = 1 tr(L(S0)) L(S0) = K ∑ k=1 β k Lk , subject to K ∑ k=1 β k = 1.<label>(10)</label></formula><p>Interestingly, we find that β k can be treated as the corelated degree between the latent common space and each modality. It is worth noting that we do not impose the constraint of β ≥ 0, since we want to keep both positive and negative co-relations. A positive coefficient indicates the positive correlation between the modality space and the latent common space, while a negative coefficient reflects the negative correlation, which may be due to the noisy data of the modality. The larger the β k is, the higher correlation between the latent space and the k-th modality will be. In the end, the final objective function can be written as,</p><formula xml:id="formula_16">min f ,β N ∑ i=1 (yi -fi) 2 + λ K ∑ k=1 ∑ K i=1 βi Li -Lk 2 F + µf T K ∑ k=1 β k Lk f + θ β 2 , subject to e T β = 1,<label>(11)</label></formula><p>where</p><formula xml:id="formula_17">β = [β1, β2, • • • , βK ] T ∈ R K and e = [1, 1, • • • , 1] T ∈ R K .</formula><p>θ is the regularization parameter, introduced to avoid the overfitting problem. We denote the objective function of Eqn. <ref type="bibr" target="#b11">(11)</ref> as Γ. We adopt the alternating optimization strategy to solve the two variables f and β in Γ. In particular, we optimize one variable while fixing the other one in each iteration. We keep this iterative procedure until the Γ converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Computing βj with f fixed</head><p>We first fix f and transform the objective function Γ as, min</p><formula xml:id="formula_18">β λ K ∑ k=1 N +M ∑ t=1 M (t) β - l(t) k 2 F + µg T β + θ β 2 ,</formula><p>subject to e T β = 1,</p><p>where</p><formula xml:id="formula_20">g = [f T L1f , f T L2f , • • • , f T LK f ] T ∈ R K , M (t) = [ l(t) 1 , l(t) 2 , • • • , l(t) K ] ∈ R (N +M )×K and l(t) k ∈ R N +M</formula><p>denotes the t-th column of Lk . For simplicity, we replace l(t) K with l(t) k e T β, as e T β = 1. With the help of Lagrangian, Γ can be rewritten as follows.</p><formula xml:id="formula_21">min β λ K ∑ k=1 N +M ∑ t=1 (M (t) - l(t) k e T )β 2 F + µg T β + δ(1 -e T β) + θ β 2 , (<label>13</label></formula><formula xml:id="formula_22">)</formula><p>where δ is a nonnegative Lagrange multiplier. Taking derivative of Eqn. <ref type="bibr" target="#b13">(13)</ref> with respect to β, we have,</p><formula xml:id="formula_23">∂Γ ∂β = Hβ + µg -δe, (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>where,</p><formula xml:id="formula_25">H = 2 [( λ K ∑ k=1 N +M ∑ t=1 (M (t) - l(t) k e T ) T (M (t) - l(t) k e T ) ) + θI ] , (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>and I is a K × K identity matrix. Setting Eqn. <ref type="bibr" target="#b14">(14)</ref> to zero, we have,</p><formula xml:id="formula_27">β = H -1 (δe -µg). (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>Substituting Eqn.( <ref type="formula" target="#formula_27">16</ref>) into e T β = 1, we have,</p><formula xml:id="formula_29">       δ = 1 + µe T H -1 g e T H -1 e , β = H -1 [ e + µe T H -1 ge e T H -1 e -µg</formula><p>] .</p><p>(</p><formula xml:id="formula_30">)<label>17</label></formula><p>According to the definition of positive-definite matrix, H is always positive definite and hence invertible. Therefore, H -1 is also positive definite, which ensures e T H -1 e &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Computing f with βj fixed</head><p>With fixed βj, taking derivative of Γ with respect to fi, where 1 ≤ i ≤ N , we have,</p><formula xml:id="formula_31">∂Γ ∂fi = 2(fi -yi) + 2µ N +M ∑ j=1 L(i, j)fj. (<label>18</label></formula><formula xml:id="formula_32">)</formula><p>We then take derivative of the Γ with respect to fi, where</p><formula xml:id="formula_33">N + 1 ≤ i ≤ N + M . We reach, ∂Γ ∂fi = 2µ N +M ∑ j=1 L(i, j)fj. (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>In a vector-wise form, we restate the solution of f as follows,</p><formula xml:id="formula_35">f = G -1 ŷ,<label>(20)</label></formula><p>where</p><formula xml:id="formula_36">G = Î+µ ∑ K k=1 β k Lk , ŷ = {y1, y2, • • • , yN , 0, 0, • • • , 0} and Î ∈ R (N +M )×(N +M ) is defined as follows, Î(i, j) = { 1 if i = j, and 1 ≤ i ≤ N , 0 otherwise. (<label>21</label></formula><formula xml:id="formula_37">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FEATURE EXTRACTION</head><p>It is apparent that both the publisher influence and content influence contribute to the popularity of UGCs. In particular, we characterized the publisher influence via the social modality, and the content influence via visual, acoustic and textual modalities. For content influence, we first examined the popular micro-videos in our dataset and propose three common characteristics of online microvideos. For each characteristic, we then explained the insights, and transformed it into a set of features for video representation. Finally, we developed a rich set of popularity-oriented features from each modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Observations</head><p>Universal Appeal. The subjects of widely popular micro-videos cannot be something that can only be appreciated by a small group of people. Therefore, the topics and objects contained in micro-videos should be something common so that to be interpreted the same way across people and cultures. To capture this characteristic, we extracted Sentence2Vector feature from the textual modality and deep object feature from the visual one.</p><p>Emotional Content. People are naturally drawn to things that arouse their emotions. Micro-videos showing funny animals or lovely babies make people feel urge to share them to express the same emotions. As a result, microvideos that are highly emotional are more likely to be shared. Therefore, we extracted textual sentiment, visual sentiment features for each video as well as several acoustic features, which is widely used in emotion recognition in music <ref type="bibr" target="#b33">[33]</ref>.</p><p>High Quality and Aesthetic Design. When people share information on social networks, people are actually showing a little piece of themselves to their audience. Therefore, high quality and aesthetic design of the content, which could reflect the taste of people, is another important characteristic of popular micro-videos. Color histogram, aesthetic feature and visual quality feature were thus extracted to encode such characteristic. In addition, the acoustic features we extracted are frequently used in music modeling, which could help to detect music in the audio track of micro-videos <ref type="bibr" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Social Modality</head><p>It is intuitive that micro-videos posted by users, who has more followers or has a verified account, are more likely to be propagated, and thus tend to receive a higher number of audiences. To characterize the influence of micro-video publishers, we developed the following publisher-centric features for micro-videos.</p><p>• Follower/Followee Count. The number of followers and followees of the given micro-video publisher.</p><p>• Loop Count. The total number of loops received by all the posts of the publisher.</p><p>• Post Count. The number of posts generated by the publisher.</p><p>• Twitter Verification. A binary value indicating whether the publisher has been verified by Twitter 11 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visual Modality</head><p>Due to the short-length of micro-videos, the visual content is usually highly related to a single theme, which enables us to only employ a few key frames to represent the whole micro-video. Inspired by this, we extracted the visual features from certain key frames. The mean pooling was performed across all the key frames to create a fixed-length vector representation of each micro-video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Color Histogram</head><p>It has been found that most basic visual features (i.e., intensity and the mean value of different color channels in HSV space) except color histogram, have little correlation with popularity <ref type="bibr" target="#b15">[15]</ref>. Color histogram has outstanding correlation due to the fact that striking colors tend to catch users' eyes. Therefore, we only extracted color histogram as the basic visual feature to characterize popular microvideos. To reduce the size of color space, we grouped the color space into 50 distinct colors, which results in a 50-D vector for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Object Features</head><p>It has been studied that popular UGCs are strongly correlated with the objects contained in the videos <ref type="bibr" target="#b10">[10]</ref>. We believe that the presence of certain objects affect microvideos' popularity. For example, micro-videos with 'cute 11 A Vine account can be verified by Twitter, if it is linked to a verified Twitter account.</p><p>dogs' or 'beautiful girls' are more likely to be popular than those with 'desks' and 'stones'. We thus employed the deep convolutional neural networks (CNNs) <ref type="bibr" target="#b16">[16]</ref>, a powerful model for image recognition problems <ref type="bibr" target="#b35">[35]</ref>, to detect objects in micro-videos. Specifically, we applied the well-trained AlexNet DNN provided by the Caffe software package <ref type="bibr" target="#b13">[13]</ref> to the input key frames. The output of the fc7 layer and the final 1, 000-way softmax layer in AlexNet is a probability distribution over the 1, 000 class labels predefined in ImageNet. We treat them as our feature representation of each frame. In the end, a mean pooling was performed over the frames to generate a single 4, 096-D vector and 1, 000-D vector for each micro-video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">SentiBank Features</head><p>We performed the sentiment analysis of the visual modality due to that the sentiment of UGCs has been proven to be strongly correlated with their popularity <ref type="bibr" target="#b10">[10]</ref>.</p><p>In particular, we extracted the visual sentiment features based on the deep CNNs model which was trained on the SentiBank dataset <ref type="bibr" target="#b4">[4]</ref>. SentiBank contains 2, 089 concepts and each of them invokes specific sentiments such as 'cute girls' and 'funny animals'. Therefore, after mean pooling among keyframes, each micro-video is represented by a 2, 089-D vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Aesthetic Features</head><p>Aesthetic features are a set of handful selected features related to the principles of the nature and appreciation of beauty, which have been studied and found to be effective in popularity prediction <ref type="bibr" target="#b8">[8]</ref>. Intuitively, micro-videos that are objectively aesthetic are more likely to be popular. We employed the released tool<ref type="foot" target="#foot_8">12</ref>  <ref type="bibr" target="#b3">[3]</ref> to extract the following aesthetic features: a) dark channel feature; b) luminosity feature; c) s3 sharpness; d) symmetry; e) low depth of field; f) white balance; g) colorfulness; h) color harmony, and i) eye sensitivity, at 3 × 3 grids over each key frame. We then calculated: a) normalized area of dominant object; and b) normalized distances of centroid of dominant objects with respect to four stress points at frame level. In the end, we obtained 149-D aesthetic features for each micro-video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.5">Visual Quality Assessment Features</head><p>It is important that the visual quality of popular contents are maintained at an acceptable level, given rising consumer expectations of the quality of multimedia content delivered to them <ref type="bibr" target="#b25">[25]</ref>. In particular, we employed the released tool <ref type="foot" target="#foot_9">13</ref>to extract the micro-videos quality features based on the motion and spatio-temporal information, which have been proven to correlate highly with human visual judgments of quality. This results in a 46-D features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Acoustic Modality</head><p>Acoustic modality usually works as an important complement to visual modality in many video-related tasks, such as video classification <ref type="bibr" target="#b34">[34]</ref>. In fact, audio channels embedded in the micro-videos may also contribute to the popularity of micro-videos to a large extent. For example, the audio channel may indicate the quality of a given micro-video and convey rich background information about the emotion as well as the scene contained in the micro-video, which significantly affects the popularity of a micro-video. The acoustic information is especially useful for the cases where the visual features could not carry enough information. Therefore, we adopted the following widely-used acoustic features, i.e., Mel-Frequency Cepstral Coefficients (MFCC) <ref type="bibr" target="#b17">[17]</ref> and Audio-Six (i.e., Energy Entropy, Signal Energy, Zero Crossing Rate, Spectral Rolloff, Spectral Centroid, and Spectral Flux <ref type="bibr" target="#b33">[33]</ref>). These features are frequently used in different audio-related tasks, such as emotion detection and music recognition. We finally obtained a 36-D acoustic feature vector for each micro-video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Textual Modality</head><p>Micro-videos are usually associated with textual modality in the form of descriptions, such as "when Leo finally gets the Oscar" and "Puppy dog dreams", which may precisely summarize the micro-videos. Such summarization may depict the topics and sentiment information regarding the micro-videos, which has been proven to be of significance in online article popularity prediction <ref type="bibr" target="#b2">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Sentence2Vector</head><p>We found that the popular micro-videos are sometimes related to the topics of the textual descriptions. This observation propels us to conduct content analysis over the textual descriptions of micro-videos. Considering the short-length of descriptions, to perform content analysis, we employed the state-of-the-art textual feature extraction tool Sentence2Vector 14 , which was developed on the basis of work embedding algorithm Word2Vector <ref type="bibr" target="#b21">[21]</ref>. In this way, we extracted 100-D features for video descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Textual Sentiment</head><p>We also analyze the sentiments over texts, which has been proven to play an important role in popularity prediction <ref type="bibr" target="#b1">[1]</ref>. With the help of the Sentiment Analysis tool in Stanford CoreNLP tools 15 , we assigned each micro-video a sentiment score ranging from 0 to 4 and they correspond to very negative, negative, neutral, positive, and very positive, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENT</head><p>In this section, we conducted extensive experiments to comparatively verify our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment Settings</head><p>The remaining experiments were conducted over a cluster of 50 servers equipped with Intel Xeon(2x) CPU E5-2620 v3 at 2.40 GHz on 64 GB RAM, 24 cores and 64-bit Linux operating system. Regarding the deep feature extraction, we deployed Caffe framework <ref type="bibr" target="#b13">[13]</ref> on a server equipped with a NVIDIA Titan Z GPU. The experimental results reported in this paper were based on 10-fold cross-validation. In each round of the 10-fold cross-validation, we split our dataset into two chunks: 90% of the micro-videos were used for training, 10% were used for testing. We report performance in terms of normalised Mean Square Error (nMSE) <ref type="bibr" target="#b22">[22]</ref> between the predicted popularity and the actual popularity. The nMSE is an estimator of the overall deviations between 14 https://github.com/klb3713/sentence2vec. 15 http://stanfordnlp.github.io/CoreNLP/.</p><p>predicted and measured values. It is defined as,</p><formula xml:id="formula_38">nM SE = ∑ i=1 (pi -ri) 2 ∑ i=1 r 2 i , (<label>22</label></formula><formula xml:id="formula_39">)</formula><p>where pi is the predicted value and ri is the target value in ground truth.</p><p>We have three key parameters as shown in Eqn. <ref type="bibr" target="#b8">(8)</ref>. The optimal values of these parameters were carefully tuned with the training data in each of the 10 fold. We employed the grid search strategy to obtain the optimal parameters between 10 -5 to 10 2 with small but adaptive step sizes. In particular, the step sizes were 0.00001, 0.0001, 0.001, 0.01, 0.1, 1 and 10 for the range of [0.00001,0.0001], [0.0001,0.001], [0.001,0.01], [0.01,0.1], [0.1,1], <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b10">10]</ref> and <ref type="bibr" target="#b10">[10,</ref><ref type="bibr">100]</ref>, respectively. The parameters corresponding to the best nMSE were used to report the final results. For other compared systems, the procedures to tune the parameters are analogous to ensure the fair comparison. Considering one fold as an example, we observed that our model reached the optimal performance at λ = 1, µ = 0.01 and θ = 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">On Model Comparison</head><p>To demonstrate the effectiveness of our proposed TMALL model, we carried out experiments with several state-of-theart multi-view learning approaches:</p><p>• Early Fusion. The first baseline concatenates the features extracted from the four modalities into a single joint feature vector, on which traditional machine learning models can be applied. In this work, we adopted the widely used regression model-SVR, and implemented it with the help of scikit-learn <ref type="bibr" target="#b23">[23]</ref>. • Late Fusion. The second baseline first separately predicts the popularity of micro-videos from each modality via SVR model, and then linearly integrates them to obtain the final results. • regMVMT. The third baseline is the regularized multi-view learning model <ref type="bibr" target="#b37">[37]</ref>. This model only regulates the relationships among different views within the original space. • MSNL. The fourth one is the multiple social network learning (MSNL) model proposed in <ref type="bibr" target="#b27">[27]</ref>. This model takes the source confidence and source consistency into consideration. • MvDA. The fifth baseline is a multi-view discriminant analysis (MvDA) model <ref type="bibr" target="#b14">[14]</ref>, which aims to learn a single unified discriminant common space for multiple views by jointly optimizing multiple view-specific transforms, one for each view. The model exploits both the intra-view and inter-view correlations.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the performance comparison among different models. From this table, we have the following observations: 1) TMALL outperforms the Early Fusion and Late Fusion. Regarding the Early Fusion, features extracted from various sources may not fall into the same semantic space. Simply appending all features actually brings in a certain amount of noise and ambiguity. Besides, Early Fusion may lead to the curse of dimensionality since the final feature vector would be of very high dimension. For the Late Fusion, the fused result however might not be reasonably accurate due to two reasons. First, a single modality might not be sufficiently descriptive to represent the complex semantics of the videos. Separate results would be thus suboptimal and the integration may not result in a desired outcome. Second, it is labor-intensive to tune the fusion weights for different modalities. Even worse, the optimal parameters for one application cannot be directly applied to another one. 2) TMALL achieves better performance, as compared with regMVMT and MSNL. This could be explained that linking different modalities via a unified latent space is better than imposing disagreement penalty directly over original spaces.</p><p>3) The less satisfactory performance of MvDA indicates that it is necessary to explore the consistency among different modalities when building the latent space. And 4) as compared to the multiview learning baselines, such as regMVMT, MSNL, and MvDA, our model stably demonstrates its advantage. This signals that the proposed transductive models can achieve higher performance than inductive models under the same experimental settings. This can be explained by the fact that TMALL leverages the knowledge of testing samples. Moreover, we performed the paired t-test between TMALL and each baseline on 10-fold cross validation. We found that all the p-values are much smaller than 0.05, which shows that the performance improvements of our proposed model over other baselines are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">On Modality Comparison</head><p>To verify the effectiveness of multi-modal integration, we also conducted experiments over different modality combinations of the four modalities. Table <ref type="table" target="#tab_1">2</ref> summarizes the multi-modal analysis and the paired t-test results. It is obvious that the more modalities we incorporated, the better performance we can obtain. This implies the complementary relationships rather than mutual conflicting relationships among the different modalities. Moreover, we found that removing features from any of these four modalities suffers from a decrease in performance.</p><p>In a sense, this is consensus with the old saying "two heads are better than one". Additionally, as the performance obtained from different combinations are not the same, this validates that incorporating β which controls the confidence of different modalities is reasonable. Interestingly, we observed that the combination without social modality obtains the worst result which indicates that the social modality plays a pivotal role in micro-video propagation, as compared to visual, textual or acoustic modality. This also validates that the features developed from social modality are much discriminative, even though they are with low-dimensions. On the other hand, the textual modality contributes the least among all modalities, as the performance of our model without textual modality still achieves good performance. This may be </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">On Visual Feature Comparison</head><p>To further examine the discriminative visual features we extracted, we conducted experiments over different kinds of visual features using TMALL. We also performed significant test to validate the advantage of combining multiple features. Table <ref type="table" target="#tab_2">3</ref> comparatively shows the performance of TMALL in terms of different visual feature configurations. It can be seen that the object, visual sentiment and aesthetic features achieve similar improvement in performance, as compared to color histogram features. This reveals that micro-videos' popularity is better reflected by their content, sentiment and design, including what objects they contain, which emotion they convey and what design standards they follow. This is highly consistent with our oberservations and also implies that micro-videos which aim to gain high popularity need to be well designed and considered more from the visual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Illustrative Examples</head><p>To gain the insights of the influential factors in the task of popularity prediction of micro-videos, we comparatively illustrate a few representative examples in Figure <ref type="figure" target="#fig_4">4</ref>. From this figure, we have the following observations: 1) Figure <ref type="figure" target="#fig_4">4</ref>(a) shows three micro-video pairs. Each of the three microvideo pairs describes the similar semantics, i.e., animals, football game and sunset, respectively, but they were published by different users. The publishers of the videos in top row are much more famous than those of the bottom. We found that the corresponding popularity of micro-videos in the second row are much lower than those in the first row, although they have no significant difference from the perspective of video contents, which clearly justifies the importance of social modality. 2) Figure <ref type="figure" target="#fig_4">4</ref>(b) illustrates three micro-video pairs, where each pair of micro-videos were published by the same user. However, the micro-videos in the first row achieve much higher popularity than those in the second row, which demonstrates that the contents of micro-videos also contribute to their popularity. In  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Complexity Analysis</head><p>To theoretically analyze the computational cost of our proposed TMALL model, we first compute the complexity in the construction of H and g, as well as the inverse of matrices H and G. The construction of H has the time complexity of O(K 2 (N + M )). Fortunately, H keeps the same in each iteration, and thus can be computed by offline. The computation of g needs the time cost O(K(N + M ) 2 ). In addition, computing the inverse of H and G has the complexity of O(K 3 ) and O((N + M ) 3 ) , respectively. The computation cost of β in Eqn. <ref type="bibr" target="#b17">(17)</ref> is O(K 2 ). Therefore, the speed bottleneck lies in the computation of the inverse of G. In practice, the proposed TMALL model converges very fast, which on average takes less than 10 iterations. Overall, the learning process over 720 micro-videos can be accomplished within 50 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION AND FUTURE WORK</head><p>This paper presents a novel transductive multi-modal learning method (TMALL), to predict the popularity of micro-videos. In particular, TMALL works by learning an optimal latent common space from multi-modalities of the given micro-videos, in which the popularity of microvideos are much more distinguishable. The latent common space is capable of unifying and preserving information from different modalities, and it helps to alleviate the modality limitation problem. To verify our model, we built a benchmark dataset and extracted a rich set of popularityoriented features to characterize micro-videos from multiple perspectives. By conducting extensive experiments, we draw the following conclusions: 1) the optimal latent common space exists and works; 2) the more modalities we incorporate to learn the common space, the more discriminant it is; and 3) the features extracted to describe the social and content influence are representative. As a side research contribution, we have released the dataset, codes and parameters to facilitate other researchers. In the future, we plan to incorporate the cross-domain knowledge, such as the hot topics on Twitter, to enhance the performance of popularity prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Duration distribution over our collected 303, 242 micro-videos.</figDesc><graphic coords="2,74.62,53.81,194.38,100.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Micro-video popularity prediction via our proposed TMALL model.</figDesc><graphic coords="3,87.32,53.58,431.67,155.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of the number of comments, likes, reposts and loops of micro-videos in our dataset.</figDesc><graphic coords="4,71.02,53.80,201.60,102.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Illustration of three micro-video pairs, and each pair was published by two distinct users. The publishers of the videos in top row are much more famous than those of the bottom. (b) Illustration of three micro-video pairs, and each pair was published by the same user. The videos in the first row are much more acoustically comfortable, visually joyful, and aesthetically beautiful than those in the second row.(c) Illustration of three popular micro-videos with different textual descriptions, which contains superstar names, hot events, and detail information, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparative illustration of video examples. They respectively justify the importance of social, acoustic as well as visual, and textual modalities. We use three key frames to represent each video. particular, the comparisons in Figure 4(b), from left to right, are i) the existence of 'skillful pianolude' compared with 'noisy dance music', ii) 'funny animals' compared with 'motionless dog', and iii) 'beautiful flowers' compared with 'gloomy sky'. These examples indicate the necessity of developing acoustic features, visual sentiment and visual aesthetic features for the task of micro-video popularity. And 3) Figure 4(c) shows a group of micro-videos, whose textual descriptions contain either superstar names, hot hashtags, or informative descriptions. These micro-videos received a lot of loops, comments, likes and reposts. These examples thus reflect the value of textual modality.</figDesc><graphic coords="9,58.52,335.24,489.47,73.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Performance comparison between our proposed TMALL model and several state-of-the- art baselines in terms of nMSE.</head><label>1</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>nMSE</cell><cell>p-value</cell></row><row><cell>Early Fusion</cell><cell>59.931 ± 41.09</cell><cell>9.91E-04</cell></row><row><cell>Late Fusion</cell><cell>8.461 ± 5.34</cell><cell>3.25E-03</cell></row><row><cell>regMVMT</cell><cell>1.058 ± 0.05</cell><cell>1.88E-03</cell></row><row><cell>MSNL</cell><cell>1.098 ± 0.13</cell><cell>1.42E-02</cell></row><row><cell>MvDA</cell><cell>0.982 ± 7.00E-03</cell><cell>9.91E-04</cell></row><row><cell>TMALL</cell><cell>0.979 ± 9.42E-03</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Performance comparison among different modality combinations with respect to nMSE. We denote T, V, A and S as textual, visual, acoustic and social modality, respectively.</head><label>2</label><figDesc></figDesc><table><row><cell>View combinations</cell><cell>nMSE</cell><cell>p-value</cell></row><row><cell>T+V+A</cell><cell>0.996 ± 4.20E-03</cell><cell>2.62E-05</cell></row><row><cell>T+A+S</cell><cell>0.982 ± 4.27E-03</cell><cell>2.59E-05</cell></row><row><cell>T+V+S</cell><cell>0.982 ± 4.13E-03</cell><cell>3.05E-04</cell></row><row><cell>V+A+S</cell><cell>0.981 ± 5.16E-03</cell><cell>2.16E-05</cell></row><row><cell>T+V+A+S</cell><cell>0.979 ± 9.42E-03</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Performance comparison among different visual features with respect to nMSE.</head><label>3</label><figDesc></figDesc><table><row><cell>Features</cell><cell>nMSE</cell><cell>p-value</cell></row><row><cell>Color Histogram</cell><cell>0.996 ± 6.88E-03</cell><cell>1.94E-04</cell></row><row><cell>Object Feature</cell><cell>0.994 ± 6.71E-03</cell><cell>2.47E-04</cell></row><row><cell>Visual Sentiment</cell><cell>0.994 ± 6.72E-03</cell><cell>2.49E-04</cell></row><row><cell>Aesthetic Feature</cell><cell>0.984 ± 6.95E-03</cell><cell>4.44E-01</cell></row><row><cell>ALL</cell><cell>0.979 ± 9.42E-03</cell><cell>-</cell></row><row><cell cols="3">caused by the sparse textual description, which is usually</cell></row><row><cell cols="2">given in one short sentence.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://snapchat.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://vine.co.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Loops refer to the times a micro-video has been viewed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://blog.vine.co.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://goo.gl/YYnBbd.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://twitter.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>The dataset can be accessible via http://acmmm2016.wix.com/ micro-videos.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://rankzoo.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>http://www.ee.columbia.edu/˜subh/Software.php.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9"><p>http://live.ece.utexas.edu/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arousal increases social transmission of information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="891" to="893" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What makes online content viral</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Milkman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="192" to="205" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards a comprehensive computational model foraesthetic assessment of videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="361" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent factors of visual popularity prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cappallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view clustering via canonical correlation analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-view learning in the presence of view disagreement</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>CoRR, abs/1206.3242</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view subspace clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4238" to="4246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image popularity prediction in social media using sentiment and context features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="907" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting the popularity of web 2.0 items based on user comments</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting popular messages in twitter</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on World Wide Web</title>
		<meeting>the ACM International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="57" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-view discriminant analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What makes an image popular</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on World Wide Web</title>
		<meeting>the ACM International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>NIPS Foundation</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-reference audio quality assessment for online live music recordings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-modal clique-graph matching for view-based 3d model retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2103" to="2116" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph-based characteristic view set extraction and matching for 3d model retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="429" to="442" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">nobody comes here anymore, it&apos;s too crowded&quot;; predicting image popularity on flickr</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Mcparlane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moshfeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia Retrieval</title>
		<meeting>the ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>NIPS Foundation</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond doctors: future health prediction from multimedia and multimodal observations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">6 seconds of sound and vision: Creativity in micro-videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>O'hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schifanella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trevisiol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4272" to="4279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A co-regularization approach to semi-supervised learning with multiple views</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiple social network learning and its application in volunteerism tendency prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interest inference via structure-constrained multi-source multi-task learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2371" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Predicting popularity of online videos using support vector regression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rokita</surname></persName>
		</author>
		<idno>CoRR, abs/1510.06223</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unified video annotation via multigraph learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="733" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multimodal graph-based reranking for web image search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4649" to="4661" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual classification by 1 -hypergraph modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2564" to="2574" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Music emotion recognition by multi-label multi-layer multi-instance multi-view learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring inter-feature and inter-class relationships with deep neural networks for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online collaborative learning for open-vocabulary visual classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Play and rewind: optimizing binary representations of videos by self-supervised temporal hashing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Inductive multi-task learning with multiple view data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
