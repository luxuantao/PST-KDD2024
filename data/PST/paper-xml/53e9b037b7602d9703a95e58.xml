<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gabor feature based robust representation and classification for face recognition with Gabor occlusion dictionary</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-07-06">6 July 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Meng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comp.polyu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">C K</forename><surname>Shiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gabor feature based robust representation and classification for face recognition with Gabor occlusion dictionary</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-07-06">6 July 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">0290A04FFDEBCFF959EE671492808409</idno>
					<idno type="DOI">10.1016/j.patcog.2012.06.022</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Robust face recognition Gabor occlusion dictionary Sparse representation Collaborative representation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>By representing the input testing image as a sparse linear combination of the training samples via l 1 -norm minimization, sparse representation based classification (SRC) has shown promising results for face recognition (FR). Particularly, by introducing an identity occlusion dictionary to code the occluded portions of face images, SRC could lead to robust FR results against face occlusion. However, the l 1 -norm minimization and the high number of atoms in the identity occlusion dictionary make the SRC scheme computationally very expensive. In this paper, a Gabor feature based robust representation and classification (GRRC) scheme is proposed for robust FR. The use of Gabor features not only increases the discrimination power of face representation, but also allows us to compute a compact Gabor occlusion dictionary which has much less atoms than the identity occlusion dictionary. Furthermore, we show that with Gabor feature transformation, l 2 -norm could take the role of l 1 -norm to regularize the coding coefficients, which reduces significantly the computational cost in coding occluded face images. Our extensive experiments on benchmark face databases, which have variations of lighting, expression, pose and occlusion, demonstrated the high effectiveness and efficiency of the proposed GRRC method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic face recognition (FR) is one of the most visible and challenging research topics in computer vision, machine learning and biometrics <ref type="bibr" target="#b10">[11]</ref>. Although facial images have a high dimensionality, they usually lie in a lower dimensional subspaces or sub-manifolds. Therefore, subspace learning and manifold learning methods have been dominantly and successfully used in appearance based FR <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. The classical Eigenface and Fisherface <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> algorithms consider only the global scatter of training samples and they fail to reveal the essential data structures nonlinearly embedded in high dimensional space. The manifold learning methods were proposed to overcome this limitation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, and the representative manifold learning methods include locality preserving projection (LPP) <ref type="bibr" target="#b6">[7]</ref>, local discriminant embedding (LDE) <ref type="bibr" target="#b7">[8]</ref>, unsupervised discriminant projection (UDP) <ref type="bibr" target="#b8">[9]</ref>, etc. Besides, in order to better exploit the prior knowledge that face images from a single subject could construct a subspace, nearest subspace (NS) classifiers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> are developed, which are usually superior to the simple nearest neighbor (NN) classifier.</p><p>The success of manifold learning implies that the high dimensional face images can be sparsely represented or coded by the representative samples on the manifold. Recently an interesting work was reported by Wright et al. <ref type="bibr" target="#b9">[10]</ref>, where the sparse representation (SR) technique is employed for FR. In Wright et al.'s pioneer work, the training face images are used as the dictionary to code an input testing image as a sparse linear combination of them via l 1 -norm minimization. The SR based classification (SRC) of face images is conducted by evaluating which class of training samples could result in the minimal reconstruction error of the input testing image with the associated sparse coding coefficients. To make the l 1 -norm sparse coding computationally feasible, in general the dimensionality of the training and testing face images should be reduced, or a set of features could be extracted from the original image for SRC. In the case of FR without occlusion, Wright et al. <ref type="bibr" target="#b9">[10]</ref> tested different types of features, including Eigenface <ref type="bibr" target="#b1">[2]</ref>, Randomface <ref type="bibr" target="#b9">[10]</ref> and Fisherface <ref type="bibr" target="#b2">[3]</ref>, and they claimed that SRC is insensitive to feature types when the feature dimension is large enough. In the case of FR with occlusion/corruption, an occlusion dictionary was introduced in SRC to code the occluded/corrupted components <ref type="bibr" target="#b9">[10]</ref>. Since the occluded face image can be viewed as a summation of non-occluded face image and the occlusion, with the sparsity constraint the non-occluded face part is expected to be sparsely coded by the training face dictionary only, while the occlusion part is expected to be coded by the occlusion dictionary only.</p><p>Consequently, the classification can be performed based on the reconstruction residuals using the coding coefficients over the training face dictionary. Such a scheme has shown to be effective in overcoming the problem of face occlusion.</p><p>The success of SRC boosts the research of sparsity based FR, and many works have been consequently reported. Gao et al. <ref type="bibr" target="#b27">[28]</ref> proposed the kernel sparse representation for FR and image classification, while FR with continuous occlusion and misalignment via sparse representation have been presented in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Elhamifar et al. <ref type="bibr" target="#b29">[30]</ref> discussed classification using structure sparse representation to exploit the block structure of the dictionary, and Yang et al. <ref type="bibr" target="#b30">[31]</ref> proposed a robust sparse coding model with a maximum likelihood estimator like fidelity term. Moreover, learning a discriminative dictionary under the sparse representation framework for classification has also attracted much attention. Zhang and Li <ref type="bibr" target="#b39">[40]</ref> introduced discrimination information into the algorithm of K-SVD <ref type="bibr" target="#b38">[39]</ref> by learning a linear classifier; Jiang et al. <ref type="bibr" target="#b40">[41]</ref> further enhanced dictionary's discriminative ability by adding label consistent information. Very recently, Yang et al. <ref type="bibr" target="#b41">[42]</ref> imposed Fisher discrimination criterion on the coding residuals and coefficients, and proposed a Fisher discrimination dictionary learning method.</p><p>Although the SRC based FR scheme proposed in <ref type="bibr" target="#b9">[10]</ref> is very creative and effective, there are two issues to be further addressed. First, the features of Eigenface, Randomface and Fisherface tested in <ref type="bibr" target="#b9">[10]</ref> are all holistic features. Since in practice the number of training samples is often limited, such holistic features cannot effectively handle the variations of illumination, expression, pose and local deformations. The claim made in <ref type="bibr" target="#b9">[10]</ref> that feature extraction is not so important to SRC actually holds only for holistic features. Second, the occlusion matrix proposed in <ref type="bibr" target="#b9">[10]</ref> is an orthogonal matrix, such as the identify matrix, Fourier bases or Haar wavelet bases, etc. However, the number of atoms required in the orthogonal occlusion matrix is very high. For example, if the dimensionality of features used in SRC is 3000, then a 3000 Â 3000 occlusion matrix is needed. Such a big occlusion matrix makes the sparse coding process very computationally expensive, and even prohibitive. These two issues are not fully solved by the sparsity based FR improvers <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. For instance, only holistic features are considered in <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>, FR with occlusion is ignored in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, and no occlusion dictionary is learned in <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>.</p><p>In our previous work <ref type="bibr" target="#b37">[38]</ref>, the Gabor features were adopted for SRC and a Gabor occlusion dictionary was learned under the sparse representation framework. Although the so-called Gaborbased SRC scheme improves much the accuracy and efficiency of original SRC, the l 1 -norm sparsity constraint on the coding coefficients still makes it time consuming. Very recently, Zhang et al. <ref type="bibr" target="#b31">[32]</ref> showed that it is the collaborative representation mechanism (i.e., representing the query image collaboratively by samples from all the classes) but not the l 1 -norm sparsity constraint on coding coefficients that makes SRC effective for FR. In light of this finding, in this paper we propose a Gabor-feature based robust representation and classification (GRRC) scheme for FR, which will not only be robust to face occlusion but also have much higher computational efficiency than the previous methods such as Gabor-based SRC.</p><p>The Gabor filter was first introduced by David Gabor in 1946 <ref type="bibr" target="#b13">[14]</ref>, and was later shown as models of simple cell receptive fields <ref type="bibr" target="#b14">[15]</ref>. The Gabor filters, which could effectively extract the image local directional features on multiple scales, have been successfully and prevalently used in FR <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Very recently, Zhou and Sadka <ref type="bibr" target="#b46">[47]</ref> proposed to combine the perceptual features by Gabor filtering with diffusion distance for FR; Du and Ward <ref type="bibr" target="#b47">[48]</ref> proposed to perform FR with non-uniform multilevel selection of Gabor features instead of the uniform down-sampling of Gabor features; a local Gabor based FR with improved accuracy by the selection of Gabor jets was presented in <ref type="bibr" target="#b48">[49]</ref>; and multimodal FR using Gabor feature was presented in <ref type="bibr" target="#b49">[50]</ref>. All of these methods lead to state-of-the-art results. The local Gabor features are less sensitive to variations of illumination, expression and poses than the holistic features such as Eigenface and Randomface <ref type="bibr" target="#b9">[10]</ref>. In the proposed GRRC, the use of Gabor kernels will not only improve much the FR accuracy, it will also allow us to learn a compact occlusion dictionary to deal with face occlusions. Compared with the occlusion dictionary used in SRC, the number of atoms is significantly reduced (often with a ratio of 40:1-50:1 in our experiments) in the Gabor occlusion dictionary (GOD) used in GRRC. Particularly, it is found that the coding coefficients over the compact GOD can be regularized by l 2 -norm, instead of the l 1 -norm adopted in Gabor-based SRC <ref type="bibr" target="#b37">[38]</ref>. This significantly reduces the computational cost in coding occluded face images. Our experiments on benchmark face databases clearly validate the performance of the proposed GRRC method.</p><p>The rest of the paper is organized as follows. Section 2 briefly reviews SRC, collaborative representation based classification (CRC) and Gabor filters. Section 3 presents the proposed GRRC algorithm. Section 4 conducts experiments. Section 5 gives some discussions and Section 6 concludes the paper.</p><p>Table <ref type="table">1</ref> summarizes the abbreviations used throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sparse representation based classification (SRC)</head><p>The sparse representation based classification (SRC) method was presented in <ref type="bibr" target="#b9">[10]</ref> for robust face recognition (FR). Denote by A i ¼ ½s i,1 ,s i,2 ,:::,s i,n i A R mÂn i the set of training samples of the ith object class, where s i,j , j ¼1,2,y,n i , is an m-dimensional vector stretched by the jth sample of the ith class. For a test sample y 0 A R m from the ith class, intuitively, y 0 could be well approximated by the linear combination of the samples within A i , i.e., y 0 %</p><formula xml:id="formula_0">P n i j ¼ 1 a i,j s i,j ¼ A i a i</formula><p>, where a i ¼ ½a i,1 ,a i,2 ,:::,a i,n i T A R n i is the coding vector. Suppose we have K object classes, and let A¼[A 1 , A 2 ,y, A K ] be the concatenation of the n training samples from all the K classes, where n¼n 1 þn 2 þ?þn K , then the linear representation of y 0 can be written in terms of all training samples as y 0 EAa, where a¼[a 1 ;?;a i ;?; a K ]¼[0,y,0,a i,1 ,a i,2 ,:::,a i,n i , 0,y,0] T . In SRC without occlusion, first y 0 is sparsely coded on A via l 1 -minimization â ¼ argmin a :y 0 ÀAa:</p><formula xml:id="formula_1">2 2 þ l:a: 1 n o<label>ð1Þ</label></formula><p>where l is a scalar constant. Then classification is made by</p><formula xml:id="formula_2">identity y 0 À Á ¼ arg min i e i f g<label>ð2Þ</label></formula><formula xml:id="formula_3">where e i ¼ :y 0 ÀA i d i â À Á : 2 , â ¼ ½d 1 â À Á ; Á Á Á ; d i â À Á ; Á Á Á ; d K â À Á ,<label>and</label></formula><formula xml:id="formula_4">d i U ð Þ : R n -R n i</formula><p>is the characteristic function which selects the coefficients associated with the ith class.</p><p>In SRC with occlusion or corruption, the test sample y is rewritten as</p><formula xml:id="formula_5">y ¼ y 0 þe 0 ¼ Aa þe 0 ¼ A,A e ½ a a e " # ¼ Bx<label>ð3Þ</label></formula><formula xml:id="formula_6">where B ¼ A, A e ½ A R mÂ n þ ne ð Þ</formula><p>, and the clean face image y 0 and the corruption error e 0 have sparse representations over the training sample dictionary A and occlusion dictionary A e A R mÂne , respectively. In <ref type="bibr" target="#b9">[10]</ref>, the corruption dictionary A e was set as an orthogonal matrix, such as identity matrix, Fourier bases, Haar wavelet bases, and so on. The sparse coding coefficient x could be solved via l 1 -minimization like Eq. ( <ref type="formula" target="#formula_1">1</ref>) and the classification is done via Eq. ( <ref type="formula" target="#formula_5">3</ref>) with e i ¼ :yÀA i d i a</p><p>ð ÞÀA e a e : 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Collaborative representation based classification (CRC)</head><p>Though it was claimed in <ref type="bibr" target="#b9">[10]</ref> that the l 1 -norm sparsity imposed on coding coefficient a in Eq. ( <ref type="formula" target="#formula_1">1</ref>) is the key for the success of SRC, recently it has been shown in <ref type="bibr" target="#b31">[32]</ref> that it is the collaborative representation (CR) mechanism, but not the l 1 -norm sparsity on a, that truly makes SRC effective for face classification. Using l 2 -norm to regularize a leads to similar FR results.</p><p>The robustness to outliers in SRC actually comes from the use of l 1 -norm to model the coding error, i.e., :a e : 1 . Without considering the robustness to outliers, the coding model of collaborative representation for classification (CRC) <ref type="bibr" target="#b31">[32]</ref> is â1 ¼ arg min a :y 0 ÀAa:</p><formula xml:id="formula_7">2 2 þl:a: 2 2 n o<label>ð4Þ</label></formula><p>The classification of CRC is performed by checking which class yields the minimal regularized coding error, which is similar to that of SRC.</p><p>It is shown in <ref type="bibr" target="#b31">[32]</ref> that CRC has very competing performance with SRC in FR without occlusion but with much faster speed. However, the standard CRC in Eq. (4) does not aim to deal with FR with occlusion. Compared to CRC <ref type="bibr" target="#b31">[32]</ref> which has no occlusion dictionary, in this paper Gabor occlusion dictionary is learnt to effectively handle occluded portions in facial images. In addition, the use of Gabor features instead of original image intensity also enhances much the discrimination of face representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Gabor features</head><p>The Gabor filters (kernels) with orientation m and scale n are defined as <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_8">c m,n ðzÞ ¼ :k m,n : 2 s 2 e À:km,n: 2 :z: 2 =2s 2 ðe ikm,nz Àe Às 2 =2 Þ ð<label>5Þ</label></formula><p>where z ¼(x,y) denotes the pixel, :.: denotes the norm operator, and the wave vector k m,n is defined as</p><formula xml:id="formula_9">k m,n ¼ k n e if m with k n ¼ k max /f n</formula><p>and f m ¼ pm=8. k max is the maximum frequency, and f is the spacing factor between kernels in the frequency domain. In addition, s determines the ratio of the Gaussian window width to wavelength.</p><p>Convolving image Img with a Gabor kernel c m,n outputs</p><formula xml:id="formula_10">G m,n z ð Þ ¼ Img z ð Þnc m,n z ð Þ</formula><p>, where ''n'' denotes the convolution operator. The complex Gabor filtering coefficient G m,n (z) can be rewritten as</p><formula xml:id="formula_11">G m,n z ð Þ ¼ M m,n z ð Þexpðiy m,n<label>ðzÞÞ</label></formula><p>with M m,n being the magnitude and y m,n being the phase. It is known that magnitude information contain the variation of local energy in the image. As a multi-scale and multi-orientation feature extraction technique, Gabor filtering generates highly redundant features, and thus it is necessary to down-sample the filtering outputs to reduce the Gabor feature dimension as well as the time and space complexity. In <ref type="bibr" target="#b16">[17]</ref>, the augmented Gabor feature vector v is defined via uniform down-sampling, normalization and concatenation of the Gabor filtering coefficients:</p><formula xml:id="formula_12">v ¼ a r ð Þ 0,0 ; a r ð Þ 1,0 ; Á Á Á ; a r ð Þ 7,4</formula><p>where a r ð Þ m,n is the concatenated column vector of magnitude matrix M r ð Þ m,n down-sampled by a factor of r. Images from the same face, taken at (nearly) the same pose but under varying illumination, often lie in a low-dimensional linear subspace known as the harmonic plane or illumination cone <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>. This implies that if there are only variations of illumination, SRC can work very well. However, SRC with the holistic image features is less effective when there are local deformations of face images, such as certain amount of variations of expressions and pose.</p><p>The augmented Gabor face feature vector v, which is a local feature descriptor, can not only enhance the face feature but also tolerate image local deformation to some extent. So we propose to use v to replace the holistic face features for face representation, and the Gabor-feature based representation without face occlusion is</p><formula xml:id="formula_13">v y 0 À Á ¼ XðA 1 Þb 1 þ XðA 2 Þb 2 þ Á Á Á þXðA K Þb K ¼ XðAÞb<label>ð6Þ</label></formula><p>where</p><formula xml:id="formula_14">X A ð Þ ¼ X A 1 ð Þ X A 2 ð Þ ÁÁÁ X A K ð Þ ½ , X A i ð Þ¼ v s i,1 À Á ,v s i,2 À Á ,. . .,v s i,n i À Á Â Ã , b ¼ b 1 ; b 2 ; Á Á Á ; b K Â Ã</formula><p>When the query face image is occluded, similar to SRC, an occlusion dictionary with Gabor features could be introduced to code the occlusion components, and the Gabor-feature based robust representation could be formulated as</p><formula xml:id="formula_15">vðyÞ ¼ ½XðAÞ, XðA e Þ½b; b e ¼ X B ð Þx<label>ð7Þ</label></formula><p>where X(A e ) is the Gabor-feature based occlusion dictionary, and b e is the coding vector of the input Gabor feature vector v(y) over X(A e ).</p><p>For the convenience of expression, we call the representation in either Eq. ( <ref type="formula" target="#formula_13">6</ref>) (for FR without occlusion) or Eq. ( <ref type="formula" target="#formula_15">7</ref>) (for FR with occlusion) the Gabor-feature based robust representation (GRR), and the representation vector in the GRR model can be solved by</p><formula xml:id="formula_16">min b :v y 0 À Á ÀX A ð Þb: 2 2 þl:b: lp n o or min x :v y ð ÞÀX B ð Þx: 2 2 þl:x: lp n o<label>ð8Þ</label></formula><p>where :d: lp means the l p -norm, and p ¼1 or 2 in our paper. In the case of occlusion, the selection of occlusion dictionary X(A e ) has a big affect on the performance of GRR, and thus one key issue is how to define X(A e ) to make the GRR effective and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discussions on occlusion dictionary</head><p>SRC <ref type="bibr" target="#b9">[10]</ref> is successful in solving the problem of face occlusion by introducing an occlusion dictionary A e to code the occluded face components; however, one drawback of SRC is that the number of atoms in the used occlusion dictionary is very big. More specifically, the identity matrix was employed in SRC so that the number of atoms equals to the dimensionality of the image feature vector. For example, if the feature vector has a dimensionality of 3000, then the occlusion dictionary is of size 3000 Â 3000.</p><p>Such a high dimensional dictionary makes the sparse coding very expensive, and even computationally prohibitive. Suppose the size of the dictionary is m Â n, then the empirical complexity of the commonly used l 1 -regularized sparse coding methods (such as l 1 _ls <ref type="bibr" target="#b23">[24]</ref>, l 1 _magic <ref type="bibr" target="#b24">[25]</ref>, and MOSEK <ref type="bibr" target="#b25">[26]</ref>) to solve Eq. ( <ref type="formula" target="#formula_1">1</ref>) is O(m 2 n e ) with eE1.5 <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>. So if the number of atoms (i.e., n) in the occlusion dictionary is too big, the computational cost will be huge, especially in dealing with FR with occlusion.</p><p>By using Gabor features for face representation, the feature dictionary A and the occlusion dictionary A e in Eq. ( <ref type="formula" target="#formula_5">3</ref>) will be transformed into the Gabor feature dictionary X(A) and the Gaborfeature based occlusion dictionary X(A e ) in Eq. <ref type="bibr" target="#b6">(7)</ref>. Fortunately, X(A e ) is compressible. This can be easily illustrated by Fig. <ref type="figure">1</ref>.</p><p>Fig. <ref type="figure">1</ref>(a) illustrates the process of Gabor filtering. It is easy to see there are a rich amount of redundancies in the filtering responses across different scales and orientations. Therefore after the band-pass Gabor filtering, a uniform spatial down-sampling with a factor of r is conducted to form the augmented Gabor feature vector v, as indicated by the red pixels in Fig. <ref type="figure">1(b</ref>). The spatial down-sampling is performed for all the Gabor filtering outputs along different orientations and on different scales. Therefore, the number of (spatial) pixels in the augmented Gabor feature vector v is 1/r times that of the original face image; meanwhile, at each location, e.g., P1 or P2 in Fig. <ref type="figure">1(b)</ref>, there is a set of directional and scale features extracted by Gabor filtering in the neighborhood (e.g., the circles centered on P1 and P2). Certainly, the directional and scale features at the same spatial location have some correlation, and there are often some overlaps between the supports of Gabor filters, which make the Gabor features at neighboring positions also have some redundancies.</p><p>Considering that ''occlusion'' is a phenomenon of spatial domain, a spatial down-sampling of the Gabor features with a factor of r implies that we can use approximately 1/r times the occlusion bases to code the Gabor features of the occluded face image. In other words, the Gabor-feature based occlusion dictionary X(A e ) can be compressed because the Gabor features are redundant as we discussed above. To validate this conclusion, we suppose that the image size is 50 Â 50, and in the original SRC the occlusion dictionary is an identity matrix A e ¼ IA R 2500Â2500 . Then the Gabor-feature based occlusion matrix X(A e ) A R 2560Â2500 , where the dimensionality of augmented Gabor feature is 2560 with r¼39.06, m¼{0,y,7}, n¼{0,y,4}. Fig. <ref type="figure">2</ref> shows the singular values of X(A e ). Obviously, although all the basis vectors of identity matrix I (i.e., A e ) have equal importance, only a few (60, with energy proportion of 99.67%) singular vectors of X(A e ) have significant singular values, as shown in Fig. <ref type="figure">2</ref>. This implies that X(A e ) can be much more compactly represented by using only a few atoms generated from X(A e ), often with a compression ratio about r:1. For example, in this experiment we have 2500/ 60¼ 41.7Er¼39.06. Next we present an algorithm to compute a more compact occlusion dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gabor occlusion dictionary (GOD) computing</head><p>Now that X(A e ) is compressible, we propose to compute a compact occlusion dictionary from it with suitable regularization on the coefficients. Here a compact dictionary, denoted by D A R mÂn , refers to a dictionary which has much less columns (i.e., the so-called atoms) than rows (i.e., n5m). From the view of handling occlusion, a compact occlusion dictionary means that the learnt dictionary atoms have lower correlation and stronger ability in representing face occlusions. In addition, the coding speed would be much faster because of the reduced size of occlusion dictionary. We call the computed compact occlusion dictionary Gabor occlusion dictionary (GOD) and denote it as C. Then we could replace X(A e ) by C in the GRR based FR.</p><p>For the convenience of expression, we denote by Z ¼ XðA e Þ ¼ ½z 1 ,. . .,z ne A R mrÂne the original Gabor-feature based occlusion matrix, with each column z i being the augmented Gabor-feature vector generated from each atom of A e . The compact occlusion dictionary to be computed is denoted by C ¼ ½d 1 ,d 2 ,:::,d q A R mrÂq , where q can be set as slightly less than n e /r in practice. It is required that each occlusion basis d j , j ¼1, 2, y, q, is a unit column vector, i.e. d T j d j ¼ 1. Since we want to replace Z by C, it is expected that the original dictionary Z can be well represented by C with the representation coefficients being regularized via l p -norm regularization. Obviously, p ¼1 means that we require sparse representation on the learnt GOD. Inspired by the success of l 2 -norm regularization in CRC <ref type="bibr" target="#b31">[32]</ref>, we can also use l 2 -norm coefficient regularization. With such considerations, the objective function for determining C is defined as</p><formula xml:id="formula_17">J C,K ¼ arg min C,K :ZÀCK: 2 F þ z:K: lp n o , s:t: d T j d j ¼ 1, 8j<label>ð9Þ</label></formula><p>where K is the representation matrix of Z over dictionary C, z is a positive scalar that balances the F-norm term and the l p -norm term (here p ¼1 for :U: 1 and p ¼2 for :U: 2 F ). Eq. ( <ref type="formula" target="#formula_17">9</ref>) is a joint optimization problem of the occlusion dictionary C and the representation matrix K. Like in many multi-variable optimization problems, we solve Eq. ( <ref type="formula" target="#formula_17">9</ref>) by optimizing C and K alternatively. The optimization procedures are described in Table <ref type="table">2</ref>. Based on our experiments, the random initialization of the dictionary affects little the learned GOD as well as the final face recognition accuracy. In general, about 10 iterations are needed to stop the algorithm of GOD.</p><p>It is straightforward that the above GOD computing algorithm converges because in each iteration J C,K will decrease. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates an example of GOD on the AR database <ref type="bibr" target="#b20">[21]</ref>. Based on our experiments, on other datasets the algorithm of GOD learning also converges quickly. Consequently, in GRR we use the GOD C to replace the X(A e ) in Eq. <ref type="bibr" target="#b6">(7)</ref>. Finally, the coding problem in GRRC with face occlusion is</p><formula xml:id="formula_18">min x :v y ð ÞÀB C x C : 2 2 þl:x C : lp n o where B C ¼ ½XðAÞ C, x C ¼ ½b; b C<label>ð17Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">GRR based classification (GRRC)</head><p>The SRC scheme <ref type="bibr" target="#b9">[10]</ref> assumes that the face image representation residual is sparse, and thus uses the l 1 -norm to characterize the representation coefficients associated with the occlusion dictionary, i.e., the identity matrix. Because the number of atoms in the identity matrix is very big (equal to the dimensionality of face image), it is necessary to impose the l 1 -norm sparsity on the coding coefficients for a robust and unique representation, yet this makes the complexity of SRC very high. However, when Gabor feature is adopted, a compact GOD C (with only about 1/40 times the size of the identity matrix) can be learnt, and thus it may not be necessary to use the l 1 -norm sparsity to regularize the coding coefficients over the dictionary anymore.</p><p>For a given face Gabor feature v(y), often its dimensionality is much higher than the number of atoms in dictionary B C ¼[X(A)! ] after GOD computing, which means that the dictionary B C is not over-complete, and hence the system</p><formula xml:id="formula_19">wðyÞ % B C x C<label>ð18Þ</label></formula><p>is generally an over-determined system. This implies that the solution of Eq. ( <ref type="formula" target="#formula_19">18</ref>) is stable even without any regularization. Although Eq. ( <ref type="formula" target="#formula_19">18</ref>) is stable even without any regularization, a suitable the regularization (e.g., l 2 -norm) could make the representation more stable. In addition, the regularization will make the representation coefficients smaller, which would make Table <ref type="table">2</ref> Algorithm of Gabor occlusion dictionary (GOD) computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Initialize C</head><p>We initialize each column of G as a random vector with unit l 2 -norm. 2. Fix C and solve K By fixing C, the objective function in Eq. ( <ref type="formula" target="#formula_17">9</ref>) will be reduced to</p><formula xml:id="formula_20">J K ¼ arg min K :ZÀCK: 2 F þ z:K: lp n o<label>ð10Þ</label></formula><p>The minimization of Eq. ( <ref type="formula" target="#formula_20">10</ref>) for p ¼1 can be achieved by the l 1 -norm minimization techniques. In this paper, we use the algorithm in <ref type="bibr" target="#b23">[24]</ref>. The minimization of Eq. ( <ref type="formula" target="#formula_20">10</ref>) for p ¼ 2 could be efficiently solved since it has a closed-form least square solution <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Fix K and update C</head><p>Now the objective function is reduced to</p><formula xml:id="formula_21">J C ¼ arg min C :ZÀCK: 2 F n o , s:t: d T j d j ¼ 1, 8j<label>ð11Þ</label></formula><p>We can write matrix K as K¼[b 1 ;b 2 ,y,b p ], where b j , j¼ 1,2,y,q, is the row vector of K. We update the occlusion bases one by one. When updating d j , all the other columns of C, i.e., d l , l aj, are fixed. Then J C in Eq. ( <ref type="formula" target="#formula_21">11</ref>) is converted into</p><formula xml:id="formula_22">J dj ¼ arg min dj :ZÀ X l a j d l b l Àd j b j : 2 F , s:t: d T j d j ¼ 1<label>ð12Þ</label></formula><p>Let Y ¼ ZÀ P l a j d l b l , Eq. ( <ref type="formula" target="#formula_22">12</ref>) can be written as</p><formula xml:id="formula_23">J dj ¼ arg min dj :YÀd j b j : 2 F , s:t: d T j d j ¼ 1<label>ð13Þ</label></formula><p>Using Langrage multiplier, J dj is equivalent to</p><formula xml:id="formula_24">J dj ,g ¼ arg min dj tr ÀYb T j d T j Àd j Ub j Y T þ d j Uðb j b T j ÀgÞd T j þ g<label>ð14Þ</label></formula><p>where g is a scalar variable. Differentiating J dj ,g with respect to d j , and let it be 0, we have</p><formula xml:id="formula_25">d j ¼ Yb T j b j b T j Àg À1<label>ð15Þ</label></formula><p>Since b j b T j Àg is a scalar and g is a variable, the solution of Eq. ( <ref type="formula" target="#formula_25">15</ref>) under constrain d</p><formula xml:id="formula_26">T j d j ¼ 1 is d j ¼ Yb T j =:Yb T j : 2<label>ð16Þ</label></formula><p>Using the above procedures, we can update all the vectors d j , and hence the whole set C is updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Output C</head><p>Go back to step 2 until the values of J C,K in adjacent iterations are close enough, or the maximum number of iterations is reached. Finally, output C. the coefficients associated with wrong class have lower value. This increases the discrimination of representation coefficients, which benefits the final face recognition. In this paper we test the results by using both l 1 -norm and l 2 -norm to regularize the coding coefficients. We name the GRR based classification (GRRC) with l 1 -norm regularization GRRC_L 1 , and the GRRC with l 2 -norm regularization GRRC_L 2 . The GRRC algorithm is summarized in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Time complexity</head><p>The empirical complexity of the commonly used l 1 -regularized sparse coding methods is O(m 2 n e ) with eE1.5 <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>, while the time complexity of l 2 -norm regularized coding is only O(mn) <ref type="bibr" target="#b31">[32]</ref> for that the coding projection matrix could be computed offline, where m is facial feature dimensionality and n is the number of dictionary atoms. For GRRC, in Fourier domain it is very fast to extract Gabor features, whose time complexity could be negligible compared with that of l 1 -norm regularized sparse coding.</p><p>In the case of FR without occlusion, n is the number of training samples. Therefore, GRRC_L 1 has similar computational burden to SRC, but GRRC_L 2 has much lower time complexity than GRRC_L 1 and SRC. For FR without occlusion, there is a fast version of SRC, namely SRC using Hashing <ref type="bibr" target="#b45">[46]</ref>. This method is usually faster than the original SRC because the used random projection matrix is very sparse. So GRRC_L 1 would have a little higher time complexity than SRC using Hashing, but GRRC_L 2 is still much faster than SRC using Hashing.</p><p>In the case of FR with occlusion, it is easy to get that the time complexity of GRRC_L 1 is O(m 2 (nþm/r) e ), where rE40. This is much lower than SRC whose time complexity is O(m 2 (n þm) e ). Obviously, GRRC_L 2 's time complexity is O(m(n þm/r)) and it is the fastest one among the three methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In this section, we present experiments on benchmark face databases to demonstrate the superiority of GRRC to SRC. Before giving the detailed experimental results, we discuss the selection of Gabor features and regularization of GOD computing in Section 4.1. To evaluate more comprehensively the performance of GRRC, in Section 4.2 we first test FR with little deformation; then in Section 4.3 we demonstrate the robustness of GRRC to expression and pose variation; finally in Section 4.4 we test FR against block occlusion and real disguise. In our implementation of Gabor filters, the parameters are set as</p><formula xml:id="formula_27">K max ¼p/2, f ¼ ffiffiffi 2 p ,</formula><p>s¼1.5p, m¼{0,y,7}, n¼{0,y,4} by our experiences and they are fixed for all the experiments. The parameter l in GRRC should be set as a small positive value to make the representation more stable and the coding coefficient regularized. A large value of l would make the regularization too strong so that the signal representation fidelity can be reduced, resulting in the decrease of recognition accuracy. In the experiments, l in GRRC is fixed as 0.0005 for FR without and with occlusion. We also give the results of GRRC with l¼0.001 for FR without occlusion to show that GRRC is very robust to parameter's value. In addition, all the face images are cropped and aligned by using the location of eyes, which is provided by the face databases (for Mulit-PIE, we manually locate the positions of eyes).</p><p>In the following tables of this section, the results of competing methods with reference numbers are cropped from the original papers. All the other results are computed by us with reporting their best recognition rates. The Matlab source code of the proposed algorithm can be downloaded at http://www4.comp. polyu.edu.hk/$ cslzhang/code.htm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Gabor features and regularization of GOD computing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Gabor features</head><p>In GRRC, we adopt the Gabor magnitude as the augmented facial features. Here we also evaluate other Gabor features, such as Gabor real parts, Gabor imaginary parts, and the concatenation of Gabor real and imaginary parts. We replace Gabor magnitude features in GRRC_L 2 by these Gabor features, and test their performance on the AR database (the detailed experimental setting is described in Section 4.2). Table 4 lists the recognition rates. It is easy to see that the features of Gabor real parts (denoted by GRRC_L 2 (real parts)), Gabor imaginary parts (denoted by GRRC_L 2 (imaginary parts)) and their concatenation (denoted by GRRC_L 2 (realþimaginary)) do not lead to good results. This demonstrates that Gabor magnitude (denoted by GRRC_L 2 (magnitude)) is more discriminative in the Gabor feature-based representation scheme. The results by SRC <ref type="bibr" target="#b9">[10]</ref> and CRC <ref type="bibr" target="#b31">[32]</ref> schemes with holistic PCA features are also listed in the table for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Algorithm of GRR based classification (GRRC).</p><p>1. Input: Gabor feature dictionary X(A), GOD C, and the Gabor feature v(y o ) (for testing sample without occlusion) or v(y) (for testing sample with occlusion). 2. Solve the l p -minimization (p¼ 1 or 2) problem (the Lagrange formulation): b ¼ arg min b :vðy o ÞÀXðAÞb:</p><formula xml:id="formula_28">2 2 þ l:b: lp n o<label>ð19Þ</label></formula><formula xml:id="formula_29">or (let x G ¼ ½b; b G ) xG ¼ arg minx G :vðyÞÀ XðAÞ C ½ x G : 2 2 þ l:x G : lp n o<label>ð20Þ</label></formula><p>where xG ¼ ½ b; bG and l is a positive scalar that balances the coding residual and regularization strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compute the residuals</head><formula xml:id="formula_30">r i y o À Á ¼ :vðy o ÞÀX A i ð Þd i b : 2 for i ¼ 1,. . .,K:<label>ð21Þ</label></formula><formula xml:id="formula_31">or r i y ð Þ ¼ :vðyÞÀC bC ÀX A i ð Þd i b : 2 for i ¼ 1,. . .,K:<label>ð22Þ</label></formula><p>4. Output identity(y o ) ¼ argmin i r i (y o ) or identity (y) ¼argmin i r i (y). Here we use an FR experiment on Extended Yale B <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> with random block face occlusion (about 45% occlusion) to discuss the selection of l p -norm. The detailed experimental setting will be presented in the experiments of FR with random block occlusion in Section 4.4. We set the parameter z in the model (Eq. ( <ref type="formula" target="#formula_17">9</ref>)) of GOD computing as 0.005. The recognition rates of l p -norm regularized GOD computing versus different regularization parameters l in coding (Eq. ( <ref type="formula" target="#formula_29">20</ref>) with l 1 -norm regularization) of the classification stage are shown in Fig. <ref type="figure">4</ref>. It can be seen that there is not much difference in recognition accuracy between l 1 -norm and l 2 -norm regularization in GOD computing. The reason is that the redundancy of Gabor feature transformation (analyzed in Section 3.2) makes the learnt GOD dictionary compact so that the GOD dictionary is obviously over-determined (Here ''over-determined'' means that the number of atoms (i.e., columns) in GOD is much less than the number of rows in GOD.). An over-determined dictionary itself could stably represent the testing sample even without regularization, while the l 1 -norm or l 2 -norm constraint on coding coefficient in GRRC could make the representation more stable and make the coding coefficients more discriminative. Therefore, the l 1 -norm and l 2 -norm regularizations will lead to stable occluded face representation and similar recognition results. Considering that the recognition rates by l 1 -norm and l 2 -norm regularized GOD computing are similar, we prefer to use the l 2 -norm regularized one for its fast speed. In our paper, the parameter z in GOD computing is set as a small scalar, e.g., 0.001.</p><p>In order to give an intuitive illustration of the leant GOD, we plot the 1st, 51st, 101st and 151st atom of l 1 -norm regularized GOD in Fig. <ref type="figure">5</ref>. We could see that the learnt GOD atoms are roughly periodic signals, which have 40 repeated patterns, and each pattern corresponds to one orientation on one scale of the Gabor feature (the Gabor feature is the concatenation of 40 downsampling Gabor magnitudes). The original occlusion dictionary (i.e., the identity matrix) has clear spatial meaning, e.g., each atom is a unit vector representing one pixel of the image. However, the  size of such an occlusion dictionary is too big (e.g., 8064 Â 8064 in this experiment). Because the spatial size of the augmented down-sampling Gabor feature is greatly reduced and occlusion is a phenomenon in spatial domain, the number of atoms in GOD could be greatly reduced. The learnt GOD not only has much smaller size (e.g., 8940 Â 200), but also have very clear spatial meaning, i.e., on each down-sampled Gabor magnitude feature, the corresponding atom of GOD is a local basis to represent the scale and orientation information at that location. Therefore, GOD is much more efficient to handle occlusion.</p><formula xml:id="formula_32">0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face recognition with little deformation</head><p>We evaluate the proposed GRRC scheme on four representative facial image databases: Extended Yale B <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, AR <ref type="bibr" target="#b20">[21]</ref>, Multi-PIE <ref type="bibr" target="#b26">[27]</ref> and FERET <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. We compare GRRC with SRC <ref type="bibr" target="#b9">[10]</ref>, CRC <ref type="bibr" target="#b31">[32]</ref>, Linear Regression for Classification (LRC) <ref type="bibr" target="#b36">[37]</ref>, linear Support Vector Machine (SVM) and Nearest Neighbor (NN) methods. If no specific instruction, for all the competing methods we use PCA to reduce the feature dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Extended Yale B database</head><p>The Extended Yale B database consists of 2414 frontal-face images of 38 individuals, captured under various laboratorycontrolled lighting conditions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. For each subject, we randomly selected half of the images for training (i.e., 32 images per subject), and used the other half for testing. The images are normalized to 192 Â 168, and the dimension of the augmented Gabor feature vector of each image is 19,760 <ref type="bibr">(40 Â 26 Â 19)</ref>. The results of all the methods versus the feature dimension are listed in Table <ref type="table" target="#tab_2">5</ref>. It can be seen that GRRC is better than SRC, CRC and other methods in all the dimensions except that SRC is slightly better GRRC_L 2 in the dimension of 56. This shows that the l 1 -norm sparse constraint will be more effective than the non-sparse l 2 -norm constraint in classification when the discrimination of feature (e.g., 56-dimensional) is not high and the dictionary (e.g., with the size of 56 Â 1206) is an over-complete matrix. GRRC_L 2 has similar performance to GRRC_L 1 when the dimension is greater than 56. On this database, the maximal recognition rates of the competing methods are 99.2% for GRRC_L 1 , 99.1% for GRRC_L 2 , 97.9% for SRC, 98.0% for CRC, 96.4 for SVM, 95.7% for LRC, and 92.0% for NN. In addition, it can be seen that GRRC is not sensitive to the value of l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">AR database</head><p>The AR database consists of over 4000 frontal images from 126 individuals <ref type="bibr" target="#b20">[21]</ref>. For each individual, 26 pictures were taken in two separate sessions. As in <ref type="bibr" target="#b9">[10]</ref>, in the experiment we chose a subset of the dataset consisting of 50 male subjects and 50 female subjects. For each subject, the seven images with illumination change and expressions from Session 1 were used for training, and the other seven images with only illumination change and expression from Session 2 were used for testing. The size of original face image is 165 Â 120, and the Gabor-feature vector is of dimension 12,000 <ref type="bibr">(40 Â 20 Â 15)</ref>. The comparison of GRRC and the competitors are shown in Table <ref type="table" target="#tab_3">6</ref>. Again we can see that GRRC performs much better than all the other methods under all the dimensions, especially with more than 3% improvement when the dimension is larger than 54. On this database, the maximal recognition rate of GRRC_L 1 , GRRC_L 2 , SRC, CRC, SVM are 97.1%, 97.3%, 93.5%, 93.9% and 88.8%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Large-scale Multi-PIE database</head><p>The CMU Multi-PIE database <ref type="bibr" target="#b26">[27]</ref> contains images of 337 subjects captured in four sessions with simultaneous variations in pose, expression, and illumination. In the experiments, all the 249 subjects in Session 1 were used. For the training set, we used the 14 frontal images with illuminations {0,1, <ref type="bibr">3,4,6,7,8,11,13, 14,16,17,18,19}</ref> and neutral expression. For the testing sets, 10 typical frontal images of even-number illuminations taken with neutral expressions from Session 2 to Session 4 were used. The image size is cropped and normalized to 100 Â 82, and the Gabor feature vector is of the dimension of 8320 (40 Â 16 Â 13). We use PCA to reduce the dimensionality of the input feature to 300. Table <ref type="table" target="#tab_4">7</ref> lists the recognition rates in three tests by the competing methods. The results validate that GRRC methods get the best in accuracy, at least 3% higher than that of SRC and CRC in session 2 and about 5% higher than that of SRC and CRC in other sessions. NN, LRC and SVM cannot get good recognition accuracy (lower than 90%) in this database, much lower than SRC, CRC and GRRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Large-scale FERET database</head><p>The FERET database <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> is often used to validate an algorithm's effectiveness because it contains many kinds of image variations. By taking 'Fa' subset as a gallery, the probe subsets 'Fb' and 'Fc' were captured with expression and illumination variations. Especially, 'Dup1' and 'Dup2' consist of images that were taken at different times with more than one year interval.</p><p>Here we should note that in the Gallery set 'Fa', each subject only has one sample, which is very challenging for SRC and GRRC because usually they need several samples for each subject to construct the subspace. The image size is cropped and normalized to 150 Â 130, and the Gabor feature vector is of dimension 21,000 <ref type="bibr">(40 Â 25 Â 21)</ref>. For all the competing methods, we used LDA to reduce the original feature dimensionality to 428 for LDA could achieve better performance than PCA in this challenging dataset. Table <ref type="table" target="#tab_5">8</ref> shows the face recognition results on FERET database. It is surprised that SRC and CRC have higher accuracy than NN and SVM except for 'Fb' even only one sample for each subject in the training set. GRRC methods achieve the best performance with over 95% recognition rates in 'Fb' and 'Fc' and about 78% in 'Dup1' and 'Dup2'. It can also be seen that for 'Fb', GRRC has at least 8% improvements compared to other methods, while with about 20%, 27% and 43% improvements for 'Fc', 'Dup1' and 'Dup2', respectively. According to the recent state-of-the-art FR results on the FERET database, e.g., Xie et al.'s method <ref type="bibr" target="#b44">[45]</ref>, further improvement could be achieved if more discriminative features, e.g., fused Gabor magnitude and phase feature <ref type="bibr" target="#b44">[45]</ref>, are utilized in the framework of GRRC.</p><p>From the experimental results in Extended Yale B, AR, Multi-PIE and FERET, we could see that GRRC is very robust to the value of l and GRRC_L 1 and GRRC_L 2 have very similar performance (the gap usually is less than 0.5% in high dimensional feature), showing that GRRC_L 2 is very suitable for the practical FR systems due to its fast speed and good performance. Besides, the improvements brought by GRRC on the AR, Multi-PIE, and FERET are much bigger than that on the Extended Yale B database. This is because mostly there is only illumination variation between the training images and testing images, and the number of training samples (i.e., 32) in the Extended Yale B database is also high. Thus the original SRC and CRC work well on it. However for the more challenging cases (e.g., the training and testing samples of the AR, Multi-PIE and FERET have much more variations, including time, illumination, etc., but with very limited number of training samples), the local feature based GRRC is much more robust than the holistic feature based SRC, CRC, SVM, LRC and NN in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face recognition with pose and expression variations</head><p>In this section, we verify the robustness of GRRC to pose and expression variations on the pose subset of FERET database <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> and expression subset of Multi-PIE <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">FERET pose database</head><p>Here we used the pose subset of the FERET database <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, which includes 1400 images from 198 subjects (about 7 each). This subset is composed of the images marked with 'ba', 'bd', 'be', 'bf', 'bg', 'bj', and 'bk'. In our experiment, each image has the size of 80 Â 80 and the dimension of Gabor feature is 6760 (40 Â 13 Â 13). Some sample images of one person are shown in Fig. <ref type="figure">6</ref>.</p><p>Five tests with different pose angles were performed. In test 1 (pose angle is zero degree), images marked with 'ba' and 'bj' were used as the training set, and images marked with 'bk' were used as the testing set. In all the other four tests, we used images marked with 'ba', 'bj' and 'bk' as gallery, and used the images with 'bg', 'bf', 'be' and 'bd' as probes, respectively. Here we use 350-dimension Eigenfaces as the input feature. Table <ref type="table">9</ref> lists the results of different methods for various face poses. Obviously, we can see that GRRC has much higher recognition rates than SRC and other methods. In particular, when the pose variation is moderate (0 o and 715 o ), about 20% improvement is achieved by GRRC compared to SRC. We could also see that GRRC_L 2 performs very similarly to GRRC_L 1 . It is undeniable that GRRC's performance also degrades much when pose variation becomes large (e.g. 7251). Nevertheless, GRRC can much improve the robustness to moderate pose variation, and thus it could tolerate registration error (e.g., pose variation, misalignment) to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Multi-PIE expression subset</head><p>All the 249 subjects in Session 1 were used as training set in this experiment. To make the FR more challenging, four subsets with both illumination and expression variations in Sessions 1, 2 and 3 were used for testing. For the training set, as in <ref type="bibr" target="#b33">[34]</ref> we used the 7 frontal images with extreme illuminations {0, 1, 7, 13, 14, 16, 18} and neutral expression (refer to Fig. <ref type="figure" target="#fig_5">7(a</ref>) for examples). For the testing set, 4 typical frontal images with illuminations {0, 2, 7, 13} and different expressions (smile in Sessions 1 and 3, squint and surprise in Session 2) are used (refer to Fig. <ref type="figure" target="#fig_5">7(b</ref>) for examples with surprise in Session 2, Fig. <ref type="figure" target="#fig_5">7(c</ref>) for examples with squint in Session 2, Fig. <ref type="figure" target="#fig_5">7(d</ref>) for examples with smile in Session 1, and Fig. <ref type="figure" target="#fig_5">7</ref>(e) for examples with smile in Session 3). We used the Eigenface with dimensionality 900 as the face feature.</p><p>Table <ref type="table">10</ref> lists the recognition rates in four testing sets by the competing methods, including SRC using Hasing <ref type="bibr" target="#b45">[46]</ref> (e.g., HashþOMP and HashþL 1 ). It can be seen that GRRC achieves the best performance in all tests and SRC performs the second best. It can also be seen that SRC using Hashing has lower recognition rates than SRC, which may result from its use of random projection matrix for dimensionality reduction. In addition, all the methods achieve their best results when Smile-S1 is used for testing because the training set is also from Session 1. The highest rates of GRRC_L 1 and GRRC_L 2 are 97.4% and 97.3%, respectively, more than 3% improvement over the third best one, SRC. From testing set Smile-S1 to set Smile-S3, the variations Fig. <ref type="figure">6</ref>. Some samples of a subject on the pose subset of the FERET database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9</head><p>Face recognition results (%) on the pose subset of the FERET database. For GRRC, r 1 (r 2 ) means that r 1 is the recognition rate for l¼0.0005, and r 2 for l¼0.001.  From the experiments on FR with local deformation (e.g., pose and expression variations), we could see that there is almost no difference between GRRC with l¼0.001 and GRRC with l¼0.0005, showing that GRRC is very robust to the value of l.</p><p>GRRC is much superior to the other methods, including SRC and CRC. This not only shows that collaborative representation based classification strategy with l 1 or l 2 norm regularization is more powerful than other classifiers, such as NN, LRC and SVM, but also demonstrates that Gabor magnitude features are more robust to the variations of pose and expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Recognition against occlusion</head><p>In this sub-section, we test the robustness of GRRC to face occlusions, including block occlusion and real disguise. FR with random block occlusion is performed on the Extended Yale B database <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, while FR with real disguise is performed on the AR database <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">FR with random block occlusion</head><p>As in <ref type="bibr" target="#b9">[10]</ref>, we chose Subsets 1 and 2 (717 images, normal-tomoderate lighting conditions) for training, and Subset 3 (453 images, more extreme lighting conditions) for testing. In accordance to the experiments in <ref type="bibr" target="#b9">[10]</ref>, the images were resized to 96 Â 84, and the occlusion dictionary A e in SRC is set to an identity matrix.</p><p>With the above settings, in SRC the size of matrix B in Eq. ( <ref type="formula" target="#formula_5">3</ref>) is 8064 Â 8761. In the proposed GRRC, the dimension of augmented Gabor-feature vector is 8960 (40 Â 16 Â 14, rE40). The GOD C is then computed using Algorithm in Table <ref type="table">2</ref>. In the experiment, we set the number of atoms in C to 200 (i.e., p¼200, with compression ratio about 40:1), and hence the size of dictionary B C in 0Eq. <ref type="bibr" target="#b16">(17)</ref> is 8960 Â 917. Compared with the original SRC, the dictionary size of GRRC is reduced from 8761 to 917.</p><p>As in <ref type="bibr" target="#b9">[10]</ref>, we simulated various levels of contiguous occlusion, form 0% to 50%, by replacing a randomly located square block in each test image with an unrelated image, whose size is determined by the occlusion percentage. The location of occlusion was randomly chosen for each test image and is unknown to the computer. Fig. <ref type="figure" target="#fig_6">8</ref> illustrates the classification process by using an example. Fig. <ref type="figure" target="#fig_6">8</ref>(a) shows a test image with 30% randomly located occlusion; Fig. <ref type="figure" target="#fig_6">8(b)</ref> shows the augmented Gabor features of the test image. The residual of GRRC_L 2 are plotted in Fig. <ref type="figure" target="#fig_6">8(c</ref>), and a template image of the identified subject is shown in Fig. <ref type="figure" target="#fig_6">8(d)</ref>. The detailed recognition rates of GRRC, SRC, CRC and PCAþNN (used as the baseline) are listed in the Table <ref type="table" target="#tab_8">12</ref>. We see that GRRC </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 10</head><p>Face recognition rates on Multi-PIE expression database. For GRRC, r 1 (r 2 ) means that r 1 is the recognition rate for l¼0.0005, and r 2 for l¼0.001. can correctly classify all the test images when the occlusion percentage is less than or equal to 30%. When the occlusion percentage becomes larger, the advantage of GRRC over SRC is getting higher. Especially, GRRC_L 1 can still have a recognition rate of 87.4% when half of image is occluded, while SRC and CRC only achieve a rate of 65.3% and 61.0 respectively. PCAþNN gets the worst results for it does not consider the occlusion. We could also see that good performance is still achieved when the representation coefficients on Gabor occlusion dictionary are regularized by l 2 -norn in GRRC_L 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">FR with real disguise</head><p>A subset from the AR database was used in this experiment. This subset consists of 1199 images from 100 subjects (14 samples each class except for a corrupted image w-027-14.bmp), 50 males and 50 females. 799 images (about 8 samples per subject) of non-occluded frontal views with various facial expressions were used for training, while the others for testing. The images are resized to 83 Â 60. So in original SRC, the size of matrix B in Eq. ( <ref type="formula" target="#formula_5">3</ref>) is 4980 Â 5779. In the proposed GRRC, the dimension of Gabor-feature vectors is 5200 (40 Â 13 Â 10, rE38), and 100 atoms (with compression ratio 50:1) are computed to form the GOD by Algorithm in Table <ref type="table">2</ref>. Thus the size of dictionary B C in Eq. ( <ref type="formula" target="#formula_18">17</ref>) is 5200 Â 899, and the dictionary size is reduced from 5779 to 899 for GRRC.</p><p>We consider two separate test sets of 200 images (1 sample each session and each subject, with neural expression). The first test set contains images of the subjects wearing sunglasses, which occlude roughly 20% of the image. The second test set is composed of images of the subjects wearing a scarf, which occlude roughly 40% of the images. The results by GRRC, SRC, CRC, PCAþNN and SVM are listed in Table <ref type="table" target="#tab_9">13</ref> (where the results of SRC and PCAþ NN are copied from the original paper <ref type="bibr" target="#b9">[10]</ref>). We see that on faces occluded by sunglasses, GRRC achieves a recognition rate of 93.0%, over 5% higher than that of SRC, while for occlusion by scarves, the proposed GRRC_L 1 (GRRC_L 2 ) achieves a recognition rate of 79% (77.5%), about 20% higher than that of SRC. It is surprising that CRC gets 90.5% in the scarf case but with very low recognition accuracy in sunglass case. SVM gets bad performance for that it cannot learn the occlusion information from the training set without occlusion.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, the authors also partitioned the image into blocks for face classification by assuming that the occlusion is continuous. Such an SRC scheme is denoted by SRC-p, with the CRC scheme denoted by CRC-p. Here, after partitioning the image into several blocks, we calculate the Gabor features of each block and then use GRRC to classify each block image. The final classification result is obtained by voting. We denote by GRRC-p the GRRC with partitioning. In experiments, as <ref type="bibr" target="#b9">[10]</ref> we partitioned the images into eight (4 Â 2) blocks of size 20 Â 30. The Gabor-feature vector of each block is of dimension 800, and the number of atoms in the computed GOD C is set to 20. Thus the dictionary B in SRC is of size 600 Â 1379, while the dictionary B C in GRRC is of size 800 Â 819. The recognition rates of SRC-p, CRC-p and GRRC-p are also listed in Table <ref type="table" target="#tab_9">13</ref>. We see that with partitioning, GRRC can lead to recognition rates of 100% on sunglasses and 99% on scarves, also better than SRC and CRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Running time comparison</head><p>The recognition rates and running time of the proposed GRRC and SRC on a more challenging FR experiment with real disguise are compared here. A subset of 50 males and 50 females are selected from the AR database. For each subject, 7 samples with no occlusion from session 1 are used for training, with all the remaining samples with disguises for testing. These testing samples (including 3 sunglass samples in Session 1, 3 sunglass samples in Session 2, 3 scarf samples in Session 1 and 3 scarf samples in Session 2 per subject) not only have disguises, but also have variations of time and illumination. The image size and the extraction of Gabor feature of GRRC remains the same as before.</p><p>Here l¼0.005 for GRRC and the programming environment is Matlab version R2011a. The desktop used is of 1.86 GHz CPU and with 2.99G RAM. All the l 1 -minimization problem is solved by using the fast solver: ALM <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. The recognition rates and running time of GRRC and SRC are listed in Table <ref type="table" target="#tab_10">14</ref>. The recognition rates of GRRC in all cases are much higher than SRC  and CRC, especially with over 7% improvement on FR with sunglasses of session 1, and at least 43% in FR with scarf. It can also be seen that GRRC_L 1 is slightly better in FR with scarf, while GRRC_L 2 slightly better in FR with sunglasses. Fig. <ref type="figure">9</ref> plots the representation coefficients and residuals of a sample from class 1.</p><p>As shown in Fig. <ref type="figure">9</ref>(b), the sample is wrongly classified by GRRC_L 1 though the coefficients are sparse (see Fig. <ref type="figure">9</ref>(a)). Although the representation coefficients of GRRC_L 2 are dense (Fig. <ref type="figure">9</ref>(c)), the sample is correctly classified, as shown in Fig. <ref type="figure">9(d)</ref>.</p><p>The running time of SRC per testing sample is about 12 s, while GRRC_L 1 only needs about 1.5 s. However, this is still long for practical FR system. With l 2 -norm regularization on the Gabor feature representation coefficients, the running time of GRRC_L 2 is only about 0.3 s, where 0.29 s is the running time of Gabor feature extraction. Although CRC is the fastest one, its recognition rate is also very low, similar to that of SRC. The speedup of GRRC_L2 and GRRC_L 1 over SRC are 37.09 and 7.98 times, respectively.</p><p>It can be seen from the FR experiments with occlusion that GRRC could achieve much higher recognition accuracy than SRC and CRC. More importantly, with Gabor transformation, the occlusion dictionary could be compressed, which reduces significantly the number of unknown parameters and the computational burden. It should be noted that GRRC_L 2 which regularizes the coding coefficients by l 2 norm could achieve very competing performance as GRRC_L 1 . This is because Gabor magnitude features could make original sparse representation in original image domain into a dense representation in the transformed domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion of regularization on coding coefficients</head><p>In this section, we discuss the effect of feature dimension on the regularization (l 1 -norm or l 2 -norm) of coding coefficient.   The number of dictionary atoms is 3486 (14 Â 249). From Fig. <ref type="figure" target="#fig_8">10</ref>, we get that when the feature dimension is too low compared to the number of dictionary atoms, GRRC_L 1 has better performance than GRRC_L 2 . However, as the feature dimensionality increases, their recognition rates will become close. This result can be easily explained by the fact that when the feature dimension is much lower than the number of dictionary atoms, the dictionary is more over-complete, and thus the sparsity constraint on the representation coefficients is more reasonable. When the feature dimension is comparable to the number of dictionary atoms, especially for the problem of FR where high correlation exists in the dictionary, l 2 -norm regularized GRRC has very competing performance with GRRC_L 1 , implying that the time-consuming sparsity constraint on the coding coefficients is not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a Gabor-feature based robust representation and classification (GRRC) scheme for face recognition, and proposed an associated Gabor occlusion dictionary (GOD) computing algorithm to handle the occluded face images. Apart from the improved face recognition rate, one important advantage of GRRC is its compact occlusion dictionary, which has much less atoms than that of the original SRC scheme. More importantly, the coding coefficients on the learnt GOD could be regularized by l 2 -norm instead of the commonly used l 1 -norm. This greatly reduces the computational cost of coding. We evaluated the proposed method on different conditions, including variations of illumination, expression and pose, as well as block occlusion and disguise occlusion. The experimental results clearly demonstrated that the proposed GRRC has much better performance than SRC, leading to much higher recognition rates while spending much less computational cost. This makes it much more practical to use than SRC in real world face recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3. Gabor-feature based robust representation and classification 3 . 1 .</head><label>31</label><figDesc>Gabor-feature based robust representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Gabor feature extraction: (a) multi-scale and multi-orientation Gabor filtering and (b) the uniform down-sampling of Gabor filtering responses.</figDesc><graphic coords="4,91.16,451.25,403.21,122.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Illustration of the convergence of the proposed Gabor occlusion dictionary (GOD) computing algorithm on the AR database. A GOD with 100 atoms is computed from the original Gabor-feature based occlusion matrix with 4980 columns. The compression ratio is nearly 50:1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Recognition rates by using l 1 -norm and l 2 -norm regularized GOD computing in the experiment of FR with random block occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>42.0) 95.5(95.5) 99.0(99.0) 89.0(89.5) 44.5(45.0) GRRC_L 2 41.0(41.0) 95.5(95.5) 99.0(99.0) 91.5(91.5) 44.0(44.0) increase because of the longer data acquisition time interval and expression changes (refer to Fig. 7(d) and (e)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A subject in Multi-PIE database: (a) training samples with only illumination variations, (b) testing samples with surprise expression and illuminations in Session 2, (c) testing samples with squint expression and illuminations in Session 2 and (d) and (e) show the testing samples with smile expression and illumination variations in Sessions 1 and 3, respectively.</figDesc><graphic coords="10,124.76,58.64,336.24,193.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. An example of face recognition with block occlusion: (a) a 30% occluded test face image y from the first class of Extended Yale B, (b) uniformly down-sampled Gabor features v(y) of the test image, (c) Estimated residuals r i (y), i¼1, 2, y, 38 and (d) one sample of the class to which the test image is classified.</figDesc><graphic coords="11,44.48,70.31,516.34,104.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 . 1 Fig. 10 .</head><label>9110</label><figDesc>Fig. 9. Representation coefficient and residual of a sample from class 1: (a) and (c) plot the coefficients of GRRC_L 1 and GRRC_L 2 , respectively; (b) and (d) illustrate the representation residual associated to each class by GRRC_L 1 and GRRC_L 2 , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 plots</head><label>10</label><figDesc>Fig.10plots the recognition rates of GRRC_L 1 and GRRC_L 2 versus different feature dimensionality with the same experiment setting on Mulit-PIE database in Section 4.3. The number of dictionary atoms is 3486 (14 Â 249). From Fig.10, we get that when the feature dimension is too low compared to the number of dictionary atoms, GRRC_L 1 has better performance than GRRC_L 2 . However, as the feature dimensionality increases, their recognition rates will become close. This result can be easily explained by the fact that when the feature dimension is much lower than the number of dictionary atoms, the dictionary is more over-complete, and thus the sparsity constraint on the representation coefficients is more reasonable. When the feature dimension is comparable to the number of dictionary atoms, especially for the problem of FR where high correlation exists in the dictionary, l 2 -norm regularized GRRC has very competing performance with GRRC_L 1 , implying that the time-consuming sparsity constraint on the coding coefficients is not necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4</head><label>4</label><figDesc>Face recognition rates (%) with different Gabor features on AR database. In the GOD computing algorithm (refer to Table2), we regularize the coding coefficient by l p -norm with p ¼1 or 2.</figDesc><table><row><cell>Dimension</cell><cell>130</cell><cell>300</cell><cell>540</cell></row><row><cell>PCAþ SRC</cell><cell>89.7</cell><cell>93.3</cell><cell>93.5</cell></row><row><cell>PCAþ CRC</cell><cell>90.0</cell><cell>93.7</cell><cell>93.9</cell></row><row><cell>GRRC_L 2 (real parts)</cell><cell>84.3</cell><cell>89.4</cell><cell>91.4</cell></row><row><cell>GRRC_L 2 (imaginary parts)</cell><cell>85.8</cell><cell>91.0</cell><cell>93.3</cell></row><row><cell>GRRC_L 2 (realþ imaginary)</cell><cell>85.0</cell><cell>91.4</cell><cell>93.6</cell></row><row><cell>GRRC_L 2 (magnitude)</cell><cell>93.1</cell><cell>96.8</cell><cell>97.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc>Face recognition results (%) on the Extended Yale B database. For GRRC, r 1 (r 2 ) means that r 1 is the recognition rate for l¼0.0005, and r 2 for l¼0.001.</figDesc><table><row><cell></cell><cell>56</cell><cell>120</cell><cell>300</cell><cell>504</cell></row><row><cell>SRC</cell><cell>92.6</cell><cell>95.6</cell><cell>97.4</cell><cell>97.9</cell></row><row><cell>CRC</cell><cell>88.6</cell><cell>95.4</cell><cell>97.4</cell><cell>98.0</cell></row><row><cell>NN</cell><cell>81.4</cell><cell>89.2</cell><cell>91.9</cell><cell>92.0</cell></row><row><cell>LRC</cell><cell>94.1</cell><cell>94.7</cell><cell>95.4</cell><cell>95.7</cell></row><row><cell>SVM</cell><cell>92.6</cell><cell>95.3</cell><cell>96.3</cell><cell>96.4</cell></row><row><cell>GRRC_L 1</cell><cell>92.7(92.7)</cell><cell>95.6(96.2)</cell><cell>97.9(97.9)</cell><cell>99.0(99.2)</cell></row><row><cell>GRRC_L 2</cell><cell>90.5(90.5)</cell><cell>96.3(96.3)</cell><cell>98.4(98.4)</cell><cell>99.1(99.1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6</head><label>6</label><figDesc>Face recognition results (%) on the AR database. For GRRC, r 1 (r 2 ) means that r 1 is the recognition rate for l¼0.0005, and r 2 for l¼0.001.</figDesc><table><row><cell></cell><cell>54</cell><cell>130</cell><cell>300</cell><cell>540</cell></row><row><cell>SRC</cell><cell>80.0</cell><cell>89.7</cell><cell>93.3</cell><cell>93.5</cell></row><row><cell>CRC</cell><cell>80.5</cell><cell>90.0</cell><cell>93.7</cell><cell>93.9</cell></row><row><cell>NN</cell><cell>67.8</cell><cell>70.1</cell><cell>71.2</cell><cell>72.1</cell></row><row><cell>LRC</cell><cell>75.4</cell><cell>76.0</cell><cell>70.7</cell><cell>76.7</cell></row><row><cell>SVM</cell><cell>77.5</cell><cell>82.7</cell><cell>87.3</cell><cell>88.8</cell></row><row><cell>GRRC_L 1</cell><cell>86.0(86.0)</cell><cell>94.0(94.0)</cell><cell>96.7(96.6)</cell><cell>97.1(97.1)</cell></row><row><cell>GRRC_L 2</cell><cell>82.7(82.7)</cell><cell>93.1(93.1)</cell><cell>96.7(96.7)</cell><cell>97.3(97.3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7</head><label>7</label><figDesc>Face recognition results (%) on the Multi-PIE database. For GRRC, r 1 (r 2 ) means that r 1 is the recognition rate for l¼0.0005, and r 2 for l¼0.001.</figDesc><table><row><cell></cell><cell>SRC</cell><cell>CRC</cell><cell>NN</cell><cell>LRC</cell><cell>SVM</cell><cell>GRRC_L 1</cell><cell>GRRC_L 2</cell></row><row><cell>Session 2</cell><cell>93.9</cell><cell>94.1</cell><cell>86.4</cell><cell>87.1</cell><cell>85.2</cell><cell>97.3(97.5)</cell><cell>97.1(97.2)</cell></row><row><cell>Session 3</cell><cell>90.0</cell><cell>89.3</cell><cell>78.8</cell><cell>81.9</cell><cell>78.1</cell><cell>96.7(96.7)</cell><cell>96.8(96.8)</cell></row><row><cell>Session 4</cell><cell>94.0</cell><cell>93.3</cell><cell>82.3</cell><cell>84.3</cell><cell>82.1</cell><cell>98.6(98.6)</cell><cell>98.7(98.7)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8</head><label>8</label><figDesc>Face recognition results (%) on the FERET database. For GRRC, r 1 (r 2 ) means that r 1 is the recognition rate for l¼0.0005, and r 2 for l¼0.001.</figDesc><table><row><cell></cell><cell>SRC</cell><cell>CRC</cell><cell>NN</cell><cell>SVM</cell><cell>GRRC_L 1</cell><cell>GRRC_L 2</cell></row><row><cell>Fb</cell><cell>86.9</cell><cell>85.4</cell><cell>87.1</cell><cell>87.1</cell><cell>95.7(95.6)</cell><cell>95.6(95.6)</cell></row><row><cell>Fc</cell><cell>77.3</cell><cell>75.8</cell><cell>73.2</cell><cell>73.2</cell><cell>97.4(97.4)</cell><cell>94.8(95.4)</cell></row><row><cell>Dup1</cell><cell>51.6</cell><cell>51.5</cell><cell>47.8</cell><cell>47.8</cell><cell>77.7(78.0)</cell><cell>79.1(78.9)</cell></row><row><cell>Dup2</cell><cell>33.3</cell><cell>35.5</cell><cell>23.9</cell><cell>23.9</cell><cell>75.6(76.5)</cell><cell>78.6(78.6)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>HashþOMP and HashþL 1 ) is compared in Table 11. Here the Gabor feature extraction for GRRC is 0.2423 s. From Table 11, it is very clear that GRRC_L 2 is the fastest one, about 4 times faster than Hash þOMP, the second fastest method. GRRC_L 1 's running time is still lower than HashþL 1 .</figDesc><table><row><cell>). The recognition</cell></row><row><cell>rates of GRRC_L 1 and GRRC_L 2 drop by 24.1% and 23.1%, respec-</cell></row><row><cell>tively, while those of SRC, CRC, NN, LRC and SVM drop by 33.2%,</cell></row><row><cell>35.7%, 43.1%, 40.6% and 42.6%, respectively, which validates that</cell></row><row><cell>GRRC is much more robust to face variation than the other</cell></row><row><cell>methods. For the testing set of Surpise-S2 and Squint-S2, GRRC</cell></row><row><cell>has about 30% improvement over all the other methods. Mean-</cell></row><row><cell>while, for all the four tests, GRRC with l 1 -norm constraint or</cell></row><row><cell>l 2 -norm constraint on coding coefficients has similar performance.</cell></row><row><cell>The running time of GRRC, SRC, and SRC using Hashing [46]</cell></row><row><cell>(e.g.,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12</head><label>12</label><figDesc>The recognition rates (%) of different methods under different levels of block occlusion.</figDesc><table><row><cell>Occlusion ratio</cell><cell>0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell></row><row><cell>SRC[10]</cell><cell>100</cell><cell>100</cell><cell>99.8</cell><cell>98.5</cell><cell>90.3</cell><cell>65.3</cell></row><row><cell>CRC</cell><cell>100</cell><cell>99.8</cell><cell>96.7</cell><cell>86.3</cell><cell>74.8</cell><cell>61.0</cell></row><row><cell>PCA þNN[10]</cell><cell>92.5</cell><cell>90.7</cell><cell>84.0</cell><cell>73.5</cell><cell>61.5</cell><cell>45.0</cell></row><row><cell>GRRC_L 1</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>96.5</cell><cell>87.4</cell></row><row><cell>GRRC_L 2</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>97.1</cell><cell>84.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 13</head><label>13</label><figDesc>Recognition rates (%) on the AR database with disguise occlusion ('-p': partitioned, '-sg': sunglasses, and '-sc': scarves).</figDesc><table><row><cell>Sunglass</cell><cell>Scarf</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 14</head><label>14</label><figDesc>Recognition rates (%) and average running time (s) of GRRC and SRC on FR with disguise.</figDesc><table><row><cell></cell><cell cols="5">Sunglass-S1 Scarf-S1 Sunglass-S2 Scarf-S2 Average</cell><cell>Speedup</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell><cell></cell></row><row><cell>SRC</cell><cell>83.3</cell><cell>48.7</cell><cell>49.0</cell><cell>29.0</cell><cell>12.278</cell><cell>-</cell></row><row><cell>CRC</cell><cell>78.0</cell><cell>52.3</cell><cell>44.7</cell><cell>29.3</cell><cell>0.084</cell><cell>146.2</cell></row><row><cell cols="2">GRRC_L 1 90.7</cell><cell>95.3</cell><cell>50.3</cell><cell>87.3</cell><cell>1.539</cell><cell>7.98</cell></row><row><cell cols="2">GRRC_L 2 92.3</cell><cell>95</cell><cell>51.7</cell><cell>84.3</cell><cell>0.331</cell><cell>37.09</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Application of the KL procedure for the characterization of human faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sirovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="108" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eigenfaces recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: recognition using class specific linear projection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriengman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why can LDA be performed in PCA transformed space?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="563" to="566" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Desilva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2325" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition using laplacianfaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="340" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local discriminant embedding and its variants</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="846" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Globally maximizing, locally minimizing: unsupervised discriminant projection with applications to face and palm biometrics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="650" to="664" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition: a literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Survey</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Interior-point Polynomial Algorithms in Convex Programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>SIAM, Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Theory of communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gabor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Institution of Electrical Engineers, Part III</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="429" to="457" />
			<date type="published" when="1946">1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1233" to="1258" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distortion invariant object recognition in the dynamic link architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vorbr Üggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V D</forename><surname>Malsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Ürtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Konen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced Fisher linear discriminant model for face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review on Gabor wavelets for face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Application</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="273" to="292" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From few to many: illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
		<idno>no. 24</idno>
		<title level="m">The AR Face Database</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">CVC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The FERET database and evaluation procedure for face recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A method for large-scale l 1 -regularized least squares</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="617" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">L1-magic: A Collection of MATLAB Routines for Solving the Convex Optimization Programs Central to Compressive Sampling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cand Es</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Available: www.acm.cal tech</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The MOSEK Optimization Tools Version 2.5. User&apos;s Manual and Reference</title>
		<ptr target="http://dx.doi.org/www.mosek.com" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MOSEK</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multi-Pie</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernel sparse representation for image classification and face recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L-T</forename><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Face recognition with contiguous occlusion using Markov random fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust classification using structured sparse representation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust sparse coding for face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation which helps face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simultaneous image transformation and sparse representation recovery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards a practical face recognition system: robust registration and illumination by sparse representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face recognition based on nearest linear combinations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Face recognition using nearest feature line method</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Network</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="439" to="443" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linear regression for face recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2106" to="2112" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gabor feature based sparse representation for face recognition with Gabor occlusion dictionary</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The K-SVD: an algorithm for designing of overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discriminative K-SVD for dictionary learning in face recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a discriminative dictionary for sparse coding via label consistent K-SVD</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fisher discrimination dictionary learning for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A review of fast l 1 -minimization algorithms for robust face recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1007.3753v2S2010</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Alternating direction algorithms for l 1 -problems in compressive sensing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0912.1185</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint) /</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fusing local patterns of Gabor magnitude and phase for face recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1349" to="1361" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rapid face recognition using hashing</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Combining perceptual features with diffusion distance for face recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sadka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on System, Man, and Cybernetics-Part C: Application and Reviews</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved face representation by nonuniform multilevel selection of Gabor convolution features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on System, Man, and Cybernetics-Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1408" to="1419" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Methodological improvement on local Gabor face recognition based on feature selection and enhanced Borda count</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Cament</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="951" to="963" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Passive multimodal 2-D þ 3-D face recognition using Gabor features and landmark distances</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jahanbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1287" to="1304" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">He is currently pursuing the Ph.D. degree in Computer Science at the Hong Kong Polytechnic University. His research interests include computer vision, pattern recognition, sparse representation, and face recognition</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2009</date>
		</imprint>
	</monogr>
	<note>Meng Yang received his B.Sc. degree in Information Engineering and M.Sc. degree in Control Theory and Applications from the Northwestern Polytechnical University</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Shenyang</surname></persName>
		</author>
		<author>
			<persName><surname>China</surname></persName>
		</author>
		<ptr target="http://www4.comp.polyu.edu.hk/$cslzhang/" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Pattern Recognition and Optimal Estimation Theory, etc. Dr. Zhang is an Associate Editor of IEEE Transactions on SMC-C, IEEE Transactions on CSVT, and Image and Vision Computing. Dr. Zhang was awarded the Faculty Merit Award in Research and Scholarly Activities 2010, and the Best Paper Award of SPIE VCIP2010. More information can be found in his homepage</title>
		<title level="s">the M.Sc. and Ph.D. degrees in</title>
		<meeting><address><addrLine>Xi&apos;an, PR China</addrLine></address></meeting>
		<imprint>
			<publisher>Lei Zhang received the B.Sc</publisher>
			<date type="published" when="1995">1995. 1998. 2001. 2001 to 2002. January 2003 to January 2006</date>
		</imprint>
		<respStmt>
			<orgName>Shenyang Institute of Aeronautical Engineering ; Control Theory and Engineering from Northwestern Polytechnical University ; The Hong Kong Polytechnic University ; Department of Electrical and Computer Engineering, McMaster University, Canada</orgName>
		</respStmt>
	</monogr>
	<note>2006, he joined the Department of Computing, The Hong Kong Polytechnic University, as an Assistant Professor. Since September 2010, he has been an Associate Professor in the same department. His research interests include Image and Video Processing</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">He worked as a system analyst and project manager between 1985 and 1990 in several business organizations in Hong Kong. His current research interests include Case-base Reasoning, Machine Learning and Soft Computing. He has coauthored the book Foundations of Soft Case-Based Reasoning</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shiu Received The</surname></persName>
		</author>
		<author>
			<persName><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">London in 1986 and the Ph</title>
		<editor>
			<persName><forename type="first">John</forename><surname>Wiley</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sons</surname></persName>
		</editor>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>University of Newcastle Upon Tyne ; Hong Kong Polytechnic University</orgName>
		</respStmt>
	</monogr>
	<note>D. degree in computing in 1997 from The Hong Kong Polytechnic University. He is an assistant professor in the Department of Computing. soft case based reasoning of the Applied Intelligence journal. He is a member of the British Computer Society and the IEEE</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The Kluwer International Series on Biometrics, and an Associate Editor of several international journals. His research interests include automated biometrics-based authentication, pattern recognition, biometric technology and systems. As a principal investigator</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Harbin</surname></persName>
		</author>
		<author>
			<persName><surname>China</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image and Graphics</title>
		<editor>
			<persName><forename type="first">)</forename><surname>Ijig</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Book</forename><surname>Editor</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1983">1983. 1985. 1994. 1986 to 1988</date>
			<pubPlace>Waterloo, Canada; Beijing, China; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>David Zhang graduated in computer science from Peking University in 1974 and received his M.Sc. and Ph.D. degrees in Computer Science and Engineering from the Harbin Institute of Technology (HIT ; Postdoctoral Fellow at Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>Currently, he is a Professor with the Hong Kong Polytechnic University, Hong Kong. He is Founder and Director of Biometrics Research Centers supported by the Government of the Hong Kong SAR (UGC/CRC). he has finished many biometrics projects since 1980. So far, he has published over 200 papers and 10 books</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
