<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Coupled ResNet for Low-Resolution Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ze</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Alex</forename><surname>Kot</surname></persName>
						</author>
						<title level="a" type="main">Deep Coupled ResNet for Low-Resolution Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">70FD3FECE64DE7D3D8220AD5348F2C0C</idno>
					<idno type="DOI">10.1109/LSP.2018.2810121</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/LSP.2018.2810121, IEEE Signal Processing Letters This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/LSP.2018.2810121, IEEE Signal Processing Letters</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN</term>
					<term>feature extraction</term>
					<term>coupled mappings</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face images captured by surveillance cameras are often of low resolution (LR), which adversely affects the performance of their matching with high-resolution (HR) gallery images. Existing methods including super resolution, coupled mappings, multidimensional scaling, and CNN yield only modest performance. In this paper, we propose the Deep Coupled ResNet (DCR) model. It consists of one trunk network and two branch networks. The trunk network, trained by face images of 3 significantly different resolutions, is used to extract discriminative features robust to the resolution change. Two branch networks, trained by HR images and images of the targeted LR, work as resolution-specific coupled mappings to transform HR and corresponding LR features to a space where their difference is minimized. Model parameters of branch networks are optimized using our proposed Coupled-Mapping (CM) loss function, which considers not only the discriminability of HR and LR features, but also the similarity between them. In order to deal with various possible resolutions of probe images, we train multiple pairs of small branch networks while using the same trunk network. Thorough evaluation on LFW and SCface databases shows that the proposed DCR model achieves consistently and considerably better performance than the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F ACE recognition (FR) has been a very active research area due to increasing security demands, commercial applications and law enforcement applications <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Promising results have been achieved under challenging conditions such as occlusion <ref type="bibr" target="#b3">[4]</ref>, variations in pose and illumination <ref type="bibr" target="#b4">[5]</ref>. While many FR approaches have been developed for recognizing HR face images <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>, there are few studies focused on FR in surveillance systems, where HR cameras are not available or there is a long distance between the camera and the subject. Under the condition of LR images, FR approaches developed for HR images usually decline <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>. It is still a challenge to recognize faces when only LR probe images are available.</p><p>Here, we focus on the LR face recogntion (LRFR) problem of matching LR probe face images with HR gallery images. Most of the approaches proposed for this task can be generally divided into two categories. One is to reconstruct the HR probe image from the LR one by super-resolution (SR) techniques and use it for classification. Although SR-based methods such as <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b12">[13]</ref> can generate visually appealing HR images,</p><p>The authors are with the School of Electrical and Electronic Engineering, Nanyang Technological University, 639798 Singapore. E-mail: {zlu008, exdjiang, eackot}@ntu.edu.sg.</p><p>This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore. This research was partly supported by Singapore Ministry of Education Academic Research Fund Tier 1 RG 123/15. The ROSE Lab is supported by the Infocomm Media Development Authority, Singapore. they are computationally expensive and not optimized for recognition purposes thus the results can be further improved <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>.</p><p>The other category is to simultaneously transform the LR probe and corresponding HR gallery images into a common feature subspace where the distance between them is minimized. Li et al. in <ref type="bibr" target="#b1">[2]</ref> propose to learn two matrices which project the face images with different resolutions into a unified feature space where the difference between the LR image and its HR counterpart is minimized. Based on the idea of linear discriminant analysis, several discriminant subspace methods are proposed in <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Instead of using linear methods, Ren et al. in <ref type="bibr" target="#b19">[20]</ref> project the LR and HR face images into an infinite common subspace by minimizing the dissimilarities captured by kernel Gram matrices. Multidimensional scaling (MDS) is employed in <ref type="bibr" target="#b13">[14]</ref> to simultaneously transform the features from the poor quality probe images and the highquality gallery images in the manner that their distances approximate those between gallery images. The same authors propose a reference-based approach for reducing the computational cost in <ref type="bibr" target="#b20">[21]</ref>. Two discriminative MDS methods are proposed in <ref type="bibr" target="#b15">[16]</ref> to make full use of identity information, including both inter-class and intra-class distances. Their new objective function is claimed to enlarge the inter-class distances to ensure discriminability.</p><p>In general, subspace-based methods achieve better recognition performance than SR-based methods. However, subspacebased methods usually extract pixel values or scale-invariant feature transform (SIFT) from images as feature representations. Their performance can be boosted by using feature representations which are robust to the resolution change. Motivated by the superior performance of convolutional neural networks (CNN) <ref type="bibr" target="#b21">[22]</ref>, authors in <ref type="bibr" target="#b14">[15]</ref> train a deep convolutional network, Resolution-Invariance CNN (RICNN), to learn resolution invariant features in a supervised way by mixing the real HR images with the upsampled LR ones. Although RICNN improves the performance of LRFR, it is sensitive to resolution change of probe images as indicated in <ref type="bibr" target="#b15">[16]</ref>.</p><p>In this paper, we propose a CNN-based approach, the Deep Coupled ResNet (DCR) model, to solve above mentioned problems in LRFR. The novelty and contribution of the DCR model come from following 4 aspects. i) The DCR model consists of one big trunk network and two small branch networks.</p><p>ii) The trunk network is trained only once to learn discriminant features shared by face images of different resolutions. It is constructed based on recently proposed residential modules <ref type="bibr" target="#b22">[23]</ref>. iii) Two branch networks are trained to learn resolutionspecific coupled-mappings so that HR gallery images and LR probe images are projected to a space where their distances are minimized. A Coupled-Mapping (CM) loss is proposed to optimize model parameters of branch networks; iv) In reality, there can be various resolutions of probe images to be matched with HR gallery images, the proposed DCR model solves this problem by training multiple pairs of small branch networks (2MB for each pair) while using the same big trunk network (105MB). Resolution indicator methods <ref type="bibr" target="#b23">[24]</ref> can be employed to determine the resolution of probe images and which branch network to be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEEP COUPLED RESNET MODEL A. Architecture of DCR</head><p>The key of low-resolution face recognition (LRFR) is to extract the feature that is robust to resolution change and to measure the similarity between the HR gallery image and the LR probe image. Recently, CNN has been widely used for feature learning in FR and excellent performances have been achieved as in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>. This motivates us to employ CNN in learning discriminative features shared by different image resolutions and in deriving coupled mappings to minimize the distance between the HR gallery image and the LR counterpart of a specific resolution. The architecture of the proposed Deep Coupled ResNet (DCR) model is shown on Fig. <ref type="figure" target="#fig_0">1</ref>. It consists of one trunk network and two branch networks. The trunk network is designed to extract discriminant face features which are robust to the resolution degradation of face images. Two branch networks are trained to learn coupled mappings which minimize the distance between feature vectors extracted by the trunk branch from a HR image and its corresponding LR image of a specific resolution k.</p><p>The trunk network of the proposed DCR model is constructed based on the CNN model in <ref type="bibr" target="#b7">[8]</ref> and the ResNet model proposed in <ref type="bibr" target="#b22">[23]</ref>. Different from previous CNN architectures such as VGG, ResNet in <ref type="bibr" target="#b22">[23]</ref> consists of residual modules which conduct additive merging of signals. The authors in <ref type="bibr" target="#b22">[23]</ref> argue that residual connections are inherently important for training very deep architectures. ResNet has become a seminal work, demonstrating that the degradation problem of deep networks can be solved through the use of residual modules. As shown on Fig. <ref type="figure" target="#fig_0">1</ref>, the trunk network takes raw pixels of face images as input. C, P and F indicate convolutional layer, max-pooling layer and fully-connected layer, respectively. The kernel size of convolution layers is 3 × 3 with stride 1 and the kernel size of max-pooling layers is 2 × 2 with stride 2. Each convolutional layer is followed by a PReLU <ref type="bibr" target="#b25">[26]</ref> non-linear unit. The number of output feature maps in convolutional layers and the number of outputs in fully connected layers are indicated by those on top of each layer. '×h' represents a residual module that repeats for h times.</p><p>The output feature vector v of the second last fullyconnected layer in the trunk network forms input of the branch networks. The HR feature vector and its LR counterpart are fed into the branch networks in pair. Each branch network consists of two fully-connected layers and one PReLU unit in the middle. Thus the coupled mappings are formed as two non-linear mappings. Output feature vectors of the first fully-connected layers (x and z) are taken as the feature representations for HR and LR images, respectively. The trunk network outputs feature representations which are robust to the resolution change of face images. Its model size is around 105MB. The branch network reduces the distance between the HR feature and its LR counterpart of a specific resolution. Its model size is around 2MB, 1.9% of the whole model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Strategy and Coupled-Mapping Loss</head><p>The recently released CASIA-WebFace <ref type="bibr" target="#b26">[27]</ref> database is used to train the proposed DCR model. The 434,793 images of 9,067 subjects, which contain at least 14 images per subject, compose the training set. Face images are normalized to 112 × 96 pixels with an affine transformation according to the coordinates of five sparse facial points, i.e., both eye centers, the nose tip, and both mouth corners. We employ an offthe-shelf face alignment tool <ref type="bibr" target="#b27">[28]</ref> for facial point detection. Some example face images are shown on Fig. <ref type="figure" target="#fig_1">2</ref>. A two-step training strategy is adopted to effectively optimize parameters of the DCR model. In the first step, the trunk network is trained using face images of 3 significantly different image resolutions, 112 × 96, 40 × 40 and 6 × 6, so that it can be applied to the feature extraction of images whose resolution varies from 112 × 96 to 6 × 6. Similar to <ref type="bibr" target="#b14">[15]</ref>, images are firstly down-sampled to LR images. After that, LR images are rescaled to the required input size of the network using bicubic interpolation before being fed to the network for training. In our case, the required size of input images is 112 × 96. This process effectively makes the LR images the blurred HR images. The joint supervision of softmax loss and center loss <ref type="bibr" target="#b7">[8]</ref>, L t , is used to train the trunk network as below:</p><formula xml:id="formula_0">L t = -Σ 3m i=1 log e W T y i vi+dy i Σ n j=1 e W T j vi+dj + βΣ 3m i=1 ||v i -c v yi || 2 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where m is the number of training samples of the same resolution and n indicates the number of subjects in the training data. v i indicates the feature vector extracted by trunk network from the ith training image. W j denotes the jth column of the weights W in the last fully-connected layers of trunk networks, and d is the bias item. y i is the class label for the ith sample and c v yi denotes the y i th class center of deep features v. β is a scaling factor.</p><p>In the second step, model parameters of the trunk network remain unchanged and two branch networks are trained using HR images (112 × 96) and LR images of similar resolution to that of the targeted LR probe images. The Coupled-Mapping (CM) loss function is proposed to supervise the training of branch networks.</p><p>In the training process of branch networks, we first expect to maximize the distances between each face image and its neighbors from different classes within HR or LR features. The softmax loss defined as in equation ( <ref type="formula" target="#formula_2">2</ref>) facilitates interclass separation.</p><formula xml:id="formula_2">L s = -Σ m i=1 log e U T y i xi+by i Σ n j=1 e U T j xi+bj -Σ m i=1 log e V T y i zi+ay i Σ n j=1 e V T j zi+aj ,<label>(2)</label></formula><p>where x i and z i indicate the feature vectors extracted by branch networks from the ith HR and LR images, respectively. U j , V j denote the jth column of the weights U, V in the last fully-connected layers of branch networks, and a, b are the bias items, for HR feature and its LR counterpart, respectively.</p><p>In order to preserve intraclass compactness, we minimize the distances between each face image and its neighbors from the same class within HR or LR features. The center loss defined as in equation ( <ref type="formula" target="#formula_3">3</ref>) minimizes the intra-class variations.</p><formula xml:id="formula_3">L c = Σ m i=1 ||x i -c x yi || 2 2 + Σ m i=1 ||z i -c z yi || 2 2 , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where c x yi and c z yi denote the y i th class centers of HR features x and LR features z, respectively.</p><p>For the task of LRFR, it is important to ensure the consistency between the LR feature and corresponding HR feature. Specifically, the final LR and corresponding HR feature vectors should be as close as possible. This can be achieved by minimizing the Euclidean loss defined in equation <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_5">L e = Σ m i=1 ||x i -z i || 2 2 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Considering the above three criteria, the Coupled-Mapping (CM) loss used for optimizing parameters of branch networks is constructed as below:</p><formula xml:id="formula_7">L CM = L s + λL c + αL e ,<label>(5)</label></formula><p>where λ and α are two scaling factors used for balancing three loss functions. In this way, not only the discriminability of HR and LR features are taken into account, but also the relationship between HR and corresponding LR features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Matching Face Images</head><p>Once the training is finished, both HR gallery images and LR probe images can be fed into DCR to obtain their feature representations. Face verification or identification can be performed by computing their similarities. The big trunk network contains 98.1% of parameters of the whole DCR model, while small branch networks contain 1.9% of all DCR parameters. In real applications, probe images captured by surveillance cameras can be of many different low resolutions. Thus multiple pairs of branch networks are trained in DCR to deal with different resolutions of LR probe images. Hence, multiple branch networks greatly increase the effectiveness and efficiency of the DCR model to match HR images to different resolutions of probe face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>Extensive experiments are conducted on LFW <ref type="bibr" target="#b28">[29]</ref> and SCface <ref type="bibr" target="#b29">[30]</ref> databases to evaluate the proposed DCR model for matching LR probe images with HR gallery face images. Both LFW and SCface databases are widely-used benchmarks for FR in unconstrained environments. On LFW, the face verification performance of DCR is compared with VGGFace <ref type="bibr" target="#b24">[25]</ref>, LightCNN <ref type="bibr" target="#b30">[31]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, and their finetuned versions, VGGFace-FT, LightCNN-FT, and ResNet-FT, respectively. We fine-tune the pre-trained models by the same LR training data for the same number of epochs as in the training process of the DCR model. The open-source deep learning toolkit Caffe <ref type="bibr" target="#b31">[32]</ref> is utilized to fine-tune the deep models. During fine-tuning, the batch size is set to 128. The models are fine-tuned with descending learning ratios and the fine-tuning stops when the loss does not decrease any more. Moreover, to evaluate the effectiveness of the proposed branch network for other networks, we replace the trunk network with LightCNN and VGGFace, and name the obtained models as Coupled-LightCNN and Coupled-VGGFace, respectively. Furthermore, the performance of the trunk network trained by face images of 3 different resolutions is also reported for comparison. Different sizes of LR probe images are used for testing. Please note that different sizes of face images are rescaled to the required input size of the network using bicubic interpolation before being fed to the network for feature extraction. On SCface, besides the above mentioned 9 CNN models, 4 state-of-the-art LRFR approaches, MDS <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, DMDS, LDMDS <ref type="bibr" target="#b15">[16]</ref>, and RICNN <ref type="bibr" target="#b14">[15]</ref>, are also used for comparison with the DCR model. The values of β in equation ( <ref type="formula" target="#formula_0">1</ref>), λ and α in equation ( <ref type="formula" target="#formula_7">5</ref>) are set to 0.008 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on LFW</head><p>The LFW database contains 13,233 images of 5,749 subjects. Images in this database exhibit rich intra-personal variations of pose, illumination, and expression. LFW has been extensively studied for the research of unconstrained FR in recent years. We follow the "Unrestricted, Labeled Outside Data Results" protocol in <ref type="bibr" target="#b28">[29]</ref> and compute the mean verification accuracy by the 10-fold cross-validation scheme on the View 2 data. Face images are normalized and aligned using same methods as on CASIA-WebFace images. For two images in the face verification paradigm, we take the first one as HR (112 × 96) gallery image and down-sample the second one to 8×8, 12×12, 16×16, or 20×20 as the LR probe image. Same sizes of CASIA-WebFace images are used for training of corresponding branch networks. The same face images are used for the fine-tuning of LightCNN, VGGFace, and ResNet models. PCA and cosine distance are used to calculate the similarity between two features. The total scatter matrix of PCA is computed using the 9 training folds of LFW data in the 10-fold cross validation. Face verification accuracies of VGGFace <ref type="bibr" target="#b24">[25]</ref>, VGGFace-FT, Coupled-VGGFace, LightCNN <ref type="bibr" target="#b30">[31]</ref>, LightCNN-FT, Coupled-LightCNN, ResNet <ref type="bibr" target="#b7">[8]</ref>, ResNet-FT, the trunk network and the proposed DCR model on LFW are shown on Table <ref type="table">I</ref>. In the last column, the accuracies for HR probe images of the same resolution as gallery images are also presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on SCface</head><p>The SCface database contains images of 130 subjects taken in uncontrolled indoor environment using five video surveillance cameras of various qualities. For each subject, there are 15 images taken at three distances (5 images at each distance), 4.20m (d1), 2.60m (d2) and 1.00m (d3), by surveillance cameras, and one frontal mugshot image taken by a digital camera. Following the experimental settings in <ref type="bibr" target="#b15">[16]</ref>, frontal mugshot images are employed as gallery images and images taken by surveillance cameras at distance di, i = 1, 2, 3 are used as probe images. We take CASIA-WebFace images of size 112 × 96 as HR images and those of 112×96, 30×30 and 20×20 as LR images for training of branch networks at distance of d3, d2 and d1, respectively. Same as in <ref type="bibr" target="#b15">[16]</ref>, 50 out of 130 subjects in the SCface database are randomly chosen for fine-tuning of the branch networks and training of PCA. Rest of the subjects are for testing. Thus, there is no identity overlap between the training and test sets. The same face images from CasiaWebface and SCface datasets are used for the fine-tuning of LightCNN, VGGFace, and ResNet models. The nearestneighbor classifier is used to classify all probe images. We report the face recognition rates of MDS <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, DMDS, LDMDS <ref type="bibr" target="#b15">[16]</ref>, RICNN <ref type="bibr" target="#b14">[15]</ref>, VGGFace <ref type="bibr" target="#b24">[25]</ref>, VGGFace-FT, Coupled-VGGFace, LightCNN <ref type="bibr" target="#b30">[31]</ref>, LightCNN-FT, Coupled-LightCNN, ResNet <ref type="bibr" target="#b7">[8]</ref>, ResNet-FT, the trunk network and the proposed DCR model on Table <ref type="table">II</ref>. We can observe from Table <ref type="table">I</ref> and Table <ref type="table">II</ref> that: (1), the branch network greatly increases the performance of LightC-NN, VGGFace and the trunk (ResNet) networks; <ref type="bibr" target="#b1">(2)</ref>, the proposed DCR model achieves much higher face recognition accuracy than the state-of-the-art methods consistently for different resolutions of probe images on LFW and SCface datasets. The performance gain is significant for very LR probe images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we propose a novel CNN-based approach named as the Deep Coupled ResNet model for the task of low resolution face recognition. It first extracts discriminative features shared by face images of different resolutions by a ResNet-like network, the trunk network. After that, coupled mappings are learned by branch networks to project features of HR images and corresponding LR images of a specific resolution into a common subspace where their distance is minimized. Experiments on LFW and SCface datasets show that the proposed DCR model achieves consistently and considerably better performance than the state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the proposed Deep Coupled ResNet (DCR) model. The trunk network learns discriminant features (indicated by v) shared by different resolutions of images, and the branch networks are trained as coupled mappings (indicated by x for HR features and z for LR features, respectively). C, P and F indicate convolutional layer, max-pooling layer and fully-connected layer, respectively. The number of output feature maps in convolutional layers and the number of outputs in fully connected layers are indicated by those on top of each layer. '×h' represents a residual module that repeats for h times. k indicates the resolution of LR training images and β is a scaling parameter for center loss.</figDesc><graphic coords="2,48.96,56.07,514.09,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example face images from CASIA-WebFace.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, OCTOBER 2017</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Local gabor binary patterns based on kullback-leibler divergence for partially occluded face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="875" to="878" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition via coupled locality preserving mappings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A color channel fusion approach for face recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1839" to="1843" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modular weighted global sparse representation for robust face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="571" to="574" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Illumination normalization based on weber&apos;s law with application to face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="462" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust face recognition via multimodal deep face representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effect of image resolution on the performance of a face recognition system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Spreeuwers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Veldhuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. Control, Automation, Robotics and Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="327" to="340" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneous superresolution and feature extraction for recognition of low-resolution faces</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep joint face hallucination and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08091</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Studying very low resolution recognition using deep networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4792" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pose-robust recognition of low-resolution face images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3037" to="3049" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards resolution invariant face recognition in uncontrolled scenarios</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. Biometrics. IEEE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative multidimensional scaling for low-resolution face recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-resolution face recognition via simultaneous discriminant analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Joint Conf. Biometrics. IEEE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coupled marginal fisher analysis for low-resolution face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="240" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From local geometry to global structure: Learning latent subspace for low-resolution face image recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="554" to="558" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coupled kernel embedding for lowresolution face image recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3770" to="3783" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low resolution face recognition across variations in pose and illumination</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mudunuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1034" to="1040" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic amelioration of resolution mismatches for local feature based identity inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1200" to="1203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conf</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Amherst, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scface-surveillance cameras face database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Delac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grgic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia tools and applications</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A light cnn for deep face representation with noisy labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02683</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conf. Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
