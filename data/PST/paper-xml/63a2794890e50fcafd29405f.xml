<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Models Are Reasoning Teachers</title>
				<funder ref="#_a25rAcy">
					<orgName type="full">National Research Foundation of Korea</orgName>
				</funder>
				<funder ref="#_uknnbfP">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">Artificial Intelligence Graduate School Program (KAIST)</orgName>
				</funder>
				<funder>
					<orgName type="full">Stochastic Analysis and Application Research Center</orgName>
					<orgName type="abbreviated">SAARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-20">20 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Namgyu</forename><surname>Ho</surname></persName>
							<email>itsnamgyu@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Laura</forename><surname>Schmid</surname></persName>
							<email>laura.schmid@kaist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
							<email>yunseyoung@kaist.ac.kr</email>
						</author>
						<title level="a" type="main">Large Language Models Are Reasoning Teachers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-20">20 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2212.10071v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language models (LMs) have demonstrated remarkable performance on downstream tasks, using in-context exemplars or human instructions. Recent works have shown that chainof-thought (CoT) prompting can elicit models to solve complex reasoning tasks, step-bystep. However, the efficacy of prompt-based CoT methods is restricted to very large LMs such as GPT-3 (175B), thus limiting deployability. In this paper, we revisit the finetuning approach to enable complex reasoning in smaller LMs, optimized to efficiently perform a specific task. We propose Fine-tune-CoT, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via fine-tuning. We evaluate our method on publicly available LMs across a wide range of complex tasks and model sizes. We find that Fine-tune-CoT enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance. Student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. We conduct extensive ablations and sample studies to understand the reasoning capabilities of student models. We also identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT and address them in our analysis. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs) have demonstrated remarkable performance in a wide range of downstream tasks, mainly attributed to their scalability enabled by the Transformer architecture <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> and availability of web-scale training data. Previous works on language models have followed the paradigm of pre-training on a large corpus and then fine-tuning on downstream tasks 1 Our code implementation is publicly available at https://github.com/itsnamgyu/reasoning-teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head><p>Large Teacher ? ? ? ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small Student</head><p>Questions Zero-shot-CoT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning samples</head><p>Fine-tuning Prompt Figure 1: Fine-tune-CoT. We consider a method consisting of multiple stages. First, a large teacher model is prompted to answer questions using multi-step reasoning, without relying on correct examples. That is, the teacher employs zero-shot chain-of-thought reasoning to generate output. We then use the resulting reasoning samples (consisting of the question and teacher output) for fine-tuning a much smaller student model. <ref type="bibr" target="#b30">(Raffel et al., 2020;</ref><ref type="bibr" target="#b5">Devlin et al., 2018)</ref>. Recently, large language models (LLMs) have demonstrated in-context generalization capabilities: performing downstream tasks simply by conditioning on few in-context exemplars or plain natural language task descriptions <ref type="bibr" target="#b0">(Brown et al., 2020;</ref><ref type="bibr" target="#b39">Sun et al., 2021)</ref>.</p><p>LMs have also demonstrated the ability to solve complex tasks, when prompted to generate intermediate rationales. Standard prompting methods, which use few-shot exemplars of question-answer pairs or zero-shot instructions, have been shown to be insufficient for downstream tasks which require multiple reasoning steps <ref type="bibr" target="#b2">(Chowdhery et al., 2022)</ref>. However, recent works have demonstrated the possibility of eliciting complex reasoning abili-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Sample</head><p>Question A pet store had 56 puppies. In one day they sold 24 of them and put the rest into cages with 4 in each cage. How many cages did they use? Answer .8.</p><p>Prompt (Zero-shot-CoT) Q: A pet store had 56 puppies. In one day they sold 24 of them and put the rest into cages with 4 in each cage. How many cages did they use? A: Let's think step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Completion (Generated)</head><p>The store started with 56 puppies. 24 of them were sold, so that means that there are now 32 puppies left. Since there are 4 puppies in each cage, that means that the store now has .8 cages.</p><p>Step 1. Reasoning Generation Large GPT-3 175B Teacher LM*</p><p>Step 2. Curation Small Student LM Prompt A pet store had 56 puppies. In one day they sold 24 of them and put the rest into cages with 4 in each cage. How many cages did they use? ###</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Completion</head><p>The store started with 56 puppies. 24 of them were sold, so that means that there are now 32 puppies left. Since there are 4 puppies in each cage, that means that the store now has 8 cages. Step 1: a very large teacher model is prompted to solve complex questions (yellow) by generating multi-step reasoning explanations (green).</p><p>Step 2: generated completions are filtered based on the correctness of the final prediction (red). The question, rationale, and answer are used to compose a reasoning sample comprised of the prompt and a multi-step completion.</p><p>Step 3: the curated reasoning samples are used to fine-tune a small, lightweight student to exhibit reasoning capabilities. The application of an LM-based teacher enables diverse reasoning-generating multiple distinct rationales for each original sample to enrich the fine-tuning data. This boosts the performance of student models without requiring human annotation.</p><p>ties in LLMs through few-shot exemplars containing chain-of-thought (CoT) reasoning <ref type="bibr">(Wang et al., 2022b)</ref> or prompting the model to think step-bystep <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>.</p><p>A major drawback of prompt-based CoT reasoning methods is their reliance on extremely large LMs, spanning hundreds of billions of parameters <ref type="bibr">(Wei et al., 2022b;</ref><ref type="bibr" target="#b17">Kojima et al., 2022)</ref>. These models are prohibitive to deploy at scale due to overwhelming computational requirements and inference costs <ref type="bibr">(Wei et al., 2022b)</ref>. Therefore, we strive to enable complex reasoning in small models for use in real-world applications.</p><p>In this light, we propose an approach named Fine-tune-CoT, which aims to utilize the CoT reasoning capabilities of very large LMs to teach small models how to solve complex tasks. To elaborate, we apply existing zero-shot CoT prompting <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref> to generate rationales from very large teacher models, and use them to fine-tune smaller student models<ref type="foot" target="#foot_0">2</ref> , as shown in Figure <ref type="figure" target="#fig_0">2</ref>. We note that similar to standard prompting, it has been shown that vanilla fine-tuning is often inadequate for training LMs to solve complex reasoning tasks. While there have been attempts to fine-tune small models with explicit reasoning steps to tackle this issue, they require arduous reasoning annotation and often also task-specific training setups <ref type="bibr" target="#b26">(Nye et al., 2021;</ref><ref type="bibr" target="#b3">Cobbe et al., 2021)</ref>. Our approach, on the other hand, can be readily applied to novel downstream tasks, owing to the remarkable zero-shot reasoning ability of LM-based teachers <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>, without hand-crafted reasoning annotations or task-specific engineering. In essence, our method preserves the versatility of prompt-based CoT without demanding excessively large models.</p><p>We propose an extension to our method, referred to as diverse reasoning, which maximizes the teaching effects of Fine-tune-CoT by generating multiple reasoning solutions for each training sample. This can be achieved simply through repeated stochastic sampling. Diverse reasoning is motivated by the intuition that multiple reasoning paths can be used to solve complex, type-2 tasks (Evans, 2010). We posit that such diversity in reasoning paths as well as linguistic templates can substantially aid in fine-tuning for complex reasoning<ref type="foot" target="#foot_1">3</ref> .</p><p>We perform empirical evaluations of Fine-tune-CoT and diverse reasoning on various tasks and model sizes using publicly available GPT-3 models. Our fine-tuning approach elicits notable reasoning performance in small models on complex tasks, whereas previous prompt-based methods achieve near-random performance. We show that small models under Fine-tune-CoT even outperform their very large teachers in some tasks. With diverse reasoning, we find that the performance of Finetune-CoT is highly scalable, and leads to high sample efficiency and notable reasoning performance even with few-shot training examples. We conduct thorough samples studies and ablations of Finetune-CoT and its performance on a multitude of datasets, while demonstrating its value on much smaller models. In doing so, we shed light on important nuances of fine-tuning on CoT reasoning that have not been considered in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Downstream transfer in language models Much previous work established a "pre-train and fine-tune" paradigm for enhancing large language models' performance on downstream tasks <ref type="bibr" target="#b29">(Radford et al., 2018;</ref><ref type="bibr" target="#b30">Raffel et al., 2020;</ref><ref type="bibr" target="#b6">Dong et al., 2019;</ref><ref type="bibr" target="#b41">Vaswani et al., 2017;</ref><ref type="bibr" target="#b5">Devlin et al., 2018)</ref>. However, given that fine-tuning requires a very large dataset of task-specific labeled examples, and often does not generalize well to out-of-distribution settings, it is not always easily applicable <ref type="bibr" target="#b23">(Liu et al., 2021;</ref><ref type="bibr" target="#b11">Hendrycks et al., 2020)</ref>.</p><p>More recent literature exhibits a paradigm shift towards "prompting" the model to predict the desired output <ref type="bibr" target="#b23">(Liu et al., 2021;</ref><ref type="bibr" target="#b30">Raffel et al., 2020)</ref>. Large LMs can exhibit strong performance in this setting <ref type="bibr" target="#b0">(Brown et al., 2020)</ref>. For smaller models to be able to perform similarly, additional engineering is usually required <ref type="bibr" target="#b8">(Gao et al., 2021;</ref><ref type="bibr">Schick and Sch?tze, 2021b;</ref><ref type="bibr" target="#b34">Schick et al., 2020)</ref>. In more complex tasks, the performance of very large LMs under prompting can be boosted with chain-ofthought (CoT) prompting <ref type="bibr">(Wei et al., 2022b)</ref>. This approach is preceded by the idea of using samples with explicit reasoning steps for fine-tuning a model, which however usually requires human reasoning annotation and task-specific training setups <ref type="bibr" target="#b26">(Nye et al., 2021;</ref><ref type="bibr" target="#b3">Cobbe et al., 2021)</ref>. <ref type="bibr">Li et al., 2022b;</ref><ref type="bibr" target="#b15">Huang et al., 2022)</ref>, but its effects on finetuning have not been explicitly investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chain-of-thought</head><p>In few-shot CoT prompting, the model is fed examples of step-by-step reasoning in natural language. It can then generate intermediate reasoning steps leading to a problem solution. This improves performance on a wide range of tasks <ref type="bibr">(Wang et al., 2022b)</ref>. Additionally, LLMs can perform well in an unsupervised task-agnostic setting, using zero-shot CoT <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>. This requires no fine-tuning or task specific conditioning, and substantially outperforms standard zero-shot learning and sometimes even few-shot learning on a wide number of tasks. Yet, prior work has shown that CoT requires extremely large models for optimal performance <ref type="bibr" target="#b13">(Hoffmann et al., 2022;</ref><ref type="bibr" target="#b2">Chowdhery et al., 2022)</ref>. In our work, we contrast this by showing how to utilize CoT reasoning methods for smaller models by fine-tuning them on rationales generated by a very large model. Using various LLMgenerated explanations for fine-tuning smaller models has been successfully used in prior work <ref type="bibr">(Li et al., 2022a)</ref>. Also, a similar approach to ours is mentioned in <ref type="bibr" target="#b15">(Huang et al., 2022)</ref>; however we note that the focus of this concurrent work lies on using few-shot CoT to self-generate fine-tuning examples by and for very large proprietary models. The authors provide a brief glimpse into using zero-shot CoT to generate reasoning examples for fine-tuning smaller distilled models, but the results are limited to one dataset and very large models that are also inaccessible to the general community. In contrast, we provide a rich set of results and qualitative/quantitative analysis on a wide range of datasets, using open-source models that are small and accessible to everyone.</p><p>Knowledge distillation Typically, knowledge distillation (KD) refers to training small models derived from large models in order to reduce model size and latency, while still preserving accuracy and capacity to generalize <ref type="bibr" target="#b12">(Hinton et al., 2015;</ref><ref type="bibr" target="#b33">Sanh et al., 2019)</ref>. Essentially, KD is a form of model compression, making efficient deployment to capacity-limited devices possible <ref type="bibr" target="#b1">(Bucilua et al., 2006)</ref>. We note that our work could also be considered a distant variant of KD (see <ref type="bibr" target="#b10">Gou et al. (2021)</ref> for a survey), similar to works on improving prompt-based methods such as <ref type="bibr" target="#b46">Yoo et al. (2021)</ref>; <ref type="bibr">Schick and Sch?tze (2021b,a)</ref>; <ref type="bibr" target="#b47">Zelikman et al. (2022)</ref>. It most closely resembles a form of data-free distillation <ref type="bibr" target="#b24">(Micaelli and Storkey, 2019;</ref><ref type="bibr" target="#b25">Nayak et al., 2019;</ref><ref type="bibr" target="#b37">Shen et al., 2021)</ref>, where the otherwise inaccessible transfer data is synthetically generated from a large teacher model. Similarly, sequence-level distillation, i.e. training a smaller student model on the output of beam search of a larger teacher, can make neural machine translation performance more efficient without significant loss <ref type="bibr" target="#b16">(Kim and Rush, 2016)</ref>. Related KD approaches have also been used to improve the performance of non-autoregressive translation by training on output generated by an autoregressive translation model <ref type="bibr" target="#b49">(Zhou et al., 2020)</ref>. Despite being similar in spirit, our method still distinguishes itself from such previous work. The role of the teacher model in our method is to teach the notion of intermediate reasoning. It is not the specific output that is the main supervising signal for reasoning, but rather the generation's structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Chain of Thought Fine-Tuning</head><p>We propose Fine-tune-CoT, a task-agnostic approach to enable chain-of-thought reasoning in small language models. The core idea is to generate reasoning samples from very large teacher models using prompt-based CoT methods and subsequently fine-tune small student models using the generated samples. This approach preserves the advantages of task-agnostic prompt-based CoT methods while overcoming their reliance on prohibitively large models. To maximize versatility, we use the most recent Zero-shot-CoT prompting method <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref> on teacher models, as it does not require any hand-annotated reasoning explanations. We note that our approach is not limited to this way of prompting the teacher model. In the following, we characterize Fine-tune-CoT in three distinct steps, as also shown in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>Step 1. Reasoning generation First, we utilize a large teacher model to generate CoT reasoning explanations for a given task. Consider a standard sample S i consisting of a question q i and its true answer a i . Using Zero-shot-CoT<ref type="foot" target="#foot_2">4</ref> , we prompt the teacher model to generate a reasoning explanation, or rationale, ri to solve question q i and make a final answer prediction ?i . The resulting text sequence, including the prompt and generations, takes the following form: "Q: &lt;q i &gt;. A: Let's think step by step. &lt;r i &gt; Therefore, the answer is &lt;? i &gt;".</p><p>Step 2. Curation To prepare fine-tuning samples, we filter the generated samples and reformat them into prompt-completion pairs. For filtering, we simply compare the final prediction of the teacher model ?i with the ground-truth answer a i , following previous works <ref type="bibr" target="#b47">(Zelikman et al., 2022;</ref><ref type="bibr" target="#b15">Huang et al., 2022)</ref>. For all such instances i where ?i = a i , we repackage (S i , ri , ?i ) into a reasoning sample S i = (p i , c i ), a prompt-completion pair. Since our method is aimed at training efficient taskspecific models, we use a special-character based text format to minimize token usage. Specifically, p i and c i each take the form of "&lt;q i &gt; ###" and "&lt;r i &gt; --&gt; &lt;a i &gt; END", respectively. We note that filtering based on answer predictions does not ensure the correctness of the rationales, especially for multi-choice questions where random-guessing is likely. However, this has not been addressed in previous works. We provide an analysis on rationale filtering in Subsection 4.2, where we discuss a trade-off between sample quantity and quality.</p><p>Step 3. Fine-tune Finally, we fine-tune a small pre-trained student model on the assembled reasoning samples using the widely accessible OpenAI API. We use the same training objective of that used during pre-training, i.e., autoregressive language modeling objective, or next-token prediction <ref type="bibr" target="#b29">(Radford et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diverse reasoning</head><p>To maximize the sample efficiency of Fine-tune-CoT, we can generate multiple reasoning explanations for each training sample, thereby augmenting the fine-tuning data. We refer to this as diverse reasoning. In detail, for a given sample S i , instead of applying Zero-shot-CoT using greedy decoding to obtain a single explanationanswer pair (? i , ?i ), we use a stochastic sampling strategy, i.e., temperature sampling with large T , to obtain D distinct generations {(r ij , ?ij )} D j . Subsequent reasoning sample curation and fine-tuning then proceeds as before. We refer to D as the degree of reasoning diversity. Diverse reasoning is motivated by the intuition that multiple reasoning paths can be used to solve complex tasks, i.e., type-2 tasks <ref type="bibr" target="#b7">(Evans, 2010)</ref>. In sample studies, we confirm that diverse reasoning samples contain various reasoning paths as well as linguistic templates, which can also be observed in the fine-tuned students. This is similar to <ref type="bibr">Wang et al. (2022b)</ref>; <ref type="bibr" target="#b47">Zelikman et al. (2022)</ref>; <ref type="bibr" target="#b15">Huang et al. (2022)</ref>, where diverse reasoning paths are generated and marginal-ized to find the optimal answer. Diverse reasoning also draws parallels with Yoo et al. ( <ref type="formula">2021</ref>) which utilizes the generative power of LLMs to augment training data with synthesized samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Tasks and datasets We evaluate our method on 12 datasets pertaining to 4 different categories of complex reasoning, following <ref type="bibr" target="#b17">Kojima et al. (2022)</ref>: arithmetic, commonsense, symbolic, and other. For arithmetic reasoning, we evaluate on SingleEq <ref type="bibr" target="#b18">(Koncel-Kedziorski et al., 2015)</ref>, AddSub <ref type="bibr" target="#b14">(Hosseini et al., 2014)</ref> and MultiArith <ref type="bibr" target="#b32">(Roy and Roth, 2016)</ref> from the Math World Problem Repository <ref type="bibr" target="#b19">(Koncel-Kedziorski et al., 2016)</ref> as well as more recent datasets, GSM8K <ref type="bibr" target="#b3">(Cobbe et al., 2021)</ref>, AQUA-RAT <ref type="bibr" target="#b22">(Ling et al., 2017)</ref> and SVAMP <ref type="bibr" target="#b28">(Patel et al., 2021)</ref>. For commonsense reasoning, we use Com-monsenseQA <ref type="bibr" target="#b40">(Talmor et al., 2018)</ref> and StrategyQA <ref type="bibr" target="#b9">(Geva et al., 2021)</ref>. Last Letters, Coin Flip are used to evaluate symbolic reasoning <ref type="bibr">(Wei et al., 2022b)</ref> and Date Understanding and Tracking Shuffled Objects from BIG-bench <ref type="bibr" target="#b38">(Srivastava et al., 2022)</ref>  Models We evaluate our methods on the GPT-3 family of models, as inference and fine-tuning operations are readily available via APIs provided by OpenAI. For the teacher model we use text-davinci-002, which is based on the largest InstructGPT model with 175B parameters <ref type="bibr" target="#b27">(Ouyang et al., 2022)</ref> <ref type="foot" target="#foot_3">5</ref> . For student models, we use ada, babbage and curie, each based on GPT-3 <ref type="bibr" target="#b0">(Brown et al., 2020</ref>) 350M (0.3B), 1.3B and 6.7B.</p><p>The student models are 25-500x smaller than the very large teacher model, thus considerably more feasible for real-world deployment. Table <ref type="table" target="#tab_0">1</ref> provides a summary of the models used in our study.</p><p>Baseline methods We provide a comparison of our method, Fine-tune-CoT, with three baseline methods: Zero-shot, Fine-tune, and Zero-shot-CoT <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>. Fine-tune refers to finetuning with original training samples {(q i , a i )} i , specifically using prompts and completions in the form of "&lt;q i &gt; ###" and "&lt;a i &gt; END". An LM's fine-tune performance represents its ability to learn to solve complex tasks using only questionanswer pairs, without explicit supervision on reasoning. Zero-shot-CoT performance represents task-agnostic prompt-based CoT performance. The taxonomy of methods used in our study is outlined in Table <ref type="table" target="#tab_1">2</ref> for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>In this section, we present results on the reasoning performance of small models using Fine-tune-CoT, across various model scales. We also demonstrate how the accuracy of our method scales with the number of diverse reasoning samples.</p><p>In a sample study, we further identify the overall strengths and weaknesses of our method, demonstrating failure patterns and factors for success. For more details on this line of analysis, we refer to Appendix C.</p><p>Fine-tuning elicits complex reasoning in small models Table <ref type="table" target="#tab_3">3</ref> summarizes the accuracy of student models using the proposed Fine-tune-CoT, compared to the existing task-agnostic prompting baseline, Zero-shot-CoT <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>, as well as standard zero-shot prompting and finetuning using standard samples without any reasoning. While Zero-shot-CoT exhibits remarkable performance on the very large 175B model <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>, it fails to enable complex reasoning in smaller models, including the 6.7B model, showing near-negligible performance across all tasks. On the other hand, Fine-tune-CoT elicits notable  reasoning performance on the same tasks, demonstrating significant gains over Zero-shot-CoT using smaller models. For complex arithmetic, Fine-tune-CoT achieves a notable 33% accuracy on Multi-Arith while Zero-shot-CoT only reaches 5%. For two commonsense reasoning tasks, Fine-tune-CoT is shown to outperform the near-random performance of Zero-shot-CoT by 37%p and 5%p, respectively. Fine-tune-CoT performance is most notable in relatively simple tasks including other reasoning tasks (Date Understanding, Tracking Shuffled Objects) and symbolic reasoning (Last Letter Concatenation, Coin Flip), while Zero-shot CoT fails to overcome random-guess performance.</p><p>Small models can outperform very large teachers in reasoning Table <ref type="table" target="#tab_3">3</ref> also shows that Finetune-CoT is highly effective on small models. In two tasks with less complexity, Shuffled Objects and Coin Flip, Fine-tune-CoT is shown to outperform the 175B teacher model using 1.3B and 6.7B parameters, respectively, i.e., reducing the number of required parameters by approx. 25-100x. We also find that Fine-tune-CoT with the very small 0.3B model consistently outperforms the 6.7B model under Zero-shot-CoT.</p><p>Fine-tune vs Fine-tune-CoT Similarly, we find that Fine-tune-CoT outperforms vanilla fine-tuning across a wide range of various tasks, as shown in Table 3. This is most pronounced in Date Understanding and Shuffle Objects, for which fine-tuning with standard question-answer pairs results in random-  ing Shuffled Objects, where Fine-tune-CoT accuracy surpasses that of vanilla Fine-tune by nearly twofold. Moreover, Fine-tune-CoT has the unique advantage of being able to benefit from multiple teacher-generated reasoning paths for a given question, as we will discuss in the following paragraph.</p><p>Diverse reasoning substantially improves Finetune-CoT performance To examine the learning effects of diverse reasoning, we apply Finetune-CoT using 1-64 reasoning explanations per sample across three model scales on SVAMP. Table 4 shows that diverse reasoning can significantly improve the performance of student models using Fine-tune-CoT. With 64 reasoning explanations per sample, the 0.3B model shows a near five-fold improvement, surpassing the baseline Fine-tune-CoT performance of the 6.7B model. Moreover, we find that diverse reasoning can boost the performance of Fine-tune-CoT to surpass that of vanilla fine-tuning, across all model sizes.</p><p>Fine-tune-CoT with diverse reasoning is highly sample efficient We consider applying diverse reasoning in the few-shot data regime to maximize the utility of data samples for Fine-tune-CoT. Using as few as 32 samples, we see that the performance of the 6.7B model scales with the reasoning diversity, improving the efficacy of Fine-tune-CoT. We do however observe limited performance using very few, i.e., 8 samples even when using diverse reasoning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>In this section, we bring to attention several critical nuances in fine-tuning for CoT which have not been addressed in previous or concurrent work <ref type="bibr" target="#b47">(Zelikman et al., 2022;</ref><ref type="bibr">Li et al., 2022a;</ref><ref type="bibr" target="#b15">Huang et al., 2022)</ref>. We provide a set of analyses to shed light on these issues.</p><p>Templated datasets Upon inspection, we found that many datasets contain groups of samples which share common templates. This brings into question the validity of naive samplewise data split, as it has the potential to leak the same templates into the train and test sets. To investigate whether the student models are truly learning to reason rather than matching simple patterns, we manually group samples by template and evaluate Fine-tune-CoT using a template-wise data split. Table <ref type="table" target="#tab_5">5</ref> shows the performance of Fine-tune-CoT when using sample-wise vs template-wise split, using the same train-test ratio of 70:30. While student performance is typically lower with a template-wise split, it still significantly outperforms random guess performance, as well as zero-shot and vanilla fine-tuning baselines shown in Table <ref type="table" target="#tab_3">3</ref>. This reaffirms that Fine-tune-CoT is able to elicit complex reasoning capabilities in small language models.</p><p>Rationale filtering It is possible for the teacher model to answer correctly despite incorrect reasoning, especially in multi-choice questions where the random-guess probability is significant. To investigate the potential impact of a better filtering scheme (as opposed to our baseline answer-based filtering) we manually annotate the correctness of rationales from the teacher model and evaluate student performance when fine-tuning on correctly reasoned samples. based on answer predictions vs golden samples, hand-picked based on correctness of rationales. We find that 28% of correct samples have incorrect rationales-significantly more than the randomguess performance of 17.12%, indicating the importance of filtering. Surprisingly, we however find that answer-based filtering outperforms the more stringent human-filtering by 5-11%, given the same initial samples. When we match the number of samples post-filtering (via undersampling), we do find that fine-tuning on golden samples outperforms that on correct samples by 5-8%. These results suggest that there is a tradeoff between the quality and quantity of reasoning samples which must be addressed when considering sample-filtering methods.</p><p>Sequence length Following the canonical setting for Zero-shot-CoT, we limit the max sequence length, or max tokens, allowed for the teachergenerated rationale and student reasoning predictions, denoted L r , L p , to 128 initially, following <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>. However, we find that this can be insufficient in many datasets. Allowing for longer inference, we observe that model performance improves significantly on AQUA and commonsense reasoning tasks (Appendix Table <ref type="table" target="#tab_10">9</ref>). Sample inspection shows that rationales with over ?500 tokens are typically repetitive or too digressive. To investigate the effect of the max length L r of the teacher rationale on fine-tuning, we compare student performance using L e = {128, 512} (Table <ref type="table" target="#tab_6">7</ref>). The effect of L r on student performance varies across datasets, and increased L e does not necessarily improve student performance on tasks that require longer rationales, such as AQUA. Finally, we examine the length distribution of the generated rationales from the teacher model and student trained on short (L r = 128) and long (L r = 512) reasoning samples, respectively (Appendix Figure <ref type="figure">4</ref>). We find that the distribution is different for each dataset. Notably, we find that while the distributions from the long students were similar to that of the teacher, the generated rationale from the short students were typically limited to less than ?128 tokens. These findings are in line with the intuition that different tasks require different lengths of rationales, and suggest that careful consideration is needed in determining parameters related to sequence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Versatility and accessibility of Fine-tune-CoT Owing to the versatility of the underlying promptbased generation methods, e.g., Zero-shot-CoT, our method can be readily applied to any complex task without any task-specific engineering. In fact, it is possible to generate samples from very large teacher models using readily and publicly available APIs such as those provided by OpenAI, without the need to use proprietary or otherwise specialized models. Fine-tuning and inference on student models can also be performed on much more accessible hardware, in contrast to working with very large models. This can reduce long term computational costs and minimize environmental impact, while making our method fully accessible to a wide community.</p><p>Optimizing for efficiency in single tasks We note that being able to use small models to solve tasks also implies that these models do not need to excel at everything. When using large models, which are highly expensive to train and fine-tune, the requirement for their efficient use is that they need to perform well on any task. In contrast, using small models allows for a more task-specific approach and optimization if needed, given that these models are cheap to train and easy to deploy.</p><p>As an example, we can consider optimal generation length, which could differ between tasks (see Subsection 4.2).</p><p>Towards concise answers Sample studies show that rationales output from student models may occasionally be repetitive and digressive. This is undesirable not only in terms of inference-time efficiency, but also the interpretability and utility of the rationales. As a minor optimization to inference computation, we construct our fine-tuning sample templates using special-character based delimiters instead of natural language used in concurrent work <ref type="bibr" target="#b15">(Huang et al., 2022)</ref> to minimize sequence length. Preliminary findings showed this had no significant impact on reasoning performance. More importantly, it is desirable to train student models to generate concise answers in terms of substance. Subsection 4.2 hints at the possibility for this, showing that fine-tuning on shorter reasoning samples causes the student model to also produce shorter rationales. We believe this is an important direction for future work.</p><p>Vanilla fine-tuning versus Fine-tune-CoT for complex tasks Fine-tuning on vanilla training samples actually shows meaningful performance in complex tasks. While the model is initially unable to solve tasks that require complex reasoning, but is able to learn this capability from simple questionanswer samples which do not explicitly contain reasoning explanations. This suggests that fine-tuning, i.e., the simple training objective of maximizing the likelihood of next-token prediction, enables the model to infer the reasoning process for solving tasks based on the final answer. We note that this success does not negate the motivation for using ft-CoT, seeing that our method still outperforms vanilla fine-tuning on the majority of benchmarks (and even more when we include diverse reasoning). Fine-tune-CoT also gives intermediate, easy to trace reasoning steps instead of a complete black box. This also fits with our previous observation that different sets of tasks may require different approaches for optimal performance when using small models.</p><p>Reasoning in small language models Table <ref type="table" target="#tab_4">4</ref> shows that Fine-tune-CoT performance scales with diversity of reasoning and amount of available samples. We note here that this also factors into a tradeoff between quantity and quality when it comes to samples used for fine-tuning student models: as we have found in the corresponding ablation study (Subsection 4.2), having fewer but perfectly curated reasoning samples is not necessarily as helpful as having a larger amount of reasoning samples that might not always be fully correct. This hints at the true role of the fine-tuning samples when it comes to enabling CoT reasoning in small models: it appears that learning to use the reasoning process of the teacher model by a larger number of observations as cues is more important than learning from a smaller amount of perfect reasoning. That is, the student model imitates the process of the teacher of splitting large tasks into smaller sub-tasks, without having to rely on the teacher's actual predictions in the training set. Intuitively, the student model might not have the same memorization and abstraction skills as the large teacher, but with enough cues to work with (i.e. a large enough amount of finetuning samples demonstrating how to use reasoning to get to an answer), it can in fact gain capabilities reminiscent of larger models. This assumption is supported by the observation that student models using Fine-tune-CoT can in fact generalize to previously unseen tasks when it comes to the way they perform intermediate reasoning.</p><p>Limitations and future work We note that the performance of our method is currently not stateof-the-art. However, it can benefit from advances in teacher models, as well as different prompting methods used in teacher models. For example, future work should include a wider array of teachers, such as the highly versatile ChatGPT, or text-davinci-003, which builds on Instruct-GPT. Furthermore, previous work shows that Fewshot CoT <ref type="bibr">(Wei et al., 2022b)</ref> can improve accuracy over Zero-shot-CoT by a wide margin , e.g., going from 78.7% to 93.0% when using only eight incontext reasoning samples on MultiArith <ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>. Both of these avenues are promising for future work. Another potential improvement may lie in using a different knowledge distillation method, such as sequence-level distillation that trains on the output of beam search in the teacher <ref type="bibr" target="#b16">(Kim and Rush, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have demonstrated how the power of large language models can be used to teach much smaller student models how to reason step-by-step. We do this by prompting a large model for chain-of-thought rationales, and using its completions as samples for a smaller model to fine-tune on. Our results show that this method significantly improves the performance of small models on a range of different tasks with high sample efficiency, and can even reach or exceed the teacher performance in many cases. We add to these findings with a rich set of ablation studies and analysis. By leveraging publicly available models with zero-shot prompting, we demonstrate a task-agnostic method to elicit reasoning performance in small models, accessible to the broader community.</p><p>A Experimental Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Generation</head><p>We use the publicly available OpenAI API to generate reasoning samples and reasoning predictions with our teacher and student models, respectively.</p><p>Maximum sequence length For the maximum sequence length of teacher-generated rationales, ri , we use L r = 128, following <ref type="bibr" target="#b17">Kojima et al. (2022)</ref>, unless stated otherwise. For the maximum sequence length of the student model predictions, we use L p = 1024, unless stated otherwise. We retroactively applied L p = 1024 as the default, after discovering that L p = 128 is insufficient for many tasks, as discussed in Subsection 4.2</p><p>Sampling temperature We use a sampling temperature of T = 0 for all generations, except diverse reasoning, to obtain deterministic results. For diverse reasoning, we use T = 0.7 to obtain unique generations, following a similar approach from <ref type="bibr">Wang et al. (2022b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Fine-tuning</head><p>We use the publicly available OpenAI API to finetune student models, based on GPT-3. We use the default parameters provided by the API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Answer cleansing</head><p>We follow the method used in <ref type="bibr" target="#b17">Kojima et al. (2022)</ref> to cleanse answers generated by models to assess their correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>We provide a summary of datasets used in our experiments in Table <ref type="table" target="#tab_9">8</ref>. We consider the 10 datasets from <ref type="bibr" target="#b17">Kojima et al. (2022)</ref>, used to measure reasoning performance. For Last Letter Concatenation and Coin Flip, we use the publicly available data provided by <ref type="bibr" target="#b17">Kojima et al. (2022)</ref>.</p><p>Train </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Sample Study</head><p>To understand the strengths and weaknesses of our method, we randomly choose 50 samples per dataset and analyze the reasoning performance of Fine-tune-CoT. To do so, we compare its generations for these 50 samples with (1) the output of the large teacher model, (2) a student model using zero-shot-CoT and (3) a student model using fine-tuning without chain of thought reasoning. We show representative examples in Tables <ref type="table" target="#tab_0">11</ref><ref type="table" target="#tab_1">12</ref><ref type="table" target="#tab_3">13</ref><ref type="table" target="#tab_4">14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Weaknesses and error analysis</head><p>For our analysis of the method's weaknesses, we take a look at datasets where we find particularly bad performance compared to other methods, in particular fine-tuning. We summarize our observations below. First, we observe that the sets GSM8K and AQUA are too difficult for a small student model, in particular given that already the teacher model gets below 50% accuracy on both. In fact, even the tasks that the models answer correctly are dominated by bad reasoning and merely randomly correct answers due to the high complexity of the tasks <ref type="bibr">(Tab. 11a,</ref><ref type="bibr">b)</ref>. For AQUA in particular, we note that while we occasionally find meaningful reasoning in the 6.7B student model, students clearly cannot sufficiently learn to solve the tasks. A similar, if less salient, issue arises for StrategyQA. Here, the teacher also performs only 3% above the random guess accuracy of 50%. While the smaller student models actually manage to improve on this performance, in particular vanilla fine-tuning, the errors arising in fine-tune-CoT then often look very similar to the ones in the large teacher model. None of the models' capacities for identifying salient pieces of information and putting them together suffice in order to generate mostly correct answers. Very often, the models instead merely look up information related to the question, but cannot synthesize an answer from it (Tables <ref type="table" target="#tab_0">11c,</ref><ref type="table" target="#tab_1">12a</ref>).</p><p>Next, we note that small models exhibit weak arithmetic skills. This has already been discussed in previous literature, where calculation capability has been found to scale with model size <ref type="bibr">(Wei et</ref>   2022a). Especially in SingleEq (Table <ref type="table" target="#tab_1">12b</ref>) and AddSub (Table <ref type="table" target="#tab_1">12c</ref>), a majority of errors simply arise from wrong calculations, less so bad reasoning. This is also a major factor in the bad performance our method exhibits on SVAMP as well as GSM8K; even correct multistep reasoning cannot compensate for the fact that the model's arithmetic tends to be wrong already on intermediate steps (Tables <ref type="table" target="#tab_1">12d,</ref><ref type="table" target="#tab_3">13a</ref>). The teacher model then does better on these tasks, given its larger size.</p><p>We furthermore find that models seem sensitive to how a question is formulated. This is noticeable in all datasets, in particular in SVAMP. In particular, we observe this issue when there is redundant information present in the question (Table <ref type="table" target="#tab_3">13b</ref>). Such cases elicit wrong reasoning, or lead the model to become stuck on the question, similarly to what usually happens with zero-shot CoT in the student model. Other common sources of errors are when hidden variables make up the first part of the task (i.e. those tasks that force the model to calculate a previously unknown value that is described in the first sentence, see Table <ref type="table" target="#tab_3">13c</ref>), or when the model encounters overloaded words (e.g. "landing", see Table <ref type="table" target="#tab_3">13d</ref>). Another common source of errors is We also observe samples where the model gets stuck on an intermediate result (Table <ref type="table" target="#tab_4">14a</ref>). This observation fits with previous findings that language models have a recency bias <ref type="bibr" target="#b48">(Zhao et al., 2021)</ref>.</p><p>Meanwhile, when looking at our method's performance in CommonsenseQA, we note that in fact its reasoning skills in this sample set are not the issue for many of the tasks. We find that the student model utilizing ft-CoT can generate logical reasoning paths for many of the samples that are marked as false (Table14b). Rather, the exact answer is often very subjective, making it very difficult to guess the correct output from logical reasoning alone (Ta-ble14c). CommonsenseQA thus is not an ideal benchmark when judged on accuracy, but gives insight into how well the model can reason. Also, comparing the negative samples from this dataset with the negative samples from StrategyQA, we note that while the sources of eventual error are usually different (in StrategyQA, answer synthesis from a number of different facts is more of an issue), the generation of the reasoning itself is very often correct for both of these datasets.</p><p>Importantly, we note that for each dataset, there seems to be a difference between "easy" and "hard" instances. When we consider the accuracy of the teacher and other student models (using fine-tuning or zero-shot-CoT) on tasks where our method fails, we find that it is always lower than on tasks where our method is successful. That is, successes tend to be aligned across the different methods, and so are failures. We can hypothesize that factors like content bias may play a role here; language models have been found to fail depending on context and content of the task, in a way similar to human reasoners <ref type="bibr" target="#b4">(Dasgupta et al., 2022)</ref>. We can identify samples that hint at this issue when we look at questions that include phrasing that seems contradictory or counterintuitive (Table14d). Additionally, previous work shows that GPT-3 exhibits a performance gap between instances including terms that are frequent in the pretraining corpus, and instances including less frequent terms <ref type="bibr" target="#b31">(Razeghi et al., 2022)</ref>. This kind of leakage can contribute to uneven performance on a multitude of (especially numerical) tasks across different methods and model sizes, as the presence of more frequent terms makes it easier to perform calculations. We can then surmise the observed differences in accuracy to stem from the various sources of errors for each method; we note that e.g. fine-tuning has much less room for error than fine-tune-CoT, which can additionally make mistakes on intermediate reasonings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Strengths</head><p>Having analyzed the main source of errors, we can now focus on the datasets that elicit good performance from our method. As arithmetic errors are one of the main reasons for bad performance of small student models, it comes as little surprise that our method performs best on datasets that are mainly text-based and do not require actual calculation skills, such as DateUnderstanding, Coin Flip, ShuffledObjects, and Last Letter Concatenation. These datasets also have very clear patterns in their tasks, which helps fine-tune-CoT to perform well by providing cues on how to solve a specific task. We note that in contrast, classic finetuning does not have an advantage in these datasets, and it gets significantly lower accuracy than finetune-CoT on all four. The same is also true for MultiArith, which we have used as a benchmark in the main text. While arithmetic errors cause the absolute accuracy of our method to be lower than the teacher, it significantly outperforms fine-tuning on MultiArith even without using diverse reasoning. Indeed, we find that also in the presence of arithmetic errors, our model reasons correctly in many cases. We can surmise that the heavily patterned nature of the tasks in MultiArith helps the student model to understand what is asked of it, eliciting correct reasoning. Additionally, we note that the presence of such patterns in successful datasets does not mean that our method overfits to existing templates. In our template-split analysis (Subsection 4.2), we in fact show that while tasks look similar to one another in certain datasets such as Date Understanding, the student model's reasoning does not rely on simply matching templates or memorizing particular solutions. This implies that our method can generalize to previously unseen tasks; the patterns in the datasets do not produce overfitting, but can be surmised to act as cues for the model's understanding of its current task.</p><p>We can further compare fine-tune-CoT with the purely prompt-based zero-shot-CoT in the student model. Here, we observe that the reasoning skills of a student using fine-tune-CoT can overcome the smaller model capacity (which proves to be completely prohibitive for zs-CoT to have any success on the various tasks), due to having been trained on many reasoning samples. Where zero-shot-CoT fails to reason and simply gets prompted to repeat the question or come up with answers that only vaguely pertain to the question, our method can in fact reason, even when it cannot eventually grasp the correct answer. This is particularly noticeable in the to QA datasets (Commonsense and Strategy). While our method does not always produce the required ground truth answer, such that it effectively still underperforms vanilla fine-tuning, it can put together a logical path from the question to the prediction, and thus elicits performance similar to the teacher performance despite the small model size. Fine-tune-CoT hence combines many of the advantages of vanilla fine-tuning and CoT reasoning on smaller models. Figure <ref type="figure">4</ref>: Distribution of the length of generated reasoning sequences from the 175B teacher model and fine-tuned 6.7B student models on four datasets. Student (Short) refers to baseline students that were fine-tuned on reasoning samples with maximum rationale sequence length of L r = 128, and Student (Long) refers to students that were fine-tuned on longer reasoning samples with L r = 512.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Detailed overview of the proposed Fine-tune-CoT method.Step 1: a very large teacher model is prompted to solve complex questions (yellow) by generating multi-step reasoning explanations (green).Step 2: generated completions are filtered based on the correctness of the final prediction (red). The question, rationale, and answer are used to compose a reasoning sample comprised of the prompt and a multi-step completion.Step 3: the curated reasoning samples are used to fine-tune a small, lightweight student to exhibit reasoning capabilities. The application of an LM-based teacher enables diverse reasoning-generating multiple distinct rationales for each original sample to enrich the fine-tuning data. This boosts the performance of student models without requiring human annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Sample efficiency of Fine-tune-CoT with diverse reasoning. Accuracy (%) of Fine-tune-CoT for 6.7B student models on SVAMP using the full datsaet or few-shot data, across varying degrees of diverse reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>are used for other reasoning. For Last Letters and Coin Flip, we use the publicly available data from<ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>. We provide details on datasets used in Appendix B List of models used in our experiments.</figDesc><table><row><cell>Model</cell><cell cols="2">Params Role</cell><cell>Model Name in API</cell></row><row><cell cols="2">InstructGPT 175B</cell><cell cols="2">Teacher text-instruct-002</cell></row><row><cell>GPT-3</cell><cell>6.7B</cell><cell cols="2">Student curie</cell></row><row><cell>GPT-3</cell><cell>1.3B</cell><cell cols="2">Student babbage</cell></row><row><cell>GPT-3</cell><cell>0.3B</cell><cell cols="2">Student ada</cell></row><row><cell>Method</cell><cell cols="3">Model Updates Output Usage CoT Sample Reference</cell></row><row><cell>Zero-shot</cell><cell></cell><cell></cell></row><row><cell>Zero-shot-CoT</cell><cell></cell><cell></cell><cell>(Kojima et al., 2022)</cell></row><row><cell>Fine-tune</cell><cell></cell><cell></cell><cell>(Radford et al., 2018)</cell></row><row><cell>Fine-tune-CoT</cell><cell></cell><cell></cell><cell>Ours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Taxonomy of methods used in our experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Fine-tune-CoT performance. Accuracy (%) of baseline zero-shot and fine-tune methods with and without CoT reasoning for student models on 12 tasks. 'Random' refers to random-guess performance derived based on the number of choices in tasks comprised of multichoice questions, i.e., the performance of a model that is only capable of outputting a random answer in the correct format.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Diverse reasoning performance. Accuracy (%) of Zero-shot-CoT, Fine-tune-CoT and Fine-tune-CoT with diverse reasoning samples for student models on SVAMP. 'Usage' refers to the ratio (%) of original training samples that are used for fine-tuning, i.e., had at least one correct reasoning output from the teacher.</figDesc><table><row><cell>Method</cell><cell>D Usage</cell><cell cols="3">Model 0.3B 1.3B 6.7B</cell></row><row><cell>Zero-shot-CoT</cell><cell></cell><cell>3.00</cell><cell>1.00</cell><cell>1.33</cell></row><row><cell>Fine-tune</cell><cell></cell><cell>7.67</cell><cell cols="2">14.33 20.67</cell></row><row><cell>Fine-tune-CoT</cell><cell>64.0</cell><cell>5.00</cell><cell>8.00</cell><cell>12.67</cell></row><row><cell></cell><cell>1 64.6</cell><cell>5.33</cell><cell>7.00</cell><cell>13.33</cell></row><row><cell></cell><cell>2 77.3</cell><cell>6.00</cell><cell>8.33</cell><cell>15.00</cell></row><row><cell></cell><cell>4 86.9</cell><cell>9.00</cell><cell cols="2">10.33 18.00</cell></row><row><cell>Fine-tune-CoT</cell><cell>8 91.1</cell><cell>9.33</cell><cell cols="2">12.33 24.00</cell></row><row><cell></cell><cell>16 95.0</cell><cell cols="3">12.00 17.33 29.67</cell></row><row><cell></cell><cell>32 96.7</cell><cell>8.67</cell><cell cols="2">18.67 29.00</cell></row><row><cell></cell><cell>64 97.4</cell><cell cols="3">14.33 16.33 30.33</cell></row><row><cell cols="5">guess accuracy, while fine-tuning on samples that</cell></row><row><cell cols="5">contain intermediate reasoning achieves approx. 2-</cell></row><row><cell cols="5">4x random-performance. We note that fine-tune</cell></row><row><cell cols="5">occasionally outperforms Fine-tune-CoT, not only</cell></row><row><cell cols="5">in single-step tasks such as SingleEq and AddSub,</cell></row><row><cell cols="5">but also complex tasks such as SVAMP and com-</cell></row><row><cell cols="5">monsense reasoning. This hints at the ability of</cell></row><row><cell cols="5">LMs to learn task-specific reasoning by fine-tuning</cell></row></table><note><p>on simple question-answer pairs without explicit reasoning examples. Nevertheless, Fine-tune-CoT performance shows a more reliable scaling curve with model size and demonstrates a clear advantage in tasks that require multiple steps, such as Track-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Sample-wise vs template-wise split. Accuracy (%) of Fine-tune-CoT for student models on two moderately templated datasets when using a samplewise vs template-wise train-test split.</figDesc><table><row><cell cols="2">Params Split</cell><cell cols="2">MultiArith Date Understanding</cell></row><row><cell>0.3B</cell><cell cols="2">Sample-wise Template-wise 5.35 5.56</cell><cell>17.12 22.22</cell></row><row><cell>1.3B</cell><cell cols="2">Sample-wise Template-wise 7.49 13.89</cell><cell>38.74 35.19</cell></row><row><cell>6.7B</cell><cell cols="2">Sample-wise Template-wise 21.39 34.44</cell><cell>60.36 49.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Longer reasoning samples for fine-tuning.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Filter</cell><cell cols="3">Samples 0.3B 1.3B 6.7B</cell></row><row><cell cols="2">Zero-shot-CoT</cell><cell></cell><cell></cell><cell cols="2">10.81 14.41 15.32</cell></row><row><cell></cell><cell></cell><cell>Correct</cell><cell>170</cell><cell cols="2">17.12 38.74 60.36</cell></row><row><cell cols="2">Fine-tune-CoT</cell><cell cols="2">Correct  ? 123</cell><cell cols="2">17.12 19.82 50.45</cell></row><row><cell></cell><cell></cell><cell>Golden</cell><cell>123</cell><cell cols="2">17.12 27.93 55.86</cell></row><row><cell cols="6">Table 6: Better rationale filtering. Accuracy (%) of</cell></row><row><cell cols="6">Fine-tune-CoT on student models when using samples</cell></row><row><cell cols="6">filtered using answer predictions (Correct), or filtered</cell></row><row><cell cols="6">by humans based on the correctness of the rationale</cell></row><row><cell cols="6">(Golden). Correct  ? refers to using a randomly sampled</cell></row><row><cell cols="6">subset of the correct samples to match the number of</cell></row><row><cell cols="2">golden samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Params</cell><cell cols="5">Max Tokens GSM8K AQUA Common Strategy</cell></row><row><cell>0.3B</cell><cell>128 512</cell><cell>3.11 3.41</cell><cell>23.62 15.35</cell><cell>32.68 32.10</cell><cell>52.55 52.98</cell></row><row><cell>1.3B</cell><cell>128 512</cell><cell>4.70 3.79</cell><cell>19.69 18.90</cell><cell>43.08 43.65</cell><cell>52.69 53.42</cell></row><row><cell>6.7B</cell><cell>128 512</cell><cell>6.75 7.96</cell><cell>24.02 18.90</cell><cell>56.76 58.15</cell><cell>55.02 54.15</cell></row><row><cell>Random</cell><cell></cell><cell>1.01</cell><cell>20.44</cell><cell>20.01</cell><cell>50.18</cell></row></table><note><p><p><p>Table</p>6</p>compares the Fine-tune-CoT performance of student models on Date Understanding when using correct samples filtered Accuracy (%) of Fine-tune-CoT for student models on four datasets which require longer rationales, when trained on reasoning samples with maximum rationale sequence lengths of L e = 128, 512.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>al.,    </figDesc><table><row><cell>Dataset</cell><cell cols="5">Choices Training Samples Test Samples Data Split References</cell></row><row><cell>SingleEq</cell><cell>-</cell><cell>356</cell><cell>152</cell><cell>70:30</cell><cell>Koncel-Kedziorski et al. (2015)</cell></row><row><cell>AddSub</cell><cell>-</cell><cell>276</cell><cell>119</cell><cell>70:30</cell><cell>Hosseini et al. (2014)</cell></row><row><cell>MultiArith</cell><cell>-</cell><cell>420</cell><cell>180</cell><cell>70:30</cell><cell>Roy and Roth (2016)</cell></row><row><cell>GSM8K</cell><cell>-</cell><cell>7473</cell><cell>1319</cell><cell>Original</cell><cell>Cobbe et al. (2021)</cell></row><row><cell>AQUA-RAT</cell><cell>5</cell><cell>10000</cell><cell>254</cell><cell>Custom</cell><cell>Ling et al. (2017)</cell></row><row><cell>SVAMP</cell><cell></cell><cell>700</cell><cell>300</cell><cell>70:30</cell><cell>Patel et al. (2021)</cell></row><row><cell>Date Understanding</cell><cell>5-6</cell><cell>258</cell><cell>111</cell><cell>70:30</cell><cell>Srivastava et al. (2022)</cell></row><row><cell>Tracking Shuffled Objects</cell><cell>3</cell><cell>525</cell><cell>225</cell><cell>70:30</cell><cell>Srivastava et al. (2022)</cell></row><row><cell>Last Letter Concatenation</cell><cell>-</cell><cell>350</cell><cell>150</cell><cell>70:30</cell><cell>Wei et al. (2022b); Kojima et al. (2022)</cell></row><row><cell>Coin Flip</cell><cell>2</cell><cell>350</cell><cell>150</cell><cell>70:30</cell><cell>Wei et al. (2022b); Kojima et al. (2022)</cell></row><row><cell>CommonSenseQA</cell><cell>5</cell><cell>9741</cell><cell>1221</cell><cell>Original</cell><cell>Talmor et al. (2018)</cell></row><row><cell>StrategyQA</cell><cell>2</cell><cell>1603</cell><cell>687</cell><cell>70:30</cell><cell>Geva et al. (2021)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Description of datasets used in our study.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Ablation on maximum sequence length. Accuracy (%) of Zero-shot-CoT on the teacher model and Finetune-CoT on student models, based on maximum sequence length. Values in parentheses refer to the percentage of generated rationales that were completed within the allotted maximum sequence length.</figDesc><table><row><cell>Model</cell><cell>Max Tokens</cell><cell>Single Eq</cell><cell>Add Sub</cell><cell cols="4">Multi GSM8K Aqua SVAMP Arith</cell><cell cols="3">Date Understanding Objects Letter Shuffled Last</cell><cell>Coin Flip</cell><cell cols="2">Common Strategy SenseQA QA</cell></row><row><cell>Teacher</cell><cell>128</cell><cell>81.18 (84.83)</cell><cell>75.72 (90.22)</cell><cell>76.90 (95.24)</cell><cell>42.42 (69.85)</cell><cell>29.63 (44.04)</cell><cell>64.00 (86.57)</cell><cell>65.89 (98.06)</cell><cell>54.10 (97.14)</cell><cell>57.43 (99.71)</cell><cell>89.71 (97.14)</cell><cell>59.86 (82.55)</cell><cell>53.40 (71.55)</cell></row><row><cell>(175B)</cell><cell>2048</cell><cell>81.18 (84.83)</cell><cell>75.72 (90.22)</cell><cell>76.48 (94.29)</cell><cell>47.73 (99.34)</cell><cell>34.77 (96.42)</cell><cell>66.00 (99.00)</cell><cell>63.28 (97.14)</cell><cell>54.10 (97.14)</cell><cell>57.43 (99.71)</cell><cell>89.71 (97.14)</cell><cell>59.40 (99.92)</cell><cell>53.03 (99.69)</cell></row><row><cell>Student</cell><cell>128</cell><cell>7.24 (96.05)</cell><cell>6.72 (99.16)</cell><cell>5.56 (96.11)</cell><cell>3.11 (74.75)</cell><cell>16.54 (45.67)</cell><cell>4.33 (91.33)</cell><cell>17.12 (100.00)</cell><cell>48.89 (100.00)</cell><cell cols="2">50.67 (100.00) (100.00) 99.33</cell><cell>30.30 (86.73)</cell><cell>47.16 (87.63)</cell></row><row><cell>(0.3B)</cell><cell>1024</cell><cell>7.24 (98.68)</cell><cell>6.72 (99.16)</cell><cell>6.11 (97.22)</cell><cell>3.11 (99.77)</cell><cell>23.62 (100.00)</cell><cell>5.00 (97.33)</cell><cell>17.12 (100.00)</cell><cell>49.33 (100.00)</cell><cell cols="2">50.67 (100.00) (100.00) 99.33</cell><cell>32.68 (100.00)</cell><cell>52.55 (99.71)</cell></row><row><cell>Student</cell><cell>128</cell><cell>11.18 (92.76)</cell><cell>11.76 (96.64)</cell><cell>13.89 (98.89)</cell><cell>4.02 (75.36)</cell><cell>15.35 (48.03)</cell><cell>7.33 (90.33)</cell><cell>38.74 (100.00)</cell><cell>53.78 (99.56)</cell><cell cols="2">50.67 100.00 (100.00) (100.00)</cell><cell>40.95 (86.57)</cell><cell>47.02 (83.99)</cell></row><row><cell>(1.3B)</cell><cell>1024</cell><cell>11.18 (98.68)</cell><cell>11.76 (98.32)</cell><cell>13.33 (99.44)</cell><cell>4.70 (99.92)</cell><cell>19.69 (99.61)</cell><cell>8.00 (99.00)</cell><cell>38.74 (100.00)</cell><cell>52.44 (100.00)</cell><cell cols="2">50.67 100.00 (100.00) (100.00)</cell><cell>43.08 (99.92)</cell><cell>52.69 (98.98)</cell></row><row><cell>Student</cell><cell>128</cell><cell>21.05 (92.76)</cell><cell>20.17 (97.48)</cell><cell>34.44 (99.44)</cell><cell>7.20 (76.19)</cell><cell>16.93 (55.91)</cell><cell>12.67 (93.67)</cell><cell>60.36 (99.10)</cell><cell>64.00 (100.00)</cell><cell cols="2">52.00 (100.00) (100.00) 98.00</cell><cell>51.27 (85.26)</cell><cell>47.16 (84.28)</cell></row><row><cell>(6.7B)</cell><cell>1024</cell><cell cols="3">20.39 (98.68) (100.00) (100.00) 21.01 33.33</cell><cell>6.75 (99.92)</cell><cell>24.02 (100.00)</cell><cell>12.67 (99.00)</cell><cell>60.36 (100.00)</cell><cell>64.44 (100.00)</cell><cell cols="2">52.67 (100.00) (100.00) 98.67</cell><cell>56.76 (100.00)</cell><cell>55.02 (99.71)</cell></row><row><cell>Random</cell><cell></cell><cell>0.70</cell><cell>0.65</cell><cell>2.64</cell><cell>1.01</cell><cell>20.44</cell><cell>2.40</cell><cell>16.09</cell><cell>33.33</cell><cell>0.01</cell><cell>49.66</cell><cell>20.01</cell><cell>50.18</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This can be interpreted as variant of knowledge distillation<ref type="bibr" target="#b12">(Hinton et al., 2015)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Generating multiple reasoning paths has been considered in previous work to enhance inference(Wang et al., 2022a,b;   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Note that Zero-shot-CoT is itself a two-step prompting method. The reasoning (blue) is generated in the first step and answer prediction (red) is generated in the second step, each using the preceding text as the prompt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We use InstructGPT for the teacher model, for its superior zero-shot reasoning performance, compared to GPT-3 of the same size<ref type="bibr" target="#b17">(Kojima et al., 2022)</ref>. Fine-tuning for InstructGPT is unavailable.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="institution">Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs> (No. <rs type="grantNumber">2019-0-00075</rs>, <rs type="funder">Artificial Intelligence Graduate School Program (KAIST)</rs>) and <rs type="funder">Stochastic Analysis and Application Research Center (SAARC)</rs> under <rs type="funder">National Research Foundation of Korea</rs> grant <rs type="grantNumber">NRF-2019R1A5A1028324</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uknnbfP">
					<idno type="grant-number">2019-0-00075</idno>
				</org>
				<org type="funding" xml:id="_a25rAcy">
					<idno type="grant-number">NRF-2019R1A5A1028324</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Completion</head><p>Zaid spends 1/4 of his salary on rent, so that's 300$ per month. Zaid spends 1/3 of his salary on car fuel, so that's 150$ per month. That leaves him with 3000$ per month. He donates half of the remaining amount to his favorite charity, so that's 1500$ per month. That leaves him with 1500$ per month. He gives his daughter 200$ per month, so that's 400$ per month. He gives his wife 700$ per month, so that's 350$ per month. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Completion</head><p>First, Nuno Gomes would need to find a way to get to the Red Sea's deepest point. This might be difficult, since the Red Sea is a body of water that is located between two landmasses, namely the Arabian Peninsula and Africa. Second, once at the Red Sea's deepest point, Nuno Gomes would need to find a way to dive to the bottom of the sea. This might be difficult, since the Red Sea is a body of water that is located under land, and therefore the bottom of the sea is not as deep as the ocean floor. Third, once at Prediction None  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Completion</head><p>A landing is a place where something can touch the ground. A landing at the top of an ocean would be a very strange place to find, since there would be no ground to touch. A landing at the top of an apartment building would be a very strange place to find, since there would be no ground to touch. A landing at the top of a stairwell would be a very strange place to find, since there would be no ground to touch. A landing at the top of an airport would be a very strange place to find, since there would be no ground to Prediction A </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bucilua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<title level="m">Training verifiers to solve math word problems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.07051</idno>
		<title level="m">Language models show human-like content effects on reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intuition and reasoning: A dual-process perspective</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Inquiry</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="346" to="361" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06100</idno>
		<title level="m">Pretrained transformers improve out-of-distribution robustness</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<title level="m">Training compute-optimal large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><forename type="middle">Shane</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11610</idno>
		<title level="m">Large language models can self-improve</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<title level="m">Sequencelevel knowledge distillation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11916</idno>
		<title level="m">Large language models are zero-shot reasoners</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siena</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mawps: A math word problem repository</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06726</idno>
		<title level="m">Explanations from large language models make small reasoners better</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02336</idno>
		<title level="m">On the advance of making language models better reasoners</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04146</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zero-Shot Knowledge Transfer via Adversarial Belief Matching, chapter</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates Inc</publisher>
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-shot knowledge distillation in deep networks</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Kumar Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaisakh</forename><surname>Shaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Babu Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Johan Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00114</idno>
		<title level="m">Show your work: Scratchpads for intermediate computation with language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Arkil</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07191</idno>
		<title level="m">Are nlp models really able to solve simple math word problems? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Impact of pretraining term frequencies on few-shot reasoning</title>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07206</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01413</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatically identifying words that can serve as labels for few-shot text classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.488</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5569" to="5578" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2021b. It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Progressive network grafting for few-shot knowledge distillation</title>
		<author>
			<persName><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youtan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihui</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2541" to="2549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri?</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weibao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhou</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02137</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00937</idno>
		<title level="m">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00747</idno>
		<title level="m">Rationaleaugmented ensembles in language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<title level="m">Emergent abilities of large language models. Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Survey Certification</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Gpt3mix: Leveraging large-scale language models for text augmentation</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongju</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewook</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woomyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08826</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14465</idno>
		<title level="m">Star: Bootstrapping reasoning with reasoning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding knowledge distillation in nonautoregressive machine translation</title>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
