<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Brain and Cognition</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Brains, Minds and Machines</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kohitij</forename><surname>Kar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Brains, Minds and Machines</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rishi</forename><surname>Rajalingham</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ha</forename><surname>Hong</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Bay Labs Inc</orgName>
								<address>
									<postCode>94102</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Najib</forename><forename type="middle">J</forename><surname>Majaj</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Center for Neural Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elias</forename><forename type="middle">B</forename><surname>Issa</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Neuroscience</orgName>
								<orgName type="institution" key="instit1">Zuckerman Mind Brain Behavior Institute</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pouya</forename><surname>Bashivan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Prescott-Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kailyn</forename><surname>Schmidt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aran</forename><surname>Nayebi</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution" key="instit1">Neurosciences PhD Program</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Bear</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">L K</forename><surname>Yamins</surname></persName>
							<affiliation key="aff8">
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">McGovern Institute for Brain Research</orgName>
								<orgName type="institution" key="instit2">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Brain and Cognitive Sciences</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Brains, Minds and Machines</orgName>
								<orgName type="institution">MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB7953211735091C22F24D04371F6147</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S "IT" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Notorious for their superior performance in object recognition tasks, artificial neural networks (ANNs) have also witnessed a tremendous success in the neuroscience community as currently the best class of models of the neural mechanisms of visual processing. Surprisingly, after training deep feedforward ANNs to perform the standard ImageNet categorization task <ref type="bibr" target="#b5">[6]</ref>, intermediate layers in ANNs can partly account for how neurons in intermediate layers of the primate visual system will respond to any given image, even ones that the model has never seen before <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47]</ref>. Moreover, these networks also partly predict human and non-human primate object recognition performance and object similarity judgments <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21]</ref>. Having strong models of the brain opened up unexpected possibilities of noninvasive brain-machine interfaces where models are used to generate stimuli, optimized to elicit desired responses in primate visual system <ref type="bibr" target="#b1">[2]</ref>.</p><p>How can we push these models to capture brain processing even more stringently? Continued architectural optimization on ImageNet alone no longer seems like a viable option. Indeed, more recent and deeper ANNs have not been shown to further improve on measures of brain-likeness <ref type="bibr" target="#b32">[33]</ref>, even though their ImageNet performance has vastly increased <ref type="bibr" target="#b33">[34]</ref>. Moreover, while the initial limited number of layers could easily be assigned to the different areas of the ventral stream, the link between the handful of ventral stream areas and several hundred layers in ResNet <ref type="bibr" target="#b9">[10]</ref> or complex, branching structures in Inception and NASNet <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b26">27]</ref> is not obvious. Finally, high-performing models for object recognition remain feedforward, whereas recent studies established an important functional involvement of recurrent processes in object recognition <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>We propose that aligning ANNs to neuroanatomy might lead to more compact, interpretable and, most importantly, functionally brain-like ANNs. To test this, we here demonstrate that a neuroanatomically more aligned ANN, CORnet-S, exhibits an improved match to measurements from the ventral stream while maintaining high performance on ImageNet. CORnet-S commits to a shallow recurrent anatomical structure of the ventral visual stream, and thus achieves a much more compact architecture while retaining a strong ImageNet top-1 performance of 73.1% and setting the new state-of-the-art in predicting neural firing rates and image-by-image human behavior on Brain-Score, a novel large-scale benchmark composed of neural recordings and behavioral measurements. We identify that these results are primarily driven by recurrent connections, in line with our understanding of how the primate visual system processes visual information <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16]</ref>. In fact, comparing the high level ("IT") neural representations between recurrent steps in the model and time-varying primate IT recordings, we find that CORnet-S partly captures these neural response trajectories -the first model to do so on this neural benchmark.</p><p>2 CORnet-S: Brain-driven model architecture</p><p>We developed CORnet-S based on the following criteria (based on <ref type="bibr" target="#b19">[20]</ref>):</p><p>(1) Predictivity, so that it is a mechanistic model of the brain. We are not only interested in having correct model outputs (behaviors) but also internals that match the brain's anatomical and functional constraints. We prefer ANNs because neurons are the units of online information transmission and models without neurons cannot be obviously mapped to neural spiking data <ref type="bibr" target="#b46">[47]</ref>.</p><p>(2) Compactness, i.e. among models with similar scores, we prefer simpler models as they are potentially easier to understand and more efficient to experiment with. However, there are many ways to define this simplicity. Motivated by the observation that the feedforward path from retinal input to IT is fairly limited in length (e.g., <ref type="bibr" target="#b42">[43]</ref>), for the purposes of this study we use depth as a simple proxy to meeting the biological constraint in artificial neural networks. Here we defined depth as the number of convolutional and fully connected layers in the longest feedforward path of a model.</p><p>(3) Recurrence: while core object recognition was originally believed to be largely feedforward because of its fast time scale <ref type="bibr" target="#b6">[7]</ref>, it has long been suspected that recurrent connections must be relevant for some aspects of object perception <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b43">44]</ref>, and recent studies have shown their role even at short time scales <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b4">5]</ref>. Moreover, responses in the visual system have a temporal profile, so models at least should be able to produce responses over time too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CORnet-S model specifics</head><p>CORnet-S (Fig. <ref type="figure">1</ref>) aims to rival the best models on Brain-Score by transforming very deep feedforward architectures into a shallow recurrent model. Specifically, CORnet-S draws inspiration from ResNets that are some of the best models on our behavioral benchmark (Fig. <ref type="figure">1</ref>; <ref type="bibr" target="#b32">[33]</ref>) and can be thought of as unrolled recurrent networks <ref type="bibr" target="#b24">[25]</ref>. Recent studies further demonstrated that weight sharing in ResNets was indeed possible without a significant loss in CIFAR and ImageNet performance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Moreover, CORnet-S specifically commits to an anatomical mapping to brain areas. While for comparison models we establish this mapping by searching for the layer in the model that best explains responses in a given brain area, ideally such mapping would already be provided by the model, leaving no free parameters. Thus, CORnet-S has four computational areas, conceptualized as analogous to the ventral visual areas V1, V2, V4, and IT, and a linear category decoder that maps from the population of neurons in the model's last visual area to its behavioral choices. This simplistic assumption of clearly separate regions with repeated circuitry was a first step for us to aim at building as shallow a model as possible, and we are excited about exploring less constrained mappings (such as just treating everything as a neuron without the distinction into regions) and more diverse circuitry (that might in turn improve model scores) in the future.</p><p>Each visual area implements a particular neural circuitry with neurons performing simple canonical computations: convolution, addition, nonlinearity, response normalization or pooling over a receptive field. The circuitry is identical in each of its visual areas (except for V1 COR ), but we vary the total number of neurons in each area. Due to high computational demands, first area V1 COR performs a 7 × 7 convolution with stride 2, 3 × 3 max pooling with stride 2, and a 3 × 3 convolution. Areas V2 COR , V4 COR and IT COR perform two 1 × 1 convolutions, a bottleneck-style 3 × 3 convolution with stride 2, expanding the number of features fourfold, and a 1 × 1 convolution. To implement recurrence, outputs of an area are passed through that area several times. For instance, after V2 COR processed the input once, that result is passed into V2 COR again and treated as a new input (while the original input is discarded, see "gate" in Fig. <ref type="figure">1</ref>). V2 COR and IT COR are repeated twice, V4 COR is repeated four times as this results in the most minimal configuration that produced the best model as determined by our scores (see Fig. <ref type="figure">4</ref>). As in ResNet, each convolution (except the first 1 × 1) is followed by batch normalization <ref type="bibr" target="#b13">[14]</ref> and ReLU nonlinearity. Batch normalization was not shared over time as suggested by Jastrzebski et al. <ref type="bibr" target="#b14">[15]</ref>. There are no across-area bypass or across-area feedback connections in the current definition of CORnet-S and retinal and LGN processing are not explicitly modeled.</p><p>The decoder part of a model implements a simple linear classifier -a set of weighted linear sums with one sum for each object category. To reduce the amount of neural responses projecting to this classifier, we first average responses over the entire receptive field per feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Implementation Details</head><p>We used PyTorch 0.4.1 and trained the model using ImageNet 2012 <ref type="bibr" target="#b33">[34]</ref>. Images were preprocessed <ref type="bibr" target="#b0">(1)</ref> for training -random crop to 224 × 224 pixels and random flipping left and right; (2) for validation -central crop to 224 × 224 pixels; (3) for Brain-Score -resizing to 224 × 224 pixels. In all cases, this preprocessing was followed by normalization by mean subtraction and division by standard deviation of the dataset. We used a batch size of 256 images and trained on 2 GPUs (NVIDIA Titan X / GeForce 1080Ti) for 43 epochs. We use similar learning rate scheduling to ResNet with more variable learning rate updates (primarily in order to train faster): 0.1, divided by 10 every 20 epochs. For optimization, we use Stochastic Gradient Descent with momentum .9, a cross-entropy loss between image labels and model predictions (logits).</p><p>ImageNet-pretrained CORnet-S is available at github.com/dicarlolab/cornet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison to other models</head><p>Liang &amp; Hu <ref type="bibr" target="#b23">[24]</ref> introduced a deep recurrent neural network intended for object recognition by adding a variant of a simple recurrent cell to a shallow five-layer convolutional neural network backbone. Zamir et al. <ref type="bibr" target="#b48">[49]</ref> built a more powerful version by employing LSTM cells, and a similar approach was used by <ref type="bibr" target="#b37">[38]</ref> who showed that a simple version of a recurrent net can improve network performance on an MNIST-based task. Liao &amp; Poggio <ref type="bibr" target="#b24">[25]</ref> argued that ResNets can be thought of as recurrent neural networks unrolled over time with non-shared weights, and demonstrated the first working version of a folded ResNet, also explored by <ref type="bibr" target="#b14">[15]</ref>. However, all of these networks were only tested on CIFAR-100 at best. As noted by Nayebi et al. <ref type="bibr" target="#b28">[29]</ref>, while many networks may do well on a simpler task, they may differentiate once the task becomes sufficiently difficult. Moreover, our preliminary testing indicated that non-ImageNet-trained models do not appear to score high on Brain-Score, so even for practical purposes we needed models that could be trained on ImageNet. Leroux et al. <ref type="bibr" target="#b22">[23]</ref> proposed probably the first recurrent architecture that performed well on ImageNet. In an attempt to explore the recurrent net space in a more principled way, Nayebi et al. <ref type="bibr" target="#b28">[29]</ref> performed a large-scale search in the LSTM-based recurrent cell space by allowing the search to find the optimal combination of local and long-range recurrent connections. The best model demonstrated a strong ImageNet performance while being shallower than feedforward controls. In this work, we wanted to go one step further and build a maximally compact model that would nonetheless yield top Brain-Score and outperform other recurrent networks on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Brain-Score: Comparing models to brain</head><p>To obtain quantified scores for brain-likeness, we built Brain-Score, a composite benchmark that measures how well models can predict (a) mean neural response of each neural recording site to each and every tested naturalistic image in non-human primate visual areas V4 and IT (data from <ref type="bibr" target="#b27">[28]</ref>); (b) mean pooled human choices when reporting a target object to each tested naturalistic image (data from <ref type="bibr" target="#b32">[33]</ref>), and (c) when object category is resolved in non-human primate area IT (data from <ref type="bibr" target="#b15">[16]</ref>). To rank models on an overall score, we take the mean of the behavioral score, the V4 neural score, the IT neural score, and the neural dynamics score (explained below).</p><p>Brain-Score is open-sourced as a platform to score neural networks on brain data through the Brain-Score.org website for an overview of scores and through github.com/brain-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural predictivity</head><p>Neural predictivity is used to evaluate how well responses to given images in a source system (e.g., a deep ANN) predict the responses in a target system (e.g., a single neuron's response in visual area IT; <ref type="bibr" target="#b47">[48]</ref>). As inputs, this metric requires two assemblies of the form stimuli × neuroid where neuroids can either be neural recordings or model activations.</p><p>A total of 2,560 images containing a single object pasted randomly on a natural background were presented centrally to passively fixated monkeys for 100 ms and neural responses were obtained from 88 V4 sites and 168 IT sites. For our analyses, we used normalized time-averaged neural responses in the 70-170 ms window. For models, we reported the most predictive layer or (for CORnet-S) designated model areas and the best time point. Source neuroids were mapped to each target neuroid linearly using a PLS regression model with 25 components. The mapping procedure was performed for each neuron using 90% of image responses and tested on the remaining 10% in a 10-fold cross-validation strategy with stratification over objects. In each run, the weights were fit to map from source neuroids to a target neuroid using training images, and then using these weights predicted responses were obtained for the held-out images. To speed up this procedure, we first reduced input dimensionality to 1000 components using PCA. We used the neuroids from V4 and IT separately to compute these fits. The median over neurons of the Pearson's r between the predicted and actual response constituted the final neural fit score for each visual area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Behavioral predictivity</head><p>The purpose of behavioral benchmarks it to compute the similarity between source (e.g., an ANN model) and target (e.g., human or monkey) behavioral responses in any given task <ref type="bibr" target="#b32">[33]</ref>. For core object recognition tasks, primates (both human and monkey) exhibit behavioral patterns that differ from ground truth labels. Thus, our primary benchmark here is a behavioral response pattern metric, not an overall accuracy metric, and higher scores are obtained by ANNs that produce and predict the primate patterns of successes and failures. One consequence of this is that ANNs that achieve 100% accuracy will not achieve a perfect behavioral similarity score.</p><p>A total of 2,400 images containing a single object pasted randomly on a natural background were presented to 1,472 humans for 100 ms and they were asked to choose from two options which object they saw. For further analyses, we used participants response accuracies of 240 images that had around 60 responses per object-distractor pair (~300,000 unique responses). For evaluating models, we used model responses to 2,400 images from the layer just prior to 1,000-value category vectors. 2,160 of those images were used to build a 24-way logistic regression decoder, where each 24-value vector entry is the probability that a given object is in the image. This regression was then used to estimate probabilities for the 240 held-out images.</p><p>Next, both for human model responses, for each image, all normalized object-distractor pair probabilities were computed from the 24-way probability vector as follows: p(truth) p(truth)+p(choice) . These probabilities were converted into a d measure: d = Z(Hit Rate) -Z(False Alarms Rate), where Z is the estimated z-score of responses, Hit Rate is the accuracy of a given object-distractor pair, and the False Alarms Rate corresponds to how often the observers incorrectly reported seeing that target object in images where another object was presented. For instance, if a given image contained a dog and distractor was a bear, the Hit Rate for the dog-bear pair for that image came straight from the 240 × 24 matrix, while in order to obtain the False Alarms Rate, all cells from that matrix that did not have dogs in the image but had a dog as a distractor were averaged, and 1 minus that value was used as a False Alarm Rate. All d above 5 were clipped. This transformation helped to remove bias in responses and also to diminish ceiling effects (since many primate accuracies were close to 1), but empirically observed benefits of d in this dataset were small; see <ref type="bibr" target="#b32">[33]</ref> for a thorough explanation. The resulting response matrix was further refined by subtracting the mean d across trials of the same object-distractor pair (e.g., for dog-bear trials, their mean was subtracted from each trial). Such normalization exposes variance unique to each image and removes global trends that may be easier for models to capture. The behavioral predictivity score was computed as a Pearson's r correlation between the actual primate behavioral choices and model's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Object solution times</head><p>A total of 1318 grayscale images, containing images from Section 3.1 and MS COCO <ref type="bibr" target="#b25">[26]</ref>, were presented centrally to behaving monkeys for 100 ms and neural responses were obtained from 424 IT sites. Similar to <ref type="bibr" target="#b15">[16]</ref>, we fit a linear classifier on 90% of each 10 ms of model activations between 70-250 ms and used it to decode object category in each image from the non-overlapping 10% of the data. The linear classifier was based on a fully-connected layer followed by a softmax, with Xavier initialization for the weights <ref type="bibr" target="#b7">[8]</ref>, l2 regularized and decaying with 0.463, inputs were z-scored, and fit with a cross-entropy loss, a learning rate of 1e -4 over 40 epochs with a training batch size of 64, and stopped early if the loss-value went below 1e -4 . The predictions were converted to normalized d scores per image ("I1" in <ref type="bibr" target="#b32">[33]</ref>) and per time bin. By linearly interpolating between these bins, we determined the exact millisecond when the prediction surpassed a threshold value defined by the monkey's behavioral output for that image, which we refer to as "object solution times", or OSTs. Images for which either the model or the neural recordings did not produce an OST because the behavioral threshold was not met were ignored. We report a Spearman correlation between the model OSTs and the actual monkey OSTs (as computed in <ref type="bibr" target="#b15">[16]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generalization to new datasets</head><p>Neural: New neurons, old images We evaluated models on an independently collected neural dataset (288 neurons, 2 monkeys, 63 trials per image; <ref type="bibr" target="#b15">[16]</ref>) where new monkeys were presented with a subset of 640 images from the 2,560 images we used for neural predictivity.</p><p>Neural: New neurons, new images We obtained a neural dataset from <ref type="bibr" target="#b15">[16]</ref> for a selection of 1,600 of grayscale MS COCO images <ref type="bibr" target="#b25">[26]</ref>. These images are very dissimilar from the synthetic images we used in other tests, providing a strong means to test Brain-Score generalization. The dataset consisted of 288 neurons from 2 monkeys and 45 trials per image. Unlike our previous datasets, this one had a low internal consistency between neural responses, presumably due to the electrodes being near their end of life and producing unreasonably high amounts of noise. We therefore only used the 86 neurons with internal consistency of at least 0.9.</p><p>Behavioral: New images We collected a new behavioral dataset, consisting of 200 images (20 objects × 10 images) from Amazon Mechanical Turk users (185,106 trials in total). We used the same experimental paradigm as in our original behavioral test but none of the objects were from the same category as before.</p><p>CIFAR-100 Following the procedure described in <ref type="bibr" target="#b17">[18]</ref>, we tested how well these models generalize to CIFAR-100 dataset by only allowing a linear classifier to be retrained for the 100-way classification task (that is, without doing any fine-tuning). As in <ref type="bibr" target="#b17">[18]</ref>, we used a scikit-learn implementation of a multinomial logistic regression using L-BFGS <ref type="bibr" target="#b30">[31]</ref>, with the best C parameter found by searching a range from .0005 to .05 in 10 logarithmic steps (40,000 images from CIFAR-100 train set were used for training and the remaining 10,000 for testing; the search range was reduced from <ref type="bibr" target="#b17">[18]</ref> because in our earlier tests we found that all models had their optimal parameters in this range). Accuracies reported on the 10,000 test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CORnet-S is the best brain-predicting model so far</head><p>We performed a large-scale model comparison using most commonly used neural network families: AlexNet <ref type="bibr" target="#b18">[19]</ref>, VGG <ref type="bibr" target="#b36">[37]</ref>, ResNet <ref type="bibr" target="#b9">[10]</ref>, Inception <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>, SqueezeNet <ref type="bibr" target="#b12">[13]</ref>, DenseNet <ref type="bibr" target="#b11">[12]</ref>, MobileNet <ref type="bibr" target="#b10">[11]</ref>, and (P)NASNet <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b26">27]</ref>. These networks were taken from publicly available checkpoints: AlexNet, SqueezeNet, ResNet-{18,34} from PyTorch <ref type="bibr" target="#b29">[30]</ref>; Inception, ResNet-{50,101,152}, (P)NASNet, MobileNet from TensorFlow-Slim <ref type="bibr" target="#b35">[36]</ref>; and Xception, DenseNet, VGG from Keras <ref type="bibr" target="#b2">[3]</ref>. As such, the training procedure is different between models and our results should be related to those model instantiations and not to architecture families. To further map out the space of possible architectures, we included a family of models called BaseNets: lightweight AlexNet-like architectures with six convolutional layers and a single fully-connected layer, captured at various stages of training. Various hyperparameters were varied between BaseNets, such as kernel sizes, nonlinearities, learning rate etc.</p><p>Figure <ref type="figure">1</ref> shows how models perform on Brain-Score and ImageNet. CORnet-S outperforms other alternatives by a large margin with the Brain-Score of .471. Top ImageNet models also perform well, with leading models stemming from the DenseNet and ResNet families.Interestingly, models that rank the highest on ImageNet performance are also not the ones scoring high on brain data, suggesting a potential disconnect between ImageNet performance and fidelity to brain mechanisms. For instance, despite its superior performance of 82.9% top-1 accuracy on ImageNet, PNASNet only ranks 13 th on the overall Brain-Score. Models with an ImageNet top-1 performance below 70% show a strong correlation with Brain-Score of .90 but above 70% ImageNet performance there was no significant correlation (p .05, cf. Figure <ref type="figure">1</ref>).</p><p>.  We further asked if Brain-Score reflects idiosyncracies of the particular datasets that we included in this benchmark or instead, more desirably, provides an overall evaluation of how brain-like models are. To address this question, we performed four different tests with various generalization demands (Fig. <ref type="figure" target="#fig_0">2</ref>; CORnet-S was excluded). First, we compared the scores of models predicting IT neural responses to a set of new IT neural recordings <ref type="bibr" target="#b15">[16]</ref> where new monkeys were shown the same images as before. We observed a strong correlation between the two sets (Pearson r = .87). When compared on predicting IT responses to a very different image set (1600 MS COCO images <ref type="bibr" target="#b25">[26]</ref>), model rankings were still strongly correlated (Pearson r = .57). We also found a strong correlation between model scores on our original behavioral set and a newly obtained set of behavioral responses to images from 20 new categories that were not used before (200 images total; Pearson r = .85). Finally, we evaluated model feature generalization to CIFAR-100 without fine-tuning (following Kornblith et al. <ref type="bibr" target="#b17">[18]</ref>). Again, we observed a compelling correlation to Brain-Score values (Pearson r = .64).</p><p>Overall, we expect that adding more benchmarks to Brain-Score will further lead scores to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CORnet-S is the best on ImageNet and CIFAR-100 among shallow models</head><p>Due to anatomical constraints imposed by the brain, CORnet-S's architecture is much more compact than the majority of deep models in computer vision (Fig. <ref type="figure">3 middle</ref>). Compared to similar models with a depth less than 50, CORnet-S is shallower yet better than other models on ImageNet top-1 classification accuracy. AlexNet and IamNN are even shallower (depth of 8 and 14) but suffer on classification accuracy (57.7% and 69.6% top-1 respectively) -CORnet-S provides a good trade-off between the two with a depth of 15 and top-1 accuracy of 73.1%. Several epochs later in training top-1 accuracy actually climbed to 74.4% but since we are optimizing for the brain, we chose the epoch with maximum Brain-Score. CORnet-S also achieves the best transfer performance among similarly shallow models (Fig. <ref type="figure">3</ref>, right), indicating the robustness of this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CORnet-S mediates between compactness and high performance through recurrence</head><p>To determine which elements in the circuitry are critical to CORnet-S, we attempted to alter its block structure and record changes in Brain-Score (Fig. <ref type="figure">4</ref>). We only used V4, IT, and behavioral predictivity in this analysis in order to understand the non-temporal value of CORnet-S structure. We found that the most important factor was the presence of at least a few steps of recurrence in each block. Having a fairly wide bottleneck (at least 4x expansion) and a skip connection were other important factors.</p><p>On the other hand, adding more recurrence or having five areas in the model instead of four did not improve the model or hurt its Brain-Score. Other factors affected mostly ImageNet performance, including using two convolutions instead of three within a block, having more areas in the model and using batch normalization per time step instead of a global group normalization <ref type="bibr" target="#b44">[45]</ref>. The type of gating did not seem to matter. However, note that we kept training with identical hyperparameters for all these model variants. We therefore cannot rule out that the reported differences could be minimized if more optimal hyperparameters were found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CORnet-S captures neural dynamics in primate IT</head><p>Feed-forward networks cannot make any dynamic predictions over time, and thus cannot capture a critical property of the primate visual system <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42]</ref>. By introducing recurrence, CORnet-S is capable of producing temporally-varying response trajectories in the same set of neurons. Recent experimental results <ref type="bibr" target="#b15">[16]</ref> reveal that the linearly decodable solutions to object recognition are not all produced at the same time in the IT neural population -images that are particularly challenging for deep ANNs take longer to evolve in IT. This timing provides a strong test for the model: Does it predict image-by-image temporal trajectories in IT neural responses over time? We thus estimated for each image when explicit object category information becomes available in CORnet-S -termed "object solution time" (OST) -and compared it with the same measurements obtained from monkey IT cortex <ref type="bibr" target="#b15">[16]</ref>. Importantly, the model was never trained to predict monkey OSTs. Rather, a linear classifier was trained to decode object category from neural responses and from model's responses at each 10 ms window (Section 3.3). OST is defined as the time when this decoding accuracy reaches a threshold defined by monkey behavioral accuracy. We converted the two IT timesteps in CORnet-S to milliseconds by setting t 0 = 0-150 ms and t 1 = 150+ ms. We evaluated how well CORnet-S could capture the fine-grained temporal dynamics in primate IT cortex and report a correlation score of .25 (p &lt; 10 -6 ; Figure <ref type="figure">5</ref>). Feed-forward models cannot capture neural dynamics and thus scored 0.  <ref type="figure">4</ref>).</p><p>A critical component in establishing that models such as CORnet-S are strong candidate models for the brain is Brain-Score, a framework for quantitatively comparing any artificial neural network to the brain's neural network for visual processing. Even with the relatively few brain benchmarks that we have included so far, the framework already reveals interesting patterns. First, it extends prior work showing that performance correlates with brain similarity. However, adding recurrence allows us to break from this trend and achieve much better alignment to the brain. Even when the OST measure is not included in Brain-Score, CORnet-S remains one of the top models, indicating its general utility. On the other hand, we also find a potential disconnect between ImageNet performance and Brain-Score with PNASNet, a state-of-the-art model on ImageNet used in our comparisons, that is not performing well on brain measures, whereas even small networks with poor ImageNet performance achieve reasonable scores. We further observed that models that score high on Brain-Score also tend to score high on other datasets, supporting the idea that Brain-Score reflects how good a model is overall, not just on the four particular neural and behavioral benchmarks that we used.</p><p>However, it is possible that the observed lack of correlation is only specific to the way models were trained, as reported recently by Kornblith et al. <ref type="bibr" target="#b17">[18]</ref>. For instance, they found that the presence of auxiliary classifiers or label smoothing does not affect ImageNet performance too much but significantly decreases transfer performance, in particular affecting Inception and NASNet family of models, i.e., the ones that performed worse on Brain-Score than their ImageNet performance would imply. Kornblith et al. <ref type="bibr" target="#b17">[18]</ref> reported that retraining these models with optimal settings markedly improved transfer accuracy. Since Brain-Score is also a transfer learning task, we cannot rule out that Brain-Score might change if we retrained the affected models classes. Thus, we reserve our claims only about the specific pre-trained models rather than the whole architecture classes.</p><p>More broadly, we suggest that models of brain processing are a promising opportunity for collaboration between neuroscience and machine learning. These models ought to be compared through quantified scores on how brain-like they are, which we here evaluate with a composite of many neural and behavioral benchmarks in Brain-Score. With CORnet-S, we showed that neuroanatomical alignment to the brain in terms of compactness and recurrence can better capture brain processing by predicting neural firing rates, image-by-image behavior, and even neural dynamics, while simultaneously maintaining high ImageNet performance and outperforming similarly compact models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Brain-Score generalization across datasets: (a) to neural recordings in new subjects with the same stimulus set, (b) to neural recordings in new subjects with a very different stimulus set (MS COCO), (c) to behavioral responses in new subjects with new object categories, (d) to CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Depth versus (left) Brain-Score, (middle) ImageNet top-1 performance, and (right) CIFAR-100 transfer performance.Most simple models perform poorly on Brain-Score and Im-ageNet, and generalize less well to CIFAR-100, while the best models are very deep. CORnet-S offers the best of both worlds with the best Brain-Score, compelling ImageNet performance, the shallowest architecture we could achieve to date, and the best transfer performance to CIFAR-100 among shallow models. (Note: dots corresponding to MobileNets were slightly jittered along the x-axis to improve visibility.)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Simon Kornblith for helping to conduct transfer tests to CIFAR, and Maryann Rui and Harry Bleyan for the initial prototyping of the CORnet family. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 705498 (J.K.), US National Eye Institute (R01-EY014970, J.J.D.), Office of Naval Research (MURI-114407, J.J.D), the Simons Foundation (SCGB [325500, 542965], J.J.D; 543061, D.L.K.Y), the James S. McDonnell foundation (220020469, D.L.K.Y.) and the US National Science Foundation (iis-ri1703161, D.L.K.Y.). This work was also supported in part by the Semiconductor Research Corporation (SRC) and DARPA. The computational resources and services used in this work were provided in part by the VSC (Flemish Supercomputer Center), funded by the Research Foundation -Flanders (FWO) the Flemish Government -department EWI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Top-down facilitation of visual recognition</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avniel</forename><surname>Kassam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Singh Ghuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annette</forename><forename type="middle">M</forename><surname>Boshyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><forename type="middle">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><forename type="middle">S</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ksenija</forename><surname>Hämäläinen</surname></persName>
		</author>
		<author>
			<persName><surname>Marinkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Schacter</surname></persName>
		</author>
		<author>
			<persName><surname>Bruce R Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural population control via deep image synthesis</title>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Bashivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kohitij</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">6439</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep neural networks predict hierarchical spatio-temporal cortical dynamics of human visual object recognition</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Radoslaw M Cichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Pantazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.02970</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Oscillatory dynamics of perceptual to conceptual transformations in the ventral visual pathway</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">J</forename><surname>Devereux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><forename type="middle">K</forename><surname>Tyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1590" to="1605" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>James J Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><forename type="middle">C</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</title>
		<author>
			<persName><forename type="first">Umut</forename><surname>Güçlü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel Aj</forename><surname>Van Gerven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="10005" to="10014" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04773</idno>
		<title level="m">Residual connections encourage iterative inference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evidence that recurrent circuits are critical to the ventral stream&apos;s execution of core object recognition behavior</title>
		<author>
			<persName><forename type="first">Kohitij</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailyn</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><forename type="middle">B</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep supervised, but not unsupervised, models may explain it cortical representation</title>
		<author>
			<persName><forename type="first">Seyed-Mahdi</forename><surname>Khaligh-Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1003915</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08974</idno>
		<title level="m">Do Better ImageNet Models Transfer Better? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predict, then simplify</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="110" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep neural networks as a computational model for human shape sensitivity</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefania</forename><surname>Bracci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans P Op</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beeck</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1004896</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The distinct modes of vision offered by feedforward and recurrent processing</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><forename type="middle">R</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName><surname>Roelfsema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in neurosciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="571" to="579" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Iamnn: Iterative and adaptive mobile neural network for efficient image classification</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Simoens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Dhoedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10123</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bridging the gaps between residual learning, recurrent neural networks and visual cortex</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03640</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Architecture Search. arXiv preprint</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple learned weighted sums of inferior temporal neuronal firing rates accurately predict human core object recognition performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Najib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ha</forename><surname>Majaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="13402" to="13418" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Task-driven convolutional recurrent models of the visual system</title>
		<author>
			<persName><forename type="first">Aran</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kohitij</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Yamins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00053</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond core object recognition: Recurrent processes account for object recognition under occlusion</title>
		<author>
			<persName><forename type="first">Karim</forename><surname>Rajaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalda</forename><surname>Mohsenzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Ebrahimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mahdi</forename><surname>Khaligh-Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Rajalingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><forename type="middle">B</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Bashivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kohitij</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailyn</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="7255" to="7269" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Brain-inspired recurrent neural algorithms for advanced object recognition</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Technical University Munich ; LMU Munich, University of Augsburg</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tensorflow-slim image classification model library</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/models/tree/master/research/slim" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks: a better model of biological object recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Courtney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Spoerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1551</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-09">sep 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recurrent computations for visual pattern completion</title>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schrimpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Moerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Josue Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Caro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hardesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><surname>Kreiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="8835" to="8840" />
			<date type="published" when="2018">2018</date>
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neuronal processing: How fast is the speed of thought?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Tovée</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1125" to="1127" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A century of gestalt psychology in visual perception: I. perceptual grouping and figure-ground organization</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Wagemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kubovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rüdiger Von Der</forename><surname>Heydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1172</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08494</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Group normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical modular optimization of convolutional networks achieves representations similar to macaque it and human ventral stream</title>
		<author>
			<persName><forename type="first">Ha</forename><surname>Daniel L Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3093" to="3101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using goal-driven deep learning models to understand sensory cortex</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">356</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName><forename type="first">Ha</forename><surname>Daniel Lk Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darren</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feedback networks</title>
		<author>
			<persName><forename type="first">Te-Lin</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<title level="m">Learning Transferable Architectures for Scalable Image Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
