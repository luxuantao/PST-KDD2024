<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Direct Local Pattern Sampling by Efficient Two-Step Random Procedures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mario</forename><surname>Boley</surname></persName>
							<email>mario.boley@iais.fhg.de</email>
						</author>
						<author>
							<persName><forename type="first">Claudio</forename><surname>Lucchese</surname></persName>
							<email>claudio.lucchese@isti.cnr.it</email>
						</author>
						<author>
							<persName><forename type="first">-C</forename><forename type="middle">N R</forename><surname>Pisa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Paurat</surname></persName>
							<email>daniel.paurat@uni-bonn.de</email>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
							<email>thomas.gaertner@iais.fhg.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Fraunhofer IAIS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Direct Local Pattern Sampling by Efficient Two-Step Random Procedures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F5813B45766C7E886510B0F23B831872</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database Applications-Data Mining Algorithms</term>
					<term>Theory Local pattern discovery</term>
					<term>Sampling</term>
					<term>Patternbased classification</term>
					<term>Frequent sets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present several exact and highly scalable local pattern sampling algorithms. They can be used as an alternative to exhaustive local pattern discovery methods (e.g, frequent set mining or optimistic-estimator-based subgroup discovery) and can substantially improve efficiency as well as controllability of pattern discovery processes. While previous sampling approaches mainly rely on the Markov chain Monte Carlo method, our procedures are direct, i.e., non processsimulating, sampling algorithms. The advantages of these direct methods are an almost optimal time complexity per pattern as well as an exactly controlled distribution of the produced patterns. Namely, the proposed algorithms can sample (item-)sets according to frequency, area, squared frequency, and a class discriminativity measure. Experiments demonstrate that these procedures can improve the accuracy of pattern-based models similar to frequent sets and often also lead to substantial gains in terms of scalability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This paper presents simple yet effective procedures for local pattern discovery <ref type="bibr" target="#b20">[20]</ref> that attack the task from a different algorithmic angle than the standard search approachnamely, by directly generating individual patterns as the outcome of a random experiment. Local patterns such as association rules <ref type="bibr" target="#b1">[1]</ref> or emerging patterns <ref type="bibr" target="#b12">[12]</ref> are used in various application contexts from exploratory data analysis where they constitute units of discovered knowledge to predictive model construction where patterns act as binary features <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b13">13]</ref>. What all applications have in common is that usually only a few patterns can be effectively utilized-either due to the limited attention of a data analyst or because too many features can reduce the comprehensibility and performance of a global model. Standard local pattern discovery algorithms, however, are based on exhaustive search within huge pattern spaces (e.g., frequent set miners <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b27">26]</ref>, or optimistic-estimator-based subgroup and association discovery <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b24">23]</ref>). Consequently, they tend to either produce a vast amount of output patterns or at least enumerate them internally.</p><p>This motivates the invention of algorithms that only sample a representative set of patterns without explicitly searching in the pattern space. Such algorithms exist in the literature <ref type="bibr" target="#b2">[2,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b8">8]</ref> but they provide either no control over the distribution of their output or only asymptotic control by simulating a stochastic process on the pattern space using the Markov chain Monte Carlo method (MCMC). In addition to only offering approximate sampling, MCMC methods have a scalability problem: the number of required process simulation steps is often large and, even more critical, individual simulation steps typically involve expensive support counting operations. Hence, these algorithms are often infeasible for large input datasets. Therefore, we present novel pattern generation methods that sample patterns exactly and directly, i.e., without simulating time-consuming stochastic processes. More precisely, given a dataset D and a number of desired patterns k, the procedures • produce exactly k patterns each of which is generated exactly according to a distribution proportional to either frequency, squared frequency, area (i.e., frequency times size), or discriminativity (i.e., frequency in positive data portion times negative frequency in negative data portion);</p><p>• use time O( D + kn) for frequency and area or time O( D<ref type="foot" target="#foot_1">2</ref> + kn) for squared frequency and discriminativity, where n denotes the number of items and D the size of the dataset, i.e., the sum of all data record sizes<ref type="foot" target="#foot_0">1</ref> .</p><p>That is, after a linear or quadratic preprocessing phase each pattern is produced in a time linear in the number of items.</p><p>This time complexity appears to be almost optimal, because only reading the data once requires O( D ) and just printing k patterns without any further computation requires time O(kn).</p><p>After giving some more background on the idea of controlled repeated pattern sampling and reviewing other pattern sampling algorithms, the remainder of this paper is structured as follows. We define formal and notational background (Sec. 2) followed by a detailed description of the sampling procedures (Sec 3). Then we report experimental results showing that sampled patterns are equally useful for pattern-based classification as frequent sets and that pattern sampling can easily outperform exhaustive listing on large datasets (Sec. 4). Finally, we give a summarizing discussion of all results (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Pattern Sampling</head><p>The data mining literature contains several local pattern discovery algorithms that can efficiently produce large output families. Here, efficiency is defined in an output-sensitive way (see, e.g., <ref type="bibr" target="#b16">[16]</ref>), that is for instance amortized polynomial time per pattern, which is a useful notion assuming that the produced pattern collections are the final output. When viewed from a global (application-driven) perspective though, the enumerated patterns are usually only an intermediate result, from which a final-often much smallerpattern collection is selected. That is, enumeration is only one step within a surrounding local pattern discovery process. This two phase approach, which we refer to as "exhaustive search" is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(a): during the enumeration step part of the implicitly defined pattern space is physically constructed-we refer to that part as "enumeration space"-and then, during the selection step, the most valuable patterns from this enumeration space are collected with respect to some interestingness measure.</p><p>An example for this paradigm is listing frequent sets of an input dataset, but subsequently using only those sets that provide rules with a large value of some measure that reflects the primary user interest (e.g., lift). A further example is optimistic-estimator-based pruning for subgroup or association discovery. There the enumeration space is the family of all sets having a large enough optimistic estimate of their interestingness, and the patterns that are selected for the result family are those with a large value of the actual interestingness measure. Note that, in this example, enumeration and selection are algorithmically interweaved, i.e., sets are already selected during the enumeration phase. Many more examples emerge from the LeGo approach to data mining <ref type="bibr" target="#b22">[22]</ref> where patterns are selected according to their utility for constructing global models.</p><p>Two reasons usually motivate this two phase approach: either there is no efficient algorithm that directly optimizes the measure of primary interest or this measure is not known in advance (this happens in exploratory settings where interestingness can be user-specific). In either case, the enumeration step can constitute a severe bottleneck. Even if enumeration is performed by an amortized polynomial time algorithm, its computation time is essentially unpredictable: the size of the enumeration space cannot be directly controlled and its explicit construction takes time at least proportional to that size. On the other hand, if one enforces a maximum computation time by aborting the execution at a certain point, one ends up with an uncontrolled subset of the enumeration space, which depends on the internal search order of the enumeration algorithm.</p><p>In contrast, suppose we can access the pattern space L by an efficient sampling procedure simulating a distribution π : L → [0, 1] that is defined proportional to some function that either is our primary interestingness measure or a function that correlates with it. Then, for any designated number of patterns (or corresponding designated computation time) it is possible to efficiently generate a collection of exactly that many patterns that is representative of the distribution π. Consequently, since π has a semantic connection to the underlying notion of interestingness, a meaningful allocation of computational resources is guaranteed for any limited time budget.</p><p>Figure <ref type="figure" target="#fig_0">1</ref>(b) illustrates this alternative approach, which we want to refer to as "controlled repeated pattern sampling". A potentially positive side-effect of this paradigm is that instead of the usual hard constraints it utilizes parameter-free soft constraints <ref type="bibr" target="#b5">[5]</ref>. Hence, the user is freed of the often troublesome task of finding appropriate hard threshold parameters such as a minimum frequency threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>In contrast to sampling from the input database (see, e.g., <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b29">28]</ref>), it is a relatively new development in local pattern discovery to sample from the pattern space. In the context of maximal frequent subgraph mining, Chaoji et al. <ref type="bibr" target="#b8">[8]</ref> describes a random process that stops after a certain number of steps that is bounded by the maximum number of edges present in an input graph and produces a maximal frequent subgraph. A similar process is already applied in Gunopoulos et al. <ref type="bibr" target="#b18">[18]</ref> within a Las Vegas variant of the Dualize and Advance algorithm. More precisely, it is used for the internal randomization of an algorithm with an otherwise deterministic output (all maximal frequent and minimal infrequent sets of a given input database). When applied for the final pattern discovery, however, this random process has the weakness that it provides no control over the generation probabilities of individual patterns.</p><p>Several papers propose to overcome this weakness by applying the MCMC method. Boley and Grosskreutz <ref type="bibr" target="#b7">[7]</ref> proposes frequent set sampling to approximate the effect of specific minimum frequency thresholds. The proposed algorithm simulates a simple Glauber dynamic on the frequent set lattice: starting with the empty set, in each subsequent time step a single item is either removed or added to the current set. A similar MCMC method is used in Zaki and Al Hasan <ref type="bibr" target="#b2">[2]</ref> for generating a representative set of graph patterns. These MCMC methods provide limited control of the generation probabilities, namely of the infinite limit of the state distribution. The worst-case convergence can, however, be exponentially slow in the size of the input database. For sampling from the family of frequent patterns, this problem appears to be inherent: almost uniform frequent pattern sampling can be used for approximate frequent pattern counting, which one can show to be intractable under reasonable complexity assumptions (see <ref type="bibr" target="#b7">[7]</ref>). Similar conclusions can be drawn for enumeration spaces defined by linearly scaled versions of the frequency measure such as the standard optimistic estimator for the binomial test quality function in subgroup discovery <ref type="bibr" target="#b28">[27]</ref>.</p><p>In order to avoid this implication of hard-constraint-based pattern discovery (e.g., using a hard frequency threshold), Boley et al. <ref type="bibr">[6]</ref> combines pattern space sampling with softconstraint-based pattern discovery <ref type="bibr" target="#b5">[5]</ref>-resulting in the pattern sampling paradigm described in Section 1.1 above. Still, the underlying method is again MCMC-based, and, despite using a more sophisticated chain defined on the closed set lattice of the input database, it shares the practical weaknesses of this technique. The present paper retains the idea of controlled pattern sampling without hard constraints, but proposes novel pattern generation methods that are exact and direct, i.e., they do not involve MCMC process simulation. Consequently, the resulting pattern discovery processes are efficient not only theoretically but also on a wide range of real-world benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>Before going into technical details, we introduce some basic notions and notation. For a finite set X we denote by P(X) its power set and by u(X) the uniform probability distribution on X. Moreover, for positive weights w : X → R + let w (X) denote the distribution on X arising from normalizing w, i.e., the distribution described by x → w(x)/ x ∈X w(x)-assuming that there is an x ∈ X with w(x) &gt; 0.</p><p>A binary dataset D over some finite ground set E is a bag (multiset) of sets, called data records, D1, . . . , Dm each of which is a subset of E = {e, . . . , en}. The size of D, denoted by D , is defined as the sum of all its data record sizes D∈D = |D|. Inspired by the application of market basket analysis the elements of E are often referred to as "items". More generally, one can think of E as a set of binary features describing the data records. In particular, a categorical data table can easily be represented as a binary dataset by choosing the ground set as consisting of all attribute/value equality expressions that can be formed from the table. More precisely, a categorical data table T consisting of m data row vectors d1, . . . , dm with di = (di(1), . . . , di(n)) can be represented by the dataset DT = {D1, . . . , Dm} with Di = {(j, v) : di(j) = v} over ground set</p><formula xml:id="formula_0">ET = {(j, di(j)) : 1 ≤ i ≤ m, 1 ≤ j ≤ n} .</formula><p>For a given dataset D over E, the pattern space (or pattern language) L(D) considered in this paper is the power set P(E) of the features and its elements are interpreted conjunctively. That is, the local data portion described by a set F ⊆ E, called the support (set) of F in D and denoted D[F ], is defined as the multiset of all data records from D that contain all elements of F , i.e.,</p><formula xml:id="formula_1">D[F ] = {D ∈ D : D ⊇ F }.</formula><p>An interestingness measure for a pattern language L(•) is a function</p><formula xml:id="formula_2">q : {(D, x) : D a binary dataset, x ∈ L(D)} → R .</formula><p>However, often there is a fixed dataset that is clear from the context. In such cases-and if we want to simplify the notation-we just write q as a unary function q(•) = q(D, •) and omit the first argument. The most basic measures for set patterns are the support (count), i.e., the size of its support set qsupp(D, F ) = |D[F ]| and the frequency, i.e., the relative size of its support with respect to the total number of data records q freq (D, All measures defined so far are unsupervised measures in the sense that they rely on no further information but the dataset itself. In contrast, there are so-called supervised descriptive rule induction techniques that rely on additional information in the form of class labels l(D) ∈ C = {c1, . . . , c k } associated to each data record D ∈ D. For c ∈ C we denote by Dc the data portion labeled c, i.e., Dc = {D ∈ D : l(D) = c}. Examples for this setting are emerging pattern mining <ref type="bibr" target="#b12">[12]</ref> and contrast set mining <ref type="bibr" target="#b3">[3]</ref>, where one is interested in patterns having a high support difference between the positive and the negative portion of the data records, or subgroup discovery <ref type="bibr" target="#b28">[27]</ref>, where one searches for patterns with a high distributional unusualness of these labels on their support set. An important special case is the case of binary labels, i.e., C = {⊕, }. For this case we consider the following discriminativity measure</p><formula xml:id="formula_3">F ) = |D[F ]| / |D|. For a frequency threshold t ∈ [0, 1] a set is called t-frequent (w.r.t. D) if q freq (D, F ) ≥ t. A</formula><formula xml:id="formula_4">q disc (F ) = |D⊕[F ]| |D \ D [F ]| .</formula><p>A further measure for the discriminative power of a pattern is the Fisher score q fish , which is defined for datasets with arbitrary labels C. Intuitively, it measures the relation of the inter-class variance of a feature to its intra-class variances, i.e.,</p><formula xml:id="formula_5">q fish (F ) = c∈C |Dc| (q freq (Dc, F ) -q freq (D, F )) 2 c∈C D∈Dc (δ(D ⊇ F ) -q freq (Dc, F )) 2</formula><p>where δ(D ⊇ F ) is 1 if D ⊇ F and 0 otherwise. This paper does not present a sampling algorithm for this measure. However, the Fisher score is used for post-processing generated patterns in the context of constructing global classification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SAMPLING ALGORITHMS</head><p>After the introduction of set patterns and interestingness measures, we can now present our sampling procedures. A naive approach for sampling a pattern according to a distribution π is to generate a list F1, . . . , FN of all patterns with π(F ) &gt; 0, draw an x ∈ [0, 1] uniformly at random, and then return the unique set</p><formula xml:id="formula_6">F k with k-1 i=1 π(Fi) ≤ x &lt; k i=1 π(Fi).</formula><p>However, the exhaustive enumeration of any non-constant part of the pattern space is precisely what we want to avoid. That is, we are interested in non-enumerative sampling algorithms.</p><p>Below we give such algorithms for four quality functions: frequency and squared frequency as well as area and discriminativity. The algorithms are inspired by the elementary procedures used in Karp et al. <ref type="bibr" target="#b21">[21]</ref> (for estimating the number of satisfying assignments of a DNF formula): in a first step one element of a suitably constructed set of base objects is drawn, and in a second step a sub-object is drawn that is induced by that base object; hence the term "two-step random procedures".</p><p>Note that, in contrast to the frequency measures, for the latter two quality functions it is NP-hard to find optimal patterns: Finding a set of maximum area for a given input dataset is equivalent to the NP-hard problem of computing a biclique with maximum number of edges from a given bipartite graph (see <ref type="bibr" target="#b15">[15]</ref>). The same hardness result holds for the discriminativity measure because optimizing area can be linearly reduced to optimizing discriminativity: by setting D⊕ to D and D to {E \ {e} : e ∈ E} we get  We start with sampling according to frequency and area, both of which can be achieved by very similar linear time algorithms. The key insight for frequency-based sampling, i.e., π = q freq (P(E)), is that random experiments are good in reproducing frequent events. Namely, if we look at a pattern that is supported by a random data record we are likely to observe a pattern that is supported by many data records altogether. This intuition leads to a two-step nonenumerative sampling routine (see Algorithm 1), which is as fast as it is simple: First select a data record of the input dataset randomly with a probability proportional to the size of its power set. Then return a uniformly sampled subset of that data record. Using the size of the power set in the first step is important, as otherwise the sampling routine would be biased towards sets occurring in small data records. As noted in Proposition 1 below, the random set resulting from combining both steps follows the desired distribution.</p><formula xml:id="formula_7">q disc (D⊕ ∪ D , F ) = |D[F ]| (|E| -|E| + |F |) = qarea(D,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frequency and Area</head><p>Regarding the computational complexity of the sampling algorithm we can observe that it is indeed efficient: if one has knowledge of the numbers |D| for all data records D ∈ D and, moreover, has index access to all data records, a single random set can be produced in time O(log |D| + |E|) (the two terms correspond to producing a random number for drawing a data record in step 1 and to drawing one of its subsets in step 2, respectively). Both requirements can be achieved via a single initial pass over the dataset. Thus, we have the following proposition.</p><p>Proposition 1. On input dataset D over E, a family of k realizations of the random set R ∼ q freq (P(E)) can be generated in time O( D + k(|E| + ln |D|)).</p><p>Proof. Let Z be the normalizing constant F ⊆E |D[F ]| and D denote the random data record that is drawn in step 2 of the algorithm. For the probability distribution of the returned random set we have</p><formula xml:id="formula_8">P[R = R] = D∈D P[R = R ∧ D = D] = D∈D[R] 1 2 |D| 2 |D| Z = |D[R]| Z = qsupp(D, R) Z with a normalizing Z = D∈D 2 |D| (which is equal to the desired F ⊆E |D[F ]|).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Area-based Sampling</head><p>Require: dataset D over ground set E with D &gt; 0, Returns: random set R ∼ qarea(P(E))</p><p>1. let weights w be defined for all D ∈ D by</p><formula xml:id="formula_9">w(D) = |D| 2 |D|-1 2. draw D ∼ w (D) 3. return R ∼ s(P(D)) with s(F ) = |F |</formula><p>Sampling according to area, i.e., π = qarea(P(E)), can be achieved via a slight modification of frequency-based sampling: in step two, instead of drawing a subset uniformly from a data record, draw a subset with probability proportional to its size. The latter step can be implemented, e.g., by first drawing a size s with probability proportional to |D| s and then by uniformly drawing from D a subset of size s. As a side effect, this modification affects the normalization constants and in particular the data record weights of step one. Since for the sum of all subset sizes of a data record D we have</p><formula xml:id="formula_10">F ⊆D |F | = |D| 2 |D|-1 ,</formula><p>the data record weights need to be modified accordingly. The resulting pseudo-code is given in Algorithm 2. Again, after all weights have been computed via an initial pass over the data, an arbitrary number of random sets can be produced in time O(log |D| + |E|). Hence, with a similar proof as for Proposition 1 we can conclude: It is important to note that area can be replaced by weighted area relatively easy without changing the asymptotic complexitywhere weighted area is defined as</p><formula xml:id="formula_11">qware(F ) = e∈F w(e)   D∈D[F ] w(D)  </formula><p>for a set of positive weights w : (E ∪ D) → R+. The same holds for weighted frequency. In this paper, however, for the sake of simplicity we only consider the unweighted case. such that there is an F ⊆ E with q disc (F ) &gt; 0 Returns: random set R ∼ q disc (P(E))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discriminativity and Squared Frequency</head><p>1. let weights w be defined by</p><formula xml:id="formula_12">w(D⊕, D ) = (2 |D⊕\D | -1)2 |D⊕∩D | for all (D⊕, D ) ∈ D⊕ × D 2. draw (D⊕, D ) ∼ w (D⊕ × D ) 3. return R = (F ∪ F ) with F ∼ u(P(D⊕ \ D ) \ ∅) and F ∼ u(P(D⊕ ∩ D ))</formula><p>In order to design a sampling procedure for discriminativity, i.e., π = q disc (P(E)), we can lift the principle of frequency-based sampling to a random experiment that is a little more complicated and has the following intuition: if we look at a pattern that is supported by a random positive data record and not supported by a random negative data record, we are likely to observe a pattern that is altogether supported by many positive data records and only few negative data records, i.e., we are likely to observe a pattern with a relatively high discriminativity score. Again, in order to control the resulting distribution, it is necessary to consider a pair of data records (D⊕, D ) with a probability equal to the number of sets F ⊆ E with F ⊆ D⊕ and F ⊆ D . This implies that the increased expressivity of discriminativity compared to frequency comes at a price: due to the necessity of weight computation for all pairs of positive and negative data records, we end up with a quadratic preprocessing phase. Algorithm 3 contains all the details of the resulting sampling procedure and leads to the following result.</p><p>Proposition 3. Let D be a binary labeled input dataset over ground set E such that there is a set F ⊆ E with q disc (D, F ) &gt; 0. A family of k realizations of the random set R ∼ q disc (P(E)) can be generated in time O( D<ref type="foot" target="#foot_2">2</ref> + k(|E| + ln 2 |D|)).</p><p>Proof. Let R denote the random set returned in step 3 of the algorithm and D⊕, D the data records drawn in step 2. Moreover, for D ∈ D⊕ and D ∈ D let δ(D, D ) denote the family of all sets F ⊆ E that are supported by D but not supported by D . We can rewrite this definition as</p><formula xml:id="formula_13">δ(D, D ) ={F ⊆ E : F ⊆ D, F ⊆ D } ={F ∪ F : ∅ ⊂ F ⊆ (D \ D ), F ⊆ (D ∩ D )} .</formula><p>This form shows that the weights w(•, •) assigned in step 1 are equivalent to |δ(•, •)| and, moreover, that R is a set drawn uniformly from δ(D⊕, D ). With this we can conclude similar to the previous algorithms that</p><formula xml:id="formula_14">P[R = F ] = D∈D ⊕ D ∈D P[R = F, D⊕ = D, D = D ] = D,D ∈δ -1 [F ] 1 |δ(D, D )| w(D, D ) Z = 1 Z {(D, D ) ∈ D⊕ × D : D ⊇ F, D ⊇ F } = 1 Z |D⊕[F ]| (|D | -D [F ]) with Z = D,D ∈D ⊕ ×D |δ(D, D )| = F ⊆E q disc (F ) as re- quired.</formula><p>Algorithm 4 Squared-frequency-based Sampling Require: dataset D over ground set E, Returns: random set F ∼ q 2 freq (P(E)) = q 2 supp (P(E))</p><p>1. let weights w be defined by</p><formula xml:id="formula_15">w(D1, D2) = 2 |D 1 ∩D 2 | for all (D1, D2) ∈ D × D 2. draw (D1, D2) ∼ w (D × D) 3. return F ∼ u(P(D1 ∩ D2))</formula><p>It is straightforward to see that the approach of drawing two data records can also be used to implement other potentially interesting distributions that can be expressed as the product of two support counts. A basic example is squared frequency. In order to achieve this distribution, one can consider a uniformly 2 drawn subset of two random data records, i.e., a subset of their intersection. The resulting pseudo-code with appropriate pairwise weights is given in Algorithm 4. Closely following the proof of Proposition 3 this algorithm can be used to show another proposition. Proposition 4. On input dataset D over E, a family of k realizations of the random set R ∼ q 2 freq (P(E)) can be generated in time O( D 2 + k(|E| + ln 2 |D|)).</p><p>In principle, one can design sampling algorithms for an abitrary power c of the frequency measure by drawing a subset from c random data records. However, the resulting time complexity for computing the weights for each c-tuple of data records gets out of hand quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>The sampling procedures presented in the previous section are provably efficient and correct, i.e., their randomized output follows the specified distributions. Beside their practical scalability, it remains to be evaluated how useful these distributions are in the context of local pattern discovery. It is inherently difficult to evaluate pattern discovery methods for exploratory data analysis. There one aims to find interesting patterns based on notions of interestingness that are often user-subjective. Hence, a sophisticated experimental design would be required. Here we resort to the other branch of local pattern discovery applications, i.e., pattern-based global model construction, which allows us to simply use Fisher score as primary interestingness measure and accuracy as objective evaluation metric for the overall process. In our experiments we use a variety of databases from the UCI machine learning repository <ref type="bibr" target="#b14">[14]</ref> listed in Table <ref type="table" target="#tab_0">1</ref>. In order to apply the pattern discovery algorithms, binary datasets are created from these databases by first converting them into categorical datatables using five bucket frequency discretization of all numeric data columns, and then by considering the corresponding binary datasets (using attribute / value pairs of categorical attributes as items; see Section 2).</p><p>Implementations of the sampling algorithms are available in the software section of http://www-kd.iai.uni-bonn.de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predictive Performance</head><p>We start with experiments evaluating the sampling algorithms in the context of pattern-based classification. Here one aims to improve classification accuracy by enriching given labeled training data with pattern-based features. We closely follow the framework of Cheng et al. <ref type="bibr" target="#b9">[9]</ref>. In a nutshell it consists of three basic steps: extraction of a collection of patterns (which are subsequently considered as features of the data records supporting them), feature selection based on Fisher score and pattern redundancy, and classification, for which we use a linear support vector machine.</p><p>In more detail, for an input data table T with corresponding binary dataset DT = D and class labels C = {1, . . . , c} we consider pattern collections C, R frq , R sfr , R dcr that are based on deterministic top-k frequent closed sets (these can be efficiently produced, e.g., by the algorithm of Pietracaprina and Vandin <ref type="bibr" target="#b25">[24]</ref>) and sampling according to frequency, squared frequency, and discriminativity, respectively. Area-based sampling is not considered here, because it is not designed to provide good features for classification. Each pattern collection P is the union of c collections Pi based on the class labels i ∈ C where the size of Pi is proportional to the number of training examples of that class, i.e., |Pi| = α |Di|. Consequently, the size of the complete collection is α |D|, which is linear in the input size. The parameter α is set to 32 independently of the method and the dataset. This setting is a compromise between efficiency and stability of the pattern selection process (described below) as captured by the average Fisher score of finally selected features. See Figure <ref type="figure" target="#fig_4">2</ref> for an illustration on three exemplary datasets. The feature selection step for a pattern collection P is then performed as follows: initialize P0 = ∅ and consider all sets F1, . . . , F l ∈ P having q freq (D, •) ≥ 0.05 in descending order of their Fisher score q fish (D, •). Select a pattern Fi if the number of previously uncovered data records that it covers is at least 1% of the dataset, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D[Fi] \</head><formula xml:id="formula_16">F ∈P i-1 D[F ] ≥ 0.01 |D|</formula><p>where Pi-1 is the family of sets already selected when considering Fi. The selection process is stopped either after all patterns have been considered or if Pi covers the dataset completely, i.e., F ∈P i D[F ] = D. This is a very simple instantiation of the framework of Cheng et al. with the condition of 1% coverage improvement acting as binary redundancy measure. A further deviation is that in our case the remaining patterns are not reordered after each selection step. While the original procedure constructs potentially more powerful feature collections, this simplified version is much faster: the time complexity is only linear instead of quadratic in the number of input features. For the final pattern collection P the original data table T is then augmented by binary attributes corresponding to the elements of F1, . . . , F k ∈ P . That is, the augmented table T has n + |P | columns with rows defined by</p><formula xml:id="formula_17">t i (j) =      ti(j), if j ≤ n 1, if j &gt; n and Di ⊇ Fj-n 0, otherwise</formula><p>where Di is the data record of DT corresponding the the i-th row of T .  The linear SVM of the LIBSVM software is used as classifier-wrapped in an optimization layer for its regularization parameter c. That is, the training set is first used to determine the optimal regularization parameter c ∈ {2 i : i = -5, -3, . . . , 14} using 5-fold cross-validation and then a model is trained with the optimal parameter using the whole training set. The complete workflow is validated using 10-fold cross-validation for all pattern collections simultaneously.</p><p>Table <ref type="table" target="#tab_2">2</ref> contains the results. As one can observe, for some of the datasets pattern-based classification provides a substantial accuracy boost compared to the plain SVM baseline ("tic-tac-toe", "primary-tumor", "glass"). For others there is still an improvement but to a lesser extend, or the results are roughly identical. Most importantly, the sampling methods perform stronger than exhaustive top-k mining on a majority of datasets. A Wilcoxon signed ranks test (see <ref type="bibr" target="#b11">[11]</ref>) for our N = 19 databases reveals that pattern-based classification with each of the random set collections outperforms the plain SVM significantly at the 5%-level (t-values of 42, 28.5, and 45.5 respectively; critical value 46). Moreover, although all random set collections are lying ahead of the frequent set collection on our test databases, it is not significantly outperformed by any of them. We can conclude that patternbased classification based on all tested sampling algorithms is likely to outperform the plain SVM, and is unlikely to be inferior to standard frequent-pattern-based classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scalability and Effectivity</head><p>Having evaluated the quality of the sampled patterns we now turn to scalability and effectivity studies. <ref type="foot" target="#foot_3">3</ref> The theoretical potential of the direct sampling procedures is already indicated by the guarantees of Propositions 1-4. In particular for frequency and area-based sampling they suggest applicability on larger to large-scale datasets. Below we investigate to what degree this potential can be realized in practice.</p><p>The practical advantages of direct pattern sampling over the MCMC pattern sampling algorithms are very clear: while for instance the closed set Markov chain simulation <ref type="bibr">[6]</ref> takes seven minutes to sample a closed set from a 30K row subset of the US census dataset, frequency-based direct sampling takes only 0.067 seconds to draw the first sample including preprocessing-afterwards additional samples can be produced in milliseconds. For comparing direct sampling to exhaustive search algorithms there is a very large number of possible contenders in the literature, and, in principle, each of them requires an individual comparative study. For this paper we resort to a rather general setting: we compare the computation time of the sampling algorithms with that of the linear closed frequent set mining algorithm (lcm) of Uno et al. <ref type="bibr" target="#b27">[26]</ref> per pattern. For direct sampling this constitutes a worst-case setting because within the group of exhaustive methods frequent set mining algorithms usually produce the largest output per time unit and lcm is known to be among the fastest of them (winner of the FIMI contest <ref type="bibr" target="#b4">[4]</ref>). In addition to the datasets used for the predictive performance study, we also consider several of the larger benchmark datasets of the FIMI workshop, including the 1GB sized "webdocs", and a 500MB random dataset.</p><p>The results are presented as log/log scatter-plot in Figure <ref type="figure" target="#fig_7">4</ref>. One can observe that for most configurations lcm and the frequency-based sampling generate their patterns in approximately equal time with the majority of wins going to the sampling. However, focusing on the configurations with large-scale datasets (star symbol) reveals that the sampling algorithm can substantially outperform lcm. For "webdocs" this includes a speed-up factor of 10, for the random dataset even one of 25. We can conclude that frequency-based sampling can substantially outperform (closed) frequent set listing on large datasets and it performs equally well with slight advantage on small-scale data.</p><p>While the time performance of frequency-based sampling is also representative for area-based sampling, this is not true for the two sampling procedures with quadratic time weight computation phase. For almost all of the relatively small datasets of Tab. 1, this weight computation time is only marginal, i.e., less than 0.2 seconds. For large-scale datasets, however, the quadratic complexity is prohibitive; as already indicated on the 3772 row dataset "hypothyroid" where weight computation takes 13 seconds. After data record weights are computed, the performance is essentially equal to frequency-based sampling.  The scalability experiments above compared computation times for an equal number of generated patterns. This leads to the question of effectivity, i.e., what can be said about the quality per pattern? To this end, we again use the feature selection procedure from Section 4.1 and consider the quality of features within top-k frequent closed sets and random set families of the same size with respect to our measure of primary interest, the Fisher score; again using the setting of k = 32 D . This time we do not split the datasets according to the label, in order to avoid different values for global and local frequency of patterns (hence, we ignore discriminativity, which inherently requires splits). Table <ref type="table" target="#tab_4">3</ref> shows the results of four datasets that are representative for different constellations: For datasets that contain high-frequency patterns with high discriminative power, as for instance for "lymph", the top-k paradigm is highly effective. Often, however, the patterns with high Fisher score are of relatively low frequency and are hidden (from the perspective of top-k frequent set listing) underneath a large pattern set of high frequency but low discriminativity. Such a constellation can for instance be observed for "primary-tumor" (see also Fig. <ref type="figure" target="#fig_5">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We introduced four simple direct sampling procedures that generate random set patterns distributed according to frequency, squared frequency, area, and a discriminativity measure for binary labels. All procedures come with tight theoretical performance guarantees. Moreover, we described experimental studies demonstrating that the produced patterns are as useful as frequent pattern collections for patternbased classification, and that direct sampling can compete with and often even outperform the fastest exhaustive mining algorithms when generating an equal number of patterns.</p><p>In the context of pattern-based classification there is a large amount of pattern discovery approaches that range from optimistic-estimator-based best-first-search algorithms <ref type="bibr" target="#b24">[23]</ref> to methods interweaving model training and pattern discovery <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b13">13]</ref>. Although such algorithms typically traverse much less patterns per time unit as lcm, their search is more directed towards high quality patterns. This motivates an in-depth comparative study with such methods potentially leading to more sophisticated usage of the sampling algorithms (e.g., applying it within model training just as the cited approaches do with exhaustive mining).</p><p>That said, pattern sampling as a paradigm is in no way restricted to pattern-based classification, and should also be evaluated for other in particular unsupervised model construction tasks as well as for exploratory data analysis. This is likely to motivate further variants of pattern sampling procedures. An example is the introduction of column and row weights to the interestingness measure in order to model subjective interest in certain parts of the input data or to decrease the probability of re-discovering redundant patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Exhaustive search: involves complete generation of an enumeration space guaranteed to contain all interesting patterns; however, size of that space usually has no reasonable bound w.r.t. to input size and is hard to predict. (b) Controlled pattern sampling : no explicit construction of uncontrolled, potentially huge, part of pattern space; instead random generation of designated number of patterns; no guarantee of finding patterns satisfying hard interestingness threshold, but control over time, output size, and distribution.</figDesc><graphic coords="3,56.87,54.51,231.93,116.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>further measure considered here is the area function [15] qarea(D, F ) = |F | |D[F ]|. Intuitively, the area of a set corresponds to the number of 1 entries of the submatrix (of the binary matrix representation of D) consisting of the columns corresponding to F and the rows corresponding to D[F ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>F ) for all F ⊆ E because with this construction of the negative data portion we have |D [F ]| = |E| -|F |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Frequency-based Sampling Require: dataset D over ground set E, Returns: random set R ∼ q freq (P(E)) = qsupp(P(E)) 1. let weights w be defined by w(D) = 2 |D| for all D ∈ D 2. draw D ∼ w (D) 3. return R ∼ u(P(D))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Proposition 2 .</head><label>2</label><figDesc>On input dataset D over E, a family of k realizations of the random set R ∼ qarea(P(E)) can be generated in time O( D + k(|E| + ln |D|)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 3</head><label>3</label><figDesc>Discriminativity-based SamplingRequire: binary labeled dataset D over ground set E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average Fisher score of collections produced by methods for increasing values of α.</figDesc><graphic coords="6,316.81,430.35,239.07,185.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Time of lcm versus time of frequencybased sampling for an identical number of patterns.</figDesc><graphic coords="7,344.24,432.64,184.25,180.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pattern collections generated for "primary-tumor" by frequency-based sampling, squared-frequencybased sampling, and top-k closed frequent set listing; plotted according to frequency (x-axis) and Fisher score (y-axis); highlighted patterns are selected by feature selection procedure.</figDesc><graphic coords="8,53.80,53.80,502.09,130.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>dataset</cell><cell cols="5">class nm/ct items rows density</cell></row><row><cell>autos</cell><cell>7</cell><cell>15/10</cell><cell>135</cell><cell>205</cell><cell>0.190</cell></row><row><cell>balance-scale</cell><cell>3</cell><cell>4/0</cell><cell>20</cell><cell>625</cell><cell>0.250</cell></row><row><cell>breast-cancer</cell><cell>2</cell><cell>0/9</cell><cell>51</cell><cell>286</cell><cell>0.195</cell></row><row><cell>colic</cell><cell>3</cell><cell>7/15</cell><cell>84</cell><cell>366</cell><cell>0.271</cell></row><row><cell>credit-a</cell><cell>2</cell><cell>6/9</cell><cell>71</cell><cell>690</cell><cell>0.223</cell></row><row><cell>diabetes</cell><cell>2</cell><cell>8/0</cell><cell>40</cell><cell>768</cell><cell>0.225</cell></row><row><cell>glass</cell><cell>7</cell><cell>9/0</cell><cell>45</cell><cell>214</cell><cell>0.222</cell></row><row><cell>heart-c</cell><cell>5</cell><cell>6/7</cell><cell>49</cell><cell>303</cell><cell>0.285</cell></row><row><cell>heart-h</cell><cell>5</cell><cell>6/7</cell><cell>46</cell><cell>294</cell><cell>0.246</cell></row><row><cell>heart-statlog</cell><cell>2</cell><cell>13/0</cell><cell>55</cell><cell>270</cell><cell>0.254</cell></row><row><cell>hepatitis</cell><cell>2</cell><cell>6/13</cell><cell>55</cell><cell>155</cell><cell>0.344</cell></row><row><cell>hypothyroid</cell><cell>4</cell><cell>7/22</cell><cell>78</cell><cell>3772</cell><cell>0.364</cell></row><row><cell>iris</cell><cell>3</cell><cell>4/0</cell><cell>20</cell><cell>150</cell><cell>0.250</cell></row><row><cell>lymph</cell><cell>4</cell><cell>3/15</cell><cell>57</cell><cell>148</cell><cell>0.333</cell></row><row><cell>prim.-tumor</cell><cell>22</cell><cell>0/17</cell><cell>37</cell><cell>339</cell><cell>0.468</cell></row><row><cell>sonar</cell><cell>2</cell><cell>60/0</cell><cell>300</cell><cell>208</cell><cell>0.203</cell></row><row><cell>tic-tac-toe</cell><cell>2</cell><cell>0/9</cell><cell>27</cell><cell>958</cell><cell>0.370</cell></row><row><cell>vehicle</cell><cell>4</cell><cell>18/0</cell><cell>90</cell><cell>846</cell><cell>0.211</cell></row><row><cell>zoo</cell><cell>7</cell><cell>1/16</cell><cell>135</cell><cell>101</cell><cell>0.133</cell></row></table><note><p>Benchmark datasets with basic statistics: number of classes |C|, number of numerical and categorical columns (nm/ct), number |ET | of items in corresponding binary dataset, number of rows, density |DT | |ET | / DT .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of SVM classification on plain database and with feature enrichment based on frequent sets and sampled pattern collections (bold face indicates outperformed baseline).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Average (maximum) Fisher score of features selected from pattern collections.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This assumes that exp(n) &gt; |D|; the actual complexities are O( D + k(n + ln |D|)) and O( D</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>+ k(n + ln 2 |D|)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>For sampling according to the squared area function, draw a subset with probabilities proportional to its squared size instead of uniformly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Used hardware was a 2.67GHz 2 core CPU with 8GB RAM.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Part of this work was supported by the German Science Foundation (DFG) under 'GA 1615/1-1' as well as by the European Commission under 'ICT-FP7-LIFT-255951' and 'EU-FP7-250527'. The authors would like to thank Andrea Pietracaprina and Fabio Vandin for providing the top-k closed frequent set mining software.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast discovery of association rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Verkamo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowl. Disc. and Data Min</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="307" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Output space sampling for graph patterns</title>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="730" to="741" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting group differences: Mining contrast sets</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="246" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bayardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goethals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<title level="m">Proc. IEEE ICDM Workshop on Frequent Itemset Mining Implementations</title>
		<meeting>IEEE ICDM Workshop on Frequent Itemset Mining Implementations</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">126</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proc.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Soft constraint based pattern mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bistarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data and Knowl. Engineering</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="118" to="137" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Formal concept sampling for counting and threshold-free local pattern mining</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grosskreutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIAM Int. Conf. on Data Mining (SDM 2010)</title>
		<meeting>SIAM Int. Conf. on Data Mining (SDM 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximating the number of frequent sets in dense data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grosskreutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="89" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Origami: A novel and effective approach for mining representative orthogonal graph patterns</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chaoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Besson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Anal. Data Min</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="84" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative frequent pattern analysis for effective classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. on Data Engineering (ICDE 2007)</title>
		<meeting>23rd Int. Conf. on Data Engineering (ICDE 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="716" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Direct discriminative pattern mining for effective classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. on Data Engineering (ICDE 2008)</title>
		<meeting>24th Int. Conf. on Data Engineering (ICDE 2008)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient mining of emerging patterns: Discovering trends and differences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ACM SIGKDD Int. Conf. on Knowl. Disc. and Data Mining (KDD &apos;99)</title>
		<meeting>5th ACM SIGKDD Int. Conf. on Knowl. Disc. and Data Mining (KDD &apos;99)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct mining of discriminative and essential frequent patterns via model-based search tree</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Verscheure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. on Knowl. Disc. and Data Mining (KDD &apos;08</title>
		<meeting>14th Int. Conf. on Knowl. Disc. and Data Mining (KDD &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tiling databases</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goethals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mielikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Disc. Science Conf</title>
		<meeting>7th Int. Disc. Science Conf</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3245</biblScope>
			<biblScope unit="page" from="278" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient algorithms for listing combinatorial structures</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tight optimistic estimates for fast subgroup discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grosskreutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rüping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Machine Learning and Knowl. Disc. in Databases (ECML/PKDD 2008), Part I</title>
		<meeting>European Conf. on Machine Learning and Knowl. Disc. in Databases (ECML/PKDD 2008), Part I</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5211</biblScope>
			<biblScope unit="page" from="440" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovering all most specific sentences by randomized algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. of Database Theory (ICDT &apos;97)</title>
		<meeting>6th Int. Conf. of Database Theory (ICDT &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1186</biblScope>
			<biblScope unit="page" from="215" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation: A frequent-pattern tree approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="87" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pattern detection and discovery</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ESF Workshop on Pattern Detection and Disc</title>
		<meeting>ESF Workshop on Pattern Detection and Disc</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2447</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monte-carlo approximation algorithms for enumeration problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Madras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="448" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From local patterns to global models: the lego approach to data mining</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Knobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crémilleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Local Patterns to Global Models: Proc. ECML/PKDD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m">Workshop (LEGO &apos;08)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Traversing itemset lattice with statistical metric pruning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th ACM SIGMOD-SIGACT-SIGART Symp. on Principles of Database Systems (PODS)</title>
		<meeting>19th ACM SIGMOD-SIGACT-SIGART Symp. on Principles of Database Systems (PODS)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="226" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient incremental mining of top-k frequent closed itemsets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pietracaprina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vandin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Disc. Science Conf. (DS 2007)</title>
		<meeting>10th Int. Disc. Science Conf. (DS 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="275" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding the most interesting patterns in a database quickly by using sequential sampling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="833" to="862" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An efficient algorithm for enumerating closed patterns in transaction databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Uno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Disc. Science Conf. (DS 2004)</title>
		<meeting>7th Int. Disc. Science Conf. (DS 2004)</meeting>
		<imprint>
			<biblScope unit="volume">3245</biblScope>
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An algorithm for multi-relational discovery of subgroups</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Euro. Symp. on Principles of Data Mining and Knowl. Disc. (PKDD &apos;97)</title>
		<meeting>1st Euro. Symp. on Principles of Data Mining and Knowl. Disc. (PKDD &apos;97)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">1263</biblScope>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evaluation of sampling for data mining of association rules</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Workshop on Research Issues in Data Engineering (RIDE)</title>
		<meeting>7th Workshop on Research Issues in Data Engineering (RIDE)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
