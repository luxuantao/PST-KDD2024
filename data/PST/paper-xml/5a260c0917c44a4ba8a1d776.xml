<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAFE: A Simple Approach for Feature Extraction from App Descriptions and App Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Timo</forename><surname>Johann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Stanik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alireza</forename><forename type="middle">M</forename><surname>Alizadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walid</forename><surname>Maalej</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Hamburg</orgName>
								<address>
									<settlement>Hamburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SAFE: A Simple Approach for Feature Extraction from App Descriptions and App Reviews</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B021383DA921517906C861BD304EE7A0</idno>
					<idno type="DOI">10.1109/RE.2017.71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>User Reviews</term>
					<term>App Store Analytics</term>
					<term>Software Feature</term>
					<term>Data Mining</term>
					<term>Data-Driven Requirements</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A main advantage of app stores is that they aggregate important information created by both developers and users. In the app store product pages, developers usually describe and maintain the features of their apps. In the app reviews, users comment these features. Recent studies focused on mining app features either as described by developers or as reviewed by users. However, extracting and matching the features from the app descriptions and the reviews is essential to bear the app store advantages, e.g. allowing analysts to identify which app features are actually being reviewed and which are not. In this paper, we propose SAFE, a novel uniform approach to extract app features from the single app pages, the single reviews and to match them. We manually build 18 part-of-speech patterns and 5 sentence patterns that are frequently used in text referring to app features. We then apply these patterns with several text pre-and post-processing steps. A major advantage of our approach is that it does not require large training and configuration data. To evaluate its accuracy, we manually extracted the features mentioned in the pages and reviews of 10 apps. The extraction precision and recall outperformed two state-of-the-art approaches. For well-maintained app pages such as for Google Drive our approach has a precision of 87% and on average 56% for 10 evaluated apps. SAFE also matches 87% of the features extracted from user reviews to those extracted from the app descriptions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>App stores are particularly interesting to the software and requirements engineering community for two main reasons. First, they include millions of apps with the possibility to access hundreds of millions of users <ref type="bibr" target="#b25">[26]</ref>. Second, aggregate information from customer, business, and technical perspectives <ref type="bibr" target="#b6">[7]</ref>. Each app in the store is presented by its own app page 1 . This typically includes the app name, icons, screenshots, previews, and the app description as written and maintained by the developers. Users relay on this information to find and download the app they are looking for. Later, some users review the app and comment on its features and limitations. Some apps might receive even more than a thousand reviews per day <ref type="bibr" target="#b25">[26]</ref>, some of which include hints and insights for the developers.</p><p>Over the past years, we observed a "boom" of research papers, tools, and projects on app store analytics <ref type="bibr" target="#b21">[22]</ref>. Most of these works focus on mining large amount of app store data to 1 https://developer.apple.com/app-store/product-page/ derive advice for analysts, developers, and users. One popular analytic scenario is to mine app reviews for identifying popular user complaints or requests <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and assisting release planning <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Another scenario is to mine the app pages <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> and their evolution over time <ref type="bibr" target="#b26">[27]</ref> to derive recommendations for users or to identify combinations that increase download numbers <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p><p>A common and elementary step that these approaches share is the following: how can the app features included in the natural language text be automatically and accurately identified, in particular since the text is unstructured, potentially noisy, and uses different vocabulary. Previous approaches focus on mining the features from a particular artifact in the store, typically either from the pages or from the review. However, app vendors are likely interested in a holistic approach that works for multiple types of artifacts to be able to combine the customer, business, and technical information.</p><p>In this paper, we introduce SAFE, a simple, uniform approach to extract and match the app features from the app descriptions (written by developers) and the app reviews (commented by users). Our approach neither requires an apriori training with a large dataset nor configuration of machine learning features and parameters and works on single text elements such as a single app review. We simply identify, based on a deep manual analysis, a set of general textual patterns that are frequently used in app stores to denote app features. This includes a) 18 part-of-speech patterns (such as Noun_Noun_Noun to represent a feature like "Email chat history"), b) five sentence patterns (indicating enumerations and conjunctions of features), and c) several noise filtering patterns (such as filtering contact information). After preprocessing the text in a single app description or a review, we apply those patterns and extract a list of features: each consists of two to four keywords. Finally, we match the extracted features from the reviews to those extracted from the descriptions. The SAFE patterns and the SAFE feature extraction (Section II) represent our first contribution.</p><p>We implemented our approach and applied it to the descriptions and the reviews of 10 popular apps from the Apple App Store. We also reimplemented two frequently cited state-of-theart approaches: one focuses on app pages <ref type="bibr" target="#b11">[12]</ref>, the other on reviews <ref type="bibr" target="#b9">[10]</ref>. We evaluated and compared the feature extraction accuracy as well as the matching between the features in the reviews and the descriptions. The evaluation methodology, tools, and dataset (Section III) together with the benchmark results (Section IV) represent our second contribution. Our discussion how a uniform app feature extraction can be applied in practice, and what analytic tool vendors and researchers can learn from our research (Section V) represents the third contribution. Finally, Section VI summarizes related work and Section VII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE SAFE APPROACH</head><p>Features have manifold definitions in Requirements Engineering. According to Wieger and Beatty a feature denotes "one or more logically related system capabilities that provide value to a user and are described by a set of functional requirements" <ref type="bibr" target="#b28">[29]</ref>. Kang et al. define a feature as "a prominent or distinctive uservisible aspect, quality, or characteristic of a software system or systems" <ref type="bibr" target="#b13">[14]</ref>. Harman et al. focus on apps and state "a feature to be a property, captured by a set of words in the app description and shared by a set of apps" <ref type="bibr" target="#b11">[12]</ref>.</p><p>Features of a software represent an important way to communicate its capabilities to internal and external stakeholders. Features can help users to understand what the software is about and what they can and can't do with it. Features are also important for developers and vendors to structure their software projects and artifacts. They are frequently used in release planning and in issue trackers. Features can also be used in requirements and design artifacts and are also often traceable to source code and APIs.</p><p>We focus on high-level features that describe the essential functional capabilities of software and which are both understandable to developers and users. For instance, the app Dropbox has the following features described in its app page: "back up files", "send large files", "access files online", "access files offline", "create microsoft office files", "edit microsoft office files", "share links to files", and "includes storage". In contrast, low-level features contain only a set of words e.g. Internet, Wi-Fi, Connectivity or photo, picture, upload There is no uniform way how developers describe the features of their app, but we recognized some patterns. Descriptions often contain continuous texts, which describe features and purposes of an app. In addition, features are often summarized in a bullet point list. In app pages, features from the continuous texts are often repeated within the bullet points.</p><p>We exclude games from our analysis (and the evaluation) since we assume that they represent a special type of software <ref type="bibr" target="#b9">[10]</ref>, where requirements and features are described and handled differently. Murphy-Hill et al. <ref type="bibr" target="#b22">[23]</ref> studied game development at Microsoft and found that game developers are "designing [...] an emotional experience [...] which is very subjective [...] and is an artistic achievement." The authors gave the example: "in an e-commerce application, a user has a task to complete that typically takes only a few minutes. In contrast, in some games people play for hours straight on a daily basis over the course of months. As a result, the requirement for games is that the user should be able to stay engaged on multiple timescales, and the mechanism to achieve that will vary from game to game".</p><p>Our approach includes three main types of patterns to extract those features from app pages and app reviews. First, it defines a set of text patterns that are frequently used to describe features. We have identified these patterns based on a manual analysis of real app pages and reviews. Second, we apply these patterns on app pages and app reviews with some pre-and post-processing of the text to identify the features. Finally, we use text similarity algorithms to match the extracted features from multiple app artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Identifying the SAFE Patterns</head><p>The SAFE patterns consist of two types of text patterns that are frequently used to describe features in app stores: SAFE Part-of-Speech (POS) patterns and SAFE sentence patterns.</p><p>1) SAFE POS Patterns: In a first step, we analyzed app descriptions to identify commonly used POS patterns to denote app features. For this, we crawled the descriptions of 100 apps from the Google Play store. We then POS-tagged the text using the Natural language Tool Kit (NLTK) and extracted all sentences. Afterwards, we rendered each sentence in a tool. One researcher manually looked at the sentences to find those patterns. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of this task: the upper part shows the sentences in the tool including the POS tags; in the lower part, the researcher can enter the POS pattern (that is the list of POS tags) that describe a feature and insert that pattern to the pattern catalogue. In Figure <ref type="figure" target="#fig_0">1</ref>, the POS pattern is VB NN NN (verb, noun, noun) <ref type="foot" target="#foot_0">2</ref> . It describes the feature use incognito mode. Table <ref type="table">I</ref> summarizes the most frequent POS patterns in our sample that we use in SAFE. We cut off all POS patterns that occurred less than 10 times. By far the most frequently used SAFE POS patterns include two words. The remaining patterns include up to four words such as "choose from popular versions". 2) SAFE Sentence Patterns: Existing approaches often miss app features, if a single sentence contains a lot of independent features. For example, the sentence: "send and receive images, videos and sticker" contains the following six app features: "send images", "send videos", "send stickers", "receive images", "receive videos", and "receive stickers". As language has a high ambiguity, we do not cover all possible cases but those we found frequently in the app descriptions. In order to extract • Case 1: "send and receive attachments"</p><p>• Case 2: "View documents, PDFs, photos, and videos"</p><p>• Case 3: "Discuss and annotate notes and drafts"</p><p>• Case 4: "Use camera capture to easily scan and comment on pieces of paper, including printed documents, business cards, handwriting and sketches" • Case 5: "Write, collect and capture ideas as searchable notes, notebooks, checklists and to-do lists" SAFE identifies enumerations (comma separated text), conjunctions, and feature identifiers (words might be used to construct a feature) within each sentence <ref type="bibr" target="#b0">(1)</ref>. Then SAFE searches for occurrences and the placement of conjunctions within the sentence (2). Afterwards, the text parts of the left and the right side of each conjunction are analyzed to identify the placement of possible enumerations and feature identifiers (3). Finally, the feature identifiers are combined with the remaining list of app features (4).</p><p>Case 1 is rather simple. It is identified by the single conjunction "and" as well as the feature identifiers to the left and the right side of the conjunction. SAFE combines the feature identifiers to the app features "send attachments" and "receive attachments". In Case 2 SAFE identifies a conjunction on the right side of an enumeration, so that the approach can extract "view documents", "view PDFs", "view photos", and "view videos" as app features. Case 3 contains two conjunctions and four feature identifiers. Based on the sentence structure, SAFE creates the cross product of the feature identifiers leading to the app features "discuss notes", "discuss drafts", "annotate notes", and "annotate drafts". For Case 4 SAFE first identifies two conjunctions. Then, it finds the two feature identifiers "scan" and "comment" besides the first conjunction in the sentence. At the second conjunction, there is an enumeration on the left side and a feature identifier on the right side, which will be combined to a list of the five feature identifiers "paper", "printed documents", "business cards", "handwriting" and "sketches". In a final step, SAFE combines the first set of feature identifiers "scan" and "comment" with each element of the list resulting in the features "scan paper", "comment paper", and so forth.</p><p>Case 5 also contains two conjunctions. SAFE looks at the first conjunction and identifies an enumeration on its left and a feature identifier on its right. The second conjunction also has an enumeration on its left and a feature identifier on the right side. The final step combines each feature identifier of both lists to create app features such as "write notebooks" and "collect to-do lists".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Automated Feature Extraction</head><p>Figure <ref type="figure">2</ref> illustrates the overall steps of SAFE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Text Preprocessing:</head><p>The input of the preprocessing is either a single app description or a single review. First, SAFE performs a sentence tokenization. Then, all brackets and the text in between are removed as we observed that app features are prominently stated and that text in brackets often simply contains additional explanations. Next, SAFE filters three types of sentences: 1) sentences that contain URLs and 2) sentences that contain email addresses, as those are used to provide contact information, and 3) sentences that contain quotations, as they are most likely to be quoted reviews. Those types of sentences are removed from the data set. Further, in the remaining sentences such as bullet points and multiple symbols (such as '*' or '#') are removed. These are often used to visually delineate different parts of the descriptions.</p><p>Within the clean sentences, SAFE searches for keywords that introduce a subordinate clause. We cut off subordinate clauses, because they do not describe features (what the app is doing) but give reasons of why they are useful (how the app is behaving) as the following sentence shows: "The app let you upload images, so that you don't have to worry about losing them". Next, SAFE word-tokenizes each sentence without changing the order of the words. Thereafter, it removes stop words and the name of the app from each tokenized sentence to reduce the number of words that might introduce noise in the feature extraction <ref type="bibr" target="#b9">[10]</ref>. The final step of the preprocessing is to attach Part-of-Speech tags to each word token.</p><p>2) Application of SAFE Patterns: This part starts with analyzing and decomposing the sentences based on the conjunctions, enumerations, and feature identifiers as described in Section II-A. Then, SAFE applies the sentence patterns on the decomposed sentences to extract raw app features. The raw app features as well as the remaining sentences that did not match any sentence patterns represent the input for the next step, in which the SAFE POS patterns from Table <ref type="table">I</ref> are used to extract a list of feature candidates. Finally, SAFE iterates through all extracted feature candidates, removes duplicates and noise such as identical word pairs (e.g. "email email" which matches the SAFE POS pattern Noun Noun).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Automated Feature Matching</head><p>The input of this step is a list of feature candidates extracted from a single app description as well as the extracted feature candidates from the related user reviews. SAFE matches the feature candidates from both, the app description and the related user reviews. The matching is based on a binary text similarity function at sentence level that is inspired by approaches presented by Guzman and Maalej <ref type="bibr" target="#b9">[10]</ref> and Li et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>Overall, SAFE performs three steps (see Figure <ref type="figure">2</ref>) , each representing a different approach to identify matching features, starting with the most restrictive approach. First, SAFE performs the matching on a single term level. This means, that two feature candidates are matching if each single term of them is equal, ignoring the order of the terms. For instance, "send email" and "email send" are considered matching features. Second, SAFE uses the synonym sets of the words captured from WordNet to perform the second step of finding matching features. In this approach two app features are considered a match, if their terms are synonyms. For instance, "take photo" and "capture image" represent a match. However, the accuracy of this step declines when the number of words of both candidate features is not equal. Hence, we also use a semantic similarity matching at sentence level introduced by Yuhua Li ant et al. <ref type="bibr" target="#b15">[16]</ref> as our third step. This approach incorporates two similarity metrics: on the one hand, the semantic similarity employing statistical characteristics of a lexical corpus that can be inferred as a domain knowledge base; on the other hand, the order similarity of word order vectors extracted from the sentences. Since we assume that the ordering of the words does not affect the meaning of the features, we skip the word order similarity metric.</p><p>Finally, to deal with the candidate features that have unequal number of words, we utilized cosine similarity calculated between two feature candidates through the semantic similarity </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EMPIRICAL EVALUATION</head><p>We introduce our evaluation goals, evaluation questions, and data used. We then describe the methodology followed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Goals and Data</head><p>We implemented and tested SAFE as a python library and conducted an empirical evaluation to assess the approach effectiveness and accuracy to extract and match features. We focused on the following evaluation questions:</p><p>1) How precise is SAFE when extracting features from app descriptions and from app reviews? 2) How does SAFE perform compared to other state-of-theart approaches? 3) How accurate can SAFE match the extracted features from the app descriptions and the reviews? For the evaluation, we created a dataset by crawling descriptions and reviews of 10 apps from the Apple App Store. We selected the top five free and top five paid apps (January 2017) from the category Productivity of the Apple App Store. We chose the storefront of the United States to obtain reviews in English. For each app, we gathered its description page using the iTunes Search API <ref type="foot" target="#foot_1">3</ref> . From the app description page, we extracted metadata consisting of 44 values, such as the name, version, description, category, or price. The app reviews were programmatically scraped using a self-developed tool, which accesses an internal iTunes API. A review consists of 10 values, including the reviewer name, title, description, rating, and date. For further analysis, the data was persisted inside a MongoDB database. To enable replication, we publish the dataset on our website <ref type="foot" target="#foot_2">4</ref> .</p><p>We also reimplemented two recent, frequently cited approaches for extracting features from app store data: the approach of Harman at al. <ref type="bibr" target="#b11">[12]</ref> (also used by Al-Subaihin et al. <ref type="bibr" target="#b1">[2]</ref>) focuses on app descriptions and the approach by Guzman and Maalej <ref type="bibr" target="#b9">[10]</ref> focuses on for app reviews.</p><p>The feature extraction approach by Harman et al. includes four main steps <ref type="bibr" target="#b11">[12]</ref>. First, it extracts coarse features by analyzing app descriptions of multiple apps to find the part of the description that contains app features. Then, it removes stop words to filter out noise. Afterwards, word frequencies and trigram collocations are computed, which leads to featurelets. Finally, the authors use a greedy clustering algorithm to group featurelets that share at least two words.</p><p>The approach of Guzman and Maalej focuses on feature extraction on a large set of reviews. The authors gather the title and the comments of each review of an app. Then, they perform several preprocessing steps to extract nouns, verbs, and adjectives, to remove stop words, and to perform lemmatization and group inflected words. Afterwards, the authors perform bigram collocation finding by means of a likelihood ratio test <ref type="bibr" target="#b20">[21]</ref>. Thereafter, they filter the collocations by keeping those that appear in at least three reviews while ignoring the order of the words within the collocations. As users might use different terms for the same app feature, the authors group the remaining collocations which have the same synonyms. Finally, they choose the collocation with the highest frequency to be the representation of its group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Method</head><p>To evaluate the accuracy of SAFE and compare it with the two other approaches, we first created three evaluation sets: for the extraction from app descriptions, for the extraction from app reviews, and for the feature matching. Then we instructed human coders to verify the results of the three approaches. The creation of the evaluation sets are described in each of the following subsections.</p><p>The evaluation was performed in two parts. First, we evaluated the app feature extraction on app descriptions and then on app reviews. The benchmark approaches had been replicated to the best of our knowledge. As the approach of Harman et al. focuses on app descriptions, we used it as a benchmark to evaluate the feature extraction from app descriptions. Likewise, as the Guzman and Maalej approach was developed for app reviews we applied their approach as a benchmark for extracting features from reviews. In the following we explain both procedures.</p><p>1) App Descriptions: For the creation of the evaluation set, two authors independently and manually extracted app features from the description of each app. We compared the extracted app features of both authors and agreed what features should be in the evaluation set. In case of disagreements a third person was involved. However, this was only necessary in 2 cases.</p><p>We then extracted the app features using Harman's et al. approach and SAFE and checked the results with the manual extraction. For this, we implemented a coding tool shown on Figure <ref type="figure" target="#fig_1">3</ref>. The tool shows the app description on the left side and asks two independent coders to carefully check the manual set and state, on a binary scale, if the extracted app features are part of the app description. The coding tool displays one extracted app feature at the time. We randomized the extracted app features of both approaches to reduce possible bias that  might be introduced if the results of each approach would be shown one after another. Finally, we use this evaluation set to calculate precision, recall, and the f-measure by comparing the coding results with the evaluation set.</p><p>For the manual feature extraction we agreed on the following rules:</p><p>1) Focus on functional aspects and omit quality aspects: "Quickly search emails" becomes "search emails". 2) Focus on the essential feature: "Upload your family photos" becomes "upload photos". 3) Remove the benefit gained from a feature: "Undo Send, to prevent embarrassing mistakes" becomes "undo send". 4) "Open, edit, and save documents" is split to "open documents", "edit documents", and "save documents". 2) App Reviews: For the evaluation of the feature extraction from the reviews, we selected the 80 most recent app reviews of five apps from our data set. We compared the feature extraction by Guzman and Maalej and SAFE. As the approach of Guzman and Maalej expects a big set of app reviews, we feeded it with a random sample of 5000 reviews for each app (except for one app, which only had a total of 3,742 app reviews). In contrast, SAFE expects single app reviews as input.</p><p>We also adjusted the coding tool to match the review evaluation, as shown in Figure <ref type="figure" target="#fig_2">4</ref>. Other than for the app description, the creation of the ground truth was during the coding by manually adding missing features. On the left side of the tool, a single app review is displayed. On the right side, all extracted app features from both approaches are listed randomly. Two independent coders have a binary choice to check if a feature was correctly extracted. To reduce a possible coder bias, the order of the approaches is randomized for each app review. In addition, the coder added the app features each approach missed by writing them down in the appropriate text box below the correlating approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Matching</head><p>We evaluated the feature matching by using the true positive features extracted from the app descriptions and the true positives from the app reviews. We took the extracted app features from the five apps we used for coding the reviews. We then run the feature matching of SAFE and created a table containing the results. Table III shows a simplified example of the evaluation. It includes the app features extracted from the reviews, the app features extracted from the description, as well as the matching result of SAFE and a column in which the coder evaluates the result. Two human coders evaluated the matching results. During the evaluation, the coders also had access to the list of all true positive extracted app features from the descriptions. This enabled the coders to verify if app features for which SAFE could not find a match, really did not contain any.</p><p>There are three main use cases for the matching: First, we can identify app features users refer to. Second, we can use this information to identify app features that users refers to but are not described in the app page. Finally, we might be able to extract app feature requests from mismatches. As soon as all the extracted app features of a review have been evaluated, the coder will be asked if the approaches missed any feature. This additional step allows us to calculate the recall, because we have not created an evaluation set for the app reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION RESULTS</head><p>We separately evaluated the feature extraction of the SAFE approach for descriptions and reviews, as well as the feature matching. We compared the results of the feature extraction with two recent approaches. We chose the approach of Harman et al. <ref type="bibr" target="#b11">[12]</ref>, which was designed for feature extraction from descriptions. For feature extraction from reviews we chose the approach of Guzman and Maalej <ref type="bibr" target="#b9">[10]</ref>. We calculated precision and recall for feature extraction from descriptions and reviews as well as for the feature matching as in 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P recision = tp tp + fp</head><p>(1)</p><formula xml:id="formula_0">Recall = tp tp + fn (2)</formula><p>For the feature extraction steps the precision is the fraction of retrieved features that are part of our manual evaluations set. Recall is the fraction of features that are part of our evaluation set and are retrieved by the respective approach. The true positives (tp) are the number of features that are correctly extracted by the approaches (those that are part of the evaluation set). The false positives (fp) are the number of features that are extracted by the approaches but are not considered as a feature (not part of the evaluation set). The false negatives (fn) are the features that are not extracted by the respective approach, but are part of the evaluation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction from the App Descriptions</head><p>We evaluated the features extracted from the descriptions by our approach and by the approach of Harman et al. SAFE extracts features from the whole description, whereas Harman et al's. approach only uses texts from the bullet lists of the app page. Table <ref type="table" target="#tab_4">IV</ref> shows the results. The evaluation is based on ten apps (see Section III-B) and contains 197 features (∼20 per app). The Harman et al's. approach extracted 208 and SAFE 161 feature candidates.</p><p>SAFE achieved an overall average precision of .559 and a recall of .434. Respectively, our implementation of the Harman et al. approach achieved an average precision of .278 and a recall of .292. This results in an increase of about 100% in precision and 48% in recall between both approaches. The results are highly dependent on the analyzed app (standard derivation of precision: .21 and recall: .12 for SAFE; .15/.13 for Harman et al.). Both approaches have the highest precision for the Google Drive app and the lowest for the Printer Pro app. Highest recalls are achieved for Fantastical 2 (SAFE) and Cloud App Mobile <ref type="bibr">(Harman)</ref>. The low performance for the Printer Pro app is because the verb "print", which is needed to construct app features in this case, occurred outside of the bullet list in the description. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction from the App Reviews</head><p>Similarly, we evaluated the feature extraction from reviews. Users also describe misbehavior of apps which are syntactically very similar to a feature description. For example "The app deleted all my notes". We used the SAFE approach and an implementation of the Guzman and Maalej <ref type="bibr" target="#b9">[10]</ref> approach to extract features from reviews. Table <ref type="table" target="#tab_5">V</ref> shows the results for the extractions. Both approaches achieved a comparably low precision whereas the SAFE approach has a significant higher recall (.709) compared to the approach of Guzman and Maalej (.276), which is a difference of about 157%. The low precision of the approaches can be traced back to the fact that user reviews are very noisy and do not always refer to features as it is often the case in app descriptions.</p><p>In contrast to descriptions, reviews often contain slang, typos or incorrect grammar and thus represent a more challenging input for feature extraction. The average recall of .709 means that SAFE does not miss too many features. In practice it might be more relevant to extract most of the features than the reduction of the noise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Matching Features in the Descriptions and the Reviews</head><p>Finally, we report the results of our evaluation on the matching of the extracted features. The evaluation of the app reviews showed that in total 178 app features had been extracted correctly by SAFE from reviews. The data set contains these 178 true positives. The feature matching approach of SAFE uses the data set to look for matches in the true positive extracted app features from the app descriptions. SAFE could identify 27 matches of which the coders found that 19 are correct. SAFE was able to achieve a recall of .560 and a precision of .704 to correctly identify matches. On the other hand, SAFE's precision to identify non-matches correctly is .900. Additionally, we calculated the accuracy as it also contains the true negatives and the false negatives as shown in Equation <ref type="formula" target="#formula_1">3</ref>. SAFE had a promising accuracy of .870.</p><formula xml:id="formula_1">Accuracy = tp + tn tp + tn + fp + fn<label>(3)</label></formula><p>V. DISCUSSION</p><p>We discuss how researchers and tool vendors can use SAFE and similar approaches to help software engineering teams analyze and exploit app store data. We then discuss how SAFE can be combined with other approaches to overcome its limitations. Finally, we discuss the threats to validity of our research and the measures we used to mitigate these threats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Usage Scenarios for App Feature Extraction</head><p>Feature extraction is an elementary step for implementing many app store analytic scenarios in practice <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Perhaps the most intuitive scenario is monitoring app features' health, that is to continuously identify and measure which app features are mentioned in user comments, how frequent, and which sentiments are associated with which feature <ref type="bibr" target="#b9">[10]</ref>. Moreover, app store analytics approaches that aim at classifying the reviews as informative or not <ref type="bibr" target="#b4">[5]</ref> and those that aim at classifying feedback as bug reports, feature requests, rating, and user experience <ref type="bibr" target="#b17">[18]</ref> would profit from organizing the results based on app referred features. For instance, bug reports associated with certain features might help developers to faster trace the bugs to the relevant software components or to find other bug reports related to the same feature which include additional information for the bug reproduction.</p><p>The next analytic scenario enabled by feature extraction is the identification of the features' delta and new insights, that is identifying the differences between features described by developers and those described in the user reviews. The features' delta can even distinguish between different reviews (and thus users') subpopulations, such as features mentioned by certain users, in certain regions, stores or channels (e.g. forum or social media).</p><p>Such a difference can reveal two insights: either users apply a different vocabulary or the features discussed in the reviews are missing in the app. In the first case, developers and users might learn how to communicate better and bridge their "vocabulary gap". In the second case, developers can easily get the list of suggested features or at least those that are associated with their apps but not included in them: a useful information for requirements identification, scoping, and prioritization. As apposed to analytics approaches that allow identifying reviews with feature requests (e.g. by Iakob and Harrison <ref type="bibr" target="#b12">[13]</ref> or by Maalej et al. <ref type="bibr" target="#b17">[18]</ref>), feature extraction allows also to filter the relevant review parts (e.g. sentences), which include the relevant information -something particularly useful as the quality and structure of the reviews are very heterogeneous. Relevant insightful information about the feature request might be in the first or in the last sentence of a review. This will also enable the clustering and aggregation of artifacts (such as comments) based on features as well as identifying feature duplicates in these artifacts.</p><p>Finally, feature extraction is the preliminary underlying step in many if not all feature recommendation and optimization scenarios <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Feature recommendations might target both users and developers. For users, feature extraction can help building recommender models that learn dependencies of reported and used features. From the app usage, such models can also derive which features users are interested in. When combining features mentioned in the pages of many apps (e.g. apps within the same category), a predictor model could derive the optimal combination of features, and answer questions such as "which dependencies of features get the best ratings". This might also help identifying which features are missing in a particular app release from the "feature universe".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Using SAFE in Practice</head><p>SAFE has one major advantage since it is uniform: that is, it can be applied on multiple artifacts, in particular on app descriptions and app reviews. SAFE automatically identifies app features in the descriptions and maps those to the app features in the reviews. It identifies fives feature sets:</p><p>• App features mentioned in the description.</p><p>• App features mentioned in the reviews.</p><p>• App features mentioned in the description and in the reviews (intersection). • App features mentioned only in the description (probably unpopular features). • App features mentioned only in the reviews (probably feature requests). Another major advantage of SAFE is that it does not require a training or a statistical model and works "online" on the single app pages and the single reviews. This enables processing the pages and the reviews immediately during their creations. This also reduces the risk for overfitting a particular statistical model to certain apps, domains, or users.</p><p>Nevertheless, we think that the achieved accuracy of SAFE -even if it outperforms other research approaches -is not good enough to be applied in practice. We think that a hybrid approach (a simple, pattern and similarity based as SAFE together with a machine learning approach) is probably the most appropriate. For instance, machine learning can be used to pre-filter and classify reviews before applying SAFE on them. Indeed, we think that this is the main reason why our accuracy values were rather moderate: because we were very restrictive in the evaluation and applied SAFE on "wild, random" reviews.</p><p>SAFE can also be extended by a model that is trained from multiple pages and multiples users. This might e.g. be used to support users to improve their vocabulary by auto-completing feature terms.</p><p>Also, developers and analysts should be able to correct the extracted features e.g. following a critique-based recommender model, or simply building a supervised classifier based on SAFE and continuously improving the classifier model based on developers' feedback (identifying false positives and true negatives). This can either be used to persist the list of actual app features or to train a classifier to learn or adjust patterns per app.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Limitations and Threats to Validity</head><p>As for every study that includes manual coding, the coders in our evaluation might have done mistakes a) indicating wrong features in the descriptions and reviews or b) wrongly assessing the extraction of the SAFE or the two reference approaches. To mitigate this risk, we conducted a careful peer-coding based on a coding guide (e.g. stating what is a feature) and using a uniform coding tool. Therefore, we think that the reliability and validity of the app description and feature matching evaluations -both involving peer-coding -is rather high. For the reviews, we decided to have one single coder for two reasons. First, the large number of reviews would require more resources. Second, the coders were well trained from the first phase of coding app descriptions. Still in future, manually pre extracting review features, as we did for the features from app descriptions, would certainly improve the reliability. As we share the data, we encourage follow researchers to replicate and extend the evaluation.</p><p>Concerning the sampling, our evaluation relies only on Apple App Store data and apps from one single category. While this helped concentrating the effort and to learn the evaluation apps, it certainly limits the external validity of our results. It is, however, important to mention that the SAFE patterns, the core "idea" of the approach, were identified from Google Play Store data of 100 apps from multiple categories. Since SAFE also works on Apple Store data, we feel comfortable to claim that our approach is generalizable to other apps, except for games which we ignored on purpose (see Section II).</p><p>As for the accuracy and benchmark values, we refrain from claiming that these are exact values and we think that they are rather indicative. We think that the order of magnitude of precisions and recalls calculated, as well as the differences between the approaches is significant. We took special careful measures to conduct a fair and non-biased comparison. For instance, during the evaluation of the extraction, the tool randomly showed the extracted results in the same layout, without mentioning which approach extracted which features.</p><p>Finally, when reimplementing the reference approaches we might have done mistakes. We tried to conduct the comparison as fair as possible and implemented all steps described in the papers to the best of our knowledge. In some cases (e.g. list of stop words), we tried to rather be spacious and fine-tuned the reference approach to have results comparable to those reported on the papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. App Store Analysis</head><p>App stores contain important information for app developers, analysts, and other software stakeholders <ref type="bibr">[20] [25]</ref>. App store analysis is thus a growing research field. Martin et al. <ref type="bibr" target="#b21">[22]</ref> conducted a systematic literature review on app store analysis. The earliest reported study dated to the year 2010. Martin et al. <ref type="bibr" target="#b21">[22]</ref> defined 7 subfields of app store analysis: API Analysis, Feature Analysis, Release Engineering, Review Analysis, Security, Store Ecosystem, and Size &amp; Effort Prediction. 127 technical and non-technical studies were considered relevant. Our study can be classified into the fields of Feature Analysis as well as Review Analysis and is -to the best of our knowledge -the first that combines these two fields.</p><p>Indeed, most promising app store analytics approaches proposed so far are rather app independent by nature. For instance, classifying reviews in bug reports, feature request, user experience, or ratings <ref type="bibr" target="#b17">[18]</ref> is semantically independent from the single apps and is rather related to general software engineering activities and concepts (bugs, enhancement, etc.). Approaches that tried to analyze the app problem domain rather focus on identifying topics <ref type="bibr" target="#b7">[8]</ref> (which can be everything) or filtering informative reviews <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p><p>The most related approaches to our are those of Harman et al. <ref type="bibr" target="#b11">[12]</ref> and Guzman and Maalej <ref type="bibr" target="#b9">[10]</ref>. The latter focused on feature extraction from user reviews and applied a sentiment analysis to find out how users like a certain feature. The main difference to our approach is that we do not need a large data input to train a model. The approaches are also intended for either user reviews or app descriptions but not for both. In contrast, we also enable the matching of features mentioned in the reviews and the descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction, Mining, and Recommendation</head><p>Baker et al. carried out a systematic literature review on feature extraction approaches from natural language requirements <ref type="bibr" target="#b3">[4]</ref>. They identified 13 articles that use feature extraction approaches. However, these are not related to feature extraction from app pages and user reviews in app stores. Our approach is highly tightened to these artifacts with the intention to support app store analytic scenarios as discussion in Section V.</p><p>Feature mining and recommendation are often discussed in the context of domain analysis <ref type="bibr" target="#b2">[3]</ref>. The majority of related work focused on requirements specification or other project internal documentation. These kinds of documents are already semi formatted and are not comparable to descriptions and reviews in app stores.</p><p>Studies that explicitly focus on product descriptions and reviews are yet limited, pursue different goals, or are focused on specific tasks. Hariri et al. <ref type="bibr" target="#b10">[11]</ref> rely on previous work of Dumuitru et al. <ref type="bibr" target="#b5">[6]</ref> and focus on software product descriptions, but not on the app stores. Harman et al. <ref type="bibr" target="#b11">[12]</ref> presented a feature extraction approach from app store descriptions. Their approach aimed more at the clustering and is relevant, e.g., for comparison purposes. They consider elements such as near, wifi, hotspot as a feature, whereas our approach is looking for fine-grained features such as upload pdf file or edit photos.</p><p>Acher et al. <ref type="bibr" target="#b0">[1]</ref> build a feature model, based on features described in a table structure. In app stores, descriptions are not predefined in formal or semi formal structures, the approach cannot be applied directly. Similarly, Wang et al. <ref type="bibr" target="#b29">[30]</ref> present a feature recommendation approach from online software repositories, like app stores and base their approach on feature topic models which cannot be applied on single product descriptions.</p><p>Lulu and Kuflik <ref type="bibr" target="#b14">[15]</ref> presented an approach to cluster apps based on their functionality and enable users to find a desired app faster. The authors propose an unsupervised machine learning approach that extracts function keywords found in app descriptions, blogs, and articles and processes those to build the clusters. As the goal of the authors was to create clusters, they do not extract a set of human readable app features but rather a set of function keywords from multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we presented a simple approach for feature extraction (SAFE) from both app pages and user reviews. We call it simple because it neither requires an a priori training with a large dataset nor a configuration of machine learning features and parameters. The SAFE approach performs a comprehensive preprocessing of the natural language input, applies Part-of-Speech and sentence patterns to extract human readable features and finally matches the features from app reviews to the features extracted in the related app page. On app descriptions, we obtained a precision of up to 88% with an average of 56% and a recall of 70% with an average of 43%. For unfiltered user reviews, we had an average precision of 24% but could achieve a recall of 71%. Features extracted from user reviews are thus still noisy but the relatively high recall confirms that we catch the prevailing majority of features discussed by the users. The feature matching approach performed three steps, consisting of a single term matching, a synonym matching, and a semantic similarity matching achieving an accuracy of 87%.</p><p>Our approach is an enabler for multiple app store analytics scenarios like monitoring app features' health, the identification of features' delta for gaining new insights, and feature recommendation and optimization. Therefore, SAFE gives analysts a feature-based perspective on their apps.</p><p>Even if the results are encouraging, future research and a fine tuning of SAFE is needed. A subsequent machine learning approach trained by developers can further improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tool to identify POS tag patterns used to denote app features.</figDesc><graphic coords="2,315.57,453.75,236.68,100.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Coding tool for evaluating the feature extraction from app descriptions. A: The app to be coded. B: The raw app description. C: A feature extracted by an automated approach (i.e. either SAFE or Harman et al.). D: Code if C is a feature and part of the description. E: Navigation. F: Progress bar.</figDesc><graphic coords="5,313.55,396.59,236.90,131.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Coding tool for evaluating the feature extraction from reviews. The app to be coded. B: The raw app review. C: The features extracted by both approaches (SAFE and Guzman and Maalej with random order). A checked box marks a correctly identified feature. D: Text field to add features that the approach might have missed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COSINE</head><label>II</label><figDesc>SIMILARITY OF EXEMPLARY CANDIDATE FEATURES.</figDesc><table><row><cell>Feature 1</cell><cell>Feature 2</cell><cell>Cosine Similarity</cell></row><row><cell>compose new email</cell><cell>create new email</cell><cell>0.85</cell></row><row><cell>taking image</cell><cell>capture image</cell><cell>0.73</cell></row><row><cell>send attachments</cell><cell>send attached files</cell><cell>0.63</cell></row><row><cell>add image</cell><cell>delete image</cell><cell>0.38</cell></row><row><cell>send new email</cell><cell>receive emails</cell><cell>0.35</cell></row><row><cell cols="3">algorithm introduced by Li et al. [16]. We set the threshold of</cell></row><row><cell cols="3">the similarity to .7 based on experiments we performed while</cell></row><row><cell cols="3">creating the feature matching approach. Correlation factors</cell></row><row><cell cols="3">between .7 and 1 are considered significant. Some examples</cell></row><row><cell cols="3">of the calculated similarities of candidate features are shown</cell></row><row><cell>on Table II.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SIMPLIFIED</head><label>III</label><figDesc>EXAMPLE OF THE FEATURE MATCHING EVALUATION.</figDesc><table><row><cell>Feature (review)</cell><cell>Feature (description)</cell><cell>Matching</cell><cell>Coder's</cell></row><row><cell></cell><cell></cell><cell>Result</cell><cell>Statement</cell></row><row><cell>send email</cell><cell>send emails</cell><cell>match found</cell><cell>True</cell></row><row><cell>month view</cell><cell>week view</cell><cell>match found</cell><cell>False</cell></row><row><cell>retrieve images</cell><cell>retrieve pictures</cell><cell>no match</cell><cell>False</cell></row><row><cell>email notification</cell><cell></cell><cell>no match</cell><cell>True</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EVALUATION</head><label>IV</label><figDesc>RESULTS FOR APP DESCRIPTIONS. # MEANS COUNT, P MEANS PRECISION, R MEANS RECALL. MAX VALUES IN A COLUMN ARE IN BOLD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Approaches</cell><cell></cell><cell></cell><cell></cell></row><row><cell>App Name</cell><cell># Reviews</cell><cell>Evaluation Set</cell><cell></cell><cell cols="2">Harman et al. [12]</cell><cell></cell><cell></cell><cell cols="2">SAFE</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feature #</cell><cell>#</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>#</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Forest: Stay focused, be present</cell><cell>913</cell><cell>13</cell><cell>15</cell><cell>.266</cell><cell>.286</cell><cell>.275</cell><cell>13</cell><cell>.462</cell><cell>.400</cell><cell>.429</cell></row><row><cell>Yahoo Mail -Keeps You Organized!</cell><cell>29,186</cell><cell>34</cell><cell>25</cell><cell>.400</cell><cell>.294</cell><cell>.339</cell><cell>19</cell><cell>.737</cell><cell>.389</cell><cell>.509</cell></row><row><cell>Printer Pro -Print documents, photos, emails</cell><cell>6,377</cell><cell>11</cell><cell>2</cell><cell>.000</cell><cell>.000</cell><cell>-</cell><cell>14</cell><cell>.214</cell><cell>.250</cell><cell>.231</cell></row><row><cell>Gmail -email by Google: secure, fast &amp; organized</cell><cell>57,212</cell><cell>24</cell><cell>19</cell><cell>.474</cell><cell>.391</cell><cell>.429</cell><cell>14</cell><cell>.714</cell><cell>.400</cell><cell>.513</cell></row><row><cell>Google Drive -free online storage</cell><cell>21956</cell><cell>17</cell><cell>10</cell><cell>.500</cell><cell>.357</cell><cell>.417</cell><cell>8</cell><cell>.875</cell><cell>.389</cell><cell>.538</cell></row><row><cell>CloudApp Mobile</cell><cell>111</cell><cell>26</cell><cell>41</cell><cell>.317</cell><cell>.464</cell><cell>.377</cell><cell>18</cell><cell>.722</cell><cell>.481</cell><cell>.578</cell></row><row><cell>Google Docs</cell><cell>18,766</cell><cell>12</cell><cell>13</cell><cell>.231</cell><cell>.231</cell><cell>.231</cell><cell>9</cell><cell>.667</cell><cell>.462</cell><cell>.545</cell></row><row><cell>Dropbox</cell><cell>12,563</cell><cell>9</cell><cell>6</cell><cell>.333</cell><cell>.200</cell><cell>.250</cell><cell>10</cell><cell>.300</cell><cell>.300</cell><cell>.300</cell></row><row><cell>Fantastical 2 for iPhone -Calendar and Reminders</cell><cell>3,724</cell><cell>30</cell><cell>65</cell><cell>.123</cell><cell>.258</cell><cell>.167</cell><cell>46</cell><cell>.500</cell><cell>.697</cell><cell>.582</cell></row><row><cell>iTranslate Voice -Speak &amp; Translate in Real Time</cell><cell>11,834</cell><cell>21</cell><cell>12</cell><cell>.333</cell><cell>.211</cell><cell>.258</cell><cell>10</cell><cell>.500</cell><cell>.278</cell><cell>.357</cell></row><row><cell>Overall</cell><cell></cell><cell>197</cell><cell>208</cell><cell>.278</cell><cell>.292</cell><cell>.27</cell><cell>161</cell><cell>.559</cell><cell>.434</cell><cell>.458</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V EVALUATION</head><label>V</label><figDesc>RESULTS FOR APP REVIEWS. F MEANS NUMBER OF ACTUAL FEATURES, # MEANS TOTAL FEATURES EXTRACTED, P MEANS PRECISION, R MEANS RECALL.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Approaches</cell><cell></cell><cell></cell><cell></cell></row><row><cell>App</cell><cell>Manual</cell><cell cols="4">Guzman&amp;Maalej [10]</cell><cell></cell><cell cols="2">SAFE</cell><cell></cell></row><row><cell>Name</cell><cell>#F</cell><cell>#</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>#</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Yahoo</cell><cell>46</cell><cell>147</cell><cell>.260</cell><cell>.283</cell><cell>.271</cell><cell>74</cell><cell>.238</cell><cell>.761</cell><cell>.363</cell></row><row><cell>Gmail</cell><cell>53</cell><cell>158</cell><cell>.224</cell><cell>.283</cell><cell>.250</cell><cell>104</cell><cell>.247</cell><cell>.736</cell><cell>.370</cell></row><row><cell>Dropbox</cell><cell>52</cell><cell>154</cell><cell>.237</cell><cell>.280</cell><cell>.257</cell><cell>90</cell><cell>.260</cell><cell>.727</cell><cell>.383</cell></row><row><cell cols="2">Fantastical 65</cell><cell>196</cell><cell>.189</cell><cell>.274</cell><cell>.224</cell><cell>146</cell><cell>.230</cell><cell>.652</cell><cell>.340</cell></row><row><cell>iTranslate</cell><cell>28</cell><cell>89</cell><cell>.189</cell><cell>.250</cell><cell>.215</cell><cell>60</cell><cell>.213</cell><cell>.679</cell><cell>.325</cell></row><row><cell>Overall</cell><cell>244</cell><cell>744</cell><cell>.218</cell><cell>.276</cell><cell>.244</cell><cell>474</cell><cell>.239</cell><cell>.709</cell><cell>.358</cell></row><row><cell>means</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>POS tags of the Penn Treebank Project: https://www.ling.upenn.edu/courses/ Fall_2003/ling001/penn_treebank_pos.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://affiliate.itunes.apple.com/resources/documentation/ itunes-store-web-service-search-api</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://mast.informatik.uni-hamburg.de/app-review-analysis</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is part of the OPENREQ project, funded by the Horizon 2020 program of the European Union. Project Id: 732463.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On extracting feature models from product descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Acher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cleve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Perrouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heymans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vanbeneden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lahire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Variability Modeling of Software-Intensive Systems, VaMoS &apos;12</title>
		<meeting>the Sixth International Workshop on Variability Modeling of Software-Intensive Systems, VaMoS &apos;12</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering mobile apps based on mined textual features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Al-Subaihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Capra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM &apos;16</title>
		<meeting>the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM &apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An exploratory study of information retrieval techniques in domain analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schwanninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rummler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Software Product Line Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Bakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Kasirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Salleh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="132" to="149" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AR-miner: Mining informative reviews for developers from mobile app marketplace</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Software Engineering, ICSE 2014</title>
		<meeting>the 36th International Conference on Software Engineering, ICSE 2014</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="767" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On-demand feature recommendations derived from mining public product descriptions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mobasher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castro-Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirakhorli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Software Engineering, ICSE &apos;11</title>
		<meeting>the 33rd International Conference on Software Engineering, ICSE &apos;11</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">App store analysis: Mining app stores for relationships between customer, business and technical characteristics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>RN/14/10</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>UCL Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Reseach Note</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of user comments: an approach for software requirements evolution</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Galvis Carreño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Winbladh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE &apos;13 Proceedings of the 2013 International Conference on Software Engineering</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="582" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Checking app behavior against app descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tavecchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Software Engineering, ICSE 2014</title>
		<meeting>the 36th International Conference on Software Engineering, ICSE 2014</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How do users like this feature? a fine grained sentiment analysis of app reviews</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Requirements Engineering Conference (RE)</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supporting domain analysis through mining and recommending features from online product listings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hariri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castro-Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirakhorli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mobasher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1736" to="1752" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">App store mining and analysis: Msr for app stores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Software Repositories (MSR), 2012 9th IEEE Working Conference on</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="108" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Retrieving and analyzing mobile apps feature requests from online reviews</title>
		<author>
			<persName><forename type="first">C</forename><surname>Iacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Software Repositories (MSR), 2013 10th IEEE Working Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Feature-oriented domain analysis (FODA) feasibility study</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Spencer</surname></persName>
		</author>
		<idno>CMU/SEI-90-TR-21</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Functionality-based clustering using short textual description: Helping users to find apps installed on their mobile device</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Lavid</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lulu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuflik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Intelligent User Interfaces, IUI &apos;13</title>
		<meeting>the 2013 International Conference on Intelligent User Interfaces, IUI &apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentence similarity based on semantic nets and corpus statistics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Bandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>O'shea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1138" to="1150" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the automatic classification of app reviews</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kurtanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nabil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stanik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Requirements Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="331" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bug report, feature request, or simply praise? on automatically classifying app reviews</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nabil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Requirements Engineering Conference (RE), 2015 IEEE 23rd International</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward data-driven requirements engineering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ruhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software: Special Issue on the Future of Software Engineering</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the socialness of software</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pagano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th DASC</title>
		<meeting>9th DASC</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">Foundations of Statistical Natural Language Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of app store analysis for software engineering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cowboys, ankle sprains, and keepers of quality: How is video game development different from software development?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Murphy-Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Software Engineering</title>
		<meeting>the 36th International Conference on Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Trade-off service portfolio planning-a case study on mining the android app market</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ruhe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="page">1671</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">User involvement in software evolution practice: A case study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brügge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th ICSE</title>
		<meeting>the 35th ICSE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">User feedback in the appstore: An empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st IEEE International Requirements Engineering Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature lifecycles as they spread, migrate, remain, and die in app stores</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Al-Subaihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 23rd International Requirements Engineering Conference (RE)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Release planning of mobile apps based on user reviews</title>
		<author>
			<persName><forename type="first">L</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Penta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Software Engineering, ICSE &apos;16</title>
		<meeting>the 38th International Conference on Software Engineering, ICSE &apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Software Requirements (Developer Best Practices)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beatty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Microsoft Press</publisher>
		</imprint>
	</monogr>
	<note>3rd edition edition</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining and recommending software features across multiple web repositories</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th</title>
		<meeting>the 5th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Asia-Pacific Symposium on Internetware</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2013">2013</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
