<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCAttNet: Semantic Segmentation Network with Spatial and Channel Attention Mechanism for High-Resolution Remote Sensing Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Haifeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaijian</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Liang</roleName><forename type="first">Xiaoming</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">with the School of Geosciences and Info-Physics</orgName>
								<orgName type="institution">Central South University</orgName>
								<address>
									<postCode>410083</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SCAttNet: Semantic Segmentation Network with Spatial and Channel Attention Mechanism for High-Resolution Remote Sensing Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Remote Sensing</term>
					<term>Semantic Segmentation</term>
					<term>Convolutional Neural Network</term>
					<term>Attention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-resolution remote sensing images (HRRSIs) contain substantial ground object information, such as texture, shape, and spatial location. Semantic segmentation, which is an important method for element extraction, has been widely used in processing mass HRRSIs. However, HRRSIs often exhibit large intraclass variance and small interclass variance due to the diversity and complexity of ground objects, thereby bringing great challenges to a semantic segmentation task. In this study, we propose a new end-to-end semantic segmentation network, which integrates two lightweight attention mechanisms that can refine features adaptively. We compare our method with several previous advanced networks on the ISPRS Vaihingen and Potsdam datasets. Experimental results show that our method can achieve better semantic segmentation results compared with other works. The source codes are available at https://github.com/lehaifeng/SCAttNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE Semantic segmentation of remote sensing images is a fundamental task that classifies each pixel in an image into a specified category. It plays an important role in many fields such as change detection, element extraction, and military target recognition.</p><p>Image semantic segmentation methods can be divided into two categories: traditional methods and deep learning based ones. Traditional methods use the color, texture, shape, and spatial position relationships of an object to extract features and then use clustering, classification, and threshold algorithms to segment an image <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b1">[2]</ref>. However, these methods depend heavily on artificial design features and show some bottlenecks. Recently, deep learning based methods have been regarded as a promising approach to solve image semantic segmentation problems <ref type="bibr" target="#b2">[3]</ref>[4] <ref type="bibr" target="#b4">[5]</ref>. For example, methods based on fully convolutional network (FCN) <ref type="bibr" target="#b3">[4]</ref> have achieved stateof-the-art segmentation results on many natural image datasets, such as PASCAL VOC <ref type="bibr" target="#b5">[6]</ref> and Cityscapes <ref type="bibr" target="#b6">[7]</ref>.</p><p>However, remote sensing images are different from natural images and often viewed from a high-altitude angle. Thus, the range of imaging is wide, and the background is complex and diverse. Especially in HRRSIs, the difference of ground objects becomes further notable. To segment HRRSIs effectively, Chen et al. <ref type="bibr" target="#b7">[8]</ref> used a shuffle convolution neural network and found that the method is effective in segmenting small objects. Liu et al. <ref type="bibr" target="#b8">[9]</ref> designed an hourglass-shaped network (HSN) network based on inception network and skip connection and the network achieved better results compared with the reference networks on the ISPRS Vaihingen and ISPRS Potsdam datasets. Guo et al. <ref type="bibr" target="#b9">[10]</ref> learned from the spatial pyramid pooling model to capture multiscale features in HRRSIs, and then used conditional random fields to perform postclassification refinement. Kampffmeyer et al. <ref type="bibr" target="#b10">[11]</ref> used a weighted loss function to solve the problem of category imbalance. Chen et al. <ref type="bibr" target="#b11">[12]</ref> designed two deep networks based on residual module, and achieved better results compared with FCN-8s <ref type="bibr" target="#b3">[4]</ref> and SegNet <ref type="bibr" target="#b4">[5]</ref>. In summary, these networks mainly change the depth and width of the network, or combine multimodal data such as digital surface models (DSMs) to improve the accuracy of the network.</p><p>Recently, attention mechanisms have been successfully applied to semantic segmentation. These methods can be divided into two categories. One uses attention mechanism to select meaningful features at channel axis. For example, PAN <ref type="bibr" target="#b12">[13]</ref> uses a global attention mechanism as a context prior to select precise features channel-wisely. Attention U-net <ref type="bibr" target="#b13">[14]</ref> uses a channel attention mechanism to control the fusion of high-level and low-level features at channel axis. However, these attention mechanisms do not consider to enhance the feature representation at spatial axis. Another one is called the self attention mechanism, which calculates the feature representation in each position by weighted sum the features of all other positions <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b15">[16]</ref>. Thus, it can model the longrange context information for semantic segmentation task. For example, DANet <ref type="bibr" target="#b14">[15]</ref> uses two self-attention mechanisms to model long-range context information at channel and spatial axes respectively. However, these methods are complexity in the size of model, thus yield inefficient computation which will be great challenge to process massive remote sensing images. In our work, two lightweight attention mechanisms <ref type="bibr" target="#b16">[17]</ref> which contains spatial attention and channel attention are adopted for the semantic segmentation of HRRSIs. The spatial attention mechanism decides where to enhance, and the channel attention decides what to enhance. Integrating these two attention mechanisms can effectively improve the accuracy of semantic segmentation. The contributions of this study primarily include the following three points:</p><p>(1) A new semantic segmentation network that integrates a spatial attention mechanism and a channel attention mechanism are proposed to improve the semantic segmentation  (2) The working principle of the attention mechanism is analyzed through visualization to explain why our method works.</p><p>(3) Experiments reveal that the attention mechanism can focus on valuable areas and thus our method shows remarkable performance on small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview of SCAttNet</head><p>The proposed semantic segmentation network is shown in Fig. <ref type="figure" target="#fig_1">1</ref>. It consists of two parts: a backbone for feature extraction and an attention module. The attention module is composed of a channel attention and a spatial attention in cascade. For an input remote sensing image, we first use the backbone network for feature extraction. Then we feed the extracted feature map into the channel attention mechanism to refine the features in channels. Afterward, we feed the refined channel feature map into the spatial attention model for refinement in space. Lastly, we can obtain the semantic segmentation results via convolution and SoftMax operations. Specific network details and design ideas are shown in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention Mechanism</head><p>Channel Attention: Given a high-resolution remote sensing image, it will produce a multichannel feature map F ∈ R C×H×W (where C, H, and W denote the number of channels, the height, the width of the feature map, respectively) after passing through several convolutional layers. The information expressed in the feature map of each channel is different. Channel attention aims to use the relationships between each channel of the feature map to learn a 1D weight W c ∈ R C×1×1 , and then multiply it to the corresponding channel. In this manner, it can pay more attention to the meaningful semantic information for the current task. To learn effective weight representation, we first aggregate spatial dimension information through global average pooling and global max pooling to generate two feature descriptors for each channel. Then we feed the two feature descriptors into a shared multilayer perceptron with one hidden layer (where the number of the hidden layer units is C/8) to generate more representative feature vectors. Afterward, we merge the output feature vectors through an element-wise summation operation. Finally, using a sigmoid function, we can obtain the final channel attention map. The flow chart is illustrated in the channel attention module of Fig. <ref type="figure" target="#fig_1">1</ref>. The formula for calculating channel attention is shown in Formula 1:</p><formula xml:id="formula_0">Wc(F ) = sigmoid(M LP (AvgP ool(F )) + M LP (M axpool(F ))) (1)</formula><p>Spatial Attention: For a spatial attention, it focuses on where are valuable for current tasks. In HRRSIs, the ground objects are exhibited in various sizes and the distribution is complicated. Therefore, using spatial attention is useful for aggregating spatial information, especially for small ground objects. Spatial attention utilizes the relationships between different spatial positions to learn a 2D spatial weight map W s and then multiplies it to the corresponding spatial position to learn more representative features. To learn the spatial weight relationships effectively, we first generate two feature descriptors for each spatial position through global average pooling and global max pooling operations. Then we concentrate two feature descriptors together and generate a spatial attention map through a 7 × 7 convolution operation. Lastly, we use a sigmoid function to scale the spatial attention map to 0 ∼ 1. The flow chart is illustrated in the spatial attention module of Fig. <ref type="figure" target="#fig_1">1</ref>. The spatial attention calculation formula is shown in Formula 2.</p><formula xml:id="formula_1">Ws(F ) = sigmoid(f 7×7 ([Avgpool(F ); M axpool(F )]))<label>(2)</label></formula><p>where f 7×7 represents a convolution operation with 7 × 7 kernel size.</p><p>In this study, we follow the method of Woo <ref type="bibr" target="#b16">[17]</ref>to integrate the two attention mechanisms. First, we use channel attention to capture good semantic information, and then use spatial attention to capture spatial location information, which is conducive to extracting more valuable feature expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Backbone for feature extraction</head><p>In this study, we use two representative backbones for feature extraction: SegNet and ResNet50 <ref type="bibr" target="#b17">[18]</ref>. On this basis, we proposed two networks: SCAttNet V1 with SegNet as backbone and SCAttNet V2 with ResNet50 as backbone. For SegNet network, it has been widely used as the baseline model of remote sensing images semantic segmentation and achieved sound semantic segmentation results. For example, Audebert et al. <ref type="bibr" target="#b18">[19]</ref> based on SegNet network, combined with multimodal data, achieved state-of-the-art semantic segmentation results on the ISPRS Vaihingen dataset. And ResNet50 is also a common backbone for semantic segmentation tasks because it can build a deep-layer model with a wide receptive field. We use eight times downsampling for ResNet50 in our work. Unlike the SegNet and ResNet50 networks, we do not directly use the features of the last layer for semantic inference, instead, we feed the feature map of the last layer into the attention module for feature refinement and then make semantic inference, which is conducive to learning better feature expression.</p><p>We can easily compute that the number of parameters of the attention modules are so small that they can be neglected for both our SCAttNet V1 and SCAttNet V2, which have tens of millions of parameters. Specifically, the number of parameters of the channel attention mechanism is C×C/4(C is the number of channels) and the and the number of parameters of the spatial attention mechanism is 98. In addition, we only add the attention mechanism at the end layer of the backbone network; thus, it seldom brings computation complexity to our proposed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT</head><p>In this section, we evaluate the performance of our network on the ISPRS Vaihingen and ISPRS Potsdam datasets 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and evaluation metrics</head><p>The ISPRS Vaihingen dataset contains 33 orthophoto maps and related DSMs. Sixteen of them are labeled. The average size of the images is 2494 × 2064 and the resolution is 9 cm. Each image contains three bands: near infrared, red and green bands. Moreover, it includes six categories: impervious surfaces, buildings, low vegetation, trees, cars and clutter/background. The ISPRS Potsdam dataset contains 38 orthophoto maps and related normalized DSMs. Twenty-four of them are labeled. The size of each image is 6000 × 6000 and the resolution is 5 cm. Each image contains four bands: near infrared, red, green and blue bands. It has the similar categories as the ISPRS Vaihingen dataset.</p><p>To evaluate our proposed model, we use three evaluation metrics including mean inter-section over union (MIoU), average F1-score (AF) and overall accuracy (OA) to evaluate semantic segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>Considering that many remote sensing datasets in practice do not have DSMs, we do not use such datasets in this experiment for wide application value. For the ISPRS Vaihingen 1 ISPRS 2d semantic labeling dataset.http://www2.isprs.org/ commissions/comm3/wg4/semantic-labeling.html dataset, we divide the labeled dataset into two parts, in which (ID 30, 32, 34, 37) are for evaluating the performance of the network, and the remaining 12 images are for training. The number of Vaihingen dataset is small; thus, to prevent overfitting, we first crop the training dataset randomly into a 256 × 256 size and then expand the data through rotation and translation operations, and finally obtain 12,000 patches for training. The Potsdam dataset is also divided into two parts, in which (ID 2 12, 3 12, 4 12, 5 12, 6 12, 7 12) are for testing, and the remaining 18 images are for training. Then, we crop it randomly and obtain 27,000 256 × 256 patches for training. We trained all the model from the scratch without belts and whistles. The models including FCN-32s, FCN-8s, U-net, SegNet and G-FRNet all use VGG-16 as backbone. As for RefineNet and DeepLabv3+, we adopt the ResNet50 as backbone with 32 times downsampling for RefineNet and eight times downsampling for DeepLabv3+ as the original paper. We also adopt ResNet50 as backbone with eight times downsampling for CBAM. To train the proposed SCAttNet v1, we set the learning rate of SCAttNet V1 on the Vaihingen and Potsdam datasets to 1e-3 and 1e-4, respectively. To train SCAttNet V2, we set the learning rate of SCAttNet V2 on both datasets to 1e-3. The proposed models all adopt Adam as the optimizer and cross-entropy as the loss function. And the training epochs is set to 50. Considering limited computing resources, the batch size is set to 16. To test all the above models, we only use a sliding window without overlap to crop the images and then stitch together. We conduct all our experiments in Tensorflow platform with a NVIDIA 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Vaihingen dataset</head><p>Table I reports the semantic segmentation of the ISPRS Vaihingen dataset. We adopt the practice of <ref type="bibr" target="#b8">[9]</ref>[10] <ref type="bibr" target="#b10">[11]</ref> and do not report the accuracy of the clutter/background class because the Vaihingen dataset has less clutter/background. From the experimental results in Table <ref type="table">1</ref>, the MIoU/AF/OA increased by 1.4%/1.25%/0.56% after incorporating the channel attention module, compared with original SegNet. And it achieves competitive results as the SegNet network after incorporating spatial attention module, but the performance has an obvious increase on small objects such as cars. With the combination of channel and spatial attention mechanisms, the MIoU and AF increased by 2.9% and 2.5% compared with the original Seg-Net. Moreover, in SCAttNet V2, the MIoU/AF/OA increased by 1.21%/0.83%/0.90% compared with ResNet50 which as a baseline model. In addition, we find that in FCN-32s, the car category shows a low accuracy, but the car category in FCN-8s has improved much compared with FCN-32s, which shows the importance of low-level features for segmenting small objects.</p><p>To compare the semantic segmentation results, we visualize the semantic segmentation results of ID32. The visualization results are shown in the first row of Fig. <ref type="figure" target="#fig_2">2</ref>. We can see that our SCAttNet V1 achieves more coherent results compared with the original SegNet model. In addition, our model infers more accurately in small object areas such as cars in the right middle of the image. We also test the accuracy of the ID32 area. The MIoU/AF /OA of SegNet is 60.57%/74.01%/82.78%, whereas that of our SCAttNet V1 is 64.92%/77.74%/85.49%. On this basis, our network can improve semantic segmentation accuracy by incorporating the attention mechanism effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Potsdam dataset</head><p>Table II reports the semantic segmentation results of the IS-PRS Potsdam dataset. From the semantic segmentation results, the proposed SCAttNet V2 model is superior to the comparative models. Moreover, it achieves 1.12%/1.08%/1.03% higher in MIoU/AF/OA metrics compared with ResNet50 which as a baseline model. In SCAttNet V1, The MIoU/AF/OA has increased by 2.08%, 1.74% and 1.35%, respectively compared with SegNet which as a baseline model. Thus, we further confirm the effectiveness of the attention mechanism.</p><p>The second row of Fig. <ref type="figure" target="#fig_2">2</ref> shows the visualization results of the ID 5 12 area of the ISPRS Potsdam dataset. Compared with the comparative model, our proposed network achieves improved segmentation results in the building area, especially in terms of avoiding background interference. We also test the semantic segmentation performance in this area. The MIoU/AF/OA of the SegNet network is 57.19%/68.87%/83.81%, whereas that of SCAttNet V1 network is 59.50%/71.16%/85.04%. We find that our network achieves higher accuracy in all three metrics compared with the comparative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative analysis of improved interpretability</head><p>To analyze the enhancement of attention mechanism to the network, we visualize the feature expression of the SegNet network and our SCAttNet V1 network on the ISPRS Vaihingen dataset. We can simply overlay the heatmap of the network before SoftMax operation with an original image, and the relevant areas can be highlighted for a specific category. The visualization results are shown in Fig. <ref type="figure" target="#fig_3">3</ref>. After incorporating the attention mechanism, our network can focus on the areas of target and suppress the influence of other categories better than before. As shown in the first column of Fig. <ref type="figure" target="#fig_3">3</ref>, after incorporating the attention mechanism, our network only has a strong response in the car category and other areas are cold toned, which means that the relevance in other areas is very low. However, in the original SegNet network, the impervious area also shows a certain response that may cause interference for the car category. In addition, as shown in the building category in the third column of Fig. <ref type="figure" target="#fig_3">3</ref> , the impervious surface area is suppressed after incorporating the attention mechanism. Thus, the impervious surface area avoids classifying into buildings.</p><p>IV. CONCLUSION In this study, we propose a new semantic segmentation network that can adaptively refine features based on the attention mechanism. Experiments on the ISPRS Vaihingen and Potsdam datasets demonstrate the effectiveness of our method. However, some shortcomings remain. The study only uses two common attention mechanisms. Thus, how to design the attention mechanism effectively and capture more discriminative features for semantic inference remains a promising direction for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:1912.09121v1 [cs.CV] 19 Dec 2019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our proposed SCAttNet network</figDesc><graphic url="image-1.png" coords="2,108.00,44.73,396.02,156.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization results of the Vaihingen and Potsdam datasets. The first row presents the visualization results of the ISPRS Vaihingen dataset, from left to right: original image, SegNet output results, SCAttNet V1 output results and ground truth. The second row presents the visualization results of the ISPRS Potsdam dataset, from left to right: original image, SegNet output results, SCAttNet V1 output results and ground truth.</figDesc><graphic url="image-2.png" coords="4,311.98,375.66,252.01,146.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization analysis results of Vaihingen dataset. From left to right: the first two columns represent car categories, the second two columns represent building category, the third two columns represent impervious surfaces category, the fourth two columns represent low vegetation category, the fifth two columns represent tree category.</figDesc><graphic url="image-3.png" coords="5,90.00,44.73,431.99,133.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SEMANTIC</head><label>I</label><figDesc>SEGMENTATION RESULTS OF ISPRS VAIHINGEN DATASET. THE ACCUARCY OF EACH CATEGORY IS PRESENTED IN THE IOU/F1-SCORE FORM.</figDesc><table><row><cell>Model</cell><cell>Imp. Surfaces</cell><cell>Building</cell><cell>Low veg.</cell><cell>Tree</cell><cell>Car</cell><cell cols="3">MIoU (%) AF(%) OA(%)</cell></row><row><cell>FCN-32s [4]</cell><cell>68.31/81.17</cell><cell cols="3">70.59/82.76 57.22/72.79 60.95/75.74</cell><cell>8.16/15.09</cell><cell>53.04</cell><cell>65.51</cell><cell>78.01</cell></row><row><cell>FCN-8s [4]</cell><cell>71.32/83.26</cell><cell cols="4">71.64/83.48 63.67/77.80 65.78/79.36 44.73/61.81</cell><cell>63.43</cell><cell>77.14</cell><cell>80.99</cell></row><row><cell>U-net [3]</cell><cell>79.48/88.57</cell><cell cols="4">81.91/90.06 65.03/78.81 66.78/80.08 52.36/68.74</cell><cell>69.11</cell><cell>81.25</cell><cell>84.88</cell></row><row><cell>SegNet [5]</cell><cell>75.73/86.19</cell><cell cols="4">79.09/88.32 62.83/77.17 65.42/79.09 37.22/54.24</cell><cell>64.06</cell><cell>77.00</cell><cell>82.92</cell></row><row><cell>SegNet + cha. att</cell><cell>76.74/86.84</cell><cell cols="4">80.46/89.17 63.31/77.53 65.13/78.89 41.67/58.83</cell><cell>65.46</cell><cell>78.25</cell><cell>83.48</cell></row><row><cell>SegNet + spa. att</cell><cell>74.46/85.36</cell><cell cols="4">77.01/87.01 63.37/77.58 65.08/78.85 40.30/57.45</cell><cell>64.05</cell><cell>77.25</cell><cell>82.38</cell></row><row><cell>SCAttNet V1(ours)</cell><cell>77.75/87.36</cell><cell cols="4">81.05/89.54 63.00/77.30 65.51/79.16 47.71/64.60</cell><cell>66.96</cell><cell>79.59</cell><cell>83.79</cell></row><row><cell>ResNet50</cell><cell>78.46/87.93</cell><cell cols="4">80.92/89.46 65.99/79.51 65.84/79.41 53.68/69.86</cell><cell>68.99</cell><cell>81.23</cell><cell>84.57</cell></row><row><cell>CBAM [17]</cell><cell>78.36/87.86</cell><cell>80.56/89.23</cell><cell cols="2">66.97/80.22 68.91/81.60</cell><cell>52.52/68.87</cell><cell>69.46</cell><cell>81.56</cell><cell>84.96</cell></row><row><cell>RefineNet [20]</cell><cell>77.42/87.27</cell><cell cols="4">76.83/86.90 64.40/78.34 66.16/79.63 61.15/75.89</cell><cell>69.19</cell><cell>81.61</cell><cell>83.36</cell></row><row><cell>DeepLabv3+ [21]</cell><cell>79.43/88.54</cell><cell cols="4">81.73/89.95 66.88/80.15 66.73/80.05 52.57/68.91</cell><cell>69.47</cell><cell>81.52</cell><cell>85.15</cell></row><row><cell>G-FRNet [22]</cell><cell>80.09/88.94</cell><cell>82.60/90.47</cell><cell cols="3">66.85/80.13 67.31/80.46 56.99/72.60</cell><cell>70.77</cell><cell>82.52</cell><cell>85.52</cell></row><row><cell>SCAttNet V2(ours)</cell><cell>80.40/89.13</cell><cell cols="4">82.32/90.30 66.73/80.04 67.09/80.31 54.44/70.50</cell><cell>70.20</cell><cell>82.06</cell><cell>85.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>SEMANTIC</head><label></label><figDesc>SEGMENTATION RESULTS OF ISPRS POTSDAM DATASET. THE ACCUARCY OF EACH CATEGORY IS PRESENTED IN THE IOU/F1-SCORE FORM.</figDesc><table><row><cell>Model</cell><cell>Imp. Surfaces</cell><cell>Building</cell><cell>Low Veg.</cell><cell>Tree</cell><cell>Car</cell><cell cols="3">Clutter/background MIoU(%) AF (%) OA(%)</cell></row><row><cell>FCN-32s [4]</cell><cell>64.28/78.26</cell><cell cols="4">71.78/83.57 55.21/71.15 49.39/66.12 58.83/74.08</cell><cell>9.58/17.49</cell><cell>51.51</cell><cell>65.11</cell><cell>75.94</cell></row><row><cell>FCN-8s [4]</cell><cell>66.81/80.10</cell><cell cols="4">74.17/85.17 57.19/72.77 51.57/68.05 69.02/81.67</cell><cell>13.11/23.18</cell><cell>55.31</cell><cell>68.49</cell><cell>77.82</cell></row><row><cell>U-net [3]</cell><cell>65.29/79.00</cell><cell cols="4">73.64/84.82 65.11/78.87 59.88/74.91 77.72/87.47</cell><cell>14.97/26.04</cell><cell>59.44</cell><cell>71.85</cell><cell>79.84</cell></row><row><cell>SegNet [5]</cell><cell>68.17/81.07</cell><cell cols="4">76.07/86.41 63.91/77.98 58.54/73.85 75.01/85.72</cell><cell>13.41/23.65</cell><cell>59.18</cell><cell>71.45</cell><cell>80.27</cell></row><row><cell>SCAttNet V1(ours)</cell><cell>69.51/82.01</cell><cell cols="4">77.40/87.26 66.72/80.03 62.50/76.92 76.20/86.49</cell><cell>15.22/26.42</cell><cell>61.26</cell><cell>73.19</cell><cell>81.62</cell></row><row><cell>ResNet50</cell><cell>81.25/89.65</cell><cell cols="3">87.39/93.27 71.13/83.13 65.41/79.09</cell><cell>80.58/89.24</cell><cell>17.39/29.62</cell><cell>67.19</cell><cell>77.34</cell><cell>86.94</cell></row><row><cell>CBAM [17]</cell><cell>79.52/88.60</cell><cell cols="4">87.03/93.07 68.96/81.63 65.11/78.87 79.86/88.81</cell><cell>18.84/31.71</cell><cell>66.56</cell><cell>77.11</cell><cell>86.24</cell></row><row><cell>RefineNet[20]</cell><cell>77.91/87.58</cell><cell cols="4">79.37/88.50 69.36/81.91 65.38/79.07 78.41/87.90</cell><cell>16.27/27.98</cell><cell>64.45</cell><cell>75.49</cell><cell>84.38</cell></row><row><cell>DeepLabv3+ [21]</cell><cell>80.70/89.31</cell><cell cols="4">86.59/92.81 71.48/83.37 64.47/78.40 78.96/88.24</cell><cell>18.74/31.56</cell><cell>66.82</cell><cell>77.28</cell><cell>86.76</cell></row><row><cell>G-FRNet [22]</cell><cell>80.50/89.20</cell><cell cols="4">86.38/92.69 70.71/82.84 65.29/79.00 75.88/86.28</cell><cell>18.05/30.58</cell><cell>66.14</cell><cell>76.77</cell><cell>86.84</cell></row><row><cell>SCAttNet V2(ours)</cell><cell>81.83/90.04</cell><cell cols="3">88.76/94.05 72.49/84.05 66.33/79.75</cell><cell>80.28/89.06</cell><cell>20.18/33.58</cell><cell>68.31</cell><cell>78.42</cell><cell>87.97</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the National Natural Science Foundation of China [grant numbers 41571397, 41501442, and 51678077].</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Information fusion of aerial images and lidar data in urban areas: vector-stacking, re-classification and post-processing approaches</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Layered object models for image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1731" to="1743" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic segmentation of aerial images with shuffling convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="177" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hourglass-shapenetwork based semantic segmentation for high resolution aerial imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Deligiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munteanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">522</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pixel-wise classification method for high resolution remote sensing imagery using deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS International Journal of Geo-Information</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
				<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Symmetrical dense-shortcut deep fully convolutional networks for semantic segmentation of very-high-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1633" to="1644" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10180</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention u-net: Learning where to look for the pancreas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic segmentation of earth observation data using multimodal and multi-scale deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="180" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3751" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
