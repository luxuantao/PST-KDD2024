<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continual Training of Language Models for Few-Shot Learning</title>
				<funder>
					<orgName type="full">National Science Foundation (NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-11">11 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Chicago</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haowei</forename><surname>Lin</surname></persName>
							<email>linhaowei@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
							<email>shaoyj@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Chicago</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
							<email>leishu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Chicago</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Now at Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>liub@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Chicago</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Continual Training of Language Models for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-11">11 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.05549v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual Post-Training), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work has shown that large LMs have the ability to perform few-shot (or even zero-shot) learning well <ref type="bibr">(Brown et al., 2020b;</ref><ref type="bibr">Rae et al., 2021;</ref><ref type="bibr" target="#b52">Smith et al., 2022)</ref>. Post-training (a.k.a., domain-adaptive pre-training or pre-finetuning) an LM with a large unlabeled domain corpus before end-task fine-tuning in the domain achieves better results <ref type="bibr" target="#b59">(Xu et al., 2019;</ref><ref type="bibr">Gururangan et al., 2020a)</ref> than directly fine-tuning the LM. This paper goes a step further to study the problem of improving an LM's ability to handle new and ever emerging domains. For this, one needs to continually posttrain the LM with a sequence of domains. A key issue associated with this problem is catastrophic forgetting (CF). <ref type="foot" target="#foot_1">2</ref> This paper thus investigates how to continually extend the LM's knowledge without suffering from CF. From a broader perspective, since training a large LM from scratch is extremely expensive and computation intensive, incrementally updating the LM with the latest language data reflecting the ever changing development of the language itself, social events and the knowledge from different fields is becoming more and more critical. As humans are very effective at incremental learning, if we can imitate this human capability with little or no forgetting, we will be pushing the AI research forward significantly.</p><p>The proposed system, called CPT, is a continual learning (CL) system for post-training. Starting from a pre-trained LM (e.g., RoBERTa <ref type="bibr">(Liu et al., 2019b)</ref>), it incrementally post-trains the LM with a sequence of domains using their unlabeled corpora. Once a task (a domain in our case)<ref type="foot" target="#foot_2">3</ref> is trained, its data is no longer accessible. At any time, the resulting continually post-trained LM can be used by end-tasks in the trained domains. This is in the task-incremental learning (TIL) setting of CL, where the task id (domain id in our case) is provided when the learned model of a task needs to be used later (the use of domain id is discussed in Sec. 2.1). <ref type="foot" target="#foot_3">4</ref> This paper proposes an effective approach called CPT and focuses on the challenging and practical scenario of few-shot end-task learning after post-training a sequence of domains.</p><p>Continual post-training is different from conventional CL <ref type="bibr" target="#b5">(Chen and Liu, 2018)</ref>. The key difference is that in conventional CL, each task is an end-task, but in our case the end-task involves fine-tuning the continual post-trained LM (called p-LM). This causes major forgetting, which we call the catastrophic butterfly effect (CBE) and does not happen in conventional CL. Our proposed system, CPT, can solve both CF and CBE, based on a novel hard masking mechanism (Sec. 2.2) and can achieve no forgetting. As shown in Sec. 3.3, naively ap-plied existing CL systems cannot effectively prevent CF (even though some existing techniques have shown almost perfect CF prevention ability in conventional CL).</p><p>Experiments in 4 domains and their corresponding end-tasks demonstrate the effectiveness of the proposed CPT system. Related Work. Overcoming CF is a major goal of CL <ref type="bibr" target="#b5">(Chen and Liu, 2018)</ref>. There are many existing approaches, e.g., regularization-based approaches <ref type="bibr" target="#b30">(Kirkpatrick et al., 2016;</ref><ref type="bibr" target="#b48">Seff et al., 2017)</ref>, replay-based approaches <ref type="bibr" target="#b46">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b38">Lopez-Paz and Ranzato, 2017)</ref> and parameter isolation based approaches <ref type="bibr" target="#b49">(Serr? et al., 2018;</ref><ref type="bibr" target="#b8">Fernando et al., 2017)</ref>. Our CPT is based on parameter isolation and uses masks in continual post-training. Recently, CL has drawn attention in NLP. It has been used for slot filling <ref type="bibr" target="#b50">(Shen et al., 2019)</ref>, language learning <ref type="bibr" target="#b32">(Li et al., 2019)</ref>, sentence embedding <ref type="bibr">(Liu et al., 2019a)</ref>, translation <ref type="bibr" target="#b29">(Khayrallah et al., 2018)</ref>, cross-lingual modeling <ref type="bibr">(Liu et al., 2020b)</ref>, question answering <ref type="bibr" target="#b11">(Greco et al., 2019)</ref> and text classification <ref type="bibr">(Ke et al., 2021a,b;</ref><ref type="bibr" target="#b54">Sun et al., 2020;</ref><ref type="bibr" target="#b23">Huang et al., 2021;</ref><ref type="bibr" target="#b6">Chuang et al., 2020;</ref><ref type="bibr" target="#b42">Mehta et al., 2021;</ref><ref type="bibr" target="#b40">Madotto et al., 2020)</ref>. However, none of them tries to improve an LM.</p><p>CPT is closely related to ELLE <ref type="bibr" target="#b44">(Qin et al., 2022)</ref>, which does continual pre-training. The key difference is that ELLE starts from random initialization, while our CPT starts from a pre-trained LM. We tried to adapt ELLE for continual post-training by learning from a pre-trained RoBERTa but it fails to converge. This also indicates it is non-trivial to do well in our setting. Readers can refer to Appendix A for a full coverage of the related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed CPT System</head><p>CPT continually post-trains RoBERTa <ref type="bibr">(Liu et al., 2019b)</ref>. This is achieved by two continual learning plug-in (called CL-plugin) modules inserted into each transformer layer of RoBERTa. CLplugin is inspired by adapters in <ref type="bibr">(Houlsby et al., 2019)</ref>. While adapters can isolate different tasks, one needs to allocate a new adapter for each task and no knowledge can be shared among different tasks' adapters. The CL-plugin, however, is a CL system that learns a sequence of tasks with adapters shared by all domains. Figure <ref type="figure" target="#fig_0">1</ref> gives the CPT architecture with two CL-plugins added to RoBERTa.</p><p>Sequential vs. Parallel CL-plugin. Instead of following the original sequential adapter (Houlsby et al., <ref type="bibr" target="#b21">2019)</ref>, CL-plugin adopts the parallel adapter idea in <ref type="bibr" target="#b17">(He et al., 2021)</ref>. The difference is that the former inserts an adapter after the FFN/attention layer while the latter inserts it before FFN/attention layer (see Fig. <ref type="figure" target="#fig_0">1</ref>). We choose the parallel version as it performs better (see <ref type="bibr">Sec. 3.3)</ref>.</p><p>In post-training, only the two CL-plugins are trained. The components of the original pre-trained RoBERTa are fixed. In end-task fine-tuning, all components are trainable. A CL-plugin is a twolayer fully connected network with a task mask mechanism. It takes two inputs: (1) hidden states h (t) from the feed-forward layer in a transformer layer and (2) task ID t needed by task incremental learning (TIL). Inside a CL-plugin, task masks (TMs), which indicate task-specific neurons, are used to deal with CF. Since TMs is differentiable, the whole CPT can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Masks (TMs)</head><p>In each layer of a CL-plugin, task masks are used to protect those neurons that are important for pre-vious tasks to overcome CF. The masks basically forbid gradient updates to those neurons during backpropagation in learning a new task. Note that a task is also a domain in our case.</p><p>Learning a new task/domain consists of two main steps: (1) apply the mask in each layer for each old task to block off the gradient flow to protect the model for the old task, and (2) learn domain t and its masks for future use. We present (2) first.</p><p>Learning Task Masks for Overcoming CF. In learning each task t, a mask (a "soft" binary mask)</p><formula xml:id="formula_0">m (t)</formula><p>l is trained for the task at each layer l in CLplugin, indicating the neurons that are important for the task. We borrow the hard attention idea in <ref type="bibr" target="#b49">(Serr? et al., 2018)</ref> and leverage the task ID embedding to train the mask. For a task ID t, its embedding e (t) l consists of differentiable deterministic parameters that can be learned together with other parts of the network. To generate the task mask m</p><formula xml:id="formula_1">(t) l from e (t) l , Sigmoid is used as a pseudo-gate (mask) function. m (t) l is computed with m (t) l = ?(e (t) l /? ),<label>(1)</label></formula><p>where ? is a temperature variable, linearly annealed from 1 to ? min (a small positive value).</p><p>In the forward pass, given the output of each layer l, k (t) l , we element-wise multiply mask m</p><formula xml:id="formula_2">(t) l , o (t) l = k (t) l ? m (t) l .</formula><p>(2)</p><p>The masked output o (t) l of the last layer in CLplugin is fed to the next layer of the RoBERTa with a skip-connection. After learning task t, the final</p><formula xml:id="formula_3">m (t)</formula><p>l is saved and added to the set {m (t) l }. Applying Task Masks. Before learning a new task t, we first accumulate and set the masks m (iprev) l on the neurons in each layer l for all old tasks i prev so that in backpropagation, the gradient g</p><formula xml:id="formula_4">(t)</formula><p>l for task t will not flow to these neurons. Since m (iprev) l is pseudo binary, we use max-pooling to achieve the accumulation and condition the gradient:</p><formula xml:id="formula_5">g (t) l = g (t) l ? (1 -(MaxPool({m (iprev) l }))). (3)</formula><p>Those gradients corresponding to the 1 entries in MaxPool({m (iprev) l }) are set to 0 (to block off gradient flow) while the others remain unchanged. In this way, neurons in old tasks are protected. Note that we expand (copy) the vector m (ta) l to match the dimensions of g (t) l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Catastrophic Butterfly Effect in Fine-tuning</head><p>To perform an end-task in a post-trained domain, we fine-tune the mask-protected model of the domain, which is indicated by the task/domain id. The fine-tuning uses the corresponding domain neurons for the specific end-task by setting ? = ? min and condition the output via Eq. 2. With the masks, there should be no forgetting for continual posttraining and the end-task fine-tuning performance should be similar to post-train each domain separately. However, we found that this is not the case. <ref type="foot" target="#foot_4">5</ref>Our investigation found that the problem is due to the pseudo-gate function in Eq. 1. No matter how small ? is, Eq. 1 can only gives us a mask almost 0 (or 1). This causes the following: (1) During posttraining, the gradients for used neurons in Eq. 3 are not exactly 0 but a very small number.</p><p>(2) During fine-tuning, we cannot make use of the corresponding neurons for the specific end-task by simply setting ? = ? min . The small change in the neurons for old domains during post-training caused by ( <ref type="formula" target="#formula_1">1</ref>) is neglect-able in conventional CL because in conventional CL we evaluate the model using test sets and no weights update involved. However, in CPT, the end-task needs to fine-tune the continually post-trained LM model (p-LM), which involves weight updating. A small change to the p-LM during continual post-training can result in a different initialization for the end-task fine-tuning and give totally different fine-tuning results. We call this butterfly effect inspired by the term indicating a small state change in nature (e.g., the flap of a butterfly's wings in Brazil) can result in large differences in a later state (e.g., a tornado in Texas).</p><p>We propose a simple method to solve it, i.e., adding a threshold ? to the m (t) l to make it a hard binary mask,</p><formula xml:id="formula_6">m (t) l = 1, m (t) l &gt; ?, 0, m (t) l &lt; ?. (4)</formula><p>We then apply it to Eq. 3 in gradient manipulation and Eq. 2 in end-task fine-tuning. ? can be easily set (we use 0.5) since Eq. 1 already gives a pseudobinary mask. Note that this has almost no effect on post-training as it is used to block the backward pass gradient flow during post-training and select the corresponding neurons during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The proposed paradigm uses a different evaluation from that of conventional continual learning (CL). After unsupervised continual post-training of an LM (RoBERTa in our case) with a sequence of domains, the resulting p-LM is used to fine-tune an end few-shot classification task from any posttrained domain. There is no CL during end-task fine-tuning. Each fine-tuning task is done separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Baselines</head><p>Datasets: We use 4 unlabeled domain datasets: Yelp Restaurant <ref type="bibr" target="#b59">(Xu et al., 2019)</ref>, AI Papers <ref type="bibr" target="#b37">(Lo et al., 2020)</ref>, ACL Papers <ref type="bibr" target="#b37">(Lo et al., 2020)</ref> and AG-News <ref type="bibr" target="#b61">(Zhang et al., 2015)</ref> and their 4 corresponding end-task classification datasets. 6   Baselines. Since no existing method can perform our task, we use 6 non-CL and 7 adapted CL methods as our baselines. The non-CL baselines include (1) RoBERTa and (2) Adapter where we directly fine-tune the pre-trained model or adpater (without any post-training); (3) RoBERTa-ONE, (4) Adapter-ONE and (5) Prompt-ONE, where we build a model for each task using a separate network. It has no knowledge transfer or CF. (6) DEMIX <ref type="bibr" target="#b14">(Gururangan et al., 2021)</ref> trains a separate adapter for each task and initializes the adapter from its most similar previous task adapter. The 7 adapted CL baselines include (7) RoBERTa-NCL and (8) Adapter-NCL, where we post-train the domains one by one with no mechanism to deal with CF/transfer. Other are state-of-the-art CL baselines and we adapt them for continual post-training. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Architecture. We adopt RoBERTa BASE as our backbone LM. A masked language model head is applied for post-training. The fine-tuning follows the standard practice <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, where we pass the final layer &lt;/s&gt; token representation 6 These are popularly used in related works. Details of the datasets are given in Appendix C. We conduct experiments using few-shot learning end-tasks. Following <ref type="bibr" target="#b12">(Gu et al., 2021)</ref> to a task-specific feed-forward layer for prediction. The feed-forward layer with softmax output is used as the classification heads, together with the categorical cross-entropy loss. Note that for the aspect sentiment classification task (see Table <ref type="table" target="#tab_3">3</ref>), we adopt the ASC formulation in <ref type="bibr" target="#b59">(Xu et al., 2019)</ref>, where the aspect (e.g., "sound") and review sentence (e.g., "The sound is great") are concatenated via &lt;/s&gt;.</p><p>Hyperparameters. Unless otherwise stated, the same hyper-parameters are used in all experiments. We use 0.0025 for ? min in Eq. 1 and ? is set to 0.5 in Eq. 4 in the main paper. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, there are two CL-plugins for each Transformer layer (one at the bottom in parallel with attention and one at the top in parallel with FFN). We search the CL-plugin size within {128, 256, 512, 768, 1024} and adopt 512 for the bottom one and 768 for the top one based on validation experiments. The task id embeddings have the same size as the hidden layer dimension of the CL-plugin. The maximum input length is set to 164 which is long enough for all datasets. We use Adam optimizer and set the learning rate to 1e-4 for post-training and 5e-5 for fine-tuning. The batch size is set to 48 for post-training and 20 for fine-tuning. Since each of our domain-specific dataset has a different size, we train CPT on each task/domain for 1 epoch for post-training, which is approximately 13K steps, following <ref type="bibr">(Gururangan et al., 2020b;</ref><ref type="bibr" target="#b59">Xu et al., 2019)</ref>. We train on end-task fine-tuning datasets for 20 epochs and take the results for the last epoch, assuming no validation sets. We empirically found 20 epochs can give us a relatively stable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Results and Analysis</head><p>We report the average results of the 4 different finetuning tasks (or datasets) in accuracy and Macro-F1 after post-training on all unlabeled domain datasets in Table <ref type="table" target="#tab_1">1</ref>. The forgetting rate (forget R.) <ref type="bibr">(Liu et al., 2020a)</ref>  Comparing with CL baselines, CPT achieves no forgetting (we can see the forgetting rate is 0), indicating the high effectiveness of the proposed approach. We also note that CPT is even slightly better than those ONE baselines, indicating some positive knowledge transfer in CPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablations</head><p>In Table <ref type="table" target="#tab_2">2</ref>, we give the ablation results. We are interested in the following:</p><p>(1) Catastrophic butterfly effect (CBE). The third row with "w/o butterfly" shows results without the hard binary mask mechanism in Eq. 4. Clearly, the results are worse and the model suffers from forgetting. This indicates CBE and our approach is effective.</p><p>(2) Different Architecture. CPT is based on CL-plugin, which is inspired by adapters. Another popular way to use adapters is to make it sequential <ref type="bibr">(Houlsby et al., 2019)</ref>. Sequential adapter (first row) is clearly poorer than our current parallel one. This conforms to the observation in <ref type="bibr" target="#b17">(He et al., 2021)</ref>.</p><p>(3) Different Orders.</p><p>Table <ref type="table" target="#tab_1">1</ref> only reports the results of one fixed domain order (Restaurant?AI? ACL ?AGNews). We are interested in how the order impacts CPT results. We give the detailed results for all the other baselines and detailed domain orders in Appendix E. We can see the results of CPT does not change much and it </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposed to continually post-train an LM with a sequence of domains using their unlabeled domain corpora. An effective method (CPT) is also proposed. An end-task from any post-trained domain can fine-tune the resulting LM. Experimental results demonstrate the effectiveness of CPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>We list two limitations of CPT. First, CPT adds CLplugins for continual post-training with no change to the underlying LM in training. Although a CLplugin is small compared to an LM, it is still interesting and may be more effective to explore the idea of updating the LM directly without any additional modules. Second, domain ids are needed in both training and testing for CPT. In some applications, it may be hard to provide a domain id for each fine-tuning end-task. We will explore these in our future work as specializing and/or incrementally improving an LM is an important problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>Our work is related to continual learning, posttraining and few-shot learning.</p><p>Continual learning (CL). In general, overcoming CF is a major goal in CL <ref type="bibr" target="#b5">(Chen and Liu, 2018)</ref>. (1) Regularization methods <ref type="bibr" target="#b30">(Kirkpatrick et al., 2016;</ref><ref type="bibr" target="#b48">Seff et al., 2017)</ref> add a regularization to ensure minimal changes to weights for previous tasks. ( <ref type="formula">2</ref>) Replay methods retain <ref type="bibr" target="#b46">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b38">Lopez-Paz and Ranzato, 2017;</ref><ref type="bibr" target="#b57">Wang et al., 2020;</ref><ref type="bibr" target="#b13">Guo et al., 2022)</ref> or generate some data of old tasks <ref type="bibr" target="#b51">(Shin et al., 2017;</ref><ref type="bibr" target="#b18">He and Jaeger, 2018)</ref> and use them in learning a new task.</p><p>(3) Parameter isolation methods <ref type="bibr" target="#b49">(Serr? et al., 2018;</ref><ref type="bibr" target="#b8">Fernando et al., 2017)</ref> allocate parameters for different tasks and mask them out in learning a new task. Our CPT is based on (3) and uses masks in continual post-training. Recently, has drawn attention in NLP. It has been used for slot filling <ref type="bibr" target="#b50">(Shen et al., 2019)</ref>, language learning <ref type="bibr" target="#b32">(Li et al., 2019)</ref>, sentence embedding <ref type="bibr">(Liu et al., 2019a)</ref>, translation <ref type="bibr" target="#b29">(Khayrallah et al., 2018)</ref>, cross-lingual modeling <ref type="bibr">(Liu et al., 2020b)</ref>, question answering <ref type="bibr" target="#b11">(Greco et al., 2019)</ref> and text classification <ref type="bibr">(Ke et al., 2021a,b;</ref><ref type="bibr" target="#b54">Sun et al., 2020;</ref><ref type="bibr" target="#b23">Huang et al., 2021;</ref><ref type="bibr" target="#b6">Chuang et al., 2020;</ref><ref type="bibr" target="#b42">Mehta et al., 2021;</ref><ref type="bibr" target="#b40">Madotto et al., 2020)</ref>. However, none of them tries to improve an LM.</p><p>Post-training is an effective approach to mitigate the discrepancies between pre-trained domains and the target domain. Researchers have applied post-training to many domains, e.g., reviews <ref type="bibr" target="#b59">(Xu et al., 2019;</ref><ref type="bibr" target="#b53">Sun et al., 2019)</ref>, news and academic papers <ref type="bibr">(Gururangan et al., 2020b)</ref>, and shown improved end-task results. However, none of them consider the continual learning paradigm.</p><p>Few-shot learning (FL) aims to learn tasks with a few labeled examples. The main issue of FL is over-fitting, due to the scarcity of labeled training data. Existing methods to overcome over-fitting fall in three main families: (i) model-based methods try to reduce the hypothesis space of the few-shot task <ref type="bibr" target="#b55">(Triantafillou et al., 2017;</ref><ref type="bibr" target="#b22">Hu et al., 2018)</ref>, (ii) data-based methods try to augment additional data to the few-shot set <ref type="bibr" target="#b1">(Benaim and Wolf, 2018;</ref><ref type="bibr" target="#b10">Gao et al., 2020)</ref>, and (iii) algorithm-based solutions try to improve strategies for searching for the best hypothesis. Recently, a new paradigm using prompts achieves promising results for fewshot language learning as shown in GPT-3 <ref type="bibr">(Brown et al., 2020a)</ref>, PET <ref type="bibr" target="#b47">(Schick and Sch?tze, 2021)</ref> and LM-BFF <ref type="bibr" target="#b9">(Gao et al., 2021)</ref>. However, none of them does few-shot fine-tuning in continual posttraining.</p><p>Continual few-shot learning. Several researchers have studied this problem recently <ref type="bibr" target="#b0">(Antoniou et al., 2020;</ref><ref type="bibr" target="#b43">Qin and Joty, 2021;</ref><ref type="bibr" target="#b24">Jin et al., 2021;</ref><ref type="bibr" target="#b58">Xia et al., 2021;</ref><ref type="bibr" target="#b60">Yin et al., 2022)</ref>. It continually learns a sequence of few-shot tasks. However, this is very different from our continual posttraining because our continual learning happens in the post-training stage instead of the end-task finetuning stage. We only evaluate the proposed CPT system after continual post-training by conducting few-shot learning tasks individually by fine-tuning the post-trained language model (p-LM) in each of the post-trained domains. No continual learning is involved in few-shot learning. Before training domain/task 2, those useful neurons for domain 1 are first masked (those previous 1's entries are turned to 0's). After training domain 2, two neurons with 1 are used by the domain. When domain t arrives, all used neurons by domains 1 and 2 are masked before training, i.e., their entries are set to 0. After training domain t, we see that domains t and 1 have a shared neuron (the cell with two colors, red and green), which is used by both of domains. After continual post-training, we evaluate CPT by individual fine-tuning. During fine-tuning (Figure <ref type="figure" target="#fig_1">2</ref> (B)), we only make use of those neurons that are useful for domain/task id t (red cells) and freeze all other neurons (grey cells).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Illustration of Task Masks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset Statistics</head><p>Table <ref type="table" target="#tab_3">3</ref> shows the statistics of the unlabeled domain datasets and end-task classification datasets. Note that the full AGNews is very large. We use only its author provided training split as our domainspecific datasets as our unlabeled AGNews dataset for continual post-training. The remaining testing set is used as the labeled end-task (AGNews-FT). The other three corresponding end task datasets are SemEval-res <ref type="bibr" target="#b59">(Xu et al., 2019)</ref>, ACL-ARC <ref type="bibr" target="#b25">(Jurgens et al., 2018)</ref>, and SCIERC <ref type="bibr" target="#b39">(Luan et al., 2018)</ref>.  (1,2) RoBERTa, Adapter <ref type="bibr">(Liu et al., 2019b;</ref><ref type="bibr">Houlsby et al., 2019)</ref> use the original RoBERTa/Adapter for the end-task fine-tuning without any post-training. These are the only two without any post-training. All the following baselines use the masked language model loss (MLM) for post-training.</p><p>(3) RoBERTa-ONE is the existing post-training method in <ref type="bibr">(Gururangan et al., 2020b)</ref>. To our knowledge, the existing post-training systems are all based on the MLM loss.</p><p>(4) Adapter-ONE <ref type="bibr" target="#b40">(Madotto et al., 2020;</ref><ref type="bibr">Houlsby et al., 2019)</ref> adds small adapter layers between layers of Transformer for post-training. We follow the adapter design in <ref type="bibr" target="#b40">(Madotto et al., 2020;</ref><ref type="bibr">Houlsby et al., 2019)</ref>. An adapter is simply two fully connected layers. During post-training, the Transformer is fixed, only the added adapters are trainable. The bottleneck size (adapter size) is set to 128. During end-task fine-tuning, both RoBERTa and the adapters are trainable to ensure fair comparison.</p><p>(5) Prompt-ONE <ref type="bibr" target="#b31">(Lester et al., 2021)</ref> adds a sequence of real vector tokens (called virtual tokens or prompt tokens) to the end of the original input sequence. In post-training, RoBERTa (the LM) is fixed and only the prompt tokens are trained. In end-task fine-tuning, both LM and the trained prompt are trainable. We initialize 100 tokens and set the learning rate of the prompt token to 0.3 in   post-training, following the setting in <ref type="bibr" target="#b31">(Lester et al., 2021)</ref>. ( <ref type="formula">6</ref>) DEMIX <ref type="bibr" target="#b14">(Gururangan et al., 2021</ref>) is a recent model to adapt a pre-trained LM with new domains. It adds a new adapter once a new domain arrives (network expansion is needed) and initializes the new adapter with the parameters of the previous trained adapter nearest to the new domain data. They use the perplexity on held-out samples to choose the most probable adapter.</p><p>Continual Learning (CL) Baselines.</p><p>(7) RoBERTa-NCL (Naive continual learning) is a naive extension of <ref type="bibr">(Gururangan et al., 2020b)</ref>, which continually/incrementally posttrains the LM to learn all domains using the MLM loss with no mechanism to deal with forgetting or CF.</p><p>(8) Adapter-NCL <ref type="bibr">(Houlsby et al., 2019)</ref> is similar to the Adapter based system. The only difference is that the same set of adapters is shared across all domains, rather than using a new adapter for each new domain.</p><p>(9) Hard attention to overcome forgetting (HAT) is derived from HAT <ref type="bibr" target="#b49">(Serr? et al., 2018)</ref>, the state-of-the-art parameter-isolation based method with almost no forgetting. However, HAT suffers from forgetting in continual post-training due to the catastrophic butterfly effect.</p><p>(10) BCL <ref type="bibr">(Ke et al., 2021c</ref>) is a continual learning model that can avoid forgetting and encourage knowledge transfer. It is similar to Adapter-NCL. The difference is that its adapters consist of two modules, one is a capsule network (a new capsule is added once a new domain arrives) to encourage transfer, and the other is similar to HAT to avoid forgetting. Similar to HAT, task/domain information is needed in end-task fine-tuning. We replace the backbone network from BERT with RoBERTa for fair comparison.</p><p>(11) Knowledge distillation (KD) <ref type="bibr" target="#b19">(Hinton et al., 2015)</ref> minimizes the representational deviation between the learned representation and the new representation in post-training. We compute the KL divergence between the representations (the output before the masked language model prediction head) of each token of the previous post-trained LM and current LM as the distillation loss.</p><p>(12) EWC <ref type="bibr" target="#b4">(Buzzega et al., 2020</ref>) is a popular regularization-based continual learning method that adopts elastic weights consolidation to add L 2 regularization to penalize parameter changes.</p><p>(13) DER++ <ref type="bibr" target="#b4">(Buzzega et al., 2020</ref>) is a recent replay method using distillation to regularize the new task training. We store 16.4K tokens for each learned domain as the memory, which is the largest memory we can use for the system to run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results for Different Domain Orders</head><p>Table <ref type="table" target="#tab_1">1</ref> in the main paper reported the results for the order Restaurant ? AI ? ACL ? AGnews. We now look at how the order affects the results. Table <ref type="table">6</ref> shows baselines and CPT's results of 4 different orders. Note that the results for the Non-CL baselines are the same across different orders Table <ref type="table">6</ref>: CPT performance averaged over all domains after the final post-trained with different orders (averaged over 5 random seeds) and the average of these orders.</p><p>(and the same as those in Table <ref type="table" target="#tab_1">1</ref>) because they are not effected by orders. We can see CPT is always better than other baselines, and achieve 0 forgetting rate, demonstrating the effectiveness of CPT. We also note that some baselines in some sequence has negative forgetting rate, indicating they have some backward transfer (new domain learning helps learned domains). However, their final results are much worse than CPT's.  <ref type="table" target="#tab_1">1</ref> (in the main paper) of CPT and the considered baselines over 5 runs with random seeds. We can see the results of CPT are stable. Some baselines (e.g., RoBERTa, RoBERTa-ONE) can have quite large standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Standard Deviations</head><p>Table <ref type="table" target="#tab_6">5</ref> reports the standard deviations of the corresponding results in Table <ref type="table" target="#tab_2">2</ref> (in the main paper) of CPT and the considered baselines over 5 runs with random seeds. We can see the results of sequential adapters has a high variance while CPT and other variants are stable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of CPT, which has two CLplugins inserted in the transformer layers of RoBERTa in a parallel manner (FFN: feed-forward network). (A) CPT for continual post-training. It uses a masked language model (MLM) head for unsupervised posttraining of the CL-plugins only. (B) CPT for individual fine-tuning. CPT is evaluated by the corresponding individual end-task performance of all post-trained tasks. Each CL-plugin has numbers and colors indicating its masks and is illustrated in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 illustrates the CPT architecture and the task mask learning. Note that fine-tuning is for evaluating the domain post-training and should not affect any parameters of post-training. During continual post-training (Figure 2 (A)), after training domain/task 1, we obtain its useful neurons indicated by the 1 entries.Before training domain/task 2, those useful neurons for domain 1 are first masked (those previous 1's entries are turned to 0's). After training domain 2, two neurons with 1 are used by the domain. When domain t arrives, all used neurons by domains 1 and 2 are masked before training, i.e., their entries are set to 0. After training domain t, we see that domains t and 1 have a shared neuron (the cell with two colors, red and green), which is used by both of domains. After continual post-training, we evaluate CPT by individual fine-tuning. During fine-tuning (Figure2(B)), we only make use of those neurons that are useful for domain/task id t (red cells) and freeze all other neurons (grey cells).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of CPT, which has two CL-plugins inserted in the transformer layers of RoBERTa in a parallel manner. (A) CPT for continual post-training. It uses a masked language model (MLM) head for unsupervised post-training of the plugins only. (B) CPT for individual fine-tuning.The performance of CPT is evaluated by the corresponding individual end-task performance of all post-trained tasks using the final post-trained model (with different mask). Each CL-plugin module (to the right of the transformer) has two fully connected layers and a skip connection. On top of each fully connected layer, there is a mask computed from task ID t with the same size as the fully connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, we use 32 training samples for Restaurant and AGNews, 48 training samples for ACL and 56 training samples for AI due to different numbers of classes in each dataset.</figDesc><table /><note><p><p>7 </p>Readers can refer to Appendix D for the detailed of each of these baselines.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>is also reported. The higher the forgetting rate is, the more forgetting it has. Negative rates indicate positive knowledge transfer.8  Superiority of CPT. Clearly, CPT outperforms all baselines and achieves no forgetting. More specifically, CPT markedly outperforms the two End-task macro-F1 (MF1), accuracy and forgetting rate results for all domains after continual posttraining of all domains. The results are averages of 5 random seeds (the domain training order is as they appear in the first row). Due to space limits, the results for different domain orders and the standard deviations are reported in Appendix E and Appendix F, respectively). Non-CL baselines has no forgetting.</figDesc><table><row><cell>Category</cell><cell>Domain Model</cell><cell>Restaurant MF1 Acc</cell><cell>MF1</cell><cell>AI</cell><cell>Acc</cell><cell>ACL MF1 Acc</cell><cell>AGNews MF1 Acc</cell><cell cols="2">Average MF1 Acc MF1 Acc Forget R.</cell></row><row><cell></cell><cell>RoBERTa</cell><cell cols="7">50.61 74.77 27.88 28.44 32.19 34.59 64.19 65.95 43.72 50.94</cell><cell>-</cell></row><row><cell>Non-CL</cell><cell cols="8">Adapter RoBERTa-ONE 53.63 76.73 29.86 30.11 33.05 35.72 62.57 65.13 44.78 51.92 45.40 67.28 23.69 24.56 24.99 27.55 64.53 66.50 39.65 46.48 Adapter-ONE 52.19 74.20 30.80 31.59 36.59 36.99 61.66 63.94 45.31 51.68</cell><cell>---</cell></row><row><cell></cell><cell>Prompt-ONE</cell><cell cols="7">28.93 59.79 21.06 22.10 28.02 29.22 60.70 62.58 34.68 43.42</cell><cell>-</cell></row><row><cell></cell><cell>DEMIX</cell><cell cols="7">53.14 75.28 27.68 27.29 37.63 38.57 63.18 65.13 45.41 51.57</cell><cell>-</cell></row><row><cell></cell><cell cols="9">RoBERTa-NCL 42.59 67.56 31.57 31.62 33.07 34.54 60.18 63.50 41.85 49.30 3.27 2.82 Adapter-NCL 47.42 70.23 29.56 29.90 35.92 37.58 61.73 64.45 43.65 50.54 2.21 1.69</cell></row><row><cell></cell><cell>HAT</cell><cell cols="8">50.45 71.78 28.33 29.41 34.93 37.15 62.97 65.05 44.17 50.85 2.43 2.04</cell></row><row><cell>CL</cell><cell>BCL KD EWC</cell><cell cols="8">51.70 74.34 29.66 30.96 32.85 34.82 63.60 65.47 44.45 51.40 1.47 0.82 39.75 67.11 29.63 29.33 38.30 42.09 62.85 65.39 42.63 50.98 4.92 3.07 48.32 71.59 30.96 31.01 35.96 38.05 62.29 64.95 44.38 51.40 1.40 0.80</cell></row><row><cell></cell><cell>DER++</cell><cell cols="8">48.09 71.79 30.71 30.54 34.25 35.77 64.24 66.11 44.32 51.05 1.79 1.62</cell></row><row><cell></cell><cell>CPT</cell><cell cols="8">53.90 75.13 30.42 30.89 37.56 38.53 63.77 65.79 46.41 52.59 0.00 0.00</cell></row><row><cell cols="6">baselines without post-training (RoBERTa and</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Adapter), indicating CPT can learn new domains</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">well. These two baselines are also significantly</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">worse than other baselines, indicating that fine-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">tuning the pre-trained RoBERTa alone is weak.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiment results.</figDesc><table><row><cell>Model</cell><cell cols="2">Final Performance MF1 Acc</cell></row><row><cell cols="2">CPT (Sequential Adapter) 43.00</cell><cell>50.25</cell></row><row><cell>CPT (w/o butterfly)</cell><cell>44.17</cell><cell>50.85</cell></row><row><cell>CPT (w/o masking)</cell><cell>43.65</cell><cell>50.54</cell></row><row><cell>CPT</cell><cell>46.41</cell><cell>52.58</cell></row></table><note><p>still outperforms other baselines. This indicates the CPT's robustness to domain orders in post-training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics for unlabeled domain datasets and end task supervised classification datasets.</figDesc><table><row><cell cols="3">Unlabeled Domain Datasets</cell><cell></cell><cell cols="2">End-Task Classification Datasets</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Source</cell><cell>#training</cell><cell>Dataset</cell><cell>Task</cell><cell cols="3">#training #testing #classes</cell></row><row><cell cols="5">Yelp Restaurant Yelp Review 1,132,359 SemEval-res Aspect Sentiment Classification</cell><cell>32</cell><cell>1,120</cell><cell>3</cell></row><row><cell>AI</cell><cell>AI Papers</cell><cell>707,368</cell><cell>SCIERC</cell><cell>Relation Classification</cell><cell>56</cell><cell>2,388</cell><cell>7</cell></row><row><cell>ACL</cell><cell cols="2">ACL Papers 1,208,449</cell><cell>ACL-ARC</cell><cell>Citation Intent Classification</cell><cell>48</cell><cell>421</cell><cell>6</cell></row><row><cell>AGNews</cell><cell>News Article</cell><cell>73,750</cell><cell>AGNews-FT</cell><cell>News Classification</cell><cell>32</cell><cell>7,568</cell><cell>4</cell></row><row><cell cols="3">D Details of the CL baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Non-Continual Learning Baselines: Each of these baselines builds a separate model for each</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">task independently. It thus has no CF.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Standard deviations of the corresponding metrics of the proposed CPT system and the baselines.</figDesc><table><row><cell>Model</cell><cell>Final Performance MF1 Acc</cell></row><row><cell cols="2">CPT (Sequential Adapter) ?0.0347 ?0.0350 CPT (w/o butterfly) ?0.0102 ?0.0079 CPT (w/o masking) ?0.0095 ?0.0137 CPT ?0.0078 ?0.0112</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Standard deviations of the corresponding metrics of the proposed CPT system and the ablations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 4 reports the standard deviations of the corresponding results in Table</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/UIC-Liu-Lab/CPT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>CF means that learning a new task/domain may need to modify the existing network, which degrades the performance of previous tasks/domains<ref type="bibr" target="#b41">(McCloskey and Cohen, 1989)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We will use the term domain in this paper to be consistent with the post-training literature</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>CL has two other settings: class-incremental learning and domain-incremental learning (van de<ref type="bibr" target="#b56">Ven and Tolias, 2019)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>For example, fine-tuning an end restaurant sentiment classification task achieves macro-F1 (MF1) of 0.64 right after post-training the restaurant domain but its fine-tuning MF1 drops to 0.44 after post-training three more domains.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>Forgetting rate is computed as as follows(Liu et al.,  2020a), 1 t-1 t-1 i=1 Ai,i -At,i, where Ai,i is the end-task performance right after its domain i is post-trained, and At,i is the performance of the end-task of domain i after posttraining the last domain. We average over all end-tasks except the last one as the last domain has no forgetting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_6"><p>arXiv:2210.05549v1 [cs.CL] 11 Oct</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2022" xml:id="foot_7"><p></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work of <rs type="person">Zixuan Ke</rs> and <rs type="person">Bing Liu</rs> was supported in part by two <rs type="funder">National Science Foundation (NSF</rs>) grants (IIS-1910424 and IIS-1838770).</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Matters. This is addressed in row 2 of Table <ref type="table">2</ref>.</p><p>We added an ablation to show the importance to 061 address the butterfly effect.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Defining benchmarks for continual few-shot learning</title>
		<author>
			<persName><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One-shot unsupervised cross domain translation</title>
		<author>
			<persName><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint/>
	</monogr>
	<note>et al. 2020a</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dark experience for general continual learning: a strong, simple baseline</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Buzzega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Boschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07211</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02123</idno>
		<title level="m">Lifelong language knowledge distillation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
		</imprint>
	</monogr>
	<note>Event</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural snowball for few-shot relation learning</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Psycholinguistics meets continual learning: Measuring catastrophic forgetting in visual question answering</title>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04229</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ppt: Pre-trained prompt tuning for few-shot learning</title>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04332</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online continual learning through mutual information maximization</title>
		<author>
			<persName><forename type="first">Yiduo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Demix layers: Disentangling domains for modular language modeling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m">a. Don&apos;t stop pretraining: adapt language models to domains and tasks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2020b. Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic interference using conceptor-aided backpropagation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Few-shot charge prediction with discriminative legal attributes</title>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018</title>
		<meeting>the 27th International Conference on Computational Linguistics, COLING 2018<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20">2018. August 20-26, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Continual learning for text classification with information disentanglement based regularization</title>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05489</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learn continually, generalize rapidly: Lifelong knowledge accumulation for fewshot learning</title>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting><address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11">2021. 16-20 November, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Measuring the evolution of a scientific field through citation frames</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raine</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">2021a. Achieving forgetting prevention and knowledge transfer in continual learning</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<imprint>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classic: Continual and contrastive learning of aspect sentiment classification tasks</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adapting bert for continual learning of a sequence of aspect sentiment classification tasks</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized training objective for continued training for domain adaptation in neural machine translation</title>
		<author>
			<persName><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compositional language continual learning</title>
		<author>
			<persName><forename type="first">Yuanpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Sedoc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09187</idno>
		<title level="m">Continual learning for sentence representations using conceptors</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An-An Liu, Bernt Schiele, and Qianru Sun. 2020a. Mnemonics training: Multiclass incremental learning without forgetting</title>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Exploring fine-tuning techniques for pre-trained cross-lingual models via continual learning</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14218</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">S2ORC: the semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunjoon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15504</idno>
		<title level="m">Continual learning in task-oriented dialogue systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An empirical investigation of the role of pre-training in lifelong learning</title>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darshan</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML CL Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">LFPT5: A unified framework for lifelong few-shot language learning based on prompt tuning of T5</title>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ELLE: efficient lifelong pre-training for emerging data</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrayne</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><surname>Bennett</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Main Online</publisher>
			<date type="published" when="2021-04-19">2021. April 19 -23, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Continual learning in generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didac</forename><surname>Suris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A progressive model to enable continual learning for semantic slot filling</title>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elton</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How to fine-tune bert for text classification</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China national conference on Chinese computational linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lamol: Language modeling is all you need for lifelong language learning</title>
		<author>
			<persName><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Hao</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Few-shot learning through an information retrieval lens</title>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>De; Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-04">2017. 2017. cember 4-9, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName><surname>Tolias</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.07734" />
		<title level="m">Three scenarios for continual learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient meta lifelonglearning with limited memory</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Incremental few-shot text classification with multi-round new classes: Formulation, dataset and system</title>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06-06">2021. June 6-11, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Contintin: Continual learning from task instructions</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
