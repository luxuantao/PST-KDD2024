<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-26">26 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dimitri</forename><surname>Von R Ütte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Biggio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannic</forename><surname>Kilcher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitri</forename><surname>Von Rütte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-26">26 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.10936v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating music with deep neural networks has been an area of active research in recent years. While the quality of generated samples has been steadily increasing, most methods are only able to exert minimal control over the generated sequence, if any. We propose the self-supervised description-to-sequence task, which allows for fine-grained controllable generation on a global level by extracting high-level features about the target sequence and learning the conditional distribution of sequences given the corresponding highlevel description in a sequence-to-sequence modelling setup. We train FIGARO (FIne-grained music Generation via Attention-based, RObust control) by applying description-to-sequence modelling to symbolic music. By combining learned high level features with domain knowledge, which acts as a strong inductive bias, the model achieves state-of-the-art results in controllable symbolic music generation and generalizes well beyond the training distribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Music is a fascinating subject that surrounds us constantly, being a source of inspiration and canvas for imagination to many. To some, creating music is a topic worthy of dedicating one's life to, which is a testament to the artistry and mastery involved. While composition is an intricate form of art that requires a deep understanding of the human experience, the idea of devising a systematic or algorithmic approach to music creation has been around for centuries <ref type="bibr" target="#b24">(Nierhaus, 2009)</ref>.</p><p>With the advent of deep learning, automatic music generation has seen renewed interest <ref type="bibr" target="#b15">(Hernandez-Olivan &amp; Beltran, 2021)</ref>. Especially the Transformer architecture <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>, popularized in the context of Natural Language processing <ref type="bibr" target="#b2">(Brown et al., 2020)</ref> and then successfully ap-plied in several other Machine Learning tasks <ref type="bibr" target="#b9">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b23">Lample &amp; Charton, 2019;</ref><ref type="bibr" target="#b1">Biggio et al., 2021)</ref>, has proven to be a powerful tool for musical sequence modelling. Initial breakthroughs by <ref type="bibr" target="#b17">Huang et al. (2018)</ref> and <ref type="bibr" target="#b27">Payne (2019)</ref> applied language modelling techniques to symbolic music to achieve state-of-the-art music generation. These models featured limited controllability, if any, and subsequent work attempts to improve on this limitation through various avenues <ref type="bibr" target="#b11">(Ens &amp; Pasquier, 2020;</ref><ref type="bibr" target="#b4">Choi et al., 2020;</ref><ref type="bibr" target="#b32">Wu &amp; Yang, 2021;</ref><ref type="bibr" target="#b14">Hadjeres &amp; Crestel, 2020)</ref>.</p><p>As deep generative models are improving and producing more and more realistic samples, it remains an area of active research how humans can interact with these models and get them to generate a desirable result. Recent efforts in text-to-image generation <ref type="bibr">(Ramesh et al., 2021)</ref> have shown the potential in usability and artistic applications of humaninterpretable controllable generative models. However in contrast to image modelling, controllable sequence modelling appears to be more challenging: On one hand, it is unclear how to incorporate meaningful attributes into the generation process in a way similar to style-based image generation methods <ref type="bibr" target="#b21">(Karras et al., 2019)</ref>. On the other hand, it is difficult to find meaningful attributes in the first place, as obtaining salient features for discrete data such as text or in this case symbolic music can be prohibitively difficult or expensive <ref type="bibr" target="#b30">(Shao et al., 2008;</ref><ref type="bibr" target="#b10">Ens &amp; Pasquier, 2019)</ref>.</p><p>Our key contribution consists of a novel self-supervised task for training conditional sequence models at scale. We call this the description-to-sequence task and apply it to the domain of symbolic music to demonstrate that it is effective at enabling controllable symbolic music generation. The goal of our work is to provide global yet fine-grained control over the generation process such that the user is able to define a guideline for the whole piece, some form of high-level instruction, which is subsequently interpreted and implemented by the model at generation time. We propose the description-to-sequence task where a high-level description is extracted automatically from a given sequence and the model learns to reconstruct the original solely based on the description. To this end, we define the description as a function that, taking as input a single bar of music, defines a set of descriptive features for that bar, resulting in a sequence of events describing the entire piece by concatenating the bar-level description features. We propose  two different description functions: The 1) hand-crafted expert description, which provides global context in the form of a low-fidelity, human-interpretable sequence and the 2) learned learned description, where we use representation learning to extract high-fidelity salient features from the source sequence. Once trained in this way, our model can be employed to generate a new music piece given a description (in the form of the two aforementioned ones), encoding the salient features of the target song. If the description is sufficiently interpretable and detailed, this procedure allows for human-controllable music generation at a possibly very fine-grained scale.</p><p>We evaluate the proposed method on its ability to adhere to the prescribed condition by comparing it to the state-of-theart method for controllable symbolic music <ref type="bibr" target="#b32">(Wu &amp; Yang, 2021)</ref>. We demonstrate quantitatively that our technique outperforms the state-of-the-art in controllable generation, modelling capability and sample quality. To evaluate sample quality, we employ subjective evaluation in the form of a user study. We further demonstrate that our models are robust and generalize well by evaluating the zero-shot performance on out-of-distribution data, indicating that the proposed description-to-sequence task is effective at learning generalized high-level concepts about the data.</p><p>First, we provide an overview of existing work and compare controllability of different generative models. Next, we introduce the description-to-sequence modelling task and describe our method applying the task to symbolic music. We then perform quantitative evaluation of our models compared to state-of-the-art methods on conditional generation and zero-shot performance. We also perform an ablation study to validate different parts of the proposed model and finally evaluate the subjective quality of generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Controllable Symbolic Music Generation</head><p>We identify two different levels of controllability over the generation process. The most prevalent form of control is global conditioning of the model, where the generation is guided by a constant set of attributes which do not change during the generation process. This can be achieved by, for example, prompt-based conditioning where control tokens are given to the model as a prompt <ref type="bibr" target="#b27">(Payne, 2019)</ref> or by conditional decoding of latent representations <ref type="bibr" target="#b3">(Brunner et al., 2018)</ref>. Fine-grained control is achieved when the generation process can be guided at any point in time, i.e. if the control attributes can be arbitrarily varied over time.</p><p>Consider controlling what instruments are playing in the generated sequence as an example of global control in contrast to controlling what instruments are playing at any point in time as an example of fine-grained control. Note that finegrained control also implies global control, as global control can be achieved by fixing the control attributes to some constant value. Fine-grained control is therefore a strictly more powerful property and seemingly harder to obtain, as is highlighted in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Related Works</head><p>The capabilities of symbolic music generative models have been steadily improving with notable contributions by <ref type="bibr" target="#b17">Huang et al. (2018)</ref>, <ref type="bibr" target="#b27">Payne (2019)</ref>, <ref type="bibr">Huang &amp; Yang (2020)</ref> and <ref type="bibr" target="#b16">Hsiao et al. (2021)</ref>. This line of work focuses on improving the quality of generated samples but does not contribute substantially toward controllable generation. An exception to that is MuseNet <ref type="bibr" target="#b27">(Payne, 2019)</ref>, which allows some control through prompt-based conditioning with control tokens. Even still, prompt-based control is very limited, as control tokens are "forgotten" by the model once the generation advances beyond the initial context size. Another line of work focuses on finding ways of controlling the generation process. <ref type="bibr" target="#b3">Brunner et al. (2018)</ref> propose MIDI-VAE as a method for conditional generation as well as genre transfer between pop and jazz music. <ref type="bibr" target="#b4">Choi et al. (2020)</ref> propose a method for generating melody-conditioned piano performances. <ref type="bibr" target="#b11">Ens &amp; Pasquier (2020)</ref> propose MMM which is capable of bar-level and track-level symbolic music inpainting. <ref type="bibr" target="#b14">Hadjeres &amp; Crestel (2020)</ref> propose VQ-CPC for gener-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Rep. Multi-Track Multi-Sig. Global Ctrl. Fine-Grained Ctrl. MIDI-VAE <ref type="bibr" target="#b3">(Brunner et al., 2018)</ref> Pianoroll -MuseNet <ref type="bibr" target="#b27">(Payne, 2019)</ref> MIDI-like ( ) 1 -MMM <ref type="bibr" target="#b11">(Ens &amp; Pasquier, 2020)</ref> MIDI-like - <ref type="bibr" target="#b4">Choi et al. (2020)</ref> MIDI-like -( ) 2 VQ-CPC <ref type="bibr" target="#b14">(Hadjeres &amp; Crestel, 2020)</ref>  ating novel variations on existing music. <ref type="bibr" target="#b32">Wu &amp; Yang (2021)</ref> propose MuseMorphose for attribute-based conditional generation and style editing. All of these approaches have various limitations that are highlighted in Table <ref type="table">1</ref>. Common simplifications include limiting the model to a single track and the 4/4 time signature. Either of these simplifications and especially the combination of both severely limit the real-world applicability of resulting models, as most music does not satisfy these assumptions. Both of these limitations are ramified in our work by using appropriate extensions to the input representation.</p><p>Fine-grained control has been a topic of interest in the recent literature <ref type="bibr" target="#b4">(Choi et al., 2020;</ref><ref type="bibr" target="#b14">Hadjeres &amp; Crestel, 2020;</ref><ref type="bibr" target="#b32">Wu &amp; Yang, 2021;</ref><ref type="bibr" target="#b7">Di et al., 2021;</ref><ref type="bibr" target="#b12">Ferreira &amp; Whitehead, 2021)</ref> and is an essential property when considering userdirected applications. In essence, fine-grained control is necessary to allow control over salient features in the generation, as saliency in music at least partly lies in how it changes over time. In addition, salient features may be impossible or prohibitively expensive to quantify <ref type="bibr" target="#b4">(Choi et al., 2020;</ref><ref type="bibr" target="#b12">Ferreira &amp; Whitehead, 2021)</ref>, emphasizing the need for un-or self-supervised fine-grained control. Our proposed method provides both global and fine-grained control through description-to-sequence modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Description-to-Sequence Modelling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description Function</head><p>The goal of our method is to define a descriptive conditioning sequence d on a high level and train a generative model that learns the data distribution conditioned on d, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(x|d).</head><p>To this end, let X be a distribution of sequences and let x ∼ X . Let p 1 , . . . , p n denote a partition of x into a finite number of sub-sequences, such that x = p 1 • • • p n with • • denoting concatenation. We define the description</p><formula xml:id="formula_0">d 1 d 2 d 3 d 4 F(x) F(x) F(x) F(x)</formula><p>Source sequence Description func.</p><p>High-level desc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstructed sequence</head><p>Sequence-to-Sequence Model function F (•) as a function that takes the i-th element of the partition p i over a finite vocabulary V seq as the input and returns another sequence d i over a finite vocabulary V desc , which we call the description of p i . A useful description function should extract high-level features about p i , aiding in the reconstruction of p i in an eventual down-stream task.</p><p>Given F and x, we call</p><formula xml:id="formula_1">d = F (p 1 ) • • • F (p n ) the description of x.</formula><p>For simplicity, we write d = F (x) in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Description-to-Sequence Task</head><p>In order to learn the conditional distribution p(x|F (x)), we minimize the reconstruction loss</p><formula xml:id="formula_2">L rec (φ) = E x∼X [− log p φ (x|F (x))],</formula><p>where X is the true distribution of sequences and φ are the parameters of a sequence-to-sequence model. Intuitively, F can be viewed as an information bottleneck: it removes some information about x but should leave enough for the model to reconstruct the original sequence as accurately as possible. We call this self-supervised setup the descriptionto-sequence task. An illustrated overview of this task is given in Figure <ref type="figure" target="#fig_1">2</ref> We propose two different description functions for generating descriptions from musical sequences. The first version is a hand-crafted algorithm, where we aim to extract human interpretable descriptions. This will enable artist-guided creation or editing of descriptions and we refer to this as the expert description. In an attempt to improve on the expert description by using learning-based methods, we also propose a second version which we call the learned description.</p><p>In this case we learn latent representations of given partitions using the VQ-VAE framework <ref type="bibr" target="#b25">(Oord et al., 2018)</ref>. In both cases we partition x into its bars b 1 , . . . , b n as this is a natural way to partition musical sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Expert Description</head><p>As a baseline and proof-of-concept, we use domain knowledge to construct a description function that is humaninterpretable and could be created from scratch by a human expert. We include trivially important features such as time signature, instruments and chords, as well as stylistically relevant features identified by previous work <ref type="bibr" target="#b4">(Choi et al., 2020)</ref>.</p><p>• Time signature: the time signature of the current bar. By convention, any bar has exactly one associated time signature.</p><p>• Note density: the number of note onsets per quarter note</p><p>• Mean pitch: average pitch of all note onsets in the given bar</p><p>• Mean velocity: average velocity of all note onsets in the given bar</p><p>• Mean duration: average duration of all notes with onsets in the given bar</p><p>• Instruments: list of all instruments that have note onsets in the given bar</p><p>• Chords: list of all chords being played during the given bar</p><p>All of these quantities are easy to understand for humans given the necessary domain knowledge. Since these quantities are efficiently computable, pairs of corresponding musical sequences and descriptions are easy to generate. For tokenization, we discretize real values (note density, mean pitch/velocity/duration) to some appropriate interval, ensuring a finite vocabulary. Chords are extracted using an adapted version of the Viterbi algorithm used by <ref type="bibr">Huang &amp; Yang (2020)</ref>. An example description is given in Figure <ref type="figure" target="#fig_2">3</ref>. More details on feature extraction and tokenization can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learned Description</head><p>While the expert description serves as a human-interpretable, strong inductive bias, it has various shortcomings. For ex-&lt;bos&gt; B a r 1 T i m e S i g n a t u r e 4 / 4 N o t e D e n s i t y 3 M e a n P i t c h 1 4 M e a n V e l o c i t y 1 9 M e a n D u r a t i o n 3 2 I n s t r u m e n t D r u m s I n s t r u m e n t P i a n o I n s t r u m e n t E − P i a n o I n s t r u m e n t S l a p B a s s Chord E : maj Chord F # : min7 B a r 2 T i m e S i g n a t u r e 4 / 4 N o t e D e n s i t y 3 . . . &lt;eos&gt; ample, it is only able to capture fairly low-fidelity features by design and suffers from non-injectivity, meaning that there can be many musical sequences that map to the same description. In an attempt to improve on these points, we use representation learning to extract features that describe the underlying sequence in more detail. We choose the VQ-VAE framework as the basis for our learned description function as it has been shown to be effective for various tasks such as image generation <ref type="bibr" target="#b25">(Oord et al., 2018)</ref>, speechrecognition <ref type="bibr" target="#b0">(Baevski et al., 2019)</ref> and most importantly music generation <ref type="bibr" target="#b6">(Dhariwal et al., 2020)</ref>. We essentially trade human-interpretability for a higher-fidelity representation.</p><p>We embed each partition into a latent vector z e , which is then discretized to 16 separate codes with a codebook size of 2048. We use the discretized latent representation z q as the learned description function in our experiments. In practice, we reuse the frozen embedding weights of the VQ-VAE, effectively feeding the quantized latent embeddings directly to the model. This improves training performance in our experiments while reducing the number of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>We train FIGARO by applying the description-to-sequence task to symbolic music. We extract bar-level features by using the expert and learned description functions and feed them to a Transformer auto-encoder trained on reconstruction. We also train FIGARO (expert) and FIGARO (learned), each using only the description indicated in parentheses. An illustrated overview of our method can be found in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">REMI+ Input Representation</head><p>To make event-stream-based MIDI input files machineinterpretable, we extend the beat-based revamped MIDI (REMI) representation <ref type="bibr">(Huang &amp; Yang, 2020)</ref> to make it suitable for our modelling task. REMI represents notes with four consecutive tokens encoding note position, pitch, velocity and duration. It additionally also includes chord and tempo events to further guide the modelling process.</p><p>As introduced, REMI does not allow for modelling multiple tracks or multiple time signatures. To alleviate this limitation we propose REMI+, an extension to REMI that makes it suitable for general multi-track modelling tasks. More details on this are given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Description-to-Sequence Model</head><p>For our description-to-sequence model we stick very close to the original Transformer auto-encoder proposed by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>. Unless specified differently, we use the same hyperparameters as the original paper. To save on computation and training time, we use a context size of 256 tokens in all of our experiments. In the same spirit, we also reduce the number of encoder layers to 4 but leave the number of decoder layers at 6. In total, the model has 44.1 M trainable parameters.</p><p>We use relative positional embeddings <ref type="bibr" target="#b19">(Huang et al., 2020)</ref> as this has been shown to be beneficial for symbolic music generation <ref type="bibr" target="#b17">(Huang et al., 2018)</ref>. In addition to the positional embeddings, we also add a learned bar embedding and a learned bar-position embedding. In the case of using both descriptions, we simply add their embeddings before passing it to the encoder.</p><p>Training. We train this model on (F (x), x) data pairs where x is a REMI+ sequence and F (x) is either the expert or learned description of x as described in Section 3. We also run experiments using a combination of both descriptions, where we simply add the embeddings of the two. We minimize the reconstruction loss as our training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">VQ-VAE Model</head><p>For our VQ-VAE model, we use a 4-layer Transformer encoder in combination with a 6-layer Transformer decoder with cross-attention. The final layer of the encoder is pooled as proposed by <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> and projected to the latent space. The latent vector then is discretized and fed to the decoder through cross-attention. The output of the decoder is the PDF p(x t |x &lt;t ) for each position in the context. We use a linear layer before and after vector-quantization to project between model space and latent space. The model has 43.7 M trainable parameters in total.</p><p>We use a modified version of the sliced vector quantization scheme proposed by <ref type="bibr" target="#b20">Kaiser et al. (2018)</ref> as our discretization bottleneck. Specifically, we decompose the latent representation z into 16 slices z 1 , . . . , z 16 and discretize each of them to a shared codebook C with |C| = 2048 using the k-means discretization technique from the original paper <ref type="bibr" target="#b25">(Oord et al., 2018)</ref>.</p><p>Training. The model is trained by minimizing the canonical β-VQ-VAE loss without the auxiliary codebook loss <ref type="bibr" target="#b26">(Oord et al., 2019)</ref> with β = 0.02 in all of our experiments. The codebook is updated using the EMA update step as proposed in the original paper. We employ random restarts <ref type="bibr" target="#b6">(Dhariwal et al., 2020)</ref> to ensure optimal codebook usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We use the LakhMIDI dataset <ref type="bibr" target="#b28">(Raffel, 2016)</ref> as training data in all of our experiments, which to the best of our knowledge is the largest publicly available symbolic music dataset. We use a 80%-10%-10% training-validation-test split. For evaluation, we generate samples conditioned on descriptions sampled from the test set for 24 hours on 8 GPU, generating 32 bars for each sample. We provide a non-cherrypicked collection of samples and encourage the reader to get an impression of the quality and diversity of the generation process by listening to some of them<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training Details</head><p>Unless specified differently, we use the following training setup in all of our experiments. We train each model for 100k steps with a batch size of 512 sequences. Models are optimized using the Adam optimizer <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2017)</ref> with β 1 = 0.9, β 2 = 0.999, = 10 −6 and 0.01 weight decay. We use the inverse-square-root learning rate schedule with initial constant warmup at 10 −4 given by 10 −4 / max(1, n/N ) where N = 4000 is the number of warmup steps. We additionally release the source code and pre-trained model weights that were used in our experiments for additional details on training and hyperparameters<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">FLUENCY</head><p>We use perplexity (PPL) as a way to measure fluency and to compare the likelihood of different models in addition to task-specific metrics. The perplexity measures the likelihood of sequences while normalizing over the sequence length, which makes it better suited to comparing sequences of different lengths than the negative log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">DESCRIPTION FIDELITY</head><p>We also quantitatively evaluate the fidelity of generated sequences to the given condition. Let x denote a test sample and F (x) its description. Then we generate x by sampling the model conditioned on F (x) and examine x and x for similarity. Metrics are computed as an empirical estimate over the test distribution. More details and exact formulas are given in Appendix C.</p><p>Accuracy. We compute accuracy metrics for categorical values, namely for instruments, chords and time signature. Instruments and chord are multi-label features for which we compute the mean F 1 score. We compute the mean accuracy for time signatures, as there is exactly one time signature per bar by the definition of REMI+.</p><p>Macro Overlapping Area. Previous work has used the overlapping area (OA) metric to quantify similarity between two musical sequences for a given feature <ref type="bibr" target="#b4">(Choi et al., 2020;</ref><ref type="bibr" target="#b32">Wu &amp; Yang, 2021)</ref>. However, we find that the standard OA metric fails to take the order of the sequences into account, as feature histograms are computed over the entire sequence.</p><p>To alleviate this limitation, we propose the macro overlapping area (MAO), which partition-wise computes the overlap in the distributions of a given feature, taking sequential order into account. We use the MOA metric to compute similarity in pitch, velocity and duration between ground truth and reconstruction.</p><p>Normalized Root-Mean-Square Error. We compute the normalized RMSE (NRMSE) for bar-wise note density. This helps compare similarity accross different feature magnitudes.</p><p>Cosine Similarity. We also compute chroma and grooving similarity as a way to quantify similarity in sound and rhythm as proposed by <ref type="bibr" target="#b32">Wu &amp; Yang (2021)</ref>. We compute barwise cosine similarity for the chroma vectors <ref type="bibr" target="#b13">(Fujishima, 1999)</ref> and grooving vectors <ref type="bibr" target="#b8">(Dixon et al., 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Conditional Generation</head><p>We compare the conditional generative capabilities of our models to an unconditional baseline based on <ref type="bibr" target="#b17">Huang et al. (2018)</ref> as well as two state-of-the-art methods for controllable generation <ref type="bibr" target="#b4">(Choi et al., 2020;</ref><ref type="bibr" target="#b32">Wu &amp; Yang, 2021)</ref>. We use the improved relative attention from <ref type="bibr" target="#b19">Huang et al. (2020)</ref> in our reimplementation of <ref type="bibr" target="#b17">Huang et al. (2018)</ref> and <ref type="bibr" target="#b4">Choi et al. (2020)</ref> to allow for fair comparison to our models. The unconditional baseline acts as a sanity check: It has no additional information about the source sequence and essentially its output can be seen as a draw from the data distribution, provided training was successful. <ref type="bibr" target="#b4">Choi et al. (2020)</ref> and <ref type="bibr" target="#b32">Wu &amp; Yang (2021)</ref> get the unmodified target sequence as an input while FIGARO gets both descriptions as input. We also train FIGARO (expert) and FIGARO (learned) models, which only get the expert or learned description as input. More details on the benchmarked models such as hyperparameters and training details can be found in Appendix D.</p><p>Perhaps unsurprisingly, FIGARO (expert) performs very well on all metrics that are directly present in the expert description. It also performs reasonably on chroma and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>H inst H chord Ground truth 3.763 4.077 <ref type="bibr" target="#b17">Huang et al. (2018)</ref>  grooving similarity, both of which are not directly present in the expert description. This shows that the expert description can act as a strong inductive bias and helps guide the generation process even beyond what information is directly represented by it. The fact that the results obtained with the expert description adhere with the content of the description itself is a highly desirable feature for controllable music generation. By using this description, the degree of control exerted by the user is enhanced and the results matches the expectations. We also observe that adding learned features further improves performance accross the board, with FI-GARO beating FIGARO (expert) in every category. The success of this hybrid approach means that we can preserve the interpretability and inductive bias, yet increase the quality of the generated music by exploiting black-box AI.</p><p>Our expert description and hybrid models both beat all baselines and the learned description model outperforms <ref type="bibr" target="#b4">Choi et al. (2020)</ref> on most metrics by a slight margin. The difference in performance is explained by the conditioning used for <ref type="bibr" target="#b4">Choi et al. (2020)</ref>, where the conditioning vector is temporally aggregated over the entire sample with any style progression throughout the sequence being lost. <ref type="bibr" target="#b32">Wu &amp; Yang (2021)</ref> attempt to alleviate this problem by deriving the conditioning vector from the current bar and varying it over time. However, the model experiences posterior collapse in our experiments when trained on the diverse LakhMIDI dataset, which is apparent by the low entropy of the model distribution (see Table <ref type="table" target="#tab_2">2</ref>) as well as the worse-than-unconditional performance on some description metrics. We provide the full list of results in Table <ref type="table">3a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Zero-Shot Medley Generation</head><p>We evaluate the out-of-distribution performance of our models by combining two sequences x (1) and x (2) into a new sequence that does not exist in the training distribution and hence the model has never seen before. To this end, we take 16 bars of each sequence and concatenate them to form a novel sequence x = b</p><p>(1) 1</p><formula xml:id="formula_3">• • • b (1) 16 b (2) 17 • • • b (2)</formula><p>32 . We then feed the description F (x) into the model and iteratively sample the output distribution to generate a "medley" <ref type="bibr" target="#b17">Huang et al. (2018)</ref> 1 Table <ref type="table">3</ref>. We compare our models to the unconditional baseline based on <ref type="bibr" target="#b17">Huang et al. (2018)</ref> and MuseMorphose <ref type="bibr" target="#b32">(Wu &amp; Yang, 2021)</ref> on perplexity (PPL) and similarity metrics. Similarity metrics include instrument F1-score (I), chord F1-score (C) and time signature accuracy (TS) as well as note density NRMSE (ND), pitch MOA (P), velocity MOA (V), duration MOA (D), chroma similarity sc and grooving similarity sg.</p><formula xml:id="formula_4">Model Fluency Accuracy Fidelity PPL ↓ I ↑ C ↑ TS ↑ ND ↓ P ↑ V ↑ D ↑ s c ↑ s g ↑</formula><p>of the input sequences. We use the same models as for the conditional generation task without further fine-tuning or other modifications. We omit <ref type="bibr" target="#b17">Huang et al. (2018)</ref> and <ref type="bibr" target="#b32">Wu &amp; Yang (2021)</ref> from the comparison as we do not expect competitive performance based on the results of the conditional generation task.</p><p>We see a drop in performance in all evaluation metrics for every model, which is expected due to distributional shifts in the data. However, models using the expert description as an input only drop significantly less than FIGARO (learned) and <ref type="bibr" target="#b4">Choi et al. (2020)</ref> compared to the conditional generation task. This shows that the expert description provides a strong inductive bias and is robust with respect to distributional shifts, which is not the case for learned representations in this experiment. We even observe the perplexity of FIGARO (expert) improve over the conditional generation task, which is most likely caused by noise. Yet it indicates that the model can handle this artificial distribution just as well as the test distribution. The complete list of results is available in Table <ref type="table">3b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study</head><p>To evaluate which parts of the expert description are essential for the model, we group the description into three components: instruments, chords and meta-information. Instruments and chords include all tokens with information about instruments and chords respectively while all other tokens (time signature, note density and mean pitch, velocity and duration) are classified as meta-tokens. We train separate models with one part of the description removed and compare the performance to FIGARO (expert), which receives the full expert description as input.</p><p>As one would expect, removing each component reduces the performance significantly in the respective metrics, indicating that each component carries useful information not entirely inferable through the remaining components. Interestingly, our experiments show that removing any component slightly decreases the over-all performance even in metrics that we would not necessarily expect to be affected. Removing instrument information, for example, increases the error for note density, mean pitch, velocity and duration, indicating that the instruments also carry implied information about those features. This seems plausible considering the fact that different styles (or genres) of music usually identifies a set of instruments that is common for said style.</p><p>The full list of results is available in Table <ref type="table">3c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Subjective Evaluation</head><p>Finally, we evaluate the subjective quality of generated samples through a user study, comparing our best model to the baselines. To this end, we have conducted a survey where participants were asked to indicate their preference between 20s excerpts of two samples chosen uniformly at random. In two types of questions, users had to choose 1) between a real sample and a generated sample or 2) between two generated samples.</p><p>Question type 1) ranks the different methods on how good generated samples are compared to real, human-composed music. In this respect, FIGARO beats all other baseline with a win rate of 39.3% compared to the next best model by <ref type="bibr" target="#b17">Huang et al. (2018)</ref>, which has a win rate of 33.2%.</p><p>Win rates of the different models are displayed in Figure <ref type="figure" target="#fig_3">4</ref> with 90% confidence intervals obtained through normal approximation. While Choi et al. ( <ref type="formula">2020</ref>) is able to adhere to the prescribed condition as shown in Section 6.1, the quality of generated samples is approximately on par with the unconditional model. FIGARO on the other hand is able to surpass the unconditional baseline in sample quality, while providing controllable generative capabilities.</p><p>Question type 2) is used to construct a pairwise ranking of models by applying the Wilcoxon signed-rank test on the study results. In this ranking, our model beats each of the baselines with a p-value of &lt; 10 −7 . The complete ranking and test results can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Future Work</head><p>While our work represents a step towards high-quality controllable symbolic music generation, we recognize several avenues for future work. In terms of quality, the proposed model is good at adhering to the prescribed description and generating music that is similar to the input over all but some salient features such as melody are often not preserved. This can be explained by a lacking bias toward reconstructing melodies accurately, which may be remedied by introducing melody-focused descriptions or auxiliary loss functions.</p><p>Furthermore, the design space for description functions is largely left unexplored. It is worth investigating whether using different functions lead to vastly different results. While we show that each component of the expert description is essential, we have no rigorous nor empirical evidence that the features we chose to use are optimal.</p><p>Finally, as the description-to-sequence objective is not exclusively applicable to symbolic music, it will be interesting to apply this method to different data. For example, the long-range conditioning ability could potentially allow for generating natural language that is coherent for much longer passages as well as allowing the user to define a story progression or over-arching structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Conclusion</head><p>We present the description-to-sequence objective as a selfsupervised modelling task and apply it in the context of symbolic music generation. We propose two description functions and show that each of them is successful in controlling the generation process globally on a fine-grained level. To the best of our knowledge, our method is the first that can generate symbolic music based on user-provided sequence-level guidelines. In terms of sample quality, our method beats other state-of-the-art symbolic music generative models trained on the LakhMIDI dataset.</p><p>The proposed description functions each have their own strengths: The expert description is a human-interpretable sequence that is easy to create and edit and acts as a strong inductive bias to the model. The learned description allows for more high-fidelity, detailed information to be expressed in the description while sacrificing humaninterpretability. We can combine both approaches to achieve human-interpretable, high fidelity controllable symbolic music generation.</p><p>On a broader perspective, we hope that the proposed method is a step toward facilitating artists in their creative process as well as enabling amateurs to express themselves by lowering the barrier of entry to music creation and making the process faster and easier over all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>Automatic music generation may raise ethical concerns similar to large natural language models. The model may exhibit biases toward a certain style of music and may not represent the music of marginalized cultures accurately. In our case, the majority of training samples are western music (mostly classical or popular music) which is not necessarily repre-sentative of music at large. The model also might reproduce copyrighted material that is present in the training data and potentially generate samples that infringe on copyright law.</p><p>spaced intervals in [0, 128] positions (12 positions per quarter note). As used by previous work, the overlapping area (OA) metric does not consider the sequential order of the investigated feature, as feature histograms are computed over the entire sequence. For example, the overlapping area of x and reverse(x) would be maximal, even though the reversed sequence does not sound like the original sequence in general.</p><p>To alleviate this limitation, we adapt the OA metric to also consider temporal order. Let x and y denote two musical sequences and let b (x) i and b (y) i denote the i-th bar of x and y respectively. We compute the overlap in feature distributions for each bar by fitting a Gaussian distribution to the feature under examination (e.g. note pitch) and compute the overlapping area between the two distributions. Let this overlap be given by overlap(b </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FineFigure 1 .</head><label>1</label><figDesc>Figure 1. Overview of FIGARO. Dashed lines indicate components that are only used during training. d denotes the hidden dimension of the model, N the number of bars and M the length of the expert description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Schematic overview of the proposed description-tosequence task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example of an expert description. The description contains information about time signature, note density, mean pitch, velocity and duration as well as which instruments and chords are played throughout the bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Win rates of generated samples against real samples. We compare FIGARO, Huang et al. (2018), Choi et al. (2020) Wu &amp; Yang (2021). Real samples are from the test set.</figDesc><graphic url="image-1.png" coords="8,307.44,67.06,234.00,103.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>ExpertDescription input musical sequence x output description d d ← () b 1 , . . . , b n ← PARTITIONINTOBARS(x) for b i ∈ (b 1 , . . . , b n ) do N ← {n | n is a note with onset in b i } I ← {inst | inst is being played during b i } C ← {chord | chord is being played during b i } q ← duration of b i inquarter notes ts ← time signature at beginning of b i nd ← DURATION(n) Quantize nd, mp, mv and md d i ← (i, ts, nd, mp, mv, md) list(I) list(C) d ← d d i end for return d C. Evaluation Metrics C.1. Macro Overlapping Area</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Then the macro overlapping area (MOA) between x and y is given by MOA(x, y) Root-Mean-Square ErrorIn order to normalize for different feature magnitudes between different samples, we compute the normalized RMSE (NRMSE) for bar-wise note density. Let x denote the ground truth, x denote the reconstruction and N denote the length of the sequences. Then the NRMSE is given byRMSE(x, x) = 1 N N i=1 (x i − x i ) 2NRMSE(x, x) = RMSE(x, x) mean(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Instrument entropy Hinst and chord entropy H chord for ground truth and modelled distributions. Model entropies are empirical estimates over samples from the conditional generation task. Closest to ground truth is best.</figDesc><table><row><cell></cell><cell>3.251</cell><cell>3.749</cell></row><row><cell>Choi et al. (2020)</cell><cell>3.647</cell><cell>3.949</cell></row><row><cell cols="2">Wu &amp; Yang (2021) 1.804</cell><cell>2.420</cell></row><row><cell>FIGARO (expert)</cell><cell>3.732</cell><cell>4.049</cell></row><row><cell cols="2">FIGARO (learned) 3.825</cell><cell>4.085</cell></row><row><cell>FIGARO</cell><cell>3.661</cell><cell>4.071</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Samples are available on Soundcloud.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Source code and model weights are available on GitHub.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">This is an assumption that could be violated in theory but does hold in practice.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. REMI+ Input Representation</head><p>We extend the original REMI representation to make it suitable for general multi-track, multi-signature symbolic music sequences. We make modifications that add time signature and instrument information, we determine a unique order of events and use quantization schemes that allow for accurate representation for a diverse set of music. An example of a REMI+ sequence is given in Figure <ref type="figure">5</ref> Time signature. We add a time signature token at the beginning of each bar, indicating the time signature of the bar which is to follow. We adapt the convention that time signature changes may only happen at the beginning a bar, which is commonly true in written music.</p><p>Instruments. We add instrument information as an additional token before each note event, indicating which instrument will play the following note.</p><p>Order of events. In theory, the order of notes within each bar can be arbitrary without compromising the validity of the sequence. But to make the modelling task easier, we define a unique and deterministic order of events. Specifically, we sort events by (Bar, Position, EventType, Instrument, Pitch) in ascending order (valid event types are {Chord, Tempo, Note}). This order is unique since a given instrument can only ever play a single note with a given pitch at a given time 3 .</p><p>Quantization. We largely follow <ref type="bibr">Huang &amp; Yang (2020)</ref> in quantization. The most significant deviation is the use of 12 note onset positions per quarter note instead of 4 as proposed in the original work. For example, there will be 48 unique note onset positions for the 4/4 time signature and 36 note onset positions for the 3/4 time signature. This allows both triplet and sixteenth notes to be quantized accurately, which is important when considering a diverse set of music.</p><p>Instruments and note pitches are not quantized as they are categorical variables with 128 possible values by the MIDI specification. Note velocity is quantized to 32 intervals in [0, 128] and note duration is quantized to position intervals defined by the following mesh:</p><p>This ensures single position accuracy up to quarter notes, then 16th and triplet accuracy up to half notes and 8th note accuracy up to a full note. To limit vocabulary size, we switch to quarter note steps up to 8 full notes and half note steps up to 16 full notes after that. Notes longer than 16 full notes are truncated to this length. Finally, tempo change events are discretized to 32 intervals in <ref type="bibr">[0,</ref><ref type="bibr">240]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Expert Description Algorithm</head><p>Pseudocode for generating the expert description is given in Algorithm 1. We quantize note density, mean pitch and velocity to 32 linearly spaced intervals in [0, 12], [0, 128] and [0, 128] respectively. Mean duration is quantized to 32 logarithmically</p><p>denote the chroma vector <ref type="bibr" target="#b13">(Fujishima, 1999)</ref> or grooving vector <ref type="bibr" target="#b8">(Dixon et al., 2004)</ref> for the i-th bar in x and y respectively. We then average the cosine similarity over the entire sequence to get the chroma/grooving similarity: We reimplement the model proposed by <ref type="bibr" target="#b17">Huang et al. (2018)</ref> as an unconditional baseline. We train the model on the REMI+ representation to allow for direct comparison to our method. We use the improved relative attention from <ref type="bibr" target="#b19">Huang et al. (2020)</ref> to eliminate any possible advantage arising from different attention mechanisms. We largely stick to the hyperparameters from our models, using 6 decoder layers with a hidden size of 512 and a filter size of 2048. Training and optimization hyperparameters are also the same as for our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Choi et al. (2020)</head><p>We reimplement the model proposed by <ref type="bibr" target="#b4">Choi et al. (2020)</ref> and train it on the REMI+ representation to allow for direct comparison to our method. We again use the improved relative attention from <ref type="bibr" target="#b19">Huang et al. (2020)</ref> and largely stick to the hyperparameters from the original paper, using 6 encoder and 6 decoder layers. Unlike the original work, we do not use data augmentation since the dataset is large enough and in order to allow for fair comparison between the models. Due to GPU memory constraints we reduce the context size from 2048 to 1024 and use an accumulated batch size of 16, ensuring stable training. Training and optimization hyperparameters are the same as for our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Wu &amp; Yang (2021)</head><p>We train MuseMorphose on the REMI+ representation to allow for direct comparison to our method. Adapting the released implementation for our experiments, we reduce the context size from 1280 to 512 tokens due to GPU memory constraints but leave all other hyperparameters as they were proposed in the original paper. The model is trained until convergence (approx. 125k steps). We limit the training data to a subset of the entire dataset (20k samples) due to technical limitations. This is still considerably more training data than what was used in the original paper (1k samples) and should not affect performance significantly compared to using the full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Subjective Evaluation</head><p>The study includes 7569 comparisons by 691 participants, each averaging 11 answers. In each comparison, participants were presented with two different samples and were asked to indicate, which of the two they preferred. For the following ranking test, samples were chosen uniformly at random from two different generative models, the pair of which again was chosen at random. In this setup, we treat real samples as one possible model, for which we simply sample the test distribution. For the other models, we generate samples as described for the conditional generation task (Section 6.1).</p><p>We apply the Wilcoxon signed-rank test to the results in order to establish a ranking of the different models. FIGARO beats each baseline except ground truth with a win rate of more than 60% and a p-value of less than 10 −7 . Out of all methods, FIGARO also has the highest win rate against real samples with a win rate of 39.3%, which is a 6% advantage over the next best method. Rankings and corresponding p-values are reported in </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rylwJxrYDS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019-09">September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural symbolic regression that scales</title>
		<author>
			<persName><forename type="first">L</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bendinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/biggio21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="936" to="945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
		<author>
			<persName><surname>Midi-Vae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07600</idno>
		<idno>arXiv: 1809.07600</idno>
		<ptr target="http://arxiv.org/abs/1809.07600" />
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoding Musical Style with Transformer Autoencoders</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/choi20b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="1899" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<idno>arXiv: 1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<title level="m">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
				<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Jukebox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<idno>arXiv: 2005.00341</idno>
		<ptr target="http://arxiv.org/abs/2005.00341" />
		<title level="m">Generative Model for Music</title>
				<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Background Music Generation with Controllable Music Transformer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3474085.3475195</idno>
		<ptr target="https://doi.org/10.1145/3474085.3475195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
				<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page" from="2037" to="2045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards Characterisation of Music via Rhythmic Patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>ISMIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<idno>arXiv: 2010.11929</idno>
		<ptr target="http://arxiv.org/abs/2010.11929" />
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
				<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Quantifying Musical Style: Ranking Symbolic Music based on Similarity to a Style. ISMIR</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasquier</surname></persName>
		</author>
		<ptr target="https://archives.ismir.net/ismir2019/paper/000107.pdf" />
		<imprint>
			<date type="published" when="2019-11">November 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring Conditional Multi-Track Music Generation with the Transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><surname>Mmm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06048</idno>
		<idno>arXiv: 2008.06048</idno>
		<ptr target="http://arxiv.org/abs/2008.06048" />
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to Generate Music With Sentiment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehead</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06125</idno>
		<idno>arXiv: 2103.06125</idno>
		<ptr target="http://arxiv.org/abs/2103.06125" />
		<imprint>
			<date type="published" when="2021-03">March 2021</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time chord recognition of musical sound: A system using common lisp music</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMC</title>
				<meeting>ICMC</meeting>
		<imprint>
			<date type="published" when="1999-10">Oct. 1999. 1999</date>
			<biblScope unit="page" from="464" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Vector Quantized Contrastive Predictive Coding for Template-based Music Generation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Crestel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10120</idno>
		<idno>arXiv: 2004.10120</idno>
		<ptr target="http://arxiv.org/abs/2004.10120" />
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Music Composition with Deep Learning: A Review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hernandez-Olivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beltran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12290</idno>
		<idno>arXiv: 2108.12290</idno>
		<ptr target="http://arxiv.org/abs/2108.12290" />
		<imprint>
			<date type="published" when="2021-09">September 2021</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02402</idno>
		<idno>arXiv: 2101.02402</idno>
		<ptr target="http://arxiv.org/abs/2101.02402" />
		<title level="m">Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs</title>
				<imprint>
			<date type="published" when="2021-01">January 2021</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Music</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><surname>Transformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<idno>arXiv: 1809.04281</idno>
		<ptr target="http://arxiv.org/abs/1809.04281" />
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00212</idno>
		<idno>arXiv: 2002.00212</idno>
		<ptr target="http://arxiv.org/abs/2002.00212" />
		<title level="m">Pop Music Transformer: Beatbased Modeling and Generation of Expressive Pop Piano Compositions</title>
				<imprint/>
	</monogr>
	<note>August 2020</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improve Transformer Models with Better Relative Position Embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13658</idno>
		<idno>arXiv: 2009.13658</idno>
		<ptr target="http://arxiv.org/abs/2009.13658" />
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast Decoding in Sequence Models using Discrete Latent Variables</title>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03382</idno>
		<idno>arXiv: 1803.03382</idno>
		<ptr target="http://arxiv.org/abs/1803.03382" />
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<idno>arXiv: 1812.04948</idno>
		<ptr target="http://arxiv.org/abs/1812.04948" />
		<imprint>
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv: 1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2017-01">January 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep learning for symbolic mathematics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Charton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Historical Development of Algorithmic Procedures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nierhaus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-211-75540-2_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-211-75540-2_2" />
	</analytic>
	<monogr>
		<title level="m">Algorithmic Composition: Paradigms of Automated Music Generation</title>
				<meeting><address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="7" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<idno>arXiv: 1711.00937</idno>
		<ptr target="http://arxiv.org/abs/1711.00937" />
		<title level="m">Neural Discrete Representation Learning</title>
				<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<idno>arXiv: 1807.03748</idno>
		<ptr target="http://arxiv.org/abs/1807.03748" />
		<title level="m">Representation Learning with Contrastive Predictive Coding</title>
				<imprint>
			<date type="published" when="2019-01">January 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><surname>Musenet</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/musenet/" />
		<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<idno>arXiv: 2102.12092</idno>
		<ptr target="http://arxiv.org/abs/2102.12092" />
		<title level="m">February 2021</title>
				<imprint/>
	</monogr>
	<note type="report_type">Zero-Shot Textto-Image Generation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantify music artist similarity based on style and mood</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
		<idno type="DOI">10.1145/1458502.1458522</idno>
		<ptr target="https://doi.org/10.1145/1458502.1458522" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM workshop on Web information and data management, WIDM &apos;08</title>
				<meeting>the 10th ACM workshop on Web information and data management, WIDM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008</date>
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Musemorphose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04090</idno>
		<ptr target="http://arxiv.org/abs/2105" />
		<title level="m">Full-Song and Fine-Grained Music Style Transfer with Just One Transformer VAE</title>
				<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
