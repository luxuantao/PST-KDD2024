<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Graph Augmentation to Improve Graph Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Susheel</forename><surname>Suresh</surname></persName>
							<email>suresh43@purdue.edu</email>
						</author>
						<author>
							<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
							<email>panli@purdue.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Georgia</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
							<email>jenneville@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Projection</forename><surname>Head</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Original Input Maximize Information Minimize Information Sample GNN-encoder Learnable Graph Data Augmentation</orgName>
								<address>
									<addrLine>1 2 3 4 5 Node Embeddings Edge Embeddings</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Graph Augmentation to Improve Graph Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning of graph neural networks (GNN) is in great need because of the widespread label scarcity issue in real-world graph/network data. Graph contrastive learning (GCL), by training GNNs to maximize the correspondence between the representations of the same graph in its different augmented forms, may yield robust and transferable GNNs even without using labels. However, GNNs trained by traditional GCL often risk capturing redundant graph features and thus may be brittle and provide sub-par performance in downstream tasks. Here, we propose a novel principle, termed adversarial-GCL (AD-GCL), which enables GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in GCL. We pair AD-GCL with theoretical explanations and design a practical instantiation based on trainable edge-dropping graph augmentation. We experimentally validate AD-GCL 2 by comparing with the state-of-the-art GCL methods and achieve performance gains of up-to 14% in unsupervised, 6% in transfer, and 3% in semi-supervised learning settings overall with 18 different benchmark datasets for the tasks of molecule property regression and classification, and social network classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph representation learning (GRL) aims to encode graph-structured data into low-dimensional vector representations, which has recently shown great potential in many applications in biochemistry, physics and social science <ref type="bibr" target="#b1">[1]</ref><ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref>. Graph neural networks (GNNs), inheriting the power of neural networks <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>, have become the almost de facto encoders for GRL <ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref>. GNNs have been mostly studied in cases with supervised end-to-end training <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref>, where a large number of task-specific labels are needed. However, in many applications, annotating labels of graph data takes a lot of time and resources <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>, e.g., identifying pharmacological effect of drug molecule graphs requires living animal experiments <ref type="bibr" target="#b19">[19]</ref>. Therefore, recent research efforts are directed towards studying self-supervised learning for GNNs, where only limited or even no labels are needed <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref>.</p><p>Designing proper self-supervised-learning principles for GNNs is crucial, as they drive what information of graph-structured data will be captured by GNNs and may heavily impact their performance in downstream tasks. Many previous works adopt the edge-reconstruction principle to match traditional network-embedding requirement <ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref>, where the edges of the input graph are expected to be reconstructed based on the output of GNNs <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b36">36]</ref>. Experiments showed that these GNN models learn to over-emphasize node proximity <ref type="bibr" target="#b23">[23]</ref> and may lose subtle but crucial structural information, thus failing in many tasks including node-role classification <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38]</ref> and graph classification <ref type="bibr" target="#b17">[17]</ref>.</p><p>Figure <ref type="figure">1:</ref> The AD-GCL principle and its instantiation based on learnable edge-dropping augmentation. AD-GCL contains two components for graph data encoding and graph data augmentation. The GNN encoder f (•) maximizes the mutual information between the original graph G and the augmented graph t(G) while the GNN augmenter optimizes the augmentation T (•) to remove the information from the original graph. The instantiation of AD-GCL proposed in this work uses edge dropping: An edge e of G is randomly dropped according to Bernoulli(ωe), where ωe is parameterized by the GNN augmenter.</p><p>To avoid the above issue, graph contrastive learning (GCL) has attracted more attention recently <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref>. GCL leverages the mutual information maximization principle (InfoMax) <ref type="bibr" target="#b39">[39]</ref> that aims to maximize the correspondence between the representations of a graph (or a node) in its different augmented forms <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref>. Perfect correspondence indicates that a representation precisely identifies its corresponding graph (or node) and thus the encoding procedure does not decrease the mutual information between them.</p><p>However, researchers have found that the InfoMax principle may be risky because it may push encoders to capture redundant information that is irrelevant to the downstream tasks: Redundant information suffices to identify each graph to achieve InfoMax, but encoding it yields brittle representations and may severely deteriorate the performance of the encoder in the downstream tasks <ref type="bibr" target="#b40">[40]</ref>. This observation reminds us of another principle, termed information bottleneck (IB) <ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref><ref type="bibr" target="#b45">[45]</ref><ref type="bibr" target="#b46">[46]</ref>. As opposed to InfoMax, IB asks the encoder to capture the minimal sufficient information for the downstream tasks. Specifically, IB minimizes the information from the original data while maximizing the information that is relevant to the downstream tasks. As the redundant information gets removed, the encoder learnt by IB tends to be more robust and transferable. Recently, IB has been applied to GNNs <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref>. But IB needs the knowledge of the downstream tasks that may not be available.</p><p>Hence, a natural question emerges: When the knowledge of downstream tasks are unavailable, how to train GNNs that may remove redundant information? Previous works highlight some solutions by designing data augmentation strategies for GCL but those strategies are typically task-related and sub-optimal. They either leverage domain knowledge <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30]</ref>, e.g., node centralities in network science or molecule motifs in bio-chemistry, or depend on extensive evaluation on the downstream tasks, where the best strategy is selected based on validation performance <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b30">30]</ref>.</p><p>In this paper, we approach this question by proposing a novel principle that pairs GCL with adversarial training, termed AD-GCL, as shown in Fig. <ref type="figure">1</ref>. We particularly focus on training self-supervised GNNs for graph-level tasks, though the idea may be generalized for node-level tasks. AD-GCL consists of two components: The first component contains a GNN encoder, which adopts InfoMax to maximize the correspondence/mutual information between the representations of the original graph and its augmented graphs. The second component contains a GNN-based augmenter, which aims to optimize the augmentation strategy to decrease redundant information from the original graph as much as possible. AD-GCL essentially allows the encoder capturing the minimal sufficient information to distinguish graphs in the dataset. We further provide theoretical explanations of AD-GCL. We show that with certain regularization on the search space of the augmenter, AD-GCL can yield a lower bound guarantee of the information related to the downstream tasks, while simultaneously holding an upper bound guarantee of the redundant information from the original graphs, which matches the aim of the IB principle. We further give an instantiation of AD-GCL: The GNN augmenter adopts a task-agnostic augmentation strategy and will learn an input-graph-dependent non-uniform-edge-drop probability to perform graph augmentation.</p><p>Finally, we extensively evaluate AD-GCL on 18 different benchmark datasets for molecule property classification and regression, and social network classification tasks in different setting viz. unsuper-vised learning (Sec. 5.1), transfer learning (Sec. 5.3) and semi-supervised learning (Sec. 5.4) learning. AD-GCL achieves significant performance gains in relative improvement and high mean ranks over the datasets compared to state-of-the-art baselines. We also study the theoretical aspects of AD-GCL with apt experiments and analyze the results to offer fresh perspectives (Sec. 5.2): Interestingly, we observe that AD-GCL outperforms traditional GCL based on non-optimizable augmentation across almost the entire range of perturbation levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and Preliminaries</head><p>We first introduce some preliminary concepts and notations for further exposition. In this work, we consider attributed graphs G = (V, E) where V is a node set and E is an edge set. G may have node attributes {X v ∈ R F | v ∈ V } and edge attributes {X e ∈ R F | e ∈ E} of dimension F . We denote the set of the neighbors of a node v as N v .</p><p>Learning Graph Representations. Given a set of graphs G i , i = 1, 2, ..., n, in some universe G, the aim is to learn an encoder f : G → R d , where f (G i ) can be further used in some downstream task. We also assume that G i 's are all IID sampled from an unknown distribution P G defined over G. In a downstream task, each G i is associated with a label y i ∈ Y. Another model q : R d → Y will be learnt to predict Y i based on q(f (G i )). We assume (G i , Y i )'s are IID sampled from a distribution P G×Y = P Y|G P G , where P Y|G is the conditional distribution of the graph label in the downstream task given the graph.</p><p>Graph Neural Networks (GNNs). In this work, we focus on using GNNs, message passing GNNs in particular <ref type="bibr" target="#b49">[49]</ref>, as the encoder f . For a graph G = (V, E), every node v ∈ V will be paired with a node representation h v initialized as h (0) v = X v . These representations will be updated by a GNN. During the k th iteration, each h</p><formula xml:id="formula_0">(k−1) v is updated using v ′ s neighbourhood information expressed as, h (k) v = UPDATE (k) h (k−1) v , AGGREGATE (k) (h (k−1) u , Xuv) | u ∈ Nv<label>(1)</label></formula><p>where AGGREGATE(•) is a trainable function that maps the set of node representations and edge attributes X uv to an aggregated vector, UPDATE(•) is another trainable function that maps both v's current representation and the aggregated vector to v's updated representation. After K iterations of Eq. 1, the graph representation is obtained by pooling the final set of node representations as,</p><formula xml:id="formula_1">f (G) :≜ h G = POOL {h (K) v | v ∈ V }<label>(2)</label></formula><p>For design choices regarding aggregation, update and pooling functions we refer the reader to <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8]</ref>.</p><p>The Mutual Information Maximization Principle. GCL is built upon the InfoMax principle <ref type="bibr" target="#b39">[39]</ref>, which prescribes to learn an encoder f that maximizes the mutual information or the correspondence between the graph and its representation. The rationale behind GCL is that a graph representation f (G) should capture the features of the graph G so that representation can distinguish this graph from other graphs. Specifically, the objective of GCL follows InfoMax:</p><formula xml:id="formula_2">max f I(G; f (G)), where G ∼ P G .<label>(3)</label></formula><p>where I(X 1 ; X 2 ) denotes the mutual information between two random variables X 1 and X 2 <ref type="bibr" target="#b50">[50]</ref>.</p><p>Note that the encoder f (•) given by GNNs is not injective in the graph space G due to its limited expressive power <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>. Specifically, for the graphs that cannot be distinguished by 1-WL test <ref type="bibr" target="#b51">[51]</ref>, GNNs will associate them with the same representations. We leave more discussion on 1-WL test in Appendix C. In contrast to using CNNs as encoders, one can never expect GNNs to identify all the graphs in G based their representations, which introduces a unique challenge for GCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial Graph Contrastive Learning</head><p>In this section, we introduce our adversarial graph contrastive learning (AD-GCL) framework and one of its instantiations based on edge perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theoretical Motivation and Formulation of AD-GCL</head><p>The InfoMax principle in Eq. 3 could be problematic in practice for general representation learning. Tschannen et al. have shown that for image classification, representations capturing the information that is entirely irrelevant to the image labels are also able to maximize the mutual information but such representations are definitely not useful for image classification <ref type="bibr" target="#b40">[40]</ref>. A similar issue can also be observed in graph representation learning, as illustrated by Fig. <ref type="figure" target="#fig_1">2</ref>: We consider a binary graph classification problem with graphs in the dataset ogbg-molbace <ref type="bibr" target="#b52">[52]</ref>. Two GNN encoders with exactly the same architecture are trained to keep mutual information maximization between graph representations and the input graphs, but one of the GNN encoders in the same time is further supervised by random graph labels. Although the GNN encoder supervised by random labels still keeps one-to-one correspondance between every input graph and its representation (i.e., mutual information maximization), we may observe significant performance degeneration of this GNN encoder when evaluating it over the downstream ground-truth labels. More detailed experiment setup is left in Appendix G.1.  This observation inspires us to rethink what a good graph representation is. Recently, the information bottleneck has applied to learn graph representations <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref>. Specifically, the objective of graph information bottleneck (GIB) follows GIB:</p><formula xml:id="formula_3">max f I(f (G); Y ) − βI(G; f (G)),<label>(4)</label></formula><p>where (G, Y ) ∼ P G×Y , β is a positive constant. Comparing Eq. 3 and Eq. 4, we may observe the different requirements between InfoMax and GIB: InfoMax asks for maximizing the information from the original graph, while GIB asks for minimizing such information but simultaneously maximizing the information that is relevant to the downstream tasks. As GIB asks to remove redundant information, GIB naturally avoids the issue encountered in Fig. <ref type="figure" target="#fig_1">2</ref>. Removing extra information also makes GNNs trained w.r.t. GIB robust to adverserial attack and strongly transferrable <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>Unfortunately, GIB requires the knowledge of the class labels Y from the downstream task and thus does not apply to self-supervised training of GNNs where there are few or no labels. Then, the question is how to learn robust and transferable GNNs in a self-supervised way.</p><p>To address this, we will develop a GCL approach that uses adversarial learning to avoid capturing redundant information during the representation learning. In general, GCL methods use graph data augmentation (GDA) processes to perturb the original observed graphs and decrease the amount of information they encode. Then, the methods apply InfoMax over perturbed graph pairs (using different GDAs) to train an encoder f to capture the remaining information. Definition 1 (Graph Data Augmentation (GDA)). For a graph G ∈ G, T (G) denotes a graph data augmentation of G, which is a distribution defined over G conditioned on G. We use t(G) ∈ G to denote a sample of T (G). Specifically, given two ways of GDA T 1 and T 2 , the objective of GCL becomes GDA-GCL:</p><formula xml:id="formula_4">max f I(f (t 1 (G)); f (t 2 (G))), where G ∼ P G , t i (G) ∼ T i (G), i ∈ {1, 2}.<label>(5)</label></formula><p>In practice, GDA processes are often pre-designed based on either domain knowledge or extensive evaluation, and improper choice of GDA may severely impact the downstream performance <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b24">24]</ref>. We will review a few GDAs adopted in existing works in Sec.4.</p><p>In contrast to previous predefined GDAs, our idea, inspired by GIB, is to learn the GDA process (over a parameterized family), so that the encoder f can capture the minimal information that is sufficient to identify each graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AD-GCL:</head><p>We optimize the following objective, over a GDA family T (defined below).</p><p>AD-GCL: min</p><formula xml:id="formula_5">T ∈T max f I(f (G); f (t(G))), where G ∼ P G , t(G) ∼ T (G),<label>(6)</label></formula><p>Definition 2 (Graph Data Augmentation Family). Let T denote a family of different GDAs T Φ (•), where Φ is the parameter in some universe. A T Φ (•) ∈ T is a specific GDA with parameter Φ.</p><p>The min-max principle in AD-GCL aims to train the encoder such that even with a very aggressive GDA (i.e., where t(G) is very different from G), the mutual information / the correspondence between the perturbed graph and the original graph can be maximized. Compared with the two GDAs adopted in GDA-GCL (Eq.5), AD-GCL views the original graph G as the anchor while pushing its perturbation T (G) as far from the anchor as it can. The automatic search over T ∈ T saves a great deal of effort evaluating different combinations of GDA as adopted in <ref type="bibr" target="#b24">[24]</ref>.</p><p>Relating AD-GCL to the downstream task. Next, we will theoretically characterize the property of the encoder trained via AD-GCL. The analysis here not only further illustrates the rationale of AD-GCL but helps design practical T when some knowledge of Y is accessible. But note that our analysis does not make any assumption on the availability of Y .</p><p>Note that GNNs learning graph representations is very different from CNNs learning image representations because GNNs are never injective mappings between the graph universe G and the representation space R d , because the expressive power of GNNs is limited by the 1-WL test <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b51">51]</ref>. So, we need to define a quotient space of G based on the equivalence given by the 1-WL test. Definition 3 (Graph Quotient Space). Define the equivalence ∼ = between two graphs</p><formula xml:id="formula_6">G 1 ∼ = G 2 if G 1 , G 2 cannot be distinguished by the 1-WL test. Define the quotient space G ′ = G/ ∼ =.</formula><p>So every element in the quotient space, i.e., G ′ ∈ G ′ , is a representative graph from a family of graphs that cannot be distinguished by the 1-WL test. Note that our definition also allows attributed graphs.</p><formula xml:id="formula_7">Definition 4 (Probability Measures in G ′ ). Define P G ′ over the space G ′ such that P G ′ (G ′ ) = P G (G ∼ = G ′ ) for any G ′ ∈ G ′ . Further define P G ′ ×Y (G ′ , Y ′ ) = P G×Y (G ∼ = G ′ , Y = Y ′ ). Given a GDA T (•) defined over G, define a distribution on G ′ , T ′ (G ′ ) = E G∼P G [T (G)|G ∼ = G ′ ] for G ′ ∈ G ′ .</formula><p>Now, we provide our theoretical results and give their implication. The proof is in the Appendix B. Theorem 1. Suppose the encoder f is implemented by a GNN as powerful as the 1-WL test. Suppose G is a countable space and thus G ′ is a countable space. Then, the optimal solution (f * , T * ) to AD-GCL satisfies, letting</p><formula xml:id="formula_8">T ′ * (G ′ ) = E G∼P G [T * (G)|G ∼ = G ′ ], 1. I(f * (t * (G)); G | Y ) ≤ min T ∈T I(t ′ (G ′ ); G ′ ) − I(t ′ * (G ′ ); Y ), where t ′ (G ′ ) ∼ T ′ (G ′ ), t ′ * (G ′ ) ∼ T ′ * (G ′ ), (G, Y ) ∼ P G×Y and (G ′ , Y ) ∼ P G ′ ×Y . 2. I(f * (G); Y ) ≥ I(f * (t ′ * (G ′ )); Y ) = I(t ′ * (G ′ ); Y ), where t ′ * (G ′ ) ∼ T ′ * (G ′ ), (G, Y ) ∼ P G×Y and (G ′ , Y ) ∼ P G ′ ×Y .</formula><p>The statement 1 in Theorem 1 guarantees a upper bound of the information that is captured by the representations but irrelevant to the downstream task, which matches our aim. This bound has a form very relevant to the GIB principle (Eq.4 when β = 1), since min</p><formula xml:id="formula_9">T ∈T I(t ′ (G ′ ); G ′ )−I(t ′ * (G ′ ); Y ) ≥ min f [I(f (G); G) − I(f (G); Y )],</formula><p>where f is a GNN encoder as powerful as the 1-WL test. But note that this inequality also implies that the encoder given by AD-GCL may be worse than the optimal encoder given by GIB (β = 1). This makes sense as GIB has the access to the downstream task Y .</p><p>The statement 2 in Theorem 1 guarantees a lower bound of the mutual information between the learnt representations and the labels of the downstream task. As long as the GDA family T has a good control, I(t ′ * (G ′ ); Y ) ≥ min T ∈T I(t ′ (G ′ ); Y ) and I(f * (G); Y ) thus cannot be too small. This implies that it is better to regularize when learning over T . In our instantiation, based on edge-dropping augmentation (Sec. 3.2), we regularize the ratio of dropped edges per graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instantiation of AD-GCL via Learnable Edge Perturbation</head><p>We now introduce a practical instantiation of the AD-GCL principle (Eq. 6) based on learnable edge-dropping augmentations as illustrated in Fig. <ref type="figure">1</ref>. (See Appendix D for a summary of AD-GCL in its algorithmic form.) The objective of AD-GCL has two folds: (1) Optimize the encoder f to maximize the mutual information between the representations of the original graph G and its augmented graph t(G); (2) Optimize the GDA T (G) where t(G) is sampled to minimize such a mutual information. We always set the encoder as a GNN f Θ with learnable parameters Θ and next we focus on the GDA, T Φ (G) that has learnable parameters Φ.</p><p>Learnable Edge Dropping GDA model T Φ (•). Edge dropping is the operation of deleting some edges in a graph. As a proof of concept, we adopt edge dropping to formulate the GDA family T . Other types of GDAs such as node dropping, edge adding and feature masking can also be paired with our AD-GCL principle. Interestingly, in our experiments, edge-dropping augmentation optimized by AD-GCL has already achieved much better performance than any pre-defined random GDAs even carefully selected via extensive evaluation <ref type="bibr" target="#b24">[24]</ref> (See Sec.5). Another reason that supports edge dropping is due to our Theorem 1 statement 2, which shows that good GDAs should keep some information related to the downstream tasks. Many GRL downstream tasks such as molecule classification only depends on the structural fingerprints that can be represented as subgraphs of the original graph <ref type="bibr" target="#b53">[53]</ref>. Dropping a few edges may not change those subgraph structures and thus keeps the information sufficient to the downstream classification. But note that this reasoning does not mean that we leverage domain knowledge to design GDA, as the family T is still broad and the specific GDA still needs to be optimized. Moreover, experiments show that our instantiation also works extremely well on social network classification and molecule property regression, where the evidence of subgraph fingerprints may not exist any more.</p><p>Parameterizing T Φ (•). For each G = (V, E), we set T Φ (G), T ∈ T as a random graph model <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b55">55]</ref> conditioning on G. Each sample t(G) ∼ T Φ (G) is a graph that shares the same node set with G while the edge set of t(G) is only a subset of E. Each edge e ∈ E will be associated with a random variable p e ∼ Bernoulli(ω e ), where e is in t(G) if p e = 1 and is dropped otherwise.</p><p>We parameterize the Bernoulli weights ω e by leveraging another GNN, i.e., the augmenter, to run on G according to Eq.1 of K layers, get the final-layer node representations {h</p><formula xml:id="formula_10">(K) v |v ∈ V } and set ω e = MLP([h (K) u ; h (K) z ])</formula><p>, where e = (u, z) and</p><formula xml:id="formula_11">{h (K) v | v ∈ V } = GNN-augmenter(G) (7)</formula><p>To train T (G) in an end-to-end fashion, we relax the discrete p e to be a continuous variable in [0, 1] and utilize the Gumbel-Max reparametrization trick <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref>. Specifically, p e = Sigmoid((log δ − log(1 − δ) + ω e )/τ ), where δ ∼ Uniform(0,1). As temperature hyper-parameter τ → 0, p e gets closer to being binary. Moreover, the gradients ∂pe ∂ωe are smooth and well defined. This style of edge dropping based on a random graph model has also been used for parameterized explanations of GNNs <ref type="bibr" target="#b58">[58]</ref>.</p><p>Regularizing T Φ (•). As shown in Theorem 1, a reasonable GDA should keep a certain amount of information related to the downstream tasks (statement 2). Hence, we expect the GDAs in the edge dropping family T not to perform very aggressive perturbation. Therefore, we regularize the ratio of edges being dropped per graph by enforcing the following constraint: For a graph G and its augmented graph t(G), we add e∈E ω e /|E| to the objective, where ω e is defined in Eq.7 indicates the probability that e gets dropped.</p><p>Putting everything together, the final objective is as follows.</p><formula xml:id="formula_12">min Φ max Θ I(f Θ (G); f Θ (t(G))) + λ reg E G e∈E ω e /|E| , where G ∼ P G , t(G) ∼ T Φ (G).<label>(8)</label></formula><p>Note Φ corresponds to the learnable parameters of the augmenter GNN and MLP used to derive the ω e 's and Θ corresponds to the learnable parameters of the GNN f .</p><p>Estimating the objective in Eq.8. In our implementation, the second (regularization) term is easy to estimate empirically. For the first (mutual information) term, we adopt InfoNCE as the estimator <ref type="bibr" target="#b59">[59]</ref><ref type="bibr" target="#b60">[60]</ref><ref type="bibr" target="#b61">[61]</ref>, which is known to be a lower bound of the mutual information and is frequently used for contrastive learning <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b62">62]</ref>. Specfically, during the training, given a minibatch of m</p><formula xml:id="formula_13">graphs {G i } m i=1 , let z i,1 = g(f Θ (G i )) and z i,2 = g(f Θ (t(G i ))) where g(•)</formula><p>is the projection head implemented by a 2-layer MLP as suggested in <ref type="bibr" target="#b62">[62]</ref>. With sim(•, •) denoting cosine similarity, we estimate the mutual information for the mini-batch as follows.</p><formula xml:id="formula_14">I(fΘ(G); fΘ(t(G))) → Î = 1 m m i=1 log exp(sim(zi,1, zi,2)) m i ′ =1,i ′ ̸ =i exp(sim(zi,1, z i ′ ,2 ))<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>GNNs for GRL is a broad field and gets a high-level review in the Sec. 1. Here, we focus on the topics that are most relevant to graph contrastive learning (GCL).</p><p>Contrastive learning (CL) <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b63">[63]</ref><ref type="bibr" target="#b64">[64]</ref><ref type="bibr" target="#b65">[65]</ref> was initially proposed to train CNNs for image representation learning and has recently achieved great success <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b66">66]</ref>. GCL applies the idea of CL on GNNs.</p><p>In contrast to the case of CNNs, GCL trained using GNNs posts us new fundamental challenges. An image often has multiple natural views, say by imposing different color filters and so on. Hence, different views of an image give natural contrastive pairs for CL to train CNNs. However, graphs are more abstract and the irregularity of graph structures typically provides crucial information. Thus, designing contrastive pairs for GCL must play with irregular graph structures and thus becomes more challenging. Some works use different parts of a graph to build contrastive pairs, including nodes v.s. whole graphs <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b67">67]</ref>, nodes v.s. nodes <ref type="bibr" target="#b68">[68]</ref>, nodes v.s. subgraphs <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b69">69]</ref>. Other works adopt graph data augmentations (GDA) such as edge perturbation <ref type="bibr" target="#b31">[31]</ref> to generate contrastive pairs. Recently. GraphCL <ref type="bibr" target="#b24">[24]</ref> gives an extensive study on different combinations of GDAs including node dropping, edge perturbation, subgraph sampling and feature masking. Extensive evaluation is required to determine good combinations. MVGRL <ref type="bibr" target="#b25">[25]</ref> and GCA <ref type="bibr" target="#b30">[30]</ref> leverage the domain knowledge of network science and adopt network centrality to perform GDAs. Note that none of the above methods consider optimizing augmentations. In contrast, our principle AD-GCL provides theoretical guiding principles to optimize augmentations. Very recently, JOAO <ref type="bibr" target="#b70">[70]</ref> adopts a bi-level optimization framework sharing some high-level ideas with our adversarial training strategy but has several differences: 1) the GDA search space in JOAO is set as different types of augmentation with uniform perturbation, such as uniform edge/node dropping while we allow augmentation with non-uniform perturbation. 2) JOAO relaxes the GDA combinatorial search problem into continuous space via Jensen's inequality and adopts projected gradient descent to optimize. Ours, instead, adopts Bayesian modeling plus reparameterization tricks to optimize. The performance comparison between AD-GCL and JOAO for the tasks investigated in Sec. 5 is given in Appendix H.</p><p>Tian et al. <ref type="bibr" target="#b71">[71]</ref> has recently proposed the InfoMin principle that shares some ideas with AD-GCL but there are several fundamental differences. Theoretically, InfoMin needs the downstream tasks to supervise the augmentation. Rephrased in our notation, the optimal augmentation T IM (G) given by InfoMin (called the sweet spot in <ref type="bibr" target="#b71">[71]</ref>) needs to satisfy</p><formula xml:id="formula_15">I(t IM (G); Y ) = I(G; Y ) and I(t IM (G); G|Y ) = 0, t IM (G) ∼ T IM (G)</formula><p>, neither of which are possible without the downstreamtask knowledge. Instead, our Theorem 1 provides more reasonable arguments and creatively suggests using regularization to control the tradeoff. Empirically, InfoMin is applied to CNNs while AD-GCL is applied to GNNs. AD-GCL needs to handle the above challenges due to irregular graph structures and the limited expressive power of GNNs <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15]</ref>, which InfoMin does not consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Analysis</head><p>This section is devoted to the empirical evaluation of the proposed instantiation of our AD-GCL principle. Our initial focus is on unsupervised learning which is followed by analysis of the effects of regularization. We further apply AD-GCL to transfer and semi-supervised learning. Summary of datasets and training details for specific experiments are provided in Appendix E and G respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Unsupervised Learning</head><p>In this setting, an encoder (specifically GIN <ref type="bibr" target="#b72">[72]</ref>) is trained with different self-supervised methods to learn graph representations, which are then evaluated by feeding these representations to make prediction for the downstream tasks. We use datasets from Open Graph Benchmark (OGB) <ref type="bibr" target="#b52">[52]</ref>, TU Dataset <ref type="bibr" target="#b73">[73]</ref> and ZINC <ref type="bibr" target="#b74">[74]</ref> for graph-level property classification and regression. More details regarding the experimental setting are provided in the Appendix G.</p><p>We consider two types of AD-GCL, where one is with a fixed regularization weight λ reg = 5 (Eq.8), termed AD-GCL-FIX, and another is with λ reg tuned over the validation set among {0.1, 0.3, 0.5, 1.0, 2.0, 5.0, 10.0}, termed AD-GCL-OPT. AD-GCL-FIX assumes any information from the downstream task as unavailable while AD-GCL-OPT assumes the augmentation search space has some weak information from the downstream task. A full range of analysis on how λ reg impacts AD-GCL will be investigated in Sec. 5.2. We compare AD-GCL with three unsupervised/selfsupervised learning baselines for graph-level tasks, which include randomly initialized untrained GIN (RU-GIN) <ref type="bibr" target="#b72">[72]</ref>, InfoGraph <ref type="bibr" target="#b18">[18]</ref> and GraphCL <ref type="bibr" target="#b24">[24]</ref>. Previous works <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b24">24]</ref> show that they generally outperform graph kernels <ref type="bibr" target="#b75">[75]</ref><ref type="bibr" target="#b76">[76]</ref><ref type="bibr" target="#b77">[77]</ref> and network embedding methods <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b79">79]</ref>.</p><p>We also adopt GCL with GDA based on non-adversarial edge dropping (NAD-GCL) for ablation study. NAD-GCL drops the edges of a graph uniformly at random. We consider NAD-GCL-FIX and NAD-GCL-OPT with different edge drop ratios. NAD-GCL-GCL adopts the edge drop ratio of AD-GCL-FIX at the saddle point of the optimization (Eq.8) while NAD-GCL-OPT optimally tunes the edge drop ratio over the validation datasets to match AD-GCL-OPT. We also adopt fully supervised GIN (F-GIN) to provide an anchor of the performance. We stress that all methods adopt GIN <ref type="bibr" target="#b72">[72]</ref> as the encoder. Except F-GIN, all methods adopt a downstream linear classifier or regressor with the same hyper-parameters for fair comparison. Adopting linear models was suggested by <ref type="bibr" target="#b40">[40]</ref>, which explicitly attributes any performance gain/drop to the quality of learnt representations.</p><p>Tables 1 show the results for unsupervised graph level property prediction in social and chemical domains respectively. We witness the big performance gain of AD-GCL as opposed to all baselines across all the datasets. Note GraphCL utilizes extensive evaluation to select the best combination of augmentions over a broad GDA family including node-dropping, edge dropping and subgraph sampling. Our results indicate that such extensive evaluation may not be necessary while optimizing the augmentation strategy in an adversarial way is greatly beneficial.</p><p>We stress that edge dropping is not cherry picked as the search space of augmentation strategies.</p><p>Other search spaces may even achieve better performance, while an extensive investigation is left for the future work.</p><p>Moreover, AD-GCL also clearly improves upon the performance against its non-adversarial counterparts (NAD-GCL) across all the datasets, which further demonstrates stable and significant advantages of the AD-GCL principle. Essentially, the input-graph-dependent augmentation learnt by AD-GCL yields much benefit. Finally, we compare AD-GCL-FIX with AD-GCL-OPT. Interestingly, two methods achieve comparable results though AD-GCL-OPT is sometimes better. This observation implies that the AD-GCL principle may be robust to the choice of λ reg and thus motivates the analysis in the next subsection. Moreover, weak information from the downstream tasks indeed help with controlling the search space and further betters the performance. We also list the optimal λ reg 's of AD-GCL-OPT for different datasets in Appendix F.1 for the purpose of comparison and reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Note on the linear downstream classifier</head><p>We find that the choice of the downstream classifier can significantly affect the evaluation of the self-supervised representations. InfoGraph <ref type="bibr" target="#b18">[18]</ref> and GraphCL <ref type="bibr" target="#b24">[24]</ref> adopt a non-linear SVM model as the downstream classifier. Such a non-linear model is more powerful than the linear model we adopt and thus causes some performance gap between the results showed in Table <ref type="table" target="#tab_0">1</ref> (TOP) and (BOTTOM) and their original results (listed in Appendix G.2.1 as Table <ref type="table">8</ref>). We argue that using a non-linear SVM model as the downstream classifier is unfair, because the performance of even a randomly initialized untrained GIN (RU-GIN) is significantly improved (comparing results from Table <ref type="table" target="#tab_0">1</ref> (TOP) to Table <ref type="table">8</ref> ). Therefore, we argue for adopting a linear classifier protocol as suggested by <ref type="bibr" target="#b40">[40]</ref>. That having been said, our methods (both AD-GCL-FIX and AD-GCL-OPT) still performs significantly better than baselines in most cases, even when a non-linear SVM classifer is adopted, as shown in Table <ref type="table">8</ref>. Several relative gains are there no matter whether the downstream classifier is a simple linear model (Tables <ref type="table" target="#tab_0">1</ref>) or a non-linear SVM (Table <ref type="table">8</ref>). AD-GCL methods significantly outperform InfoGraph in 5 over 8 datasets and GraphCL in 6 over 8 datasets. This further provides the evidence for the effectiveness of our method. Details on the practical benefits of linear downstream models can be found in Appendix G.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Regularizing the GDA Model</head><p>Here, we study how different λ reg 's impact the expected edge drop ratio of AD-GCL at the saddle point of Eq.8 and further impact the model performance on the validation datasets. Due to the page limitation, we focus on classification tasks in the main text while leaving the discussion on regression tasks in the Appendix F.2. Figure <ref type="figure" target="#fig_2">3</ref> shows the results.</p><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>(a), a large λ reg tends to yield a small expected edge drop ratio at the convergent point, which matches our expectation. λ reg ranging from 0.1 to 10.0 corresponds to dropping almost everything (80% edges) to nothing (&lt;10% edges). The validation performance in Figure <ref type="figure" target="#fig_2">3</ref>(c) is out of our expectation. We find that for classification tasks, the performance of the encoder is extremely robust to different choices of λ reg 's when trained w.r.t. the AD-GCL principle, though the edge drop ratios at the saddle point are very different. However, the non-adversarial counterpart NAD-GCL is sensitive to different edge drop ratios, especially on the molecule dataset (e.g., ogbg-molclitox, ogbg-molbbbp). We actually observe the similar issue of NAD-GCL across all molecule datasets (See Appendix F.3). More interesting aspects of our results appear at the extreme cases. When λ reg ≥ 5.0, the convergent edge drop ratio is close to 0, which means no edge dropping, but AD-GCL still significantly outperforms naive GCL with small edge drop ratio. When λ reg = 0.3, the convergent edge drop ratio is greater than 0.6, which means dropping more than half of the edges, but AD-GCL still keeps reasonable performance. We suspect that such benefit comes from the training dynamics of AD-GCL (examples as shown in Figure <ref type="figure" target="#fig_2">3(b)</ref>). Particularly, optimizing augmentations allows for non-uniform edge-dropping probability. During the optimization procedure, AD-GCL pushes high drop probability on redundant edges while low drop probability on critical edges, which allows the encoder to differentiate redundant and critical information. This cannot be fully explained by the final convergent edge drop ratio and motivates future investigation of AD-GCL from a more in-depth theoretical perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transfer Learning</head><p>Next, we evaluate the GNN encoders trained by AD-GCL on transfer learning to predict chemical molecule properties and biological protein functions. We follow the setting in <ref type="bibr" target="#b17">[17]</ref> and use the same datasets: GNNs are pre-trained on one dataset using self-supervised learning and later fine-tuned on another dataset to test out-of-distribution performance. Here, we only consider AD-GCL-FIX as AD-GCL-OPT is only expected to have better performance. We adopt baselines including no pre-trained GIN (i.e., without self-supervised training on the first dataset and with only fine-tuning), InfoGraph <ref type="bibr" target="#b18">[18]</ref>, GraphCL <ref type="bibr" target="#b24">[24]</ref>, three different pre-train strategies in <ref type="bibr" target="#b17">[17]</ref> including edge prediction,  node attribute masking and context prediction that utilize edge, node and subgraph context respectively. More detailed setup is given in Appendix G.</p><p>According to Table <ref type="table" target="#tab_1">2</ref>, AD-GCL-FIX significantly outperforms baselines in 3 out of 9 datasets and achieves a mean rank of 2.4 across these 9 datasets which is better than all baselines. Note that although AD-GCL only achieves 5th on some datasets, AD-GCL still significantly outperforms InfoGraph <ref type="bibr" target="#b18">[18]</ref> and GraphCL <ref type="bibr" target="#b24">[24]</ref>, both of which are strong GNN self-training baselines. In contrast to InfoGraph <ref type="bibr" target="#b18">[18]</ref> and GraphCL <ref type="bibr" target="#b24">[24]</ref>, AD-GCL achieves some performance much closer to those baselines (EdgePred, AttrMasking and ContextPred) based on domain knowledge and extensive evaluation in <ref type="bibr" target="#b17">[17]</ref>. This is rather significant as our method utilizes only edge dropping GDA, which again shows the effectiveness of the AD-GCL principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Semi-Supervised Learning</head><p>Lastly, we evaluate AD-GCL on semi-supervised learning for graph classification on the benchmark TU datasets <ref type="bibr" target="#b73">[73]</ref>. We follow the setting in <ref type="bibr" target="#b24">[24]</ref>: GNNs are pre-trained on one dataset using selfsupervised learning and later fine-tuned based on 10% label supervision on the same dataset. Again, we only consider AD-GCL-FIX and compare it with several baselines in <ref type="bibr" target="#b24">[24]</ref>: 1) no pre-trained GCN, which is directly trained by the 10% labels from scratch, 2) SS-GCN-A, a baseline that introduces more labelled data by creating random augmentations and then gets trained from scratch, 3) a predictive method GAE <ref type="bibr" target="#b20">[20]</ref> that utilizes adjacency reconstruction in the pre-training phase, and GCL methods, 4) InfoGraph <ref type="bibr" target="#b18">[18]</ref> and 5) GraphCL <ref type="bibr" target="#b24">[24]</ref>. Note that here we have to keep the encoder architecture same and thus AD-GCL-FIX adopts GCN as the encoder. Table <ref type="table" target="#tab_2">3</ref> shows the results. AD-GCL-FIX significantly outperforms baselines in 3 out of 6 datasets and achieves a mean rank of 1.5 across these 6 datasets, which again demonstrates the strength of AD-GCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work we have developed a theoretically motivated, novel principle: AD-GCL that goes a step beyond the conventional InfoMax objective for self-supervised learning of GNNs. The optimal GNN encoders that are agnostic to the downstream tasks are the ones that capture the minimal sufficient information to identify each graph in the dataset. To achieve this goal, AD-GCL suggests to better graph contrastive learning via optimizing graph augmentations in an adversarial way. Following this principle, we developed a practical instantiation based on learnable edge dropping. We have extensively analyzed and demonstrated the benefits of AD-GCL and its instantiation with real-world datasets for graph property prediction in unsupervised, transfer and semi-supervised learning settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two GNNs keep the mutual information maximized between graphs and their representations. Simultaneously, they get supervised by ground-truth labels (green) and random labels (blue) respectively. The curves show their testing performance on predicting ground-truth labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) λreg v.s. expected edge drop ratio EG[ e ωe/|E|] (measured at saddle point of Eq.8). (b) Training dynamics of expected drop ratio for λreg. (c) Validation performance for graph classification v.s. edge drop ratio. Compare AD-GCL and GCL with non-adversarial edge dropping. The markers on AD-GCL's performance curves show the λreg used.</figDesc><graphic url="image-1.png" coords="9,114.15,164.55,387.48,78.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>± 0.51 ⋆ 73.59 ± 0.65 89.25 ± 1.45 74.49 ± 0.52 73.32 ± 0.61 ⋆ 85.52 ± 0.79 ⋆ 53.00 ± 0.82 71.57 ± 1.01 49.04 ± 0.53 AD-GCL-OPT 69.67 ± 0.51 ⋆ 73.81 ± 0.46 ⋆ 89.70 ± 1.03 75.10 ± 0.39 73.32 ± 0.61 ⋆ 85.52 ± 0.79 ⋆ 54.93 ± 0.43 ⋆ 72.33 ± 0.56 ⋆ 49.89 ± 0.66 ⋆ ± 0.023 10.005 ± 4.819 0.890 ± 0.017 74.74 ± 3.64 66.33 ± 2.79 64.50 ± 5.32 69.74 ± 0.57 60.54 ± 0.90 ± 0.010 73.69 ± 3.67 67.70 ± 1.78 74.40 ± 4.92 71.65 ± 0.94 61.14 ± 1.43 Ours AD-GCL-FIX 1.217 ± 0.087 0.842 ± 0.028 ⋆ 5.150 ± 0.624 ⋆ 0.578 ± 0.012 ⋆ 76.37 ± 2.03 68.24 ± 1.47 80.77 ± 3.92 71.42 ± 0.73 63.19 ± 0.95 AD-GCL-OPT 1.136 ± 0.050 ⋆ 0.812 ± 0.020 ⋆ 4.145 ± 0.369 ⋆ 0.544 ± 0.004 ⋆ 77.27 ± 2.56 69.54 ± 1.92 80.77 ± 3.92 72.92 ± 0.86 63.19 ± 0.95 Unsupervised learning performance for (TOP) biochemical and social network classification in TU datasets [73] (Averaged accuracy ± std. over 10 runs) and (BOTTOM) chemical molecules property prediction in OGB datasets [52] (mean ± std. over 10 runs). Bold/Bold ⋆ indicats our methods outperform baselines with ≥ 0.5/≥ 2 std respectively. Fully supervised (F-GIN) results are shown only for placing GRL methods in perspective. Ablation-study (AB-S) results do not count as baselines.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>MUTAG</cell><cell>DD</cell><cell>COLLAB</cell><cell>RDT-B</cell><cell>RDT-M5K</cell><cell>IMDB-B</cell><cell>IMDB-M</cell></row><row><cell></cell><cell>F-GIN</cell><cell>78.27 ± 1.35</cell><cell cols="4">72.39 ± 2.76 90.41 ± 4.61 74.87 ± 3.56 74.82 ± 0.92</cell><cell>86.79 ± 2.04</cell><cell>53.28 ± 3.17</cell><cell>71.83 ± 1.93</cell><cell>48.46 ± 2.31</cell></row><row><cell>Baselines</cell><cell>RU-GIN [72] InfoGraph [18] GraphCL [24]</cell><cell>62.98 ± 0.10 68.13 ± 0.59 68.54 ± 0.55</cell><cell cols="4">69.03 ± 0.33 87.61 ± 0.39 74.22 ± 0.30 63.08 ± 0.10 72.57 ± 0.65 87.71 ± 1.77 75.23 ± 0.39 70.35 ± 0.64 72.86 ± 1.01 88.29 ± 1.31 74.70 ± 0.70 71.26 ± 0.55</cell><cell>58.97 ± 0.13 78.79 ± 2.14 82.63 ± 0.99</cell><cell>27.52 ± 0.61 51.11 ± 0.55 53.05 ± 0.40</cell><cell>51.86 ± 0.33 71.11 ± 0.88 70.80 ± 0.77</cell><cell>32.81 ± 0.57 48.66 ± 0.67 48.49 ± 0.63</cell></row><row><cell>AB-S</cell><cell cols="2">NAD-GCL-FIX NAD-GCL-OPT 69.30 ± 0.32 69.23 ± 0.60</cell><cell cols="4">72.81 ± 0.71 88.58 ± 1.58 74.55 ± 0.55 71.56 ± 0.58 73.18 ± 0.71 89.05 ± 1.06 74.55 ± 0.55 72.04 ± 0.67</cell><cell>83.41 ± 0.66 83.74 ± 0.76</cell><cell>52.72 ± 0.71 53.43 ± 0.26</cell><cell>70.94 ± 0.77 71.94 ± 0.59</cell><cell>48.33 ± 0.47 49.01 ± 0.93</cell></row><row><cell>Ours</cell><cell cols="5">AD-GCL-FIX 69.67 Task Regression (Downstream Classifier -Linear Regression + L2)</cell><cell cols="5">Classification (Downstream Classifier -Logistic Regression + L2)</cell></row><row><cell></cell><cell>Dataset</cell><cell>molesol</cell><cell>mollipo</cell><cell>molfreesolv</cell><cell>ZINC-10K</cell><cell>molbace</cell><cell>molbbbp</cell><cell>molclintox</cell><cell>moltox21</cell><cell>molsider</cell></row><row><cell></cell><cell>Metric</cell><cell></cell><cell>RMSE (shared) (↓)</cell><cell></cell><cell>MAE (↓)</cell><cell></cell><cell cols="3">ROC-AUC % (shared) (↑)</cell></row><row><cell></cell><cell>F-GIN</cell><cell>1.173 ± 0.057</cell><cell>0.757 ± 0.018</cell><cell>2.755 ± 0.349</cell><cell cols="6">0.254 ± 0.005 72.97 ± 4.00 68.17 ± 1.48 88.14 ± 2.51 74.91 ± 0.51 57.60 ± 1.40</cell></row><row><cell>Baselines</cell><cell cols="3">RU-GIN [72] InfoGraph [18] 1.005 GraphCL [24] 1.706 ± 0.180 1.075 ± 0.022 1.344 ± 0.178 1.272 ± 0.089 0.910 ± 0.016</cell><cell>7.526 ± 2.119 7.679 ± 2.748</cell><cell cols="6">0.809 ± 0.022 75.07 ± 2.23 64.48 ± 2.46 72.29 ± 4.15 71.53 ± 0.74 62.29 ± 1.12 0.627 ± 0.013 74.32 ± 2.70 68.22 ± 1.89 74.92 ± 4.42 72.40 ± 1.01 61.76 ± 1.11</cell></row><row><cell>AB-S</cell><cell cols="2">NAD-GCL-FIX NAD-GCL-OPT 1.242 ± 0.096 1.392 ± 0.065</cell><cell>0.952 ± 0.024 0.897 ± 0.022</cell><cell>5.840 ± 0.877 5.840 ± 0.877</cell><cell cols="6">0.609 ± 0.010 73.60 ± 2.73 66.12 ± 1.80 73.32 ± 3.66 71.65 ± 0.94 60.41 ± 1.48 0.609</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AD-GCL-FIX 70.01 ±1.07 76.54 ± 0.82 63.28 ± 0.79 79.78 ± 3.52 78.51 ± 0.80 78.28 ± 0.97 72.30 ± 1.61 63.07 ± 0.72 68.83 ± 1.26 Transfer learning performance for chemical molecules property prediction (mean ROC-AUC ± std. over 10 runs). Bold indicates our methods outperform baselines with ≥ 0.5 std.. Train 73.72 ± 0.24 70.40 ± 1.54 73.56 ± 0.41 73.71± 0.27 86.63 ± 0.27 51.33 ± 0.44 SS-GCN-A 73.59 ± 0.32 70.29 ± 0.64 74.30 ± 0.81 74.19 ± 0.13 87.74 ± 0.39 52.01 ± 0.20 GAE [20] 74.36 ± 0.24 70.51 ± 0.17 74.54 ± 0.68 75.09 ± 0.19 87.69 ± 0.40 53.58 ± 0.13 InfoGraph [18] 74.86 ± 0.26 72.27 ± 0.40 75.78 ± 0.34 73.76 ± 0.29 88.66 ± 0.95 53.61 ± 0.31 GraphCL [24] 74.63 ± 0.25 74.17 ± 0.34 76.17 ± 1.37 74.23 ± 0.21 89.11 ± 0.19 52.55 ± 0.45 AD-GCL-FIX 75.18 ± 0.31 73.96 ± 0.47 77.91 ± 0.73 ⋆ 75.82 ± 0.26 ⋆ 90.10 ± 0.15 ⋆ 53.49 ± 0.28</figDesc><table><row><cell>Our Ranks</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>5</cell><cell>5</cell><cell>1</cell></row><row><cell>Dataset</cell><cell></cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>DD</cell><cell>COLLAB</cell><cell cols="2">RDT-B</cell><cell>RDT-M5K</cell><cell></cell></row><row><cell cols="2">No Pre-Our Ranks</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Semi-supervised learning performance with 10% labels on TU datasets<ref type="bibr" target="#b73">[73]</ref> (10-Fold Accuracy (%)± std over 5 runs). Bold/Bold ⋆ indicate our methods outperform baselines with ≥ 0.5 std/ ≥ 2 std respectively.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We greatly thank the actionable suggestions given by reviewers and the area chair. S.S. and J.N. are supported by the National Science Foundation under contract numbers CCF-1918483 and IIS-1618690. P.L. is partly supported by the 2021 JP Morgan Faculty Award and the National Science Foundation (NSF) award HDR-2117997.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Pre-Train Dataset ZINC 2M PPI-306K Fine-Tune Dataset BBBP Tox21 SIDER ClinTox BACE HIV MUV ToxCast PPI No Pre-Train</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="issue">7792</biblScope>
			<biblScope unit="page" from="706" to="710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph neural networks in particle physics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Vlimant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21001</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of control, signals and systems</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Drug discovery and evaluation: pharmacological assays</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Vogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graphite: Iterative generative modeling of graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2434" to="2444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Self-supervised learning of graph neural networks: A unified review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10757</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00111</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Motif-driven contrastive learning of graph representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12533</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bootstrapped representation learning on graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06514</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14945</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rolx: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1231" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Information Theory Workshop (ITW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The information bottleneck problem and its applications in machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Goldfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Polyanskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Information Theory</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00821</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph information bottleneck</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recognizing predictive substructures with subgraph information bottleneck</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. JMLR. org</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>Nauchno-Technicheskaya Informatsia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Random graphs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Mathematical Statistics</title>
				<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1141" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On random graphs i</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publ. Math. Debrecen</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-organizing neural network that discovers surfaces in randomdot stereograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6356</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Sub-graph contrast for scalable self-supervised graph representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10273</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07594</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="www.graphlearning.io" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">graph2vec: Learning distributed representations of graphs</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Sub2vec: Feature learning for subgraphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="170" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Groups, graphs, algorithms: The graph isomorphism problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICM</title>
				<meeting>ICM</meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3303" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Graph isomorphisms in quasi-polynomial time</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Helfgott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04574</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
