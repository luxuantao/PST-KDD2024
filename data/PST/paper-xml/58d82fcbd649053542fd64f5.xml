<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Recognition with Dynamic Image Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
							<email>hbilen@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Efstratios Gavves</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
							<email>basura.fernando@anu.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">Efstratios Gavves</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
							<email>egavves@uva.nl</email>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Efstratios Gavves</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Research School of Engineering</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<addrLine>ACT 2601</addrLine>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Recognition with Dynamic Image Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E7C603ACF27F3F3991605F7EDDEC7940</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2769085</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2769085, IEEE Transactions on Pattern Analysis and Machine Intelligence TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>human action classification</term>
					<term>video classification</term>
					<term>motion representation</term>
					<term>deep learning</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the concept of dynamic image, a novel compact representation of videos useful for video analysis, particularly in combination with convolutional neural networks (CNNs). A dynamic image encodes temporal data such as RGB or optical flow videos by using the concept of 'rank pooling'. The idea is to learn a ranking machine that captures the temporal evolution of the data and to use the parameters of the latter as a representation. We call the resulting representation dynamic image because it summarizes the video dynamics in addition to appearance. This powerful idea allows to convert any video to an image so that existing CNN models pre-trained with still images can be immediately extended to videos. We also present an efficient approximate rank pooling operator that runs two orders of magnitude faster than the standard ones with any loss in ranking performance and can be formulated as a CNN layer. To demonstrate the power of the representation, we introduce a novel four stream CNN architecture which can learn from RGB and optical flow frames as well as from their dynamic image representations. We show that the proposed network achieves state-of-the-art performance, 95.5% and 72.5% accuracy, in the UCF101 and HMDB51 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Videos account for a large majority of the visual data in existence, surpassing by a wide margin still images. Therefore understanding the content of videos accurately and on a large scale is of paramount importance. The advent of modern learnable representations such as deep convolutional neural networks (CNNs) has improved dramatically the performance of algorithms in many image understanding tasks. Since videos are composed of a sequence of still images, some of these improvements have been shown to transfer to videos directly. However, it remains unclear how videos can be optimally represented. For example, a video can be represented as a sequence of still images, as a subspace of images or image features, as the parameters of a generative model of the video, or as the output of a neural network or even of an handcrafted encoder.</p><p>Early works <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b67">[68]</ref> represented videos as (the parameter of) models generating them. Doretto et al. <ref type="bibr" target="#b11">[12]</ref> introduced the concept of dynamic textures, reconstructing pixel intensities as the output of an auto-regressive linear dynamical system. Wang et al. <ref type="bibr" target="#b67">[68]</ref> used instead the moments of a mixture of Gaussians generating temporally local, flow-based appearance variations in the video.</p><p>More recent approaches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b56">[57]</ref> focus on the problem of understanding the content of videos, which does not necessarily requires to model their dynamics. They do so by <ref type="bibr">1.</ref> From left to right and top to bottom: "blowing hair dry", "band marching", "balancing on beam", "golf swing", "fencing", "playing the cello", "horse racing", "doing push-ups", "drumming". treating videos as stack of frames and then learning discriminative models that distill the information needed to solve specific problems such as action recognition. The majority of these methods rely on convolutional or recurrent neural networks and learn spatio-temporal filters that maximize the recognition capability of the overall system in an end-to-end manner. This allows these approaches to achieve the highest accuracy in action recognition, as their primary purpose is to model the action classes and not the motion itself.</p><p>In this paper, we propose a new representation of videos that, as in the first examples, encodes the data in a general and contentagnostic manner, resulting in a long-term, robust motion representation applicable not only to action recognition, but to other video analysis tasks as well <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b63">[64]</ref>. This new representation distills the motion information contained in all the frames of a video into a single image, which we call the dynamic image. We show that the dynamic image is a powerful, efficient, and yet simple representation of videos, particularly useful in the context of deep learning.</p><p>A popular method to represent time series is to apply a temporal pooling operator to the features extracted at individual time instants. For videos, temporal pooling has been done by using temporal templates <ref type="bibr" target="#b2">[3]</ref>, ranking functions for video frames <ref type="bibr" target="#b16">[17]</ref> and sub-videos <ref type="bibr" target="#b22">[23]</ref>, as well as more traditional pooling operators <ref type="bibr" target="#b53">[54]</ref>. CNN add another dimension to this research, as one has to decide where pooling should take place. A CNN such as AlexNet <ref type="bibr" target="#b31">[32]</ref> contains in fact a whole hierarchy of image representations, one for each layer in the network. One could pool the output of the deep fully-connected layers of the network, but this would prevent the CNN from analyzing the video dynamics. Alternatively, temporal pooling could be applied to some intermediate network layer. In this case, the lower layers would still observe single frames, but the upper layers could reason about the overall video dynamics.</p><p>The dynamic image (section 3) takes this idea to its logical extreme and captures the video dynamics directly at the level of the image pixels, by applying a pooling operator before any of the CNN layers are evaluated. The dynamic image is a single RGB image, equivalent to a still, that captures the gist of the dynamics and appearance of a whole video sequence or subsequence (fig. <ref type="figure" target="#fig_0">1</ref>). The dynamic image is obtained as the parameter of ranking machine learned to sort the frames of the video temporally, a method proposed by <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>; the key difference from this prior work is that the ranking machine is computed directly at the level of the image pixels as well as any intermediate level of a CNN feature extractor.</p><p>This idea has four keys advantages. First, the dynamic image can be processed by any of the many CNN architecture for still images while still being able to reason about the long-term dynamics in a video. Second, the dynamic image is very efficient: extracting it is simple and quick, and reducing the analysis of videos to the analysis of a single RGB images significantly accelerates recognition. Third, the representation is very compact, as a whole video is summarized by an amount of data equivalent to a single frame. Compressing videos in this manner is very useful for large scale indexing. Fourth, dynamic images can be generalized to different kinds of sequences and to different modalities, as we demonstrate by applying it to optical flow frames (section 5.9).</p><p>Our second contribution in this paper is to provide a fast approximation to learning the ranking machine which is needed to extract dynamic images. This approximation, which we call approximate rank pooling (ARP), amounts to a simple weighted summation of the video frames where the weights are fixed for all videos of the same length and can therefore be pre-computed. This makes ARP an extremely efficient in practice.</p><p>ARP defines a map from sequences of N -video frames (I σ(1) , . . . , I σ(N ) ) presented in an order σ to a single dynamic image I dyn . Unlike other commonly used temporal pooling operators like max-or average-pooling that are orderless, and therefore, time/sequence invariant, ARP is sensitive to the permutation order σ. To the best of our knowledge, we are the first to propose a temporal pooling layer for neural network architectures that is sensitive to the order of the samples within a video sequence. We show that ARP can be seamlessly integrated into the end-to-end training of CNNs for video data. We also show that ARP can be applied to the intermediate layers of a CNN too, which can be used to obtain a multi-scale representation of videos.</p><p>As a third contribution, we demonstrate the power of the dynamic image and of ARP by applying them to the recognition of human actions in video sequences. Recent works such as <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b53">[54]</ref> pointed out that long term dynamics and temporal patterns are very important cues for the recognition of actions. However, representing complex long term dynamics is challenging, particularly if one seeks compact representations that can be processed efficiently. We do so by introducing a hybrid model (section 4) that makes use of both static and dynamic images and pools information from RGB and optical flow frames from short and long video subsequences. This results in a novel four stream architecture that can efficiently and accurately recognize actions in videos, obtaining state-of-the-art performance in standard benchmarks (section 5).</p><p>This paper is an extended version of our prior conference publication <ref type="bibr" target="#b1">[2]</ref>. The new contributions are:</p><p>• a more extensive overview and comparison of the related literature,</p><p>• a more detailed formulation of the proposed pooling operations in section 3.2,</p><p>• a novel four-stream architecture, adding two new dynamic image streams using optical flow input to the standard twostream architecture of <ref type="bibr" target="#b56">[57]</ref> in section 4.4,</p><p>• the use of more powerful deep networks ResNeXt-50 and ResNeXt-101 <ref type="bibr" target="#b75">[76]</ref> which result in significantly improved baseline action classification performance section 5,</p><p>• a thorough evaluation in section 5 of the proposed ARP, when applied to intermediate layers of a CNN instead of RGB pixels, obtaining state-of-the-art action classification accuracies in popular benchmarks ,</p><p>• an alternative temporal pooling strategy, called parametric pooling, whose parameters can be automatically learned together with the other network parameters in section 5.7,</p><p>• a detailed analysis of various design choices such as temporal window length, sampling rate, temporal pooling strategies in section 5,</p><p>• an extended qualitative and quantitative comparison to the previous work (Motion History Images <ref type="bibr" target="#b2">[3]</ref>) in section 5.</p><p>The rest of the paper is organized as follows: Section 2 provides an extensive overview of related work in video modeling and action recognition. Section 3 formulates the dynamic image and approximate rank pooling. Section 4 proposes different deep neural network architectures using dynamic images and explains how the proposed pooling operators can be integrated into standard CNN models. Section 5 provides a rigorous analysis of the design choices and evaluates the performance of the proposed models in standard human action recognition benchmarks. Section 6 summarizes our findings and discusses future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Videos as stack of still images: Existing video representations can be grouped into two categories. The first one, which comprises the majority of the literature on video processing, action and event classification, be it with shallow <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b67">[68]</ref> or deep representations <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b56">[57]</ref>, considers videos either as a stream of still images <ref type="bibr" target="#b43">[44]</ref> or as a short and smooth transition between similar frames <ref type="bibr" target="#b56">[57]</ref>. <ref type="bibr" target="#b43">[44]</ref> show that treating videos as bag of static frames performs reasonably well for recognition, as the context of an action typically correlates with the action itself (e.g., "playing basketball" usually takes place in a basketball court).</p><p>Videos as spatio-temporal volumes: The second category considers videos as 3D dimensional volumes instead of collections of 2D images. Before deep learning became popular, several authors <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b55">[56]</ref> proposed to learn spatio-temporal templates from such spatio-temporal volumes. More recent works <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b64">[65]</ref> extend spatial CNNs to a third, temporal dimension <ref type="bibr" target="#b26">[27]</ref> substituting 2D filters with 3D ones. Tran et al. <ref type="bibr" target="#b64">[65]</ref> show that 3D convolutional networks perform well in the presence of large amount of annotated videos. While the extension brings a more natural representation of videos, it leads to a significant increase in the number of parameters to learn and thus requires more training data. Furthermore, such representations do not account for the fact that the third dimension, time, is not homogeneous with the first two, space.</p><p>Simonyan et al. <ref type="bibr" target="#b56">[57]</ref> show an alternative way of exploiting spatio-temporal information in videos by training a deep neural networks on pre-computed optical flow rather than raw RGB frames and report significant improvements over previous state-ofthe-art. Similarly, <ref type="bibr" target="#b20">[21]</ref> uses action tubes to to fit a double stream appearance-and motion-based neural network that captures the movement of an actor.</p><p>Short and long-term dynamics: While the aforementioned methods successfully capture the local changes within a small time window, they cannot capture longer-term motion patterns associated with certain actions. An alternative solution is to consider a second family of architectures based on recurrent neural networks (RNNs) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b60">[61]</ref>. RNNs typically consider memory cells <ref type="bibr" target="#b23">[24]</ref>, which are sensitive to both short as well as longer term patterns. RNNs parse the video frames sequentially and encode the frame-level information in their memory. In <ref type="bibr" target="#b10">[11]</ref> LSTMs are used together with convolutional neural network activations to either output an action label or a video description. In <ref type="bibr" target="#b60">[61]</ref> an autoencoder-like LSTM architecture is proposed such that either the current frame or the next frame is accurately reconstructed. Finally, the authors of <ref type="bibr" target="#b77">[78]</ref> propose an LSTM with a temporal attention model for densely labeling video frames.</p><p>Many of the ideas in video CNNs originated in earlier architectures that used hand-crafted features. For example, the authors of <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref> have shown that local motion patterns in short frame sequences can capture very well the short temporal structures in actions. The rank pooling idea, on which our dynamic images are based, was proposed in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> using hand-crafted representation of the frames. Similar to our work, a concurrent work <ref type="bibr" target="#b68">[69]</ref> also employs the dynamic image networks on RGB and optical flow frames with a three-stream model.</p><p>Multi-stream networks: Our static/dynamic CNN uses a multi-stream architecture. Multiple streams have been used in a variety of different contexts. Examples include Siamese architectures for learning metrics for face identification <ref type="bibr" target="#b5">[6]</ref>, for unsupervised training of CNNs <ref type="bibr" target="#b9">[10]</ref> or, for training externally a visual object tracker to track by searching instances <ref type="bibr" target="#b63">[64]</ref>. Simonyan et al. <ref type="bibr" target="#b56">[57]</ref> use two streams to encode respectively static frames and optical flow frames in action recognition. Recently, Feichtenhofer et al. <ref type="bibr" target="#b14">[15]</ref> show that fusing two streams via a 3D convolution further improves the classification performance. The authors of <ref type="bibr" target="#b40">[41]</ref> propose a dual loss neural network, where coarse and fine outputs are jointly optimized. A difference of our model compared to these is that we branch off two streams at arbitrary location in the network, either at the input, at the level of the convolutional layers, or at the level of the fully-connected layers.</p><p>Motion information: Motion is a rich source of information for recognizing human actions. Kinematic feature design is heavily studied in the context of human action recognition. In this regard, techniques such as motion summary methods <ref type="bibr" target="#b2">[3]</ref>, optical flow <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b54">[55]</ref>, and dynamic textures <ref type="bibr" target="#b29">[30]</ref> are used to capture motion.</p><p>Our work is also related to early work on motion summary techniques such as motion energy image (MEI) and motion history image (MHI) <ref type="bibr" target="#b2">[3]</ref>. Given an image sequence, the binary MEIs highlight regions in the image where any form of motion was present. To construct MEIs, the summation of the square of consecutive image differences is used as a robust spatial motiondistribution signal. To encode the motion of an image sequence, the motion history images (MHI) are used. In an MHI, pixel intensity is a function of the motion history at that location, where brighter values correspond to more recent motion. As a matter of fact, we compare the proposed method to MHI quantitatively and qualitatively in section 5.</p><p>Optical-flow based methods estimate the optical-flow between successive frames and then summarize the motion using principle components <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b54">[55]</ref>. In some instances the optical flow is computed on sub-volumes of the whole video using integral videos <ref type="bibr" target="#b28">[29]</ref>, or surrounding the central motion <ref type="bibr" target="#b12">[13]</ref>. However, normally, the optical-flow, provides only the local dynamics and aggregation of local motion is performed using simple summarization methods.</p><p>Spatio-temporal dynamics: Dynamic texture <ref type="bibr" target="#b11">[12]</ref> uses autoregressive moving average process which estimates the parameters of the model using sequence data. Dynamic textures methods evolved from techniques originally designed for recognizing textures in 2D images <ref type="bibr" target="#b11">[12]</ref>, where they were extended to time-varying "dynamic textures" <ref type="bibr" target="#b29">[30]</ref> for sequence recognition tasks. The Local Binary Patterns (LBP) <ref type="bibr" target="#b44">[45]</ref>, for example, use short binary strings to encode the micro-texture centered around each pixel. A whole 2D image is represented by the frequencies of these binary strings. In <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b81">[82]</ref> the LBP descriptor was extended to 3D video data and successfully applied to facial expression recognition tasks. Subspace-based methods are used in <ref type="bibr" target="#b36">[37]</ref>. These methods captured some time-varying information for sequence classification tasks.</p><p>Even though these techniques <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b54">[55]</ref> provides a solution to capture motion of video sequences, none of them use a learning strategy based on optimization to summarize the motion dynamics of video sequence as our method. Moreover, we are the first to use a motion summary images to train CNNs for human action recognition. Our motion summary concept is based on rank pooling and can be applied at different levels of CNN architecture.</p><p>Learning to rank videos: More recently the rank pooling <ref type="bibr" target="#b17">[18]</ref> method is extended in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b19">[20]</ref> to increase the capacity of rank pooling using a hierarchical approach. In <ref type="bibr" target="#b18">[19]</ref>, an end-to-end video representation learning method is proposed using CNNs and rankpooling. Our method is also based on rank pooling <ref type="bibr" target="#b17">[18]</ref>, however, compared <ref type="bibr" target="#b17">[18]</ref> we learn the video representations end-to-end while being more efficient than <ref type="bibr" target="#b18">[19]</ref>. The end-to-end video classification method <ref type="bibr" target="#b18">[19]</ref> relies on computing the exact gradient of the rank pooling operator where as we argue that it is a good compromise to approximate the gradient of the rank pooling function considering exact method of <ref type="bibr" target="#b18">[19]</ref> has to rely on bi-level optimization <ref type="bibr" target="#b21">[22]</ref>. In this paper, we only take the first gradient step of the rank pooling operator which allows us to obtain a reasonable solution to the initial optimization problem. To the best of our knowledge such effective optimization trick has not been tried before in the context of CNN-based learning.</p><p>The impact of objects in action recognition is studied in <ref type="bibr" target="#b24">[25]</ref>. Fisher vector <ref type="bibr" target="#b48">[49]</ref> and VLAD descriptor based action recognition has shown promising results along with hand-crafted features <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Attributes <ref type="bibr" target="#b39">[40]</ref>, action-parts <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b72">[73]</ref>, hierarchy <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b73">[74]</ref>, trajectory pooled deep features <ref type="bibr" target="#b69">[70]</ref>, human pose <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b76">[77]</ref> and the context <ref type="bibr" target="#b41">[42]</ref> also have been used for action recognition. Overall, the good practices in action recognition is described in <ref type="bibr" target="#b74">[75]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DYNAMIC IMAGES</head><p>In this section we introduce the concept of dynamic image, which is a standard RGB image that summarizes the appearance and dynamics of a whole video sequence (section 3.1). Then, we we propose a fast approximation to accelerate the computation of dynamic images (section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing dynamic images</head><p>While CNNs can learn automatically powerful data representations, they can only operate within the confines of a specific hand-crafted architecture. In designing a CNN for video data, in particular, it is necessary to think of how the video information should be presented to the CNN. As discussed in section 2, standard solutions include encoding sub-videos of a fixed duration as multi-dimensional arrays or using recurrent architectures. Here we propose an alternative and more efficient approach in which the video content is summarized by a single still image. This image can then be processed by a standard CNN architecture such as CaffeNet <ref type="bibr" target="#b27">[28]</ref> or ResNeXt <ref type="bibr" target="#b75">[76]</ref>.</p><p>Summarizing the video content in a single still image may seem difficult. In particular, it is not clear how image pixels, which already contain appearance information in the video frames, could be overloaded to reflect dynamic information as well, and in particular the long-term dynamics that are important in action recognition.</p><p>We show here that the construction of Fernando et al. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> can be used to obtain exactly such an image. The idea of their work is to represent a video as a ranking function for its frames I 1 , . . . , I T . In more detail, let ψ(I t ) ∈ R d be a representation or feature vector extracted from each individual frame I t in the video. Let V t = 1 t t τ =1 ψ(I τ ) be time average of these features up to time t. The ranking function associates to each time t a score S(t|d) = d, V t , where d ∈ R d is a vector of parameters. The function parameters d are learned so that the scores reflect the rank of the frames in the video. Therefore, later times are associated with larger scores, i.e. ∀ {q, t} s.t. q t =⇒ S(q|d) &gt; S(t|d). Learning d is posed as a convex optimization problem using the RankSVM <ref type="bibr" target="#b57">[58]</ref> formulation:</p><formula xml:id="formula_0">d * = ρ(I 1 , . . . , I T ; ψ) = argmin d E(d), E(d) = λ 2 d 2 + (1) 2 T (T -1) × q&gt;t max{0, 1 -S(q|d) + S(t|d)}.</formula><p>The first term in this objective function is the usual quadratic regularizer used in SVMs. The second term is a hinge-loss softcounting how many pairs q t are incorrectly ranked by the Although fundamentally different both methodologically, as well as in terms of applications, they both seem to capture time in a similar manner.</p><p>scoring function. Note in particular that a pair is considered correctly ranked only if scores are separated by at least a unit margin, i.e. S(q|d) &gt; S(t|d) + 1.</p><p>The optimizer to eq. ( <ref type="formula" target="#formula_12">1</ref>) is written as a function ρ(I 1 , . . . , I T ; ψ) that maps a sequence of T video frames to a single vector d * . Since this vector contains enough information to rank all the frames in the video, it aggregates information from all of them and can be used as a video descriptor. The process of constructing d * from a sequence of video frames is known as rank pooling <ref type="bibr" target="#b17">[18]</ref>.</p><p>In <ref type="bibr" target="#b16">[17]</ref> the map ψ(•) used in this construction is set to be the Fisher Vector coding of a number of local features (histogram of gradients (HOG) <ref type="bibr" target="#b6">[7]</ref>, histogram of optical flow (HOF) <ref type="bibr" target="#b35">[36]</ref>, motion boundary histograms (MBH) <ref type="bibr" target="#b7">[8]</ref>, improved dense trajectories (IDT) <ref type="bibr" target="#b67">[68]</ref>) extracted from individual video frames. Here, we propose to apply rank pooling directly to the RGB image pixels instead. While this idea is simple, in the next several sections we will show that it has remarkable advantages.</p><p>The function ψ(I t ) is now an operator that stacks the RGB components of each pixel in image I t on a large vector. Alternatively, ψ(I t ) may incorporate a simple component-wise nonlinearity, such as the square root function √ • (which corresponds to using the Hellinger's kernel in the SVM). In all cases, the descriptor d * is a real vector that has the same number of elements as a single video frame. Therefore, d * can be interpreted as a standard RGB image. Furthermore, since this image is obtained by rank pooling the video frames, it summarizes information from the whole video sequence.</p><p>A few examples of dynamic images are shown in fig. <ref type="figure" target="#fig_0">1</ref>. Several observations can be made. First, interestingly the dynamic images tend to focus mainly on the acting objects, such as humans or horses in the "horse racing" action, or drums in the "drumming" action. On the contrary, background pixels and background motion patterns tend to be averaged away. Hence, the pixels in the dynamic image tend to focus on the identity and motion of the salient actors in videos, suggesting that they may contain the information needed to perform action recognition.</p><p>Second, we observe that dynamic images behave differently for actions of different speeds. For slow actions, like "blowing hair dry" in the first row of fig. <ref type="figure" target="#fig_0">1</ref>, the motion seems to be dragged Fig. <ref type="figure">3</ref>: The graph compares the approximate rank pooling weighting functions α t (for T = 5, T = 10 and T = 20 samples) of eq. ( <ref type="formula" target="#formula_7">3</ref>) using time-averaged feature frames V t to the variant eq. ( <ref type="formula" target="#formula_5">2</ref>) that ranks directly the feature frames ψ t as is. over many frames. For faster actions, such as "golf swing" in the second row of fig. <ref type="figure" target="#fig_0">1</ref>, the dynamic image reflects key steps of the action such as preparing to swing and stopping after swinging. For longer term actions such as "horse riding" in the third row of fig. <ref type="figure" target="#fig_0">1</ref>, the dynamic image reflects different parts of the video; for instance, the rails that appear as a secondary motion contributor are superimposed on top of the horses and the jockeys who are the main actors. Such observations were also made in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Last, it is interesting to note that dynamic images are reminiscent of some other imaging effects that convey motion and time, such as motion blur or panning, an analogy is illustrated in fig. <ref type="figure" target="#fig_1">2</ref>. While motion blur captures the time and motion by integrating over subsequent pixel intensities defined by the camera shutter speed, dynamic images capture time by integrating and reordering the pixel intensities over time within a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fast dynamic image computation</head><p>Computing a dynamic image entails solving the optimization problem of eq. ( <ref type="formula" target="#formula_12">1</ref>). While this is not particularly slow with modern solvers, in this section we propose an approximation to rank pooling which is much faster and works as well in practice. Later, this technique, which we call approximate rank pooling (ARP), will be critical in incorporating rank pooling in intermediate layers of a deep CNN and to allow back-prop training through it.</p><p>The derivation of ARP is based on the idea of considering the first step in a gradient-based optimization of eq. ( <ref type="formula" target="#formula_12">1</ref>). Starting with d = 0, the first approximated solution obtained by gradient descent is</p><formula xml:id="formula_1">d * = 0 -η∇E(d)| d= 0 ∝ -∇E(d)| d= 0 for any η &gt; 0, where ∇E( 0) ∝ q&gt;t ∇ max{0, 1 -S(q|d) + S(t|d)}| d= 0 = q&gt;t ∇ d, V t -V q = q&gt;t V t -V q .</formula><p>We can further expand d * as follows</p><formula xml:id="formula_2">d * ∝ q&gt;t V q -V t = T t=1 β t V t</formula><p>where β t are scalar coefficients. By expanding the sum</p><formula xml:id="formula_3">q&gt;t V q -V t =(V 2 -V 1 ) +(V 3 -V 1 ) + (V 3 -V 2 )</formula><p>. . .</p><formula xml:id="formula_4">+(V T -V 1 ) + (V T -V 2 ) + . . . + (V T -V T -1 ).</formula><p>one can simply see that each V t with positive or negative sign occurs (t -1) and (T -t) times respectively. Now we can write β t in terms of time and video length:</p><formula xml:id="formula_5">β t = (t -1) -(T -t) = 2t -T -1.<label>(2)</label></formula><p>The time average vectors V t can be written in terms of feature vectors ψ t and d * can be written as a linear combination of ψ t</p><formula xml:id="formula_6">d * ∝ β t V t = α t ψ(I t )</formula><p>where the coefficients α t are given by</p><formula xml:id="formula_7">α t = 2(T -t + 1) -(T + 1)(H T -H t-1 ),<label>(3)</label></formula><p>where H t = t i=1 1/t is the t-th Harmonic number and H 0 = 0. Note that the values of α t are constant for a given video length (T ) and thus do not depend on the content of video. The α t coefficients can be derived from the observation that each ψ t occurs T i=t β i H i times in the sum. Hence the rank pooling operator reduces to</p><formula xml:id="formula_8">ρ(I 1 , . . . , I T ; ψ) = T t=1 α t ψ(I t ).<label>(4)</label></formula><p>which is a weighted combination of the data points (ψ(I t )). In particular, the dynamic image computation reduces to accumulating the video frames after pre-multiplying them by α t . The function α t is illustrated in fig. <ref type="figure">3</ref>. An alternative construction of the rank pooling does not compute the intermediate average features V t = (1/t) T q=1 ψ(I q ), but uses directly individual video features ψ(I t ) in the definition of the ranking scores <ref type="bibr" target="#b0">(1)</ref>. In this case, the derivation above results in a weighting function of the type</p><formula xml:id="formula_9">α t = 2t -T -1<label>(5)</label></formula><p>which is linear in t. The two scoring functions eq. ( <ref type="formula" target="#formula_7">3</ref>) and eq. ( <ref type="formula" target="#formula_5">2</ref>) are compared in fig. <ref type="figure">3</ref> and in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DYNAMIC MAPS NETWORKS</head><p>In the previous section we have introduced the concept of dynamic image as a method to pool the information contained in a number of video frames into a single RGB image. Here, we notice that every layer of a CNN produces as output a feature map which, having a spatial structure similar to an image, can be used in place of video frames in this construction. We call the result of applying rank pooling to such features a dynamic feature map, or dynamic map in short. In the rest of the section we explain this construction can be incorporated in a CNN as a rank-pooling layer (section 4.1) and how ARP can be used to accelerate it significantly as well as to perform back-propagation for end-to-end learning (section 4.2). denote the feature maps computed at the l -1 layers of the architecture, one for each of the T video frames. Then, we use the rank pooling equation (1) to aggregate these maps into a single dynamic map,</p><formula xml:id="formula_10">a (l) = ρ(a (l-1) 1 , . . . , a (l-1) T ).<label>(6)</label></formula><p>Note that, compared to eq. ( <ref type="formula" target="#formula_12">1</ref>), we dropped the term ψ; since networks are already learning feature maps, we set this term to the identity function. The dynamic image network is obtained by setting l = 1 in this construction.</p><p>Rank pooling layer (RankPool) &amp; backpropagation. In order to train a CNN with rank pooling as an intermediate layer, it is necessary to compute the derivatives of eq. ( <ref type="formula" target="#formula_10">6</ref>) for the backpropagation step. We can rewrite eq. ( <ref type="formula" target="#formula_10">6</ref>) as a linear combination of the input data V 1 , . . . , V T , namely</p><formula xml:id="formula_11">a (l) = T t=1 β t (V 1 , . . . , V T )V t<label>(7)</label></formula><p>In turn, V t is the temporal average of the input features and is therefore a linear function V t (a</p><formula xml:id="formula_12">(l-1)<label>1</label></formula><p>, . . . , a (l-1) t</p><p>). Substituting, we can rewrite a (l) as</p><formula xml:id="formula_13">a (l) = T t=1 α t (a (l-1) 1 , . . . , a (l-1) T )a (l-1) t .<label>(8)</label></formula><p>Unfortunately, we observe that due to the non-linear nature of the optimization problem of equation ( <ref type="formula" target="#formula_12">1</ref>), the coefficients β t , α t depend on the data a (l-1) t themselves. Computing the gradient of a (l) with respect to the per frame data points a (l-1) t is a challenging derivation. Hence, using dynamic maps and rank pooling directly as a layer in a CNN is not straightforward. This problem is solved in the next section.</p><p>We note that the rank pooling layer (RankPool) constitutes a new type of portable convolutional network layer, just like a max-pooling or a ReLU layer. It can be used whenever dynamic information must be pooled across time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximate dynamic maps</head><p>Due to the intrinsic noisy nature of image and video data, computing dynamic images/maps to a high degree of accuracy may not be necessary in practice. In fact, accurate optimization of eq. ( <ref type="formula" target="#formula_10">6</ref>) has two disadvantages: optimization is slow and computing the derivative for backpropagation is difficult. This is especially important in the context of CNNs, where efficient computation and end-to-end learning is extremely important for training on large datasets.</p><p>To this end we replace once again rank pooling with ARP. ARP significantly accelerates the computations, up to a factor of 45 as we show later in the experiments. Furthermore, and more importantly, the ARP is a linear combination of frames, where the per frame coefficients are given by eq. ( <ref type="formula" target="#formula_7">3</ref>). These coefficients are independent of the frame features V t and ψ(I t ). Hence, the derivative of ARP is simpler and fast to compute:</p><formula xml:id="formula_14">∂ vec a (l) ∂(vec a (l-1) t ) = α t I (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where I is the identity matrix and vec denotes the tensor stacking operator <ref type="bibr" target="#b30">[31]</ref>. Formally, the same expression can be obtained by computing the derivative of eq. ( <ref type="formula" target="#formula_13">8</ref>) pretending that the coefficients α t do not depend on the video frames. We conclude that using ARP in the context of CNNs speeds up evaluation and dramatically simplifies optimization through backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Single and multiple dynamic map networks</head><p>Dynamic images and maps can be computed over an arbitrary length video sequences. Here we propose a few deep network variants that can use a single or multiple dynamic images or maps integrated over different video sequence durations.</p><p>Single Dynamic Image/Map (SDI/SDM). In the first scenario, a dynamic image/map summarizes an entire video sequence. By training a CNN on top of such dynamic images, the method implicitly captures the temporal patterns contained in the video. However, since the CNN is still applied to images, we can start from a CNN pre-trained for still image recognition, such as CaffeNet pre-trained on the ImageNet ILSVRC data, and fine-tune it on a dataset of dynamic images. Fine-tuning allows the CNN to learn features that capture the video dynamics without the need to train the architecture from scratch. This is an important benefit of our method because training large CNNs require millions of data samples which may be difficult to obtain for videos.</p><p>Multiple Dynamic Images/Maps (MDI/MDM). While finetuning requires less annotated data than needed for training a CNN from scratch, the domain gap between natural and dynamic images is sufficiently large that fine-tuning may still requires a relatively large annotated dataset. Unfortunately, as noted above, in most cases there are only a few videos available for training.</p><p>In order to address this potential limitation, in the second scenario we propose to generate multiple dynamic images/maps from each video by breaking it into segments. In particular, for each video we extract multiple partially-overlapping segments of duration τ and with stride s. In this manner, we create multiple video segments per video, essentially multiplying the dataset size by a factor of approximately T /s, where T is the average number of frames per video. This can also be seen as a data augmentation step, where instead of mirroring, cropping, or shearing images we simply take a subset of the video frames. From each of the new video segments, we can then compute a dynamic image/map to train the CNN, using as ground truth class information of each subsequence the class of the original video.</p><p>The use of multiple, shorter subsequences also reduces the amount of temporal information that is squeezed in a single dynamic image or map, which can be beneficial in modeling highly-complex videos with many temporal changes.</p><p>The resulting network architecture (fig. <ref type="figure">4</ref>.(c)) takes a sequence of frames from a video as input and splits them into fixed length subsequences, generating a dynamic image/map for each one subsequence. The last convolutional layer is followed by a "temporal pooling" layer which merges the dynamic images/maps into one. We evaluate different choices for this temporal pooling layer in the experiments section. Note that, while (fig. <ref type="figure">4</ref>.(c)) show the case of a multiple dynamic image (MDM) network, the figure is easily adapted to a multiple dynamic map (MDM) by moving the approximate rank pooling layer at higher layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Four-stream architecture</head><p>The concept of dynamic image can be applied to different video input modalities such as depth and optical flow data. As the combination of RGB and optical flow has been shown to be very useful in action recognition in the two-stream architecture of Simonyan and Zisserman <ref type="bibr" target="#b56">[57]</ref>, we experiment here with a similar idea and propose a new four-stream architecture for action recognition (fig. <ref type="figure" target="#fig_3">5</ref>). As for the two-stream model, this architecture combines RGB and optical flow data streams, either directly or by computing dynamic images/maps. This means that the network processes static appearance and visual context information from the RGB stream, low-level motion information from the optical flow stream, mid-level motion information from dynamic images computed from RGB data (dynamic image stream), and higherlevel motion information from dynamic images computed from optical flow data (dynamic optical flow stream).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this part, we first give details for the video classification benchmarks (section 5.1) and experimental setup (section 5.2) used in the paper. Then, we thoroughly evaluate and compare the SDI and MDI architectures, ARP, dynamic maps and parametric pooling. Finally we show that our results are on par and complementary to the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We benchmark on two popular datasets for action recognition: UCF101 [60] and HMDB51 <ref type="bibr" target="#b32">[33]</ref>. UCF101. The UCF101 dataset <ref type="bibr" target="#b59">[60]</ref> comprises of 101 human action categories, like "Apply Eye Makeup" and "Rock Climbing" and contains over 13, 320 videos. The videos are realistic and relatively clean. They contain little background clutter, a single action label, and are trimmed around that action (thus almost all frames relate to the labelled action). Performance is evaluated in term of average recognition accuracy over three data splits provided by the authors. HMDB51. The HMDB51 dataset <ref type="bibr" target="#b32">[33]</ref> consists of 51 human action categories, such as "backhand flip" and "swing baseball bat" and spans over 6, 766 videos. The videos are realistic (downloaded from Youtube) each containing a single human action. This dataset is split in three parts and accuracy is averaged over all three parts, similar to UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation details</head><p>This section describes the details of the models used in the experiment. Full source code and models using the MatConvNet toolbox <ref type="bibr" target="#b65">[66]</ref> are available online. 2 . Network: We use two deep neural network architectures. The first is the BVLC reference model (CaffeNet) <ref type="bibr" target="#b27">[28]</ref> which is reasonably efficient to train and evaluate. We use this model to analyze various design decisions such as different pooling methods and their point of application in the network. After identifying the most promising settings, we use them with ResNeXt-50 and ResNeXt-101 <ref type="bibr" target="#b75">[76]</ref> (using the 32 × 4d variant). All models are are pretrained on ImageNet ILSVRC 2012 <ref type="bibr" target="#b52">[53]</ref> and fine-tuned by using stochastic gradient descent. During training, we randomly flip images, jitter image size and aspect ratio and rescale it to 224 × 224. In test time, we use a single center crop for each dynamic image. RGB and optical flow: We take each video and convert it into frames at its original frame rate. In addition to the extracted RGB frames, we also precompute optical flow using the method of <ref type="bibr" target="#b79">[80]</ref> and store the flow fields as JPEG images after clipping displacement to 20 pixel and rescaling the flow values in the range 0 to 255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Rank pooling</head><p>Max and average pooling. First, we evaluate the "single dynamic image" (SDI) setting (section 4.3), namely extracting a single dynamic image for each video sequence. We compare SDI with the two most popular pooling methods, namely mean and max pooling, obtaining alternative video summary images. All pooling methods are applied offline and the resulting summary images are cached. Dynamic images, in particular, are computed using the SVR software of <ref type="bibr" target="#b57">[58]</ref>. Then, the CaffeNet model (see fig. Results are shown in table <ref type="table" target="#tab_0">1</ref>. We observe that DIs achieve the highest accuracy and conclude that rank pooling is preferable to mean and max pooling for video summarization.</p><p>Motion History Images (MHI). MHI <ref type="bibr" target="#b2">[3]</ref> is a direct competitor to our dynamic image as it also generates a single image summarizing a video. An MHI is a scalar-valued image built such that pixels that changed more recently in the video are brighter, so that pixel intensity is a function of the changes observed at that point. A qualitative comparison between dynamic images (generated using ARP) and MHIs in fig. <ref type="figure" target="#fig_5">6</ref>. The figure shows, from top to bottom, a representative frame for a given video and the corresponding MHI and DI. We first note that DIs provide more detailed representation of the videos, as the range of intensity values are not limited with the number frames as in MHIs. Second, DIs are more robust to moving viewpoint and background. Finally, MHIs can only represent the motion gradient in object boundaries in contrast to DIs.</p><p>MHIs were originally designed for action recognition. A set of moment-based descriptors are extracted from a set of MHIs, then a distance metric over each action category is learnt and classification is performed using the computed metrics. Such a pipeline is not competitive for the modern datasets, and, thus, we adapt it to fit modern pipelines. Similar to our method, we compute a single MHI for each video, which we use as input to the CaffeNet model and train on the UCF101 dataset. For this representation, we obtain 46.6% accuracy in the first split of the UCF101 dataset which is significantly lower (-10%) than what we obtain with SDI. This suggests that the qualitative advantages translate in better quantitative classification performance as well.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Approximating rank pooling</head><p>Next, we compare the rank pooling (RP) to approximate rank pooling (ARP) in terms of speed (frames per second) and pairwise ranking accuracy, which is a common measure for evaluating learning-to-rank methods. Hence the goal is to assess the ability of ARP to sort video frames correctly compared to its "exact" counterpart RP. More importantly, we also compare RP and ARP in terms of overall recognition performance to see if the approximation impacts the ability of the method to represent video effectively.</p><p>To do so, we apply RP and ARP to the videos from the first test split of UCF101 dataset which contains 3783 videoclips in varying lengths. We report results with the mean and the standard deviations in table 2. Interestingly, ARP achieves slightly better pairwise ranking accuracy than RP (96.5% vs 95.5%). This can be attributed to the fact that ARP optimizes the actual target pairwise ranking loss (see eq. ( <ref type="formula" target="#formula_12">1</ref>)), while the Support Vector Machine Regression solves a regression problem from RGB values to frame index. Although both pooling obtain high ranking performances overall, we observe that both method have relatively lower performance to correctly rank frames from categories with periodic motion and static background such as "PushUp" (82.5%), "JugglingBalls" (92.7%) and "PlayingGuitar" (92.7%) have lower ranking accuracies.</p><p>In terms of run-time, RP takes a second to learn a dynamic window stride Accuracy We also compare RP and ARP in terms of classification accuracy on the first split of the UCF101 dataset. Here the single dynamic image representation with ARP obtains 55.2% accuracy which is 2.7 points lower than RP. ARP is however much faster than RP and still significantly outperforms mean (52.6 points) and max pooling (48 points).</p><p>Due to the excellent accuracy and speed, in the rest of the paper we will use by default approximate dynamic images, computed with using ARP instead of RP, unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Single vs multiple dynamic image networks</head><p>So far, we used a single dynamic image to represent each video using the CNN architecture of fig. <ref type="figure">4</ref>.(a). In this section we show that splitting a video into multiple sub-sequences and encoding each of them as a dynamic image achieves better classification accuracy than using a single dynamic image. To do so, we use the multiple dynamic image (MDI) network of fig. <ref type="figure">4</ref>.(c). After applying the ARP to each sub-sequences and extracting sequencespecific features, this CNN uses an additional temporal maxpooling layer to merge the resulting convolutional features into one (denoted temp-pool in fig. <ref type="figure">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.(c)).</head><p>First, we evaluate the effect of window size (τ ) and stride (s) which determine the sub-sequence length and frame sampling frequency, respectively (see section 4.3 for the definition of such parameters). Note that using video-length window size is equivalent to computing a single dynamic image per video and a single frame window corresponds to using RGB frames as input. As shown in table 3, using a medium-length windows of 10 samples with 40% overlap yields the best classification performance. However, while MDI is more accurate than SDI, it is also slower due to the fact that MDI extract features from a number of dynamic images proportional to the video length. In practice, MDI is 5 times slower than SDI for a medium-length video, so SDI can be preferable when speed is paramount.</p><p>Figure <ref type="figure" target="#fig_7">8</ref> shows several dynamic images computed by varying window sizes (from top to bottom: 10 samples, 50 samples and whole video length). As more frames are used to generate dynamic images, more pixels are activated to represent longer motions. For instance, in fig. <ref type="figure" target="#fig_7">8</ref>.(a) and (b) using longer windows results in images that capture more revolutions of wheels and hula-hoops. We also notice that the dynamic image representation fails to  Finally, we evaluate different choices for the temporal pooling layer temp-pool. Using mean pooling, max pooling and APR for this layer results in 66.2, 68.3 and 65.2% mean video classification accuracies respectively. The fact that max pooling gives the best result can be explained with the fact that max pooling is invariant to the temporal position of the target action instance and does not require any alignment of start and end of action instances across different videos. This is in contrast with encoding shorter video sequences, where we demonstrated that ARP is better than both sum and max pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Dynamic maps</head><p>So far we used ARP as the first layer of the CNN to generate dynamic images. Here we move ARP deeper down the network to generate dynamic maps instead. Table <ref type="table" target="#tab_4">4</ref> reports the mean class accuracy on the UCF101 and HMDB51 datasets, where "convX" corresponds to positioning the approximate rank before the X-th convolutional layer. For instance "conv1" means that ARP is at applied directly to the input images. Each network is trained in an end-to-end manner with multiple dynamic maps (see the network architecture in fig. <ref type="figure">4</ref>.(b)). Please note that such an end-to-end training is only possible because ARP enables backpropagation, which would be difficult to do with RP (section 4.2).</p><p>We see that locating ARP before "conv3" performs slightly better than "conv1" and "conv2" and the classification performance starts degrading after this level. The degradation can be explained with the fact that the convolutional features in the earlier layers, which capture low-level structures such as edges, blobs and patterns, are more useful to express the motion and dynamics of a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Parametric pooling</head><p>As shown in section 4.2, ARP is a fixed linear combination of input images where the mixing coefficients α t are given by the formula eq. ( <ref type="formula" target="#formula_8">4</ref>) derived from the ranking objective. A natural question is whether better coefficients α t could be obtained by optimise the target task of video classification end-to-end instead of using ranking as a proxy task. We call this setting parametric pooling. Similarly to ARP, parametric pooling takes a number of frames or feature tensors from a video as input and pools them into a single frame/feature tensor. In contrast to ARP, for which eq. ( <ref type="formula" target="#formula_8">4</ref>) applies to videos of any length, parametric pooling requires videos of a fixed length.</p><p>In practice, parametric pooling can be implemented as a subnetwork which is composed of a number of convolutional layers 0162-8828 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  Best seen in color.</p><p>followed by non-linear operators. In case of a single convolution and ReLU layers, it can be formulated as</p><formula xml:id="formula_16">φ(I 1 , . . . , I T ; ψ) = σ T t=1 α t ψ(I t ) + b t</formula><p>where σ is the ReLU function and b t is a bias parameter. Scalar multiplication and sum over time by (α t , b t ) can be interpreted as a fully-connected layer with one scalar output applied to temporal slices of the data tensors. This can be extended to use several temporal layers, as we do next.</p><p>In table <ref type="table" target="#tab_6">5</ref> we evaluate the performance of the proposed parametric pooling on the UCF101 and HMDB51 datasets for two settings PP1 and PP2, using one or two layers in the parametric pooler. Similar to the dynamic map experiments, we apply parametric pooling after the ReLU layer following the specified convolutional layer and parametric pooling is followed by an additional ReLU. The parameters of the networks are trained in end-to-end fashion with the pooling coefficients. The single layer parametric pooling (PP1) is implemented as a 10 × 1 temporal convolutional layer over 10 frames that belong to the same video. This also means that the number of frames is fixed to 10 for each  video subsequence (see table <ref type="table">3</ref>). PP2 extend PP1 by considering a chain of 10 × 10 and 10 × 1 temporal fully-connected layers (with ReLU in between). Table <ref type="table" target="#tab_6">5</ref> shows that PP1 and PP2 performance is similar to ARP with the exception that parametric pooling performs worse on the raw video frames ("conv1").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Dynamic images with deeper networks</head><p>In the previous experiments we have used the fast CaffeNet architecture to explore certain design decisions. Next, we evaluate action recognition with dynamic images using more recent networks such as ResNeXt-50 <ref type="bibr" target="#b75">[76]</ref>. Results are reported in table <ref type="table">6</ref>.</p><p>We make several observations. First, switching from CaffeNet to ResNeXt-50 boosts performance significantly, up to 15% for UCF101 and 20% for HMDB51. This is in line with the top-1 error rate in the validation split of the ImageNet dataset i.e. 42.6 and 22.6% for CaffeNet and ResNeXt-50 respectively. Second, SI and DI streams are highly complementary both for CaffeNet and ResNeXt-50. Third, while for UCF101 the performances of SI and DI are on par, for HMDB51 the DI stream alone scores considerably higher (4%). The reason is that in UCF101 many videos can be recognized from only the static context and background, while in HMDB51 backgrounds are more complex and dynamic.</p><p>We further break down the comparison of static and dynamic image networks on a per-class basis. In order to do so, we compute the top 3 classes based on the relative performances for SI and DI. DI performs better for "Nunchucks", "Jumping-Jack", "WallPushups", where longer term motion is dominant and discriminating between motion patterns is important. SI works better for classes such as "HammerThrow", "Shotput" and "BreastStroke", where context is already quite revealing (e.g. swimming pool for "BreastStroke") and dynamics are not enough themselves to distinguish an action type from another (e.g. DI confuses "BreastStroke" with "FrontCrawling" and "Rowing"). TABLE 6: Classification accuracy (%) with dynamic images when using CaffeNet <ref type="bibr" target="#b27">[28]</ref> and deeper convolutional network architectures, specifically ResNeXt-50 <ref type="bibr" target="#b75">[76]</ref>. As we can observe, dynamic images can reap all the benefits of deeper architectures of modern convolutional neural networks.</p><p>We conclude that dynamic images are useful recognition of actions with characteristic motion patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Dynamic optical flow</head><p>So far, we use RGB frames as input to our dynamic image/map networks. Inspired by the success of two stream networks <ref type="bibr" target="#b56">[57]</ref> which combine both RGB and optical flow images, we extend our models to include optical flow images as well. This, in a similar fashion to the static image (SI) and dynamic image (DI) networks, we introduce optical flow (OF) and dynamic optical flow (DOF) streams in our model. DOF is obtained by applying ARP to 10 optical flow frames. Differently from DI, a DOF contains two channels (corresponding to horizontal and vertical flow) rather than 3 RGB channels. Figure <ref type="figure" target="#fig_8">9</ref> shows samples from different videos and action categories for SI, DI, OF and DOF streams. We observe two main differences between the raw optical flow and dynamic optical flow samples (third and last rows respectively). We first see that DOF can capture longer-term motion than OF. This is expected as optical flow by definition captures the motion information between only subsequent frames. For instance, DOF can represent longer temporal history for the billiard balls and a longer span for a "punching" action (see in fig. <ref type="figure" target="#fig_8">9</ref>). Second DOF  can also represent higher order statistics such as "velocity" of optical flow.</p><p>In the examples, one can note the forward acceleration of the boxer and of the toothbrush and the upward acceleration of the weightlifter. Table <ref type="table" target="#tab_8">7</ref> compares the action classification performance using ResNeXt-50 for the OF and DOF streams as well as their their combination. First, we note that combining optical flow with dynamic optical flow improves the performance of the individual streams, confirming that the two features are complementary. Second, ARP works as well for optical flow image as for RGB images. In fact, dynamic optical flow alone achieves a very high accuracy on UCF101 (86.6%). Last we show that OF and DOF streams are complimentary and using two-stream OF and DOF leads to 89.1% and 62.6% in the UCF101 and HMDB51 and obtains significant improvement over the individual streams.</p><p>Next we break down our analysis on a per-class basis, focusing on the top 3 classes with the highest relative performances for OF and DOF. While action categories characterised by longer term motion and higher order statistics such as "Nunchucks", "HandstandWalking" and "JumpingJack" are the best for DOF, actions characterised by shorter motions such as "BreastStroke", "HighJump", "BlowDryHair" are best for OF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Four stream networks</head><p>Next, we evaluate the combinations of SI, DI, OF and DOF streams. While these networks are trained individually, at test Next, we break down the analysis on a per-class basis looking at the worst performing classes in the UCF101 dataset for the fourstream ResNeXt-50 model. The most challenging five categories for this model are "PizzaTossing", "Lunges", "HammerThrow", "ShavingBeard" and "BrushingTeeth" with the respective accuracies of 74, 74.6, 77.2, 78.7 and 80.2%. Hence the four-stream architecture may fail to distinguish action categories separated by subtle differences. For instance, "Lunges", "CleanAndJerk", "BodyWeightSquats" may all involve subactions like lifting or lowering a barbell and kneeing, and are mostly distinguished by the order between such subactions. A possible solution could be to discover such subactions during learning and model their order by using rank pooling. Other similar actions, such as "Shaving-Beard", "BrushingTeeth" and "ApplyLipstick" that involve similar motions may be confused in some cases. Incorporating specialized networks for facial and human body pose analysis may help in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">State-of-the-art comparisons</head><p>Table <ref type="table" target="#tab_10">9</ref> depicts a quantitative comparison of our four-stream network (SI+DI+OF+DOF) to the state-of-the-art on the UCF101 and HMDB51. In addition to ResNeXt-50 model, here we also train our model with the deeper ResNeXt-101 <ref type="bibr" target="#b75">[76]</ref> and report its performance as well. In order to provide a fair comparison, we split the table into two parts, the ones incorporate their methods with the handcrafted improved dense trajectories (iDT) <ref type="bibr" target="#b67">[68]</ref> to improve their final accuracy, and those that do not.</p><p>First we look at the ones without iDT and see that the proposed four-stream network obtains the highest accuracy with 95.4% and 96.0% with ResNeXt-50 and ResNeXt-101 respectively. We outperform the state-of-the-art methods with a significant margin with the exception of I3D+ <ref type="bibr" target="#b3">[4]</ref> (98% and 80.7%). Note that this Method UCF101 HMDB51</p><p>CNN-hid6 <ref type="bibr" target="#b80">[81]</ref> 79.3 -Comp-LSTM <ref type="bibr" target="#b61">[62]</ref> 84.3 44.0 C3D+SVM <ref type="bibr" target="#b64">[65]</ref> 85.2 -2S-CNN <ref type="bibr" target="#b78">[79]</ref> 88.0 59.4 FSTCN <ref type="bibr" target="#b62">[63]</ref> 88.1 59.1 2S-CNN+Pool <ref type="bibr" target="#b78">[79]</ref> 88.2 -Objects+Motion(R * ) <ref type="bibr" target="#b25">[26]</ref> 88.5 61.4 2S-CNN+LSTM <ref type="bibr" target="#b78">[79]</ref> 88.6 -TDD <ref type="bibr" target="#b70">[71]</ref> 90.3 63.2 Temporal Segment Networks <ref type="bibr" target="#b71">[72]</ref> 94.2 69.4 Two-Stream I3D <ref type="bibr" target="#b3">[4]</ref> 93. FV+IDT <ref type="bibr" target="#b47">[48]</ref> 84.8 57.2 SFV+STP+IDT <ref type="bibr" target="#b47">[48]</ref> 86.0 60.1 FM+IDT <ref type="bibr" target="#b46">[47]</ref> 87.9 61.1 MIFS+IDT <ref type="bibr" target="#b34">[35]</ref> 89.1 65.1 CNN-hid6+IDT <ref type="bibr" target="#b80">[81]</ref> 89.6 -C3D Ensemble+IDT (Sports-1M) <ref type="bibr" target="#b64">[65]</ref> 90.1 -C3D+IDT+SVM <ref type="bibr" target="#b64">[65]</ref> 90.4 -TDD+IDT <ref type="bibr" target="#b70">[71]</ref> 91.5 65.9 Sympathy <ref type="bibr" target="#b8">[9]</ref> 92.5 70.4 Two-Stream Fusion+IDT <ref type="bibr" target="#b14">[15]</ref> 93.5 69.2 ST-ResNet+IDT <ref type="bibr" target="#b13">[14]</ref> 94 Comparison with the state-of-the-art in terms of mean multi-class accuracy (%). Our method outperforms the state stateof-the-art. Please note that better performing Two-Stream I3D+ <ref type="bibr" target="#b3">[4]</ref> has been pre-trained on a large-scale video dataset, Kinetics300k.</p><p>method is pre-trained on additional 300,000 videos and relies on a two-stream variant. When trained on the UCF101 and HMDB51 alone, the I3D is outperformed by our four-stream architecture (93.4% and 66.4%). In any case, the I3D architecture can also incorporate dynamic images and enjoy a further boost. Remarkably, our method using only static and dynamic images and no optical flow still scores an impressive 90.6%, outperforming most competitors who rely on handcrafted optical flow input. The four-stream architecture outperforms all previous methods even after incorporating the improved trajectory technique. This is encouraging as most of the best existing methods require improved trajectories to reach state-of-the-art accuracies. Furthermore, our four stream models do not improve significantly after the inclusion of improved trajectories (95.5% → 96.0% and 72.5% → 74.9%), showing that the vast majority of the benefit is intrinsic to the proposed architecture. This is interesting, as our four stream models are one of the first models together with I3D <ref type="bibr" target="#b3">[4]</ref> which manages to surpass the 95% and 70% barriers on respective UCF101 and HMDB51 without relying on handcrafted features.</p><p>We show that long term dynamics encoded in dynamic images and dynamic optical flow images are complementary to the short term RGB and optical flow images. Similarly, Temporal Segment Networks (TSN) <ref type="bibr" target="#b71">[72]</ref> focus on the long-term aspect of actions by pooling the information in the later layers of the network. We believe that TSN <ref type="bibr" target="#b71">[72]</ref> would be orthogonal to our model, which pools information in the input level and this would allow for encoding even longer dynamics for TSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have introduced the concepts of dynamic images and dynamic optical flow, powerful and yet simple video representations that summarizes videos into single images. Dynamic images and dynamic optical flow are able to encode the gist of the video first and second order dynamics, allowing for excellent action recognition performance. As they effectively comprise inputs to models, they can be used with any of the existing or future CNN architectures. In fact, applying dynamic images and dynamic optical flow with recent very deep convolutional neural networks enables end-to-end video action recognition with excellent results. We, furthermore, introduce a novel temporal pooling layer called approximate rank pooling, which accelerates dynamic image computation, while generalizing the idea to any intermediate feature map computed by a CNN. Approximate rank pooling allows for allowing back propagation of the gradients for learning. Furthermore, we proposed a novel four-stream architecture that combines complementary static and dynamic information from RGB and optical flow frames. Experiments on public action recognition benchmarks clearly demonstrate the benefits of th four-stream architecture, computing dynamic images onr RGB and optical flow images and achieving impressive performance despite their implementation simplicity.</p><p>Dynamic images have some notable limitations as well. Even though they are good at capturing smooth dynamics, they are less good at handling abrupt changes in very complex video sequences. Second, appearance and dynamics are highly correlated in the spatial and temporal domain, and it could be more efficient to build representations after decorrelating spatio-temporal volumes. Third, dynamic images operate at a single level of temporal pooling with a fixed window size. In future we plan to explore applying dynamic pooling at multiple levels of abstraction by allowing the network to adapt according to the complexity of temporal data. Furthermore, it would be interesting to evaluate extending the representation to modalities other than RGB and optical flow, such as depth and multi-spectral video data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Examples of dynamic images summarizing short video sequences as still images. They provide a simple, powerful, and efficient representation of videos for action recognition. Can you guess what actions are visualized? 1</figDesc><graphic coords="1,321.94,448.43,75.60,56.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Left column: dynamic images. Right column: motion blur.Although fundamentally different both methodologically, as well as in terms of applications, they both seem to capture time in a similar manner.</figDesc><graphic coords="4,320.75,132.06,115.92,86.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 : 4 . 1</head><label>441</label><figDesc>Fig. 4: Illustration of various dynamic image/map network architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The illustration of four stream dynamic image architecture that combines RGB data, Optical Flow with Dynamic Images and Dynamic Optical Flow.</figDesc><graphic coords="7,324.60,43.70,226.80,128.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>4.(a)) is fine-tuned for each summary image variant using the first train/test split of the UCF101 dataset. In order to generate dynamic images, we follow the pipeline suggested in Fernando et al. [17]: i) square root the RGB pixel values (ψ(•)), ii) use a time varying mean representation of √ •, iii) learn ranking hyperplanes for each channel, iv) scale the computed dynamic images into [0, 255] range again. The dynamic images are precomputed and fed into a CNN as input in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Comparing Dynamic Images (DI) to Motion History Images (MHI) [3].The top row shows representative frames from different videos, middle and bottom rows depict MHI and DI of corresponding videos respectively. While both methods can represent the evolution of pixels along time, our method produces more interpretable images which are more robust to long-range and background motion. Method fps Ranking Acc. Classification Acc. RP 131 95.5 ± 0.6 57.9 ARP 5920 96.5 ± 0.5 55.2</figDesc><graphic coords="8,387.67,195.04,75.52,75.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 0Fig. 7 :</head><label>107</label><figDesc>Fig.7: Comparison between four score profiles and pairwise ranking accuracies (%) of ranking functions for approximate rank pooling (ARP) and rank pooling (RP). Generally the approximate rank pooling follows the trend of rank pooling.</figDesc><graphic coords="10,60.82,188.91,75.40,75.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Visual analysis of different window sizes (τ ) on dynamic images. The top, middle and bottom rows depict dynamic images for τ = 10, τ = 50 and τ =(whole video length) respectively. Best seen in color.</figDesc><graphic coords="10,60.82,339.72,75.40,75.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Visualizing static images (SI), dynamic images (DI), optical flow (OF) and dynamic optical flow (DOF) in each row respectively. Best seen in color. SI DI SI+DI UCF101 CaffeNet [28] 68.5 70.9 76.5 UCF101 ResNeXt-50 [76] 87.6 86.6 90.6 HMDB51 CaffeNet [28] 36.0 37.2 39.7 HMDB51 ResNeXt-50 [76] 53.5 57.3 61.3</figDesc><graphic coords="11,86.70,240.87,87.72,65.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Comparing the performance of various single image video representation methods on split-1 the UCF101 dataset in terms of mean class accuracy (%).</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Mean Image</cell><cell>52.6</cell></row><row><cell>Max Image</cell><cell>48.0</cell></row><row><cell>Motion History Image</cell><cell>46.6</cell></row><row><cell>Dynamic Image</cell><cell>57.2</cell></row></table><note><p>2. https://github.com/hbilen/dynamic-image-nets</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note><p>Approximate rank pooling vs rank pooling in terms of speed, ranking accuracy (%) and classification accuracy (%)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Classification accuracy (%) for dynamic image and map networks at different depths on the UCF101 and HMDB51 datasets. "conv1" corresponds to placing the ARP as the first layer of network.</figDesc><table><row><cell>capture very complex motion in fig. 8.(c) as the number of frames</cell></row><row><cell>increases too much.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 :</head><label>5</label><figDesc>Classification accuracy (%) for approximate rank pooling and parametric pooling at different depths on the UCF101 and HMDB51 datasets. "conv1" corresponds to placing the ARP or PPX as the first layer of network. PP1 and PP2 correspond to 1 and 2 layer sub-networks respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc></figDesc><table /><note><p>Optical flow and dynamic optical flow streams: A twostream ResNeXt-50 architecture for action classification in terms of mean multi-class accuracy (%).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>Combinations of various RGB and optical flow streams with ResNeXt-50 in terms of mean multi-class accuracy (%).</figDesc><table><row><cell></cell><cell cols="2">HMDB51 UCF101</cell></row><row><cell>SI</cell><cell>53.5</cell><cell>87.6</cell></row><row><cell>DI</cell><cell>57.3</cell><cell>86.6</cell></row><row><cell>OF</cell><cell>55.8</cell><cell>84.9</cell></row><row><cell>DOF</cell><cell>58.9</cell><cell>86.6</cell></row><row><cell>SI+OF</cell><cell>67.5</cell><cell>93.9</cell></row><row><cell>SI+DI</cell><cell>61.3</cell><cell>90.6</cell></row><row><cell>OF+DOF</cell><cell>62.6</cell><cell>89.1</cell></row><row><cell>SI+DI+OF+DOF</cell><cell>71.5</cell><cell>95.0</cell></row><row><cell cols="3">time their outputs (or classification scores) are simply averaged</cell></row><row><cell cols="3">to obtain an overall score. We show the results of different</cell></row><row><cell cols="3">stream combinations in table 8. First we see that combining SI</cell></row><row><cell cols="3">and DI even without using any optical flow achieves significant</cell></row><row><cell cols="3">improvement (7.8 and 4 points in HMDB51 and 3 and 4 points in</cell></row><row><cell cols="3">UCF101) over individual SI and DI streams respectively. Similarly</cell></row><row><cell cols="3">DOF is complementary to the OF stream, their combination leads</cell></row><row><cell cols="3">to 62.6% and 89.1% in HMDB51 and UCF101 resp. Finally,</cell></row><row><cell cols="3">combining all the four streams obtains remarkable classification</cell></row><row><cell cols="3">accuracy, improving over all the two stream networks (4 and</cell></row><row><cell cols="3">1.1 points over SI+OF, 10 and 4.4 over SI+DI, 8.9 and 5.9 over</cell></row><row><cell cols="3">OF+DOF). To demonstrate the significance of the improvements,</cell></row><row><cell cols="3">we run independent two-sample t-tests for all the two stream</cell></row><row><cell cols="3">combinations for RGB frames and optical flow (SI+DI, OF+DOF)</cell></row><row><cell cols="3">and the four stream one (SI+DI+OF+DOF). The statistical test</cell></row><row><cell cols="3">results validate that the improvements are statistically significant</cell></row><row><cell>at 0.05 level.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc></figDesc><table><row><cell>.6</cell><cell>70.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work acknowledges the support of the EPSRC grant EP/L024683/1, the ERC Starting Grant IDIU and the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Andrea Vedaldi is an associate professor in the Visual Geometry Group at the University of Oxford.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human action recognition in videos using kinematic features and multiple instance learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="288" to="303" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07750</idno>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03607</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sympathy for the details: Dense trajectories and hybrid classification architectures for action recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="697" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic textures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical rank pooling for activity recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rank pooling for action recognition</title>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning end-to-end video classification with rank-pooling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminatively learned hierarchical rank pooling networks</title>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2017">6 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On differentiating parameterized argmin and argmax problems with application to bi-level optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving human action recognition using score distribution and ranking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient visual event detection using volumetric features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="166" to="173" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human activity recognition using a dynamic texture based method</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kellokumpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrals and derivatives for correlated gaussian fuctions using matrix differential calculus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Kinghorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Quantum Chemestry</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="141" to="155" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical mid-level action elements</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vlad3: Encoding dynamics of deep features for action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Tracking by natural language specification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conceptlets: Selective semantics for classifying video events</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mazloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bilinear classifiers for visual recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1482" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discovering discriminative action parts from mid-level video representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Action mach a spatiotemporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Space-time behavior based correlation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="405" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>1406.2199</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical sequence summarization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Ordered pooling of optical flow sequences for action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="page" from="168" to="176" />
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Hidden part models for human action recognition: Probabilistic versus max margin</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1310" to="1323" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Towards good practices for action video encoding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Does human action recognition benefit from pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Exploiting image-trained cnn architectures for unconstrained video classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
