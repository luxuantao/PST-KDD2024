<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction</title>
				<funder ref="#_zGgTg3J">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-16">16 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
							<email>tsafavi@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>dougd@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Allen Institute for Artificial Intelligence (AI2)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Hope</surname></persName>
							<email>tomh@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Allen Institute for Artificial Intelligence (AI2)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-16">16 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.08012v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph (KG) link prediction is a fundamental task in artificial intelligence, with applications in natural language processing, information retrieval, and biomedicine. Recently, promising results have been achieved by leveraging cross-modal information in KGs, using ensembles that combine knowledge graph embeddings (KGEs) and contextual language models (LMs). However, existing ensembles are either (1) not consistently effective in terms of ranking accuracy gains or (2) impractically inefficient on larger datasets due to the combinatorial explosion problem of pairwise ranking with deep language models. In this paper, we propose a novel tiered ranking architecture CASCADER to maintain the ranking accuracy of full ensembling while improving efficiency considerably. CASCADER uses LMs to rerank the outputs of more efficient base KGEs, relying on an adaptive subset selection scheme aimed at invoking the LMs minimally while maximizing accuracy gain over the KGE. Extensive experiments demonstrate that CASCADER improves MRR by up to 9 points over KGE baselines, setting new state-of-the-art performance on four benchmarks while improving efficiency by one or more orders of magnitude over competitive cross-modal baselines. Our empirical analyses reveal that diversity of models across modalities and preservation of individual models' confidence signals help explain the effectiveness of CASCADER, and suggest promising directions for cross-modal cascaded architectures. Code and pretrained models are available at https://github.com/tsafavi/cascader.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are critical ingredients for many applications across natural language processing, information retrieval, and biomedicine <ref type="bibr" target="#b43">[Weikum et al., 2021]</ref>. Motivated by the observation that most KGs have high precision but low coverage <ref type="bibr" target="#b9">[Gal?rraga et al., 2017]</ref>, the goal of knowledge graph link prediction, also known as KG completion, is to automatically augment KGs with new factual information by predicting missing links between KG entities <ref type="bibr" target="#b27">[Nickel et al., 2015]</ref>.</p><p>Link prediction is typically framed as a ranking problem in a multi-relational graph <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref>: Given a query consisting of a head entity (e.g., aspirin) and relation type (e.g., treats), the task is to rank candidate tail entities by the likelihood that they answer the query and form a factual link the graph. Currently, the prevailing approach is to learn vector representations of entities and relations, or knowledge graph embeddings (KGEs), and apply various vector composition functions to the embeddings to score candidate links <ref type="bibr" target="#b30">[Ruffinelli et al., 2020]</ref>. While often effective at modeling structural and logical KG patterns <ref type="bibr" target="#b32">[Sun et al., 2019]</ref>, KGEs typically do not utilize the abundant textual information in KGs, even though auxiliary texts like entity descriptions can ameliorate KG sparsity <ref type="bibr" target="#b4">[Chandak et al., 2022]</ref> and improve ranking accuracy <ref type="bibr" target="#b45">[Xie et al., 2016]</ref>.</p><p>Figure <ref type="figure">1</ref>: CASCADER maintains accuracy (dev MRR) while improving efficiency (inference wallclock time) by one or more orders of magnitude compared to our most competitive ensemble baseline on CODEX-M. Dual-enc. refers to a dual-encoder LM, and cross-enc. refers to a cross-encoder LM; we discuss the differences in these architectures in ? 2.2. For CASCADER, we consider a three-tier structure with dynamic answer pruning at quantiles q = 0.5 and q = 0.9 ( ? 3.3).</p><p>To address this gap, several recent studies have proposed to ensemble KGEs with advanced language models (LMs) like BERT <ref type="bibr" target="#b8">[Devlin et al., 2019]</ref> in order to integrate structure and text for link prediction <ref type="bibr" target="#b25">[Nadkarni et al., 2021</ref><ref type="bibr" target="#b39">, Wang et al., 2021]</ref>. These studies have demonstrated that crossmodal KGE and LM ensembles can improve ranking accuracy over KGEs alone, but results are inconclusive due to several challenges: (1) The cross-modal ensembles that lead to the largest gains rely on impractically expensive models that require jointly encoding pairs of texts with deep language models <ref type="bibr" target="#b25">[Nadkarni et al., 2021]</ref>, adding several orders of magnitude of computational cost compared to shallow KGEs (e.g., increasing inference time from a few minutes with a KGE to one month with an LM on the same dataset <ref type="bibr" target="#b19">[Kocijan and Lukasiewicz, 2021]</ref>); and (2) the cross-modal ensembles that rely on more efficient "Siamese" dual-encoder LM architectures do not necessarily improve performance over KGEs alone <ref type="bibr" target="#b39">[Wang et al., 2021]</ref>.</p><p>In this paper, we introduce a novel cross-modal ensemble that simultaneously maximizes ranking accuracy (i.e., mean reciprocal rank (MRR), hits@k) while maintaining efficiency. Our approach, CASCADER, is a tiered ranking architecture that uses a sequence of LMs with increasing complexity to adaptively reweight and rerank the outputs of more efficient base KGEs. CASCADER relies on a novel subset selection scheme: At each tier t of the architecture, CASCADER predicts the minimal set of candidates that should progress to be reweighted at the subsequent tier t + 1. By passing progressively smaller sets of outputs from one tier to the next, CASCADER balances accuracy gain and efficiency, as it minimizes invocation of the more computationally complex LMs further down the cascade. Evaluated on five link prediction datasets, CASCADER achieves appreciable accuracy gains over baselines across all datasets, improving MRR over our strongest KGE baseline by up to 9 points, while also improving efficiency by one or more orders of magnitude over our most accurate but computationally intensive ensemble baseline (Figure <ref type="figure">1</ref>). In summary, our contributions include:</p><p>? Cross-modal cascade: We propose CASCADER, a novel tiered ranking architecture for link prediction. CASCADER balances the accuracy-efficiency tradeoff by using more costly language models to reweight and rerank small subsets of candidates scored by a more efficient base KGE with an adaptive answer selection strategy that, for each query, predicts the number of top-ranked candidates to progress to the following tier.</p><p>? Robust performance: We evaluate CASCADER on five transductive link prediction benchmarks. Across all datasets, CASCADER achieves appreciable gains over single-modality baselines in ranking accuracy while also significantly improving efficiency over competitive cross-modal baselines.</p><p>? Empirical analysis: Finally, we provide qualitative analyses to illuminate how cross-modal ensembling uniquely exploits complementary signals among graph and text models. We observe that cross-modal ensembles have more diverse predictions than single-modality ensembles, and show that improved diversity correlates with improved accuracy. We also find that weighted additive combinations of models preserve helpful confidence signals in the score margins of individual models, motivating the reweighting approach taken by CASCADER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we first introduce the link prediction problem. We then provide preliminary analyses on cross-modal ensembles for link prediction to motivate the design of CASCADER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem definition</head><p>We consider the task of ranking-based link prediction in a knowledge graph G consisting of entities E, relations R, and factual (head, relation, tail) triples (h, r, t) ? E ? R ? E. The link prediction task consists of two directionalities: Score all tail entities t ? E to answer queries (h, r, ?), and score all head entities ? ? E to answer queries (?, r, t). The evaluation metrics are mean reciprocal rank (MRR), or the average reciprocal of each gold entity's rank over all queries, and hits@k, or the proportion of queries for which the gold entity is ranked in the top-k candidates.</p><p>We define a link prediction model as a scoring function f : E ? R ? E ? R that outputs real values indicating the plausibility of triples in G. At inference time, assume that we have N test link prediction queries of interest, half of type (h, r, ?) and half of type (?, r, t). For each query we have |E| potential answers, which are the entities in the KG. We define a link prediction query-answer score matrix S ? R Ntest?|E| , in which S ij denotes the predicted probability that entity j answers link prediction query i. Then, for each query i, all candidate entities j are ranked by their score descending, and the model's ranking accuracy is evaluated using these rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Single-modality link prediction</head><p>We provide a brief overview of single-modality link prediction approaches, with additional comparison details in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure-based</head><p>The most competitive structure-based approaches to link prediction are shallow knowledge graph embeddings (KGEs), which are decoder models that train entity and relation embeddings to directly optimize for the link prediction ranking task. The main architectural distinction among different KGEs is how embeddings are combined to produce scores for links, as both additive <ref type="bibr" target="#b1">[Bordes et al., 2013</ref><ref type="bibr" target="#b32">, Sun et al., 2019]</ref> and multiplicative <ref type="bibr" target="#b47">[Yang et al., 2015</ref><ref type="bibr" target="#b36">, Trouillon et al., 2016</ref><ref type="bibr" target="#b0">, Balazevic et al., 2019]</ref> scoring functions have been proposed. For details on existing KGEs, we refer the reader to relevant surveys <ref type="bibr" target="#b27">[Nickel et al., 2015</ref><ref type="bibr" target="#b41">, Wang et al., 2017</ref><ref type="bibr" target="#b14">, Ji et al., 2020]</ref>.</p><p>Text-based Recent text-based approaches to link prediction rely on advanced encoder language models (LMs) like BERT <ref type="bibr" target="#b8">[Devlin et al., 2019]</ref> based on the Transformer sequence modeling architecture <ref type="bibr" target="#b37">[Vaswani et al., 2017]</ref>. Let X h = [w 1 , . . . , w h ] denote the description of head entity h and X t = [w 1 , . . . , w t ] the description of tail entity t; for example, given the entity aspirin, a corresponding description could be "aspirin is known as a salicylate and a nonsteroidal anti-inflammatory drug." Siamese or dual-encoder link prediction approaches assume two LMs, potentially with shared weights <ref type="bibr" target="#b39">[Wang et al., 2021]</ref>. As input, one LM takes the head entity description [[CLS], X h , [SEP]] and the other takes the tail entity description [[CLS], X t , [SEP]], where [CLS] and [SEP] refer to the LM's special classification and delimiter tokens. Both encoders output embeddings of their input sequences, and these embeddings are optimized such that linked entity pairs in the KG are scored highly compared to negative samples. Note that previous studies have found that the relation text may be omitted without loss in accuracy, and that including a relation disambiguation loss is usually more important than encoding the relation text for performance <ref type="bibr" target="#b17">[Kim et al., 2020</ref><ref type="bibr" target="#b25">, Nadkarni et al., 2021]</ref>.</p><p>By contrast, cross-encoder LM approaches pack entity description pairs into a single sequence [[CLS], X h , [SEP], X t , [SEP]], and perform full cross-attention over all tokens in the sequence <ref type="bibr" target="#b48">[Yao et al., 2019</ref><ref type="bibr" target="#b25">, Nadkarni et al., 2021]</ref>. At the output of the encoder, these approaches stack a scoring layer and train with classification and/or margin ranking losses <ref type="bibr" target="#b17">[Kim et al., 2020]</ref>. Cross-encoder LMs are typically more powerful for pairwise text ranking than dual-encoders <ref type="bibr" target="#b29">[Reimers and</ref><ref type="bibr">Gurevych, 2019, Luan et al., 2021]</ref>. However, they are less efficient. Whereas dual-encoders can precompute all text embeddings and score text pairs at test time with fast dot products <ref type="bibr" target="#b15">[Karpukhin et al., 2020]</ref>,</p><p>cross-encoders must jointly encode and score each text pair, which makes them impractically slow for large-scale text ranking <ref type="bibr" target="#b29">[Reimers and Gurevych, 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-modal link prediction</head><p>Recently, it has been proposed to integrate structure and text into link prediction by ensembling KGEs and LMs with additive reweighting <ref type="bibr" target="#b39">[Wang et al., 2021</ref><ref type="bibr" target="#b25">, Nadkarni et al., 2021]</ref>. Given a query i and candidate answer j, additive reweighting outputs a new link prediction score as the convex combination of the base models' scores:</p><formula xml:id="formula_0">S ens ij = ? ? S KGE ij + (1 -?) ? S LM ij ,<label>(1)</label></formula><p>where the weight ? ? [0, 1] is a hyperparameter tuned on a held-out set.</p><p>As shown in Figure <ref type="figure">1</ref>, additive ensembling can significantly improve link prediction ranking accuracy, up to 4 points MRR by combining a KGE with a cross-encoder LM. However, Figure <ref type="figure">1</ref> also shows that ensembling with an LM increases inference complexity over the KGE by several orders of magnitude. Whereas the base KGE alone requires less than a minute to score all query-answer pairs on a single NVIDIA Quadro RTX 8000 GPU, the KGE + dual-encoder ensemble takes around 3 hours, and the KGE + cross-encoder ensemble takes over 11 days on the same hardware. This added expense is due to the fact that Transformer LMs are deep neural networks typically consisting of 12 or more layers, and the complexity of Transformer encoding scales quadratically with the input length. Moreover, as discussed previously, cross-encoders must jointly encode all query/answer pairs, which further increases computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Assuming that we are willing to pay some computational cost to improve link prediction performance, how can we achieve the ranking accuracy of the KGE + cross-encoder ensemble while maintaining efficiency closer to that of the KGE + dual-encoder ensemble, as shown in Figure <ref type="figure">1</ref>? Our answer is CASCADER, a cross-modal progressive refinement architecture that combines the best of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CASCADER overview</head><p>As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, our approach CASCADER is a sequential reranking architecture. We first obtain a base set of link prediction scores with a KGE, then use increasingly complex LMs to sequentially rerank the base scores on progressively smaller sets of outputs. The key idea of CASCADER is that we can invoke LMs to progressively refine the rankings of the most-promising candidates only, while leaving the rankings of the less-promising candidates static. As shown in Figure <ref type="figure">1</ref>, this allows us to benefit from the performance gains of cross-modal ensembles without the computational overhead of full LM usage.</p><p>More formally, assume we have a set of n ? 2 trained link prediction models {f (i) , i = 1 . . . n} consisting of one KGE and one or more LMs. We sort the models by computational complexity, leading to an ordered sequence (f (1) , . . . , f (n) ) in which f (1) is the KGE and the subsequent models are LMs in ascending order of complexity (i.e., dual-encoders before cross-encoders). We use the KGE to score all query/answer pairs in the inference set, leading to a score matrix S ? R Ntest?|E| in which S ij denotes the KGE's score reflecting whether entity j answer the i-th link prediction query. Then, at each tier t = 1, . . . , n -1, we apply a pruning function that, for each query i, selects a subset of candidate answer entities E (t) i ? E to be reranked by the next-tier LM f (t+1) ; we postpone the discussion of pruning strategies to ? 3.2 and 3.3.</p><p>For link prediction query i (that is, a query entity and relation type) and candidate answer entity j, we define the additive reranking function between tiers t and t + 1 as follows:</p><formula xml:id="formula_1">I j?E (t) i ? (t) ? S (t) ij + 1 -? (t) ? S (t+1) ij + I j ?E (t) i S (t) ij ,<label>(2)</label></formula><p>where I denotes the set indicator function, S</p><p>ij denotes the query-answer score output at tier t, and ? (t) ? [0, 1] is a hyperparameter that controls the additive influence of model f (t) in reranking the candidates in E (t) i . Intuitively, Eq. ( <ref type="formula" target="#formula_1">2</ref>) states that if the candidate answer entity j is within the subset E (t) i of candidates progressed to tier t + 1, then we reweight its score with that of the model at tier t + 1. If the candidate answer entity j ? E (t) i , then we do not reweight its score at tier t + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Static candidate pruning</head><p>Candidate pruning with CASCADER should intuitively balance two aims: (1) Coverage: For each link prediction query (h, r, ?) or (?, r, t), progress a sufficiently large set of candidate entities to the following tier, in order to increase coverage of the correct candidates; and (2) Efficiency: Progress as few candidates as possible to avoid unnecessarily invoking the next-tier rerankers on candidates that would not benefit from it.</p><p>A straightforward pruning approach used in information retrieval cascades is to progress only the top-k candidates from tier to tier using a predefined global value of k <ref type="bibr" target="#b40">[Wang et al., 2011</ref><ref type="bibr" target="#b24">, Matsubara et al., 2020]</ref>. The value of k naturally helps control the accuracy-efficiency tradeoff, as a smaller k decreases coverage of promising candidates but improves efficiency, whereas a larger k increases coverage of promising candidates but incurs more computational cost. Formally, given query i and a selected value of k, we define static pruning as selecting the subset of candidates</p><formula xml:id="formula_3">E (t) i such that E (t) i = arg max E (t) i ?E and |E (t) i |=k |E| j=1 S (t) ij .</formula><p>Of course, the challenge with this pruning approach is selecting the correct value of k. One solution is to set k ad-hoc, e.g., k = 100 or k = 1000 <ref type="bibr" target="#b24">[Matsubara et al., 2020]</ref>. However, setting the wrong value of k may result in suboptimal performance from the accuracy or efficiency perspectives.</p><p>To address this challenge, we propose a more principled strategy that searches for the best value of k per dataset and per tier t of the cascade. Given tier t and held-out query i, we obtain the cascade's rank R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(t)</head><p>i of the gold answer entity. We construct a distribution of ranks R</p><p>i over all hold-out queries, and use quantiles of this distribution to choose the grid of k (t) over which to search. For example, quantiles of 0.5, 0.75, and 0.9 means that we search over k (t) equal to the median, 75th percentile, and 90th percentile of ranks R (t) i , which helps ensure that our choice of k (t) is tailored to each dataset and each tier of reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamic candidate pruning</head><p>We propose to extend our static pruning strategy to an adaptive dynamic pruning approach in which, at any given tier t, we improve the quality of the current ranking by predicting how many top-ranked candidates for each query we should progress to find the gold entity: That is, given a cascade tier t and query i, we predict an integer k(t) i that represents the number of candidates to progress to tier t + 1. For each query, we predict the rank of the gold answer entity using quantile regression <ref type="bibr" target="#b20">[Koenker and Hallock, 2001]</ref>, again on a hold-out set. Given a chosen quantile q and the rank of the gold answer entity R (t) i at tier t, we train a regressor to predict k(t) i by minimizing As input features to our quantile regressor, we represent the i-th query by its sorted |E|-dimensional score distribution</p><formula xml:id="formula_5">L q ( k(t) i , R (t) i ) = max q(R (t) i - k(t) i ), (q -1)(R (t) i - k(t) i ) .</formula><formula xml:id="formula_6">S (t) i1 , . . . S (t)</formula><p>i|E| from tier t of the cascade, hypothesizing that these score distributions encode uncertainty information correlated to the difficulty of queries. In practice, we implement our regressor as a single-layer MLP trained on half of the dev set, and validated on the remaining dev examples. We will show in our experiments that this approach boosts CASCADER's ability to balance effectiveness and efficiency compared to static pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we evaluate CASCADER on five link prediction datasets. As introduced in ? 2.1, the evaluation task we consider is ranking-based link prediction. Our evaluation metrics are MRR and hits@k for k ? {1, 3, 10}. Following the standard in the literature <ref type="bibr" target="#b1">[Bordes et al., 2013</ref><ref type="bibr">, Ruffinelli et al., 2020]</ref>, we report metrics in the filtered setting, masking out all known answers to test queries other than the gold answer entity in question, in order to avoid false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We consider the following link prediction benchmarks: FB15K-237 <ref type="bibr">[Toutanova and Chen, 2015]</ref>, WN18RR <ref type="bibr" target="#b7">[Dettmers et al., 2018]</ref>, CODEX-S and CODEX-M <ref type="bibr" target="#b31">[Safavi and Koutra, 2020]</ref>, and REPODB <ref type="bibr" target="#b3">[Brown and</ref><ref type="bibr">Patel, 2017, Nadkarni et al., 2021]</ref>. In terms of content, FB15K-237, CODEX-S, and CODEX-M comprise encyclopedic knowledge drawn from Freebase and Wikidata, WN18RR is a subset of the WordNet semantic network, and REPODB is a subset of the RepoDB drug repurposing biomedical database <ref type="bibr">[Brown and Patel, 2017]</ref>. Table <ref type="table" target="#tab_0">1</ref> provides structural and textual statistics for the datasets. For all datasets, we use the standard splits provided by the authors. We use the entity descriptions provided by <ref type="bibr" target="#b39">Wang et al. [2021]</ref> for FB15K-237 and WN18RR, <ref type="bibr" target="#b25">Nadkarni et al. [2021]</ref> for REPODB, and <ref type="bibr" target="#b31">Safavi and Koutra [2020]</ref> for CODEX-S and CODEX-M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We consider the following baselines: KGE baselines We consider the following KGEs, all of which have achieved competitive or stateof-the-art performance on one or more of the datasets we consider: RESCAL <ref type="bibr" target="#b26">[Nickel et al., 2011]</ref>, TransE <ref type="bibr" target="#b1">[Bordes et al., 2013]</ref>, ComplEx <ref type="bibr" target="#b36">[Trouillon et al., 2016]</ref>, and RotatE <ref type="bibr" target="#b32">[Sun et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM baselines</head><p>We consider the StAR dual-encoder architecture <ref type="bibr" target="#b39">[Wang et al., 2021]</ref> and the KG-BERT cross-encoder architecture <ref type="bibr" target="#b48">[Yao et al., 2019]</ref>, both trained in a multi-task setting with triple classification, margin ranking, and relation classification losses following the literature <ref type="bibr" target="#b17">[Kim et al., 2020]</ref>. Note that due to the inference cost of KG-BERT on the larger datasets FB15K-237 and WN18RR (e.g., around one month for FB15K-237 <ref type="bibr" target="#b19">[Kocijan and Lukasiewicz, 2021]</ref>), we report results for these datasets from <ref type="bibr" target="#b17">[Kim et al., 2020]</ref>.</p><p>Ensemble baselines We consider the following additive ensembling baselines, as defined in ? 2.3, controlled by a weighting hyperparameter ? tuned on the dev set: KGE + KGE ensembles the two strongest KGE baselines in terms of dev MRR; KGE + StAR <ref type="bibr" target="#b39">[Wang et al., 2021]</ref> ensembles the best KGE with StAR; and KGE + KG-BERT <ref type="bibr" target="#b25">[Nadkarni et al., 2021]</ref>   SOTA We report the best published performance of which we are aware as of April 2022: NBFNet <ref type="bibr" target="#b49">[Zhu et al., 2021]</ref> on FB15K-237, self-adaptive KGE + LM ensembling <ref type="bibr" target="#b39">[Wang et al., 2021]</ref> on WN18RR, and ComplEx for the CoDEx datasets. Note that REPODB has not been considered under the full entity ranking setting before, as <ref type="bibr" target="#b25">Nadkarni et al. [2021]</ref> used a partial sample of true negatives for each test query. Therefore, we do not report SOTA for REPODB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CASCADER</head><p>Our first-tier KGE in CASCADER is the best-performing baseline KGE in terms of dev MRR.</p><p>We set inference time limits to 2 hours for our two smallest datasets CODEX-S and REPODB, 24 hours for our two largest datasets CODEX-M and FB15K-237, and 6 hours for WN18RR, which has the largest number of candidate entities per query (40K) but a relatively small number of test queries (3K). We search for the optimal cascade in terms of dev MRR among the following hyperparameters: The choice of LMs in the cascade (StAR dual-encoder, KG-BERT cross-encoder, or both); candidate pruning strategy (static versus dynamic); quantile q ? {0.5, 0.75, 0.9, 0.95}; and weighting hyperparameter ? (t) ? [0.05, 0.95] at each tier. Appendix A.2 provides additional details on hardware, software, model selection, and timing.  (a) Dynamic pruning balances effectiveness and efficiency better than static pruning. We consider a three-tier cascade with pruning only between tiers two and three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and discussion</head><p>(b) CASCADER is effective at any level of cost constraint compared to the base KGE. For the three-tier cascade, we use dynamic pruning only between tiers two and three.</p><p>Figure <ref type="figure">3</ref>: Top-left corner is best: Pareto curve analysis on the dev set of FB15K-237. We use quantiles q ? {0.5, 0.75, 0.9, 0.95, 1} in our analyses and exclude any quantiles that lead to CASCADER exceeding our inference time limit of 24 hours.</p><p>S, CODEX-M, and REPODB, and performing second to the reported SOTA on FB15K-237. It outperforms the best KGE by up to 8.70 points MRR (WN18RR) and the best LM by up to 16.84 points MRR (REPODB), demonstrating that cross-modal ensembling can significantly improve upon single-modality approaches.</p><p>We also remark that full additive ensembling is not necessary to maximize accuracy in terms of ranking performance. Our KGE + KG-BERT additive ensemble baseline is very competitive on CODEX-S and REPODB, but it encounters out-of-time errors on FB15K-237, WN18RR, and CODEX-M. By contrast, CASCADER achieves state-of-the-art or competitive performance on all five datasets while staying within our time limits. This suggests that full additive ensembling is not necessary to achieve the majority of gains in link prediction, and that cascaded reranking is sufficient.</p><p>Pareto curve analysis Next, to drill down into the accuracy-efficiency tradeoff of CASCADER, we provide a Pareto curve analysis. In Figure <ref type="figure">3</ref>, we plot the accuracy (dev MRR) and efficiency (inference cost in wall-clock time) against CASCADER's key hyperparameters, the candidate pruning strategy and the number of tiers. We observe that dynamic pruning balances accuracy and efficiency better than static pruning. As shown in Figure <ref type="figure">3a</ref>, dynamic pruning leads to steeper MRR improvements than its static counterpart with comparable inference times.</p><p>We also find that, consistent with the information retrieval literature <ref type="bibr" target="#b24">[Matsubara et al., 2020</ref><ref type="bibr" target="#b23">, Luan et al., 2021]</ref>, cross-encoders improve ranking performance more than their dual-encoder counterparts. Figure <ref type="figure">3b</ref> confirms that two-tiered CASCADER with a cross-encoder is much more effective than two-tiered CASCADER with a dual-encoder. At an inference time of around 1000 seconds, our twotiered dual-encoder and cross-encoder architectures achieve 0.3683 and 0.3738 MRR respectively. These results suggest that reranking very few candidates with a cross-encoder is often more beneficial than reranking many candidates with a dual-encoder. Table <ref type="table">5</ref>: The ranks of gold answers are least correlated between the KGE and the cross-encoder, suggesting that these two types of models are more diverse or complementary in terms of link prediction performance. Ranks are computed across head and tail queries on the validation set, and correlation is reported with Pearson's correlation coefficient. All p-values are 0.</p><p>Figure <ref type="figure">4</ref>: The cross-encoder's score distributions are skewed left compared to the score distributions of the KGE and dual-encoder LM. Shown are two randomly-selected link prediction queries from CODEX-M. We observe that the trends in the plots hold across queries and datasets.</p><p>Qualitative analysis Finally, we provide brief qualitative analyses to help elucidate the benefits of cross-modal ensembling. We conduct our analyses from the perspective of model diversity, which is known to be a key characteristic of effective ensembles <ref type="bibr" target="#b21">[Kuncheva and Whitaker, 2003]</ref>. To this end, we explore two simple measures of diversity between two models in an ensemble. The first is rank correlation, or the Pearson correlation coefficient of the gold answer ranks between two models. We report the rank correlation on REPODB, CODEX-S, and CODEX-M between the best and second-best KGEs, the best KGE and the StAR dual-encoder, and the best KGE and the KG-BERT cross-encoder in Table <ref type="table">5</ref>. As shown in the table, the trend is clear: Rank correlation is highest between two KGEs and lowest between the KGE and cross-encoder. This finding also corresponds with observed ranking performance, as we find that the most "diverse" cascades in terms of rank correlation (i.e., those that combine a KGE and a cross-encoder) are the most competitive.</p><p>Next, we consider each model's empirical score distributions in Figure <ref type="figure">4</ref>. The figure demonstrates that the cross-encoder's score distributions are skewed left compared to those of the KGE and the dual-encoder LM. On further visual inspection, we find that this trend holds across datasets, suggesting a fundamental difference in scoring behavior. We hypothesize that this difference may be due to cross-encoders more aggressively filtering out irrelevant candidate answers to queries, perhaps because they can capture term overlap (or lack thereof) between text pairs with relatively high precision compared to dense retrieval models that do not use cross-attention <ref type="bibr" target="#b23">[Luan et al., 2021]</ref>. Again, this finding corresponds well with ranking performance, as we find that the model pairs with more diverse scoring behaviors (i.e., KGEs and cross-encoders) make stronger cascades.</p><p>Having established the importance of diversity in terms of modality and model architecture, we finally answer the question: When combining a highly diverse pair of models (i.e., a KGE and a cross-encoder), how important is it to reweight their link prediction scores as defined in Eq. ( <ref type="formula" target="#formula_0">1</ref>)? As shown in Table <ref type="table" target="#tab_5">6</ref>, which compares the MRR of the best CASCADER architectures with and without additive reweighting, performance drops significantly without reweighting. To provide a hypothesis as to why reweighting is crucial, we investigate the average margin between the gold answer and all negative candidates for a query, i.e., for query i with gold tail entity j + and N -negative candidates j -, the average margin is defined as 1</p><formula xml:id="formula_7">N - j -S + ij -S - ij .</formula><p>In Figure <ref type="figure">5</ref>, we plot the gold ranks and reciprocal ranks of answer entities on CODEX-M against their average margins with and without reweighting. We observe that the correlation between margins and gold answer ranks is higher under additive reweighting, which suggests that the weighting terms help preserve the confidence signals in the base models' margins, whereas ensembling without reweighting dilutes these signals.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Joint modeling on KGs The task of inferring novel links in KGs has been studied widely over the last decade. The most prevalent approaches are structure-only KG embeddings <ref type="bibr" target="#b26">[Nickel et al., 2011</ref><ref type="bibr" target="#b1">, Bordes et al., 2013</ref><ref type="bibr" target="#b36">, Trouillon et al., 2016</ref><ref type="bibr" target="#b32">, Sun et al., 2019</ref><ref type="bibr" target="#b0">, Balazevic et al., 2019</ref><ref type="bibr" target="#b14">, Ji et al., 2020]</ref>. That said, prior to the introduction of pretrained contextual language models, a few crossmodal structure and text modeling approaches were also proposed <ref type="bibr">[Toutanova et al., 2015</ref><ref type="bibr">, 2016</ref><ref type="bibr" target="#b45">, Xie et al., 2016]</ref>. Such approaches rely on convolutional text representation architectures to obtain embeddings of entities or relations using, e.g., entity descriptions <ref type="bibr" target="#b45">[Xie et al., 2016]</ref> or textual relation mentions <ref type="bibr">[Toutanova et al., 2015]</ref>. These text-based embeddings are then composed using structural KG embedding scoring functions to rank potential novel links in the KG.</p><p>More recently, motivated by the successes of Transformer language models, pretrained Transformer LMs like BERT <ref type="bibr" target="#b8">[Devlin et al., 2019]</ref> have begun to gain traction for variants of the link prediction task <ref type="bibr" target="#b48">[Yao et al., 2019</ref><ref type="bibr" target="#b17">, Kim et al., 2020</ref><ref type="bibr" target="#b6">, Daza et al., 2021</ref><ref type="bibr" target="#b39">, Wang et al., 2021</ref><ref type="bibr" target="#b25">, Nadkarni et al., 2021]</ref>. The approaches most related to CASCADER are the ensembles considered by <ref type="bibr" target="#b39">Wang et al. [2021]</ref> and <ref type="bibr" target="#b25">Nadkarni et al. [2021]</ref>, both of which construct additive ensembles of structural KG embeddings and contextual LMs. We consider both of these ensembles as baselines, showing that CASCADER improves accuracy and efficiency compared to these existing approaches.</p><p>Cascade models Multi-stage cascade ensembles have been successful in computer vision <ref type="bibr" target="#b38">[Viola and</ref><ref type="bibr">Jones, 2001, Wang et al., 2022</ref>] and text retrieval <ref type="bibr" target="#b40">[Wang et al., 2011</ref><ref type="bibr" target="#b5">, Chen et al., 2017</ref><ref type="bibr" target="#b11">, Gallagher et al., 2019</ref><ref type="bibr" target="#b22">, Lin et al., 2021]</ref>. Recently, several studies have proposed to use BERT as a late-stage ranker in multi-stage document retrieval <ref type="bibr" target="#b28">[Nogueira et al., 2019]</ref> and passage retrieval <ref type="bibr" target="#b24">[Matsubara et al., 2020]</ref> pipelines. Similar to our work, these studies are motivated by the observation that using BERT in a multi-stage cascaded setting can significantly boost retrieval accuracy while maintaining efficiency <ref type="bibr" target="#b22">[Lin et al., 2021]</ref>. Yet other studies have attempted to balance the effectiveness-efficiency tradeoff by proposing dual-encoding architectures that are more efficient but usually less effective than cross-encoder BERT models for information retrieval <ref type="bibr" target="#b29">[Reimers and Gurevych, 2019</ref><ref type="bibr" target="#b13">, Humeau et al., 2020</ref><ref type="bibr" target="#b15">, Karpukhin et al., 2020</ref><ref type="bibr" target="#b46">, Xiong et al., 2020</ref><ref type="bibr" target="#b16">, Khattab and Zaharia, 2020]</ref>. Our work builds upon all of these important insights, which have been instrumental in scaling contextual LMs to large-scale text ranking. As far as we are aware, we are the first to bridge these ideas with the traditional graph learning task of link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have considered the task of KG link prediction with cross-modal ensembles. Motivated by the accuracy-efficiency tradeoff inherent to ensembling, proposed CASCADER, a novel cross-modal reranking architecture that uses deep language models to rerank the outputs of knowledge graph embeddings. We show that CASCADER achieves state-of-the-art performance on multiple link prediction benchmarks by effectively combining structure and text, while improving efficiency over our strongest ensemble baseline by one or more orders of magnitude.</p><p>Our work opens up many avenues for future research. For one, more advanced candidate pruning strategies may further increase the efficiency of CASCADER while maintaining effectiveness. For another, hybrid language modeling approaches that attempt to interpolate between the efficiency of dual-encoders and the effectiveness of cross-encoders <ref type="bibr" target="#b16">[Khattab and</ref><ref type="bibr">Zaharia, 2020, Luan et al., 2021]</ref> may improve CASCADER's ability to balance these two desiderata. Finally, extensions of CASCADER may help solve the inductive link prediction task in which novel entities/relations are presented at test time <ref type="bibr" target="#b10">[Galkin et al., 2022]</ref>, as contextual language models are naturally inductive and can correct the deficiencies of transductive KGEs in this setting.  Table <ref type="table">7</ref>: Time complexity comparison between KGEs, dual-encoder LMs, and cross-encoder LMs. d refers to the hidden dimension of the given model; note that here we consider KGEs whose entity/relation embeddings have the same dimension (e.g., ComplEx <ref type="bibr" target="#b36">[Trouillon et al., 2016]</ref>, Ro-tatE <ref type="bibr" target="#b32">[Sun et al., 2019]</ref>). H LM refers to the number of hidden layers in the LM. L desc refers to the average length of an entity description in the KG. N train and N test refer to the total number of training examples and the total number of test queries, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Comparison of single-modality approaches Accuracy Several studies have demonstrated that KGEs vastly outperform LMs on metrics that emphasize recall within the first few ranking positions, such as MRR and hits@k for small k <ref type="bibr" target="#b48">[Yao et al., 2019</ref><ref type="bibr" target="#b39">, Wang et al., 2021]</ref>. By contrast, LMs outperform KGEs on metrics that emphasize the model's overall ability to find the correct answer to every query, such hits@k for larger k and mean rank, or the average rank of all gold answers in the test set. Figures <ref type="figure" target="#fig_3">6</ref> and<ref type="figure" target="#fig_4">7</ref> demonstrate this phenomenon clearly, showing that KGEs outperform LMs on hits@k for k ? 10, but underperform for larger values of k due to their longer tail of correct answer ranks. While the reason for this disparity is unclear, it has been hypothesized that LMs underperform on recall at low values of k because they do not disambiguate entities and often cannot distinguish between lexically similar entity descriptions <ref type="bibr" target="#b39">[Wang et al., 2021]</ref>.</p><p>Efficiency To formalize the efficiency differences between KGEs, dual-encoders, and crossencoders, Table <ref type="table">7</ref> provides a comparison of theoretical complexity at training and inference times. In particular, the table shows that the combinatorial nature of pairwise encoding makes cross-encoders impractically slow at inference time. To score all answers to a single query, a cross-encoder must separately encode the query entity's textual description alongside the description of every other entity in the KG's entity set E. This adds an extra factor of |E| to the cost of inference compared to dual-encoder LMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CASCADER sequential reranking architecture.</figDesc><graphic url="image-2.png" coords="5,109.98,72.00,392.05,87.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5: Ensembling with additive reweighting preserves the correlation between gold answer ranks and average margins.</figDesc><graphic url="image-8.png" coords="10,309.74,187.00,186.13,77.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hits@k for k ? [1, 1000] on the dev set of CODEX-M.</figDesc><graphic url="image-9.png" coords="16,216.90,72.00,178.20,101.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ranks of the gold answer entities on the dev set of CODEX-M.</figDesc><graphic url="image-10.png" coords="16,177.30,210.47,257.39,77.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the existing KG link prediction datasets considered in our experiments.</figDesc><table><row><cell></cell><cell>|E|</cell><cell>|R|</cell><cell>Structure # train</cell><cell># dev</cell><cell># test</cell><cell>Avg. desc. length</cell></row><row><cell>CODEX-S</cell><cell>2,034</cell><cell>42</cell><cell>32,888</cell><cell>1827</cell><cell>1828</cell><cell>259.24</cell></row><row><cell>REPODB</cell><cell>2,748</cell><cell>1</cell><cell>5,342</cell><cell>667</cell><cell>668</cell><cell>55.46</cell></row><row><cell>FB15K-237</cell><cell cols="5">14,541 237 272,115 17,535 20,466</cell><cell>138.95</cell></row><row><cell>CODEX-M</cell><cell cols="5">17,050 51 185,584 10,310 10,311</cell><cell>159.48</cell></row><row><cell>WN18RR</cell><cell cols="2">40,943 11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell><cell>13.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ensembles the best KGE with KG-BERT. CASCADER outperforms or is competitive with the state of the art on FB15K-237 and WN18RR. Bold + underline: Best performance. Underline: Second-best performance. The performance of StAR, KG-BERT, and SOTA are reported from papers referenced in 4.2. OOT refers to out-of-time using our inference cost budget.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">FB15K-237</cell><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell></row><row><cell>RESCAL</cell><cell cols="4">0.3559 0.2629 0.3926 0.5406</cell><cell cols="4">0.4666 0.4387 0.4797 0.5172</cell></row><row><cell>TransE</cell><cell cols="4">0.3128 0.2206 0.3473 0.4973</cell><cell cols="4">0.2278 0.0531 0.3682 0.5201</cell></row><row><cell>ComplEx</cell><cell cols="4">0.3477 0.2533 0.3836 0.5359</cell><cell cols="4">0.4749 0.4381 0.4898 0.5474</cell></row><row><cell>RotatE</cell><cell cols="4">0.3333 0.2396 0.3676 0.5218</cell><cell cols="4">0.4781 0.4395 0.4941 0.5527</cell></row><row><cell>StAR</cell><cell>0.296</cell><cell>0.205</cell><cell>0.322</cell><cell>0.482</cell><cell>0.401</cell><cell>0.243</cell><cell>0.491</cell><cell>0.709</cell></row><row><cell>KG-BERT</cell><cell>0.267</cell><cell>0.172</cell><cell>0.298</cell><cell>0.458</cell><cell>0.331</cell><cell>0.203</cell><cell>0.383</cell><cell>0.597</cell></row><row><cell>KGE + KGE</cell><cell cols="4">0.3630 0.2672 0.4016 0.5535</cell><cell cols="4">0.4900 0.4521 0.5016 0.5617</cell></row><row><cell>KGE + StAR</cell><cell cols="4">0.3643 0.2709 0.3989 0.5522</cell><cell cols="4">0.5385 0.4716 0.5645 0.6651</cell></row><row><cell>KGE + KG-BERT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell></row><row><cell>SOTA</cell><cell>0.415</cell><cell>0.321</cell><cell>0.454</cell><cell>0.599</cell><cell>0.551</cell><cell>0.459</cell><cell>0.594</cell><cell>0.732</cell></row><row><cell>CASCADER</cell><cell cols="4">0.3860 0.2903 0.4231 0.5782</cell><cell cols="4">0.5651 0.4756 0.6126 0.7379</cell></row><row><cell></cell><cell></cell><cell cols="2">CODEX-S</cell><cell></cell><cell></cell><cell cols="2">CODEX-M</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell></row><row><cell>RESCAL</cell><cell cols="4">0.4040 0.2935 0.4494 0.6225</cell><cell cols="4">0.3173 0.2444 0.3477 0.4557</cell></row><row><cell>TransE</cell><cell cols="4">0.3540 0.2185 0.4218 0.6335</cell><cell cols="4">0.3026 0.2232 0.3363 0.4535</cell></row><row><cell>ComplEx</cell><cell cols="4">0.4646 0.3714 0.5038 0.6455</cell><cell cols="4">0.3365 0.2624 0.3701 0.4758</cell></row><row><cell>RotatE</cell><cell cols="4">0.2587 0.1586 0.2916 0.4609</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>StAR</cell><cell cols="4">0.3540 0.2306 0.4051 0.6007</cell><cell cols="4">0.2726 0.1888 0.3042 0.4342</cell></row><row><cell>KG-BERT</cell><cell cols="4">0.2849 0.1472 0.3310 0.5848</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell></row><row><cell>KGE + KGE</cell><cell cols="4">0.4665 0.3712 0.5082 0.6518</cell><cell cols="4">0.3466 0.2695 0.3808 0.4925</cell></row><row><cell>KGE + StAR</cell><cell cols="4">0.4751 0.3717 0.5249 0.6712</cell><cell cols="4">0.3554 0.2767 0.3901 0.5064</cell></row><row><cell cols="5">KGE + KG-BERT 0.4812 0.3764 0.5290 0.6898</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell><cell>OOT</cell></row><row><cell>SOTA</cell><cell cols="4">0.4646 0.3714 0.5038 0.6455</cell><cell cols="4">0.3365 0.2624 0.3701 0.4758</cell></row><row><cell>CASCADER</cell><cell cols="4">0.4839 0.3764 0.5383 0.6871</cell><cell cols="4">0.3830 0.2998 0.4221 0.5423</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>CASCADER achieves state-of-the-art test performance on CODEX-S and CODEX-M. Bold + underline: Best performance. Underline: Second-best performance. OOM refers to out-ofmemory during training. OOT refers to out-of-time using our inference cost budget.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 ,</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell></row><row><cell>RESCAL</cell><cell cols="4">0.4351 0.3144 0.4903 0.6767</cell></row><row><cell>TransE</cell><cell cols="4">0.3472 0.2043 0.3728 0.6400</cell></row><row><cell>ComplEx</cell><cell cols="4">0.4620 0.3406 0.5225 0.7043</cell></row><row><cell>RotatE</cell><cell cols="4">0.2971 0.1811 0.4903 0.5314</cell></row><row><cell>StAR</cell><cell cols="4">0.3472 0.2043 0.4102 0.6400</cell></row><row><cell>KG-BERT</cell><cell cols="4">0.2991 0.1602 0.3428 0.5996</cell></row><row><cell>KGE + KGE</cell><cell cols="4">0.4637 0.3398 0.5262 0.7081</cell></row><row><cell>KGE + StAR</cell><cell cols="4">0.4774 0.3496 0.5434 0.7208</cell></row><row><cell cols="5">KGE + KG-BERT 0.5101 0.3713 0.5771 0.7799</cell></row><row><cell>CASCADER</cell><cell cols="4">0.5156 0.3817 0.5831 0.7814</cell></row></table><note><p>3, and 4 provide link prediction performance results for FB15K-237 and WN18RR, the CODEX datasets, and REPODB, respectively. We observe that CASCADER achieves robust and appreciable gains over baselines across datasets, setting a new state of the art on WN18RR, CODEX-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>CASCADER sets a new state of the art on the drug repurposing benchmark REPODB.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Additive reweighting is crucial to combining KGEs and LMs. We compare the MRR of the best CASCADER architecture with additive reweighting as defined in Eq 1 to the best CASCADER architecture without the reweighting term.</figDesc><table><row><cell></cell><cell cols="2">No reweighting Reweighting</cell></row><row><cell>REPODB</cell><cell>0.4498</cell><cell>0.5156</cell></row><row><cell>CODEX-S</cell><cell>0.4204</cell><cell>0.4839</cell></row><row><cell>CODEX-M</cell><cell>0.3114</cell><cell>0.3830</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We thank <rs type="person">Hannaneh Hajishirzi</rs> and <rs type="person">Danai Koutra</rs> for stimulating discussions and feedback. This work was supported in part by <rs type="funder">NSF</rs> Convergence Accelerator Award #<rs type="grantNumber">2132318</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zGgTg3J">
					<idno type="grant-number">2132318</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model selection</head><p>We implement all KG embeddings using the open-source LibKGE PyTorch library <ref type="bibr" target="#b2">[Broscheit et al., 2020]</ref>. We use the pretrained KGE checkpoints provided by LibKGE for FB15K-237 and WN18RR.</p><p>For the other datasets, we follow a similar hyperparameter tuning strategy to that proposed by by <ref type="bibr" target="#b30">Ruffinelli et al. [2020]</ref> for tuning our KGE baselines.</p><p>We implement all LMs with the Huggingface transformers PyTorch library <ref type="bibr" target="#b44">[Wolf et al., 2020]</ref> using the same base language model, which is BERT-BASE <ref type="bibr" target="#b8">[Devlin et al., 2019]</ref> for all benchmarks except REPODB, and PUBMEDBERT <ref type="bibr" target="#b12">[Gu et al., 2021]</ref> for REPODB. We use the following hyperparameters: Batch size of 16, learning rate of 10 -5 , and 10 epochs. We use a maximum sequence length of 32, 64, and 256 respectively for WN18RR, REPODB, and all other datasets. For the dual-encoder LM we use 16 negative samples per positive. For the cross-encoder LM we use 2 negative samples per positive.</p><p>For all of the ensemble baselines and CASCADER, we tune the weighting hyperparameter ? ? [0.05, 0.95]. All experiments are conducted on a single NVIDIA Quadro RTX 8000 GPU with 48 GB of RAM. All main results reported in the paper use a three-tiered structure with no pruning between tiers one and two and dynamic pruning at q = 0.9 between tiers two and three. We provide exact wall-clock inference time for CASCADER in Table <ref type="table">8</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor factorization for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1522</idno>
		<ptr target="https://aclanthology.org/D19-1522" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LibKGE -a knowledge graph embedding library for reproducible research</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.22</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.22" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A standard database for drug repositioning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chirag J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/sdata201729" />
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building a knowledge graph to enable precision medicine</title>
		<author>
			<persName><forename type="first">Payal</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="DOI">10.1101/2022.05.01.489928v2</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/2022.05.01" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">489928</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient cost-aware cascade ranking in multi-stage retrieval</title>
		<author>
			<persName><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Shane</forename><surname>Culpepper</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080819</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3077136.3080819" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="445" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive entity representations from text via link prediction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Groth</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450141</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3442381.3450141" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="798" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting completeness knowledge bases</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gal?rraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Amarilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1145/3018661.3018739</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3018661.3018739" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An open challenge for inductive link prediction on knowledge graphs</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Tapley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoyt</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01520</idno>
		<ptr target="https://arxiv.org/pdf/2203.01520" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint optimization of cascade ranking models</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruey-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Shane</forename><surname>Culpepper</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3290986</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3289600.3290986" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458754</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3458754" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1905.01969.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00388</idno>
		<ptr target="https://arxiv.org/pdf/2002.00388.pdf" />
		<title level="m">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.550" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><surname>Colbert</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401075</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3397271.3401075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning for knowledge graph completion with pre-trained language models</title>
		<author>
			<persName><forename type="first">Bosung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesuk</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoong</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungyun</forename><surname>Seo</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="1/2020.coling-main" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
			<biblScope unit="page" from="1737" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="https://aclanthology.org/2020.coling-main.153" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge base completion meets transfer learning</title>
		<author>
			<persName><forename type="first">Vid</forename><surname>Kocijan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.524</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.524" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">November 2021</date>
			<biblScope unit="page" from="6521" to="6533" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantile regression</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Koenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">F</forename><surname>Hallock</surname></persName>
		</author>
		<idno type="DOI">https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143</idno>
		<ptr target="https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143" />
	</analytic>
	<monogr>
		<title level="j">Journal of economic perspectives</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ludmila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><surname>Whitaker</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022859003006.pdf</idno>
		<ptr target="https://link.springer.com/content/pdf/10.1023/A:1022859003006.pdf" />
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="207" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pretrained transformers for text ranking: Bert and beyond</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<idno type="DOI">10.2200/S01123ED1V01Y202108HLT053</idno>
		<ptr target="https://www.morganclaypool.com/doi/pdfplus/10.2200/S01123ED1V01Y202108HLT053" />
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="325" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">dense, and attentional representations for text retrieval</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00369</idno>
		<ptr target="https://aclanthology.org/2021.tacl-1" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="329" to="345" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reranking for efficient transformerbased answer selection</title>
		<author>
			<persName><forename type="first">Yoshitomo</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401266</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3397271.3401266" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1577" to="1580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scientific language models for biomedical knowledge base completion: An empirical study</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Nadkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hope</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2106.09700.pdf" />
	</analytic>
	<monogr>
		<title level="m">3rd Conference on Automated Knowledge Base Construction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
		<ptr target="https://icml.cc/2011/papers/438_icmlpaper.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Gabrilovich</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/document/7358050" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14424</idno>
		<title level="m">Multi-stage document ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://aclanthology.org/D19-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">You can teach an old dog new tricks! on training knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=BkxSmlBFvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CoDEx: A Comprehensive Knowledge Graph Completion Benchmark</title>
		<author>
			<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.emnlp-main.669" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="8328" to="8350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL CVSC Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<idno>doi: 10.18653</idno>
		<ptr target="https://aclanthology.org/D15-1174" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="15" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compositional learning of embeddings for relation paths in knowledge base and text</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1136</idno>
		<ptr target="https://aclanthology.org/P16-1136" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1434" to="1444" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/trouillon16.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/iel5/7768/21353/00990517.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition</title>
		<meeting>the 2001 IEEE computer society conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structure-augmented text representation learning for efficient knowledge graph completion</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450043</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/3442381.3450043" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1737" to="1748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A cascade ranking model for efficient ranked retrieval</title>
		<author>
			<persName><forename type="first">Lidan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="DOI">10.1145/2009916.2009934</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1145/2009916" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011">2011. 2009934</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://ieeexplore.ieee.org/iel7/69/4358933/08047276.pdf" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wisdom of committees: An overlooked approach to faster and more accurate models</title>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Eban</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>id=MvO2t0vbs4-</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Machine knowledge: Creation and curation of comprehensive knowledge bases</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Razniewski</surname></persName>
		</author>
		<author>
			<persName><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1561/1900000064</idno>
		<ptr target="https://www.nowpublishers.com/article/DownloadSummary/DBS-064" />
	</analytic>
	<monogr>
		<title level="j">Found. Trends Databases</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="108" to="490" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">October 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/10329/10188" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor negative contrastive learning for dense text retrieval</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Fung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Overwijk</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=zeFrfgyZln" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Kg-bert: Bert for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<ptr target="https://arxiv.org/pdf/1909.03193.pdf?ref=https://githubhelp.com" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Neural bellman-ford networks: A general graph neural network framework for link prediction</title>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/f6" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
	<note>a673f09493afcd8b129a0bcf1cd5bc-Abstract.html</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
