<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphBIG: Understanding Graph Computing in the Context of Industrial Solutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lifeng</forename><surname>Nai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
							<email>yxia@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilie</forename><forename type="middle">G</forename><surname>Tanase</surname></persName>
							<email>igtanase@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
							<email>hyesoon.kim@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
							<email>chingyung@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphBIG: Understanding Graph Computing in the Context of Industrial Solutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1D37F1002351A189007337C502EE2063</idno>
					<idno type="DOI">10.1145/2807591.2807626</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the emergence of data science, graph computing is becoming a crucial tool for processing big connected data. Although efficient implementations of specific graph applications exist, the behavior of full-spectrum graph computing remains unknown. To understand graph computing, we must consider multiple graph computation types, graph frameworks, data representations, and various data sources in a holistic way.</p><p>In this paper, we present GraphBIG, a benchmark suite inspired by IBM System G project. To cover major graph computation types and data sources, GraphBIG selects representative datastructures, workloads and data sets from 21 real-world use cases of multiple application domains. We characterized GraphBIG on real machines and observed extremely irregular memory patterns and significant diverse behavior across different computations. GraphBIG helps users understand the impact of modern graph computing on the hardware architecture and enables future architecture and system research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the big data era, information is often linked to form large-scale graphs. Processing connected big data has been a major challenge. With the emergence of data and network science, graph computing is becoming one of the most important techniques for processing, analyzing, and visualizing connected data <ref type="bibr" target="#b23">[24]</ref>  <ref type="bibr" target="#b28">[29]</ref>.</p><p>Graph computing is comprised of multiple research areas, from low level architecture design to high level data mining and visualization algorithms. Enormous research efforts across multiple communities have been invested in this discipline <ref type="bibr" target="#b18">[19]</ref>. However, researchers focus more on analyzing and mining graph data, while paying relatively less attention to the performance of graph computing <ref type="bibr" target="#b42">[43]</ref>  <ref type="bibr" target="#b40">[41]</ref>. Although high performance implementations of specific graph applications and systems exist in prior literature, a comprehensive study on the full spectrum of graph computing is still miss-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ing <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b43">[44]</ref>. Unlike prior work focusing on graph traversals and assuming simplified data structures, graph computing today has a much broader scope. In today's graph applications, not only has the structure of graphs analyzed grown in size, but the data associated with vertices and edges has become richer and more dynamic, enabling new hybrid content and graph analysis <ref type="bibr" target="#b7">[8]</ref>. Besides, the computing platforms are becoming heterogeneous. More than just parallel graph computing on CPUs, there is a growing field of graph computing on Graphic Processing Units (GPUs).</p><p>The challenges in graph computing come from multiple key issues like frameworks, data representations, computation types, and data sources <ref type="bibr" target="#b7">[8]</ref>. First, most of the industrial solutions deployed by clients today are in the form of an integrated framework <ref type="bibr" target="#b0">[1]</ref> [40] <ref type="bibr" target="#b36">[37]</ref>. In this context, elementary graph operations, such as find-vertex and addedge are part of a rich interface supported by graph datastructures and they account for a large portion of the total execution time, significantly impacting the performance. Second, the interaction of data representations with memory subsystems greatly impacts performance. Third, although graph traversals are considered to be representative graph applications, in practice, graph computing has a much broader scope. Typical graph applications can be grouped into three computation types: (1) computation on graph structure, (2) computation on rich properties, and (3) computation on dynamic graphs. Finally, as a data-centric computing tool, graph computing is sensitive to the structure of input data. Several graph processing frameworks have been proposed lately by both academia and industry <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b19">[20]</ref> [2] <ref type="bibr" target="#b36">[37]</ref> [30] <ref type="bibr" target="#b15">[16]</ref>. Despite the variety of these frameworks, benchmarking efforts have focused mainly on simplified static memory representations and graph traversals, leaving a large area of graph computing unexplored <ref type="bibr" target="#b24">[25]</ref>  <ref type="bibr" target="#b6">[7]</ref>. Little is known, for example, about the behavior of fullspectrum graph computing with dynamic data representations. Likewise, graph traversal is only one computation type. What is the behavior of other algorithms that build graphs or modify complex properties on vertices and edges? How is the behavior of graph computing influenced by the structure of the input data? To answer these questions, we have to analyze graph workloads across a broader spectrum of computation types and build our benchmarks with extended data representations.</p><p>To understand the full-spectrum of graph computing, we propose a benchmark suite, GraphBIG 1 , and analyze it on contemporary hardware. GraphBIG is inspired by IBM's System G framework, which is a comprehensive set of industrial graph computing toolkits used by many commercial clients <ref type="bibr" target="#b36">[37]</ref>. Based on our experience with a large set of real world user problems, we selected representative graph data representations, interfaces, and graph workloads to design our GraphBIG benchmark suite. GraphBIG utilizes a dynamic, vertex-centric data representation, which is widely utilized in real-world graph systems, and selects workloads and datasets inspired by use cases from a comprehensive selection of application domains. By ensuring the representativeness of data representations and graph workloads, GraphBIG is able to address the shortcomings of previous benchmarking efforts and achieve a generic benchmarking solution. The characterization of performance on GraphBIG workloads can help researchers understand not only the architectural behaviors of specific graph workloads, but also the trend and correlations of full-spectrum graph computing.</p><p>The main contributions of this paper are as follows:</p><p>• We present the first comprehensive architectural study of full-spectrum graph computing within modern graph frameworks.</p><p>• We propose GraphBIG, a suite of CPU/GPU benchmarks. GraphBIG utilizes modern graph frameworks and covers all major graph computation types and data sources.</p><p>• We analyze the memory behavior of graph computing.</p><p>Our results indicate high L2/L3 cache miss rates on CPUs as well as high branch/memory divergence on GPUs. However, L1D cache and ICache both show a low miss rate because of the locality of non-graph data and the flat hierarchy of the underlying framework respectively.</p><p>• We investigate various workloads and observe diverse architectural behavior across various graph computation types.</p><p>• We explore several data sources and observe that graph workloads consistently exhibit a high degree of data sensitivity.</p><p>The rest of this paper is organized as follows. In Section 2, we discuss and summarize the key factors of graph computing. Section 3 introduces previous related work. Section 4 illustrates the methodology and workloads of our proposed GraphBIG. In Section 5, we characterize the workloads from multiple perspectives on CPU/GPU hardware. Finally, in Section 6, we conclude our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GRAPH COMPUTING: KEY FACTORS</head><p>Although graph traversals, such as Breadth-first Search and Shortest-path, are usually considered as representative graph applications, real-world graph computing also performs various other comprehensive computations. In realworld practices, graph computing contains a broad scope of use cases, from cognitive analytics to data exploration. The wide range of use cases introduces not only unique, but also diverse features of graph computing. The uniqueness codes, datasets, and documents are released in our github repository (http://github.com/graphbig/graphBIG). and diversity are reflected in multiple key factors, including frameworks, data representations, computation types, and data sources. To understand graph computing in a holistic way, we first analyze these key factors of graph computing in this section.</p><p>Framework: Unlike standalone prototypes of graph algorithms, graph computing systems largely rely on specific frameworks to achieve various functionalities. By hiding the details of managing both graph data and requests, the graph frameworks provide users primitives for elementary graph operations. The examples of graph computing frameworks include GraphLab <ref type="bibr" target="#b19">[20]</ref>, Pregel <ref type="bibr" target="#b21">[22]</ref>, Apache Giraph <ref type="bibr" target="#b1">[2]</ref>, and IBM System G <ref type="bibr" target="#b36">[37]</ref>. They all share significant similarity in their graph models and user primitives. First, unlike simplified algorithm prototypes, graph systems represent graph data as a property graph, which associates user-defined properties with each vertex and edge. The properties can include meta-data (e.g., user profiles), program states (e.g., vertex status in BFS or graph coloring), and even complex probability tables (e.g., Bayesian inference). Second, instead of directly operating on graph data, the user defined applications achieve their algorithms via framework-defined primitives, which usually include find/delete/add vertices/edges, traverse neighbours, and update properties. To estimate the framework's impact on the graph system performance, we performed profiling experiments on a series of typical graph workloads with IBM System G framework. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, a significant portion of time is contributed by the framework for most workloads, especially for graph traversal based ones. On average, the in-framework time is as high as 76%. It clearly shows that the heavy reliance on the framework indeed results in a large portion of in-framework execution time. It can bring significant impacts on the architecture behaviors of the upper layer graph workloads. Therefore, to understand graph computing, it is not enough to study only simple standalone prototypes. Workload analysis should be performed with representative frameworks, in which multiple other factors, such as flexibility and complexity, are considered, leading to design choices different from academic prototypes.</p><p>Data representation: Within the graph frameworks, various data representations can be incorporated for organizing in-memory graph data. The differences between inmemory data representations can significantly affect the architectural behaviors, especially memory sub-system related features, and eventually impact the overall performance.</p><p>One of the most popular data representation structure is Compressed Sparse Row (CSR). As illustrated in   exist. For example, Coordinate List (COO) format replaces the vertex array in CSR with an array of source vertices of each edge.) The compact format of CSR saves memory size and simplifies graph build/copy/transfer complexity. Because of its simplicity, CSR is widely used in the literature. However, its drawback is also obvious. CSR is only suitable for static data with no structural updates. This is the case for most graph algorithm prototypes. Nevertheless, real-world graph systems usually are highly dynamic in both topologies and properties. Thus, more flexible data representations are incorporated in graph systems. For example, IBM System G, as well as multiple other frameworks, is using a vertex-centric structure, in which a vertex is the basic unit of a graph. As shown in Figure <ref type="figure" target="#fig_4">2</ref>(c), the vertex property and the outgoing edges stay within the same vertex structure. Meanwhile, all vertices form up an adjacency list with indices. Although the compact format of CSR may bring better locality and lead to better cache performance, graph computing systems usually utilize vertexcentric structures because of the flexibility requirement of real-world use cases <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b36">[37]</ref>.</p><p>Computation types: Numerous graph applications exist in previous literature and real-world practices. Despite the variance of implementation details, generally, graph computing applications can be classified into a few computation types <ref type="bibr" target="#b41">[42]</ref>. As shown in Table <ref type="table" target="#tab_0">1</ref>, we summarize the applications into three categories according to their different computation targets: graph structure, graph properties, and dynamic graphs. They have different features in terms of read/write/numeric intensity. (1) Computation on the graph structure incorporates a large number of memory accesses and limited numeric operations. Their irregular memory access pattern leads to extremely poor spatial locality.</p><p>(2) On the contrary, computation on graphs with rich properties introduces lots of numeric computations on properties, which leads to hybrid workload behaviors. (3) For computation on dynamic graphs, it also shows an irregular pattern as the first computation type. However, the updates of graph structure lead to high write intensity and dynamic memory footprint.</p><p>Graph data sources: As a data-centric computing tool, graph computing heavily relies on data inputs. As shown in Table <ref type="table" target="#tab_1">2</ref>, we summarize graph data into four sources <ref type="bibr" target="#b41">[42]</ref>. The social network represents the interactions between individuals/organizations. The key features of social networks include high degree variances, small shortest path lengths, and large connected components <ref type="bibr" target="#b25">[26]</ref>. On the contrary, an information network is a structure, in which the dominant interaction is the dissemination of information along edges. It usually shows large vertex degrees, and large twohop neighbourhoods. The nature network is a graph of biological/cognitive objects. Examples include gene network <ref type="bibr" target="#b30">[31]</ref>, deep belief network (DBN) <ref type="bibr" target="#b5">[6]</ref> and biological network <ref type="bibr" target="#b9">[10]</ref>.They typically incorporate structured topologies and rich properties addressing different targets. Man-made technology networks are formed by specific man-made technologies. A typical example is a road network, which usually maintains small vertex degrees and a regular topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATED WORK</head><p>Previous benchmarking efforts of graph computing are summarized in Table <ref type="table" target="#tab_2">3</ref>. Most of existing benchmarks target other evaluation purposes, which are much broader than graph computing. For example, CloudSuite <ref type="bibr" target="#b10">[11]</ref> and Big-DataBench <ref type="bibr" target="#b38">[39]</ref> target cloud computing and big data computing respectively. Graph is only a small portion of their applications. Similarly, PBBS <ref type="bibr" target="#b33">[34]</ref> targets evaluations of parallel programming methodologies. Parboil <ref type="bibr" target="#b35">[36]</ref> and Rodinia <ref type="bibr" target="#b8">[9]</ref> are both for general GPU benchmarking purposes. Lonestar <ref type="bibr" target="#b6">[7]</ref> focuses on irregular GPU applications, which include not only graph computing but also several other aspects. As one of the most famous graph benchmarks, Graph 500 <ref type="bibr" target="#b24">[25]</ref> was proposed for system performance ranking purposes. Although reference codes exist in Graph 500, because of its special purpose, it provides limited number of workloads.</p><p>Besides, graph computing has a broad scope, covering multiple computation types. As shown in Table <ref type="table" target="#tab_2">3</ref>, most of existing benchmarks are highly biased to graph traversal related workloads (CompStruct). The other two graph computation types, computation on dynamic graphs and on rich properties, are less studied. However, as we illustrated in the previous section, both of them are important graph computation types and cannot be overlooked when analyzing the full-scope graph computing.</p><p>Moreover, without incorporating realistic frameworks, most prior graph benchmarks assume simplified static graph structures with no complex properties attached to vertices and edges. However, this is not the case for most real-world graph processing systems. The underlying framework plays a crucial role in the graph system performance. Moreover, in real-world systems, graphs are dynamic and both vertices and edges are associated with rich properties.</p><p>Multiple system-level benchmarking efforts are also ongoing for evaluating and comparing existing graph systems. Examples include LDBC benchmark, GraphBench, G. Yong's characterization work <ref type="bibr" target="#b12">[13]</ref>, and A. L. Varbanescu's study <ref type="bibr" target="#b37">[38]</ref>. We excluded them in the summary of Table <ref type="table" target="#tab_2">3</ref> because of their limited usability for architectural research. In these benchmarking efforts, very few ready-to-use open-source benchmarks are provided. Detailed analysis on the architectural behaviors is also lacking.</p><p>Examples of existing graph computing frameworks include Pregel <ref type="bibr" target="#b21">[22]</ref>, Giraph <ref type="bibr" target="#b1">[2]</ref>, Trinity <ref type="bibr" target="#b32">[33]</ref>, GraphLab <ref type="bibr" target="#b19">[20]</ref>, and System G <ref type="bibr" target="#b36">[37]</ref>. Multiple academic research efforts also have been proposed, such as GraphChi <ref type="bibr" target="#b16">[17]</ref>, X-stream <ref type="bibr" target="#b29">[30]</ref>, Cusha <ref type="bibr" target="#b15">[16]</ref>, and Mapgraph <ref type="bibr" target="#b11">[12]</ref>. They incorporate various techniques to achieve different optimization targets on specific platforms. For example, GraphChi utilizes the Parallel Sliding Window    <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>.</p><p>(PSW) technique to optimize disk IO performance. Cusha extends the similar technique on GPU platforms to improve data access locality. System G and other related projects like Boost Graph Library <ref type="bibr" target="#b0">[1]</ref>, Neo4j <ref type="bibr" target="#b39">[40]</ref>, Giraph <ref type="bibr" target="#b1">[2]</ref>, GraphLab <ref type="bibr" target="#b19">[20]</ref>, are comprehensive graph processing toolkits that are used to build real world applications deployed by clients in both industry and academia. We refer to these projects as industrial solutions and we oppose them to simple graph infrastructures that only aim to study the property of various datastructures and algorithms <ref type="bibr" target="#b24">[25]</ref>. The design of an industrial framework is concerned with not only performance, but also usability, complexity, and extensibility. In GraphBIG, instead of directly using a specific industrial framework, we abstract one based on our experience with System G and a large number of interactions with our clients. The workloads are all from representative use cases and cover all major computation types. Meanwhile, the framework and data representation design are both following generic techniques widely used by multiple graph systems.We anticipate that GraphBIG and its comprehensive analysis we include in this paper can better illustrate the trend of full-spectrum graph computing and help the future graph architecture/system design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OVERVIEW OF GRAPHBIG 4.1 Methodology</head><p>To understand the graph computing, we propose Graph-BIG, a benchmark suite inspired by IBM System G, which is a comprehensive set of graph computing tools, cloud, and solutions for Big Data <ref type="bibr" target="#b36">[37]</ref>. GraphBIG includes representative benchmarks from both CPU and GPU sides to achieve a holistic view of general graph computing.</p><p>Framework: To represent real-world scenarios, Graph-BIG utilizes the framework design and data representation inspired by IBM System G, which is a comprehensive graph computing toolsets used by several real-world scenarios. Like many other industrial solutions, the major concerns of Sys-temG design include not only performance, but also flexibility, complexity, and usability. System G framework enables us to utilize its rich use case and dataset support and summarize workloads from one of the representative industrial graph solutions. By ensuring the representativeness of workloads and data representations, GraphBIG help users understand the full-spectrum graph computing.</p><p>Like several other graph systems, GraphBIG follows the vertex-centric data representation, in which a vertex is the basic unit of a graph. The vertex property and the outgoing edges stay within the same vertex structure. All vertices' structures form an adjacency list and the outgoing edges inside the vertex structure also form an adjacency list of edges. The graph computing workloads are implemented via framework primitives, such as find/add/delete vertex/edge and property update.By linking the same core code with various CUDA kernels, the GPU benchmarks are also utilizing the same framework. In addition, because of the nature of GPU computing, the graph data in GPU memory is organized as CSR/COO structure. In the graph populating step, the dynamic vertex-centric graph data in CPU main memory is converted and transferred to GPU side. Moreover, by replacing System G's commercial components with rewritten source codes, we are able to open source GraphBIG for public usage under BSD license.</p><p>Workload Selection: GraphBIG follows the workflow shown in Figure <ref type="figure">3</ref>. By analyzing real-world use cases from IBM System G customers, we summarize computation types and graph data types. Meanwhile, we select workloads and datasets according to their popularity (use frequency). To ensure the coverage, we then reselect the workloads and datasets to cover all computation and data types. After that, we finalize the workloads and datasets to form our GraphBIG benchmark suite. In this way, the representativeness and coverage are addressed at the same time.  To understand real-world graph computing, we analyzed 21 key use cases of graph computing from multiple application domains. The use cases are all from real-world practices of IBM System G customers <ref type="bibr" target="#b36">[37]</ref>  <ref type="bibr" target="#b41">[42]</ref>. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, the use cases come from six different categories, from cognitive computing to data exploration. Their percentage in each category is shown in Figure <ref type="figure" target="#fig_5">4</ref>(B), varying from 24% to 10%. Each use case involves multiple graph computing algorithms. As explained previously, we then select representative workloads from the use cases according to the number of used times. Figure <ref type="figure" target="#fig_5">4(A)</ref> shows the number of use cases of each chosen workload with the breakdown by categories. The most popular workload, BFS, is used by 10 different use cases, while the least popular one, TC, is also used by 4 use cases. From Figure <ref type="figure" target="#fig_5">4</ref>(A), we can see that the chosen workloads are all widely used in multiple real-world use cases. After the summarize step, a reselection is performed in merge step to cover all computation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Benchmark Description</head><p>As explained in Figure <ref type="figure">3</ref> and Figure <ref type="figure" target="#fig_5">4</ref>, we analyze realworld use cases and then select workloads by considering the key factors together. The workloads in our proposed Graph-BIG are summarized in Table <ref type="table" target="#tab_5">4</ref>. For explanation purpose, we group the workloads into four categories according to their high level usages. The details are further explained below.</p><p>Graph traversal: Graph traversal is the most fundamental operation of graph computing. Two workloads -Breadth-first Search (BFS) and Depth-first Search (DFS) are selected. Both are widely-used graph traversal operations.</p><p>Graph construction/update: Graph update workloads are performing computations on dynamic graphs. Three workloads are included as following. (1) Graph construction (GCons) constructs a directed graph with a given number of vertices and edges. (2) Graph update (GUp) deletes a given list of vertices and related edges from a existing graph. (3) Topology morphing (TMorph) generates an undirected moral graph from a directed-acyclic graph (DAG). It involves graph construction, graph traversal, and graph update operations.</p><p>Graph analytics: There are three groups of graph analytics, including topological analysis, graph search/match, and graph path/flow. Since basic graph traversal workloads already cover graph search behaviors, here we focus on topological analysis and graph path/flow. As shown in Table <ref type="table" target="#tab_5">4</ref>, five chosen workloads cover the two major graph analytic types and two computation types. The shortest path is a tool for graph path/flow analytics, while the others are all topological analysis. In their implementations, the shortest path is following Dijkstra's algorithm. The k-core decomposition is using Matula &amp; Beck's algorithm <ref type="bibr" target="#b22">[23]</ref>. The connected component is implemented with BFS traversals on the CPU side and with Soman's algorithm <ref type="bibr" target="#b34">[35]</ref> on the GPU side. The triangle count is based on Schank's algorithm <ref type="bibr" target="#b31">[32]</ref> and the graph coloring is following Luby-Jones' proposal <ref type="bibr" target="#b13">[14]</ref>. Besides, the Gibbs inference is performing Gibbs sampling for approximate inference in bayesian networks.</p><p>Social analysis: Due to its importance, social analysis is listed as a separate category in our work, although generally social analysis can be considered as a special case of generic graph analytics. We select graph centrality to represent social analysis workloads. Since closeness centrality shares significant similarity with shortest path, we include the betweenness centrality with Brandes' algorithm <ref type="bibr" target="#b20">[21]</ref> and degree centrality <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph data support</head><p>To address both representativeness and coverage of graph data sets, we consider two types of graph data, real-world data and synthetic data. Both are equally important, as explained in Section 2. The real-world data sets can illustrate real graph data features, while the synthetic data can help in analyzing workloads because of its flexible data size and relatively short execution time. Meanwhile, since the focus of our work is the architectural impact of graph computing, the dataset selection should not bring obstacles for architectural characterizations on various hardware platforms. Large datasets are infeasible for small-memory platforms because of their huge memory footprint sizes. Therefore, we only include one large graph data in our dataset selection.As shown in Table <ref type="table" target="#tab_6">5</ref>, we collect four real-world data sets and a synthetic data set to cover the requirements of both sides.   <ref type="table" target="#tab_6">5</ref>. The twitter graph is a preprocessed data set of twitter transactions. In this graph, twitter users are the vertices and twit/retwit communications form the edges. In IBM Knowledge Repo, two types of vertices, users and documents, form up a bipartite graph. An edge represents a particular document is accessed by a user. It is from a document recommendation system used by IBM internally. As an example of bio networks, the IBM Watson Gene graph is a data set used for bioinformatic research. It is representing the relationships between gene, chemical, and drug. The CA road network is a network of roads in California <ref type="bibr" target="#b17">[18]</ref>. Intersections and endpoints are represented by nodes and the roads connecting these intersections or road endpoints are represented by undirected edges.</p><p>(2) Synthetic data: The LDBC graph is a synthetic data set generated by LDBC data generator and represents social network features <ref type="bibr" target="#b27">[28]</ref>. The generated LDBC data set can have arbitrary data set sizes while keeping the same features as a facebook-like social network. The LDBC graph enables the possibility to perform detailed characterizations of graph workloads and compare the impact of data set size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CHARACTERIZATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Characterization Methodology</head><p>Hardware configurations: We perform our experiments on an Intel Xeon machine with Nvidia Tesla K40 GPU. The hardware and OS details are shown in Table <ref type="table" target="#tab_8">6</ref>. To avoid the uncertainty introduced by OS thread scheduling, we schedule and pin threads to different hardware cores.   Datasets: In the characterization experiments, we first use synthetic graph data to enable in-depth analysis for multiple architectural features of both CPU and GPU sides. As shown in Table <ref type="table" target="#tab_9">7</ref>, the LDBC graph with 1 million vertices is selected. Four real-world data sets are then included for data sensitivity studies. The Twitter graph is sampled in our experiments because of the extremely large size of the original graph. In our experiments, although the test platform incorporates a large memory capacity on CPU side, the GPU side has only 12GB memory, which limits the dataset size. Thus, huge size datasets are infeasible in the experiments. Moreover, we intentionally select datasets from diverse sources to cover different graph types. With the combination of different graph sizes and types, the evaluation can illustrate the data impact comprehensively. In addition, because of the special computation requirement of Gibbs Inference workload, the bayesian network MUNIN <ref type="bibr" target="#b2">[3]</ref> is used. It includes 1041 vertices, 1397 edges, and 80592 parameters.</p><p>Profiling method: In our experiments, the hardware performance counters are used for measuring detailed hardware statistics. In total, around 30 hardware counters of the CPU side and 25 hardware metrics of the GPU side are collected. For the profiling of CPU benchmarks, we designed our own profiling tool embedded within the benchmarks. It is utilizing the perf event interface of Linux kernel for accessing hardware counters and the libpfm library for encoding hardware counters from event names. For GPU benchmarks, the nvprof tool from Nvidia CUDA SDK is used.</p><p>Metrics for CPUs: In the experiments, we are following a hierarchical profiling strategy. Multiple metrics are utilized to analyze the architectural behaviors.</p><p>For the CPU benchmarks, Execution cycle breakdown is first analyzed to figure out bottleneck of workloads. The breakdown categories include frontend stall, backend stall, retiring, and bad speculation. In modern processors, frontend includes instruction fetching, decoding, and allocation. After allocated, backend is responsible for instruction renaming, scheduling, execution, and commit. It also involves memory sub-systems. Cache MPKI is then analyzed to understand memory sub-system behaviors. We estimated the MPKI values of L1D, L2, and LLC. In addition, we also measured multiple other metrics, including IPC, branch miss rate, ICache miss rate, and DTLB penalty. These metrics cover major architectural factors of modern processors.</p><p>Metrics for GPUs: For the GPU side experiments, we first analyzed the divergence of given benchmarks. Two metrics are measured, one is branch divergence rate (BDR) and another is memory divergence rate (MDR). We use the following equations to express the degree of branch and memory divergence. branch divergence rate (BDR) = inactive threads per warp warp size memory divergence rate (MDR) = replayed instructions issued instructions BDR is the average ratio of inactive threads per warp, which is typical caused by divergent branches. MDR is the fraction of issued instructions that are replayed. In modern GPUs, a load or store instruction would be replayed if there is a bank conflict or the warp accesses more than one 128-byte block in memory. The replay may happen multiple times until all the requested data have been read or written. Thus, we estimate the memory divergence by measuring the number of replayed instructions. Both BDR and MDR range from 0 to 1 with higher value representing higher divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CPU Characterization Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Workload Characterization</head><p>In this section, we characterize GraphBIG CPU workloads with a top-down characterization strategy. The results are explained as following.</p><p>Execution time breakdown: The execution time breakdown is shown in Figure <ref type="figure">5</ref>   Core analysis: Although execution stall can be triggered by multiple components in the core architecture, instruction fetch and branch prediction are usually the key inefficiency sources. Generally, a large number of ICache misses or branch miss predictions can significantly affect architectural performance, because modern processors usually don't incorporate efficient techniques to hide ICache/branch related penalties. In previous literatures, it was reported that many big data workloads, including graph applications, suffer from high ICache miss rate <ref type="bibr" target="#b10">[11]</ref>. However, in our experiments, we observe different outcomes.As shown in Figure <ref type="figure" target="#fig_6">6</ref>, the ICache MPKI of each workload all show below 0.7 values, though small variances still exist. The differ-ent ICache performance values are resulted from the design differences of the underlying frameworks. Open-source big data frameworks typically incorporate many other existing libraries and tools. Meanwhile, the included libraries may further utilize other libraries. Thus, it eventually results in deep software stacks, which lead to complex code structures and high ICache MPKI. However, in GraphBIG, very few external libraries are included and a flat software hierarchy is incorporated. Because of its low code structure complexity, GraphBIG shows a much lower ICache MPKI.</p><p>The branch prediction also shows low miss prediction rate in most workloads except for TC, which reaches as high as 10.7%. The workloads from other computation types show a miss prediction rate below 5%. The difference comes from the special intersection operations in TC workload. It is also in accordance with the above breakdown result, in which TC consumes a significant amount of cycles in BadSpeculation.</p><p>The DTLB miss penalty is shown in Figure <ref type="figure" target="#fig_6">6</ref>. The cycles wasted on DTLB misses is more than 15% of totaly execution cycles for most workloads. On average, it still takes 12.4%. The high penalty is caused by two sources. One is the large memory footprint of graph computing applications, which cover a large number of memory pages. Another is the irregular access pattern, which incorporates extremely low page locality. Diversity among workloads also exists. The DTLB miss penalty reaches as high as 21.1% for Connected Component and as low as 3.9% for TC and 1% for Gibbs. This is because for computation on properties, the memory accesses are centralized within the vertices. Thus, low DTLB-miss penalty time is observed. In Figure <ref type="figure" target="#fig_7">7</ref>, the MPKI of different levels of caches are shown. On average, a high L3 MPKI is shown, reaching as high as 48.77. Degree Centrality and Connected Component show even higher MPKI, which are 145.9 and 101.3 respectively. For computations on the graph structure (CompStruct), a generally high MPKI is observed. On the contrary, Comp-Prop shows an extremely small MPKI value compared with other workloads. This is in accordance with its computation features, in which memory accesses happen mostly inside properties with a regular pattern. The workloads of computation on dynamic graphs (CompDyn) introduce diverse results, ranging from 6.3 to 27.5 in L3 MPKI. This is because of the diverse operations of each workload. GCons adds new vertices/edges and sets their properties one by one, while GUp mostly deletes them in a random manner.</p><p>In GCons, significantly better locality is observed because each new vertex/edge will be immediately reused after insertion. The TMorph involves graph traversal, insertion, and deletion operations. Meanwhile, unlike other workloads, TMorph includes no small size local queues/stacks, leading to a high MPKI in L1D cache. However, its graph traversal pattern results in relatively good locality in L2 and L3. Data sensitivity: To study the impact of input data sets, we performed experiments on four real-world data sets from different types of sources and the LDBC synthetic data (We excluded the workloads that cannot take all input datasets).</p><p>Despite the extremely low L2/L3 hit rates, Figure <ref type="figure">9</ref> shows relatively higher L1D hit rates for almost all workloads and data sets. This is because graph computing applications all incorporate multiple small size structures, such as task queues and temporal local variables. The frequently accessed meta data introduces a large amount of L1D cache hits except for DCentr, in which there is a only limited amount of meta data accesses. From the results in Figure <ref type="figure">9</ref>, we can also see that twitter data shows highest DTLB miss penalty in most workloads. Such behavior eventually turns into lowest IPC values in most workloads except SPath, in which higher L1D cache hit rate of the twitter graph helps performance significantly. Triangle Count (TC) achieves highest IPC with the knowledge data set, because of its high L2/L3 hit rate and low TLB penalty. The high L3 hit rate of the watson data also results in a high IPC value. However, the twitter graph's high L3 hit rate is offsetted by its extremely high DTLB miss cycles, leading to the lowest IPC value. The diversity is caused by the different topological and property features of the real-world data sets. It is clearly shown that significant impacts are introduced by the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Observations</head><p>In the characterization experiments, by measuring several architectural factors, we observed multiple graph computing features. The key observations are summarized as following.</p><p>• Backend is the major bottleneck for most graph computing workloads, especially for CompStruct category. However, such behavior is much less significant in Comp-Prop category.</p><p>• The ICache miss rate of GraphBIG is as low as conventional applications, unlike many other big data applications, which are known to have high ICache miss rate. This is because of the flat code hierarchy of the underlying framework.</p><p>• Graph computing is usually considered to be cacheunfriendly. L2 and L3 caches indeed show extremely low hit rates in GraphBIG. However, L1D cache shows significantly higher hit rates. This is because of the locality of non-graph data, such as temporal local variables and task queues.</p><p>• Although typically DTLB is not an issue for conventional applications, it is a significant source of inefficiencies for graph computing. In GraphBIG, a high DTLB miss penalty is observed because of the large memory footprint and low page locality.</p><p>• Graph workloads from different computation types show significant diversity in multiple architectural features.</p><p>The study on graph computing should consider not only graph traversals, but also the other computation types.</p><p>• Input graph data has significant impact on memory sub-systems and the overall performance. The impact is from both the data volume and the graph topology.</p><p>The major inefficiency of graph workloads comes from memory subsystem. Their extremely low cache hit rate introduces challenges as well as opportunities for future graph architecture/system research. Moreover, the low ICache miss rate of GraphBIG demonstrates the importance of proper software stack design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">GPU Characterization Results</head><p>We characterize the GPU workloads of GraphBIG in this section. The experiments are performed via nvprof on real machines. The results are summarized below. Irregularity analysis: To estimate the irregularity of graph computing on GPU, we measured the degree of both branch and memory divergence. As explained in Section 5.1, the metrics we used in the experiments are branch divergence rate (BDR) and memory divergence rate (MDR). With a higher BDR value, more threads in the same warp take different execution paths, leading to a lower warp utilization. Similarly, a higher MDR value indicates that more memory requests contain bank conflicts or are accessing more than 128-byte blocks. In this case, instruction replays are triggered to fulfill all memory requests.</p><p>Figure <ref type="figure" target="#fig_9">10</ref> shows a scatter plot of the workloads where the x-axis represents MDR and the y-axis represents BDR. Each dot in the figure corresponds to a GraphBIG workload. From Figure <ref type="figure" target="#fig_9">10</ref>, we can observe that most workloads cannot be simply classified as branch-bound or memory-bound. A generally high divergence degree from both sides are shown. In addition, the workloads show a quite scatter distribution across the whole space. For example, kCore stays at the lower-left corner, showing relatively lower divergence in both branch and memory. On the contrary, DCentr is showing extremely high divergence in both sides. Meanwhile, branch divergence is the key issue of GColor and BCentr, while for CComp and TC, the issue is only from memory side.</p><p>The high branch divergence for graph computing comes from the thread-centric design, in which each thread processes one vertex. However, the working set size of each vertex is corresponding its degree, which can vary greatly. The unbalanced per-thread workload introduces divergent branches, especially for the loop condition checks, leading to branch divergence behaviors.In Figure <ref type="figure" target="#fig_9">10</ref>, a relatively higher BDR is observed in GColor and BCentr because of the heavier per-edge computation. On the contrary, CComp and TC show small BDR values because they are both following an edge-centric model, in which each thread processes one edge.</p><p>In typical graph workloads, because of the sparse dis-tributed edges, accesses to both the neighbor list and vertex property are spreading across the whole graph. Hence, the traversal of each edge involves cache misses and divergent memory accesses. Depending on the reliance of graph properties and warp utilization, the degree of memory divergence may vary. As shown in Figure <ref type="figure" target="#fig_9">10</ref>, the MDR value can be as low as 0.25 in kCore and as high as 0.87 in DCentr. Memory throughput and IPC: The GPU device memory throughput results are shown in Figure <ref type="figure" target="#fig_10">11</ref>. Although the Tesla K40 GPU supports up to 288 GB/s memory bandwidth, in our experiments, the highest read throughput is only 89.9 GB/s in CComp. The inefficient bandwidth utilization is cause by divergence in both branch and memory. The memory throughput results shown in Figure <ref type="figure" target="#fig_10">11</ref> are determined by multiple factors, including data access intensity, memory divergence, and branch divergence. For example, CComp incorporates intensive data accesses and low branch divergence. It shows the highest throughput value. DCentr has extremely intensive data accesses. Hence, even though DCentr has high branch and memory divergence, its throughput is still as high as 75.2 GB/s. A special happens at TC, which shows only 2.0 GB/s read throughput. This is because TC is mostly performing intersection operations between neighbor vertices with quite low data intensity. Since data accesses typically are the bottleneck, the memory throughput outcomes also reflect application performance in most workloads except for DCentr and TC. In DCentr, despite the high memory throughput, intensive atomic instrcutions significantly reduce performance. Meanwhile, unlike other workloads, TC involves lots of parallel arithmetic compare operations. Hence, TC shows the highest IPC value. Speedup over CPU: Although GPU is usually considered to be suitable for regular applications, irregular graph computing applications still receive performance benefits by utilizing GPUs. In Figure <ref type="figure" target="#fig_11">12</ref>, the speedup of GPU over 16core CPU is shown. In the experiments, we utilized the GraphBIG workloads that are shared between GPU and CPU sides. In our comparison, the major concern is incore computation time, not end-to-end time. The data loading/transfer/initialize time are not included. Besides, as explained in Section 4, the dynamic vertex-centric data layout is utilized at CPU side, while GPU side uses CSR graph format.</p><p>From the results in Figure <ref type="figure" target="#fig_11">12</ref>, we can see that GPU provides significant speedup in most workloads and datasets. The speedup can reach as high as 121 in CComp and around 20x in many other cases. In general, the significant speedup achieved by GPU is because of two major factors, threadlevel parallelism (TLP) and data locality. It is difficult to benefit from instruction-level parallelism in graph applications. However, the rich TLP resources in GPUs can still provide significant performance potentials. Meanwhile, the CSR format in GPUs brings better data locality than the dynamic data layout in CPUs. Specifically, the DCentr shows high speedup number with CA-RoadNet because of the low branch divergence and static working set size. Likewise, CComp also shows similar behaviors. On the contrary, BFS and SPath show significant lower speedup values because of the low efficiency introduced by varying working set size. The speedup of TC is even lower. This is because of its special computation type. In TC, each thread incorporates heavy per-vertex computation, which is inefficient for GPU cores. Dataset sensitivity: To estimate the sensitivity of input datasets, we performed divergence analysis on the four realworld datasets and LDBC-1M synthetic graph. In Figure <ref type="figure" target="#fig_12">13</ref>, the results of different workloads are shown with different symbols in the same space, meanwhile different datasets are marked with corresponding initial letters.</p><p>From Figure <ref type="figure" target="#fig_12">13</ref>, we can see that in many workloads, the divergence changes significantly for different datasets. As data-centric computing, graph workloads' behaviors are data dependent. However, the results of several datasets also tend to cluster in the same region. For CComp and TC, the branch divergence rate does not change much between different input graphs. This is expected behavior for them. They both incorporate an edge-centric implementation, in which workload is partitioned by edges, ensuring balanced workset size between threads. Because of its low branch divergence feature, kCore also shows quite low variability in branch divergence. Both BFS and SPath show similarly low BDR values for CA-RoadNet, Watson-gene, and Knowledgerepo. This is in accordance with the graph features. Both Watson and Knowledge graphs contains small-size local subgraphs, while CA-RoadNet includes much smaller vertex degree. Nevertheless, in Twitter and LDBC graphs, their social network features brings high BDR values. Meanwhile, unlike Twitter has a few vertices with extremely higher degree, the unbalanced degree distribution in LDBC involves more vertices. It leads to even higher warp divergence. Similar diversity happens in GColor and DCentr, which show much lower BDR values for CA-RoadNet graph because of its quite low vertex degrees.</p><p>Unlike branch divergence, MDR generally shows much higher variability for most workloads. It demonstrates the data sensitivity of memory divergence. Meanwhile, exceptions also exist. For example, BFS and SPath both show similar MDR values for CA-RoadNet, Watson-gene, and Knowledge-repo. As explained above, their special graph structures lead to a small number of traversed edges in each iteration. Thus, the impact of input graph is reduced. Moreover, the higher irregularity in edge distribution of LDBC leads to significantly higher MDR values in most workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Observations</head><p>Unlike the conventional applications, the irregularity of graph computing brings unique behaviors on GPUs. We summarize the key observations as follows.</p><p>• Although graph computing is usually considered as less suitable for GPUs, with proper designs, GPU graph applications can achieve significant speedup over the corresponding CPU implementations.</p><p>• Branch divergence is known to be the top issue for graph computing on GPUs. We observe that besides branch divergence, graph computing suffers from even higher memory divergence, leading to inefficient memory bandwidth utilizations.</p><p>• Graph workloads cannot fully utilize the GPU's execution capability. An extremely low IPC value is observed for most GraphBIG workloads.</p><p>• The behaviors of graph computing are data dependent. Input graph has comprehensive impacts on both branch and memory divergence. Specifically, memory divergence shows higher data sensitivity.</p><p>• Interestingly, the GPU graph workloads have significantly higher data sensitivity than the CPU ones. CPU/GPU sides show different data-application correlations, because of the architecture differences.</p><p>• Although traversal based workloads show similar behaviors, significant diverse behaviors across all workloads are observed. It is difficult to summarize general features that can be applied on all graph workloads. Hence, a representative study should cover not only graph traversals, but also the other workloads.</p><p>Although suffering from high branch and memory divergence, graph computing on GPUs still show significant performance benefits in most cases. Meanwhile, like CPU workloads, GPU graph computing also incorporate workload diversity and data dependent behaviors. In addition, comparing with CPU workloads, GPU graph workloads have much higher data sensitivity and more complex correlations between input data and application. To improve the performance of GPU graph computing, new architecture/system techniques considering both workload diversity and data sensitivity are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we discussed and summarized the key factors of graph computing, including frameworks, data representations, graph computation types, and graph data sources. We analyzed real-world use cases of IBM System G customers to summarize the computation types, and graph data sources. We also demonstrated the impact of framework and data representation.</p><p>To understand the full-spectrum graph computing, we presented GraphBIG, a suite of CPU/GPU benchmarks. Our proposed benchmark suite addressed all key factors simultaneously by utilizing System G framework design and following a comprehensive workload selection procedure. With the summary of computation types and graph data sources, we selected representative workloads from key use cases to cover all computation types. In addition, we provided realworld data sets from different source types and synthetic social network data for characterization purposes.</p><p>By performing experiments on real machines, we characterized GraphBIG workloads comprehensively. From the experiments, we observed following behaviors. (1) Conventional architectures do not perform well for graph computing. Significant inefficiencies are observed in CPU memory sub-systems and GPU warp/memory bandwidth utilizations. (2) Significant diverse behaviors are shown in different workloads and different computation types. Such diversity exists on both CPU and GPU sides, and involves multiple architectural features. (3) Graph computing on both of CPUs and GPUs are highly data sensitive. Input data has significant and complex impacts on multiple architecture features.</p><p>As the first comprehensive architectural study on fullspectrum graph computing, GraphBIG can be served for architecture and system research of graph computing. In the future, we will also extend GraphBIG to other platforms, such as near-data processing (NDP) units <ref type="bibr" target="#b4">[5]</ref>, IBM BlueGene/Q, and IBM System Z.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SC ' 15 ,</head><label>15</label><figDesc>November 15-20, 2015, Austin, TX, USA c 2015 ACM. ISBN 978-1-4503-3723-6/15/11. . . $15.00 DOI: http://dx.doi.org/10.1145/2807591.2807626</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Execution Time of Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2(a)(b), CSR organizes vertices, edges, and properties of graph G in separate compact arrays. (Variants of CSR also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of data representations. (a) graph G, (b) its CSR representation, and (c) its vertex-centric representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Real-world use case analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: DTLB Penalty, ICache MPKI, and Branch Miss Rate of GraphBIG CPU Workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Cache MPKI of GraphBIG CPU WorkloadsCache performance: As shown in previous sections, cache plays a crucial role in graph computing performance. In Figure7, the MPKI of different levels of caches are shown. On average, a high L3 MPKI is shown, reaching as high as 48.77. Degree Centrality and Connected Component show even higher MPKI, which are 145.9 and 101.3 respectively. For computations on the graph structure (CompStruct), a generally high MPKI is observed. On the contrary, Comp-Prop shows an extremely small MPKI value compared with other workloads. This is in accordance with its computation features, in which memory accesses happen mostly inside properties with a regular pattern. The workloads of computation on dynamic graphs (CompDyn) introduce diverse results, ranging from 6.3 to 27.5 in L3 MPKI. This is because of the diverse operations of each workload. GCons adds new vertices/edges and sets their properties one by one, while GUp mostly deletes them in a random manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Average Behaviors of GraphBIG CPU Workloads by Computation Types Computation type behaviors: The average behaviors of each computation type are shown in Figure 8. Although variances exist within each computation type, the average results demonstrate their diverse features. The CompStruct shows significantly higher MPKI and DTLB miss penalty values because of its irregular access pattern when traversing through graph structure. Low and medium MPKI and DTLB values are shown in CompProp and CompDyn respectively. Similarly, the CompProp suffers from a high branch miss rate while other two types do not. In the IPC results, CompStruct achieves the lowest IPC value due to the penalty from cache misses. On the contrary, CompProp shows the highest IPC value. The IPC value of CompDyn stays between them. Such feature is in accordance with their access patterns and computation types.Data sensitivity: To study the impact of input data sets, we performed experiments on four real-world data sets from different types of sources and the LDBC synthetic data (We excluded the workloads that cannot take all input datasets).Despite the extremely low L2/L3 hit rates, Figure9shows relatively higher L1D hit rates for almost all workloads and data sets. This is because graph computing applications all incorporate multiple small size structures, such as task queues and temporal local variables. The frequently accessed meta data introduces a large amount of L1D cache hits except for DCentr, in which there is a only limited amount of meta data accesses. From the results in Figure9, we can also see that twitter data shows highest DTLB miss penalty in most workloads. Such behavior eventually turns into lowest IPC values in most workloads except SPath, in which higher L1D cache hit rate of the twitter graph helps performance significantly. Triangle Count (TC) achieves highest IPC with the knowledge data set, because of its high L2/L3 hit rate and low TLB penalty. The high L3 hit rate of the watson data also results in a high IPC value. However, the twitter graph's high L3 hit rate is offsetted by its extremely high DTLB miss cycles, leading to the lowest IPC value. The diversity is caused by the different topological and property features of the real-world data sets. It is clearly shown that significant impacts are introduced by the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Branch and Memory Divergence of GraphBIG GPU workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Memory Throughput and IPC of Graph-BIG GPU Workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Speedup of GPU over 16-core CPU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Branch and Memory Divergence of GraphBIG GPU Workloads with Different Datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Graph Computation Type Summary</figDesc><table><row><cell cols="2">Graph Computation Type</cell><cell></cell><cell cols="2">Feature</cell><cell>Example</cell></row><row><cell cols="3">Computation on graph structure (CompStruct)</cell><cell cols="2">Irregular access pattern, heavy read accesses BFS traversal</cell></row><row><cell cols="5">Computation on graphs with rich properties (CompProp) Heavy numeric operations on properties</cell><cell>Belief propagation</cell></row><row><cell cols="2">Computation on dynamic graphs (CompDyn)</cell><cell></cell><cell cols="2">Dynamic graph, dynamic memory footprint</cell><cell>Streaming graph</cell></row><row><cell cols="2">No. Graph Data Source</cell><cell>Example</cell><cell></cell><cell>Feature</cell></row><row><cell>1</cell><cell cols="3">Social(/economic/political) network Twitter graph</cell><cell>Large connected components, small shortest path lengths</cell></row><row><cell>2</cell><cell>Information(/knowledge) network</cell><cell cols="3">Knowledge graph Large vertex degrees, large small hop neighbourhoods</cell></row><row><cell>3</cell><cell>Nature(/bio/cognitive) network</cell><cell cols="2">Gene network</cell><cell>Complex properties, structured topology</cell></row><row><cell>4</cell><cell>Man-made technology network</cell><cell cols="2">Road network</cell><cell>Regular topology, small vertex degrees</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Graph Data Source Summary</figDesc><table><row><cell cols="2">Benchmark Graph Workloads Framework</cell><cell>Data</cell><cell>Computation</cell><cell>Data Support</cell></row><row><cell></cell><cell></cell><cell>Representation</cell><cell>Type</cell><cell></cell></row><row><cell>SPEC int mcf, astar</cell><cell>NA</cell><cell>Arrays</cell><cell>CompStruct</cell><cell>Data type 4</cell></row><row><cell>CloudSuite [11] TunkRank</cell><cell>GraphLab [20]</cell><cell>Vertex-centric</cell><cell>CompStruct</cell><cell>Data type 1</cell></row><row><cell>Graph 500 [25] Reference code</cell><cell>NA</cell><cell>CSR</cell><cell>CompStruct</cell><cell>Synthetic data</cell></row><row><cell>BigDataBench [39] 4 workloads</cell><cell>Hadoop</cell><cell>Tables</cell><cell>CompStruct</cell><cell>Data type 1</cell></row><row><cell>SSCA [4] 4 kernels</cell><cell>NA</cell><cell>CSR</cell><cell>CompStruct</cell><cell>Synthetic data</cell></row><row><cell>PBBS [34] 5 workloads</cell><cell>NA</cell><cell>CSR</cell><cell>CompStruct</cell><cell>Synthetic data</cell></row><row><cell>Parboil [36] GPU-BFS</cell><cell>NA</cell><cell>CSR</cell><cell>CompStruct</cell><cell>Synthetic data</cell></row><row><cell>Rodinia [9] 3 GPU kernels</cell><cell>NA</cell><cell>CSR</cell><cell>CompStruct</cell><cell>Synthetic data</cell></row><row><cell>Lonestar [7] 3 GPU kernels</cell><cell>NA</cell><cell>CSR</cell><cell>CompStruct</cell><cell>Synthetic data</cell></row><row><cell>GraphBIG 12 CPU workloads</cell><cell cols="2">IBM System G [37] Vertex-centric</cell><cell>CompStruct/CompProp</cell><cell>All types &amp;</cell></row><row><cell>8 GPU workloads</cell><cell></cell><cell>/CSR</cell><cell>/CompDyn</cell><cell>synthetic data</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between GraphBIG and Prior Graph Benchmarks. Computation and Data Types are Summarized in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>of Use Cases with Each Workload (B) Distribution of Selected Use Cases in 6 Categories</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>SystemG Use Cases</cell><cell></cell><cell>Summarize Select</cell><cell></cell><cell cols="6">Computa/on Types Workloads Graph Data Types Datasets</cell><cell></cell><cell></cell><cell></cell><cell>Reselec/on</cell><cell>Representa/ve Workloads Representa/ve Datasets</cell><cell>GraphBIG</cell></row><row><cell></cell><cell cols="15">Figure 3: GraphBIG Workload Selection Flow</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Science Exploration and</cell></row><row><cell>Use Case #</cell><cell>2 4 6 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cognitive Computing Data Warehouse Augmentation Operations Analysis Security</cell></row><row><cell></cell><cell>0</cell><cell>BFS</cell><cell>DFS</cell><cell>GCons</cell><cell>GUp</cell><cell>TMorph</cell><cell>SPath</cell><cell>kCore</cell><cell>CComp</cell><cell>GColor</cell><cell>TC</cell><cell>Gibbs</cell><cell>Dcentr</cell><cell>BCentr</cell><cell>Data Exploration 360 Degree View</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(A) #</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">24%</cell><cell></cell><cell></cell><cell></cell><cell cols="2">14%</cell><cell></cell><cell cols="2">14%</cell><cell></cell><cell></cell><cell>14%</cell><cell>10%</cell><cell>24%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>GraphBIG Workload SummaryThe details of the chosen data sets are explained below. All data sets are publicly available in our github wiki.</figDesc><table><row><cell>Data Set</cell><cell>Type</cell><cell cols="2">Vertex# Edge#</cell></row><row><cell>Twitter Graph</cell><cell>Type 1</cell><cell>120M</cell><cell>1.9B</cell></row><row><cell>IBM Knowledge Repo</cell><cell>Type 2</cell><cell>154K</cell><cell>1.72M</cell></row><row><cell cols="2">IBM Watson Gene Graph Type 3</cell><cell>2M</cell><cell>12.2M</cell></row><row><cell>CA Road Network</cell><cell>Type 4</cell><cell>1.9M</cell><cell>2.8M</cell></row><row><cell>LDBC Graph</cell><cell cols="2">Synthetic Any</cell><cell>Any</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Graph Data Set Summary(1) Real-world data: Four real-world graph data sets are provided, including twitter graph, IBM knowledge Repo, IBM Watson Gene Graph, and CA road network. The vertex/edge numbers of each data set are shown in Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Test Machine configurations</figDesc><table><row><cell>Experiment Data Set</cell><cell cols="2">Vertex # Edge #</cell></row><row><cell>Twitter Graph (sampled)</cell><cell>11M</cell><cell>85M</cell></row><row><cell>IBM Knowledge Repo</cell><cell>154K</cell><cell>1.72M</cell></row><row><cell cols="2">IBM Watson Gene Graph 2M</cell><cell>12.2M</cell></row><row><cell>CA Road Network</cell><cell>1.9M</cell><cell>2.8M</cell></row><row><cell>LDBC Graph</cell><cell>1M</cell><cell>28.82M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Graph Data in the Experiments</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>and grouped by computation types. The Frontend and Backend represent the frontend Retiring is the cycles of successfully retired instructions. It is a common intuition that irregular data accesses are the major source of inefficiencies of graph computing.The breakdown of execution time also supports such intuition. It is shown that the backend indeed takes dominant time for most workloads. In extreme cases, such as kCore and GUp, the backend stall percentage can be even higher than 90%. However, different from the simple intuition, the outliers also exist. For example, the workloads of computation on rich properties (CompProp) category shows only around 50% cycles on backend stalls. The variances between computation types further demonstrates the necessity of covering different computation types.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CompStruct</cell><cell>CompProp</cell><cell>CompDyn</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">100%</cell></row><row><cell>Breakdown of Execution</cell><cell cols="2">Cycles</cell><cell></cell><cell>20% 40% 60% 80%</cell><cell>Backend Retiring BadSpeculation Frontend</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell></row><row><cell cols="5">Figure 5: Execution Time Breakdown of GraphBIG</cell></row><row><cell cols="5">CPU Workloads</cell></row><row><cell cols="5">bound and backend bound stall cycles respectively. The</cell></row><row><cell cols="5">BadSpeculation is the cycles spent on wrong speculations,</cell></row><row><cell cols="5">10% 20% while the 0% DTLB Miss Cycle %</cell><cell>CompStruct</cell><cell>CompProp</cell><cell>CompDyn</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell cols="2">ICache</cell><cell cols="2">MPKI</cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row><row><cell>Branch Miss</cell><cell cols="2">Prediction %</cell><cell cols="2">0% 4% 8% 12%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We gratefully acknowledge the support of National Science Foundation (NSF) XPS 1337177. We would like to thank IBM System G group, other HPArch members, our shepherd, and the anonymous reviewers for their comments and suggestions. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Boost Graph Library: User Guide and Reference Manual</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Addison-Wesley Longman</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Giraph</surname></persName>
		</author>
		<ptr target="http://giraph.apache.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MUNIN -an expert EMG assistant</title>
		<author>
			<persName><forename type="first">S</forename><surname>Andreassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer-Aided Electromyography and Expert Systems</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Design and implementation of the hpcs graph analysis benchmark on symmetric multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bader</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Near-data processing: Insights from a micro-46 workshop</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A quantitative study of irregular programs on gpus</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nasre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>IISWC&apos;12</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">System G Data Store: Big, rich graph data analytics in the cloud</title>
		<author>
			<persName><forename type="first">M</forename><surname>Canim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chang</surname></persName>
		</author>
		<idno>IC2E&apos;13</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rodinia: A benchmark suite for heterogeneous computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-10">Oct 2009</date>
		</imprint>
	</monogr>
	<note>IISWC&apos;09</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><surname>Chesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Number pt</title>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware. ASPLOS&apos;12</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mapgraph: A high level api for fast development of high performance graph analytics on gpus</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Personick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Benchmarking graph-processing platforms: A vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A parallel graph coloring heuristic</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Plassmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Centralities in large networks: Algorithms and observations</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM&apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cusha: Vertex-centric graph processing on gpus</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khorasani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graphchi: Large-scale graph computation on just a pc</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krevl</surname></persName>
		</author>
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social network analysis in enterprise</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012-09">Sept 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed graphlab: A framework for machine learning and data mining in the cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Madduri</surname></persName>
		</author>
		<idno>IPDPS&apos;09</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pregel: A system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD&apos;10</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smallest-last ordering and clustering and graph coloring algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Managing large dynamic graphs efficiently. SIGMOD&apos;12</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introducing the graph 500</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cray User&apos;s Group (CUG)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Information network or social network?: The structure of the twitter follow graph</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<imprint>
			<publisher>WWW Companion</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cache-conscious graph collaborative filtering on multisocket multicore systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">S3g2: A scalable structurecorrelated social graph generator. TPCTC&apos;12</title>
		<author>
			<persName><forename type="first">M.-D</forename><surname>Pham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parallel graph algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Deo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1984-09">Sept. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">X-stream: Edge-centric graph processing using streaming partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP&apos;13</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Genetic and molecular network analysis of behavior</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int Rev Neurobiol</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Finding, counting and listing all triangles in large graphs, an experimental study. WEA&apos;05</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Trinity: A distributed graph engine on a memory cloud. SIGMOD&apos;13</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Brief announcement: The problem based benchmark suite</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPAA&apos;12</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A fast gpu algorithm for graph connectivity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
		<idno>IPDPSW&apos;10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Parboil: A revised benchmark suite for scientific and commercial throughput computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Stratton</surname></persName>
		</author>
		<idno>IMPACT-12-01</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>UIUC</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A highly efficient runtime and graph library for large scale graph analytics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Can portability improve performance?: An empirical study of parallel graph analytics. ICPE&apos;15</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Varbanescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bigdatabench: A big data benchmark suite from internet services</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA&apos;14</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A programmatic introduction to neo4j. SPLASH &apos;12</title>
		<author>
			<persName><forename type="first">J</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">How accurately can one&apos;s interests be inferred from friends</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">System G: Graph analytics, storage and runtimes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tutorial on the 19th ACM PPoPP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Concurrent image query using local random walk with restart on large scale graphs. ICMEW</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-07">July 2014</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Topologically adaptive parallel breadth-first search on multicore processors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<idno>PDCS&apos;09</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
