<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangtao</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Spectral Imaging Technology CAS</orgName>
								<orgName type="department" key="dep2">Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an, Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Optical Imagery Analysis and Learning</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of the Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE409322E064BAEC28DAA0B6A1417983</idno>
					<idno type="DOI">10.1109/TGRS.2019.2893115</idno>
					<note type="submission">Manuscript received March 21, 2018; revised May 18, 2018; accepted December 22, 2018.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aerial scene classification</term>
					<term>convolutional neural networks (CNNs)</term>
					<term>Fisher vector (FV)</term>
					<term>multiscale representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a fundamental problem in earth observation, aerial scene classification tries to assign a specific semantic label to an aerial image. In recent years, the deep convolutional neural networks (CNNs) have shown advanced performances in aerial scene classification. The successful pretrained CNNs can be transferable to aerial images. However, global CNN activations may lack geometric invariance and, therefore, limit the improvement of aerial scene classification. To address this problem, this paper proposes a deep scene representation to achieve the invariance of CNN features and further enhance the discriminative power. The proposed method: 1) extracts CNN activations from the last convolutional layer of pretrained CNN; 2) performs multiscale pooling (MSP) on these activations; and 3) builds a holistic representation by the Fisher vector method. MSP is a simple and effective multiscale strategy, which enriches multiscale spatial information in affordable computational time. The proposed representation is particularly suited at aerial scenes and consistently outperforms global CNN activations without requiring feature adaptation. Extensive experiments on five aerial scene data sets indicate that the proposed method, even with a simple linear classifier, can achieve the state-of-theart performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For example, some identical land-cover types are frequently shared among different scene categories <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Such complexity of spatial and structural patterns makes aerial scene classification be a fairly challenging problem <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>To identify the semantic labels in aerial scene classification, a necessary procedure is to construct a scene representation for aerial images. In recent years, extensive efforts have been made to develop a robust scene representation and many aerial scene classification methods have been proposed. These efforts have shown that constructing a holistic scene representation for aerial scene classification is intuitively feasible and yields good performance <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>. According to the features extracted, the existing aerial scene classification methods can be generally divided into two main groups: the low-level methods and high-level methods. The low-level methods aim to represent the remote sensing scenes by encoding various handcrafted features with different local descriptor encoding methods (e.g., bag-of-visual-words (BoVWs) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, spatial pyramid matching (SPM) <ref type="bibr" target="#b1">[2]</ref>, vector of locally aggregated descriptors (VLADs) <ref type="bibr" target="#b18">[19]</ref>, and Fisher vector (FV) <ref type="bibr" target="#b19">[20]</ref>). The high-level methods <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b24">[25]</ref> use deep neural networks to learn a global feature from the raw aerial images and cast the scene representation as an end-to-end problem. Inspired by the powerful feature representation, the deep convolutional neural networks (CNNs) <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b29">[30]</ref> are now the dominant methods in the majority of aerial scene classification. Especially the high-level methods have achieved significantly improved classification performance since CNNs can obtain powerful feature representations to describe the aerial scene. Compared with the traditional low-level methods, high-level methods extract impressive representations from aerial scenes, instead of relying on manually designed features.</p><p>More recently, the early works are preferable to fully training a new CNN. Unfortunately, it is difficult to train a new CNN because it needs a considerable amount of labeled data set, propagates to overfitting and brings huge computational cost <ref type="bibr" target="#b24">[25]</ref>. Many recent works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> have demonstrated that the pretrained CNNs can be transferable to aerial scene classification without any training modalities (finetuning or training from scratch). Furthermore, some works demonstrated that the pretrained CNN used as feature extractors to construct scene representations is preferred for aerial scene classification when there are not sufficient training data available <ref type="bibr" target="#b21">[22]</ref>. Following these observations, the pretrained CNN is used as feature extractors in this paper. However, directly using the fully connected (FC) layer of the pretrained CNN might not be suited for aerial scene classification <ref type="bibr" target="#b32">[33]</ref>. There are three limitations as follows.</p><p>1) The original pooling layers in CNNs are too simple (seen in Fig. <ref type="figure" target="#fig_0">1</ref>). The local features in aerial scenes follow a complex distribution <ref type="bibr" target="#b32">[33]</ref>. Much information may be lost when performing spatial aggregation in pooling layers. 2) To be compatible with the FC layers <ref type="bibr" target="#b33">[34]</ref>, the prevalent CNNs require a fixed input image size <ref type="bibr" target="#b34">[35]</ref>. This requirement particularly makes CNN less suitable for aerial scenes that vary greatly in orientation and scales.</p><p>3) The FC layers may be more domain-specific and therefore less transferrable than the convolutional layers <ref type="bibr" target="#b35">[36]</ref>. To address these limitations, we reuse CNN activations from convolutional layers as off-the-shelf features for aerial scene classification. Instead of simply extracting deep features from the CNN activations of the FC layer <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, we build a more invariant representation on the last convolutional layers to improve classification performance. The idea is summarized in Fig. <ref type="figure">2</ref>. First, we extract activations of the last convolutional layers (conv5) from the raw image without requiring a fixed input size <ref type="bibr" target="#b33">[34]</ref> and generate convolutional feature maps of arbitrary sizes. Then, we perform multiscale max pooling on each feature map to build local descriptors of aerial scenes. Finally, we apply an encoding method to generate a multiscale invariant scene representation.</p><p>In the first stage, the activations from the last convolutional layers (conv5) of the network have achieved superior results for image classification <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. These convolutional layers are discriminative, semantically meaningful, and contain structural information crucial for the localization task <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Furthermore, compared with FC layers, these convolutional layers can mitigate the need of task-specific fine-tuning <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p><p>In the second stage, multiscale max pooling is adopted to improve invariance to multiscale deformations. In previous deep networks <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, the convolutional layers are followed by max-pooling layers. Although the max-pooling layer can enhance invariance by pooling in local spatial bins of feature maps <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, some invariance properties <ref type="bibr" target="#b38">[39]</ref> may be undermined such as invariance to larger scale and global deformations. Multiscale representation has been shown to be robust to object deformations <ref type="bibr" target="#b40">[41]</ref> such as FV-CNN <ref type="bibr" target="#b35">[36]</ref>, multiscale orderless pooling (MOP) <ref type="bibr" target="#b38">[39]</ref>, and spatial pyramid pooling (SPP) <ref type="bibr" target="#b33">[34]</ref>.</p><p>Finally, given the multiscale activations from the pooling layer, we need to generate a discriminative holistic-level representation. In contrast to directly using the FC layer to concatenate the outputs of the multiscale pooling (MSP) layer, we extract the multiscale activations as local features and generate the final scene representation by feature encoding methods including BoVWs <ref type="bibr" target="#b12">[13]</ref>, VLAD <ref type="bibr" target="#b18">[19]</ref>, and FV <ref type="bibr" target="#b19">[20]</ref>. It can be seen (Section IV) that the holistic scene representation based on FV can get better performance than the penultimate FC layer of a CNN, which is fairly close to the observations in other vision tasks <ref type="bibr" target="#b35">[36]</ref>.</p><p>The main contribution of this paper is providing a solution to exploit more benefits of a pretrained CNN for aerial scene classification. The major contributions of this paper are threefold.</p><p>1) A multiscale max-pooling strategy (Section III-B) is proposed to achieve multiscale representation. This MSP strategy improves invariance to multiscale deformations and further diversifies the last convolutional layers (conv5) with aggregation on multiple spatial bins of the feature maps <ref type="bibr" target="#b36">[37]</ref>.</p><p>2) The second contribution is to regard the output of MSP as a set of local concept descriptors <ref type="bibr" target="#b36">[37]</ref> and build a holistic representation using FV (Section III-C). This is the first work to leverage the encoding methods to generate scene representation based on CNN pooling descriptors.</p><p>3) In the experiments, we implement the proposed method using several pretrained deep CNN models for aerial scene classification. We thoroughly evaluate the transferrable ability on various pretrained CNN models (Section IV) and reveals that the VGG-S model <ref type="bibr" target="#b27">[28]</ref> achieves the best generalization performance among the evaluated CNN models.</p><p>The organization of this paper is as follows. In Section II, we give a brief introduction of the related work. In Section III, we propose a deep scene representation by incorporating multiscale max pooling and FV. Experimental results are given in Section IV. In Section V, we summarize the conclusions of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Aerial Scene Classification</head><p>Many aerial scene classification methods have been extensively studied in the past few decades. Recent research efforts have shown that constructing a holistic scene representation for aerial scene classification is intuitively feasible and yields good performance. Depending on the feature extracted, aerial scene classification methods can be roughly divided into two main groups: the low-level methods and the high-level methods.</p><p>The low-level methods aim to represent the remote sensing scenes by encoding various handcrafted features with different local descriptor encoding methods. The low-level methods first extract handcrafted local features (e.g., Gabor <ref type="bibr" target="#b41">[42]</ref>, local binary patterns (LBPs) <ref type="bibr" target="#b42">[43]</ref>, and scale-invariant feature transform (SIFT) <ref type="bibr" target="#b43">[44]</ref>) and then build a holistic scene representation by local descriptor encoding methods (e.g., BoVW <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, SPM <ref type="bibr" target="#b1">[2]</ref>, VLAD <ref type="bibr" target="#b18">[19]</ref>, and FV <ref type="bibr" target="#b19">[20]</ref>). A drawback of these methods is that the handcrafted local features are difficult Fig. <ref type="figure">2</ref>. Diagram of the proposed method. First, the activations from the last convolutional layer of pretrained CNN are used to describe the local information of the aerial scene. It has been proven that convolutional features transfer more easily than FC ones even without fine-tuning <ref type="bibr" target="#b12">[13]</ref>. Then, multiscale max pooling is performed on convolutional features and improve invariance to multiscale deformations. Finally, a discriminative representation is generated by encoding the multiscale local features based on FV.</p><p>to depict the high-diversity and the nonhomogeneous spatial distributions in aerial scenes <ref type="bibr" target="#b3">[4]</ref>.</p><p>The high-level methods use deep neural networks to learn a global feature from the raw aerial images and cast the scene representation as an end-to-end problem. Recently, numerous breakthroughs have been made in deep learning for aerial scene classification <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These deep learning methods have achieved huge improvements beyond state-of-the-art records. More specially, the deep CNNs <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b29">[30]</ref> are now the dominant methods in the majority of classification tasks. Many recent works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> have demonstrated that the pretrained CNNs can be transferable to aerial scene classification without any training modalities (fine-tuning or training from scratch). A holistic scene representation is achieved by using the deep feature extracted from the FC layer of the network. The activations from the FC layer of pretrained CNNs have been proven to be powerful generic feature representations with excellent performances. For instance, Cheng et al. <ref type="bibr" target="#b23">[24]</ref> proposed a simple but effective method to learn discriminative CNNs for aerial scene classification. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed a feature fusion strategy to integrate the multilayers of CNNs for aerial scene classification. Chaib et al. <ref type="bibr" target="#b24">[25]</ref> proposed a feature fusion strategy to further exploit the original features extracting from pretrained CNN models. Gong et al. <ref type="bibr" target="#b2">[3]</ref> proposed a deep structural metric learning method for remote sensing scene classification. Ammour et al. <ref type="bibr" target="#b44">[45]</ref> proposed an asymmetric adaptation neural network method for crossdomain classification in remote sensing images.</p><p>Although the traditional CNN methods have achieved great empirical success for aerial scene classification <ref type="bibr" target="#b2">[3]</ref>, recent works only regard CNN models as feature extractors and few concerns are concentrated on improving invariance to multiscale deformations. Different from previous works, this paper mainly focuses on enriching the discrimination of the CNN feature representations by learning multiscale invariant features. The multiscale invariant features can be learned by performing MSP in local spatial bins of the feature maps. By max pooling across different locations and over different spatial scales of the feature map, the pooled feature is robust to multiscale local transformations. The proposed method generates a scene representation by encoding all the descriptors from the MSP layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiscale Representation</head><p>Multiscale representation has been a driving motivation for research in computer vision for many years <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>. As an extension of the BoVW model <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, SPM <ref type="bibr" target="#b40">[41]</ref> is one of the most successful methods in computer vision. SPM partitions the image into divisions from finer to coarser levels and aggregates local features in them. Recently, many multiscale strategies have been proposed to improve recognition performance of CNN. Gong et al. <ref type="bibr" target="#b38">[39]</ref> proposed MOP to extract deep activation features from local patches at multiple scales. He et al. <ref type="bibr" target="#b33">[34]</ref> proposed an SPP layer to perform pooling in multilevel spatial bins. Hu et al. <ref type="bibr" target="#b12">[13]</ref> used a pretrained CNN to extract convolutional features from the multiscale input images. These multiscale convolutional features can capture multiscale information in the aerial scene. Cimpoi et al. <ref type="bibr" target="#b35">[36]</ref> regarded the last convolutional layers of a CNN as a filter bank and computed features after rescaling the input image by multiple factors.</p><p>Different from the most traditional multiscale representations, the proposed MSP performs multiscale strategy at a deeper stage of the deep networks. The traditional multiscale representations (e.g., FV-CNN and MOP) perform multiscale strategy on the image and repeatedly apply CNN on multiscale patches of the input image, which is time-consuming. In our method, we extract the feature maps from the entire image only once and then apply MSP on each candidate window of the feature maps. In this way, MSP is natural, significantly more efficient because the time-consuming convolutional network is only applied once. We will show that the multiscale strategy plays important roles in deep networks and is important for the accuracy of aerial scene classification (Section IV-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The proposed method is based on learning a scene representation from multiscale local descriptors of the aerial scenes. For image description, we employ CNN activations from the last convolutional layer as off-the-shelf features for aerial scene classification. Then, multiscale max pooling is applied on the convolutional features to build invariant local descriptors of aerial scenes. Finally, a scene representation is achieved by encoding the multiscale local descriptors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Features</head><p>Traditionally, aerial scene classification methods rely on hand-crafted features for image description, such as Gabor <ref type="bibr" target="#b41">[42]</ref>, LBPs <ref type="bibr" target="#b42">[43]</ref>, and SIFT <ref type="bibr" target="#b43">[44]</ref>. In this paper, we investigate the use of convolutional layer activations for aerial scene classification. Considering the popular seven-layer architecture, the first five layers are convolutional and the last two layers are FC. It is noted that the FC layers require inputs of fixed size, while the convolutional layers can accept images with arbitrary sizes <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>. The convolutional features, known as feature maps <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, contain the strength of the responses and the spatial positions. The activations from the last convolutional layers (conv5) of the network have recently been shown to achieve superior results for visual recognition <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Considering the feature maps of conv5 is X ∈ R a×a×M , where a × a is the spatial size of the last convolutional layer and M is the number of convolutional filters in the last convolutional layer (in our case, a = 19 and M = 512). These feature maps generated by deep convolutional layers are analogous to the local features (e.g., Gabor, LBP, and SIFT) in traditional scene classification methods <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b43">[44]</ref>. In these methods, feature maps of the last convolutional layer can be employed as local features and the image representation is obtained by feature encoding methods such as BoVW <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, VLAD <ref type="bibr" target="#b18">[19]</ref>, and FV <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this paper, we extract activations of the last convolutional layers (conv5) from the raw image without requiring a fixed input size and generate convolutional feature maps of arbitrary sizes. For example, each image in the UCM data set is 256 ×256, and it need not resize 256 ×256 to fit the input size of networks. Some details may be discarded when resizing, hence reducing the classification accuracy <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiscale Pooling</head><p>The last convolutional layer can contain much spatial information by accepting images with arbitrary sizes. An invariant feature can be learned by pooling in local spatial bins of the feature maps. The CNNs learn invariant features by a series of max-pooling operations, which imposes a capacity bottleneck and facilitates computation. Although max pooling in CNN has achieved great empirical success, the preserved spatial information such as invariance to larger scale and global deformations may be undermined.</p><p>Inspired by the multilevel pooling operations in deep learning (e.g., FV-CNN <ref type="bibr" target="#b35">[36]</ref>, MOP <ref type="bibr" target="#b38">[39]</ref>, and SPP <ref type="bibr" target="#b33">[34]</ref>), we propose a novel and simple pooling scheme in the CNN structure that significantly outperforms FC activations and convolutional features. In this paper, the MSP is achieved by performing a multiscale sliding window on each feature map. By max pooling across different locations and over different spatial scales of the feature map, the pooled feature is robust to multiscale local transformations. Fig. <ref type="figure" target="#fig_1">3</ref> illustrates the whole structure of our MSP based on convolutional features. As a sliding window pooling, the window size in n-level pool (pool n × n) is n × n, and stride str = n -1.</p><p>For example, the input size of image is 256 × 256 × 3, and the activations from the last convolutional layers (conv5) of the network is 19 × 19 × 512. The size of pooling window In SPP, the number of windows is fixed because the FC layers require fixed-size images as inputs. In our MSP, the number of windows is proportional to the image size, which make the multiscale representations robust to local spatial translations <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Furthermore, FV-CNN and MOP perform multiscale strategy on image and repeatedly apply CNN on multiscale patches of the input image, which is time-consuming. In our method, we extract the feature maps from the entire image only once and then apply MSP on each candidate window of the feature maps. In this way, MSP is significantly efficient because the time-consuming convolutional network is only applied once. Unlike other previous methods, MSP performs multiscale strategy at a deeper stage of the deep networks. We will show that the multiscale strategy plays important roles in deep networks and is important for improvement of the accuracy of aerial scene classification (Section IV-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Scene Representation</head><p>In order to develop a holistic scene representation for aerial scene classification, we compute the scene representation by encoding all the descriptors from the MSP layer. Inspired by the previous work <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, FV <ref type="bibr" target="#b19">[20]</ref> and VLAD <ref type="bibr" target="#b18">[19]</ref> have been shown to have great advantages over BoVW <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> in local descriptor encoding methods. In this paper, the last convolutional features can be pooled at multiple scales and encoded into a single FV just like SIFT. Unlike the FC layer in conventional CNN (FC-CNN) <ref type="bibr" target="#b35">[36]</ref>, FV encodes the multiscale local features into a global representation and is, therefore, more apt at describing multiscale spatial information <ref type="bibr" target="#b35">[36]</ref>. By discarding the computation of the FC layers, the input image can be of any sizes without costly resizing operations.</p><p>The proposed scene representation is related to those in <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and <ref type="bibr" target="#b36">[37]</ref>, including FV-CNN <ref type="bibr" target="#b35">[36]</ref> and SPP <ref type="bibr" target="#b33">[34]</ref>.</p><p>A key difference of the proposed scene representation is that multiscale strategy is performed on the convolutional features rather than the input image. This is significantly more accurate and more efficient, as MSP aggregates information at a deeper stage of the deep networks and does not require recomputing the network for each extracted local descriptor. To the best of our knowledge, this is the first work on the scene representation of convolutional features and we broaden the encoding methods from local descriptors to CNN descriptors in aerial scene analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>This section evaluates the proposed scene representation method for aerial scene images classification. The detailed experimental setup is presented in Section IV-A. Section IV-B discusses the performances of different pretrained CNN for aerial scene classification. Section IV-C presents the results of conv5 features by using different encoding methods. The effectiveness of MSP is analyzed in Section IV-D. Finally, we compare our results to the state-of-the-art results in Section IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup 1) Data Sets:</head><p>To evaluate the effectiveness of the proposed method, five publicly available aerial scene data sets are used: VHR17, UCM<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b47">[48]</ref>, WHU-RS19<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b48">[49]</ref>, RSSCN7<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b49">[50]</ref>, and AID<ref type="foot" target="#foot_3">4</ref>  <ref type="bibr" target="#b3">[4]</ref>, which can be seen in Fig. <ref type="figure" target="#fig_3">4</ref>. These data sets are very diverse. VHR17 data set contains 17 scene categories and 100 samples of size 256 × 256 in each scene category. UCM data set contains 21 distinctive scene categories and 100 samples of size 256 × 256 in each scene category. WHU-RS19 data set has 19 different scene categories and 50 samples of size 600×600 in each scene category <ref type="bibr" target="#b50">[51]</ref>. RSSCN7 data set contains seven scene categories and 400 samples of size 400× 400 in each scene category. AID data set has 30 different scene categories and about 200 to 400 samples of size 600 × 600 in each scene category. For a fair comparison, we follow the experimental setup in <ref type="bibr" target="#b3">[4]</ref> and randomly select some samples from each scene category for training and leave the remaining images for testing. We fix the proportion of training samples in the VHR17 data set, UCM data set, WHU-RS19 data set, RSSCN7 data set, and AID data set to be 80%, 80%, 60%, 50%, and 50%, respectively. 2) Evaluation Measures: In all the experiments, we apply linear support vector machine and follow the preprocessing in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b35">[36]</ref> on CNN descriptor extraction. FC-CNN <ref type="bibr" target="#b35">[36]</ref> considers the activations from the first FC layer as a 4096-dimensional feature and builds the final scene representation. The convolutional features (conv5) are extracted from the last convolutional layer without the fixed size requirements of the input images. CNN descriptors are extracted using the implementation in the MatConvNet library<ref type="foot" target="#foot_4">5</ref>  <ref type="bibr" target="#b51">[52]</ref>. We utilize VLFeat<ref type="foot" target="#foot_5">6</ref>  <ref type="bibr" target="#b52">[53]</ref> to generate BoVW, VLAD, and FV representation.</p><p>To compare the classification results quantitatively, we compute the commonly used measures: overall accuracy (OA), average precision (AP) for each classes, and mean AP (mAP) over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transferrable Ability of CNN Architectures</head><p>As mentioned previously, the pretrained CNNs can be transferred for scene data sets without any training modalities (fine-tuning or training from scratch). We investigate the representative power of CNN features and evaluate transferrable ability of various pretrained CNN models for aerial scene classification. To evaluate the performance of different deep CNN models, the 4096-dimensional activations extracted from the first FC layer are considered as a global scene representation of the input image. In this paper, we briefly compare some successful modern CNN architectures, such as AlexNet <ref type="bibr" target="#b25">[26]</ref>, CaffeNet <ref type="bibr" target="#b26">[27]</ref>, GoogleNet <ref type="bibr" target="#b29">[30]</ref>, VGG-F <ref type="bibr" target="#b27">[28]</ref>, VGG-M <ref type="bibr" target="#b27">[28]</ref>, VGG-S <ref type="bibr" target="#b27">[28]</ref>, VGG-VD16 <ref type="bibr" target="#b28">[29]</ref>, and VGG-VD19 <ref type="bibr" target="#b28">[29]</ref>. Results for the five data sets are presented in Table <ref type="table" target="#tab_0">I</ref>. We note that the VGG-S model consistently outperforms other CNNs, except for RSSCN7 data set where AlexNet achieves the best accuracy. This result indicates that VGG-S can generate powerful representations for aerial scenes through transferring the pretrained CNNs.</p><p>To further investigate the performance of the different pretrained CNN models, we also adopt the MSP and FV to encode the last convolutional layer feature (i.e., conv-MSP) for various CNN models. The proposed method (conv5-MSP5-FV) is evaluated with various different pretrained CNN models. Table <ref type="table" target="#tab_0">II</ref> shows the classification performances (mAP and OA) with different pretrained CNN models. It can be noted that the VGG-S model consistently achieves competitive accuracies compared with other different pretrained CNN models. As shown in Tables I and II, the proposed conv5-MSP5-FV method consistently outperforms FC features in different pretrained CNN models. As excepted, the proposed conv5-MSP5-FV method is more suitable to generate a scene representation for improving the performance of aerial scene classification. It can be concluded that the convolutional features (VGG-S model) achieved better performance than the FC features for aerial scene classification. Therefore, we use pretrained VGG-S model to build the proposed scene representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison of Feature Encoding Methods</head><p>In this experiment, we use the pretrained CNN as feature extractors, and the last convolutional features (conv5 descriptors) are extracted as local features. The final scene representation is generated by three different feature encoding methods, including BoVW, VLAD, and FV. BoVW generates a scene representation by encoding the conv5 descriptors with frequencies of local visual words. Instead of using k-means, FV uses Gaussian mixture modelto encode the conv5 descriptors and achieves a scene representation for aerial scene classification. Finally, VLAD uses a nonprobabilistic k-means clustering to encode the conv5 descriptors. Following a common practice in the literature <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b52">[53]</ref>, the code-book sizes are set to 4096, 256, and 256 with respect to each method (BoVW, VLAD, and FV). The dictionary size of BoVW is fixed to be 4096, the dictionary size of VLAD is fixed to be 256, and the dictionary size of FV is fixed to be 256.</p><p>The classification performances of different encoding methods (BoVW, VLAD, and FV) are given in Table <ref type="table" target="#tab_0">III</ref>. It is shown in Table III that all encoding methods achieved better results than FC-CNN (i.e., the activations from the first FC layer). This means that FC features perform worse than the shallower layer (convolutional features). A possible explanation is that the FC features of CNN models are more specific to categories in the ImageNet data set, where the convolutional features are more generalized to other data sets. When comparing the results using three different feature encoding methods, the FV method is generally comparable to FC-CNN, BoVW, and VLAD. The FV method leads to such a high accuracy with only conv5 features. This indicates that conv5-FV is more suitable to generate a scene representation for improving the performance of aerial scene classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effectiveness of Multiscale Pooling</head><p>To validate MSP, we investigate the effects of using different pooling schemes. We consider the proposed MSP with seven different scale settings: MSP1, MSP2, MSP3, MSP4, MSP5, MSP6, and MSP7. MSP1 is a single-scale pooling. MSP2 is a two-scale pooling {2 × 2, 1 × 1}, MSP3 is a   We first validate the effectiveness of the different pooling size in a single-scale pooling (MSP1). Fig. <ref type="figure" target="#fig_2">5</ref> shows the classification accuracies (OA) of different pooling sizes on five data sets. It can be seen in Fig. <ref type="figure" target="#fig_2">5</ref> that the OA values follow a similar observation on all data sets: the performances increase slightly from pooling size 1 × 1 to 2 × 2 and then decrease rapidly. A possible explanation is that performing max pooling in local spatial bins can enhance invariance and therefore improve performances. However, when the pooling size is larger than 3, the performance will decrease. This is  because some information may be lost when performing max pooling in large spatial bins. Therefore, we choose the pooling size 3 × 3 as a baseline method, namely, conv5-pool-FV.</p><p>The results reported in Table IV are obtained by using conv5 features and FV, with respect to different multilevel pooling strategies: FV-CNN <ref type="bibr" target="#b35">[36]</ref> and SPP <ref type="bibr" target="#b33">[34]</ref>. FV-CNN performs multiscale strategy by rescaling the input image with factors {2 s }, s = -3, -2.5, . . . , 1.5, while SPP performs multiscale strategy by replacing the pool5 layer with an SPP layer. Following <ref type="bibr" target="#b33">[34]</ref>, SPP4 is defined as a four-level pyramid {4 × 4, 3 × 3, 2 × 2, 1 × 1} with 30 bins and SPP6 is denoted as a pyramid {6 × 6, 3 × 3, 2 × 2, 1 × 1} with 50 bins. As a baseline, conv5-pool-FV refers to pooling conv5 feature with 3 × 3 and then builds a global representation by FV. Here, to verify the effectiveness of pooling operation on conv5 features, we also evaluate the performance using FV under several operations: relu5-FV and pool5-FV. relu5 is the relu layer after conv5 while pool5 is the pooling layer after relu5.</p><p>We can see that the most methods generally lead to lower accuracies than conv5-FV, except FV-CNN and the proposed MSPs. This indicates that performing max pooling on the convolutional features do not work well as conv5-FV because much spatial information of the aerial scene is discarded in pooling operation. Although the proposed MSPs also perform the max-pooling operation, the spatial information in conv5 is retained when the size of the pooling window is 1 × 1. Similarly, spatial information in conv5 is also retained in FV-CNN when the scale factor equals 1 (the original image). However, in contrast to FV-CNN, we do not need to repeat evaluations of the CNN since the conv5 features can be computed just once for the entire image. This makes MSP not only faster but also much more accurate with small scale factor (FV-CNN uses ten scales). As shown in Table <ref type="table" target="#tab_2">IV</ref>, we can achieve a comparable performance (MSP3) with only threescale pooling. The better performances achieved with MSPs in all cases confirm that it is a simple but effective technique for increasing performance.</p><p>Among the MSPs with different scales, we can see that the MSP5 generally leads to the highest accuracies. MSP2, MSP3, MSP4, MSP6, and MSP7 do not work as well as MSP5. Hence, we conclude that MSP is beneficial for increasing the classification accuracies, but the excessive scales (MSP6 and MSP7) may have a negative effect on accuracies due to the considerable redundancy of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison With State-of-the-Art Methods</head><p>As shown in Table <ref type="table" target="#tab_2">V</ref>, we compare our best results (conv5-MSP5-FV) with various state-of-the-art methods on several publicly available data sets. The compared methods are three high-level methods (CaffeNet, VGG-VD-16, and GoogleNet) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>As can be seen in Table <ref type="table" target="#tab_2">V</ref>, our method outperforms other compared methods in the mentioned data sets. Moreover, our method is relatively fast because the time-consuming convolutional network is only applied once. Furthermore, the corresponding confusion matrices of each data set using the proposed method are shown in Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>We not only give the corresponding confusion matrix of the proposed method but also give the APs for all scene categories. The APs of each scene and mAP for different data sets are reported in Figs. 7-10. The corresponding AP results of other compared methods (CaffeNet, VGG-VD-16, and GoogleNet) are shown for each data set. The AP results on UCM data set are shown in Fig. <ref type="figure" target="#fig_6">7</ref>, the AP results on WHU-RS19 data set are shown in Fig. <ref type="figure" target="#fig_7">8</ref>, the AP results on RSSCN7 data set are shown in Fig. <ref type="figure" target="#fig_8">9</ref>, and the AP results on AID data set are shown in Fig. <ref type="figure" target="#fig_9">10</ref>. From these AP results on each data set, we get the following observations: although most scenes can easy to be distinguished, some scenes cannot be distinguished well. Alternately, we can observe that the scene individual AP almost achieves 100% by the proposed method on all data sets. This indicates that the proposed method is competitive compared with compared methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a deep scene representation for aerial scene classification. In order to obtain a multiscale invariant scene representation, we propose to use convolutional features with the MSP for scene classification. The multiscale invariant information can be retained by performing multiscale sliding window pooling on each feature map. Finally, the scene representation is generated by encoding all the descriptors from the MSP layer. Exhaustive experiments on five data sets show that MSP of convolutional features is a remarkably good local descriptor and can achieve the state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Convolutional feature maps contain structural spatial information and are apt at describing local information of aerial scene. However, much spatial information may be discarded after max pooling.</figDesc><graphic coords="2,50.27,65.09,62.42,62.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of five-scale pooling in the proposed MSP. Here, the size of input image is 256 × 256 × 3 and the size of conv5 features is 19 × 19 × 512. The size of pooling window in pool 5 × 5, pool 4 × 4, pool 3 × 3, pool 2 × 2, and pool 1 × 1 is 5 × 5, 4 × 4, 3 × 3, 2 × 2, and 1 × 1, respectively. Multiscale local features are generated by max pooling across different locations and over different spatial scales of the feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>in pool 5</head><label>5</label><figDesc>× 5, pool 4 × 4, pool 3 × 3, pool 2 × 2, and pool 1 × 1 is 5 × 5, 4 × 4, 3 × 3, 2 × 2, and 1 × 1, respectively. By performing multiscale max pooling on the convolutional feature maps, the pooled features are robust to multiscale local transformations. The size of the pooled features after pool 5 × 5, pool 4 × 4, pool 3 × 3, pool 2 × 2, and pool 1 × 1 is 4 × 4 × 512, 6 × 6 × 512, 9 × 9 × 512, 18 × 18 × 512, and 19 × 19 × 512, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sample images of VHR17 (first row), UCM (second row), WHU-RS19 (third row), RSSCN7 (fourth row), and AID (last row) aerial scene data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Effectiveness of the different pooling size in single-scale pooling (MSP1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Confusion matrix and OA for the data sets using the proposed method. (a) VHR17 data set. (b) UCM data set. (c) WHU-RS17 data set. (d) RSSCN7 data set. (e) AID data set.</figDesc><graphic coords="7,170.39,246.65,128.78,128.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. AP of each scene and mAP for the UCM data set using different methods. (a) CaffeNet. (b) VGG-VD-16. (c) GoogleNet. (d) Proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. AP of each scene and mAP for the WHU-RS19 data set using different methods. (a) CaffeNet. (b) VGG-VD-16. (c) GoogleNet. (d) Proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. AP of each scene and mAP for the RSSCN7 data set using different methods. (a) CaffeNet. (b) VGG-VD-16. (c) GoogleNet. (d) Proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. AP of each scene and mAP for the AID data set using different methods. (a) CaffeNet. (b) VGG-VD-16. (c) GoogleNet. (d) Proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>ACCURACY OF FC-CNN USING DIFFERENT PRETRAINED CNNS</figDesc><table><row><cell>TABLE II</cell></row><row><cell>CLASSIFICATION ACCURACY OF CONV5-MSP5-FV USING DIFFERENT PRETRAINED CNNS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON OF STATE-OF-THE-ART METHODS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://vision.ucmerced.edu/data sets/landuse.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://dsp.whu.edu.cn/cn/staff/yw/HRSscene.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://sites.google.com/site/qinzoucn/documents</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://captain.whu.edu.cn/project/AID/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://www.vlfeat.org/matconvnet/pretrained/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://www.vlfeat.org/</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the State Key Program of National Natural Science of China under Grant 61632018, in part by the National Natural Science Foundation of China under Grant 61772510 and Grant 61806193, in part by the Young Top-Notch Talent Program of the Chinese Academy of Sciences under Grant QYZDB-SSW-JSC015, in part by the Open Research Fund of State Key Laboratory of Transient Optics and Photonics, Chinese Academy of Sciences under Grant SKLST2017010, in part by the CAS "Light of West China" Program under Grant XAB2017B26, and in part by the Xi'an Postdoctoral Innovation Base Scientific Research Project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring models and data for remote sensing image caption generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2183" to="2195" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Remote sensing scene classification by unsupervised representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5148" to="5157" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diversity-promoting deep structural metric learning for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="371" to="390" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AID: A benchmark data set for performance evaluation of aerial scene classification</title>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3965" to="3981" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by spatialspectral preservation in selected bands</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5185" to="5197" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for large-scale remote-sensing image classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="645" to="657" />
			<date type="published" when="2017-02">Feb. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperspectral image superresolution by transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1963" to="1974" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Background prior-based salient object detection via deep reconstruction residual</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A target detection method for hyperspectral image based on mixture noise model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page" from="331" to="341" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral-spatial kernel regularized for hyperspectral image denoising</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3815" to="3832" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepSat: A learning framework for satellite imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Adv. Geograp</title>
		<meeting>Int. Conf. Adv. Geograp</meeting>
		<imprint>
			<date type="published" when="2015-11">Nov. 2015</date>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering diverse subset for unsupervised hyperspectral band selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="64" />
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="14680" to="14707" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint dictionary learning for multispectral change detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="884" to="897" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency-guided unsupervised feature learning for scene classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2175" to="2184" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Link the remote sensing big data to the image features via wavelet transformation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cluster Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="793" to="810" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th IEEE ICCV</title>
		<meeting>9th IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2003-10">Oct. 2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Land-use scene classification using a concentric circle-structured multiscale bag-of-visual-words model</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Z</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4620" to="4631" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification using bag of convolutional features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1735" to="1739" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integrating multilayer features of convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5653" to="5665" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aerial scene classification via multilevel fusion based on deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="291" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative CNNs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2811" to="2821" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep feature fusion for VHR remote sensing scene classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4775" to="4784" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM Int. Conf. Multimedia</title>
		<meeting>22nd ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1405.3531" />
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Land use classification in remote sensing images by convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Castelluccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1508.00092" />
		<imprint>
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harvesting discriminative meta objects with deep CNN features for scene classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1287" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aggregating rich hierarchical features for scene classification in remote sensing imagery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4104" to="4115" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th IEEE Conf. Comput. Vis</title>
		<meeting>28th IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015-06">Jun. 2015</date>
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Workshop (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Workshop (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="621" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014-09">Sep. 2014</date>
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu</title>
		<meeting>26th Annu</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gabor descriptors for aerial image classification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Risojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Momić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Babić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Adapt. Natural Comput. Algorithms</title>
		<meeting>Int. Conf. Adapt. Natural Comput. Algorithms</meeting>
		<imprint>
			<date type="published" when="2011-04">Apr. 2011</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Asymmetric adaptation of deep features for cross-domain classification in remote sensing imagery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ammour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bashmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Al Rahhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zuair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="597" to="601" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object recognition with features inspired by visual cortex</title>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="994" to="1000" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bag-of-visual-words and spatial extensions for land-use classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th ACM SIGSPATIAL Int. Conf. Adv. Geogr</title>
		<meeting>18th ACM SIGSPATIAL Int. Conf. Adv. Geogr</meeting>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Satellite image classification via two-layer sparse coding with biased image representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="176" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep learning based feature selection for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2321" to="2325" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Land-use classification with compressive sensing multifeature fusion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Mekhalfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alajlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2155" to="2159" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">MatConvNet: Convolutional neural networks for MATLAB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th ACM Int. Conf. Multimedia (MM)</title>
		<meeting>18th ACM Int. Conf. Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2010-10">Oct. 2010</date>
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m">Xiangtao Zheng received the M.Sc. and Ph.D. degrees in signal and information processing from the Chinese Academy of Sciences</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014 and 2017</date>
		</imprint>
	</monogr>
	<note>respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
