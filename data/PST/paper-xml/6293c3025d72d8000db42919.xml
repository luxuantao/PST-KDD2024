<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Framework for Neural Network Architecture and Compile Co-optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">W</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">China and University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Zhejiang Lab</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">of Computing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Framework for Neural Network Architecture and Compile Co-optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3533251</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Computing methodologies → Machine learning</term>
					<term>• Software and its engineering → Software notations and tools</term>
					<term>DNN-scheduling Co-design, hardware-aware neural architecture search, compiler optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The efficiency of deep neural network (DNN) solutions on real hardware devices are mainly decided by the DNN architecture and the compiler-level scheduling strategy on the hardware. When we try to fully exploit the underlying hardware and obtain the optimal tradeoff between DNN accuracy and runtime performance, we discovered that the two optimization goals of DNN architecture and scheduling policy are intimately related to each other. However, current hardware-aware Neural Architecture Search (NAS) methods primarily focus on the DNN architecture search process, ignoring the effects of various compiler-level scheduling strategies (e.g., graph-level optimization, loop transformations, parallelization, etc.) on network candidates being evaluated in the search process. As a result, they may overlook the true-optimal DNN implementations on hardware, which can only be discovered by trying-out different combinations of scheduling strategies and DNN architectures. This work proposes a NAS framework (CHaNAS) that searches for not only the network architecture but also the dedicated compiler-level scheduling policy, as the optimal co-design solution on the target hardware. We propose to use a block-based pre-scheduling methodology to reduce the co-design search space and enable the automatic generation of the optimal co-design, including the network architecture and the tensor programs that practice the scheduling policy. Further, we introduce a new search objective function based on the generalization gap to prevent the selection of architectures that are prone to overfitting. We evaluate CHaNAS on Imagenet on different hardware back-ends against the state-of-the-art hardware-aware search method based on the MobileNet-v3 search space. Experimental results show that the co-design solutions obtained by ChaNAS show up to 1.6×, 1.9×, and 1.7× performance boost on NVIDIA P100 GPU, Intel Xeon 8163 CPU, and Samsung Note 10 Mobile, respectively, over the baselines of the same-level accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The widespread use of deep learning applications has fueled the development of Deep Neural Network (DNN) model design and efficient deployment on rapidly evolving hardware. There is a deep stack of optimization technologies accessible when building an efficient application or domain-specific neural network system <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b15">17]</ref>, including enhanced neural network architecture <ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref><ref type="bibr" target="#b24">[26]</ref><ref type="bibr" target="#b52">54]</ref>, optimized frameworks and compilers <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b44">46]</ref>, and even customized hardware design <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b51">53]</ref>. However, rather than using DNN optimization technology on its own, it has been demonstrated that cross-stack co-design approaches that orchestrate techniques from several layers can produce better outcomes <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b50">52]</ref>. For example, some recent works <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b35">37]</ref> have investigated how to design an optimized neural network architecture that fully considers the characteristics of the target hardware. Such a hardware-aware network architecture design method, i.e., hardware-aware Neural Architecture Search (NAS), can automate the DNN design and even exceed earlier human-designed models by incorporating hardware features into the NAS loop.</p><p>Previous hardware-aware NAS methods simply took into account the co-optimization of DNN architecture and hardware-related design variables like representation precision and resource provision <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b48">50]</ref>. However, as we have emphasized, the efficiency of DNN applications also depends on multiple layers of optimization techniques. Specifically, in addition to neural network architectures, how to schedule the neural network onto the hardware at the compiler level, such as task graph reordering, loop reordering, loop tiling, memory customization, and other compilermanaged scheduling policies, plays an important role in the performance of the target neural network system <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b27">29]</ref>. For example, a typical hardware-aware NAS framework usually relies on a performance model to estimate the performance of the candidate neural networks on the target hardware, then decides whether the discovered network should be kept or updated during the iterative search process. However, in these evaluation-and-search iterations, almost all prior hardware-aware NAS works assume a fixed network scheduling policy that may not extract the best performance from the under-evaluation network architecture on the hardware. Some other works focus on tuning the schedule mapping strategy when given a neural network model to optimize the performance for different hardware <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b47">49]</ref>, but they ignore the correlation between the interactive stacks and lead to locally optimal solutions. In contrast, we show that to discover the best solution at the system level, an ideal hardware-aware NAS framework should incorporate compiler-level optimization, i.e., scheduling strategies, for the target hardware. In this article, we offer the Compiler and Hardware aware Network Architecture Search (CHaNAS) framework for the first time to achieve this goal.</p><p>In ChaNAS, we orchestrate two key components from the deep learning optimization stack: the DNN architecture and the scheduling policy in the compiler that tactically maps a model onto the target hardware. As shown in the Figure <ref type="figure">1</ref>, ChaNAS constructs a joint search space that combines 5:3 Fig. <ref type="figure">1</ref>. CHaNAS jointly design the neural architecture and the computation-graph scheduler to fit the target hardware characteristics. With an enlarged optimization design search space, CHaNAS is more likely to achieve better results than existing scheduler-agnostic hardware-aware NAS methods.</p><p>the DNN architectures and scheduling-level optimization, as opposed to traditional methods that either optimize the neural network using NAS based on a fixed schedule policy from a given deep learning library (e.g., Tensorflow) or tunes the scheduling policy to maximize the inference speed for a given network. For each candidate network evaluated in the search space, CHaNAS can measure and discover its realistic peak performance that can be revealed by trying-out different schedule policies such as instruction-level scheduling, memory allocation, loop transformations, and so on, and then choose the true-optimal one. Realizing this goal also faces multiple challenges. First, it requires us to automatically construct the joint search space to cover as many DNN/Schedule policy design pairs as possible for the given AI task and target hardware. Second, we must precisely and effectively analyze each design pair, which is a time-consuming process. Finally, we need to search efficiently in the huge search space to return the optimal co-design solution, which includes not only the DNN implementation but also the corresponding high-performance tensor programs for the potential hardware back-ends.</p><p>To this end, ChaNAS utilizes a block-based hierarchical design representation to construct the huge co-design search space automatically. In the CHaNAS search space, both the network architecture design and the network scheduling is conducted on the basis of neural network blocks, which help divide the entire network design space exploration into two coordinated phases. We then apply a supernet that acts as an accuracy predictor and use the pre-scheduled blocks to reduce the evaluation cost. Finally, to reduce the search cost, we divide the search space and employ an evolutionary search technique. Overall, as an automated schedule-aware neural architecture search method, ChaNAS makes full use of the NAS's advantage and schedule optimization that fit the target hardware, allowing a larger design space for architecture design. Therefore, with a larger degree of design freedom than previous schedule-agnostic hardware-aware NAS, it is more likely to find the optimal specialized DNN solution for varied tasks and different hardware back-ends. We evaluate CHaNAS on Imagenet on different hardware back-ends against the state-of-the-art (SOTA) hardware-aware search method MobileNet-v3 <ref type="bibr" target="#b22">[24]</ref>. Experiments show that the co-design solutions obtained by ChaNAS show up to 1.6×, 1.9×, and 1.7× performance boost on NVIDIA P100 GPU, Intel Xeon 8163 CPU, and Samsung Note 10 Mobile, respectively, over the baselines of the same-level accuracy.</p><p>In summary, this article makes the following contributions:</p><p>• We propose the first automatic joint search methodology for neural network architecture and scheduling policy on different hardware, CHaNAS. By using a block-based hierarchical design representation, CHaNAS constructs a large joint co-design space and is more able to locate the optimal solution for the target hardware with specified design constraints than existing scheduler-agnostic hardware-aware NAS methods. • We carefully design the blocks that are used to construct the target DNN model and consider the block size to tradeoff the architecture search cost and schedule optimization effect. We also propose an elastic supernet that includes all blocks and is employed by CHaNAS to generate child neural network models in the solution search process. The basic building blocks for this supernet can be automatically pre-scheduled and evaluated on the target hardware to expose the optimal performance to the CHaNAS during the search process with little cost. • Due to the huge co-design search space, we divide the co-design search space according to the constraints of the deployment scenario to reduce the search cost, and employ an evolutionary search strategy to explore the reduced co-design search space, which can gradually generate higher-quality co-design results for the target hardware platform. • To further improve the quality of the automatically generated co-design solution, We propose a new objective function to enhance the generalization capability of the target network, which has been empirically proved to help get networks that are less influenced by the weight sharing problem. In our experiment, the introduction of the new objective function improves the accuracy of the discovered DNN solutions by 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>In this section, we first introduce two critical factors that impact the DNN solution on the target hardware: the efficient DNN architecture design and the schedule strategy that maps the DNN on different hardware. Then we introduce the heuristic approach that is not only typically used in the DNN architecture search but also used in our co-design process. Finally, we present the observation that motivated us to automatically co-design the DNN architecture and corresponding schedule strategy for the target hardware platforms. Efficient Neural Network Design. Efficient DNN model design is crucial for the overall performance of the deep learning system. Many efficient neural network architectures are proposed to improve hardware efficiency, such as Squeezenet <ref type="bibr" target="#b24">[26]</ref>, MobileNets <ref type="bibr" target="#b23">[25]</ref>, ShuffleNets <ref type="bibr" target="#b52">[54]</ref>, and so on, which mainly focus on reducing the computation (e.g., adopting depthwise). To reduce the manual effort in the efficient DNN design, NAS tends to automate the architecture design process and begins to dominate the recent efficient network design, while the early NAS methods <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b57">59]</ref> search for high accuracy architectures without considering hardware efficiency. With the development of NAS techniques <ref type="bibr" target="#b6">[8]</ref>, researchers begin to combine multi-objective optimization into the NAS process <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b56">58]</ref>. They typically focus on searching efficient NAS models with higher accuracy, lower parameters, Flops (number of multi-add operations), and lower latency (or energy). As a result, those methods incorporate hardware characteristics into the NAS loop to increase inference efficiency, but their performance highly depends on the quality of the search space and search strategy <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b40">42]</ref>. To evaluate each architecture efficiently, One-shot NAS substantially reduces the computation cost by training only one super-network, a.k.a. supernet, to approximate the performance of every architecture in the search space via weight-sharing. Specifically, One-shot NAS uses the supernet to predict the performance of a specific architecture by deactivating the extra edges w.r.t. a target architecture on the supernet via masking, then performing evaluations using the masked supernet. In general, people follow the manual design heuristic for NAS search space design and use either heuristic or ML method for design search.</p><p>DNN Model Deployment on Hardware. How to map DNNs onto a particular hardware device is crucial to the system performance <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b34">36]</ref>. Currently, contemporary DNN compilers <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b41">43]</ref> rely on an intermediate representation (IR) of task graph to abstract the network architecture. So they can either manipulate the network IR at the graph-level or redefine the implementation of each single operator that is a node in the IR graph. As shown in the Figure <ref type="figure" target="#fig_0">2</ref>, optimizing these DNN models is generally performed in two steps: (i) high-level graph optimizations and (ii) lowlevel kernel optimizations such as those found in vendor libraries. Typically, at the beginning of the compile optimization, the compiler usually partitions the large computation graph of a DNN model into several subgraphs, which becomes the basic unit in the whole compilation process. This partition has a negligible effect on the performance due to the layer-by-layer construction nature of DNN <ref type="bibr" target="#b53">[55]</ref>. Then some graph-level optimization techniques are applied to the subgraphs. Graph-level optimization techniques include layout optimizations, operator fusion, constant folding, auto-batching, and so on. In contrast, operator-level computation optimization is often hardware specific. For example, achieving latency hiding for operator computation requires different strategies on different hardware back-ends. On the CPU, memory latency hiding is achieved implicitly with simultaneous multi-threading or hardware pre-fetching <ref type="bibr" target="#b10">[12]</ref>. While GPUs rely on rapid context switching of many warps of threads. Generally, in the deep learning compile process, to optimize an operator or a (sub-)graph of multiple operators, the compiler requires users to define the computation in a high-level declarative language, and the compiler then searches for programs tailored toward different hardware platforms based on some human-written schedule templates, including different schedule primitives (e.g., split, reorder, fuse, etc.) and proper parameters to achieve parallelism, vectorization, memory tilling, and memory access latency hiding for different hardware. For example, TVM <ref type="bibr" target="#b10">[12]</ref> requires the user to write a template that defines the structure  of the operator programs with some tunable parameters. Then the compiler searches for the best values of these parameters for a specific input shape configuration and a specific hardware target.</p><p>Heuristic search method. Many algorithms have been applied in both the NAS process and schedule strategy search process, e.g., heuristic algorithm <ref type="bibr" target="#b10">[12]</ref>, Bayesian optimization <ref type="bibr" target="#b55">[57]</ref>, reinforcement learning <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b57">59]</ref>, and so on. In this article, we mainly consider the heuristic method in the co-design search process, so we make a brief introduction to the heuristic algorithm here. As illustrated in the Figure <ref type="figure" target="#fig_1">3</ref>, the heuristic method mainly consists of five parts. At first, a group of candidates is chosen to serve as the initialized population. The algorithm will then select the best candidate after several iterations. In each iteration, according to the selected individuals (called parents), some new individuals are created (called children), depending on if mutation or crossover should be utilized. Then each child will be evaluated to get the fitness value that determines the quality of the new individual. The new individual is inserted into the population if its fitness value is better than the worst performing individual of the population; otherwise, the new individual is omitted. After that, the population is updated. Each iteration leads to a new generation, and the iteration terminates if the number of max generations is reached.</p><p>Observation. Figure <ref type="figure" target="#fig_2">4</ref> reveals a phenomenon we discovered in the SOTA NAS frameworks that run realistic workloads. For different neural network task specifications, e.g., different dataset, 5:7 optimization goals, or different performance constraints like the accuracy or latency requirements, an ideal NAS framework is supposed to generate the optimal neural network model for the target hardware through the automated search mechanism. However, as the user-specified constraint of accuracy changes, the Pareto frontiers of the NAS schemes that assume different scheduling solutions will intersect with others, which means none of the NAS baselines in Figure <ref type="figure" target="#fig_2">4</ref> can always generate a better solution than the others when the task specification or constraint changes. For example, in Figure <ref type="figure" target="#fig_2">4</ref>, if the designer seeks a neural network that must run faster than 35 ms, then the NAS scheme assumes Schedule-B is better, while the one with Schedule-A is more accurate when the latency constraint changes to 20 ms. In other words, prior schedule-agnostic NAS technologies cannot guarantee the optimal network solution in the search process as they assume a fixed compiling strategy. Thus, enabling effective and automatic co-optimization of neural network architecture and the scheduling policy is necessary for an optimized neural network system. In this work, we are the first to investigate the scheduler and hardware-aware NAS that searches not only for the network architecture choices but also the corresponding network scheduling solution on the target hardware at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CHANAS FRAMEWORK 3.1 Problem Formalization</head><p>CHaNAS explores the massive joint design space, which is the confluence of the DNN architecture space and the network scheduling space for the target hardware. The DNN architecture space includes the model hyper-parameters such as the layers type, input size, channels, and so on. While the scheduling space contains many compiler-level optimization knobs with their parameters, such as loop tiling, operators fusion, reordering, and so on. Assuming the DNN architecture search space is {arch} = {arch 1 , . . . , arch n }, the scheduling strategy space is C = {c 1 , . . . , c m }, and the target design performance constraint is P thr es . The objective of CHaNAS is to search from the joint search space {{arch}, C} for the best DNN architecture arch * with the associated scheduling strategy c * ar ch * , which together contribute to the maximum network accuracy and improved performance P that must at least satisfies constraint P thr es . In all, we formalize this problem as</p><formula xml:id="formula_0">arch * , c * ar ch * ∈ argmin ar ch * ∈{ar ch },c * ∈C L val (arch, ω, c) s.t . P (arch * , c * ar ch * ) &lt; P thr es s.t . ω * = argmin L train (ω, arch), (1)</formula><p>where L train and L val are the task loss of training and validation, respectively; ω presents the network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CHaNAS Overview</head><p>As shown in Figure <ref type="figure">1(c)</ref>, in contrast to prior works that only search for solutions inside the network architecture space {arch}, CHaNAS has a significantly larger design space to explore. Thus, we need to answer the following questions: (i) How should we automatically construct the codesign search space? It needs to cover both the DNN model and scheduling policy, which requires a compact design space representation to describe the potential co-design solutions with high efficiency. (ii) How should we evaluate each candidate co-design pair efficiently? We have to perform the costly DNN model training and sophisticated schedule optimation to get the real performance on the target hardware. (iii) How should we reduce the huge search space and design an effective search strategy that quickly converges into high-quality co-design solutions? Thus, for the first time, CHaNAS conducts the architecture/schedule-policy co-design by employing a block-based hierarchical design representation to construct the co-design search space efficiently and reduce the efforts taken to explore unnecessary options.</p><p>As a sub-graph of interconnected neural network operators (e.g., capsule conv2d, dilated conv2d), a CHaNAS block is the fundamental unit in both network architecture design and scheduling policy search. The design choice of block-level construction is based on two observations. First, the block is the basic unit in network architecture search, as the mainstream NAS approaches will pre-define the elementary blocks and generate the inter-block connection to form a candidate network architecture. Second, in the deep learning compile system, the compiler usually divides the full computation graph into sub-graphs, where the sub-graphs can be treated as blocks. Then the scheduler will attempt to reorder the operators in blocks and then map each block onto hardware by trying out back-end scheduling techniques such as loop-unrolling and blocking. Therefore, block-level construction is flexible enough to support the different design constraints and different hardware platforms. Furthermore, such a factorized hierarchical search space makes a good balance between the diversity of potential co-design options and the size of the entire co-design search space. We address the difficulties in the model-level schedule optimization. Suppose we partition the network into B blocks, and each block has a parameter search space size of R with an average of S scheduling policies per block. Due to our block-based design, our neural network architecture search and schedule policy search are separated, and their search space size is R B and B * S, respectively. So our total search space would be B * S + R B , versus the original single-level co-design search space at the size of R B * S B .</p><p>In the whole design process, the hierarchical exploration flow of CHaNAS is hinged on neural blocks and involves two coordinated phases: the top-down network scheduling component will pre-generate the implementations for each possible neural block and evaluate them on the target hardware, while the bottom-up network architecture generation unit will search through the possible sequences of the neural blocks that have already been virtually mapped and optimized on hardware by the scheduling unit. With these two exploration paths, the optimal co-design solution can be found at a high probability. Figure <ref type="figure" target="#fig_3">5</ref> depicts the overview of CHaNAS, which has three major components: (1) a block-stacked super-net that captures the DNN architecture search space. (all candidate DNN architectures can be extracted from the super-net, which achieves fast accuracy evaluation as the candidate DNN directly inherits its parameters to bypass the expensive network training stage); (2) a block-based scheduling space explorer that transforms each block into a computational sub-graph and optimizes it on the target hardware; and (3) an search algorithm that divides the joint space to reduce the search cost and then search within the co-design sub-space. In CHaNAS, there are three major steps in the co-design flow.</p><p>Step 1: Construction of the Elastic Super-Net. We at first build the super-net from which many candidate DNN architectures can be derived. The super-net is stacked with blocks. Specifically, each block in super-net has many variable hyper-parameters, such as kernel size, channels, input shape, and so on. After the super-net training completes, CHaNAS extracts child networks for the target hardware from the super-net in the search process for co-design solutions. Specifically, in the super-network training process, we use a new generalization-based search objective function, which avoids the overfitting in previous one-shot NAS methods.</p><p>Step 2: Block-level Pre-Scheduling. For the scheduling-level optimization space, the basic DNN building blocks are virtually pre-scheduled and optimized on the target hardware, so that it generates many block-level co-designs that will be used in the evaluation stage of the global joint design space search. Given a parameterized block, the scheduling space, including the graph-level optimization for overall block topology and the operator-level optimizations that are explored via a heuristic method, and the performance of the scheduling policies will be fine-tuned under the direction of a learned cost model until the best scheduling point for that block is obtained.</p><p>Step 3: Co-design Exploration. Given the deployment constraints (latency in our test as an example), we first reduce the co-design search space according to the cumulative distribution function of network inference latency. Then we use an evolutionary-based searcher to explore in the reduced search space, for which we build a DNN accuracy predictor and a performance predictor based on the block performance Look Up Table (LUT) profiled at the block-level pre-scheduling stage. At last, the target DNN model with a dedicated scheduling strategy is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Elastic Super-net Construction</head><p>Block is the basic unit in CHaNAS architecture search and scheduling search. However, we have a tradeoff in designing blocks size. In the CHaNAS design space, if the block structure is defined at an over-fine granularity, i.e., fewer layers in the block, then the graph-level scheduling search space in a block will also be too small to conduct thorough scheduling-level optimization. In contrast, if the blocks are too large and lead to a massive block design space, then it will be less likely for the search algorithm to converge to the optimal architecture in an oversized architecture search space. Therefore, we propose a medium-grained block design method to build an elastic super-net to balance the architecture search efficiency and scheduling search efficacy. The super-net will not induce too much search complexity and contains various large-enough blocks to explore the potential of scheduling-level search.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> shows the architecture of super-net, which defines the architecture search space. First, the DNN is composed of N basic building blocks. In this work, to design hardware-friendly DNNs and to reduce search time, we adopt the single-path DNN structure without branches <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b56">58]</ref>. Second, for each block block i , (1 ≤ i ≤ N ), there are M units included. Following the common practice of NAS approaches <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b42">44]</ref>, we adopt the elastic MBConv cell as the basic unit in a block. A elastic MBConv cell is composed of sequential layers of conv-1 × 1 (convolution with kernel size 1 × 1), dwconv-k × k (depthwise convolution with kernel size k), SE (Squeeze-and-Excitation), and conv-1 × 1. Between conv-1x1 and dwconv-kxk, the number of channels expands by a ratio of E. In the elastic MBConv, we can search the kernel size k from {3, 5, 7}, and we also search for the number of channels expansion ratio of E from {3, 6} for each block except for the first one, which has the default expansion ratio of 1. Similarly as in Reference <ref type="bibr" target="#b7">[9]</ref>, we allow a parameterizable kernel size through a kernel transformation matrix in each MBConv, which expedites the training process. Furthermore, the depth D of the block can also be variable, which means only the first D units are kept in the sampled block. In addition to these configurable parameters, we also allow the DNN model to take arbitrary input image sizes by assigning the model with a size ratio. The elastic design of blocks allows the super-net to offer the candidate sub-networks that are sufficiently flexible for different deployment constraints and makes the search cost affordable. To reduce the evaluation cost further, we train the super network that contains all the possible sub-networks through weight sharing and use it to estimate the accuracy of each sub-network. Specifically, in the super-net training process, we have designed an objective function to reduce overfitting. In the following section, we introduce the generation-based loss function design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objective of the Super-net</head><p>It has been demonstrated that by sharing the weight parameters among all the architecture instances, we will gain several orders of magnitude speedup in the search process <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b39">41]</ref>. However, this comes with the cost that the performance of all the candidate networks cannot be fairly evaluated, since they inherit the sub-optimal shared parameters from the supernet. This is due to the super-net containing many sub-networks of different sizes and shapes, and they share the weights with each other, making the sub-networks inherit the common sub-optimal weights. Intuitively, this factor leads to low-quality decision-making of the search controller when selecting the candidate network without considering its genuine optimal performance. Therefore, we propose to use a new training objective function to reduce the phenomenon of model overfitting and prevent interference between the sub-networks.</p><p>Recall previous NAS methods <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b57">59]</ref>, they either use training loss or validation loss to update the network parameters, i.e.,</p><formula xml:id="formula_1">E [L train (arch, w )] , or E [L val (arch, w )|].<label>(2)</label></formula><p>Compared to the naive approach, our novel objective function is based on the generalization gap of the network architecture. The intuition behind this is that the selected sub-network should perform well on the data that it has not trained on, thus reducing the effect of the parameter sharing problems. Therefore, we address enhancing the generalization ability of these candidate networks and adding a generation loss into the super-net training objective function. This can improve the accuracy of the search controller by enforcing a fair evaluation result, and help it to select the network models that have higher generation performance. Formally, we define the objective function in the super-net training process as follows: where w represents the current network's weights and L val (arch, w ) − L train (arch, w ) can be treated as the generation loss. The scalar variable λ balances the training loss and the generalization loss. We observe that λ = 0.45 works well in our experiments. In our experiments, we find the generation loss helps CHaNAS gain a 0.5% network accuracy improvement, and we present a detailed analysis on how the generation loss helps us improve the overall performance in the evaluation chapter.</p><formula xml:id="formula_2">E [L train (arch, w ) + λ |L val (arch, w ) − L train (arch, w )|] ,<label>(3) 5:11</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Block-level Pre-scheduling</head><p>In the prior NAS process, when a network architecture candidate is selected, it is mapped to the underlying hardware or the hardware model for performance evaluation. However, in CHaNAS, the generated network must first try-out and choose the optimum scheduling policy for that specific hardware, before being tested for true-optimal performance. As a result, we must ensure its blocks are mapped to the hardware with the best scheduling policy for each network. To select the best scheduling point for a block on the target hardware, we build a block schedule optimizer F that takes the parameterized block block i = (A 1 , A 2 , . . . , A b ) from the super-net and the hardware descriptor D har d as inputs, and the output of the schedule optimizer is the best scheduling policy c * block and the associated network performance P c * block on the hardware:</p><formula xml:id="formula_3">c * block , P c * block = F (A 1 , A 2 , . . . , A b , D har d ).<label>(4)</label></formula><p>To guarantee efficient execution of these blocks for the target hardware. The process of block scheduling is depicted in Figure <ref type="figure" target="#fig_5">7</ref>, which mainly comprises two parts: the graph-level optimization and the lower-level operators scheduling for the hardware platform. For graph-level optimization, we mainly adopt two methods: layer fusion, including intrinsic fusion, pointwise fusion, and kernel fusion, which reduces the memory and communication overhead for the intermediate data, and data layout transformation, such as the flatten, concat, and reorganization operation, which transform the feature data layouts into back-end friendly patterns.</p><p>For lower-level operator scheduling, as each block is composed of MBConv cells with similar structures but different in tensor shapes, we optimize the schedule of each operator (e.g., conv2d, depthwise conv2d) independently in the MBConv cell. The lower-level operator scheduling is a self-tuning process that aims to optimize low-level implementations for maximum hardware performance. Initially, we generate a scheduling space by enumerating different combinations of  <ref type="table" target="#tab_0">1</ref>), including the memory tiling, loop transformations, vectorization/tensorization, and parallelization strategies <ref type="bibr" target="#b11">[13]</ref>. Then, without requiring human intervention, we apply a heuristic method to search the scheduling space and find schedules that maximize performance for a particular combination of operator, tensor shape, and hardware configuration.</p><p>Encoding the operator schedules. The operator schedule is encoded using a vector, with each element representing a specific primitive or parameter choice. Figures <ref type="figure" target="#fig_6">8(c</ref>) and (e) are two scheduling examples for one conv2d operator instance in Figure <ref type="figure" target="#fig_6">8(b)</ref>. Both Figures <ref type="figure" target="#fig_6">8(c</ref>) and (e) split the original seven loops of conv2d into 13 sub-loops but with different split parameters, then reorder them, and generate a larger out-most loop by fusion. Figures <ref type="figure" target="#fig_6">8(d</ref>) and (f) are the corresponding encoded representation of Figures <ref type="figure" target="#fig_6">8(c</ref>) and (e), respectively. The encoding follows a fixed order to ensure that the configuration points with the same number of parameters are put together. For operator split, we record the split factors and create a Cartesian product of sub-spaces. Supposing that a loop in a two-dimensional (2D) convolution of size L is split into Z parts, we can generate different choices via Z -factorization of integer L, where the split of 2D convolution can be represented as [f 1 , f 2 , . . . , f Z ], where f 1 × f 2 . . . × f Z = L. For reordering, we just record the new order of loops. For fusion, the loops that are not recorded are designed to be fused with their nearby outer loops during fusion. For unrolling, each expanded loop corresponds to a value of 1 when expanded and otherwise 0 when not expanded. Besides, we always parallelize the outer-most loop and vectorize the inner-most loop; therefore, parallel and vectorize are not encoded.</p><p>Scheduling-level Search. To explore this large design space efficiently, We employ a heuristic method to explore the scheduling space with a Gaussian-Process-(GP) based cost model, which is pre-trained using runtime measurement data collected from the target hardware. Different points in the search space are evaluated by querying through the GP-based cost model. Figure <ref type="figure">9</ref> has shown the process of schedule exploration. Before the scheduling strategy exploration, an   <ref type="figure">9</ref>. Overview of the operator-level schedule exploration process, which breeds the next generation using mutation and crossover operators. The schedule explorer evaluates the performance of each schedule candidate using a Gaussian-Process-based cost model and then selects the schedule with the highest score to run on a distributed device cluster via RPC, and, finally, we get the real performance and collect the performance data into the database H. To improve the cost model's predictive accuracy, we update the cost model periodically using collected data in the database H. initial set H is maintained, in which the scheduling points p 1 , p 2 . . . p H and their performance result E p are kept. In the scheduling-policy exploration process, we choose a starting point p in H with the probability exp</p><formula xml:id="formula_4">−γ (E * −Ep ) E *</formula><p>, where γ is a hyperparameter and E * is the performance of the best schedule point in H . As can be seen, the closer E p is to E * , the more likely it is that p will be chosen. We then take this point to generate a set of new points through mutation and evaluate these schedule points from the GP-based cost model. We also define some rules as a mutation mechanism to avoid the generation of inefficient schedule points. For example, we discover that in the vast majority of cases, a divisible split factor is the most efficient, but other split factors produce inferior outcomes. As a result, for each loop in the mutation process, we limit the split factors to be divisible. Then the top-k newly generated points will be chosen and evaluated on the hardware and then added to H . All these performance measurement data are then used to update the GP-based cost model. Thanks to these blocks' structural similarity, we can reuse the GP-based cost model to speed up the scheduling-policy exploration. Consequently, we gradually find the optimal scheduling policy of an operator on the hardware platform after a certain number of iterations, which will be kept in a LUT for CHaNAS to pool during the coordinated search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Co-design Space Exploration</head><p>The size of the factorized hierarchical search space has been reduced exponentially when compared to the original joint space, since the candidate DNN architectures in the design space can be directly compared by their best performance, given the blocks that comprise the DNNs have already been evaluated on the target hardware by the pre-scheduling stage. However, the sub-net search space is still enormous; evaluating each candidate DNN still needs additional effort even when we use the weight sharing to avoid the direct training. We first automatically divide the search space according to the performance requirements, which eliminates the search in the sub-spaces that do not satisfy the constraint. Then we use the evolutionary method to search for the target solution.</p><p>Automated NAS search space division. In our analysis, we find that input resolution and the model expansion ratio are the two most important factors affecting the network's performance, so we divide the network search space according to these two factors. In this work, we can select from an input resolution factor In_size = {160, 176, 192, 208, 224} and a model expansion ratio Md_E = {1.0, 1.1, 1.2, 1.3} in our experiment. These parameters can be adjusted according to different situations. The space division leads to In_size ×Md_E (5×4 = 20 in our experiment) possible sub-spaces. Our first-step goal is to find the best sub-space that contains the target model.</p><p>Locating the target sub-space is non-trivial. One way is to perform a network search on each of the sub-search space and compare the final results. However, the computation overhead will be astronomical. Instead, we evaluate the quality of the search space by randomly sampling δ networks from the search space and comparing the distribution of the qualified networks. We collect the Cumulative Distribution Function (CDF) of each satisfying network's latency and choose the sub-search space with the lowest average latency. The intuition is that, with the same model family, the lower CDF of latency is more likely to achieve higher performance. For computing the network's latency, we construct a block performance LUT for the target device to enable fast performance estimation of DNN candidates. In the LUT, we track block-level latency metrics on real devices with different input dimensions, channel expansion ratio, and so on. Thus, the latency of the network model can be obtained by referring to the block-level performance Lat block i in LUT Lat net :</p><formula xml:id="formula_5">Lat net = F i=1 Lat block i ,<label>(5)</label></formula><p>where F denotes the total number of blocks, as the block-level method includes the optimal graphs-level and operator-level runtime optimizations. This can be more accurate than the previous operator-wise lookup table as the block-level method captures the runtime optimizations by adding the representation of model graphs and corresponding performance. Evolutionary search. After finding the best sub-search space, we also adopt an evolutionary(heuristic) search algorithm to find the target model efficiently. In evolutionary search 5:15 process, each DNN architecture is represented by as a genome vector, denoted as arch i = [block 1 , . . . ,block b ] ∈ R v , where v is the length of the vector. In evolution iterations, we randomly choose 15K sub-networks with different architecture and then measure their accuracy on 10K input samples from the validation set. To accelerate the evolution process, the [arch i , accuracy i ] pairs are used to train a multilayer perception-based accuracy predictor, which can quickly predict the model accuracy based on its architecture parameters. Through iteratively mutating high-quality DNN architectures, we can generate new DNN architectures of potentially higher quality. In each iteration, the searcher evaluates the fitness of each DNN architecture candidate based on the outputs from the accuracy and latency predictive models, then selects architectures with the highest fitness to breed the next generation using mutation and crossover operators. When a certain number of iterations is reached, or the constraints are satisfied, the searcher returns the DNN architecture from the evaluated set and the dedicated optimal scheduling policy for the target hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments Setup</head><p>To demonstrate the efficacy of our proposed method, we evaluate CHaNAS by comparing it with four previous hardware-aware NAS works (Mnasnet <ref type="bibr" target="#b42">[44]</ref>, Fbnet <ref type="bibr" target="#b46">[48]</ref>, ProxylessNas <ref type="bibr" target="#b8">[10]</ref>, and MobileNet-v3 <ref type="bibr" target="#b22">[24]</ref>) on the ImageNet2012 classification dataset. For a fair comparison, we run their source code end-to-end similar to our experiment with different random initialization seeds using hyperparameters and commands released by the authors. As mentioned before, we first build the elastic super-net that can be fit into different deployment constraints, and then we optimize each possible block used in the super-net for the target hardware. At last, we reduce the co-design search space and use the evolutionary algorithm to perform the search. We use five blocks to form the super-net with each block having four MBConvs at most. Each MBConv has a kernel size within {3, 5, 7} and a channel expand ratio E within {3, 4, 6}.The input image size is ranged within {160, 176, 192, 208, 224}, and the model expansion ratio ranges within {1.0, 1.1, 1.2, 1.3}. The standard SGD optimizer is used to train the supernet with Nesterov momentum 0.9 and weight decay 3e −5 . The initial learning rate is 2.5, and the learning rate decays as the cosine function schedules. We also use the knowledge distillation technique to progressively fine-tune the sub-networks. The whole training process takes around 4.5 days on 16 NVIDIA P100 GPUs with 32 GB memory each. For block compile optimization, we implement the optimization strategy in Python and use TVM <ref type="bibr" target="#b10">[12]</ref> tools (version 0.7.dev) for code generation targeting various hardware platforms. It is noteworthy that the block optimization process is independent of the super-net training process. Thus, the whole blocks optimization process can be executed with the training process in parallel. In general, the time overhead of the pre-scheduling process can be hidden by training, and the joint-space search can be achieved with reasonable overhead, comparable to prior NAS methods that are agnostic to the scheduling optimization space.</p><p>We apply CHaNAS to three different hardware platforms: NVIDIA P100 GPU, Intel Xeon 8163 CPU@2.5 GHz, and Samsung Note 10 phone (Snapdragon 855@2.8 GHz). For comparison, the performance of previous works on GPUs is measured in Pytorch 1.3 + Cuda 10.1; the NAS solutions On CPU are evaluated in Pytorch 1.3; On the mobile phone, the networks are implemented in TF_Lite. The performance is the averaged results of 1,000 measurements with the workloads for more stable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Improvement from Scheduling-level Optimization</head><p>To show the benefit of our block-based pre-schedule optimization, we provide the baseline implementation CHaNAS-W/O, which indicates the solutions that are extracted from the CHaNAS super-net without scheduling-level optimization. We measure the latency of their derived solutions in Pytorch on both GPU and CPU and in TF_Lite on the mobile phone. CHaNAS-W presents the solutions that have gone through block-based scheduling-level optimization in the NAS process. We also use the Note10 mobile phone as a hardware platform, as shown in Without pre-schedule optimization, CHaNAS-W/O achieves higher ImageNet top1 accuracy than MobileNet-v3 <ref type="bibr" target="#b22">[24]</ref> with similar MACs. Specifically, CHaNAS-W/O achieves 1.2×, 1.4×, and 1.5× speedup than MobileNet-v3 on GPU, CPU, and Mobile, respectively. This is attributed to the elastic super-net design; the sub-networks extracted from the CHaNAS super-net have not only high flexibility and medium-granularity suitable for hardware-oriented search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Improvement from the Adoption of Generation Loss</head><p>Previous NAS methods use the validation loss or the training loss only to update the supernet weights. For example, in the previous super-network training process, the network search controller evaluates and selects the network architectures by using its sub-optimal parameters. However, ChaNAS uses the generation loss to make sure that the candidates in the search process are fairly evaluated and thus improve the final training results of the networks under evaluation. From the Figure <ref type="figure">11</ref>, we observe that the network search solution with the generalization loss yields a better model in terms of average accuracy and performance. Hereby we give an insight on why the generalization loss helps improve the network accuracy in ChaNAS. The source of improvement obtained by the generalization loss is especially analyzed in Figure <ref type="figure">11</ref>, where the result of three different loss objectives used in the super-network training stage, and the true validation loss results during the search process are visualized. As we can see, even when the validation loss is used for updating architecture parameters, these candidate networks still suffer from the overfitting problem caused by the weight sharing. This provides evidence that the neural network architecture search can potentially benefit from the adoption of generalization loss, which may point to a different optimization direction of hardware-aware NAS in the co-design cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:17</head><p>Fig. <ref type="figure">10</ref>. Neural network implementations provided by CHaNAS; Coordinated search achieve similar accuracy but significant speedup over the neural network solutions with a fixed scheduling policy on GPU, CPU, and Mobile, respectively. Fig. <ref type="figure">11</ref>. The results for different objective functions are used as the metric to update super-network parameters. ChaNAS with L gen significantly outperforms L train -based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Benefits Gained by Co-design</head><p>To prove the effectiveness of the coordinated search method, Figure <ref type="figure">10</ref> shows the Pareto-optimal points found by different hardware-aware NAS methods on three hardware platforms. Through a joint search of network architecture and scheduling policy, CHaNAS-W achieves the highest performance over all different cases of different performance requirements. CHaNAS-W achieves 78.4% ImageNet top1 accuracy on the P100 GPU, 2.3% more accurate than MobileNet-v3, the previous best result reported through the hardware-aware NAS approach. Most importantly, CHaNAS-W runs 1.6×, 2.1×, and 2.2× faster on the P100 GPU; 1.9×, 2.2×, and 2.4× faster on the Intel Xeon 8163 CPU; and 1.7×, 2.7×, and 2.8× faster on the Note 10 phone than MobileNet-v3, Fbnet, and Mnasnet, respectively, while delivering similar output accuracy. With the co-design approach, DNN solutions can be customized and optimized according to the target hardware to achieve better performance.</p><p>We also show the co-design cost of our CHaNAS compared with previous hardware-nas in Table <ref type="table" target="#tab_1">2</ref> when developing the candidate network and schedules for N different application  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Design Points Found by CHaNAS</head><p>We visualize some of the discovered architectures for the three platforms in Figure <ref type="figure" target="#fig_8">12</ref> by using different colors and shapes to denote the kernel sizes and channel expansion ratios of MBConv. As shown in Figure <ref type="figure" target="#fig_8">12</ref>, CHaNAS uses more 7 × 7 kernels in the neural network for GPU than other platforms, and we conjecture that CHaNAS can find more efficient GPU schedules for blocks with 7 × 7 kernels, due to the large parallelism of the computation array. CHaNAS can automatically find a solution that has higher arithmetic intensity to improve GPU utilization. On a Note 10 mobile phone, CHaNAS uses network models of smaller input size but with more layers than that on Intel CPU, which implies that but deeper architectures to maintain accuracy due to the insufficiency of on-chip memory space in mobile phones. This is reasonable, since deep-but-slim network architecture requires less on-chip memory space than the wider network architectures.</p><p>We also analyze the low-level operator schedule generation result found by the schedule search strategy. Taking Note 10 as an example, select 18 different convolution from the search space and list configurations in Table <ref type="table" target="#tab_3">3</ref>. Then we compare the latency results of these 18 distinctive convolution layers that were scheduled by CHaNAS-W and TF_Lite, respectively. The absolute performance is shown in Figure <ref type="figure" target="#fig_9">13</ref>. CHaNAS-W can achieve an average latency of 0.496 ms for each layer, which is 40% lower than TFlite. To analyze why ChaNAS can achieve higher speed, we carefully study the scheduling behavior generated by CHaNAS-W. From the detailed schedule optimization behavior on Note 10 phone, CHaNAS-W can enable register blocking through multi-level tiling and vectorize the inner-most loop to achieve vectorization, which are two critical factors to improve the schedule results on a Note 10 phone. CHaNAS-W can split spatial loops and reduce loops according to the split factors from exploration results and uses split primitive and reorder primitive recursively to produce a series of small tiles to achieve the multilevel tiling. After splitting, the loops become smaller and can be potentially by the cache to exploit data locality. To exploit parallelism, CHaNAS-W dynamically fuses several outer loops into one outer-most loop and parallelizes it. The performance improvement brought by CHaNAS-W shows our scheduling-level search process can dynamically optimize the code implementation in contrast to the previous fixed mapping strategy for various hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Hardware-aware NAS. Recent hardware-aware NAS methods <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b42">44]</ref> directly incorporate the hardware feedback into the architecture search loop to discover the neural networks that work best on specific hardware. For instance, Mnasnet <ref type="bibr" target="#b42">[44]</ref> directly measures the inference latency by executing the model on mobile phones, allowing it to use performance measurement to guide the model search process. ProxylessNAS <ref type="bibr" target="#b8">[10]</ref> directly learns the DNN architecture for the ImageNet dataset by proposing a gradient-based approach to train binarized parameters. FBNet <ref type="bibr" target="#b46">[48]</ref> proposes a differentiable platform-aware NAS using Gumbel Softmax sampling. OFA <ref type="bibr" target="#b7">[9]</ref> can quickly search within a pre-trained supernet for a sub-graph network that achieves the required accuracy and speed on hardware. By these means, efficient network architectures with improved inference efficiency on particular hardware can be obtained automatically. EDD <ref type="bibr" target="#b29">[31]</ref> proposes a fully simultaneous, efficient differentiable DNN architecture and implementation co-search methodology. Targeting ASICs, Yang et al. <ref type="bibr" target="#b49">[51]</ref> propose the NASAIC that can simultaneously identify multiple DNN architectures and the associated heterogeneous ASIC accelerators to meet both DNN model accuracy and hardware performance requirements. Accelerator-aware NAS <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b56">58]</ref> explores the neural architecture and hardware accelerator co-design, by parameterizing neural architecture search and accelerator search in a unified joint search space. However, prior hardware-aware NAS methods are oblivious to the compiler-level design choices, which means they do not consider the optimization of various scheduling policies when evaluating network candidates in the search process. Consequently, they may fail to find the de-facto optimal network model that must be eventually scheduled and mapped onto the hardware via a compiler. Hence, we believe automated network-model/scheduling-policy co-search is an approach toward better co-design solutions.</p><p>High-performance DNN model scheduling. Scheduling DNN model on the target hardware is another critical factor to improve the efficiency of the DNN system and has attracted a lot of attention from both academia and industry <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref>. The best practice currently is still developing schedule libraries for different hardware and needs manually tuning for a new DNN model. Most existing deep learning frameworks <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b38">40]</ref> rely on these libraries to achieve high performance. For CPUs, MKL-DNN <ref type="bibr" target="#b0">[2]</ref> uses JIT techniques to optimize Convolutional Neural Network on Intel Xeon CPUs. SWIRL <ref type="bibr" target="#b44">[46]</ref> can generate high-quality vectorized and parallelized code for DNNs to improve the efficiency of CPUs. For GPUs, cuBlas <ref type="bibr" target="#b1">[3]</ref> can accelerate linear algebra kernels to extreme high performance, and cuDNN <ref type="bibr" target="#b15">[17]</ref> accelerates deep learning applications by assembling a set of efficient algorithms on the GPUs. TF-Lite Micro <ref type="bibr" target="#b2">[4]</ref> focuses on accelerating the DNN models on the embedded hardware. To adapt to more hardware back-ends, and improve the generality of different deep learning frameworks, Intel nGraph <ref type="bibr" target="#b16">[18]</ref> and Google XLA [1] have the role of a bridge between deep learning frameworks and hardware back-ends. Intel nGraph utilizes MKL-DNN to produce highly optimized implementations on CPUs and the PTX-emitting back-end of the LLVM to generate assembly code for GPUs. The XLA compiler acts as a back-end for TensorFlow. TVM <ref type="bibr" target="#b10">[12]</ref> proposes an ahead-of-time compiler that supports multiple front-ends and hardware platforms. These compilers adopt high-level computing graphs and leverage fusion across layers based on predetermined rules. Besides, Auto-scheduling algorithms have gradually attracted a substantial amount of attention and provide appealing productiveness. Tensor Comprehension <ref type="bibr" target="#b43">[45]</ref> adopts polyhedral IRs and employs a genetic search of affine transformation options (e.g., tile sizes, loop fusion, and scheduling strategies). PolyMage <ref type="bibr" target="#b36">[38]</ref> introduces fusion methods with loop nests and determines the rules of fusion and the range of tiling sizes to ensure a small auto-tuning space. AutoTVM <ref type="bibr" target="#b10">[12]</ref> utilizes high-level abstractions to represent the computing graph and includes a template-guided search algorithm for low-level tensor code generation. As a framework for generating tensor programs, Flextensor <ref type="bibr" target="#b54">[56]</ref> attempts to reduce human efforts in writing templates by using more general templates to cover more operators. To expand the optimization scope of tensor scheduling, Ansor <ref type="bibr" target="#b53">[55]</ref> explores a larger search space to cover the useful tensor-level program optimization options. However, none of the prior compiling or scheduling framework considers the joint search of both scheduling policy and network architecture. DNN/Compiler Co-design. While DNN/accelerator co-design has been attracting growing research interest, DNN/Compiler co-design remains largely underexplored. This may be partial, because designers are more inclined to treat compilers as well-developed tools that should be touched. Some recent works address this issue and successfully demonstrate the practicality of DNN/Compiler co-design. DNN/Compiler Co-design includes PCONV <ref type="bibr" target="#b33">[35]</ref>, PatDNN <ref type="bibr" target="#b37">[39]</ref>, and Co-CoPIE <ref type="bibr" target="#b19">[21]</ref>, which tackle model compression and compilation simultaneously. During model compression, they focus on structured pruning, guided by pre-determined compiler-friendly patterns. During compilation, they propose efficient compiler code generation, enabling the compilers to maximize or maintain both instruction-level and thread-level parallelism. MCUNet <ref type="bibr" target="#b30">[32]</ref> is another framework that integrates model design and compiler optimization. It is composed of two components, TinyNAS and TinyEngine; TinyNAS searches for specialized DNNS, while TinyEngine generates specialized code to eliminate instruction and memory redundancy.</p><p>Compared to traditional methods that either optimize the neural network using neural architecture search based on a fixed schedule strategy from a given deep learning library (e.g., Tensorflow <ref type="bibr" target="#b2">[4]</ref> and Pytorch <ref type="bibr" target="#b38">[40]</ref>) or tune the scheduling policy to maximize the inference speed for a given</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of DNN compiles optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the heuristic search method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b2">4</ref>. Relative Pareto-frontier of three different scheduling strategies. We randomly extract 200 models from the OFA<ref type="bibr" target="#b7">[9]</ref> and test them on the Note 10 mobile, Schedule-A presents the original schedule result of TF_Lite, while Schedule-B and Schedule-C present two schedule strategies that change the loop split knobs in TF_Lite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overview of CHaNAS. Both the network architecture design and the network scheduling is conducted on the basis of neural network blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Super-Net architecture, which is composed by a number of blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Block schedule optimization, including graph-level optimization and lower-level operators schedule mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The example of Operator scheduling example in CHaNAS. (a) Conv2d description. (b) Conv2d code example. (c) Schedule description. (d) Schedule encoding of (c). (e) Schedule description. (f) Schedule encoding of (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig.9. Overview of the operator-level schedule exploration process, which breeds the next generation using mutation and crossover operators. The schedule explorer evaluates the performance of each schedule candidate using a Gaussian-Process-based cost model and then selects the schedule with the highest score to run on a distributed device cluster via RPC, and, finally, we get the real performance and collect the performance data into the database H. To improve the cost model's predictive accuracy, we update the cost model periodically using collected data in the database H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Visualization of the architectures found by CHaNAS for NVIDIA P100 GPU, Intel Xeon CPU, and Note 10 mobile.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Latency results of CHaNAS-W compared to TF_lite on Note 10 mobile for different convolution layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Different Scheduling Primitives with Their Description</figDesc><table><row><cell>Name</cell><cell>Description &amp; Parameters</cell></row><row><cell>split</cell><cell>divide a loop into several sub-loops -loop to split and split factors</cell></row><row><cell>fusion</cell><cell>merge several loops into a hyper-loop -adjacent loop to fuse</cell></row><row><cell>reorder</cell><cell>change execution orders of loops -loops to reorder and new order</cell></row><row><cell>unroll</cell><cell>unroll a loop by given depth -which loop to unroll and unroll depth</cell></row><row><cell>vectorize</cell><cell>apply vector operation to a loop -which loop to vectorize</cell></row><row><cell>inline</cell><cell>inline a function -which node to inline</cell></row><row><cell>compute_at</cell><cell>put producer in the body of consumer -which node and how deep to compute at</cell></row><row><cell>parallel</cell><cell>use multithreading-which loop to parallel</cell></row><row><cell>cache</cell><cell>use shared memory to store inputs/results -how much data to cache</cell></row><row><cell>bind</cell><cell>assign a loop to parallel blocks/threads -which loop to bind to block/thread</cell></row></table><note>hardware-specific schedule primitives and the corresponding adjustable parameters (listed in Table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance Comparisons on Samsung Note10 O indicates the solutions that are extracted from the CHaNAS super-net without scheduling-level re-optimization. "AWS Cost" is calculated based on the price of on-demand P3.16xlarge instances on AWS Cloud. N is the number of deployment scenarios we experimented with in the evaluation.</figDesc><table><row><cell>Model</cell><cell>ImageNet Top-1(%)</cell><cell>Mobile Latency</cell><cell>MACs</cell><cell>Search Cost (GPU hours)</cell><cell>NN Training Cost (GPU hours)</cell><cell>Total Cost (GPU hours)</cell><cell>AWS Cost</cell></row><row><cell>Mnasnet</cell><cell>74.0</cell><cell cols="2">34.4 ms 317M</cell><cell>4000N</cell><cell>-</cell><cell>4000N</cell><cell>$12250N</cell></row><row><cell>Fbnet-C</cell><cell>74.9</cell><cell cols="2">33.6 ms 375M</cell><cell>216N</cell><cell>360N</cell><cell>576N</cell><cell>$1764N</cell></row><row><cell>ProxylessNas-R</cell><cell>74.6</cell><cell cols="2">35.7 ms 320M</cell><cell>200N</cell><cell>300N</cell><cell>500N</cell><cell>$1530N</cell></row><row><cell>MobileNet-v3(large)</cell><cell>75.2</cell><cell cols="2">28.3 ms 219M</cell><cell>-</cell><cell>180N</cell><cell>180N</cell><cell>$550N</cell></row><row><cell>CHaNAS-W/O</cell><cell>76.5</cell><cell cols="2">27.5 ms 224M</cell><cell>40N</cell><cell>1300</cell><cell>1300 +40N</cell><cell>$&lt;124N</cell></row><row><cell>CHaNAS-W</cell><cell>76.6</cell><cell cols="2">16.4 ms 240M</cell><cell>50N</cell><cell>1300</cell><cell>1300 + 50N</cell><cell>$&lt;150N</cell></row><row><cell>CHaNAS-W/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 ,</head><label>2</label><figDesc>which reports the comparison between CHaNAS-W, CHaNAS-W/O, and previous hardware-aware NAS methods on Samsung Note 10 phone. As we can see, though CHaNAS-W has higher MACs than CHaNAS-W/O, it has an obvious reduction of inference latency (16.4 ms vs. 27.5 ms), for the compiler-level co-design can dynamically adapt to different block structures and help search for the proper network schedules for the target hardware platform. In general, CHaNAS-W can achieve high performance as it strikes a balance between inter-thread and intra-thread workload decomposition on the ARM CPU by exploring numerous scheduling strategies. Besides, according to the DNN architectures obtained from CHaNAS-W and CHaNAS-W/O, the DNN architecture of CHaNAS is relatively more regular that more suitable for acceleration.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Configurations of 18 Distinctive Convolution Layers</figDesc><table><row><cell>Name</cell><cell>L1</cell><cell>L2</cell><cell>L3</cell><cell>L4</cell><cell>L5</cell><cell>L6</cell><cell>L7</cell><cell>L8</cell><cell>L9</cell><cell>L10</cell><cell>L11</cell><cell>L12</cell><cell cols="2">L13 L14</cell><cell>L15</cell><cell>L16</cell><cell>L17</cell><cell>L18</cell></row><row><cell>input_size</cell><cell>224</cell><cell>224</cell><cell>112</cell><cell>56</cell><cell>28</cell><cell>14</cell><cell>224</cell><cell>224</cell><cell>112</cell><cell>56</cell><cell>28</cell><cell>14</cell><cell>224</cell><cell>224</cell><cell>112</cell><cell></cell><cell>28</cell><cell>14</cell></row><row><cell cols="19">in/out channel 3/16 16/24 24/40 40/80 80/112 160/160 3/16 16/24 24/40 40/80 80/112 160/160 3/16 16/24 24/40 40/80 80/112 160/160</cell></row><row><cell>kernel size</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell><cell>7</cell></row><row><cell>stride</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell cols="19">scenarios of different constraints; the search cost includes pre-schedule optimization cost and co-</cell></row><row><cell cols="19">search overhead before network deployment. "AWS Cost" is calculated based on the AWS cloud-</cell></row><row><cell cols="19">charging price of on-demand P3.16x large instances. Most previous hardware-aware NAS need to</cell></row><row><cell cols="19">re-design or re-train the candidate DNN model for a new hardware platform or even the change</cell></row><row><cell cols="19">to the design constraint. For example, Mnasnet [44] needs 4,000 GPU hours (near $12,250 AWS</cell></row><row><cell cols="19">cost) for a new development scenario. While CHaNAS decouples the training and search process</cell></row><row><cell cols="19">in NAS due to the block-based design, our super-net training can be performed only once and</cell></row><row><cell cols="19">requires only a marginal search cost for fast deployment in a new application case. CHaNAS only</cell></row><row><cell cols="13">need additional 50 GPU hours ($150 AWS cost) for redeployment.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">ACM Transactions on Embedded Computing Systems, Vol. 22, No. 1, Article 5. Publication date: October 2022.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lei Zhang is also with Jeejio (Ningbo) Technology Co., Ltd. This work was supported by the National Natural Science Foundation of China (No. 61874124 and 61876173), the Strategic Priority Research Program of Chinese Academy of Science (Grant No. XDC05030201), and 2025 Key Technology Innovation Program of Ningbo City (No. 2018B10035).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>network <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13]</ref>, CHaNAS can utilize the resources better by neural network architecture-schedule policy co-design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed an automated NAS framework, CHaNAS, to co-optimize the DNN architecture and the dedicated network scheduling policy for a specific machine learning task on the hardware. It directly involves compiler optimization in the DNN NAS loop, aiming to offer a higher performance of DNN solutions for the target hardware platforms. With the proposed hierarchical co-design search space and the exploration method, both the network architecture design and the network scheduling can be effectively conducted on the basis of neural network blocks. Specifically, we also introduced a new objective function for super-network training based on the generalization gap, which helps improve the accuracy of the candidate DNN models, and we showed that it outperforms previously proposed training or validation loss functions. In the extensive experiments, when applied to the Imagenet2012 classification task on different hardware back-ends, CHaNAS can generate better co-design solutions over SOTA hardware-aware search methods MobileNet-v3. It is shown in the experiments that the co-design solutions generated by CHaNAS achieved 1.6×, 1.9×, and 1.7× performance boost, respectively, on NVIDIA P100 GPU, Intel Xeon CPU, and Samsung Note 10 Mobile, over the baselines of the same-level accuracy. This approach can transfer to other image classification tasks easily. We believe the larger neural network architecture and schedule co-design search space and effective exploration strategy could benefit the hardwareaware NAS methods further.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Inter MKL-DNN</title>
		<ptr target="https://github.com/intel/mkl-dnn" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">NVIDIA, CUBLAS Library</title>
		<ptr target="https://www.nvidia.com/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Best of both worlds: Automl co-design of a cnn and its hardware accelerator</title>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Royson</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="DOI">10.1109/DAC18072.2020.9218596</idno>
		<ptr target="https://doi.org/10.1109/DAC18072.2020.9218596" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC&apos;20)</title>
				<meeting>the 57th ACM/IEEE Design Automation Conference (DAC&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neuromorphic computing across the stack: Devices, circuits and architectures</title>
		<author>
			<persName><forename type="first">Aayush</forename><surname>Ankit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhronil</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.1109/SiPS.2018.8598419</idno>
		<ptr target="https://doi.org/10.1109/SiPS.2018.8598419" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Signal Processing Systems (SiPS&apos;18)</title>
				<meeting>the IEEE International Workshop on Signal Processing Systems (SiPS&apos;18)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A pipelined and scalable dataflow implementation of convolutional neural networks on FPGA</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bacis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Domenico</forename><surname>Santambrogio</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPSW.2017.44</idno>
		<ptr target="https://doi.org/10.1109/IPDPSW.2017.44" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW&apos;17)</title>
				<meeting>the IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW&apos;17)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML&apos;18)</title>
				<meeting>the International Conference on Machine Learning (ICML&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="550" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;20)</title>
				<meeting>the International Conference on Learning Representations (ICLR&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;19)</title>
				<meeting>the International Conference on Learning Representations (ICLR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<ptr target="http://arxiv.org/abs/1512.01274" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;18)</title>
				<meeting>the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Optimize Tensor Programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems (NeurIPS&apos;18)</title>
				<meeting>the Conference on Neural Information Processing Systems (NeurIPS&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">You only search once: A fast automation framework for single-stage DNN/Accelerator co-design</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.23919/DATE48585.2020.9116474</idno>
		<ptr target="https://doi.org/10.23919/DATE48585.2020.9116474" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Design, Automation Test in Europe Conference Exhibition (DATE&apos;20)</title>
				<meeting>the Design, Automation Test in Europe Conference Exhibition (DATE&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1283" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using dataflow to optimize energy efficiency of deep neural network accelerators</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<idno type="DOI">10.1109/MM.2017.54</idno>
		<ptr target="https://doi.org/10.1109/MM.2017.54" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50the Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 50the Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<idno type="DOI">10.1109/JETCAS.2019.2910232</idno>
		<ptr target="https://doi.org/10.1109/JETCAS.2019.2910232" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Emerg. Select. Top. Circ. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="292" to="308" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<ptr target="http://arxiv.org/abs/1410.0759" />
		<title level="m">Efficient primitives for deep learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intel ngraph: An intermediate representation, compiler, and executor for deep learning</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cyphers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><forename type="middle">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anahita</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayaram</forename><surname>Bobba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brookhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avijit</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Convey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leona</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Kanawi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08058</idno>
		<ptr target="http://arxiv.org/abs/1801.08058" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Energy-aware task mapping and scheduling for reliable embedded computing systems</title>
		<author>
			<persName><forename type="first">Anup</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharadwaj</forename><surname>Veeravalli</surname></persName>
		</author>
		<idno type="DOI">10.1145/2544375.2544392</idno>
		<ptr target="https://doi.org/10.1145/2544375.2544392" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reliability and energy-aware mapping and scheduling of multimedia applications on multiprocessor systems</title>
		<author>
			<persName><forename type="first">Anup</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharadwaj</forename><surname>Veeravalli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2015.2412137</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2015.2412137" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="869" to="884" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CoCoPIE: Enabling real-time AI on off-the-shelf mobile devices via compression-compilation co-design</title>
		<author>
			<persName><forename type="first">Shaoshan</forename><surname>Hui Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="62" to="68" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02838</idno>
		<ptr target="https://arxiv.org/abs/2003.02838" />
		<title level="m">Accelerator-aware neural network design using AutoML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FPGA/DNN co-design: An efficient design methodology for IoT intelligence on the edge</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Rupnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3316781.3317829</idno>
		<ptr target="https://doi.org/10.1145/3316781.3317829" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th ACM/IEEE Design Automation Conference (DAC&apos;19)</title>
				<meeting>the 56th ACM/IEEE Design Automation Conference (DAC&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00140</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV&apos;19)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<ptr target="http://arxiv.org/abs/1704.04861" />
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<ptr target="http://arxiv.org/abs/1602.07360" />
		<title level="m">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2654889</idno>
		<ptr target="https://doi.org/10.1145/2647868.2654889" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
				<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accuracy vs. efficiency: Achieving both through FPGA-implementation aware neural architecture search</title>
		<author>
			<persName><forename type="first">Weiwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin H.-M</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingfeng</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtong</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3316781.3317757</idno>
		<ptr target="https://doi.org/10.1145/3316781.3317757" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th ACM/IEEE Design Automation Conference (DAC&apos;19)</title>
				<meeting>the 56th ACM/IEEE Design Automation Conference (DAC&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanth</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1145/3352460.3358252</idno>
		<ptr target="https://doi.org/10.1145/3352460.3358252" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (Micro&apos;19)</title>
				<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (Micro&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="754" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-design of deep neural nets and neural net accelerators for embedded vision applications</title>
		<author>
			<persName><forename type="first">Kiseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="DOI">10.1147/JRD.2019.2942284</idno>
		<ptr target="https://doi.org/10.1147/JRD.2019.2942284" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th ACM/IEEE Design Automation Conference (DAC&apos;18)</title>
				<meeting>the 55th ACM/IEEE Design Automation Conference (DAC&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Edd: Efficient differentiable dnn architecture and implementation co-search for embedded ai solutions</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC&apos;20)</title>
				<meeting>the 57th ACM/IEEE Design Automation Conference (DAC&apos;20)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mcunet: Tiny deep learning on iot devices</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10319</idno>
		<ptr target="https://arxiv.org/abs/2007.10319" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;19)</title>
				<meeting>the International Conference on Learning Representations (ICLR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On neural architecture search for resourceconstrained hardware platforms</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtong</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00105</idno>
		<ptr target="http://arxiv.org/abs/1911.00105" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execution on mobile devices</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Ming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5117" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimizing loop operation and dataflow in FPGA acceleration of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarma</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Sun</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA&apos;17)</title>
				<meeting>the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hardware-aware machine learning: Modeling and optimization</title>
		<author>
			<persName><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ermao</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3240765.3243479</idno>
		<ptr target="https://doi.org/10.1145/3240765.3243479" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design (ICCAD&apos;18)</title>
				<meeting>the International Conference on Computer-Aided Design (ICCAD&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Polymage: Automatic optimization for image processing pipelines</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Teja Mullapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Vasista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<idno type="DOI">10.1145/2694344.2694364</idno>
		<ptr target="https://doi.org/10.1145/2694344.2694364" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Comput. Arch. News</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight pruning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="907" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<ptr target="https://arxiv.org/abs/1912.01703" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.03268" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4095" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01044</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;20)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<idno type="DOI">10.1145/2491956.2462176</idno>
		<ptr target="https://doi.org/10.1145/2491956.2462176" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00293</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00293" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;19)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<ptr target="http://arxiv.org/abs/1802.04730" />
		<title level="m">Tensor comprehensions: Framework-agnostic highperformance machine learning abstractions</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SWIRL: High-performance manycore CPU code generation for deep neural networks</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tharindu</forename><surname>Rusira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Truong</surname></persName>
		</author>
		<idno type="DOI">10.1177/1094342019866247</idno>
		<ptr target="https://doi.org/10.1177/1094342019866247" />
	</analytic>
	<monogr>
		<title level="j">Int. J. High Perf. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1275" to="1289" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DeepBurning: Automatic generation of FPGA-based learning accelerators for the neural network family</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897937.2898003</idno>
		<ptr target="https://doi.org/10.1145/2897937.2898003" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th ACM/IEEE Design Automation Conference (DAC&apos;16)</title>
				<meeting>the 53th ACM/IEEE Design Automation Conference (DAC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01099</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.01099" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;19)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring heterogeneous algorithms for accelerating deep convolutional neural networks on FPGAs</title>
		<author>
			<persName><forename type="first">Qingcheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<idno type="DOI">10.1145/3061639.3062244</idno>
		<ptr target="https://doi.org/10.1145/3061639.3062244" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Design Automation Conference</title>
				<meeting>the 54th Annual Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06392</idno>
		<ptr target="https://arxiv.org/abs/2001.06392" />
		<title level="m">Latency-aware differentiable neural architecture search</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Co-exploration of neural architectures and heterogeneous asic accelerator designs targeting multiple tasks</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC&apos;20)</title>
				<meeting>the 57th ACM/IEEE Design Automation Conference (DAC&apos;20)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Synetgy: Algorithm-hardware co-design for ConvNet accelerators on embedded FPGAs</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Lavagno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kees</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289602.3293902</idno>
		<ptr target="https://doi.org/10.1145/3289602.3293902" />
	</analytic>
	<monogr>
		<title level="m">InProceedings of the ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA&apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DeepBurning: Automatic generation of FPGA-based learning accelerators for the neural network family</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yinhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Huawei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiaowei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897937.2898003</idno>
		<ptr target="https://doi.org/10.1145/2897937.2898003" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th ACM/IEEE Design Automation Conference (DAC&apos;16</title>
				<meeting>the 53th ACM/IEEE Design Automation Conference (DAC&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00716</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00716" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;19)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi20/presentation/zheng" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;19)</title>
				<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;19)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373376.3378508</idno>
		<ptr target="https://doi.org/10.1145/3373376.3378508" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
				<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="859" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">BayesNAS: A Bayesian approach for neural architecture search</title>
		<author>
			<persName><forename type="first">Hongpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/zhou19e.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML&apos;19)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7603" to="7613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Rethinking co-design of neural architectures and hardware accelerators</title>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiyi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08619</idno>
		<ptr target="https://arxiv.org/abs/2102.08619" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;17)</title>
				<meeting>the International Conference on Learning Representations (ICLR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
