<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Image-to-Class Warping for Occluded Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xingjie</forename><surname>Wei</surname></persName>
							<email>x.wei@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Biometrics and Security Research</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<email>zlei@cbsr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@cbsr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C.-T</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Biometrics and Security Research</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Biometrics and Security Research</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Image-to-Class Warping for Occluded Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TIFS.2014.2359632</idno>
					<note type="submission">received March 15, 2014; revised July 11, 2014; accepted September 3, 2014. Date of publication September 22, 2014; date of current version November 10, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>occlusion</term>
					<term>image-to-class distance</term>
					<term>dynamic time warping</term>
					<term>biometrics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition (FR) systems in real-world applications need to deal with a wide range of interferences, such as occlusions and disguises in face images. Compared with other forms of interferences such as nonuniform illumination and pose changes, face with occlusions has not attracted enough attention yet. A novel approach, coined dynamic image-to-class warping (DICW), is proposed in this work to deal with this challenge in FR. The face consists of the forehead, eyes, nose, mouth, and chin in a natural order and this order does not change despite occlusions. Thus, a face image is partitioned into patches, which are then concatenated in the raster scan order to form an ordered sequence. Considering this order information, DICW computes the image-to-class distance between a query face and those of an enrolled subject by finding the optimal alignment between the query sequence and all sequences of that subject along both the time dimension and within-class dimension. Unlike most existing methods, our method is able to deal with occlusions which exist in both gallery and probe images. Extensive experiments on public face databases with various types of occlusions have confirmed the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F ACE recognition (FR) is one of the most active research topics in computer vision and patten recognition over the past few decades. Nowadays, automatic FR system achieves significant progress in controlled conditions. However, the performance in unconstrained conditions (e.g., large variations in illumination, pose, expression, etc.) is still unsatisfactory. In the real-world environments, faces are easily occluded by facial accessories (e.g., sunglasses, scarf, hat, veil), objects in front of the face (e.g., hand, food, mobile phone), extreme illumination (e.g., shadow), self-occlusion (e.g., non-frontal pose) or poor image quality (e.g., blurring). The difficulty of occluded FR is twofold. Firstly, occlusions distort the discriminative facial features and increases the distance between two face images of the same subject in the feature space. The intra-class variations are larger than the inter-class variations, which results in poorer recognition performance. Secondly, when facial landmarks are occluded, large registration errors usually occur and degrade the recognition rate <ref type="bibr" target="#b0">[1]</ref>.</p><p>Note that there are two related but different problems to FR with occlusions: occluded face detection and occluded face recovery. The first task is to determine whether a face image is occluded or not <ref type="bibr" target="#b1">[2]</ref>, which can be used for automatically rejecting the occluded images in applications such as passport image enrolment. This rejection mechanism is not always applicable in some scenarios (e.g., surveillance) where no alternative image can be obtained due to the lack of user cooperation. The second task is to restore the occluded region in face images <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. It can recover the occluded area but is unable to directly contribute to recognition since the identity information can be contaminated during inpainting.</p><p>An intuitive idea for handling occlusions in FR is to detect the occluded region first and then perform recognition using only the unoccluded part. Min et al. <ref type="bibr" target="#b4">[5]</ref> adopted a SVM classifier to detect the occluded region in a face image then used only the unoccluded area of a probe face (i.e., query face) as well as the corresponding area of the gallery faces (i.e., reference faces) for recognition. But note that the occlusion types in the training images are the same as those in the testing images. Jia and Martinez <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> used a skin colour based mask to remove the occluded area for recognition. However, the types of occlusions are unpredictable in practical scenarios. The location, size and shape of occlusions are unknown, hence increasing the difficulty in segmenting the occluded region from the face images. Currently most of the occlusion detectors are trained on faces with specific types of occlusions (i.e., the training is data-dependent) and hence generalise poorly to various types of occlusions in the realworld environment.</p><p>In this paper, we focus on performing recognition in the presence of occlusions. There are two main categories of approaches in this direction. The first is the reconstruction based approaches which treat occluded FR as a reconstruction problem <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b12">[13]</ref>. The sparse representation based classification (SRC) <ref type="bibr" target="#b7">[8]</ref> is a representative example. A clean image is reconstructed from an occluded probe image by a linear combination of gallery images. Then the occluded image is assigned to the class with the minimal reconstruction error.</p><p>1556-6013 © 2014 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I THREE TYPICAL OCCLUSION CASES</head><p>The reconstruction based approaches usually require a large number of samples per subject to represent a probe image. However, a sufficient number of samples are not always available in practical scenarios.</p><p>The second category is the local matching based approaches <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b16">[17]</ref>. Facial features are extracted from local areas of a face, for example, overlapping or non-overlapping patches of an image, so the affected and unaffected parts of the face can be analysed in isolation. In order to minimise matching errors due to occluded parts, different strategies such as local subspace learning <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, partial distance learning <ref type="bibr" target="#b15">[16]</ref> and multi-task sparse representation learning <ref type="bibr" target="#b16">[17]</ref> are performed. Our method belongs to this category but does not require training.</p><p>In addition to the above approaches, which focus on improving the robustness during the recognition stage, recently many researchers <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> also pay attention to the image presentation stage and attempt to extract stable, occlusioninsensitive features from face images. Since the forms of occlusions in real-world scenarios are unpredictable, it is still difficult to find a suitable representation which is insensitive to the variations in occlusions.</p><p>Most of the current methods assume that occlusions only exist in the probe images and the gallery or training images are clean. In practical scenarios, occlusions may occur in both gallery and probe images <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b19">[20]</ref>. When the number of gallery/training images is limited, excluding these occluded images would, on the one hand, lead to small sample size (SSS) problem <ref type="bibr" target="#b20">[21]</ref>, and on the other hand, ignore useful information for recognition <ref type="bibr" target="#b19">[20]</ref>. We summarise three occlusion cases in Table <ref type="table">I</ref>, which a FR system may encounter in the real-world applications. Most of the current methods rely on a clean gallery or training set and only consider the first case. The latter two cases would also occur in real environment but have not yet received much attention.</p><p>We propose a local matching based method, Dynamic Image-to-Class Warping (DICW), for occluded FR. DICW is motivated by the Dynamic Time Warping (DTW) algorithm <ref type="bibr" target="#b21">[22]</ref> which allows elastic match of two time sequences. It has been successfully applied to the area of speech recognition <ref type="bibr" target="#b21">[22]</ref>. In our work, an image is partitioned into patches, which are then concatenated in the raster scan order to form a sequence. In this way, a face is represented by a patch sequence which contains the order information of facial features. DICW calculates the Image-to-Class distance between a query face and those of an enrolled subject by finding the optimal alignment between the query sequence and all enrolled sequences of that subject. Our method allows elastic match in both time and with-class directions.</p><p>Most of the existing works that simply treat occluded FR as a signal recovery problem or just employ the framework for general object classification, neglect the inherent structure of the face. Wang et al. proposed a Markov Random Field (MRF) based method <ref type="bibr" target="#b22">[23]</ref> for FR and confirmed that the contextual information between facial features plays an important role in recognition. In this paper, we propose a novel approach that takes the facial order, which contains the geometry information of the face, into account when recognising partially occluded faces. In uncontrolled environments with uncooperative subjects, the occlusion preprocessing and the collection of sufficient and representative training samples are generally very difficult. Our method which performs recognition directly in the presence of occlusions and does not require training, is hence feasible for realistic FR applications.</p><p>This paper is built upon our preliminary work reported in <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>. The remainder of this paper is organised as follows. The proposed Dynamic Image-to-Class Warping method, from image representation, modelling to implementation, is described in Section II. Extensive experiments including discussions are presented in Section III. Further analysis about why the proposed method works; when and why it will fail and how to improve it is discussed in Section IV. Finally the work is concluded in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DYNAMIC IMAGE-TO-CLASS WARPING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Representation</head><p>An image is partitioned into J non-overlapping patches of d ×d pixels. Those patches are then concatenated in the raster scan order (i.e., from left to right and top to bottom) to form a single sequence. The reason for doing so is that the forehead, eyes, nose, mouth and chin are located in the face in a natural order, which does not change despite occlusions or imprecise registration. This spatial facial order, which is contained in the patch sequence, can be viewed as the temporal order in the time sequence. In this way, a face image can be viewed as a time sequence so the image matching problem can be handled by the time series analysis technique like DTW <ref type="bibr" target="#b21">[22]</ref>.</p><p>Let f (x, y) be the intensity of the pixel at coordinates (x, y) and f j (x, y) be the j -patch. A difference patch f j (x, y) is computed (Fig. <ref type="figure" target="#fig_0">1</ref>) by subtracting f j (x, y) from its immediate neighbouring patch f j +1 (x, y) as:</p><formula xml:id="formula_0">f j (x, y) = f j +1 (x, y) − f j (x, y) (1)</formula><p>where j ∈ {1, 2, . . . , J − 1}. Note that here the length of the difference patch sequence is J − 1.</p><p>A difference patch f j (•) actually can be viewed as the approximation of the first-order derivative of adjacent patch f j +1 (•) and f j (•). The salient facial features which represent textured regions such as eyes, nose and mouth can be enhanced since the first-order derivative operator is sensitive to edges.</p><p>We use 3,200 occluded-unoccluded image pairs of the same class and different classes from the AR database <ref type="bibr" target="#b25">[26]</ref>, respectively (6,400 pairs in total) to calculate the image distance distributions. <ref type="foot" target="#foot_0">1</ref> As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the distance distributions of the same and different classes are separated more widely when using the difference patches (Fig. <ref type="figure" target="#fig_1">2b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modelling</head><p>Face matching is implemented by defining a distance measurement between sequences and using the distance as the basis for classification. Generally, a small distance is expected if two sequences are similar to each other. DICW is based on the classical DTW algorithm <ref type="bibr" target="#b21">[22]</ref> which is used to compute the distance between two time sequences. Here we use an example to quickly illustrate the main idea of DTW (more details of the algorithm can be found in <ref type="bibr" target="#b21">[22]</ref>). As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, there are two sequences (each digit indicates a data point):</p><formula xml:id="formula_1">A = (a 1 , a 2 , a 3 , a 4 , a 5 ) = (3, 1, 10, 3, 2) B = (b 1 , b 2 , b 3 , b 4 , b 5 ) = (3, 1, 2, 10, 3).</formula><p>The Euclidean distance (i.e., using point-wise matching, Fig. <ref type="figure" target="#fig_2">3a</ref>) between them is (a</p><formula xml:id="formula_2">1 − b 1 ) 2 + • • • + (a 5 − b 5 ) 2 = √ 0 + 0 + 64 + 49 + 1 ≈ 10.</formula><p>68 which is a bit large for these two similar sequences. However, if we warp these two sequences in a non-linear way by shrinking or expanding them along the time axis during matching (i.e., allows flexible correspondences), the distance between A and B can be largely reduced<ref type="foot" target="#foot_1">2</ref> to 2 (Fig. <ref type="figure" target="#fig_2">3b</ref>). DTW, which is based on this idea, calculates the distance between two time sequences by finding the optimal alignment between them with the minimal overall cost. This will help to reduce the distance error caused by some noise data points and ensure that the distance between similar sequences is relatively small. In addition, the temporal order is considered during matching, thus cross-matching (which reverses the order of data points) is not allowed even it can lead to shorter distance (Fig. <ref type="figure" target="#fig_2">3c</ref>). Especially for FR, this is reasonable since the order of facial features should not be turned back.</p><p>Adopting this idea for FR, we want to find the optimal alignment between face sequences while minimising the distance caused by occluded patches. In this work, instead of finding the alignment between two sequences, we seek the alignment between a sequence and the sequence set of a given class (i.e., subject). A probe image consisting of M patch features is denoted by P = ( p 1 , . . . , p m , . . . , p M ).</p><p>Here P is an ordered list where each element p m is a patch feature vector (e.g., f (•) in Section II-A). The gallery set of a given class containing K images is denoted by G = {G 1 , . . . , G k , . . . , G K }. The k-th gallery image is similarly represented as a sequence of N patch features as G k = (g 1k , . . . , g nk , . . . , g Nk ) where g nk represents a patch feature vector like p m . Note that the number of patches in the probe image and that in the gallery image can be different (i.e., the values of M and N can be different) since the DTW model is able to deal with sequences with different lengths <ref type="bibr" target="#b21">[22]</ref>.</p><p>A warping path W indicating the matching correspondence of patches between P and G with T warping steps in time axis is defined as W = (w(1), . . . , w(t), . . . , w(T )) with:</p><formula xml:id="formula_3">w(t) = (m, n, k) : {1, 2, . . . , T } → {1, 2, . . . , M} ×{1, 2, . . . , N} × {1, 2, . . . , K }<label>(2)</label></formula><p>where × indicates the Cartesian product operator and max{M, N} T M + N − 1. w(t) = (m, n, k) is a triple indicating that patch p m is matched to patch g nk at step t.</p><p>Similar to the DTW model <ref type="bibr" target="#b21">[22]</ref>, W in DICW satisfies the following four constraints:</p><p>1) Boundary: w(1) = (1, 1, k) and w(T ) = (M, N, k ). The path starts at matching p 1 to g 1k and ends at matching p M to g Nk . Note that no restrictions are placed on k and k . From step 1 to T , k and k can be any value from 1 to K since the probe patch can be matched with patches from all K gallery images. 2) Monotonicity: Given w(t) = (m, n, k), the preceding triple w(t − 1) = (m , n , k ) satisfies that m m and n n. The warping path preserves the temporal order and increase monotonically.</p><p>3) Continuity: Given w(t) = (m, n, k), the preceding triple</p><formula xml:id="formula_4">w(t − 1) = (m , n , k ) satisfies that m − m 1 and n − n 1.</formula><p>The indexes of the path increase by 1 in each step, which means that each step makes smooth transitions along the time dimension. 4) Window constraint: Given w(t) = (m, n, k), it satisfies |m − n| l where l ∈ N + is the window width <ref type="bibr" target="#b21">[22]</ref>. The window constraint is designed to reduce the computational cost of DICW. But it is also meaningful for the specific FR problem since a probe patch (e.g., eye) should not match to a patch (e.g., mouth) too far away. The window with a width l is able to constrain the warping path within an appropriate range.  These constraints are extended from the constraints of the DTW algorithm. However, they are also very meaningful in the context of FR with the image representation defined in Section II-A. Our method represents a face image as a patch sequence thus here the image matching problem can be solved by the time series analysis technique.</p><p>In order to explain the concept of warping path, we take the aforementioned sequences A and B as an example. In Fig. <ref type="figure" target="#fig_3">4a</ref>, each grid on the right hand side indicates a possible matching correspondence. The indexes of the red grids indicate the matching between A and B by DTW (i.e., the optimal warping path with the minimal matching cost) as shown in the left part (here T = 6). Likewise, the same procedure of DICW is shown in Fig. <ref type="figure" target="#fig_3">4b</ref>. Compared with DTW, an additional index is added in the warping step of DICW to index different gallery sequences. In this way, the warping is performed in two directions: 1) a probe sequence P is aligned to a set of gallery sequences G according to the time dimension (maintaining the facial order) and 2) simultaneously, at each warping step, each patch in P can be matched with any patch among all gallery sequences along the within-class dimension. Our method allows elastic match in both of the aforementioned two directions.</p><p>We define the local distance <ref type="bibr" target="#b21">[22]</ref> C m,n,k = d( p m , g nk ) as the distance between two patches p m and g nk . d(•) can be any distance measurement such as the Euclidean distance or the Cosine distance. The overall matching cost of W is the sum of the local distance of each warping step:</p><formula xml:id="formula_5">S(W ) = T t =1 C w t (3)</formula><p>The optimal warping path W * (i.e., the red grid path in Fig. <ref type="figure" target="#fig_3">4b</ref>) is the path that minimises S(W). The Image-to-Class distance between P and G is simply the overall cost of W * :</p><formula xml:id="formula_6">di st D I C W ( P, G) = min W T t =1 C w t (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>After computing di st D I C W between P and each enrolled subject in the database, a classifier such as the Nearest Neighbour classifier can be adopted for classification based on di st D I C W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Through Dynamic Programming</head><p>To compute di st D I C W ( P, G) in (4), one could test every possible warping path but with a high computational cost. Fortunately, (4) can be solved efficiently by Dynamic Programming. A three-dimensional matrix D ∈ R M×N×K is created to store the cumulative distance. The element D m,n,k stores the cost of the optimal warping path of matching the first m probe patches to the set of first n patches of each gallery sequence and at the same time the m-th patch p m is matched to the patch from the k-th gallery image. The calculation of the final optimal cost di st D I C W ( P, G) is based on the results of a series of predecessors. D can be computed recursively as:</p><formula xml:id="formula_8">D m,n,k = min D {(m−1,n−1)}×{1,2,...,K } , D {(m−1,n)}×{1,2,...,K } , D {(m,n−1)}×{1,2,...,K } + C m,n,k (5)</formula><p>where the initialisation is done by extending D as an (M + 1) × (N + 1) × K matrix and setting D 0,0,• = 0, D 0,n,• = D m,0,• = ∞. Thus, di st D I C W ( P, G) can be obtained as follows:  Different from the point-wise matching (here each patch is viewed as a data point), our method tries every possible warping path under the temporal constraints then selects the one with minimal overall cost. So the warping path with large distance error will not be selected. The Image-to-Class distance is the globally optimal cost for matching. Although occlusions are not directly removed, avoiding large distance error by warping is helpful for classification from our experimental results (see Section III).</p><formula xml:id="formula_9">di st D I C W ( P, G) = min k∈{1,2,...,K } { D M,N,k }<label>(6)</label></formula><p>In addition, a patch of the probe image can be matched to patches of K different gallery images of the same class. Because the chance that all patches at the same location of the K images are occluded is low, the chance that a probe patch is compared to an unoccluded patch at the same location is thus higher. When occlusions occur in probe or/and gallery images, the Image-to-Image distance may be large. However, our model is able to exploit the information from different gallery images and reduce the effect of occlusions (Fig. <ref type="figure" target="#fig_4">5</ref>). Algorithm 1 summarises the procedure of computing the Image-to-Class distance between a probe image and a class. l is the window width and usually set to 10% of max {M, N} <ref type="bibr" target="#b21">[22]</ref>. Computational complexity is analysed in Section III-D6. minNeighbour=min </p><formula xml:id="formula_10">⎧ ⎨ ⎩ D[m − 1, n − 1, 1 : K ], D[m − 1, n, 1 : K ], D[m, n − 1, 1 : K ] ⎫ ⎬ ⎭ ; 8: for k = 1 to K do 9: D[m, n, k] =minNeighbour+C[m, n, k];</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL ANALYSIS</head><p>In this Section, we evaluate the proposed method using four databases (FRGC <ref type="bibr" target="#b26">[27]</ref>, AR <ref type="bibr" target="#b25">[26]</ref>, TFWM <ref type="bibr" target="#b27">[28]</ref> and LFW <ref type="bibr" target="#b28">[29]</ref>). We perform identification tasks according to the three cases (i.e., Uvs.O, Ovs.U and Ovs.O) described in Section I. We first consider the scenario with occlusions occur only in probe images (i.e., Uvs.O) and test our method using different number of gallery images per subject. We will demonstrate that our method works well even when a very limited number of images are available for each subject. Next, we consider the situation that occlusions exist in gallery images, which is a case most of the current works do not take account. We fix the number of gallery images per subject and conduct experiments step by step: firstly the probe images are not occluded (i.e., Ovs.U); and then both the gallery and probe images are occluded (i.e., Ovs.O). Note that, for comparison purpose, the experiments on the FRGC and the AR databases also include the case that no occlusion is presented in both gallery and probe images to confirm that DICW is also effective in general conditions. In addition, we also extend DICW to verification tasks with faces containing large uncontrolled variations.</p><p>Note that in all experiments, the gallery image set is disjoint with all probe sets. Considering that the gallery and probe images are at the same scale, in the experiments, the probe images and the gallery images are partitioned into the same number of patches, i.e., M = N as defined in Section II-B. As recommended in the work <ref type="bibr" target="#b29">[30]</ref>, the Euclidean distance and the Cosine distance are used as local distance measurements for the pixel intensity feature and the LBP feature <ref type="bibr" target="#b30">[31]</ref>, respectively.</p><p>We quantitatively compare DICW with some representative methods in the literature: the supervised linear SVM <ref type="bibr" target="#b31">[32]</ref> using PCA <ref type="bibr" target="#b32">[33]</ref> for feature extraction (PCA + LSVM), the reconstruction based SRC <ref type="bibr" target="#b7">[8]</ref> as introduced in Section I, the Image-to-Class distance based Naive Bayes Nearest Neighbour (NBNN) <ref type="bibr" target="#b33">[34]</ref> as ours, and the baseline, Hidden Markov models (HMM) <ref type="bibr" target="#b34">[35]</ref> which also considers the order information in a face. We use the difference patch representation as defined in Section II-A in NBNN and DICW. For comparison purpose, we also report the results of using the original patche (referred to OP-NBNN and OP-Warp, respectively).</p><p>Note that NBNN is a local patch based method which also exploits the Image-to-Class distance. But it does not consider the spatial relationship between patches like ours. To improve the performance, a location weight α <ref type="bibr" target="#b33">[34]</ref> is used in NBNN to constrain matching patches according to their locations. We tested different values of α and found that the performance of NBNN is highly dependent on the value of α and different testing data (e.g., different occlusion level, location) requires different value even within the same database. So we also reported the best result for each test with the optimal α value (as OP-NBNN-ub and NBNN-ub). The performance of OP-NBNN-ub and NBNN-ub can be seen as the upper bound of the performance of NBNN, which is a competitive comparison for DICW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face Identification With Randomly Located Occlusions</head><p>We first evaluate the proposed method using the Face Recognition Grand Challenge (FRGC) database <ref type="bibr" target="#b26">[27]</ref> with randomly located occlusions. Note that in each image, the locations of occlusions are randomly chosen and unknown to the algorithm. Especially, in the Ovs.O scenario, the locations of occlusions in the gallery images are different from those in the probe images. We use these images with randomly located occlusions to evaluate the effectiveness of DICW when there is no prior knowledge of the occluded location. The FRGC database contains 8,014 images from 466 subjects in two sessions. These images contain variations such as illumination and expression changes, time-lapse, etc. Similar to the work in <ref type="bibr" target="#b6">[7]</ref>, an image set of 100 subjects (eight images in two sessions are selected for each subject), is used in experiments. To simulate the randomly located occlusions, we create an occluded image set by replacing a randomly located square patch (size of 10% to 50% of the original image) from each image in the original image set with a black block (Fig. <ref type="figure" target="#fig_7">6</ref>). We design experiments according to the three occlusion scenarios: Uvs.O, Ovs.U and Ovs.O. There are 2,400 testing samples for each scenario. All images are cropped and re-sized to 80 × 65 pixels and the patch size is 6 × 5 pixels (the effect of patch size is discussed in Section III-D1).</p><p>1) Uvs.O: For each subject, we select K = 1, 2, 3 and 4 unoccluded images respectively to form the gallery sets and use the other four images with synthetic occlusions as the probe set. Fig. <ref type="figure" target="#fig_8">7</ref> shows the recognition results with different values of K . The correct identification rates of all methods increase when more gallery images are available (i.e., greater value of K ). When there are multiple gallery images per class and no occlusion (level = 0%) in images, HMM performs better than the supervised method SVM and the local matching based NBNN. But its performance is significantly affected by the increasing occlusions. In addition, when K = 1, HMM performs worst among all methods since there are not enough gallery images to train a HMM for each class. For NBNN and DICW, using the difference patch achieves better results than using the original patch (i.e., OP-NBNN and OP-Warp). Especially, by comparing DICW with OP-Warp, and NBNN with OP-NBNN, it can be found that difference patches improve the results of DICW more significantly than that of NBNN. As introduced in Section II-A, the difference patches are generated by the spatially continuous patches so they enhance the order information within a patch sequence, which is compatible with DICW. With the optimal location weights, NBNN-ub and OP-NBNN-ub perform better than SVM. When K = 1, 2, 3 and 4, the average rates for the six occlusion levels of DICW are 2.3%, 4.3%, 5.5% and 4.4% better than that of NBNN-ub, respectively. When the occlusion level = 0%, the performance of SRC is better than DICW. However, the performance drops sharply when the degree of occlusion increases. When K = 1, the Image-to-Class distance degenerates to the Image-to-Image distance. DICW, which allows time warping during matching, still achieves better results while the level of occlusion increases.</p><p>2) Ovs.U and Ovs.O: We fix the value of K to 4 and consider that occlusions exist in the galley set. For each occlusion level (from 0% to 50%), we conduct experiments with the following settings: 1) 400 occluded images (four images per subject) from the original set as the gallery set and 400 images from the unoccluded set as the probe set (Ovs.U) and 2) 400 occluded images as the gallery set and 400 occluded images as the probe set (Ovs.O). Note that the images in the gallery set are different from those in the probe sets. Fig. <ref type="figure">8</ref> shows the recognition results. The methods (e.g., HMM, SVM, SRC) which include occluded gallery images for training/modelling perform poorly in these two cases. NBNN does not perform consistently in Ovs.U and Ovs.O. Using the original patch (i.e., OP-NBNN) performs better than using the difference patch (i.e., NBNN) in Ovs.U. For DICW, using the difference patch is always better than using the original patch (i.e., OP-Warp). This confirms that the difference patch works better with DICW, as analysed before. DICW outperforms the best of NBNN (i.e., NBNN-ub) by a larger margin of 5.5% (Fig. <ref type="figure">8a</ref>) and 8.1% (Fig. <ref type="figure">8b</ref>) on average than that (4.4% in Fig. <ref type="figure" target="#fig_8">7d</ref>) in the Uvs.O tested with K = 4. These results confirm the effectiveness and robustness of DICW when the gallery and probe images are occluded. On the whole, our method performs consistently and outperforms other methods in all three occlusion cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Identification With Facial Disguises</head><p>We next test the proposed method on the AR database <ref type="bibr" target="#b25">[26]</ref> which contains real occlusions. First, we consider that no occlusion is present in both gallery and probe sets. Next, we conduct experiments according to the three occlusion cases. DICW does not rely on the prior knowledge of occlusions. We will demonstrate that it works well in both general and difficult situations later.</p><p>The AR database contains over 4,000 colour images of 126 subjects' faces. For each subject, 26 images in total are taken in two sessions (two weeks apart). These images suffer from different variations in facial expressions, illumination conditions and occlusions (i.e., sunglasses and scarf, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>). Similar to the works in <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b35">[36]</ref>, and <ref type="bibr" target="#b36">[37]</ref>, a subset of the AR database (50 men and 50 women) is used <ref type="bibr" target="#b37">[38]</ref>. All images are cropped and re-sized to 83×60 pixels and the patch size is 5×5 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Without Occlusion:</head><p>We have evaluated the performance of DICW when no occlusion exists in both gallery and probe sets in Section III-A (i.e., occlusion level = 0% in the experiments). In this section, we adopt the setting in <ref type="bibr" target="#b7">[8]</ref> using images without occlusions to further test DICW. For each subject, 14 images are chosen (four neutral faces with different illumination conditions and three faces with different expressions in each session). Seven images from Session 1 are used as the gallery set and the other seven from Session 2 as the probe set. Table <ref type="table">II</ref> shows the identification rates. HMM does not perform as good as others. This may be due to other variations such as illumination and expression changes in the training images. Again, the difference patch does not improve NBNN comparing with the original patch (i.e., OP-NBNN). With the optimal location weights, the difference patch (i.e., NBNN-ub) is 3.7% better than the original patch (i.e., OP-NBNN-ub). For DICW, using the difference patch is 3.1% better than using the original patch (OP-Warp). As analysed in Section III-A, the difference patch can enhance the relative order of adjacent patches, the results in Table II also indicates that the difference patch is more compatible with these methods which considers the order information. When there is no occlusion in the gallery and probe images, both reconstruction based method (e.g., SRC) and local matching based methods (e.g., NBNN and DICW) achieve relatively satisfactory results. DICW significantly outperforms NBNN and is still slightly better than the upper bound of NBNN (i.e., NBNN-ub).</p><p>2) Uvs.O: The unoccluded frontal view images with various expressions are used as the gallery images (eight images per subject). For each subject, we select K = 1, 2, 4, 6 and 8 images to form the gallery sets, respectively. Two separate image sets (200 images each) containing sunglasses (cover about 30% of the image) and scarves (cover about 50% of the image) respectively are used as probe sets. Fig. <ref type="figure">9</ref> shows the recognition results. The correct identification rates increase when more gallery images are available. HMM and SVM are generic training based methods and are unable to deal with unseen occlusions in the probe images. In the scarf testing set, the performance of SRC deteriorates significantly compared with that on the sunglasses set due to the occluded area is much larger. Local matching based NBNN and DICW perform better than others on the whole. With the optimal location weights, NBNN-ub achieves very comparable performance to DICW. But DICW is slightly superior. Even at K = 1, DICW still achieves 90% and 83% on the sunglasses set and scarf set, respectively.</p><p>With the same experimental setting, we also compare DICW with the state-of-the-art algorithms (using eight gallery images per subject, K = 8). The results are shown in Table <ref type="table">III</ref>. Only the pixel intensity feature is used except the MLERPM method. MLERPM, which is also a local matching based method as ours, uses SIFT <ref type="bibr" target="#b38">[39]</ref> and SURF <ref type="bibr" target="#b39">[40]</ref> features to handle the misalignment of images. Note that compared with other methods, DICW does not require training. It achieves comparable or better recognition rates among these methods and with a relatively low computational complexity (see Section III-D6). In the scarf set, albeit the fact that nearly half of the face is occluded, only 2% images are misclassified by DICW. To the best of our knowledge, this is the best result achieved on the scarf set under the same experimental setting.</p><p>3) Ovs.U and Ovs.O: For the Ovs.U scenario, we select four images with sunglasses and scarves to form the gallery set and eight unoccluded images as the probe set. For the Ovs.O scenario, we conduct two experiments: 1) two images with scarves as the gallery set and two images with sunglasses as the probe set; 2) vice versa. Note that with this setting, in each test the occlusion type in the gallery set is different from that in the probe set, which is very challenging for recognition.</p><p>The results are shown in Fig. <ref type="figure" target="#fig_5">10</ref>. On the gallery set which contains occluded faces, the results of HMM and SVM are much worse than others as expected. In the Ovs.O testing, there are only two gallery images per subject. It is very difficult for SRC to reconstruct an unoccluded probe image with such limited number of gallery images. Local matching based NBNN and DICW perform better. Comparing OP-NBNN with OP-NBNN-ub, and NBNN with NBNN-ub, it can be found that the performance of NBNN is highly dependent on the optimal location weights. Overall, DICW consistently outperforms the best of NBNN (i.e., NBNN-ub) by about 4% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Face Identification With General Occlusions in Realistic Environment</head><p>In this Section, we test our method on the The Face We Make (TFWM) <ref type="bibr" target="#b27">[28]</ref> database captured under natural and arbitrary conditions. It has more than 2,000 images which contains frontal view faces of strangers on the streets with uncontrolled lighting. The sources of occlusions include glasses, sunglasses, hat, hair and hand on the face. Besides occlusions, these images also contain expression, pose and head rotation variations. In our experiments, we use images of 100 subjects (ten images per subject) containing various types of occlusions (Fig. <ref type="figure" target="#fig_10">11</ref>). For each subject, we choose K = 1, 3, 5 and 8 unoccluded images as gallery sets, respectively, and the remaining two images as the probe set. Occlusions occur at random in the gallery or probe set or in both. This includes all the three occlusion scenarios in Section I. The face area of each image is cropped from the background and re-sized to 80 × 60 pixels and the patch size is 5 × 5 pixels. Only the pixel intensity feature is used in all methods.</p><p>The recognition results are shown in Fig. <ref type="figure" target="#fig_11">12</ref>. Note that the images used in the experiments are not well aligned due to the uncontrolled variations. Some occlusions (e.g., hand) have very similar texture as the face, which are difficult to be detected by skin colour based models <ref type="bibr" target="#b41">[42]</ref>. NBNN, which only relies on the texture similarity without the structural constraint of a face, does not achieve comparable performance as ours. As more gallery images are available, the accuracies of all methods increase. When K = 8, most methods reach a bottleneck with the rate around 65%. DICW outperforms these methods by a notable margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>1) The Effect of Patch Size: To investigate this the impact of patch size on the performance, we use 400 unoccluded images (size of 80×65 pixels) of 100 subjects from the FRGC database as the gallery set and 400 images in each of six probe sets, which contain randomly located occlusions from 0% to 50% level, respectively. We test DICW with the patch sizes from 3 × 3 pixels to 10 × 10 pixels. Note that we employ this dataset because the location and size of the occlusions is independent to the patch size.</p><p>The correct identification rates with respect to the patch size are shown as Fig. <ref type="figure" target="#fig_12">13</ref>. There is no sharp fluctuation in each of the rate curve when the patch size is less than or equal to 6 × 5 pixels. Our method is robust to different patch sizes in an appropriate range despite the ratio of occlusions. The relatively smaller patches lead to better recognition rate since they provide more flexibility to use spatial information than the larger ones. Based on the experimental results, sizes smaller than 6 × 5 pixels are recommended.  2) The Effect of Patch Overlap: In the previous experiments we used the difference patch to enhance the textured features in patches. It is interesting to see if the overlapping patch has this similar effect. We conducted experiments on the AR database to investigate this since it contains real occlusions with different textures. We selected four unoccluded images from Session 1 for each subject as the gallery set and two images with sunglasses and scarves from Session 2 as the probe set so the testing dataset contains variations of occlusions and illumination changes. We tested the use of different patch sizes (4×4 to 16×15 pixels) with different overlap ratios (0%, 25%, 50%, 75%) and compared their results with that of using the difference patch. 25% ratio means the adjacent patches have a 25% horizontal overlap. So the larger the ratio is, the larger the number of patches will be in each image sequence. Note that 0% overlap ratio means using the original patches (with intensity features).</p><p>Fig. <ref type="figure" target="#fig_13">14</ref> shows the recognition results. On the whole, large overlap ratio leads to better accuracy. Note that higher overlap ratio also increases the number of patches in each image sequence, which leads to a higher computational cost. For small patch sizes (i.e., 4 × 4, 5 × 5 and 8 × 6 pixels), using the difference patch yields significantly better results than using the overlapping patch. This is compatible with our analysis in Section II-A. A difference patch is the approximation of the first-order derivative of adjacent small patches. The first-order derivative operator is sensitive to edges, which is able to enhance the textured regions. When the patch becomes large (i.e., 10×10, 10×15 and 16×15 pixels), the advantage of using the difference patch is not obvious. This is reasonable since the texture in a large patch is less uniform. Note that the overall performance of using the small patch is better than that of using the large patch. DICW is compatible with the small patch as analysed in Section III-D1 so in the experiments we used the best one, the difference patch instead of the overlapping patch.</p><p>3) The Effect of Image Descriptor: In Section III-D2, our experiments indicate that the difference patch leads to better accuracy since it is able to enhance the textured regions in a face image. In this section we will carry out experiments to compare the discriminative power of the proposed difference patches and other local image descriptors such as 2D-DCT (Discrete Cosine Transform coefficients), Gabor <ref type="bibr" target="#b42">[43]</ref>, LBP <ref type="bibr" target="#b30">[31]</ref> and dense SIFT <ref type="bibr" target="#b43">[44]</ref>. We use the same dataset in Section III-D2 and test both small patch size (i.e., 5×5 pixels) and large patch size (i.e., 16 × 15 pixels).</p><p>Fig. <ref type="figure" target="#fig_14">15</ref> shows the recognition results. The 2D-DCT feature is not as discriminative as others so it performs worst. For large patch as analysed before, the difference patch does not perform very well. For small patches, the performance of difference patch is comparable with that of SIFT and LBP. The Gabor features do not perform better than the difference patch since the patch is too small to extract discriminative features. Note that the computation of difference patch is much simpler than other images descriptors. From Fig. <ref type="figure" target="#fig_14">15</ref> we can see, the local image descriptor is able to strengthen DICW when the image contains uncontrolled variations such as illumination changes and occlusions. When dealing with the uncontrolled data, applying these local features can further improve the performance of DICW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Robustness to Misalignment:</head><p>The face registration error can largely degrade the recognition performance <ref type="bibr" target="#b0">[1]</ref> as we mentioned in Section I. To evaluate the robustness of DICW  to the of face images, use a subset of the AR database with 110 subjects (referred to AR-VJ) used in the work in <ref type="bibr" target="#b44">[45]</ref>. The faces in AR-VJ are automatically detected by the Viola &amp; Jones detector <ref type="bibr" target="#b45">[46]</ref> and cropped directly from the images without any alignment. Different from the images in the original AR database which are well cropped (Fig. <ref type="figure" target="#fig_4">5</ref>), these images contain large crop and alignment errors as shown in Fig. <ref type="figure" target="#fig_15">16</ref>.</p><p>Following the same experimental setting in <ref type="bibr" target="#b44">[45]</ref>, seven images of each subject from the first session are used as the gallery set and the other seven images from the second session as the probe set. All images are re-sized to 65 × 65 pixels and the patch size is 5 × 5 pixels. As analysed in the last section, we use the LBP u2 8,2 descriptor <ref type="bibr" target="#b30">[31]</ref> for feature extraction to handle the illumination variations.</p><p>The recognition results are shown in Table <ref type="table" target="#tab_1">IV</ref>. DICW outperforms other methods and achieves very close result to P2DW-FOSE <ref type="bibr" target="#b44">[45]</ref>, which is also a training-free method like ours. But different from DICW, which performs warping on the patch level, P2DW-FOSE is a pseudo 2D warping method on the pixel level and its time complexity is quadratic in the number of pixels <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Extension to Face Verification in the Wild:</head><p>In this Section, we extend DICW for face verification tasks using the Labeled Faces in the Wild (LFW) database <ref type="bibr" target="#b28">[29]</ref>, which is the most active benchmark for FR. The task of face verification under the LFW database's protocol is to determine if a pair of face images belongs to the same subject or not. Note that in the verification of each pair, it is a Image-to-Image comparison. So the experiments on the LFW database can be considered as an evaluation for the effectiveness of DICW when only time warping is used (no within-class warping).</p><p>The LFW database contains 13,233 face images of 5,749 subjects collected from the Internet. These images are captured in uncontrolled environments and contain large variations in pose, illumination, expression, time-lapse and various types of occlusions (Fig. <ref type="figure" target="#fig_16">17</ref>). Following the testing Chen et al.'s work <ref type="bibr" target="#b48">[49]</ref> produces very competitive results on the LFW database by using the high-dimensional LBP feature. It is confirmed that features sampled at facial landmarks lead to better recognition performance than those sampled from regular grids. Motivated by this, we also select 25 landmarks <ref type="bibr" target="#b49">[50]</ref> of the inner face and follow the similar process as in <ref type="bibr" target="#b48">[49]</ref>: 1) normalise the unaligned images according to 2 facial landmarks (i.e., the tip of the nose and the centre of the mouth), and 2) extract image blocks (size of 30×30 pixels) centred around 25 facial landmarks from each image. Each block is partitioned into 3 × 3 pixels patches which are then concatenated to form a sequence. The original DICW algorithm is performed according to each block (i.e., sequence) and a corresponding distance is generated respectively. The sum of these distances is the final distance for each pair. refer our method with this strategy (i.e., sampling features around landmarks) as DICW-L and the original DICW (i.e., sampling features from regular grids) as DICW-G.</p><p>LFW is an extremely challenging database containing large uncontrolled variations, especially pose changes. As presented in <ref type="bibr" target="#b50">[51]</ref>, the first several principal components (PCs) usually capture these uncontrolled variations in the principal component analysis (PCA) subspace <ref type="bibr" target="#b32">[33]</ref>. Therefore, we adopt the component analysis process in <ref type="bibr" target="#b50">[51]</ref> to remove the first several PCs for performance improvement by:</p><formula xml:id="formula_11">F = F − X i X T i F (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where F is the original feature vector of an image by concatenating all the patch features of the image sequence (i.e., P or G k in Section II-B) and X i is the first i components in the PCA subspace. We quantitatively test the value of i using the View 1 dataset provided by the LFW database and set the optimal value i = 8. F is the improved feature vector used in the experiments for the LFW database. In this way, the large uncontrolled variations can be reduced to some extent. At the same time, different from the general dimension reduction operation (i.e., the original PCA), the topological structure of  each image still maintained so our patch based DICW can be performed directly on the improved features by this process. We compare DICW with other methods under the same testing protocol without outside training data. In the experiments, only LBP u2 8,2 descriptor <ref type="bibr" target="#b30">[31]</ref> is used. We draw the the ROC (Receiver Operating Characteristic) curves of DICW and other state-of-the-art methods in Fig. <ref type="figure" target="#fig_17">18</ref>. It shows the performance of DICW-G is better than other methods which use only single feature such as SD-MATCHES (SIFT <ref type="bibr" target="#b38">[39]</ref>), H-XS-40 (LBP <ref type="bibr" target="#b30">[31]</ref>), GJD-BC-100 (Gabor <ref type="bibr" target="#b42">[43]</ref>), LARK (locally adaptive regression kernel descriptor <ref type="bibr" target="#b51">[52]</ref>) and LHS (local higher-order statistics <ref type="bibr" target="#b52">[53]</ref>). When extracting features around facial landmarks, the performance of DICW is further improved with a large margin. The area under the ROC curve (AUC) of DICW-L is 0.874 as shown in Table <ref type="table" target="#tab_2">V</ref>, which is the best among all methods. These experimental results confirm the effectiveness of DICW even only time warping is performed.</p><p>6) Computational Complexity and Usability Analysis: From Algorithm 1 in Section II-C we can see that the time complexity of DICW for computing the distance between a query image and an enrolled class is O(max {M, N}l K ), where M, N are the numbers of patches in each probe sequence and gallery sequence, respectively. l is the window width as mentioned in Section II-B. For better readability, here we use M to represent max{M, N}. The number of gallery images per class K is very small compared with the number of patches M in each sequence (i.e., K M ). Thus the complexity is represented as O(M l). Note that usually l = 10%M , so the warping distance can be obtained very efficiently. On the other hand, the computational cost of the reconstruction based method SRC is very high <ref type="bibr" target="#b7">[8]</ref>. To facilitate intuitive comparisons, Table <ref type="table" target="#tab_3">VI</ref> shows the runtime of DICW and SRC <ref type="foot" target="#foot_2">3</ref> for classifying a query image under the same setting as the experiments of Table <ref type="table">III</ref> using Matlab implementation (running on a platform with quad-core 3.10GHz CPUs and 8 GB memory). DICW is about 15 times faster than SRC <ref type="bibr" target="#b7">[8]</ref> when classifying a query image.</p><p>Compared with the reconstruction based approaches, which represent a query image using all enrolled images, DICW computes the distance between the probe image and each enrolled class independently. So in the real FR applications, the distance matrix can be generated in parallel and the enrolled database can be updated incrementally. This is very practical for the real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FURTHER ANALYSIS AND IMPROVEMENT</head><p>In the previous sections, we evaluate DICW using extensive experiments with face images with various uncontrolled variations. In this section we will further analysis why the DICW works compared with similar methods, and when and why it will fail. We also discuss the idea for improving the performance of DICW.</p><p>NBNN <ref type="bibr" target="#b33">[34]</ref> presented in the previous sections is a similar method to ours. It also calculates the Image-to-Class distance between a probe patch set and a gallery patch set from a given class. The difference is that it does not consider the spatial relationship between patches like ours and each probe patch can be matched to any patches from any location in the gallery patch set. Fig. <ref type="figure" target="#fig_18">19</ref> is an illustration example. The occluded probe image is from class 74 but is incorrectly classified to the class 5 by NBNN. Actually the images from class 74 and class 5 are not alike. But the texture of sunglasses is very similar to that of beard in class 5. Without the location constraint, the beard patches are wrongly matched to the sunglasses thus the distance is affected by this occlusion. On the other hand, DICW keeps the order information and matches patches within a proper range which leads to correct classification.</p><p>NBNN calculates the distance between two patch sets and the overall distance is the sum of patch-pair distances. On the other hand, in DICW, the probe and gallery patch set are ordered. The spatial relationship between patches is encoded. When a probe patch is matched to a gallery patch, the following probe patches will only be matched to the gallery patches within a proper range. This is guaranteed by the four constraints mentioned in Section II-B. In addition, with the help of Dynamic Programming, DICW actually tries every possible combination of matching correspondence of patch pairs so the final matching is the global optimum for the probe patch set and the gallery patch set. Compared with NBNN, DICW considers both the texture similarity and the geometry similarity of patches. The work in <ref type="bibr" target="#b22">[23]</ref> points out that the contextual information between facial features plays an important role in recognition. Our work confirms their observation. Although a location weight can be adopted in NBNN, the weight needs to be manually set for different testing dataset as analysed before, which is not suitable for practical applications. In DICW, the order constraint is naturally encoded during distance computation.</p><p>DICW represents a face image as a patch sequence which maintains the facial order of a face. To some extent, the geometric information of a face is reduced from 2D to 1D. However, the direct 2D image warping is an NP-complete problem <ref type="bibr" target="#b54">[55]</ref>. P2DW-FOSE mentioned in Section III-D4 is a pseudo 2D warping method but with a remarkably large computational cost (i.e., quadratic in the number of pixels) <ref type="bibr" target="#b44">[45]</ref>. DICW incurs a lower computational cost due to its patch sequence representation. In addition, each patch still contains the local 2D information which is helpful for classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Improving the Performance With Random Selection and Majority Voting Scheme</head><p>Fig. <ref type="figure" target="#fig_19">20</ref> shows a fail example which can not be correctly classified by both DICW and NBNN. The discriminative eyes region is occluded by sunglasses, which makes recognition difficult. In addition, a probe face with sunglasses (Fig. <ref type="figure" target="#fig_19">20a</ref>) is more similar to a gallery face with glasses (Fig. <ref type="figure" target="#fig_19">20b</ref>) in the feature space, which leads to misclassification.</p><p>Looking back to the definition of DICW in Section II-B, although warping is helpful for avoiding large distance error caused by occlusions, the occluded area is not directly removed during matching. Here we employ a simple but very effective scheme for improving the performance of DICW. As shown in Fig. <ref type="figure" target="#fig_20">21</ref>, we do not use all patches in a probe sequence for warping, instead, we randomly select a subset of patch set then compute the Image-to-Class distance based on this subset. We repeat this n times and generate a class label (the class with the shortest distance) each time according to the calculated distance. Finally, the final class label is decided by majority voting by n experts. With random selection, it is possible to skip the occluded patches. It is also possible that the occluded patches are chosen but this effect will be eliminated by the majority voting strategy since we assume that the occluded areas only take up a small part of a face. This assumption is reasonable since if most parts of a face are occluded, even a human being will feel difficult to recognise it. Different from the occlusion detection based methods which attempt to detect and remove occlusion area as we mentioned before, this simple strategy does not rely on any prior knowledge nor any data-dependent training.</p><p>Here we use the same setting to Section III-D2. We randomly select 15% patches in a sequence each time as an expert and select n = 50 experts in total. Since this scheme is based on random selection, we repeat the whole classification process ten times and calculate the average identification rate. The results are shown in Table <ref type="table" target="#tab_4">VII</ref>. The performance of DICW is improved by 2% on average by using only 50 experts (Note that for each expert, the computation of DICW is much faster than before since the number of subset patches is much smaller than that the whole sequence). Generally, more experts will lead to higher accuracy since this increases the diversity of decision views, which is more robust to different variations. But this will also raise the whole computational cost, which needs to be considered to keep a balance between accuracy and computation. The improvement is more obvious when the number of image per class is limited. A preliminary study of using this scheme to improve DICW when K = 1 is discussed in <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>We have addressed the problem of face recognition with occlusions in uncontrolled environments. Different from most of the current works, we consider the situation that occlusions exist in both gallery and probe sets. We proposed a novel approach, Dynamic Image-to-Class Warping (DICW), which considers the contextual order of facial components, for the recognition of occluded faces. We first represent a face image as an ordered sequence, then treat the image matching problem as the process of finding optimal alignment between a probe sequence and a set of gallery sequences. Finally, we employ the Dynamic Programming technique to compute the Imageto-Class distance for classification. Extensive experiments on the FRGC, AR, TFWM and LFW face databases show that DICW achieves promising performance when handling various types of occlusions. In the most challenging cases where occlusions exist in both gallery and probe sets and only a limited number of gallery images are available for each subject, DICW still performs satisfactorily. DICW can be applied directly to face images without performing occlusion detection in advance and does not require a training process. All of these make our approach more applicable in real-world scenarios. Given its merits, DICW is applicable and extendible to deal with other problems caused by local deformations in FR (e.g., the facial expression problem), as well as other object recognition problems where the geometric relationship or contextual information of features should be considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Image representation of DICW.</figDesc><graphic url="image-1.png" coords="3,92.22,57.55,163.75,98.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distributions of face image distance of the same and different classes. Using the difference patch (b), the distance distribution of the same class and that of the different classes are separated more widely compared with those using the original patch (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Various ways of sequence matching. a) Point-wise matching, b) DTW matching, and c) cross matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An illustration of warping path in (a) DTW and the (b) proposed DICW. The arrows indicate the matching correspondence. The dashed line marks the optimal warping path. Black blocks indicate the occluded patches.</figDesc><graphic url="image-7.png" coords="5,81.95,351.17,184.46,90.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The illustration of (a) the Image-to-Image and (b) the Image-to-Class comparison. Matched features are indicated by the same symbol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1</head><label>1</label><figDesc>Dynamic Image-to-Class Warping Distance DICW( P, G, l) Input: P: a probe sequence with M patches; G: a set of K gallery sequences (each with N patches) of a given class; l: the window width; Output: dist D I C W : the Image-to-Class distance between P and G; 1: Set each element in D to ∞; 2: D[0, 0, 1 : K ] = 0; 3: l = max{l, |M − N|}; 4: Compute the local distance matrix C; 5: for m = 1 to M do 6: for n = max {1, m − l} to min {N, m + l} do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>end for 12: end for 13: dist D I C W = min { D[M, N, 1 : K ]}; 14: return dist D I C W ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Sample images from the FRGC database with randomly located occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Uvs.O: identification results on the FRGC database with different number of gallery images per subject: a) K = 1, b) K = 2, c) K = 3 and d) K = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Uvs.O: identification results on the AR database with (a) sunglasses occlusion and (b) scarf occlusion.TABLE III UVS.O: COMPARISON OF DICW AND THE-STATE-OF-THE-ART METHODS (K = 8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Sample images from the TFWM database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Identification rates (%) on the TFWM database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Identification rates (%) with respect to the patch size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig.<ref type="bibr" target="#b13">14</ref>. Identification rates (%) with respect to the overlap ratio comparing with using the difference patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Identification rates (%) of using different image descriptors and the difference patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Sample images of the same subject from the AR database without alignment (AR-VJ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Sample images from the LFW database (six matched image pairs for six subjects).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. ROC curves of the-state-of-the-art methods and DICW on the LFW database.</figDesc><graphic url="image-50.png" coords="12,344.99,47.33,184.10,191.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. (a) The probe image from class 74. (b) Classification result (class 5) by NBNN. (c) Classification result (class 74) by DICW. Distance to each class computed by (d) NBNN and by (e) DICW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. (a) A probe image from class 51. (b) The wrong class (class 72) classified by DICW. (c) The gallery image from class 51.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Random selection and majority voting scheme for improving the performance of DICW.</figDesc><graphic url="image-57.png" coords="14,110.51,144.77,127.70,106.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE IV IDENTIFICATION</head><label>IV</label><figDesc>RATE (%) ON THE AR-VJ DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V AREA</head><label>V</label><figDesc>UNDER ROC CURVE (AUC) ON THE LFW DATABASE</figDesc><table><row><cell>UNDER UNSUPERVISED SETTING</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF AVERAGE RUNTIME (S)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VII IDENTIFICATION</head><label>VII</label><figDesc>RATES (%) OF DICW AND THE IMPROVEMENT SCHEME ON THE AR DATABASE</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use Euclidean distance as measurement. The image size is 83 × 60 pixels and the patch size is 5 × 5 pixels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Computation details see<ref type="bibr" target="#b21">[22]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We use the l1_ls package for implementation. http://www.stanford.edu/b oyd/l1_ls/</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of Z. Lei was supported by the National Natural Science Foundation of China under Project 61103156 and Project 61473291. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Gérard Medioni.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Why is facial occlusion a challenging problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Ekenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR 3rd Int. Conf. Biometrics (ICB</title>
				<meeting>IAPR 3rd Int. Conf. Biometrics (ICB</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Occlusion detection for ICAO compliant facial photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Storer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
				<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quality-driven face occlusion detection and recovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Restoring occluded regions using FW-PCA for face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hosoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
				<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving the recognition of faces occluded by facial accessories</title>
		<author>
			<persName><forename type="first">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. (FG)</title>
				<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit. (FG)</meeting>
		<imprint>
			<date type="published" when="2011-03">Mar. 2011</date>
			<biblScope unit="page" from="442" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face recognition with occlusions in the training and testing sets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. (FG)</title>
				<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit. (FG)</meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support Vector Machines in face recognition with occlusions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="136" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linear regression for face recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Togneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2106" to="2112" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gabor feature based sparse representation for face recognition with Gabor occlusion dictionary</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Eur. Conf. Comput. Vis. (ECCV)</title>
				<meeting>11th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6316</biblScope>
			<biblScope unit="page" from="448" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation: Which helps face recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
				<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum correntropy criterion for robust face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1561" to="1576" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust face recognition under varying illumination and occlusion considering structured sparsity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Digit</title>
				<meeting>Int. Conf. Digit</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing imprecisely localized, partially occluded, and expression variant faces from a single sample per class</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="748" to="763" />
			<date type="published" when="2002-06">Jun. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing partially occluded, expression variant faces from single training image per person with SOM and soft k-NN ensemble</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="875" to="886" />
			<date type="published" when="2005-07">Jul. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face recognition under occlusions and variant expressions with partial similarity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="230" />
			<date type="published" when="2009-06">Jun. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Partial face recognition: Alignmentfree approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1193" to="1205" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Subspace learning from image gradient orientations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2454" to="2466" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminant analysis with Gabor phase for robust face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th IAPR Int. Conf. Biometrics (ICB)</title>
				<meeting>5th IAPR Int. Conf. Biometrics (ICB)</meeting>
		<imprint>
			<date type="published" when="2012-04">Mar./Apr. 2012</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-rank matrix recovery with structural incoherence for robust face recognition</title>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2618" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face recognition from a single image per person: A survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1725" to="1745" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithm optimization for spoken word recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sakoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="49" />
			<date type="published" when="1978-02">Feb. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian face recognition based on Markov random field modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-01793-3_5</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR 3rd Int. Conf. Biometrics (ICB)</title>
				<meeting>IAPR 3rd Int. Conf. Biometrics (ICB)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5558</biblScope>
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face recognition with occlusion using dynamic image-to-class warping (DICW)</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit. (FG)</title>
				<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit. (FG)</meeting>
		<imprint>
			<date type="published" when="2013-04">Apr. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust face recognition with occlusions in both reference and query images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Biometrics Forensics</title>
				<meeting>Int. Workshop Biometrics Forensics</meeting>
		<imprint>
			<date type="published" when="2013-04">Apr. 2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The AR face database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Univ. Barcelona</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Face We Make</title>
		<author>
			<persName><forename type="first">D</forename><surname>Miranda</surname></persName>
		</author>
		<ptr target="http://www.thefacewemake.org" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Massachusetts</title>
		<imprint>
			<biblScope unit="page" from="7" to="49" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face recognition using local quantized patterns</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Napoléon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf. (BMVC)</title>
				<meeting>Brit. Mach. Vis. Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="99" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="1991-06">Jun. 1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">In defense of nearest-neighbor based image classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hidden Markov models for face recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nefian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.1998.678085</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
				<meeting>IEEE Int. Conf. Acoust., Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2721" to="2724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust and practical face recognition via structured sparsity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Eur. Conf. Comput. Vis. (ECCV)</title>
				<meeting>12th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7575</biblScope>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust feature set matching for partial face recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
				<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">PCA versus LDA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Eur. Conf. Comput. Vis. (ECCV)</title>
				<meeting>9th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3951</biblScope>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face recognition with contiguous occlusion using Markov random fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis. (ICCV)</title>
				<meeting>IEEE 12th Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
			<biblScope unit="page" from="1050" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Skin colour-based face detection in colour images</title>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Adv. Video Signal-Based Surveill. (AVSS)</title>
				<meeting>IEEE Int. Conf. Adv. Video Signal-Based Surveill. (AVSS)</meeting>
		<imprint>
			<date type="published" when="2006-11">Nov. 2006</date>
			<biblScope unit="page" from="56" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image warping for face recognition: From local optimality towards global optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3131" to="3140" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptively weighted sub-pattern PCA for face recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="505" to="511" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SURF-face: Face recognition under viewpoint consistency constraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Steingrube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf. (BMVC)</title>
				<meeting>Brit. Mach. Vis. Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="7" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: Highdimensional feature and its efficient compression for face verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995-01">Jan. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The CASIA NIR-VIS 2.0 face database</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Face verification using the LARK representation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1275" to="1286" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Local higher-order statistics (LHS) for texture categorization and facial analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Eur. Conf. Comput. Vis. (ECCV)</title>
				<meeting>12th Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7578</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recognition of faces in unconstrained environments: A comparative study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Del Solar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verschae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Correa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2009-01-19">2009. 19. Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Elastic image matching is NP-complete</title>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="445" to="453" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fixation and saccade based face recognition from single image per person with various occlusions and expressions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</title>
				<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">She has authored a number of research papers in the field of face analysis and recognition. Her current research interests include biometrics, multimedia forensics, and computer vision. Chang-Tsun Li received the B.E. degree in elec</title>
	</analytic>
	<monogr>
		<title level="m">Xingjie Wei received the B.E. degree in computer science and technology from Central South University</title>
				<meeting><address><addrLine>Changsha, China; Coventry, U.K.; Newcastle University, Newcastle upon Tyne, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Cheng Institute of Technology (CCIT), National Defense Univer-</orgName>
		</respStmt>
	</monogr>
	<note>She is currently a Post-Doctoral Researcher with the School of Computing Science</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
