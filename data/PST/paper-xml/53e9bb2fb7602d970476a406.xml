<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches</title>
				<funder ref="#_7TdMKMA">
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
				</funder>
				<funder ref="#_HYPDErt">
					<orgName type="full">Intel Research Council</orgName>
				</funder>
				<funder ref="#_kerQMgz">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="laboratory">Computer Architecture and Technology Laboratory</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">ASPLOSX</orgName>
								<address>
									<addrLine>10/02</addrLine>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated On-Chip Caches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Growing wire delays will force substantive changes in the designs of large caches. Traditional cache architectures assume that each level in the cache hierarchy has a single, uniform access time. Increases in on-chip communication delays will make the hit time of large on-chip caches a function of a line's physical location within the cache. Consequently, cache access times will become a continuum of latencies rather than a single discrete latency. This nonuniformity can be exploited to provide faster access to cache lines in the portions of the cache that reside closer to the processor. In this paper, we evaluate a series of cache designs that provides fast hits to multi-megabyte cache memories. We first propose physical designs for these Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow important data to migrate toward the processor within the same level of the cache. We show that, for multi-megabyte level-two caches, an adaptive, dynamic NUCA design achieves 1.5 times the IPC of a Uniform Cache Architecture of any size, outperforms the best static NUCA scheme by 11%, outperforms the best three-level hierarchywhile using less silicon area-by 13%, and comes within 13% of an ideal minimal hit latency solution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Today's high performance processors incorporate large level-two (L2) caches on the processor die. The Alpha 21364 <ref type="bibr" target="#b7">[8]</ref> will contain a 1.75MB L2 cache, the HP PA-8700 will contain 2.25MB of unified on-chip cache <ref type="bibr" target="#b9">[ 10]</ref>, and the Intel Itanium2 will contain 3MB of on-chip L3 cache. The sizes of on-chip L2 and L3 cache memories are expected to continue increasing as the bandwidth demands on the package grow, and as smaller technologies permit more bits per mm 2 <ref type="bibr" target="#b12">[13]</ref>.</p><p>Current multi-level cache hierarchies are organized into a few discrete levels. Typically, each level obeys inclusion, replicating the contents of the smaller level above it, and reducing accesses to the lower levels of the cache hierarchy. When choosing the size of each level, designers must balance access time and capacity, while</p><p>? Mapping: How many addressable banks should future caches contain, and how should lines be mapped into those banks?</p><p>? Search: What is the fight strategy for searching the set of possible locations for a line?</p><p>? Movement: Should a line always be placed in the same bank?</p><p>If not, how is a line moved, either while resident in the cache or across different lifetimes in the cache?</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the types of organizations that we explore in this paper, listing the number of banks and the average access times, assuming 16MB caches modeled with a 50nm technology. The numbers superimposed on the cache banks show the latency of a single contentionless request, derived from a modified version of the Cacti cache modeling tool. The average loaded access times shown below are derived from performance simulations that use the unloaded latency as the access time but which include port and channel contention.</p><p>We call a traditional'cache a Uniform Cache Architecture (UCg), shown in Figure la. Although we support aggressive sub-banking, our models indicate that this cache would perform poorly due to internal wire delays and restricted numbers of ports.</p><p>Figure <ref type="figure">lb</ref> shows a traditional multi-level cache (L2 and L3), called HL-UCA. Both levels are aggressively banked for supporting multiple parallel accesses, although the banks are not shown in the figure. Inclusion is enforced, so a line in the smaller level implies two copies in the cache, consuming extra space. Figure lc shows an aggressively banked cache, which supports non-uniform access to the different banks without the inclusion overhead of ML-UCA. The mapping of data into banks is predetermined, based on the block index, and thus can reside in only one bank of the cache. Each bank uses a private, two-way, pipetined transmission channel to service requests. We call this statically mapped, non-uniform cache S-NUCA-1.</p><p>When the delay to route a signal across a cache is significant, increasing the number of banks can improve performance. A large bank can be subdivided into smaller banks, some of which will be closer to the cache controller, and hence faster than those farther from the cache controller. The original, larger bank was necessarily accessed at the speed of the farthest, and hence slowest, sub-bank. Increasing the number of banks, however, can increase wire and decoder area overhead. Private per-bank channels, used in S-NUCA-1, heavily restrict the number of banks that can be implemented, since the per-bank channel wires adds significant area overhead to the cache if the number of banks is large. To circumvent that limitation, we propose a static NUCA design that uses a two-dimensional switched network instead of private per-bank channels, permitting a larger number of smaller, faster banks. This organization, called S-NUCA-2, is shown in Figure <ref type="figure">ld</ref>.</p><p>Even with an aggressive multi-banked design, performance may still be improved by exploiting the fact that accessing closer banks is faster than accessing farther banks. By permitting data to be mapped to many banks within the cache, and to migrate among them, a cache can be automatically managed in such a way that most requests are serviced by the fastest banks. Using the switched network, data can be gradually promoted to faster banks as they are frequently used. This promotion is enabled by spreading sets across multiple banks, where each bank forms one way of a set. Thus, cache lines in closer ways can be accessed faster than lines in farther ways.</p><p>In this paper, we compare this dynamic non-uniform scheme (D-NUCA), depicted in Figure <ref type="figure">le</ref>, to the S-NUCA schemes and ML-UCA. A D-HUCA organization incurs fewer misses than an inclusive HL-UCA design using the same area, since D-HUCA does not enforce inclusion, preventing redundant copies of the same line. We show that a D-NUCA cache achieves highest IPC across diverse applications, by adapting to the working set of each application and moving it into the banks closest to the processor. In an ML-UCA organization, conversely, the faster level may not match the working set size of an application, either being too large (and thus too slow), or being too small, incurring unnecessary misses to the slower backing level. For large caches in wire-delay-dominated technologies (16MB at 50nm), the best D-NUCA organization we explore outperforms the best S-NUCA organization by 18% and the best ML-UCA organization by 20%.</p><p>The remainder of this paper is organized as follows. Section 2 describes the limitations of single-banked caches, and describes our per-bank evaluation methodology. Section 3 describes and evaluates the $-NUCA-1 and S-NUCA-2 designs, as well as details our modeling of contention and wire pipelining. Section 4 presents a D-NUCA, in which flexible mapping policies dynamically migrate important data to nearer, faster banks along a switched network. In this section, we also compare the D-NUCA design with a number of HL-UCA hierarchies. We briefly discuss the effect of changing technology projections in Section 6. We discuss related work in Section 7, and conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">UNIFORM ACCESS CACHES</head><p>Large modern caches are subdivided into multiple sub-banks to minimize access time. Cache modeling tools, such as Cacti <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>, enable fast exploration of the cache design space by automatically choosing the optimal sub-bank count, size, and orientation. To estimate the cache bank delay, we used Cacti 3.0, which accounts for capacity, sub-bank organization, area, and process technology <ref type="bibr" target="#b26">[27]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> contains an example of a Cacti-style bank, shown in the circular expanded section of one bank. The cache is modeled assuming a central pre-decoder, which drives signals to the local decoders in the sub-banks. Data are accessed at each sub-bank and returned to the output drivers after passing through muxes, where the requested line is assembled and driven to the cache controller. Cacti uses an exhaustive search to choose the number and shape of sub-banks to minimize access time. Despite the use of an optimal sub-banking organization, large caches of this type perform poorly in a wire-delay-dominated process, since the delay to receive the portion of a line from the slowest of the sub-banks is large. caches, and extended the sire-alpha simulator <ref type="bibr" target="#b5">[6]</ref> to simulate different cache organizations with parameters derived from Cacti. sire-alpha models an Alpha 21264 core in detail <ref type="bibr" target="#b17">[18]</ref>. We assumed that all microarchitectural parameters other than the L2 organization match those of the 21264, including issue width, fetch bandwidth, and clustering. The L1 caches we simulated are similar to those of the 21264: 3-cycle access to the 64KB, 2-way set associative L1 data cache, and single-cycle access to the similarly configured L1 I-cache. All line sizes in this study were fixed at 64 bytes. In all cache experiments, we assumed that the off-chip memory controller resides near the L2 memory controller. Thus, writebacks need to be pulled out of the cache, and demand misses, when the pertinent line arrives, are injected into the cache by the L2 controller, with all contention modeled as necessary. However, we do not model any routing latency from the off-chip memory controller to the L2 cache controller. Since Cacti produces timing estimates in nanoseconds, we converted cache delays to processor cycles by assuming an aggressive clock of 8 FO4 inverter delays 1 per cycle for each technology. Recent work has shown that 8 FO4 delays is close to the optimal clock for superscalar microprocessors <ref type="bibr" target="#b11">[12]</ref>. We assume an unloaded 132cycle access to main memory, obtained by scaling the memory latency of an actual Alpha 21264 system-a DS-10L workstation at 66 cycles-by a factor equal to the ratio of the assumed and actual clock rates. Since the 21264 has approximately twice as many gate delays per cycle (16---20 FO4 versus 8 FO4), we multiplied the DS-10L 66 cycle memory latency by two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experimental Methodology</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the benchmarks used in our experiments, chosen for their high L1 miss rates. The 16 applications include six SPEC2000 floating-point benchmarks <ref type="bibr" target="#b29">[30]</ref>, six SPEC2000 integer benchmarks, three scientific applications from the NAS suite <ref type="bibr" target="#b2">[3]</ref>, and Sphinx, a speech recognition application <ref type="bibr" target="#b20">[21]</ref>. For each benchmark we simulated the sequence of instructions which capture the core repetitive phase of the program, determined empirically by plotting the L2 miss rates over one execution of each benchmark, and choosing the smallest subsequence that captured the recurrent behavior of the benchmark. Table <ref type="table" target="#tab_0">1</ref> lists the number of instructions skipped to reach the phase start (FFWD) and the number of instructions simulated (RUN). Table <ref type="table" target="#tab_0">1</ref> also shows the anticipated L2 load, listing the number of L2 accesses per 1 million instructions assuming 64KB level-1 instruction and data caches (this metric was proposed by Kessler et al. <ref type="bibr" target="#b18">[19]</ref>).  (IPC) of the UCA organization. For the rest of this paper, we assume a constant L2 cache area and vary the technology generation to scale cache capacity within that area, using the SIA Roadmap <ref type="bibr" target="#b25">[26]</ref> predictions, from 2MB of on-chip L2 at 130 nm devices to 16MB at 50 nm devices. In Table <ref type="table" target="#tab_1">2</ref>, the unloaded latency is the average access time (in cycles) assuming uniform bank access distribution and no contention. The loaded latency is obtained by averaging the actual L2 cache access time-including contention-across all of the benchmarks. Contention can include both bank contention, when a request must stall because the needed bank is busy servicing a different request, and channel contention, when the bank is free but the routing path to the bank is busy, delaying a request. The reported IPCs are the harmonic mean of all IPC values across our benchmarks, and the cache configuration displayed for each capacity is the one that produced the best IPC; we varied the number and aspect ratio of sub-banks exhaustively, as well as the number of banks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">UCA Evaluation</head><p>In the UCA cache, the unloaded access latencies are sufficiently high that contention could be a serious problem. Multiported ceils are a poor solution for overlapping accesses in large caches, as increases in area will expand loaded access times significantly: for a 2-ported, 16MB cache at 50nm, Cacti reports a significant increase in the unloaded latency, which makes a 2-ported solution perform worse than a single-ported L2 cache. Instead of multiple physical ports per cell, we assume perfect pipelining: that all routing and logic have latches, and that a new request could be initiated at an interval determined by the maximal sub-bank delay, which is shown in column 4 of Table <ref type="table" target="#tab_1">2</ref>. We did not model the area or delay consumed by the pipeline latches, resulting in optimistic performance projections for an UCA organization.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows that, despite the aggressive cache pipelining, the loaded latency grows significantly as the cache size increases, from 68 cycles at 2MB to 255 cycles at 16MB. The best overall cache size is 2MB, at which the increases in L2 latency are subsumed by the improvement in miss rates. For larger caches, the latency increases overwhelm the continued reduction in L2 misses. While the UCA organization is inappropriate for large, wire-dominated caches, it serves as a baseline for measuring the performance improvement of more sophisticated cache organizations, described in the following section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STATIC NUCA IMPLEMENTATIONS</head><p>Much performance is lost by requiring worst-case uniform access in a wire-delay dominated cache. Multiple banks can mitigate those losses, if each bank can be accessed at different speeds, proportional to the distance of the bank from the cache controller. In our banked cache models, each bank is independently addressable, and is sized and partitioned into a locally optimal physical sub-bank organization. As before, the number and physical organization of banks and sub-banks were chosen to maximize overall IPC, after an exhaustive exploration of the design space.</p><p>Data are statically mapped into banks, with the low-order bits of the index determining the bank. Each bank we simulate is fourway set associative. These static, non-uniform cache architectures (S-NUCA) have two advantages over the UCA organization previously described. First, accesses to banks closer to the cache controller incur lower latency. Second, accesses to different banks may proceed in parallel, reducing contention. We call these caches S -NUCA caches, since the mappings of data to banks are static, and the banks have non-uniform access times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Private Channels (S-NUCA-1)</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, each addressable bank in the S-NUCA-1 organization has two private, per-bank 128-bit channels, one going in each direction. Cacti 3.0 is not suited for modeling these long transmission channels, since it uses the Rubenstein RC wire delay model <ref type="bibr" target="#b13">[14]</ref> and assumes bit-line capacitative loading on each wire. We replaced that model with the more aggressive repeater and scaled wire model of Agarwal et al. for the long address and data busses to and from the banks <ref type="bibr" target="#b0">[1]</ref>.</p><p>Since banks have private channels, each bank can be accessed independently at its maximum speed. While smaller banks would provide more concurrency and a greater fidelity of non-uniform access, the numerous per-bank channels add area overhead to the array that constrains the number of banks.</p><p>When a bank conflict occurs, we model contention in two ways. A conservative policy assumes a simple scheduler that does not place a request on a bank channel until the previous request to that bank has completed. Bank requests may thus be initiated every b + 2d + 3 cycles, where b is the actual bank access time, d is the one-way transmission time on a bank's channel, and the additional 3 cycles are needed to drain the additional data packets on the channel in the case of a read request following a writeback. Since each channel is 16 bytes, and the L2 cache line size is 64 bytes, it takes 4 cycles to remove a cache line from the channel.</p><p>An aggressive pipelining policy assumes that a request to a bank may be initiated every b + 3 cycles, where b is the access latency of the bank itself. This channel model is optimistic, as we do not model the delay or area overhead of the latches necessary to have multiple requests in flight on a channel at once, although we do model the delay of the wire repeaters.</p><p>Table <ref type="table" target="#tab_3">3</ref> shows a breakdown of the access delays for the various cache sizes and technology points: the number of banks to which independent requests can be sent simultaneously, the raw bank access delay, the minimum, average, and maximum access latency of a single request to various banks, and the average latency seen at run-time (including channel contention). We assume that the cache controller resides in the middle of one side of the bank array, so the farthest distance that must be traversed is half of one dimension and the entire other dimension. Unlike UCA, the average IPC increases as the cache sizes increases, until 8 MB. At 16MB, the large area taken by the cache causes the hit latencies to overwhelm the reduced misses, even though the access latencies grow more slowly than with an UCA organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data bus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Address bus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>_? r~SWitCh</head><p>As technology advances, both the access time of individual banks and the routing delay to the farthest banks increase The bank access times for S-NUCA-1 increase from 3 cycles at 100nm to 8 cycles at 50 nm because the best organization at smaller technologies uses larger banks. The overhead of the larger, slower banks is less than the delays that would be caused by the extra wires required for more numerous, smaller banks.</p><p>The greater wire delays at small technologies cause increased routing delays to the farther banks. At 130nm, the worst-case routing delay is 10 cycles. It increases steadily to reach 33 cycles at 50nm. While raw routing delays in the cache are significant, contention is less of a problem. Contention for banks and channels can be measured by subtracting the average loaded latency from the average unloaded latency in Table <ref type="table" target="#tab_3">3</ref>. The aggressive pipelining of the request transmission on the channels eliminates from 1.3 to 4.0 cycles from the conservative pipelining average loaded bank access latency, resulting in a 5% improvement in IPC at 16MB.</p><p>The number of banks increases from 16 at 2MB to 32 at 4MB. At 8MB and 16MB, the optimal number of banks does not increase further, due to the area overhead of the per-bank channels, so each bank grows larger and slower as the cache size increases. That constraint prevents the S-NUCA-1 organization from exploiting the potential access fidelity of small, fast banks. In the next subsection, we describe a inter-bank network that mitigates the per-bank channel area constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Switched Channels (S-NUCA-2)</head><p>Figure <ref type="figure">3</ref> shows an organization that removes most of the large number of wires resulting from per-bank channels. This organization, called S-NUCA-2, embeds a lightweight, wormhole-routed 2-D mesh with point-to-point links in the cache, placing simpl e switches at each bank. Each link has two separate 128-bit channels for bidirectional routing. We modeled the switch logic in HSPICE to obtain the delay for each switch and incorporate that delay into our performance simulations. We again used the Agarwal et al.</p><p>model for measuring wire delay between switches. As in the previous configurations, we assume 4-way set associative banks. We modeled contention by implementing wormhole-routed flow control, and by simulating the mesh itself and the individual switch occupancy in detail as a part of our performance simulations. In our simulations, each switch buffers 16-byte packets, and each bank contains a larger buffer to hold an entire pending request. Thus, exactly one request can be queued at a specific bank while another is being serviced. A third arrival would block the network links, buffering the third request in the network switches and delaying other requests requiring those switches. Other banks along different network paths could still be accessed in parallel, of course.</p><p>In the highest-performing bank organization presented, each bank was sized so that the routing delay along one bank was just under one cycle. We simulated switches that had buffer slots for four flits per channel, since our sensitivity analysis showed that more than four slots per switch gained little additional IPC. In our 16MB S-NUCA-2 simulations, the cache incurred an average of 0.8 cycles of bank contention and 0.7 cycles of link contention in the network.</p><p>Table <ref type="table" target="#tab_5">4</ref> shows the IPC of the S-NUCA-2 design. For 4MB and larger caches, the minimum, average, and maximum bank latencies are significantly smaller than those for S-NUCA-1. The switched network speeds up cache accesses because it consumes less area than the private, per-bank channels, resulting in a smaller array and faster access to all banks. At 50nm with 32 banks, our models indicate that the S-NUCA-1 organization's wires consume 20.9% of the bank area, whereas the S-NUCA-2 channel overhead is just 5.9% the total area of the banks.</p><p>The S-NUCA-2 cache is faster at every technology than 9 -NUCA-1, and furthermore at 50nm with a 16MB cache, the average loaded latency is 24.2 cycles, as opposed to 34.2 cycles for S-NUCA-1. At 16MB, that reduction in latency results in a 10% average improvement in IPC across the benchmark suite. An additional benefit from the reduced per-bank wire overhead is that larger numbers of banks are possible and desirable, as we show in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DYNAMIC NUCA IMPLEMENTATIONS</head><p>In this section, we show how to exploit future cache access nonuniformity by placing frequently accessed data in closer (faster) banks and less important-yet still cached-data in farther banks. We evaluate a number of hardware policies that migrate data among the banks to reduce average L2 cache access time and improve overall performance. For these policies, we answer three important questions about the management of data in the cache: ( 1 ) mapping: how the data are mapped to the banks, and in which banks a datum can reside, (2) search: how the set of possible locations are searched to find a line, (3) movement: under what conditions the data should be migrated from one bank to another. We explore these questions in each of the following three subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Logical to Physical Cache Mapping</head><p>A large number of banks provides substantial flexibility for mapping lines to banks. At one extreme are the S-NUCA strategies, in which a line of data can only be mapped to a single statically determined bank. At the other extreme, a line could be mapped into any cache bank. While the latter approach maximizes placement flexibility, the overhead of locating the line may be too large as each bank must be searched, either through a centralized tag store or by broadcasting the tags to all of the banks.</p><p>We explore an intermediate solution called spread sets in which the multibanked cache is treated as a set-associative structure, each set is spread across multiple banks, and each bank holds one "way" of the set. The collection of banks used to implement this associativity is called a bank set and the number of banks in the set corresponds to the associativity.</p><p>A cache can be comprised of multiple bank sets. For example, as shown in Figure <ref type="figure" target="#fig_3">4a</ref>, a cache array with 32 banks could be organized as a 4-way set-associative cache, with eight bank sets, each consisting of 4 cache banks. To check for a hit in a spread-set cache, the pertinent tag in each of the 4 banks of the bank set must be checked. Note that the primary distinction between this organization and a traditional set-associative cache is that the different associative ways have different access latencies.</p><p>We propose three methods of allocating banks to bank sets and ways: simple mapping, fair mapping, and shared mapping. With the simple mapping, shown in Figure <ref type="figure" target="#fig_3">4a</ref>, each column of banks in the cache becomes a bank set, and all banks within that column comprise the set-associative ways. Thus, the cache may be searched for a line by first selecting the bank column, selecting the set within the column, and finally performing a tag match on banks within that column of the cache. The two drawbacks of this scheme are that the number of rows may not correspond to the number of desired ways in each bank set, and that latencies to access all bank sets are not the same; some bank sets will be faster than others, since some rows are closer to the cache controller than others.</p><p>Figure <ref type="figure" target="#fig_3">4b</ref> shows the fair mapping policy, which addresses both problems of the simple mapping policy at the cost of additional complexity. The mapping of sets to the physical banks is indicated with the arrows and shading in the diagram. With this model, banks are allocated to bank sets so that the average access time across all bank sets are equalized. We do not present results for this policy, but describe it for completeness. The advantage of the fair mapping policy is an approximately equal average access time for each bank set. The disadvantage is a more complex routing path from bank to bank within a set, causing potentially longer routing latencies and more contention in the network.</p><p>The shared mapping policy, shown in Figure <ref type="figure" target="#fig_3">4c</ref>, attempts to provide fastest-bank access to all bank sets by sharing the closest banks among multiple bank sets. This policy requires that if n bank sets share a single bank, then all banks in the cache are n-way set associative. Otherwise, a swap from a solely owned bank into a shared bank could result in a line that cannot be placed into the solely owned bank, since the shared bank has fewer sets than the nonshared bank. In this study, we allow a maximum of two bank sets  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Locating Cached Lines</head><p>Searching for a line among a bank set can be done with two distinct policies. The first is incremental search, in which the banks are searched in order starting from the closest bank until the requested line is found or a miss occurs in the last bank. This policy minimizes the number of messages in the cache network and keeps energy consumption low, since fewer banks are accessed while checking for a hit, at the cost of reduced performance.</p><p>The second policy is called multicast search, in which the requested address is multicast to some or all of the banks in the requested bank set. Lookups proceed roughly in parallel, but at different actual times due to routing delays through the network. This scheme offers higher performance at the cost of increased energy consumption and network contention, since hits to banks far from the processor will be serviced faster than in the incremental search policy. One potential performance drawback to multicast search is that the extra address bandwidth consumed as the address is routed to each bank may slow other accesses.</p><p>Hybrid intermediate policies are possible, such as limited multicast, in which the first M of the N banks in a bank set are searched in parallel, followed by an incremental search of the rest. Most of the hits will thus be serviced by a fast lookup, but the energy and network bandwidth consumed by accessing all of the ways at once will be avoided. Another hybrid policy is partitioned multicast, in which the bank set is broken down into subsets of banks. Each subset is searched iteratively, but the members of each subset are searched in parallel, similar to a multi-level, set-associative cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Smart Search</head><p>A distributed cache array, in which the tags are distributed with the banks, creates two new challenges. First, many banks may need to be searched to find a line on a cache hit. Second, if the line is not in the cache, the slowest bank determines the time necessary to resolve that the request is a miss. The miss resolution time thus grows as the number of banks in the bank set increases. While the incremental search policy can reduce the number of bank lookups, the serialized tag lookup time increases both the hit latency and the miss resolution time.</p><p>We applied the idea of the partial tag comparison proposed by Kessler et al. [20] to reduce both the number of bank lookups and the miss resolution time. The D-NUCA policy using partial tag comparisons, which we call smart search, stores the: partial tag bits into a smart search array located in the cache controller.</p><p>We evaluated two smart search policies: ss-performance and ssenergy. In the ss-performance policy, the cache array is searched as in previous policies. However, in parallel, the stored partial tag bits are compared with the corresponding bits of the requested tag, and if no matches occur, the miss processing is commenced early. In this policy, the smart search array must contain enough of the tag bits per line to make the possibility of false hits low; so that upon a miss, accidental partial matches of cached tags to the requested tag are infrequent. We typically cached 6 bits from each tag, balancing the probability of incurring a false hit with the access latency to the smart search array.</p><p>In the ss-energy policy, the partial tag comparison is used to reduce the number of banks that are searched upon a miss. Since the smart search array takes multiple cycles (typically 4 to 6) to access, serializing the smart search array access before any cache access would significantly reduce performance. As an optimization, we allowed the access of the closest bank to proceed in parallel with the smart search array access. After that access, if a hit in the closest bank does not occur, all other banks for which the partial tag comparison was successful are searched in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dynamic Movement of Lines</head><p>Since the goal of the dynamic NUCA approach is to maximize the number of hits in the closest banks, a desirable policy would be to use LRU ordering to order the lines in the bank sets, with the closest bank holding the MRU line, second closest holding second most-recently used, etc. The problem with that approach is that most accesses would result in heavy movement of lines among banks. In a traditional cache, the LRU state bits are adjusted to reflect the access history of the lines, but the tags and data of the lines are not moved. In an n-way spread set, however, an access to the LRU line would result in n copy operations. Practical policies must balance the increased contention and power consumption of copying with the benefits expected from bank set ordering.</p><p>We use generational promotion to reduce the amount of copying required by a pure LRU mapping, while still approximating an LRU list mapped onto the physical topology of a bank set. Generational replacement was recently proposed by Hallnor et al. for making replacement decisions in a software-managed UCA called the Indirect Index Cache <ref type="bibr" target="#b8">[9]</ref>. In our scheme, when a hit occurs to a cache line, it is swapped with the line in the bank that is the next closest to the cache controller. Heavily used lines will thus migrate toward close, fast banks, whereas infrequently used lines will be demoted into farther, slower banks.</p><p>A D-NUCA policy must determine the placement of an incoming block resulting from a cache miss. A replacement may be loaded  close to the processor, displacing an important block. The replacement may be loaded in a distant bank, in which case an important block would require several accesses before it is eventually migrated to the fastest banks. Another policy decision involves what to do with a victim upon a replacement; the two polices we evaluated were one in which the victim is evicted from the cache (a zero-copy policy), and one in which the victim is moved to a lower-priority bank, replacing a less important line farther from the controller (one-copy policy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">D-NUCA Policies</head><p>The policies we explore for D-NUCA consist of four major components: (1) Mapping: simple or shared. (2) Search: multicast, incremental, or combination. We restrict the combined policies such that a block set is partitioned into just two groups, which may then each vary in size (number of blocks) and the method of access (incremental or multicast). (3) Promotion: described by promotion distance, measured in cache banks, and promotion trigger, measured in number of hits to a bank before a promotion occurs. (4) Insertion: identifies the location to place an incoming block and what to do with the block it replaces (zero copy or one copy policies).</p><p>Our simple, baseline configuration uses simple mapping, multicast search, one-bank promotion on each hit, and a replacement policy that chooses the block in the slowest bank as the victim upon a miss. To examine how effectively this replacement policy compares to pure LRU, we measured the distribution of accesses across the sets for a traditional 16-way set associative cache and a corresponding 16MB, D-NUCA cache with an 16-way bank set. Figure <ref type="figure" target="#fig_4">5</ref> shows the distribution of hits to the various sets for each cache, averaged across the benchmark suite. For both caches, most hits are concentrated in the first two ways of each set. These results are consistent with the results originally shown by So and Rechtschaffen <ref type="bibr" target="#b27">[28]</ref>, which showed that more than 90% of cache hits were to the most recently used ways in a four-way set asso-ciative cache. So and Rechtschaffen noted that a transient increase in non-MRU accesses could be used to mark phase transitions, in which a new working set was being loaded.</p><p>The D-NUCA accesses are still concentrated in the banks corresponding to the most recently used bank. However, the experiments demonstrate a larger number of accesses to the non-MRU ways, since each line must gradually traverse the spread set to reach the fastest bank, instead of being instantly loaded into the MRU position, as in a conventional cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">PERFORMANCE EVALUATION</head><p>Table <ref type="table" target="#tab_6">5</ref> shows the performance of the baseline D-NUCA configuration, which uses the simple mapping, multicast search, tail insertion, and single-bank promotion upon each hit. As with all other experiments, for each capacity, we chose the bank and network organization that maximized overall performance. Since our shared mapping policy requires 2-way associative banks, we modeled all banks in each experiment as being 2-way set associative.</p><p>As the capacities increase with the smaller technologies, from 2MB to 16MB, the average D-NUCA access latency increases by 10 cycles, from 8.4 to 18.3. The ML-UCA and S-NUCA designs incur higher average latencies at 16MB, which are 22.3 and 30.4 cycles, respectively. Data migration enables the low average latency a! 161~IB, which, despite the cache's larger capacity and smaller device sizes, is less than the average hit latency for the 130nm, 2MB UCA organization.</p><p>At smaller capacities such as 2MB, the base D-NUCA policy shows small (,-~4%) IPC gains over the best of the S-NUCA and UCA organizations. The disparity grows as the cache size increases, with the base 16MB D-NUCA organization showing an average 9% IPC boost over the best-performing S-NUCA organization.</p><p>Table <ref type="table" target="#tab_6">5</ref> also lists miss rates and the total number of accesses to individual cache banks. The number of bank accesses decreases as the cache size grows because the miss rate decreases and fewer cache fills and evictions are required. However, at 8MB and 16MB the number of bank accesses increase significantly because the multicast policy generates substantially more cache bank accesses when the number of banks in each bank set doubles from 4 to 8 at 8MB, and again from 8 to 16 at 16MB. Incremental search policies reduce the number of bank accesses at the cost of occasional added hit latency and slightly reduced IPC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Policy Exploration</head><p>Table <ref type="table" target="#tab_7">6</ref> shows the IPC effects of using the baseline configuration and adjusting each policy independently. Changing the mapping function from simple to fair reduces IPC due to contention in the switched network, even though unloaded latencies are lower. Shifting from the baseline multicast to a purely incremental search policy substantially reduces the number of bank accesses (from 266 million to 89 million). However, even though most data are found in one of the first two banks, the incremental policy increases the average access latency from 18.3 cycles to 24.9 cycles and reduces IPC by 10%. The hybrid policies (such as multicast-2/multicast-6) gain back most of the loss in access latency <ref type="bibr">(</ref> The data promotion policy, in which blocks may be promoted only after multiple hits, or blocks may be promoted multiple banks on a hit, has little effect on overall IPC, as seen by the three experiments in Table <ref type="table" target="#tab_7">6</ref>. The best eviction policy is as shown in the base case, replacing the block at the tail. By replacing the head, and copying it into a random, lower-priority set, the average hit time is reduced, but the increase in misses (11.4% to 11.7%) offsets the gains from the lower access latencies.</p><p>While the baseline policy is among the best-performing, using the 2 multicast/6-multicast hybrid look-up reduces the number of bank accesses to 134 million (a 50% reduction) with a mere 1% drop in IPC. However, the number of bank accesses is still significantly higher than any of the static cache organizations. Table <ref type="table" target="#tab_8">7</ref>, shows the efficacy of the smart search policy at improving IPC and reducing bank accesses. We computed the size and access width of the different possible smart search configurations, and model their access latencies accurately using Cacti.</p><p>By initiating misses early, the SS-performance policy results in a 8% IPC gain, at the cost of an additional 1-2% area (a 224KB smart search tag array). In the SS-energy policy, a reduction of 85% of the bank lookups can be achieved by caching 7 bits of tag per line, with a 6% IPC gain over the base D-NUCA configuration. Coupling the SS-energy policy with the shared mapping policy results in a slightly larger tag array due to the increased associativity, so we had to reduce the smart search tag width to 6 bits to keep the array access time at 5 cycles. However, that policy results in what we believe our best policy to be: 47M bank accesses on average, and a mean IPC of 0.75. The last two rows of Table <ref type="table" target="#tab_8">7</ref> shows two upper bounds on IPC. The first upper bound row shows the mean IPC that would result if all accesses hit in the closest bank with no contention, costing 3 cycles. The second row shows the same metric, but with early initiation of misses provided by the smart search array. The highest IPC achievable was 0.89, which is 16% better than the highest-performing D -N U C A configuration. We call the policy of SS-energy with the shared mapping the "best" D-NUCA policy D N -b e s t , since it balances high performance with a relatively small number of bank accesses. The upper bound is 19% than the D N -b e s t policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to ML-UCA</head><p>Multi-level hierarchies permit a subset of frequently used data to migrate to a smaller, closer structure, just as does a D-NUCA, but at a coarser grain than individual banks. We compared the NUCA schemes with a two-level hierarchy (L2 and L3), called HL-UCA. We modeled the L2/L3 hierarchy as follows: we assumed that both levels were aggressively pipelined and banked UCA structures. We also assumed that the L3 had the same size as the comparable HUCA cache, and chose the L2 size and L3 organization that maximized overall IPC. The HL-UCA organization thus consumes more area than the single-level L2 caches, and has a greater total capacity of bits. In addition, we assumed no additional routing penalty to get from the L2 to the L3 upon an L2 miss, essentially assuming that the L2 and the L3 reside in the same space, making the multi-level model optimistic.</p><p>Table <ref type="table" target="#tab_9">8</ref> compares the IPC of the ideal two-level ML-UCA with a D-NUCA. In addition to the optimistic ML-UCA assumptions listed above, we assumed that the two levels were searched in parallel upon every access 2. The IPC of the two schemes is roughly comparable at 2MB, but diverges as the caches grow larger. At 16MB, overall IPC is 17% higher with D N -b e s t than with the ML-UCA, since many of the applications have working sets greater than 2MB, incurring unnecessary misses, and some have working sets smaller than 2MB, rendering the HL-UCA L2 too slow.</p><p>The two designs compared in this subsection are not the only points in the design space. For example, one could view a simplymapped D-NUCA as an n-level cache (where n is the bank associativity) that does not force inclusion, and in which a line is migrated to the next highest level upon a hit, rather than the highest. A D-NUCA could be designed that permitted limited inclusion, supporting multiple copies within a spread set. Alternatively, a ML-UCA in which the two (or more) levels were each organized as S-NUCA-2 designs, and in which inclusion was not enforced, would start to resemble a D-NUCA organization in which lines could only be mapped to two places. However, our experiments with many D-NUCA policies indicate that the ability to effectively adjust the size of the active working set, clustered near the processor, provides 2IPC of M L -U C A was 4% to 5% worse when the L2 and L3 were searched serially instead of in parallel.   The results show that D N -b e s t is the best cache for all but three of the benchmarks (mgrid, gcc, and and bt). In those three, D N -b e s t IPC was only slightly worse than the best organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cache Design Comparison</head><p>The second-best policy varies widely across the benchmarks; it is ML-UCA for some, S-NUCA-1 for others, and S-NUCA-2 for yet others. The D N -b e s t organization thus offers not only the best but the most stable performance. The ideal bound (labeled Upper on the graphs) shows the per-benchmark IPC assuming a loaded L2 access latency of 3 cycles, and produces an average ideal IPC across all benchmarks of 0.89. Surprisingly, the D N -b e s t IPC is only 16% worse than Upper on average, with most of that difference concentrated in four benchmarks (applu, art, mcf, and sphinx).</p><p>This result indicates that DN-be s t is close to ideal and unlikely to benefit from better migration policies or compiler support to place data in the right banks. Figure <ref type="figure">7</ref> shows how the various schemes perform across technology generations and thus cache sizes. The IPC of art, with its small working set size, is shown in Figure <ref type="figure">7a</ref>. Figure <ref type="figure">7b</ref> shows the same information for a benchmark (mcf) that has a larger-than-average working set size. Figure <ref type="figure">7c</ref> shows the harmonic mean IPC across all benchmarks.</p><p>First, the IPC improvements of D -N U C A over the other organizations grows as the cache grows larger. The adaptive nature of the D-NUCA architecture permits consistently increased IPC with increased capacity, even in the face of longer wire and on-chip communication delays. Second, the D-NUCA organization is stable, in that it makes the largest cache size the best performer for twelve applications, within 1% of the best for two applications, within 5% for one application, and within 10% for one application. Figure <ref type="figure">7a</ref> shows this disparity most clearly in that D-NUCA is the only organization for which art showed improved IPC for caches larger than 4MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">TECHNOLOGY PROJECTIONS</head><p>We initially performed this study using the technology projections from the 1999 SIA roadmap. Subsequently, the 2001 SIA projections were released, in which wire latencies are projected to grow more severe in future technologies. All results presented in this paper thus far assume the 2001 projections. However, it is instructive to compare the ideal NUCA designs using both the 1999 and 2001 projections, since the technology projections have a major effect on the ideal cache organization. Table <ref type="table" target="#tab_10">9</ref> shows the effect of technology projections on different NUCA configurations. For example, at 50nm technology, the 2001 projections reduce the wire aspect ratio from 2.8 to 2.5. They also increase the conductor resistivity and interlevel dielectric constant from 1.8 to 2.2 and from 1.5 to 2.1, respectively.</p><p>These changes in the projections of material properties results in increased wire resistance and capacitance, and therefore increased delay. The increased projected wire delays causes an increase in the optimal number of D-NUCA banks, from 64 to 256, to reduce the channel delay between banks. However, the area overhead of private channels in the S-NUCA-1 organizations restricts the number of banks. The performance of $-NUCA-1 drops by 9% when the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>This work is the first to propose novel designs for large, wiredominated on-chip caches, but significant prior work has evaluated large cache designs. Kessler examined designs for multi-megabyte caches built with discrete components <ref type="bibr" target="#b16">[17]</ref>. Hallnor and Reinhardt <ref type="bibr" target="#b8">[9]</ref> studied a fully associative software-managed design for large on-chip L2 caches, but not did not consider non-uniform access times.</p><p>While our concept of bank-mapped spread sets is new, other work has examined using associativity to balance power and performance. Albonesi examines turning off "ways" of each set to save power when cache demand is low <ref type="bibr" target="#b1">[2]</ref>. Poweil et al. evaluate the balance between incremental searches of the sets to balance power and performance <ref type="bibr" target="#b23">[24]</ref>, as we do with our multicast versus incremental policies, and as Kessler et al. did to optimize for speed <ref type="bibr" target="#b19">[20]</ref>. Bank sets do not lend themselves to less regular set mappings that reduce conflicts, such as skewed associativity <ref type="bibr" target="#b3">[4]</ref>.</p><p>Other researchers have examined using multiple banks for high bandwidth, as we do to reduce contention. Sohi and Franklin <ref type="bibr" target="#b28">[29]</ref> proposed interleaving banks to create ports, and also examined the need for L2 cache ports on less powerful processors than today's. <ref type="bibr">Wilson and Olukotun [32]</ref> performed an exhaustive study of the trade-offs involved with port and bank replication and line buffers for level-one caches. Our work aims to flatten deepening hierarchies; a goal that should be compared with Przybylski's dissertation, in which be exhaustively searched the space of multi-level caches to find the performance-optimal point <ref type="bibr" target="#b24">[25]</ref>.</p><p>Finally, many researchers have examined adaptive cache policies, a concept which is inherent in the D-NUCA organization. Dahlgren et al. studied creative ways to avoid conflicts in directmapped on-chip caches by virtually binding regions of the address space to portions of the cache <ref type="bibr" target="#b4">[5]</ref>. They also studied, as did Johnson and Hwu <ref type="bibr" target="#b14">[15]</ref>, adapting the block size to different workload needs. While the D-NUCA scheme leaves data with low locality in banks far from the processor, an alternative approach is not to cache low-locality lines at all. Gonzfilez, Aliagas, and Valero examined both a cache organization that could adaptively avoid caching data with low locality, and a locality detection scheme to split the cache into temporal and spatial caches <ref type="bibr" target="#b6">[7]</ref>. Tyson et al. also proposed a scheme to permit low-locality data to bypass the cache <ref type="bibr" target="#b30">[31 ]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">SUMMARY AND CONCLUSIONS</head><p>Non-uniform accesses have started to appear in high performance cache designs <ref type="bibr" target="#b22">[23]</ref>. In this paper, we proposed several new designs that treat the cache as a network of banks and facilitates nonuniform accesses to different physical regions. We have shown that these non-uniform cache access (NUCA) architectures achieve the following three goals:</p><p>Low latency access: the best 16MB D-NUCA configuration, simulated with projected 50nm technology parameters, demonstrated an average access time of 17 cycles at an 8 FO4 clock, which is a lower absolute latency than conventional L2 caches.</p><p>Technology scalability: Increasing wire delays will increase access times for traditional, uniform access caches. The D-NUCA design scales much better with technology than conventional caches, since most accesses are serviced by close banks, which can be kept numerous and small due to the switched network. Keeping cache area constant, the average loaded D-NUCA access times increase only slowly, from 8.4 cycles for a 2MB, 180nm cache to 18.3 cycles for a 16MB, 50nm cache.</p><p>? Performance stability: The ability of a D-NUCA to migrate data eliminates the trade-off between larger, slower caches for applications with large working sets and smaller, faster caches for applications that are less memory intensive.</p><p>? Flattening the memory hierarchy: The D-lqUCA design outperforms multi-level caches built in an equivalent area, since the multi-level cache has fixed partitions that are slower than an individual bank. This D-IqUCA result augurs a reversal of the trend of deepening memory hierarchies. We foresee future memory hierarchies having two or at most three levels: a fast L1 tightly coupled to the processor, a large on-chip NUCA L2, and perhaps an off-chip L3 that uses a memory device technology other than SRAM. Future work will examine a further flattening of the cache hierarchy into a single NUCA structure.</p><p>While the emergence of non-uniform cache latencies creates difficulties for some traditional optimization techniques, such as loaduse speculation or compiler scheduling, we view the emergence of non-uniform accesses as inevitable. Those optimization techniques must be augmented to handle the non-uniformity where possible, or simply discarded where not.</p><p>Maintaining coherence among multiple NUCA caches presents new challenges. A variant of the partial tag compare scheme of Kessler et al. [20] may make snooping feasible. The undesirable alternatives to a smart search coherence array are (1) maintaining a huge centralized tag bank for snooping, or (2) broadcasting snoops into the NUCA array upon every bus read and write request.</p><p>Finally, emerging chip multiprocessors (CMP) architectures will likely benefit from the flexibility and scalability of NUCA memory systems. A natural organization places multiple processors and an array of cache banks on a single die. As the workload changes, NUCA cache banks can be dynamically partitioned and reailocated to different processors. Since the banks are individually addressable, the memory system may be reconfigured to support different programming models-such as streaming or vector workloads-by sending configuration commands to individual banks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:Level-2 Cache Architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: UCA and S-NUCA-1 cache dcsigu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>to share a bank. Each of the n/2 farthest bank sets shares half of the closest bank for one of the closest n/2 bank sets. This policy (a) Simple Mapping lllllll (b) Fair Mapping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Mapping bank sets to banks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Way distribution of cache hits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 6: 16MB Cache Performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>Figure6compares the 16MB/50nm IPC obtained by the best of each major scheme that we evaluated: (1) UCA, (2) aggressively pipelined S-NUCA-1, (3) S-NUCA-2, (4) aggressively pipelined, optimally sized, parallel lookup ML-UCA, (5) D N -b e s t , and (6) an ideal D-NUCA upper bound. This ideal bound is a cache in which references always hit in the closest bank, never incurring any contention, resulting in a constant 3-cycle hit latency, and which includes the smart search capability for faster miss resolution.The results show that D N -b e s t is the best cache for all but</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Benchmarks used for performance experiments</figDesc><table><row><cell></cell><cell>Phase</cell><cell></cell><cell>L2 load accesses/</cell><cell></cell><cell cols="2">Phase</cell><cell>L2 load accesses/</cell></row><row><cell cols="3">SPECINT2000 FFWD RUN</cell><cell>Million instr</cell><cell cols="2">SPECFP2000 FFWD</cell><cell>RUN</cell><cell>Million instr</cell></row><row><cell>176.gcc</cell><cell cols="2">2.367B 300M</cell><cell>25,900</cell><cell>172.mgrid</cell><cell cols="2">550M 1.06B</cell><cell>21,000</cell></row><row><cell>181.mcf</cell><cell cols="2">5.0B 200M</cell><cell>260,620</cell><cell>177.mesa</cell><cell cols="2">570M 200M</cell><cell>2,500</cell></row><row><cell>197.parser</cell><cell cols="2">3.709B 200M</cell><cell cols="2">14,400 173.applu</cell><cell cols="2">267M 650M</cell><cell>43,300</cell></row><row><cell>253.perlbmk</cell><cell cols="2">5.0B 200M</cell><cell>26,500</cell><cell>179.art</cell><cell cols="2">2.2B 200M</cell><cell>136,500</cell></row><row><cell>256.bzip2</cell><cell>744M</cell><cell>1.0B</cell><cell>9,300</cell><cell>178.galgel</cell><cell cols="2">4.0B 200M</cell><cell>44,600</cell></row><row><cell>300.twolf</cell><cell cols="2">511M 200M</cell><cell>22,500</cell><cell cols="3">183.equake 4.459B 200M</cell><cell>41,100</cell></row><row><cell>Speech</cell><cell></cell><cell></cell><cell></cell><cell>NAS</cell><cell></cell><cell></cell></row><row><cell>sphinx</cell><cell cols="2">~ 6.0B 200M</cell><cell>54,200</cell><cell>cg</cell><cell cols="2">600M 200M</cell><cell>113,900</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>bt</cell><cell cols="2">800M 650M</cell><cell>34,500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sp</cell><cell cols="2">2.5B 200M</cell><cell>67,200</cell></row></table><note><p>To evaluate the effects of different cache organizations on system performance, we used Cacti to derive the access times for</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows the parameters and achieved instructions per cycle</cell></row><row><cell>1One FO4 is the delay of one inverter driving four copies of itself.</cell></row><row><cell>Delays measured in FO4 are independent of technology; we model</cell></row><row><cell>one FO4 as 360 picoseconds times the transistor's effective gate</cell></row><row><cell>length in microns [11].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Performance of UCA organizations</head><label>2</label><figDesc></figDesc><table><row><cell>Miss</cell></row><row><cell>Rate</cell></row><row><cell>0.23</cell></row><row><cell>0.20</cell></row><row><cell>0.17</cell></row><row><cell>0.13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>S-NUCA-1 evaluation</figDesc><table><row><cell>Technology</cell><cell>L2</cell><cell>Num.</cell><cell></cell><cell cols="2">Unloaded latency</cell><cell></cell><cell cols="2">Conservative</cell><cell cols="2">Aggressive</cell></row><row><cell>(nm)</cell><cell>size</cell><cell>banks</cell><cell cols="4">bank min max avg.</cell><cell cols="2">Loaded IPC</cell><cell cols="2">Loaded 1PC</cell></row><row><cell>130</cell><cell>2MB</cell><cell>16</cell><cell>3</cell><cell>7</cell><cell>13</cell><cell>10</cell><cell>11.3</cell><cell>0.54</cell><cell cols="2">10.0 : 0.55</cell></row><row><cell>100</cell><cell>4MB</cell><cell>32</cell><cell>3</cell><cell>9</cell><cell>21</cell><cell>15</cell><cell>17.3</cell><cell>0.56</cell><cell>15.3</cell><cell>0.57</cell></row><row><cell>70</cell><cell>8MB</cell><cell>32</cell><cell>5</cell><cell>12</cell><cell>26</cell><cell>19</cell><cell>21.9</cell><cell>0.61</cell><cell>19.3</cell><cell>0.63</cell></row><row><cell>50</cell><cell>16MB</cell><cell>32</cell><cell>8</cell><cell>17</cell><cell>41</cell><cell>29</cell><cell>34.2</cell><cell>0.59</cell><cell>30.2</cell><cell>0.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Tag arraySub-bank   </figDesc><table><row><cell></cell><cell></cell><cell>Wor dl l ne ddver r and decoder</cell></row><row><cell>~l</cell><cell>~T ~</cell><cell>Pr edecOder</cell></row><row><cell></cell><cell></cell><cell>Sense amplifier</cell></row><row><cell></cell><cell>Bank</cell><cell></cell></row><row><cell cols="2">Figure 3: Switched NUCA design</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>S-NUCA-2 performance</figDesc><table><row><cell></cell><cell>L2</cell><cell>Num.</cell><cell></cell><cell cols="2">Unloaded Latency</cell><cell></cell><cell>Loaded</cell><cell></cell><cell>Bank</cell></row><row><cell>(nm)</cell><cell>Size</cell><cell cols="5">Banks bank min max avg.</cell><cell>Latency</cell><cell cols="2">IPC Requests</cell></row><row><cell>130</cell><cell>2MB</cell><cell>16</cell><cell>3</cell><cell>4</cell><cell>11</cell><cell>8</cell><cell>9.7</cell><cell>0.55</cell><cell>17M</cell></row><row><cell>100</cell><cell>4MB</cell><cell>32</cell><cell>3</cell><cell>4</cell><cell>15</cell><cell>10</cell><cell>11.9</cell><cell>0.58</cell><cell>16M</cell></row><row><cell>70</cell><cell>8MB</cell><cell>32</cell><cell>5</cell><cell>6</cell><cell>29</cell><cell>18</cell><cell>20.6</cell><cell>0.62</cell><cell>15M</cell></row><row><cell>50</cell><cell>16MB</cell><cell>32</cell><cell>8</cell><cell>9</cell><cell>32</cell><cell>21</cell><cell>24.2</cell><cell>0.65</cell><cell>15M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : D-NUCA base performance</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Technology (nm)</cell><cell>L2 Size</cell><cell>Bank org. (rows x sets)</cell><cell cols="2">Unloaded Latency Bank min max avg.</cell><cell>Loaded avg.</cell><cell>Miss IPC Rate Accesses/Set Bank</cell></row><row><cell></cell><cell></cell><cell>130 100 70 50</cell><cell>2MB 4MB 8MB 16MB</cell><cell>4x4 8x4 16x8 16x16</cell><cell>3 3 3 3</cell><cell>4 4 4 31 11 I 8 15 10 18 3 [ 47 . 25</cell><cell>8.4 10.0 15.2 18.3</cell><cell>0.57 0.23 0.63 0.19 0.67 0.15 0.71 0.11</cell><cell>73M 72M 138M 266M</cell></row><row><cell></cell><cell>1.0-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8 ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6-</cell><cell></cell><cell></cell><cell cols="2">D perfect LRU</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>o D-NUCA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?.J</cell><cell>0.4-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>? ? ..ll .o _=</cell><cell></cell><cell cols="2">_a ,e Ill</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">0 1 2 3 4 5 6 7 8 9 101112131415</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Associative Way Number</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc><ref type="bibr" target="#b18">19</ref>.1 cycles) and nearly D-NUCA policy space evaluation</figDesc><table><row><cell>Policy Incremental</cell><cell>Lat. Search IPC 24.9 0.65 0.114 Rate</cell><cell>Access 89M</cell><cell cols="2">1-bank/2-hit</cell><cell>Policy</cell><cell cols="2">lat. Promotion [ [ I I IPC Rate 1 1 8 . 5 1 0 . 7 1 1 0 . 1 1 5 [ 25~M Acce:;s__JJ</cell></row><row><cell>2 mcast + 6 inc</cell><cell>23.8 0.65 0.113</cell><cell>96M</cell><cell cols="2">2-bank/l-hit</cell><cell></cell><cell cols="2">17.7 0.71 0.114</cell><cell>26~ M</cell></row><row><cell>2 inc + 6 mcast</cell><cell>20.1 0.70 0.114</cell><cell>127M</cell><cell cols="2">2-bank/2-hit</cell><cell></cell><cell cols="2">18.3 0.71 0.115</cell><cell>25~ !vl</cell></row><row><cell cols="2">2 mcast + 6 recast 19.1 0.71 0.113</cell><cell>134M</cell><cell></cell><cell></cell><cell></cell><cell>Eviction</cell></row><row><cell>Fastshared</cell><cell cols="7">Mapping 116.6 I 0.73 I 0.119 266M insert middle, evict random, lcopy 16.6 0.70 0.114 2611 insert head, evict random, 1 copy 15.5 0.70 0.117 261 lvl</cell></row><row><cell cols="4">[I Baseline: simple map, multicast, 1-bank/l-hit, insert at tail</cell><cell></cell><cell></cell><cell cols="2">18.3 0.71 0.114</cell><cell>266] 3</cell></row><row><cell></cell><cell>Configuration</cell><cell cols="3">Loaded Average</cell><cell>Miss</cell><cell>Bank</cell><cell>Tag</cell><cell>Search</cell></row><row><cell></cell><cell></cell><cell cols="2">Latency</cell><cell>IPC</cell><cell>Rate</cell><cell>Accesses</cell><cell>Bits</cell><cell>Array</cell></row><row><cell></cell><cell>Base D-NUCA</cell><cell></cell><cell>18.3</cell><cell>0.71</cell><cell>0.113</cell><cell>266M</cell><cell>--</cell></row><row><cell></cell><cell>SS-performance</cell><cell></cell><cell>18.3</cell><cell>0.76</cell><cell>0.113</cell><cell>253M</cell><cell>7</cell><cell>224KB</cell></row><row><cell></cell><cell>SS-energy</cell><cell></cell><cell>20.8</cell><cell>0.74</cell><cell>0.113</cell><cell>40M</cell><cell>7</cell><cell>224KB</cell></row><row><cell></cell><cell cols="2">SS-performance + shared bank</cell><cell>16.6</cell><cell>0.77</cell><cell>0.119</cell><cell>2661</cell><cell>6</cell><cell>216KB</cell></row><row><cell></cell><cell>SS-energy + shared bank</cell><cell></cell><cell>19.2</cell><cell>0.75</cell><cell>0.119</cell><cell>47M</cell><cell>6</cell><cell>216KB</cell></row><row><cell></cell><cell>Upper bound</cell><cell></cell><cell>3.0</cell><cell>0.83</cell><cell>0.114</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell cols="2">Upper bound + SS-performance</cell><cell>3.0</cell><cell>0.89</cell><cell>0.114</cell><cell></cell><cell>7</cell><cell>224KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 : Performance of D-NUCA with smart search all</head><label>7</label><figDesc>of the IPC, while still eliminating a great many of the extra bank accesses.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Performance of an L2/L3 Hierarchy</figDesc><table><row><cell>Techology</cell><cell>L2/L3</cell><cell cols="2">Num. Unloaded</cell><cell>Loaded</cell><cell cols="2">ML-UCA DN-best</cell></row><row><cell>(nm)</cell><cell>Size</cell><cell>Banks</cell><cell>Latency</cell><cell>Latency</cell><cell>IPC</cell><cell>IPC</cell></row><row><cell>130</cell><cell>512KB/2MB</cell><cell>4/16</cell><cell>6/13</cell><cell>7.1/13.2</cell><cell>0.55</cell><cell>0.58</cell></row><row><cell>100</cell><cell>512KB/4MB</cell><cell>4/32</cell><cell>7/21</cell><cell>8.0/21.1</cell><cell>0.57</cell><cell>0.63</cell></row><row><cell>70</cell><cell>1MB/8MB</cell><cell>8/32</cell><cell>9/26</cell><cell>9.9/26.1</cell><cell>0.64</cell><cell>0.70</cell></row><row><cell>50</cell><cell>1MB/16MB</cell><cell>8/32</cell><cell>10/41</cell><cell>10.9/41.3</cell><cell>0.64</cell><cell>0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 : Effect of technology models on results</head><label>9</label><figDesc>BE S T performance is reduced by a smaller 4%. Because of the data migration capability, which permits more numerous, smaller banks as the wires slow, D-NUCA organizations are more robust, when faced with slowing wires, than are S-NUCA-1 designs. Finally, we note that the smart search capabilities become more important for technologies with slower wires, as the increased number of banks would otherwise result in a corresponding increase in bank accesses.</figDesc><table><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0,6</cell><cell cols="2">UPPER D-NUCA ~ S-NUCA2</cell><cell></cell><cell>/ g/</cell><cell></cell><cell>0.8 -</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">-,-ML-UCA</cell><cell>/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0,4</cell><cell cols="2">.... S-NUCA1</cell><cell>/</cell><cell>/</cell><cell></cell><cell>0.6-</cell><cell></cell></row><row><cell>" 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell>-+. UCA ?</cell><cell cols="3">lid ........ -4.-... r</cell><cell>~</cell><cell>0.4-0.2-</cell><cell></cell></row><row><cell>0.C</cell><cell cols="2">2 MB 130nm</cell><cell>4 MB 90nm</cell><cell cols="2">8 MB 70nm</cell><cell>16MB 50rim</cell><cell>0.0</cell><cell>2MB 130nm</cell><cell>4MB 90nm</cell><cell cols="2">8MB 70nm</cell><cell>16MB 50nm</cell><cell>0,0</cell><cell>2MB 130rim</cell><cell>4MB 90rim</cell><cell>8MB 70rim</cell><cell>16 MB 50nm</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) 179.art</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) 181.mcf</cell><cell></cell><cell></cell><cell></cell><cell>(c) All Benchmarks</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Figure 7: Performance summary of major cache organizations</cell></row><row><cell>Tech.</cell><cell></cell><cell cols="2">Global wire</cell><cell></cell><cell cols="4">Conductor Insulator Dielectric</cell><cell>Num.</cell><cell></cell><cell></cell><cell>Configuration</cell><cell></cell><cell cols="2">Loaded Average Miss</cell><cell>Bank [</cell></row><row><cell cols="2">model SIA 1999</cell><cell cols="3">Aspect Ratio 2.8</cell><cell cols="2">Resistivity 1.8</cell><cell>Constant 1.5</cell><cell></cell><cell>banks 32</cell><cell></cell><cell></cell><cell>S-NUCA1</cell><cell></cell><cell>latency 21.9</cell><cell>IPC 0.68</cell><cell>rate 0.13</cell><cell>accesses I 15M I</cell></row><row><cell cols="2">SIA 2001</cell><cell></cell><cell>2.5</cell><cell></cell><cell></cell><cell>2.2</cell><cell>2.1</cell><cell></cell><cell>64 3526</cell><cell cols="4">Shared bank D-NUC,~ SS-energy + shared bank S-NUCA 1 Sharedbank D-NUCA SS-energy + shared bank</cell><cell>12.5 15.6 30.2 16.6 19.2</cell><cell>0.78 0.78 0.62 0.73 0.75</cell><cell>0.12 0.12 0.13 0.12 0.12</cell><cell>144M 36M 15M I I 266M I 47M</cell></row><row><cell cols="10">2001 technology projections replace those from 1999, whereas the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DN-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We gratefully acknowledge <rs type="person">Chuck Moore</rs> and <rs type="person">Seongmoo Heo</rs> for their helpful comments on the final version of this paper. We also thank <rs type="person">Mark Hill</rs> for his suggestion to use the partial tag matching scheme, which resulted in our highly effective smart search policy. This research is supported by the <rs type="funder">Defense Advanced Research Projects Agency</rs> under contract <rs type="grantNumber">F33615-01-C-1892</rs>, <rs type="funder">NSF</rs> CAREER grants <rs type="grantNumber">CCR-9985 i 09</rs> and <rs type="grantNumber">CCR-9984336</rs>, two IBM University Partnership awards, and a grant from the <rs type="funder">Intel Research Council</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7TdMKMA">
					<idno type="grant-number">F33615-01-C-1892</idno>
				</org>
				<org type="funding" xml:id="_kerQMgz">
					<idno type="grant-number">CCR-9985 i 09</idno>
				</org>
				<org type="funding" xml:id="_HYPDErt">
					<idno type="grant-number">CCR-9984336</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clock rate vs. IPC: The end of the road for conventional microprocessors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hrishikesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In_ Proceedings of the 27th Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selective cache ways: On-demand cache resource allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Symposium on Microarchitecture</title>
		<meeting>the 32nd International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1999-12">December 1999</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The NAS parallel benchmarks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
		<idno>RNR-91-002 Revision 2</idno>
		<imprint>
			<date type="published" when="1991-08">August 1991</date>
			<pubPlace>Mountain View, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>NASA Ames Research Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skewed associativity enhances performance predictability</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 22nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stenstr0m. On reconfigurable on-chip data caches</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Symposium on Microarchitecture</title>
		<meeting>the 24th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1991-11">November 1991</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Simalpha: A validated execution-driven alpha 21264 simulator</title>
		<author>
			<persName><forename type="first">R</forename><surname>Desikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
		<idno>TR-01-23</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Sciences, University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A data cache with multiple caching strategies tuned to different types of locality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzfilez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliagas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 International Conference on Supercomputing</title>
		<meeting>the 1995 International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1995-07">July 1995</date>
			<biblScope unit="page" from="338" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Alpha 21364 to ease memory bottleneck</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gwennap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fully associative software-managed cache design</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Symposium on Computer Architecture</title>
		<meeting>the 27th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A 900MHz 2.25 MB cache with on-chip CPU now in Cu SOl</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEElnternational Solid-State Circuits Conference</title>
		<meeting>the IEEElnternational Solid-State Circuits Conference</meeting>
		<imprint>
			<date type="published" when="2001-02">February 2001</date>
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The future of wires</title>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminconductor Research Corporation Workshop on Interconnects for Systems on a Chip</title>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The optimal logic depth per pipeline stage is 6 to 8 FO4 inverter delays</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hrishikesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkishore</forename><surname>Shivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Computer Architecture</title>
		<meeting>the 29th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring the design space of future CMPs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the lOth International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the lOth International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="page" from="199" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Signal delay in RC tree networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Penfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="211" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
	<note>CAD</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Run-time adaptive cache hierarchy management via reference analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Computer Architecture</title>
		<meeting>the 24th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="315" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An enhanced access and cycle time model for on-chip caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilton</surname></persName>
		</author>
		<idno>TR-93-5</idno>
	</analytic>
	<monogr>
		<title level="j">Compaq WRL</title>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Analysis of Multi-Megabyte Secondary CPU Cache Memories</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-12">December 1989</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The alpha 21264 microprocessor</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="1999-04">March/April 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparison of trace-sampling techniques for multi-megabyte caches</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="664" to="675" />
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inexpensive implementations of set-associativity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jooss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual International Symposium on Computer Architecture</title>
		<meeting>the 16th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1989-05">May 1989</date>
			<biblScope unit="page" from="131" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview of the SPHINX speech recognition system</title>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="44" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Will physical scalability sabotage performance gains?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matzke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="37" to="39" />
			<date type="published" when="1997-09">September 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An 833MHz 1.5w 18Mb CMOS SRAM with 1.67Gb/s/pin</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Covino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lamphier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Traver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 IEEE International Solid-State Circuits Conference</title>
		<meeting>the 2000 IEEE International Solid-State Circuits Conference</meeting>
		<imprint>
			<date type="published" when="2000-02">February 2000</date>
			<biblScope unit="page" from="266" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing set-associative cache energy via wayprediction and selective direct-mapping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Symposium on Microarchitecture</title>
		<meeting>the 34th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2001-12">December 2001</date>
			<biblScope unit="page" from="54" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Performance-Directed Memory Hierarchy Design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Przybylski</surname></persName>
		</author>
		<idno>CSL-TR-88-366</idno>
		<imprint>
			<date type="published" when="1988-09">September 1988</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The national technology roadmap for semiconductors. Semiconductor Industry Association</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cacti 3.0: An integrated cache timing, power and area model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
		</imprint>
		<respStmt>
			<orgName>Compaq Computer Corporation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cache operations by MRU change</title>
		<author>
			<persName><forename type="first">K</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Rechtshaffen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="700" to="109" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-performance data memory systems for superscalar processors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Symposium on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Fourth Symposium on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1991-04">April 1991</date>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Standard Performance Evaluation Corporation</title>
	</analytic>
	<monogr>
		<title level="j">SPEC Newsletter</title>
		<imprint>
			<date type="published" when="2000-09">September 2000</date>
			<pubPlace>Fairfax, VA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A modified approach to data cache management</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pleszkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Symposium on Microarchitecture</title>
		<meeting>the 28th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1995-12">December 1995</date>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Designing high bandwidth onchip caches</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual&apos; International Symposium on Computer Architecture</title>
		<meeting>the 24th Annual&apos; International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cacti: An enhanced cache access and cycle time model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="677" to="688" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
