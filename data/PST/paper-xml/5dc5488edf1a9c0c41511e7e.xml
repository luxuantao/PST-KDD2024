<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Impact of Cache Inclusion Policies on Cache Management Techniques</title>
				<funder>
					<orgName type="full">Texas A&amp;M High Performance Research Computing</orgName>
				</funder>
				<funder ref="#_rGtMcZ6">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luna</forename><surname>Backes</surname></persName>
							<email>luna.backes@tamu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
							<email>djimenez@acm.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>Barcelona Supercomputing Center</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>11 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Impact of Cache Inclusion Policies on Cache Management Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3357526.3357547</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Inclusion policies</term>
					<term>cache hierarchy</term>
					<term>cache management</term>
					<term>cache replacement policies</term>
					<term>hardware prefetching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Caches were designed to mitigate the large disparity between processor and memory speeds. Many last-level cache (LLC) management techniques have been designed to further improve performance. A first observation is that most techniques in the literature have been designed and evaluated on non-inclusive caches. A second observation is that many modern processors implement either inclusive or exclusive policies in their LLCs. Exclusive caches are becoming increasingly popular as the number of cores increases because of their larger effective capacity. It is expected therefore that future multi-core caches will be exclusive. These two observations unveil a mismatch between the assumptions made for evaluation in terms of cache inclusion policy, and the reality in nowadays and potentially future processor implementations. This paper addresses the question of how sensitive existing cache management techniques are to the inclusion policy, quantifies how effective they are for multiple inclusivity options, and proposes a concept LLC design including the features that best suit exclusive caches. The results show that state-of-the-art prefetchers are fundamental when evaluating replacement policies due to their tight interplay, and that inclusive caches require a less aggressive prefetching mechanism to prevent excessive back-invalidation. The results also pinpoint the features required for the replacement policy of an exclusive cache. Due to the lack of recency information and the impracticality of knowing the memory access that allocated the data, LLCs need to receive reuse information from lower cache levels along with the data block. Moreover, the proposed LLC features include keeping global reuse information structures detached from data blocks to prevent losing that information when the data is evicted on hit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Main memory technology has not kept pace with the rapid improvement of CMOS-based processors, giving rise to the so-called Memory Wall <ref type="bibr" target="#b37">[38]</ref>. Modern multi-core processors rely on large on-chip caches to mitigate the disparity in access latency to main memory. The last-level cache (LLC), i.e., the level closer to memory and further away from cores, is typically shared among all cores in the chip. When shared, it stores blocks from all cores and is typically sized at about 1-2 megabytes (MB) per core. Increasing the number of cores requires a larger cache to maintain high core performance.</p><p>Efficient LLCs should keep data that is being used and going to be used soon. Many cache management techniques have been investigated to bring the data before its use (prefetching) and to keep only the useful data (replacement policies).</p><p>There are several ways to manage how data is allocated in the multiple levels of cache depending on whether a lower level (closer to memory) includes data resident in higher levels (closer to cores).</p><p>In inclusive caches, a data block present in a cache must also be present in all of its corresponding lower levels. The result of this policy is a lower effective cache capacity due to the data replication across cache levels, and the potential performance and energy impact of inclusiveness-induced invalidations.</p><p>Non-inclusive caches attempt to reduce the limitations of inclusive accesses by not enforcing inclusivity in higher cache levels. When a data block is accessed, it is still allocated in all cache levels. However, an eviction on a cache does not trigger back invalidations to higher levels. There is still data replication, but there are not inclusiveness-induced invalidations affecting performance and energy.</p><p>Exclusive caches go one step further by enforcing that a data block present in a cache cannot be also present in the corresponding higher-level cache. Only data evicted from higher levels is present in the exclusive cache, effectively making the last-level cache into a victim cache <ref type="bibr" target="#b22">[23]</ref>. Thus, there is no data replication and there cannot be inclusiveness-induced invalidations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Industry</head><p>Modern processors use different types of inclusion policies in each cache level. The most common is to either use an inclusive or an exclusive policy in the LLC and an inclusive or non-inclusive in the lower levels. Below are a few examples of real processors with the information on the inclusion policy they use. None of which has a non-inclusive LLC.</p><p>AMD processors generally use exclusive LLCs and Intel processors, inclusive. The AMD Athlon (Thurderbird architecture) had an exclusive L2 (LLC), while its rival at the time, the Pentium 4 (from Willamette) <ref type="bibr" target="#b13">[14]</ref> had inclusive L2 (LLC). Currently, the latest AMD Zen architecture has a (mostly) exclusive L3. Current Intel processors like Sandy Bridge, Ivy Bridge and Skylake have an inclusive L3 and a non-inclusive L2 <ref type="bibr" target="#b14">[15]</ref>. The Intel Knights Landing has an L2 (LLC) that is inclusive of the L1D and non-inclusive of L1I <ref type="bibr" target="#b15">[16]</ref>. However, Intel recently announced the Skylake-X which has a mostly exclusive LLC (which they call non-inclusive) <ref type="bibr" target="#b0">[1]</ref>.</p><p>The ARM Cortex-A9 can have an (optional) L2 cache (LLC). The core has support to be attached to exclusive L2 caches as long as that is properly configured both in the core and L2 controller sides <ref type="bibr" target="#b1">[2]</ref>.</p><p>The processors in the IBM POWER series had mostly L3 (LLC) exclusive caches. The POWER5 has an L3 exclusive and an L2 inclusive of both L1D and L1I <ref type="bibr" target="#b10">[11]</ref>. The POWER6 has an L3 exclusive cache and the POWER7 has an L3 mostly exclusive cache <ref type="bibr" target="#b12">[13]</ref>. The IBM zEC12 has an inclusive L3 (on die) and an inclusive L4 (off-die, on-package) <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Motivation</head><p>Research in replacement policies (RPs) has been focused on noninclusive and inclusive caches. Table <ref type="table">1</ref> shows a list of relevant RPs and the evaluated inclusion policy. There has been a lower effort in exclusive caches.</p><p>Inclusive Non-Incl. Exclusive RRIP <ref type="bibr" target="#b20">[21]</ref> X SDBP <ref type="bibr" target="#b24">[25]</ref> X SHiP <ref type="bibr" target="#b35">[36]</ref> X GIPPR <ref type="bibr" target="#b21">[22]</ref> X MDPP <ref type="bibr" target="#b32">[33]</ref> X EAF <ref type="bibr" target="#b27">[28]</ref> X Perceptron <ref type="bibr" target="#b33">[34]</ref> X KPCR <ref type="bibr" target="#b25">[26]</ref> X Hawkeye <ref type="bibr" target="#b17">[18]</ref> X Bypass and Insertion <ref type="bibr" target="#b9">[10]</ref> X CHAR <ref type="bibr" target="#b4">[5]</ref> X X ExDRRIP <ref type="bibr" target="#b19">[20]</ref> X</p><p>Table <ref type="table">1</ref>: State-of-the-art RPs and their inclusion policy.</p><p>The number of cores in multicore chips has been increasing in recent years. A rule of thumb for the size of the LLC is 1-2MB per core. High performance cores typically include 256-512KB of private cache. With an inclusive cache, that could be a large amount of capacity wasted on maintaining inclusivity in the LLC. Exclusive caches are a promising option to increase the effective capacity without increasing chip area.</p><p>Most RPs in the literature have been designed and evaluated in inclusive and non-inclusive caches. However, that does not mean that they would also work well on exclusive caches since they have different properties. For example, in an exclusive LLC, a block is evicted on hit while many RPs are based on the reuse of a block.</p><p>Inclusive policies in RP design are often overlooked or not even mentioned. The motivation of this paper is to determine whether there is a need for designing different cache management techniques depending on the inclusion policy, with a focus on the exclusive caches. For this, we investigate how the existing RPs perform in different inclusion policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Contributions</head><p>The contributions of this paper are:</p><p>? A comprehensive evaluation of multiple cache configurations including multiple RPs and prefetchers (PFs) for all three inclusion policies: inclusive, non-inclusive and exclusive caches. ? A discussion on the results targeting to understand the gaps in the design of cache management to improve performance in the presence of a given inclusion policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>One solution to mitigate the processor and memory performance gap was introducing several levels of cache into the memory hierarchy to bring data closer to the processor. To make them more efficient, there has been extensive research on improving memory management, mostly PFs and RPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prefetchers</head><p>Prefetching is a technique used to hide memory latency by bringing data that will potentially be needed by the processor to a closer level of the memory hierarchy. There is, however, the risk of polluting the cache. The PF needs to be accurate (bring the data that is going to be requested) and timely (the prefetched data arrived to the cache made the accesses hit). We describe below the PFs used in our experiments.</p><p>Stride-based. Code and data structures (e.g. arrays, matrices), are stored sequentially in memory and accessed consecutively. These PFs exploit this spatial locality by prefetching: the next block (stride of 1) like the Next-line <ref type="bibr" target="#b30">[31]</ref>; the block plus an offset (stride of "offset" for timeliness) like the Instruction pointer-based stride <ref type="bibr" target="#b2">[3]</ref>; the block plus a dynamic offset (dynamic stride) like Best Offset <ref type="bibr" target="#b26">[27]</ref>.</p><p>DRAM-Aware Access Map Pattern Matching. Ishii et al <ref type="bibr" target="#b16">[17]</ref> proposed a PF to exploit locality in DRAM called DRAM-aware access map pattern matching (DAAMPM). Before the time the prefetch is going to be used, they suggest waiting and reordering prefetch requests to optimize row activation. The prefetches are reordered so that all blocks that need to access the same row are done together. They also claim that many RPs are unaware about which blocks were from a prefetch or by demand. They added a prefetch bit to avoid promoting a prefetch hit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kill the Program Counter.</head><p>There has been little work on studying the interaction between cache RPs and PFs, and their effect in each of the cache levels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>. Those studies show that the benefit of RPs can be small or negative when combined with a PF. Kim et al. proposed a holistic approach to speculatively manage all cache levels with coordinated PF and RP <ref type="bibr" target="#b25">[26]</ref> called kill the program counter (KPC). This approach aims to improve performance and reduce the overall hardware budget necessary for both PF and RP. The PF component (KPCP), is a PF that decides in which level of the hierarchy to prefetch each specific block. They use a signature table to store a compressed history of past L1 misses. The history is used as a signature to index a pattern table to predict the next block. The predicted block plus the previous history generates another signature which is again used. This technique has an initial training phase that sets a confidence value that is increased as PFs are useful. Later, PFs are only triggered if confidence on the prediction is high. The RP component (KPCR), is a low-overhead RP that uses two global hysteresis to predict dead blocks by tracking global reuse behavior. One hysteresis is for demands and, the other, for prefetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Replacement Policies</head><p>RPs improve the management of cache contents to evict first the blocks that are not likely to be used again. RPs are used at allocation time when a cache set is full: the algorithm chooses which block to evict from the cache to place the new one. They could also hurt performance if they mispredict.</p><p>B?l?dy proposed an optimal cache replacement algorithm assuming knowledge on the future <ref type="bibr" target="#b3">[4]</ref>. As a processor does not have such knowledge, there has been plenty of work in cache replacement algorithms. We explain below a few, focusing on the ones we experimented with.</p><p>Aging-based. Many RPs are based on the block's reuse, by aging the blocks over time and evicting the older ones <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Jaleel et al. proposed an exclusive version of RRIP where they keep a bit in L2 to know if the block came from LLC (previous hit) or from memory <ref type="bibr" target="#b19">[20]</ref>.</p><p>Signature-based. Other RPs make use of the program counter as a signature to predict dead blocks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Evicted Address Filter. Seshadri et al. proposed evicted address filter (EAF) claiming to address previous efforts deficiencies in preventing both cache pollution and thrashing at the same time <ref type="bibr" target="#b27">[28]</ref>.</p><p>Bypass and Insertion Policies for Exclusive caches. Gaur et al. proposed to use the number of trips to LLC (LLC hits) and their use count in the L2 (L2 hits) cache <ref type="bibr" target="#b9">[10]</ref>. Blocks with either a high use or trip count will usually be predicted alive. They later extended this work to also accommodate inclusive caches by using the same prediction to hint the LLC with a dead block prediction <ref type="bibr" target="#b4">[5]</ref>. When the use or trip count number is low the LLC will mark the block as the next victim. The same hint could be used to bypass in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inclusion Policies</head><p>A cache level is related to the previous/higher level (if it exists) depending on which data blocks each level contains. A particular cache level can contain exactly all, exactly none or some of the data blocks of the higher level. An inclusive cache contains all data blocks of the higher level. An exclusive cache does not contain any of the data blocks of the higher level. A non-inclusive cache can contain some blocks from the higher level but not necessarily all or none. Inclusion policy trades off ease of implementation of cache coherence with capacity. The choice of inclusion policy for a cache hierarchy is a design decision with many inputs beyond the scope of this work, however as we demonstrate this choice has a large impact on the effectiveness of cache management techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.1</head><p>Inclusive. An inclusive cache level contains all data blocks from higher levels plus some other blocks. That is, a data block is replicated in both cache levels.</p><p>In a 2-level cache hierarchy, the data block will be placed in both cache levels on an L2 miss. If the block is evicted from the L1 and, later, a request comes (L1 miss), the data may still be in the L2, thus avoiding accessing main memory. On an L1 eviction, only write backs of dirty blocks are required. If the block is clean, there is no need to copy it back to the L2 because it is already there as per the inclusion policy. A potential problem is on an L2 eviction: to preserve inclusivity, if the block was present in L1, it must be evicted too.</p><p>In a multi-core system, an inclusive policy simplifies the coherence protocol implementation. A cache wanting to invalidate copies of a block in other caches has to notify the LLC because it has the information of all blocks in all higher level caches. Therefore, it avoids coherence message broadcasts, thus reducing complexity and energy consumption. Also, coherence information (state and caches having a copy) can be encoded with the cache block so the information is available when accessing it, thus cutting latency of potentially having to access separate structures, such as a directory.</p><p>One disadvantage is the effective cache size due to data duplication. The effective size of the cache hierarchy is the size of the LLC. For example, in a 2-level cache hierarchy, the effective size is the one from L2 because it contains all contents from L1. The L1 cache only keeps data closer but does not contribute with additional capacity.</p><p>Another disadvantage is back invalidations. An eviction from the LLC can generate an invalidation in L1 but, if it was present in L1, the block may be in use. This can be a problem if the RP is not aware of the usage of a block in the higher caches. Jaleel et al. confirmed the limited performance of an inclusive cache comes from back invalidations because the LLC RP is unaware of the core presence and recency <ref type="bibr" target="#b18">[19]</ref>.</p><p>A related problem is that an inclusive cache has less flexibility to improve cache management due to the impossibility of bypassing the LLC to maintain inclusivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Non-Inclusive.</head><p>A non-inclusive cache level may or may not contain blocks from higher levels. The data is replicated when there is a miss in a cache level, and the block is allocated in that cache level and all higher ones. For example, in an L1 miss where the block is in none of the caches, the block will be allocated in L2 and L1. The difference with an inclusive cache is that the inclusivity is not enforced. That means, when a block is evicted from a lower level, it does not generate back invalidations to the higher levels. This simplifies the implementation of this type of caches.</p><p>The effective cache size is between the size of L2 and the sum of both. The cache hierarchy usage in a case with non-inclusive cache changes depending on the application and RP. Conflict misses will be reduced in the intermediate or LLC. The blocks that are referenced frequently stay in L1, therefore L2 has space for other blocks.</p><p>One disadvantage of non-inclusive caches is coherence. A noninclusive LLC that needs to evict a block will have to broadcast the invalidation to the higher level caches, because that information is not present in the LLC, unless a separate directory is implemented and then it must access the directory and pay its extra latency. However, there has been work to separate the cache coherence structures (i.e. directory) from the data blocks of the cache. Zhao et al. proposed a non-inclusive cache with an inclusive directory to keep the positive features of both inclusive and non-inclusive policies <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.3</head><p>Exclusive. An exclusive cache does not include any replicated block from higher levels, thus increasing the total amount of data blocks that can fit in the whole cache hierarchy.</p><p>The exclusive inclusion policy is similar to a victim cache <ref type="bibr" target="#b22">[23]</ref>. In a three-level cache hierarchy, the LLC would be the victim cache of a two-level cache hierarchy. Victim caches contain the evicted blocks from the higher levels aiming to reduce conflict misses. This was originally introduced as a fully associative small cache to reduce conflict misses from direct-mapped caches.</p><p>However, it incurs higher complexity. In the example of two cache levels (L1 and L2), when a block that is in L1 (and not in L2) is evicted, it will be allocated in L2. When the block is accessed again, it will be invalidated in L2 and allocated in L1. This generates more work to do on an L2 hit. Also, it makes impossible to use the recency of a block to choose which block to replace when the L2 cache is full, as it only contains data that was evicted from L1 and not accessed again since that eviction.</p><p>Jouppi and Wilton identified the benefits of exclusive caching and evaluated them <ref type="bibr" target="#b23">[24]</ref>. They found that the extra space of not duplicating the data in the two levels of cache and a higher associativity in the LLC was indeed beneficial.</p><p>Ten years later, Zheng et al. evaluated the performance of exclusive caches with respect to inclusive <ref type="bibr" target="#b39">[40]</ref>. They found that exclusive caching is beneficial for most of the benchmarks they tried <ref type="bibr">(SPEC 2000)</ref>, but especially for smaller lower-level caches. They suggest that exclusive caches are more suitable for server applications and embedded systems.</p><p>The main benefits when using exclusive caches are:</p><p>? Less conflict misses like with a higher associativity: two memory references that are mapped to the same set can reside one in each level instead of only one. ? Higher hit rate thanks to a higher effective space by avoiding the blocks duplication in different levels. This is especially relevant in caches with more than 3 levels of cache or with large higher level caches. ? Avoids premature evictions from the higher levels by not requiring back-invalidations (compared to inclusive).</p><p>The main drawbacks and limitations of exclusive caches are:</p><p>? Less design flexibility because the block size of the exclusive cache has to be equal as the other levels. ? More control complexity and power consumption due to the higher data movement of blocks.</p><p>? More complex cache coherence protocols and more area required in symmetric multiprocessing (SMP).</p><p>One fundamental difference of exclusive caches is that clean lines are moved to the LLC when evicted from the L2. In inclusive and non-inclusive caches, only dirty lines are moved. This significantly increases the traffic generated by L2 evictions and affects data reusability in the LLC. In non-inclusive, the LLC may have already evicted a line before, when it is evicted from the L2. If the line is clean, the L2 would just invalidate/replace the line with no effects in the LLC. In the exclusive cache, conversely, the L2 copies the clean evicted data to the LLC, which allocates it, potentially replacing other data which may be useful in the near future.</p><p>Some exclusive cache designs allocate block in an inclusive way, i.e., allocate in all levels, for specific types of data, for example for data shared between multiple threads. In this case the reasoning is that if other threads may access the data, having it allocated in the last-level cache avoid the three-way trip to read it from a remote private cache. For this type of exclusive behavior with exception, we use the term non-exclusive. This types of caches are sometimes referred as non-inclusive. In this paper, non-inclusive cache only refers to the definition provided in Section 2.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Benchmarks</head><p>We used all the traces from the SPECspeed CPU2017 benchmark suite <ref type="bibr" target="#b31">[32]</ref>, single-threaded and both integer and floating point. We also used three single threaded server workload traces from Cloud-Suite <ref type="bibr" target="#b8">[9]</ref> (data_caching, sat_solver and graph_analytics using the default inputs specified in the paper), and a trace from a machine learning workload "mlpack_cf" <ref type="bibr" target="#b6">[7]</ref>. In total, 24 benchmarks.</p><p>Multi-core simulations execute a single benchmark per core. We used 20 mixes of SPECspeed CPU2017 benchmarks, a memory intensive selection from many runs, excluding mixes that were too similar.</p><p>All traces warm-up until all finish 200 million instructions. For the timing modeling phase, all cores run until all have run at least one billion instructions, also known as last <ref type="bibr" target="#b34">[35]</ref>. Some cores will run more than one billion instructions but only the first billion will count towards the statistics. This methodology is used so that all cores are running at the same time during all the execution. This is important to model the effects of a shared LLC in a more realistic environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simulator</head><p>We used the ChampSim simulator, which is an extended version of the simulator used in both 2 nd Data and Cache Replacement Championships <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. This simulator models a simple multi-core out-of-order. The configuration we used is described in Table <ref type="table" target="#tab_0">2</ref>. The baseline consists of a 3-level cache hierarchy with all levels following a non-inclusive policy. All caches use copy back with write allocate write policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Configurations</head><p>We evaluated multiple combinations of PFs, RPs and inclusion policies for both single-threaded and multiprogrammed workloads. In these experiments, we vary the inclusion policy of the LLC and keep the private L2 cache non-inclusive of the L1 cache, except in the case of the inclusive LLC, in which both the L2 and the L3 are inclusive.</p><p>There are six evaluated L2 PFs: no PF, ip stride, next line, BOP, DAAMPM and KPCP. There are two evaluated L1 PFs: no PF and next line.</p><p>There are five evaluated RPs: LRU, EAF, KPCR, SHiP and DRRIP. These PFs, replacement and inclusion policies are explained in detail in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Performance Measurement</head><p>The baseline configuration used as reference in our evaluation is: non-inclusive cache inclusion policy, next-line L1 PF, no L2 PF, and the LRU RP.</p><p>For the single-core, we show the speedup of a configuration over the baseline running a particular benchmark by extracting the instructions per cycle (IPC).</p><p>For the multi-core simulations, we show the weighted speedup normalized to the baseline configuration. We compute the IPC of each of the cores in the multi-core execution and divide it by their single-core IPC (same cache size as in multi-core). We sum all the IPC to get a weighted speedup and then we normalize it to the baseline configuration. Equation <ref type="formula" target="#formula_0">1</ref>shows the speedup calculation used.</p><formula xml:id="formula_0">Speedup i = I PC i I PC i s in?l e -cor e IPC basel ine<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>In this section we present our results. We analyze single-and multicore experiments of all cache configurations previously described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single-Core Results</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the geometric mean speedup of multiple cache configurations across all SPECspeed CPU2017 benchmarks. The baseline in this figure is: next_line L1 PF, no L2 PF, LRU RP and noninclusive. Similarly, Figure <ref type="figure" target="#fig_1">2</ref> shows the same results across three CloudSuite benchmarks, and Figure <ref type="figure" target="#fig_2">3</ref> with MLpack benchmark (machine learning).</p><p>The inclusion policy clearly affects the impact of the multiple PFs and RPs. For example, LRU is the least effective RP in combination with any of the evaluated PFs when the cache is inclusive or noninclusive. In contrast, LRU is the best RP when the cache is exclusive regardless of the PF.</p><p>Across the board, exclusive caches are inferior to inclusive and non-inclusive except for the case of LRU. When LRU is the RP of choice, exclusive caches are as performant or even marginally better than inclusive and non-inclusive. The exclusive cache results show a high ratio of copy-backs and write-backs in the LLC and an increase of the number of LLC evicts. These evictions may not be effective, given that they are not based on data reuse information but rather on the time when they were evicted from the L2.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows a larger difference between exclusive and inclusive/noninclusive, where the exclusive performs better than the baseline compared to the other benchmarks but very low compared to the other inclusion policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Prefetcher Impact.</head><p>The next_line PF and no PF configurations show the same performance for the same RP. This is because the L1 has a next_line PF in these experiments which already introduce the same kind of accesses into the L2 that, on miss, generate the same pattern a next_line L2 PF would.</p><p>KPCP is the PF with the largest difference in performance for the multiple inclusion policies. For inclusive and non-inclusive caches, it achieves similar speedups to the other PFs. However, those same replacement policies in combination with KPCP actually worsen performance for exclusive caches compared to the baseline.  DAAMPM is the best PF among all configurations for SPEC. For the exclusive cache, it is the one that achieves the best results regardless of the RP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Replacement Policy Impact.</head><p>There is no clear winner among RPs, but there are a few patterns that indicate that the LRU RP is among the best options to use in general for an exclusive cache independently of the PFs on a single core. That is not the case in the machine learning benchmark where LRU performs like the other RPs.</p><p>SHiP, similarly to others, does not work well for exclusive caches. SHiP uses a predictor on placement and updates it using the program counter (PC). PC-based policies such as SHiP rely on the correlation between the PC of a memory access instruction and the reuse behavior of the block accessed by that instruction. However, only evicted blocks are placed into an exclusive LLC. The PC of the memory access instruction triggers a block eviction has very little correlation with the reuse of that block. Thus, PC-based policies only work well with inclusive and non-inclusive caches.</p><p>The KPCR RP makes the inclusive cache perform worse than non-inclusive. Other RPs have a closer behavior for non-/inclusive policies.</p><p>Furthermore, exclusive caches do not show much sensitivity to the RP. In contrast, inclusive and non-inclusive caches are more variable. The speedup percentage differences are within 5% because several benchmarks are insensitive to cache behavior while others see a larger impact. The benchmarks that show more sensitivity are: gcc, lbm, mcf, cactuBSSN, fotonik3d, satSolver and MLpack. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Core Results</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the weighted speedup of all cache configurations in multi-core. The baseline in the figure is: next_line L1 PF, no L2 PF, LRU RP and all non-inclusive caches. This shows the speedup of the multi-core experiments compared to the single-threaded case. Similarly, Figure <ref type="figure" target="#fig_4">5</ref> shows the same results for CloudSuite (three mixes), and Figure <ref type="figure" target="#fig_5">6</ref> for MLpack (four instances).</p><p>In multi-core, the performance difference in an exclusive cache is more variable across the RPs. LRU does not work as efficiently in multi-thread as it did in single-threaded runs; it is actually the worst one, as expected.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the only one case where the exclusive is as good or better than the other inclusion policies, even though it is a small difference. And, although LRU is not the worse RP, it is still better than in non-/inclusive.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows that the difference between exclusive and inclusive/noninclusive is smaller than in one core. Overall, in all multi-core plots the exclusive cache is more competitive than for single-core. This is encouraging since one of the reasons to use exclusive caches is the increase of the number of cores per chip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Prefetcher Impact.</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows that ip_stride performs generally better across inclusion policies.</p><p>The PF seems to be determining for exclusive and non-inclusive LLCs, as different RPs using the same PF do not show large variations. The inclusive LLC, however, is more sensitive to the RP. This makes sense because replacement in an inclusive LLC may cause invalidations of useful data in upper-level caches and, therefore, getting replacements right is as important or more than precise prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Replacement Policy Impact.</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows that the best RPs for exclusive caches are EAF, SHiP and KPCR. These RPs are different from the others in this study because they store information about accesses to a block even after having evicted the block from the LLC. In an exclusive cache, a block will be evicted on hit and any bookeeping information stored with the block is lost. These policies, however, maintain reuse information in a separate hardware structure which is kept even on a block miss or eviction. For example, the EAF RP stores the evicted memory addresses and, on that address miss, it inserts at the MRU position to protect it. SHiP has a table of counters indexed by a partial tag. The counters are updated on hit and eviction. Both things happen on the cache either the block is removed or not in the LLC. Even though the program counter has no correlation with the block, SHiP is still able to get the general trend on dead blocks in multi-core. As demonstrated with KPCR, even a global up/down counter can be a good first-cut dead-block predictor. SHiP can be seen as roughly tracking the tendency of blocks to be dead within a given program phase despite the lack of correlation between specific PCs and block accesses. Similarly, KPCR also updates a global counter on hit and miss.</p><p>These techniques employ these separate hardware structures to reduce the area and power impact of the RP. Collaterally, these mechanisms help exclusive caches because blocks are invalidated on hit and therefore lose reuse information that is stored with the block. By keeping these separate hardware structures, RPs in an exclusive cache can prioritize blocks that have been evicted or accessed recently even if those blocks are not in the cache.</p><p>SHiP and KPCR are the best RPs for inclusive caches. Some benchmarks in the multiprogram mixes are cache resident in private caches. With an inclusive cache, the data of those threads will eventually be evicted given that their blocks are not promoted in the LLC by hits in the private caches. If that thread's data is inserted in the LLC with low priority it will be evicted faster. This happens with DRRIP, which inserts with a maximum age of two. SHiP and KPCR use a similar strategy but outperform DRRIP across inclusive configurations. This is because their improvements may mitigate the described effects for inclusive caches. SHiP can insert into a lower age position (higher priority) when it predicts the block is live and help keep the private-cache-resident data in the LLC for  longer. An aggressive PF of another thread can also interfere with those cache resident threads by allocating prefetched blocks in the inclusive LLC and evict useful data. KPCR keeps a separate hysteresis counter for prefetches and demand misses. If prefetches are identified as dead, they would be inserted with an age of three (the lowest priority value), therefore not displacing useful data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">L1 Prefetching</head><p>We ran the same configurations as in previous sections but without the L1 PF. Contrary to expectations, most configurations without L1 PF perform better. Figure <ref type="figure" target="#fig_6">7</ref> shows that the speedups without L1 PF are generally higher than those with a next_line L1 PF. We observed the same behavior on the other benchmarks and in multi-core.</p><p>The only configuration that performs worse without L1 PF is when there is no L2 PF either. In all other cases it appears that L1 PFs interfere with the L2 prefetches and worsen performance. We generated the same results for multi-core without L1 PF and it shows the same performance improvement over next_line L1 PF. There is a need to design L1 and L2 PFs so they cooperate instead of interfere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Studies on Inclusion Impact</head><p>We looked at the statistics of the most sensitive benchmarks to inclusion and cache management techniques. We observed that some benchmarks such as satSolver and cactuBSSN did not work well for inclusive caches for both SHiP and KPCR. satSolver shows a slowdown of 0.92 compared to either exclusive and non-inclusive, and cactuBSSN a 0.77 slowdown.</p><p>In satSolver, the L2 evictions due to the inclusivity for KPCR are between 34 and 27 million (M) for most PF combinations, while for  the rest of the RPs it is under 1M. In SHiP, the difference is also clear but smaller than KPCR being between 24-20M versus 1M. The L1 evictions due to inclusivity show the same trend, with KPCR and SHiP clearly higher than the rest. Our hypothesis is that these RPs are so effective that they predict a block dead in LLC so fast that the higher level caches might still be using those blocks. For some cases this works well when the block is indeed dead.</p><p>In cactuBSSN, the L2 evictions due to the inclusivity for SHiP are between 12M and 10M while others are below 1M. In this case, KPCR is around 2.5-1.5M, a smaller difference than SHiP. In this case, the inclusion policy degrades performance (0.7 slowdown). The ones that show more difference in evictions due to inclusivity and performance are the ones with a combination of PFs of: no PF and next_line (e.g. no L1 PF + next_line L2 PF, next_line or none in both, or another order). Our hypothesis is that a not so smart (or lack of) PF is generating back invalidations plus interfering with the SHiP replacement policy, which is not core aware and can generate more back invalidations).</p><p>Cache inclusion techniques should be designed taking into account the inclusion policy. This section indicates that the PF should also be designed accordingly not to interfere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Summary</head><p>The single-core simulations showed BOP and DAAMPM PFs worked generally better for any inclusion policy. The best RPs for inclusive and non-inclusive are EAF and SHiP, in contrast to exclusive with LRU.</p><p>The multi-core simulations showed, for non-inclusive, that the best PF depends on the cache configuration, but in general, ip_stride works well. The RP that works better in all cases is SHiP. That is very clear in non-inclusive caches. However, in inclusive caches KPCR also works well, and in exclusive both SHiP and KPCR work well in addition to EAF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Cache management techniques have been mostly designed and evaluated in the context of non-inclusive LLCs. However, many modern processors implement their LLC with either an inclusive or exclusive policy. In this paper we explored the design space of cache management techniques in different cache configurations, with a focus on the cache inclusion policy. We implemented an inclusive and an exclusive policy on top of a simulator that had a non-inclusive policy. We evaluated different prefetchers (PFs), replacement policies (RPs) and number of cores for each inclusion policy.</p><p>The results demonstrate that a cache management technique that performs well in one inclusion policy does not necessarily work well for another inclusion policy. Different inclusion policies have a different behavior which affects how different cache management techniques impact performance. Inclusive caches are more sensitive to the RP in use while non-inclusive and exclusive caches are largely influenced by PF choice. This makes sense as replacements may trigger invalidations of potentially-useful data.</p><p>RPs that keep global reuse information in separate structures perform better for exclusive caches. Exclusive caches invalidate cache blocks on hit which renders reuse information contained along cache blocks irrelevant. The use of the program counter is also useless because there is no correlation between the program counter and the block that is placed in the LLC on L2 evictions.</p><p>Exclusive caches have a larger capacity than inclusive and noninclusive. However, our results show that exclusive caches perform worse in most cases. This shows a need for PFs and RPs tailored for exclusive caches to unleash its performance potential. The multicore results are encouraging since they highlight the higher effective capacity.</p><p>Based on all this data, we propose some features that an exclusive LLC design should include. First, to overcome the lack of recency information, forward such information together with the data block on eviction, such as ExRRIP and Bypass and Insertion RPs. Second, to solve the information loss on a hit when the information lives with the block, an approach like EAF helps by keeping a separate structure that tracks the evicted addresses, so that information outlives the block and helps prioritizing blocks on future replacements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Geomean speedups of SPECspeed CPU2017 benchmarks to compare different configurations of L2 PFs, RPs and cache inclusions. The configurations compared are: L2 PF, RP and cache inclusion, always with a next_line L1 PF. The Y-axis shows the speedup over the baseline configuration: no PFs, LRU RP and a non-inclusive cache. The X-axis shows the different cache configurations, in order of: L2 PF (l2p) and RP (repl).</figDesc><graphic url="image-1.png" coords="6,136.50,91.14,339.00,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Geomean speedups of CloudSuite benchmarks to compare different configurations of L2 PFs, RPs and cache inclusions. The configurations compared are: L2 PF, RP and cache inclusion, always with a next_line L1 PF. The Y-axis shows the speedup over the baseline configuration: no PFs, LRU RP and a non-inclusive cache. The X-axis shows the different cache configurations, in order of: L2 PF (l2p) and RP (repl).</figDesc><graphic url="image-2.png" coords="6,143.76,320.89,324.48,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Geomean speedups of the Machine Learning "mlpack" benchmark to compare different configurations of L2 PFs, RPs and cache inclusions. The configurations compared are: L2 PF, RP and cache inclusion, always with a next_line L1 PF. The Y-axis shows the speedup over the baseline configuration: no PFs, LRU RP and a non-inclusive cache. The X-axis shows the different cache configurations, in order of: L2 PF (l2p) and RP (repl).</figDesc><graphic url="image-3.png" coords="7,143.76,91.14,324.48,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Weighted speedups of 4-core multiprogrammed SPECspeed CPU2017 benchmark mixes to compare different configurations of PFs, RPs and cache inclusions. The configurations compared are: L2 PF, RP and cache inclusion. The Y-axis shows the speedup over the baseline configuration: no PFs, LRU RP and a non-inclusive cache. The X-axis shows the different cache configurations, in order of: L2 PF (l2p) and RP (repl).</figDesc><graphic url="image-5.png" coords="8,145.68,316.09,320.64,154.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Geomean speedups of CloudSuite benchmarks to compare different configurations of L2 PFs, RPs and cache inclusions. The configurations compared are: L2 PF, RP and cache inclusion, always with a next_line L1 PF. The Y-axis shows the speedup over the baseline configuration: no PFs, LRU RP and a non-inclusive cache. The X-axis shows the different cache configurations, in order of: L2 PF (l2p) and RP (repl).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Geomean speedups of the Machine Learning "mlpack" benchmark to compare different configurations of L2 PFs, RPs and cache inclusions. The configurations compared are: L2 PF, RP and cache inclusion, always with a next_line L1 PF. The Y-axis shows the speedup over the baseline configuration: no PFs, LRU RP and a non-inclusive cache. The X-axis shows the different cache configurations, in order of: L2 PF (l2p) and RP (repl).</figDesc><graphic url="image-6.png" coords="9,145.68,91.14,320.64,154.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Geomean speedups to compare different configurations of PFs, RPs and cache inclusions including with and without L1 PF. The configurations compared are: L1 PF, L2 PF, RP and cache inclusion. The Y-axis shows the speedup over the baseline configuration: no PFs, LRU RP and a non-inclusive cache. The X-axis shows the different cache configurations, in order of: RP (repl), L2 PF (l2p) and L1 PF (l1p).</figDesc><graphic url="image-7.png" coords="9,53.80,313.80,509.71,198.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Simulator configuration.</figDesc><table><row><cell>Parameter</cell><cell>Configuration</cell></row><row><cell>L1 I-cache</cell><cell>64KB, 64B blocks, 8-way,</cell></row><row><cell>(private)</cell><cell>8 MSHRs, 1 cycle latency,</cell></row><row><cell></cell><cell>64 read/write/prefetch queue size</cell></row><row><cell>L1 D-cache</cell><cell>64KB, 64B blocks, 8-way,</cell></row><row><cell>(private)</cell><cell>8 MSHRs, 4 cycles latency,</cell></row><row><cell></cell><cell>64 read/write/prefetch queue size</cell></row><row><cell>L2 unified cache</cell><cell>512KB, 64B blocks, 8-way</cell></row><row><cell>(private)</cell><cell>16 MSHRs, 8 cycles latency,</cell></row><row><cell></cell><cell>32 read/write/prefetch queue size</cell></row><row><cell></cell><cell>non-inclusive</cell></row><row><cell>L3 unified cache</cell><cell>2MB/core, 64B blocks, 16-way</cell></row><row><cell>(shared)</cell><cell>32 MSHRs, 20 cycles latency,</cell></row><row><cell></cell><cell>16/core read/write/prefetch queue size</cell></row><row><cell></cell><cell>non-inclusive</cell></row><row><cell>I-TLB</cell><cell>64 entries, 4-way</cell></row><row><cell>(private)</cell><cell>8 MSHR, 1 cycle latency</cell></row><row><cell>D-TLB</cell><cell>64 entries, 4-way</cell></row><row><cell>(private)</cell><cell>8 MSHR, 1 cycle latency</cell></row><row><cell>L2 TLB</cell><cell>1536 entries, 12-way</cell></row><row><cell>(shared)</cell><cell>16 MSHR, 8 cycle latency</cell></row><row><cell>Frequency</cell><cell>4GHz</cell></row><row><cell>Page size</cell><cell>4KB</cell></row><row><cell>Fetch, decode, retire</cell><cell>4 wide</cell></row><row><cell>Execution</cell><cell>6 wide</cell></row><row><cell>Load Queue</cell><cell>2 wide</cell></row><row><cell>Store Queue</cell><cell>1 wide</cell></row><row><cell>DRAM</cell><cell>2 channels (1 DIMM per channel),</cell></row><row><cell></cell><cell>8 banks (64MB per bank),</cell></row><row><cell></cell><cell>8 ranks (512MB per rank),</cell></row><row><cell></cell><cell>4GB per DIMM</cell></row><row><cell>DRAM channel width</cell><cell>8</cell></row><row><cell>DRAM I/O frequency</cell><cell>800MHz</cell></row><row><cell>Branch Predictor</cell><cell>Perceptron</cell></row><row><cell>Reorder Buffer size</cell><cell>256</cell></row><row><cell>Pipeline depth</cell><cell>5</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="institution">Samsung</rs> for supporting this work through their <rs type="programName">Global Outreach Program</rs>. We also thank the <rs type="funder">National Science Foundation</rs> for supporting this work through grant <rs type="grantNumber">CCF-1649242</rs>. Portions of this research were conducted with the advanced computing resources provided by <rs type="funder">Texas A&amp;M High Performance Research Computing</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rGtMcZ6">
					<idno type="grant-number">CCF-1649242</idno>
					<orgName type="program" subtype="full">Global Outreach Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Anandtech</surname></persName>
		</author>
		<ptr target="https://www.anandtech.com/show/11464/intel-announces-skylakex-\bringing-18core-hcc-silicon-to-consumers-for-1999/3" />
		<title level="m">Skylake-X&apos;s New L3 Cache Architecture</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2018" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<idno>Ac- cessed: 2017-04-30</idno>
		<ptr target="http://infocenter.arm.com/help/topic/com.arm.doc.ddi" />
		<title level="m">Cortex-A9 Technical Reference Manual</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note>0388i/DDI0388I_cortex_a9_r4p1_trm</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Effective On-chip Preloading Scheme to Reduce Data Access Penalty</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1991 ACM/IEEE Conference on Supercomputing (SC&apos;91)</title>
		<meeting>the 1991 ACM/IEEE Conference on Supercomputing (SC&apos;91)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Study of Replacement Algorithms for a Virtual-storage Computer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Laszlo</surname></persName>
		</author>
		<author>
			<persName><surname>B?l?dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="78" to="101" />
			<date type="published" when="1966">1966. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing Hierarchy-awareness in Replacement and Bypass Algorithms for Last-level Caches</title>
		<author>
			<persName><forename type="first">Mainak</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithiyanandan</forename><surname>Bashyam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;12)</title>
		<meeting>the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;12)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="293" to="304" />
		</imprint>
	</monogr>
	<note>Sreenivas Subramoney, and Joseph Nuzman</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The 2nd Cache Replacement Championship</title>
		<author>
			<persName><surname>Crc</surname></persName>
		</author>
		<ptr target="http://crc2.ece.tamu.edu" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MLPACK: A Scalable C++ Machine Learning Library</title>
		<author>
			<persName><surname>Ryan R Curtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">P</forename><surname>James R Cline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Slagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parikshit</forename><surname>March</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><forename type="middle">A</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="801" to="805" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="http://comparch-conf.gatech.edu/dpc2" />
		<title level="m">DPC 2015. The 2nd Data Prefetching Championship</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clearing the Clouds: a Study of Emerging Scale-out Workloads on Modern Hardware</title>
		<author>
			<persName><forename type="first">Almutaz</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cansu</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">Daniel</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bypass and Insertion Algorithms for Exclusive Last-level Caches</title>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mainak</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<ptr target="https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/Power%20Systems/page/POWER5%20Architecture" />
		<title level="m">IBM POWER5 Architecture</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<ptr target="https://www.hotchips.org/wp-content/uploads/hc_archives/hc25/HC25.20-Processors1-epub/HC25.26.220-zEC12-Processor-Sonnelitter-IBM-v5.pdf" />
		<title level="m">IBM zEC12</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<ptr target="https://www.cs.rice.edu/~johnmc/comp522/lecture-notes/COMP522-2016-Lecture7-Power7.pdf" />
		<title level="m">IBM POWER7 Architecture</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intel-Pentium-4-Processor-1_70-GHz-256K-Cache-400-MHz-FSB</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<idno>Ac- cessed: 2017-04-30</idno>
		<ptr target="http://ark.intel.com/products/27426/" />
	</analytic>
	<monogr>
		<title level="m">Intel Pentium 4 Willamette</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 Architectures Optimization Manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2017" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<idno>Ac- cessed: 2017-04-30</idno>
		<ptr target="http://pages.cs.wisc.edu/~david/courses/cs758/Fall2016/handouts/restricted/Knights-landing.pdf" />
		<title level="m">Knights Landing Architecture</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unified Memory Optimizing Architecture: Memory Subsystem Control with a Unified Predictor</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Supercomputing (ICS&apos;12)</title>
		<meeting>the 26th ACM International Conference on Supercomputing (ICS&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Back to the future: leveraging Belady&apos;s algorithm for improved cache replacement</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA&apos;16</title>
		<meeting>the ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="78" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Achieving non-inclusive cache performance with inclusive caches: Temporal locality aware (tla) cache management policies</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Borch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malini</forename><surname>Bhandaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 43rd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO-43</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High Performing Cache Hierarchies for Server Workloads: Relaxing Inclusion to Capture the Latency Benefits of Exclusive caches</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Nuzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Moga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2015 IEEE 21st International Symposium on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="343" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High Performance Cache Replacement Using Re-reference Interval Prediction (RRIP)</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture (ISCA &apos;10</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture (ISCA &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Insertion and Promotion for Tree-based PseudoLRU Last-Level Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="284" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving Direct-mapped Cache Performance by the Addition of a Small Fully-associative Cache and Prefetch Buffers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA&apos;90</title>
		<meeting>the 17th Annual International Symposium on Computer Architecture (ISCA&apos;90</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tradeoffs in two-level on-chip caching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven Je</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><surname>Wilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the 21st Annual International Symposium on Computer Architecture (ISCA&apos;94</title>
		<meeting>the 21st Annual International Symposium on Computer Architecture (ISCA&apos;94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dead Block Replacement and Bypass with a Sampling Predictor</title>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Samira M Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 43rd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kill the Program Counter: Reconstructing Program Behavior in the Processor Cache Hierarchy</title>
		<author>
			<persName><forename type="first">Jinchun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XXII)</title>
		<meeting>the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS XXII)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="737" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Best-offset Hardware Prefetching</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd IEEE International Symposium on High Performance Computer Architecture (HPCA-22</title>
		<meeting>the 22nd IEEE International Symposium on High Performance Computer Architecture (HPCA-22</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The evicted-address filter: A unified mechanism to address both cache pollution and thrashing</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;12)</title>
		<meeting>the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mitigating prefetcher-caused pollution using informed caching policies for prefetched blocks</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samihan</forename><surname>Yedkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EELRU: Simple and Effective Adaptive Page Replacement</title>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Smaragdakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="122" to="133" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequential Program Prefetching in Memory Hierarchies</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978">1978. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">ISPEC CPU2017 benchmark suite</title>
		<ptr target="https://www.spec.org/cpu2017" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2017" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minimal Disturbance Placement and Promotion</title>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd IEEE International Symposium on High Performance Computer Architecture</title>
		<meeting>the 22nd IEEE International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="201" to="211" />
		</imprint>
	</monogr>
	<note>HPCA-22</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptron Learning for Reuse Prediction</title>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 49th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fame: Fairly measuring multithreaded architectures</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Pajuelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliverio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques (PACT&apos;07)</title>
		<meeting>the 16th International Conference on Parallel Architecture and Compilation Techniques (PACT&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SHiP: Signature-based Hit Predictor for High Performance Caching</title>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO-44</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="430" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PACMan: prefetch-aware cache management for high performance caching</title>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>MICRO-44</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="442" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hitting the Memory Wall: Implications of the Obvious</title>
		<author>
			<persName><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NCID: a non-inclusive cache, inclusive directory architecture for flexible and efficient cache hierarchies</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srihari</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Computing Frontiers (CF&apos;10</title>
		<meeting>the 7th ACM international conference on Computing Frontiers (CF&apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance Evaluation of Exclusive Cache Hierarchies</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<meeting>the 2004 IEEE International Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
	<note>ISPASS&apos;04</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
