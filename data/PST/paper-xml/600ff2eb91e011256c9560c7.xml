<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analytical Characterization and Design Space Exploration for Optimization of CNNs</title>
				<funder>
					<orgName type="full">U.S. National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
							<email>lirui@cs.utah.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yufan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aravind</forename><surname>Sukumaran-Rajam</surname></persName>
							<email>a.sukumaranrajam@wsu.edu</email>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Utah Salt Lake City</orgName>
								<address>
									<settlement>Utah</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Utah Salt Lake City</orgName>
								<address>
									<settlement>Utah</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Washington State University Pullman</orgName>
								<address>
									<settlement>Washington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Ohio State University Columbus</orgName>
								<address>
									<region>Ohio</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Utah Salt Lake City</orgName>
								<address>
									<settlement>Utah</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analytical Characterization and Design Space Exploration for Optimization of CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural networks</term>
					<term>Design space exploration</term>
					<term>Tile size optimization</term>
					<term>Performance modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Moving data through the memory hierarchy is a fundamental bottleneck that can limit the performance of core algorithms of machine learning, such as convolutional neural networks (CNNs). Looplevel optimization, including loop tiling and loop permutation, are fundamental transformations to reduce data movement. However, the search space for finding the best loop-level optimization configuration is explosively large. This paper develops an analytical modeling approach for finding the best loop-level optimization configuration for CNNs on multi-core CPUs. Experimental evaluation shows that this approach achieves comparable or better performance than state-of-the-art libraries and auto-tuning based optimizers for CNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutional Neural Networks (CNNs) have had transformative impact on several domains including image/video classification, language processing, genetic analysis, etc. CNNs are computationally very demanding. Therefore there has been tremendous interest in optimized implementation of the CNN stages needed in Deep Neural Network (DNN) pipelines. CNN stages of varied shapes and sizes are needed even within a single DNN pipeline.</p><p>Since the cost of data movement dominates the cost of floatingpoint arithmetic computations on all current hardware platforms, loop tiling is a crucial transformation for the development of optimized code for CNN. However, a fundamental challenge is the explosive size of the space of possible tiled loop variants for the CNN computation: </p><p>To Appear in Proceedings of ASPLOS <ref type="bibr">'21</ref> The computation can be expressed as a 7-dimensional loop nest, with one loop per index. Allowing for any order of accumulation of additive contributions for each result tensor element, all 7 loops are fully permutable and hence fully tileable with hyper-rectangular tiles. Considering a three-level memory hierarchy, up to three levels of tiling may be appropriate, leading to an explosively large search space with three groups of 7 tiling loops, with 7! possible permutations of the tiling loops within each group, i.e., 1.28 ? 10 11 configurations. Further, for each configuration of tiling loops, a very large number of possible choices exist for the tile sizes, resulting in an explosive number of alternatives from which to select.</p><p>To the best of our knowledge, all previously developed approaches for CNN optimization have used heuristics and/or empirical auto-tuning to search a limited subset of the explosive space of permutations and tile size choices <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">37]</ref>. This is a fundamental limitation to achieving consistently high performance across the wide range of CNN instances used in DNN pipelines. We aim to solve this problem in a principled and comprehensive way. To achieve this, we develop the first approach that models analytically the data movement for any CNN stage in a multi-level memory hierarchy. Using this model, we show how to explore the entire search space, looking for the configuration that minimizes the bandwidth-scaled data movement in the limiting level of the memory hierarchy. The insight of our approach, which differentiates it from previous CNN optimization efforts, is that analytical modeling and reasoning enable dramatic pruning of the space of permutations and tile sizes, reducing it to a small number of non-linear optimization problems that can be solved by off-the shelf solvers. This paper targets multicore CPUs, but the analytical machinery is applicable to targets such as GPUs, TPUs, FPGAs, and spatial arrays of accelerators.</p><p>Our modeling approach addresses a key limitation of existing efforts for CNN optimization. To demonstrate its utility, in this paper we combine this modeling with our custom code generator to achieve CNN performance that matches or exceeds the performance possible with state-of-the-art approaches. In the long run, our techniques provide a critical building block for these existing approaches, allowing them to overcome one of their fundamental limitations. This existing work falls in the following three categories. Libraries of optimized functions: Tuned vendor libraries are currently the primary means of achieving high performance for Although vendor libraries can achieve very good performance, we demonstrate through our experimental evaluation of Intel's stateof-the-art oneDNN library that there is scope for improvement if wider exploration of the search space is be undertaken using the approach proposed in this paper (the discussion in Sec. 12 elaborates on this). Auto-tuning and ML-based tuning: One of the most successful recent efforts in optimizing tensor computations has been TVM <ref type="bibr" target="#b6">[7]</ref>. TVM uses a combination of auto-tuning (actual execution of candidate code variants on the target platform) and a dynamically trained Machine Learning model to guide the design-space exploration. However the enormous search space poses a problem and manual expertise is required to set up optimization scripts that control the search space. We present experiments demonstrating the greater effectiveness of our new approach over TVM's auto-tuning over a constrained search space. By combining the model-driven comprehensive design space exploration from our work with the auto-tuning framework in TVM, further improvement in performance is feasible (the discussion in Sec. 12 elaborates on this). Polyhedral compilers: Such compilers incorporate powerful transformations for affine programs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">39]</ref>. The CNN computation in Eq. 1 is affine and can be automatically tiled and optimized by this approach. However, the performance achieved by state-of-the-art polyhedral compilers is very far from that provided by vendor libraries or by auto-tuning-based code generators such as TVM <ref type="bibr" target="#b6">[7]</ref>. These compilers face a fundamental challenge: they must separate the key consideration of tile size optimizationinherently non-linear-from the choice of loop transformations. The only recourse is to use an outer auto-tuning loop that explores a limited space of tile sizes, and an inner loop that generates code for them <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39]</ref>. Our approach can be generalized for analytical modeling of data movement in a class of tiled tensor computations and incorporated into polyhedral compilers, thereby overcoming this fundamental limitation. (Sec. 12 elaborates on this). Contributions: The paper makes the following contributions:</p><p>? It develops, to the best of our knowledge, the first comprehensive analytical modeling for data movement volume for multi-level tiled CNN execution on a system with a multi-level memory hierarchy, covering the full space of permutations and tile sizes. While the modeling approach is used in the context of multicore CPUs, it can also be used for CNN optimization on other platforms, such as GPUs, FPGAs, distributed-memory systems, and accelerator arrays.</p><p>? It presents the first analysis that exploits algebraic properties of the analytical expressions for data-movement volume to dramatically prune the number of distinct cases from thousands to only eight in order to find the global optimum in the entire space of tile-loop permutations for a single-level tiled CNN. The factor of reduction in the search space that is enabled by this algebraic analysis is exponentially higher for multi-level tile-size optimization.</p><p>? It demonstrates the use of the new analytical modeling and optimization approach through the generation of high-performance multicore CPU code for three CNN benchmarks, including all CNN stages of MobileNet <ref type="bibr" target="#b14">[15]</ref>, ResNet-18 <ref type="bibr" target="#b13">[14]</ref>, and Yolo9000 <ref type="bibr" target="#b29">[31]</ref>. The achieved performance is comparable to or better than both the state-of-the-art CNN library (Intel's oneDNN <ref type="bibr" target="#b26">[27]</ref>) and the stateof-the-art framework for auto-tuned code generation (TVM <ref type="bibr" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OVERVIEW 2.1 System Overview</head><p>Fig. <ref type="figure" target="#fig_1">1</ref> shows the components of the MOpt system (Modeling-based Optimizer) for generating optimized CNN code for multicore processors, based on a novel comprehensive design-space exploration approach for tile-loop optimization. The leftmost component represents a conceptual methodology for pruning the space of possible permutations of tile-loops for single-level tiling. This methodology uses analytical modeling of data movement volume to identify a very small subset-containing only 8 elements-of the full space of tile-loop permutations, guaranteed to contain an optimal configuration that minimizes data volume for tiled execution. The rest of this section highlights the key ideas behind this modeling, while Sec. 3 and 4 provide a more detailed description.</p><p>The right portion of the figure shows the tool components for code generation for a specific CNN. From the insights provided by the modeling methodology, together with the specific sizes of the kernel and input/output of the CNN, a set of constrained nonlinear optimization problems are automatically generated. These problems capture the search for optimal tile sizes for multi-level tiling (Sec. 5). The optimization problems are then solved using an off-the-shelf non-linear solver (we use AMPL <ref type="bibr" target="#b9">[10]</ref> with Ipopt <ref type="bibr" target="#b38">[40]</ref>) to produce optimal tile sizes ? ?,? and data movement costs ? ? (here ? ranges over the levels of the memory hierarchy). The best solution gives the tile sizes and tile-loop permutation to be used to generate customized C code for the CNN stage, with tile loops surrounding a CNN microkernel that implements register-tiling using vector intrinsics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Key Ideas for Analytical Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>/ / Ni / Nj / Nk a r e p e r f e c t m u l t i p l e s o f T i / T j / Tk</head><p>for ( i t = 0 ; i t &lt; Ni ; i t += Ti ) for ( j t = 0 ; j t &lt; Nj ; j t += Tj ) for ( k t = 0 ; k t &lt; Nk ; k t +=Tk ) for ( i = 0 ; i &lt; Ti ; i ++) for ( j = 0 ; j &lt; Tj ; j ++)</p><formula xml:id="formula_1">for ( k = 0 ; k &lt; Tk ; k ++) C[ i + i t ] [ j + j t ]+= A[ i + i t ] [ k+ k t ] * B [ k+ k t ] [ j + j t ] ;</formula><p>Listing 1: Single-level tiled matrix multiplication Consider the data footprint of a single tile from Listing 1. This footprint is the sum of the volumes of the data slices accessed by the three arrays ?, ?, and ?, respectively ? ? ? ? , ? ? ? ? , and ? ? ? ? . This is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Among all possible combinations of tile sizes chosen such that the total data-footprint does not exceed cache capacity, we want to find the one(s) achieving minimal data movement between main memory and cache:</p><formula xml:id="formula_2">? ? ? ? + ? ? ? ? + ? ? ? ? ? ? (2)</formula><p>As is the case with much of the prior work on analytical modeling of cache misses for loop computations <ref type="bibr" target="#b12">[13]</ref>[4] <ref type="bibr" target="#b17">[18]</ref>, we only model cold misses (first access of data) and capacity misses but not conflict misses arising from finite set-associativity of caches. We demonstrate through experimental evaluation that this idealized model of cache behavior is very effective in tile optimization for CNNs. Consider the iterations of the innermost tiling loop kt. As kt is changed, and different tiles are executed, we can observe (Fig. <ref type="figure" target="#fig_2">2</ref>) that the accessed data slices are completely distinct (i.e., without any reuse of data between tiles) for ? and ?, whereas exactly the same data slice of ? is used for all the tiles. The total volume of data movement between main memory and cache for the complete execution of the innermost tiling loop kt is DV ? kt = ? ? ? ? and DV ? kt = ? ? ? ? for arrays ? and ?, respectively. For ?, since the same data slice ? [it:it+? ? -1] [jt:jt+? ? -1] is repeatedly accessed for each value of the tile-loop iterator kt, with a fully associative cache each data element will only be brought in once from memory.</p><p>The combined data volume for all three arrays, DV kt , is as follows (the factor of 2 associated with the data volume for ? is due to the need to move each element in both directions, first from memory to cache and finally back from cache to memory):</p><formula xml:id="formula_3">DV kt = DV ? kt + DV ? kt + DV ? kt = ? ? ? ? + ? ? ? ? + 2? ? ? ?</formula><p>The modeling of total data movement volume between memory and cache for the execution of the innermost kt tile-loop was facilitated by the fact that two of the arrays did not have any inter-tile data reuse, while the third one had complete inter-tile data reuse of a slice of data that was small enough to fit in the cache. As we attempt to analyze the volume of data movement through the outer two tiling loops, the data footprints of the arrays increase and the analysis of hits and misses becomes very complicated, with many combinations of possibilities depending on the chosen tile sizes.</p><p>A key to developing our analytical parametric modeling approach is the recognition that for the purpose of tile-size optimization, we do not need to accurately model data-movement volume for all possible tile sizes, but it is sufficient to carry out such modeling for those tile sizes that effectively utilize the available capacity of the cache/scratchpad. We therefore assume that the collective data footprint of two adjacent tiles will exceed the cache capacityif not, the chosen tile sizes are too small and wasteful and should be increased to make better use of the available capacity. Under such an assumption, we can continue the parametric analysis of data volume for the entire execution of the tiled matrix multiplication algorithm. For any tiling loop, we have two possibilities with respect to any array: the loop iterator is either used in the indexing of the array (it is a present index), or it is not used and thus is an absent index (e.g., tile-loop iterator it does not affect the accessed elements of array ? [?] [?] because ? is an absent index for ?). If the tile-loop iterator is a present index, the data slice accessed for each value of the iterator is distinct, and the total accessed data volume over the execution of the tile loop is the product of the number of tile-loop iterations and the data volume corresponding to the inner nested loops. Even if the tile-loop iterator is an absent index, if the data footprint of the slice accessed by inner loops has exceeded cache capacity, the total data movement is again the product of the number of tile-loop iterations and the data volume accessed in execution of the inner loops. Based on these observations, the following cost expression applies to the two innermost tile-loops:</p><formula xml:id="formula_4">DV jt,kt = ? ? ? ? DV kt = ? ? ? ? ? ? ? ? + ? ? ? ? + 2? ? ? ? Similarly, DV it,jt,kt = ? ? ? ? DV jt,kt = ? ? ? ? ? ? ? ? ? ? ? ? + ? ? ? ? + 2? ? ? ? = ? ? ? ? ? ? 1 ? ? + 1 ? ? + 2 ? ?<label>(3)</label></formula><p>Given specific values for ? ? , ? ? , ? ? , the parametric expression in Eq. 3 can be minimized subject to the capacity constraints in Eq. 2. However, this is only one of 6 permutations of the tiling loops, and we desire the combination of tile-loop permutation and tile sizes that minimize total data movement between memory and cache. When this modeling is generalized to the CNN computation (as described in the next section), a brute-force enumeration and solution of a constrained optimization problem for each possible tile-loop permutation leads to a huge number of cases. For example, for multi-level tiling of the 7-dimensional loop nest for CNN, with 4 levels of tiling loops (register-tiling, L1, L2, and L3 cache), the number of cases is (7!) 4 , i.e., over 645 trillion cases. However, as we elaborate in Sec. 4, algebraic reasoning can be used to reduce the total number of parametric symbolic expressions to be considered for modeling all tile-loop permutations at one level of for ( n = 0 ; n &lt; Nn ; n + + )</p><p>for ( k = 0 ; k &lt; Nk ; k + + ) for ( c = 0 ; c &lt; Nc ; c ++) for ( r = 0 ; r &lt; Nr ; r ++) for ( s = 0 ; s &lt; Ns ; s ++) for ( h = 0 ; h &lt; Nh ; h + + ) for (w = 0 ; w &lt; Nw; w++)</p><formula xml:id="formula_5">Out [ n ] [ k ] [ h ] [w] += In [ n ] [ c ] [ h+ r ] [w+ s ] * Ker [ k ] [ c ] [ r ] [ s ]</formula><p>Listing 2: CNN loops </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYTICAL MODELING FOR SINGLE-LEVEL TILING</head><p>Given a specific permutation of the tile-loops for a single level of tiling of the CNN computation, we aim to develop a parametric expression for the total volume of data movement (as a function of tile sizes) between main memory and an idealized fully-associative LRU cache with a capacity of ? words and unit line-size. In the next section, we present a pruning strategy to dramatically reduce the number of tile-loop permutations to be considered in solving the tile-optimization problem. Given the original CNN code in Listing 2, Listing 3 shows one particular single-level tiled version. <ref type="foot" target="#foot_0">1</ref>We will use ? ? = ?? 7 , . . . , ? 1 ? to denote a particular permutation of the tile-loop iterators ??, ??, . . . in the tiled code, where ? 1 is the innermost tile-loop iterator in the tile-loop nest. The corresponding tile sizes for a particular tiled version will be denoted by</p><formula xml:id="formula_6">? ? = ?? 7 , . . . ,? 1 ? ? N 7 .</formula><p>Here each tile size? ? is such that 1 ? ? ? ? ? ? where ? ? is the corresponding problem size. We assume that each problem size ? ? is a multiple of the corresponding tile size ? ? . This assumption is used only for the presentation of cost modeling; the actual code generation handles the general case of partial tiles. A tiling configuration is a pair ? ? ?, ? ? ?. In the execution, the iterators from ? ? will be instantiated with concrete values. Each such instance is an iteration vector and will be denoted by ? ? ? N 7 . In any such ? ?, the value of iterator ? ? is always a multiple of the corresponding tile size ? ? . To simplify the discussion, in our cost modeling we will normalize ? ? in ? ? by? ? . Thus, the ?-th element of ? ? now takes values in the set {0, 1, . . . , ? ? /? ? }. Execution of the code defined by a configuration ? ? ?, ? ? ? corresponds to a sequence of tiles defined by a lexicographic order of all vectors ? ?. A key component of our modeling is an analytical description of the amount of data movement in executing two consecutive tiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Modeling of Inter-Tile Data Reuse and Total Data Movement</head><p>Given ? ? = ?? 7 , . . . , ? 1 ?, we construct an analytical expression to model the amount of data movement when the corresponding tiled execution occurs. Note that the expression is parametric in the tile sizes ? ? and will later be used to define a constrained optimization problem in which the objective function is this cost expression and the unknowns are the tile sizes in ? ? . Thus, for any code version (as defined by a loop permutation ? ?), the solution of this optimization problem provides concrete tile sizes to minimize the cost expression.</p><p>The modeling analysis is done separately for each of the three arrays In, Out, and Ker. For any array ?, let ? ? for be innermost (i.e., rightmost) position in ? ? of an iterator that occurs in the array reference for ?. For example, suppose ? ? = ?. . . , ct, nt?. For array reference Out [n, k, h, w] from the original code we have ? Out = 1, since in the tiled code this reference becomes Out [n + nt, k + kt, h + ht, w + wt] which contains nt, and nt is in position 1 in ? ?. For array reference In[n, c, h + r, w + s], both nt and ct occur in the tiled code, but nt occurs at position 1 in ? ? (i.e., in the innermost/rightmost position) and thus ? In = 1. Finally, for Ker [k, c, r, s] we have ? Ker = 2 since ct occurs at position 2 in ? ?. Consider a tile with tile sizes ? ? , ? ? , ? ? , ? ? , ? ? , ? ? , ? ? . The execution of the tile will access a 4-D slice of</p><formula xml:id="formula_7">? ? ? ? ? ? ? ? ele- ments of Out [n, k, h, w] and ? ? ? ? ? ? ? ? elements of Ker [k, c, r, s]. For In[n, c, h + r, w + s],</formula><p>the data slice accessed in the tile will have ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1) elements. This is because the index expression ? + ? takes ? ? + ? ? -1 distinct values in a contiguous range as ? varies over some contiguous range of ? ? values and ? ranges over a range of ? ? values. The capacity constraint specifying that the total data footprint must not exceed cache capacity is:</p><formula xml:id="formula_8">? ? ? ? (? ? + ? ? -1) (? ? + ? ? -1) + ? ? ? ? ? ? ? ? + ? ? ? ? ? ? ? ? ? ? (4)</formula><p>As illustrated in Sec. 2 with the matrix-multiplication example, the analytical modeling of data volume for execution of the CNN loop nest for a specific tile-loop permutation is done by an inner to outer traversal of the tile-loops. Starting with the inner-most tile loop, that loop's index is either absent or present in the tensor's index expressions. For example, consider the particular tile-loop order shown in Listing 3. The innermost tile-loop corresponds to loop index wt, which is an absent iterator for Ker and a present iterator for In and Out. This means that for Ker the data slices accessed for successive tiles as we step through the wt tile-loop will be exactly the same, i.e., full inter-tile data reuse is achieved. In contrast, completely distinct data slices of Out are accessed by the different tiles that are executed as wt is varied, i.e., there is absolutely no data reuse across the tiles. For In, the original indexing expression involving ? is of the form ? + ?. Hence there is some partial overlap of the data slices accessed by successive tiles as wt iterates (as detailed below).</p><p>For any permutation ? ?, for the innermost tile-loop there is complete data reuse between successive tiles if that iterator is absent in a tensor's index expressions, and no reuse or partial reuse for any tensor where the index is present. Further, after the execution of all tiles in the innermost tile-loop, eviction of data from previous tiles should occur for any tensor with that index present. This is a consequence of our choice in only modeling data-movement volume for tile sizes that are sufficiently large so that cache capacity is not wasted (i.e, the combined tile footprint of two adjacent tiles always exceeds cache capacity). Thus, for any tensors with the innermost tile loop index being present, no data reuse is possible at any outer tiling loops even if that outer index is absent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cost Expressions for Data Movement</head><p>Based on these properties, there are two cases for the cost computation. The first case is for arrays Out and Ker, as well as for In when the iterator at position ? In is nt or ct. Here the cost computation simply considers the number of pairs of consecutive iteration vectors ? ? and ? ? ? in the lexicographic order such that the value at position ? ? changes from the first to the second vector. In all such cases, the second tile accesses a completely different slice of the corresponding array ?. Thus, the amount of data movement is the number</p><formula xml:id="formula_9">? ? ? ? ?7 ? ?</formula><p>? ? of such pairs multiplied by the tile footprint for that array.</p><p>As discussed earlier, for Out the tile footprint is ? ? ? ? ? ? ? ? and for Ker this footprint is ? ? ? ? ? ? ? ? . For array In, the footprint is</p><formula xml:id="formula_10">? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1)</formula><p>. Multiplying this footprint with the number of pairs of consecutive tiles for which data movement occurs (as defined above) gives the complete data volume for a particular loop permutation ? ?. The second case is for In[n, c, h + r, w + s] when the iterator at position ? In is wt, ht, st, or rt. Consider one execution of the loop for this iterator. Each time the iterator changes, there is partial reuse across consecutive tiles. As a result, the inter-tile movement cost along the corresponding data dimension is the tile size for the iterator. For example, if the iterator at position ? In is wt, the tile footprint in that data dimension is ? ? + ? ? -1, but due to partial overlap between tiles the actual amount of new data in that data dimension is ? ? . For one execution of the wt loop, there are ? ? /? ? -1 such iterator changes. Thus, the cost is? ? (? ? /?? -1) = ? ? -??. The number of times this cost is incurred is determined by the loops surrounding wt, and is the product of ? ? /? ? for the positions ? around ? In .</p><p>More generally, we have a cost term which is the product of</p><formula xml:id="formula_11">? In &lt; ? ?7 ? ?</formula><p>? ? and one of the following:</p><formula xml:id="formula_12">? ? ? ? ? (? ? + ? ? -1)(? ? -? ? ) when wt is at ? In ? ? ? ? ? (? ? + ? ? -1)(? ? -? ? ) when st is at ? In ? ? ? ? ? (? ? -? ? )(? ? + ? ? -1) when ht is at ? In ? ? ? ? ? (? ? -? ? )(? ? + ? ? -1)</formula><p>when rt is at ? In We also have a second term which captures data movement cost when the very first iteration of that loop occurs. For this iteration there is no reuse from the previous tile, and the cost of the entire tile footprint is incurred. This cost is the product of ? In &lt; ? ?7</p><formula xml:id="formula_13">? ? ? ? and ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PRUNING CONFIGURATIONS: SINGLE-LEVEL TILING</head><p>Sec. 3 presented symbolic expressions for total data volume as a function of parametric tile sizes, for any given permutation of the tile-loops. There are 7! possible permutations for the seven tile loops for a single level of cache, and (7!) ? permutations for ? levels of cache. In this section, we show that massive pruning of the search space is possible via algebraic analysis that reduces the number of permutations to be considered to just 8 of the 7! = 5040 total permutations of the seven tile-loops. This is done by proving that the solution to one of these eight optimization problems is guaranteed to be as good as or better than any solutions for the remaining 5032 cases.</p><p>The identification of the pruned subset of tile-loop permutations is done via an inner-to-outer analysis of tiling loops and reasoning about the implications on total data movement cost, for different choices for tile-loop indices made at each level. The array indexing structure for the CNN computation is such that each of the seven loop indices is present in exactly two of the three tensors and absent in one tensor: ?, ?, and ? are all present for In and Out, but absent for Ker; ?, ? , and ? are present for In and Ker, but absent for Out; ? is present for Ker and Out but absent for In. As per the analysis in the previous section, the total data movement cost for two of the three arrays will be fully determined just from the choice of the innermost tile-loop. The rest of this section describes these cases and summarizes the final result of this reasoning. Innermost wt: If we choose the innermost tile-loop to be wt, the data movement volume for the the seven tiling loops will be</p><formula xml:id="formula_14">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1) for In and 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>? ? ? ? ? ? ? ? ? ? for Out (the factor of 2 is due to the need to read and write each element of Out).</p><p>The order of the six surrounding tile-loops will not affect the total data movement cost of In and Out, but will affect the data movement cost for Ker. As per the analysis in Sec. 3, the expression for data movement for Ker is a product of the tile footprint's volume (? ? ? ? ? ? ? ? ) and the product of ? ? /? ? for all tile-loops from the first present iterator and all surrounding iterators. The volume will be minimized if all absent indices are lower in the nesting order than all present indices. This is achieved by placing the tile-loops for absent indices ht and nt (in either order) in a band just above wt, with the tile-loops for present indices kt, ct, rt, and st in a band (in any order) above the tile-loops for ht and nt. We will use the notation ?{kt, ct, rt, st}, {nt, ht}, wt? to denote the set of tile-loop configurations described above: innermost tile-loop for wt, surrounded by a band of two tile-loops for nt and ht (in either order), and an outermost band of tile-loops for indices kt, ct, rt, st, in any relative order among those four tile-loops. Note that this notation represents a set of 4! ? 2! = 48 iterator permutations; however, all elements of this set are equivalent with respect to the cost model, as their cost expressions are exactly the same. When exploring the search space, one arbitrary representative of this set will be chosen and will be subjected to non-linear optimization. The same applies for the other seven cases described below: each case defines a set of cost-equivalent permutations, and one arbitrary representative of the set is selected for tile size optimization.</p><p>The parametric expression for the total data movement cost for any configuration in set ?{kt, ct, rt, st}, {nt, ht}, wt?, e.g., ?kt, ct, rt, st, nt, ht, wt? is:</p><formula xml:id="formula_15">DV kt,ct,rt,st,nt,ht,wt = ? ? ? ? ?? ?? ?? ?? ?? ?? [? ? ? ? ? ? ? ? + ?? ?? ? ? ? ? (2 ?? ?? ? ? ? ? ? ? ? ? + ? ? ? ? (? ? + ? ? -1) (? ? + ? ? -1)) ]<label>(5)</label></formula><p>The solution of a constrained optimization problem to minimize the expression in Eq. 5, subject to the capacity constraint in Eq. 4 will find the lowest possible data volume among all possible permutations with ?? as the innermost tiling loop.</p><p>Innermost ht: The analysis for tile-loop configurations with ht at the innermost position can be done similarly to the case with wt being innermost. The minimal possible data movement will be achieved with any arbitrary member of the set ?{kt, ct, rt, st}, {nt, wt}, ht?, e.g., ?kt, ct, rt, st, nt, wt, ht?:</p><formula xml:id="formula_16">DV kt,ct,rt,st,nt,wt,ht = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? [? ? ? ? ? ? ? ? + ? ? ? ? ? ? ? ? (2 ? ? ? ? ? ? ? ? ? ? ? ? + ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -<label>1</label></formula><p>))] Innermost st: Since st is present for In and Ker, the data movement costs for these two tensors will be independent of the permutations of the remaining outer tile-loop indices:</p><formula xml:id="formula_17">DV Ker ...,st = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? DV In ...,st = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1)</formula><p>The data-movement cost for Out will depend on the permutation of the outer tile-loops. The lowest cost is obtained when the absent indices for Out are placed immediately above st. The absent indices for Out are ct and rt. Any permutation in the set ?{nt, kt, ht, wt}, {ct, rt}, st? will achieve the lowest possible data movement cost for Out:</p><formula xml:id="formula_18">DV Out ...,st = 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>The optimization problem for any permutation in the set ?{nt, kt, ht, wt}, {ct, rt}, st? is to minimize the sum of these three DV cost expressions subject to the constraint in Eq. 4.</p><p>Innermost rt: The reasoning for this case is similar to the case for innermost st. ? ? ? ? ? ? ? ? ? ? for Ker. Since kt is absent in In, the next surrounding loop will contain an iterator that is present in In. This next iterator uniquely determines the cost function. The six cases for this choice can be separated in two groups: {wt, ht, st, rt} and {nt, ct}. As discussed shortly, the second group of choices can be ignored. Any choice from the first group gives rise to a different cost expression; thus, each of those 4 cases has to be solved separately. Together with the 4 cases described earlier (i.e., innermost loop is wt, ht, st, or rt), this gives us the 8 overall cases mentioned previously.</p><p>The cost functions for the first group are similar to those discussed earlier. For example, the cost for ?. . . , wt, kt? is similar to the one for ?. . . , wt?, but now a factor ? ? ? ? is missing because kt is the innermost loop and does not affect In. Now consider the second group {nt, ct} of choices-for example, ?. . . , nt, kt?. Compare this cost with the corresponding one for configuration ?. . . , wt, kt?. It is easy to show that the only difference is a factor of ? ? ? ? (? ? + ? ? -1) in the cost for ?. . . , nt, kt?, which is changed to ? ? + ? ? -1 in the cost for ?. . . , wt, kt?. Since ? ? ? ? ? 1, the cost for ?. . . , nt, kt? will never be lower than the one for ?. . . , wt, kt?. Thus, nt (and, similarly, ct) should not be chosen for the loop immediately surrounding the innermost loop kt.</p><p>For completeness, below are the details of the cost expressions for the four relevant cases. Based on different choices for the second innermost iterator, the data movement volume expression is as follows:</p><p>For Innermost nt and ct: As discussed above, choosing nt or ct as the second loop in ?. . . , kt? is inferior to choosing one of {wt, ht, st, rt}.</p><p>A similar argument can be used to establish that choosing nt or ct as the innermost loop is inferior to choosing one of {wt, ht, st, rt}.</p><p>The only difference between the two arguments is that now all cost functions have an extra factor ? ? ? ? (since kt is not the innermost loop anymore), but the rest of the reasoning still applies. Thus, no additional cases arise to be solved. Summary: By analyzing the algebraic structure of the cost expressions, as described above, we have identified that only eight equivalence classes of tiling permutations need to be considered: ?{kt, ct, rt, st }, {nt, ht }, wt ? ?{kt, ct, rt, st }, {nt, wt }, ht ? ?{nt, kt, ht, wt }, {ct, rt }, st ? ?{nt, kt, ht, wt }, {ct, st }, rt ? ?{nt, ct, ht, rt, st }, wt, kt ? ?{nt, ct, wt, rt, st }, ht, kt ? ?{nt, ct, ht, wt, rt }, st, kt ? ?{nt, ct, ht, wt, st }, rt, kt ?</p><p>Only one arbitrary representative permutation from each set is selected for further analysis, since all elements in the set have exactly the same cost expression for data movement. Thus, the search space is drastically reduced from 5040 distinct tile-loop permutations to only 8 cases for single-level tiling, and 8 ? cases for ?-level tiling instead of 5040 ? cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MULTI-LEVEL TILE-SIZE OPTIMIZATION</head><p>In this section, we present our approach to optimizing multi-level tiled CNN. Due to the multiple levels of cache on multiprocessors, multi-level tiling is beneficial to optimize data movement at the different levels in the memory hierarchy. In general, while cache capacities at later levels increase, the bandwidth for data movement between adjacent levels in the hierarchy decreases. Thus the overhead (in time) to move data between different levels in the memory hierarchy will be different. Assuming that concurrent data transfers (of different data) can occur between different levels of the memory hierarchy, we seek to minimize the maximum bandwidth-scaled data-volume across all levels.</p><p>For ?-level tiling, the number of tile parameters will be 7?, seven tile sizes per level. Since the tiled execution corresponds to a 7? loop nest, the range of execution for any iterator ? at tile-level ? will be ? ?+1 ? , i.e., the tile-size for that loop variable at the next outer tiling level, and ? ? for the outer-most tile. In the previous section, the data volume expressions for single-level tiling featured ratios of the problem size over the tile size along the different iteration space dimensions, ? ? /? ? . For multi-level tiling, the expressions will have terms of the form ? ?+1 ? /? ? ? , i.e., the expressions for each level involve parametric tile sizes for that tile level and the next outer tile level.</p><p>Let BW ? represent the bandwidth available for data transfers and DV ? the volume of data moved between levels ? and ? + 1 in the memory hierarchy. We seek a tile configuration that minimizes max ? DV ? BW ? . However, although several publicly available nonlinear solvers can be used to solve the optimization problem developed in the previous section for single-level tiling, none can directly solve a constrained min(max ()) nonlinear optimization problem. Hence we use the following approach to solve the ?-level tile optimization problem: solve ? constrained optimization problems, where the parametric data volume expression for each level ? is minimized in one of those. For the instance of the minimization problem for level ?, additional constraints are added to the effect that DV ? BW ? must be greater than or equal to DV ? BW ? for ? ? ?. Our approach to multi-level tile optimization is illustrated by a simpler example of one-dimensional functions. Fig. <ref type="figure" target="#fig_3">3</ref> shows three functions: ? 1 (?) (colored black), ? 2 (?) (colored red), and ? 3 (?) (colored blue). Consider the problem of finding min(max (? 1 , ? 2 , ? 3 )), where analytical expressions as a function of variable ? are available for ? 1 , ? 2 , and ? 3 . We need to find the minimum of the function ? comp , shown by the dotted line in Fig. <ref type="figure" target="#fig_3">3</ref>, but no analytical expression is available for ? comp that can be input to a constrained non-linear optimization solver. We solve the min-max problem by solving three separate min(? ) problems, over the three regions ?, ?, and ?, respectively. ? is the region over ? where function ? 1 is greater than or equal to ? 2 and ? 3 . Similarly, ? and ? represent regions over ? where ? 2 and ? 3 , respectively, are greater than or equal to the other two functions. The minimum value of ? comp over the full range of ? can be expressed as ???(? 1 , ? 2 , ? 3 ), where</p><formula xml:id="formula_19">? 1 = min ? (? 1 (?)), ? 2 = min ? (? 2 (?)), ? 3 = min ? (? 3 (?)).</formula><p>In order to solve for ? 123 = min(max (? 1 (?), ? 2 (?), ? 3 (?))), ? lo &lt; ? &lt; ? hi we can solve three minimization problems, one each for regions over which the corresponding function has the highest value (regions respectively marked ?, ?, and ? in Fig. <ref type="figure" target="#fig_4">3)</ref>:  Along with data movement optimizations, optimizing the throughput of compute-units is critical for achieving close to peak performance. The principal computations in convolutions can be realized using the Fused-Multiply-Add (FMA) operator, which can be efficiently executed by the SIMD (vector) units in modern processors. Each core in our benchmarking machines contains two AVX2 (256 bits == 8 floats) SIMD units, which can achieve a combined throughput of 2 ? 8 FMA operations (16 FMA ops), and has a latency of 4 to 6 clock cycles. The amount of parallelism required to fully utilize the SIMD pipeline can be computed using Little's Law as latency ? throughput = 6 ? 16 = 96. Note that these operations should not carry any dependencies.</p><formula xml:id="formula_20">? 1 = min(? 1 (?)), ? 1 (?) ? ? 2 (?), ? 1 (?) ? ? 3 (?), ? lo &lt; ? &lt; ? hi ? 2 = min(? 2 (?)), ? 2 (?) ? ? 1 (?), ? 2 (?) ? ? 3 (?), ? lo &lt; ? &lt; ? hi</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MICROKERNEL DESIGN FOR CNN</head><p>An outer product scheme, similar to BLIS <ref type="bibr" target="#b25">[26]</ref>, is used to achieve the required parallelism. Figure <ref type="figure" target="#fig_5">4</ref> shows the conceptual view of our for ( j 4 = 0 ; j 4 &lt; Nj ; j 4 += Tj3 ) for ( i 3 = i 4 ; i 3 &lt; i 4 + T i 3 ; i 3 += T i 2 ) for ( j 3 = j 4 ; j 3 &lt; j 4 + Tj3 ; j 3 += Tj2 ) / / A f t e r p a r a l l e l i z a t i o n for ( i 4 = 0 ; i 4 &lt; Ni ; i 4 += Ti3 )</p><p>for ( j 4 = 0 ; j 4 &lt; Nj ; j 4 += Tj3 ) for ( i p = i 4 + t i d / ( T j 3 / PTj3 ) * Tip ; ip &lt; i 4 + T i 3 ; i p +=( T i 3 / PTi3 ) * Tip ) / / p a r a l l e l for ( j p = j 4 + t i d %( T j 3 / PTj3 ) * Tjp ; jp &lt; j 4 + T j 3 ; j p +=( T j 3 / PTj3 ) * Tjp ) / / p a r a l l e l for ( i 3 = i p ; i 3 &lt; i p + Tip ; i 3 += T i 2 )</p><p>for ( j 3 = j p ; j 3 &lt; j p + Tjp ; j 3 += T j 2 )</p><p>Listing 5: Loop structure before and after parallelization outer product scheme. The output feature is distributed across the vector lanes. In AVX2, each vector register can hold eight singleprecision floating-point elements. Two such registers are used to hold the ?????? elements. Six vector registers, each of which holds a single input image point, are populated using vector broadcasts. The outer product of these six vector registers and two kernel registers are computed using efficient vectorized Fused Multiply Add (FMA) instructions and stored in twelve vector registers. Listing 4 shows the loop structure of our micro-kernel. The actual implementation of the entire microkernel, including loops, is implemented using x86 assembly code.</p><p>Packing: Efficient vectorization requires stride-1 access along the vectorization dimension. Our scheme vectorizes the output feature dimension (? ). However, since the kernel layout is [?, ?, ?, ?], ? is not the fastest varying dimension. Hence a data layout transformation is performed to make ? the fastest varying dimension before the convolutions are processed. We split the dimension ? into vector-length sized chunks, and each chunk is laid out contiguously in memory ([?, ?, ?, ?] ? [?/VecLen, ?, ?, ?, VecLen]). Our code generator automatically generates the packing code and this packing cost is included in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">OPTIMIZING FOR PARALLELISM</head><p>We describe how the sequential cost model is adapted to handle tiled parallel execution. We assume that each core owns a set of private caches (typically L1 and L2) and collectively shares a set of shared caches (typically L3). Since the L3 cache is shared, parallelizing loops that iterate over L3 tiles will cause cache interference. Loops that iterate over L2 tiles as well as loops that iterate over L1 tiles can be parallelized without cache interference. But parallelizing L1 loops will reduce data locality within L2 tiles. Further, parallelizing L2 tile loops achieve coarser parallelism, with lower scheduling overheads. Hence we sub-tile L2 tiling loops to create two-loop bands. Listing 5 shows the tile structure before and after parallelization of a 2D loopnest. The outermost band (ip and jp) is used for parallelization and the inner band (?3 and ?3) is executed sequentially by each core. Parallelizing certain dimensions like ? and ? will result in write conflicts. While these conflicts can be avoided by using atomic operations or synchronizations, the overhead is high. Hence, our model only considers parallelism along the non-reduction dimensions. The cost modeling in the parallel case is very similar to the sequential cost model explained in Sec. 5; hence we only describe the differences in this section. Even though the memory-to-L3 data movement remains the same, the effective bandwidth may be higher in the parallel case. Hence, we use a synthetic benchmark to determine the parallel memory-to-L3 bandwidth and use this bandwidth in the cost model. The parallel L3-to-L2 data movement cost may also change as the available L3 bandwidth is split across multiple cores. The per-core L3-to-L2 bandwidth is also computed using synthetic benchmarks. The parallel L3-to-L2 cost computation is similar to the cost computation explained in Sec. 5 and can be obtained by replacing ? ?3 in with ?? ? 3 where ? ? ??? ????????? ??????????. ? ?3/?? ?3 is the amount of parallelism along dimension ?. A constraint is added to ensure that the total amount of parallelism is equal to the total number of cores ( ? ?3/?? ?3 == num_cores). The rest of the constraints remain the same. The L2-to-L1 bandwidth and L1-to-register bandwidth used in the parallel case is the same as the sequential case. The parallel cost model is then solved using the same min-max formulation from Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">PUTTING IT ALL TOGETHER</head><p>In this section, we discuss some aspects of the overall process for generation of optimized CNN code that have not been previously described. We first demonstrate the way to handle parallel execution and then present the work flow of the full optimization system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System Workflow</head><p>The design of the microkernel (Section 6) is entirely dictated by the latency and throughput of the FMA units and is not dependent on the cache or memory parameters. Hence, for a given machine, the same micro-kernel is used for all problem sizes. However, the tile sizes and loop permutation of the loops surrounding the microkernel is dependent on the problem specification. Algorithm 1 shows an overview of our permutation and tile-size selection process. Function GetPrunedPermutation returns the set of pruned permutations. The loop at line 3 iterates over each permutation and finds the best tile-sizes for the given permutation. For a given permutation (pm), we initialize the FixedTileSizes as an empty array at line 5, we first find the tile-sizes for the most-constrained level and fix the tile size corresponding to this level. Next, among the remaining levels, we find the tile-sizes for the most-constrained level and find the tile-sizes for that level. This process is repeated until the tile-sizes for all levels are computed. However, the cost of each level is not known a priori. The maximum constraining level is found using the following steps. For each level: (i) add a constraint to mark the current level as the most constraining one, (ii) invoke the solver to find the tile-sizes which minimizes the cost under the former constraint, (iii) select the level with the minimum cost (min-max Algorithm 1: Permutation and Tile Selection Algorithm formulation). Each iteration of loop at line 6 represents this computation. The loop at line 8 finds the minimum cost assuming that the current level (ObjLvl) is the level with maximum constraints. Line 9 invokes the Ipopt solver <ref type="bibr" target="#b38">[40]</ref> by setting the constraint that the ObjLvl is the most constrained level. The if condition at line 10 keeps track of the minimum cost and the associated level. The tile sizes for the most constrained level are then fixed and removed from the search space (lines 16-17). Function getTileSizeforLevel is a helper function to extract the tile-sizes for a given level. This entire process is repeated for each permutation to find the best permutation and tile-sizes. Note that the tile-sizes returned from the solver are real numbers; however, tile-sizes should be integers.</p><p>We floor each tile-size to obtain the integer solution. The tile sizes are then adjusted to minimize the core idling (load balance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">MODEL VALIDATION</head><p>We present our experimental evaluation in two parts: first, in this section we discuss model validation, followed in the next section by a comparison with state-of-the-art alternatives: Intel oneDNN <ref type="bibr" target="#b26">[27]</ref> and AutoTVM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. For our experimental evaluation, we used all CNN benchmarks used by TVM in the extensive comparative evaluation <ref type="bibr" target="#b6">[7]</ref> against various other CNN optimization frameworks. The benchmarks used by TVM include all twelve conv2d operators from Resnet-18 <ref type="bibr" target="#b13">[14]</ref>, and the nine depth-wise conv2d operators from MobileNet <ref type="bibr" target="#b14">[15]</ref>. In addition we used all eleven conv2d operators from Yolo-9000 <ref type="bibr" target="#b29">[31]</ref>. All benchmark parameters are shown in Table <ref type="table">1</ref>. All input and output tensors were stored in NCHW layout and all kernel tensors were stored in KCRS layout. Any time expended in internal layout transformations was included in the measured execution time for all codes.</p><p>The experiments described in this section were performed by measuring single-core performance and profiling hardware counters on an 8-core Intel Core i7-9700K CoffeeLake processor, with 32KB L1 cache per core, 256KB L2 cache per core, and a shared 12MB L3 cache. Hardware counter events were profiled by use of Likwid <ref type="bibr" target="#b34">[36]</ref>.</p><p>For each of the 32 conv2d operators, a sampling of the space of tile-size combinations was performed to select around 100 configurations uniformly distributed in the full space of tile-size combinations. For each code configuration, we generated the modelpredicted score, measured performance by executing it, and gathered hardware counter events for data movement volume at the register, L1 cache, L2 cache, and L3 cache levels.</p><p>We sought to answer the following questions: (1) Given a set of alternative tile configurations for a benchmark, how does the rank ordering of those code configurations by use of the analytical model compare with that based on measured performance? The rationale for such an assessment is that the effectiveness of a compiler performance model in differentiating between configurations is much more important than the absolute error between modeled execution time and measured execution time.</p><p>(2) How does the rank ordering of code configurations by the model compare with the measured data volumes at the different levels of the memory hierarchy? (3) What is the loss-of-performance for a model-selected configuration when compared to the best performing configuration in the sampled set? We evaluated a top-1, top-2 and top-5 loss-ofperformance score, where top-k means the best performance among the top k predicted configurations by the model. Figure <ref type="figure">5</ref> presents the loss-of-performance comparing modelpredicted best configurations and the actual best among the 100 or so configurations evaluated for each benchmark. For each conv2d operator, we calculated three loss ratios. The top-one loss represents the loss of performance of the best-predicted case by our model over the actual best code version. The top-two loss represents the loss of performance of the better of the top-2 versions predicted by the model over the actual best code version. For the top-five loss, we take the best among the top 5 cases based on prediction. Our experiment shows that for all thirty-two conv2d operators, the model predicted best code versions always achieve less than 4.5% loss , i.e., the model always finds a code version that achieves 95.5% performance comparied to the actual best code version in the sampled configuration space. For most operators (thirty of thirtytwo), the loss is less than 3%.</p><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the correlation of predicted performance with actual performance and data movement hardware counters (registers, L1, L2, and L3) for three of the benchmarks:Resnet-9, Mobnet-2, and Yolo-5. Each of the three columns of graphs in the figure correspond to one of those three conv2d operators. In these graphs, the Y-axis represents one of the following metrics: Performance (GFLOPs), that conflict misses cause a significant drop in performance. But when we consider the top five configurations generated by the MOpt framework, it turns out that these configurations rarely experience pathological conflict miss scenarios and the best among the top five performs very well. We repeated each experiment 50 times on the system, using 8 threads on i7-9700k and 16 threads on i9-10980xe. We excluded the very first run since it often includes additional time for loading libraries. In order to avoid cache reuse across successive runs, we flushed the cache between runs and measured the execution time of each run individually. We turned off DVFS and turbo-boost, and locked the clock at base frequency to reduce the variability across runs. For each benchmark, we report mean GFLOPS achieved over 50 runs. The bar charts and the left vertical axes in Figure <ref type="figure">7</ref> show the performance, normalized to TVM's performance. As recommended by a popular approach for statistically-rigorous performance measurements <ref type="bibr" target="#b10">[11]</ref>, we also report the 95% confidence interval. The interval is shown on top of each bar, as a characterization of variance; in some cases, it is so small that it is barely visible. We also show the actual GFLOPS value of the MOpt-based code above the corresponding bar.</p><p>The geometric means of speed-up of MOpt over oneDNN are: On i7-9700k, 1.16x on the Yolo, 1.37x on the ResNet, and 1.24x on MobileNet. On i9-10980xe, 1.26x on the Yolo, 1.08x on the ResNet, and 1.14x on MobileNet. The geometric means of speed-up of MOpt over TVM are: On i7-9700k,1.73x on the Yolo, 1.40x on the ResNet, and 1.52x on MobileNet. On i9-10980XE, 1.53x on the Yolo, 1.84x on the ResNet, and 1.56x on MobileNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">RELATED WORK</head><p>Tile size optimization: Some previous research has focused on tile size optimization based on analytical modeling <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b39">41]</ref>. However, they relied on heuristic search. Recently, Li et. al <ref type="bibr" target="#b22">[23]</ref> developed an analytical modeling approach and its solution using nonlinear solvers, for optimizing data movement for tensor contractions. However, their work only addressed sequential computing and was restricted to tensor contractions and could not be applied to CNNs. Renganarayana et. al <ref type="bibr" target="#b30">[32]</ref> developed a framework based on integer geometric programming to optimized tile size selection if the optimization problem could be expressed as a posynomial. While our one-level tile-size optimization formulation is a posynomial, the constraints arising in the multi-level tile optimization problem are no longer posynomials. Some other previous efforts have formalized the tile size selection problem as a constrained optimization problem. Sarkar et. al <ref type="bibr" target="#b31">[33]</ref> presented a model for optimizing memory cost for doubly nested loops, and limited the dimension of loop nest to not greater than three. Krishna <ref type="bibr" target="#b18">[19]</ref> et. al utilized a nonlinear solver to find optimal tile sizes to minimize disk I/O for tensor contraction, but they only addressed on single level of tiling. Cociorva et. al <ref type="bibr" target="#b7">[8]</ref> proposed a model for optimizing inter-processor communication under memory constraints, restricted to tensor contraction. Lin et. al <ref type="bibr" target="#b23">[24]</ref> developed a tool that used a convex solver to optimize tile size for direct buffer access. However, it relied on heuristic search to find loop permutations and did not comprehensively cover the full loop permutation space, and they also only addressed a single level of tiling. Polyhedral compilers: Polyhedral compilers such as Polly <ref type="bibr" target="#b11">[12]</ref>, Pluto <ref type="bibr" target="#b5">[6]</ref>, PPCG <ref type="bibr" target="#b37">[39]</ref> perform tile sizes optimization and loop parallelization based on the polyhedral model. Tensor Comprehension <ref type="bibr" target="#b36">[38]</ref> is an automatic compiler for converting tensor computations to high-performance machine learning kernels based on the polyhedral model. However, a fundamental limitation of polyhedral compilers is that the cost models used for optimization are linear. The tile-size optimization problem is inherently non-linear. Polyhedral compilers are forced to separate tile-size optimization from tile-loop permutation and therefore have not demonstrated code generation for CNN whose performance matches vendor library code (like Intel oneDNN) or optimizers that use auto-tuning (like TVM). Specialized Machine Learning compilers: PlaidML [29] is a portable tensor compiler that compiles deep learning codes on mobile devices. It automatically applies tiling transformation to improve efficiency of training. XLA (Accelerated Linear Algebra) <ref type="bibr" target="#b20">[21]</ref> is a domain-specific compiler that improves performance for linear Algebra operators inside Tensorflow <ref type="bibr" target="#b2">[3]</ref>. XLA fuses Tensorflow operators in the same graph, so it reduces the requirements to write intermediate values and number of kernel calls. TVM <ref type="bibr" target="#b6">[7]</ref> is an automatic end-to-end optimizing compiler for improving the performance of deep learning systems. It works with deep learning frameworks like Pytorch <ref type="bibr" target="#b27">[28]</ref> and Keras <ref type="bibr" target="#b16">[17]</ref> and supports code generation for different hardware platforms. It extends and uses Halide <ref type="bibr" target="#b28">[30]</ref> as its internal representation. Its optimization is driven by an ML-based cost model that trains itself by using auto-tuning data collected when running on the target platform. It has been demonstrated to achieve much higher performance than other existing CNN optimizing frameworks like PPCG, PlaidML, XLA, etc. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. Thus, TVM represents the current state-of-the-art in CNN optimization. In this paper, we therefore compare performance with it. CNN libraries: Intel's oneDNN <ref type="bibr" target="#b15">[16]</ref> is a state-of-the-art optimized neural network library for Intel Architectures. We have compared performance with oneDNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">DISCUSSION</head><p>To the best of our knowledge, this paper presents the first demonstration that a purely analytical modeling approach for optimized code generation for CNN can achieve performance comparable to or better than the current state-of-the-art in both optimized vendor libraries and auto-tuning based optimizers that perform actual execution of candidate code versions on the target platform. Further improvement of performance is possible by via incorporating the strengths of these systems into MOpt, as discussed below.</p><p>Table <ref type="table" target="#tab_5">2</ref> contrasts the strengths and limitations of oneDNN, TVM, and MOpt. oneDNN is a highly optimized vendor library that includes highly optimized microkernels developed and optimized by Intel engineers over many years. However, it dynamically chooses among a small number of pre-determined tiled code structures based on the CNN array sizes provided at invocation, i.e., it performs minimal design-space exploration. TVM performs a search through a limited design space, as specified by the tuning script. A significant difference between our model-driven search methodology and TVM's auto-tuning based search is the extent of the space that can be effectively explored. Our search time is relatively independent of the problem size, while TVM's search time for a specified number of samples is essentially proportional to the number of operations of the specific CNN modeled. For example, TVM took 1 minute versus 109 minutes to search for the optimal code for the small first stage versus the large last stage of the Yolo-9000 pipeline. However, MOpt only took 9 seconds and 23 seconds, respectively, for optimizing these two problem cases. Therefore a judicious constraining of the full search space is essential for using TVM (as detailed in Sec. 10, we use the script recommended by the developers of TVM), i.e., comprehensive design-space exploration is not practical.  MOpt's strength is comprehensive design-space exploration to seek tile-loop structures and tile sizes that minimize the data volume at the bottleneck resource in the multi-level cache hierarchy. It does not use any empirical auto-tuning in its search and uses a microkernel that is not as highly optimized as oneDNN's. Nevertheless, the achieved performance of MOpt's code on the CNN stages of three DNN pipelines is almost always better and often much better than TVM's code, and comparable and sometimes much better than oneDNN. While data-movement volume is a significant factor that affects performance, other factors are also important, which are very challenging to model, such as conflict misses in real caches with finite set-associativity. A direction for ongoing/future research is to combine our model-driven approach with a limited amount of auto-tuning via actual execution on the target platform. One direction we explored was to incorporate a data-volume-model guided search within TVM's auto-tuning based search. However we faced a fundamental problem: TVM uses LLVM's compiler to generate vectorized code and it performs loop transformations in its backend that we cannot control. The performance of the final resulting code was affected very significantly by the LLVM backend so that a tile loop structure and tile sizes for which MOpt achieves very high performance can produce very low performance through the TVM-LLVM chain because of LLVM's transformations. TVM plans extensions to allow fixed microkernels at the inner-most level instead of the sole current path of LLVM code generation. When that feature is available, we expect to be able to incorporate MOpt's model-driven search into TVM's auto-tuning and gain the combined benefit of comprehensive design-space exploration and empirical auto-tuning.</p><p>Further planned work will apply the analytical modeling approach to optimize CNN on other target platforms. GPUs, FPGAs, distributed-memory systems, and accelerator arrays can be abstracted in a similar manner, as hierarchical systems with memory capacity at each level, with consideration for achieving adequate parallelism, leading to multi-level tile-size optimization problems. One important extension will be the modeling of spatial locality. This can be done by adapting the data volume expressions to count the number of cache lines (or DRAM transactions for GPUs): Use ? ? ? ? ? instead of ? ? , where ? is the cache line-size in words and ? ? is the tile size along the fastest-varying dimension of an array. This reflects the fact that the movement of data is actually in units of larger granularity-cache lines or fixed-size DRAM transactions (on GPUs)-and not individual elements.</p><p>Finally, there is significant potential for application of this modeldriven tile-optimization approach to overcome a fundamental limitation of polyhedral compilers: tile size optimization is currently infeasible because parametric tile size variables cause the array indexing expressions to become non-affine and thus out of the scope of the inherent modeling machinery within the polyhedral model. For a significant and practically important subset of matrix/tensor computations, a tile-footprint based cost-modeler and optimizer can be plugged into a polyhedral compiler, enabling iterative search across tile loop permutations and fusions by executing MOpt-like parametric tile size optimization to guide loop transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">CONCLUSION</head><p>We present a new approach to overcome the design-space explosion problem that has thwarted effective compile-time modeling and optimized code generation for CNNs. Although the space of possible configurations is extremely large, we devise an effective analytical modeling approach to search in this space. The structure of data movement cost expressions is exploited to achieve dramatic space pruning. Constrained non-linear optimization problems are used to find multi-level tile sizes that minimize bandwidth-scaled data volume at the most constraining level in the memory hierarchy. Experimental results demonstrate that achieved performance is superior to code generated by TVM and can be comparable to or better than Intel's oneDNN. Further improvements are possible by incorporating better microkernels and by using empirical autotuning. The methodology for full design-space exploration and tile-size optimization can also be used to enhance the performance of libraries such as oneDNN, optimizing code generators such as TVM, and polyhedral compilers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Out [?, ?, ?, ? ] = ?,?,? In[?, ?, ? + ?, ? + ? ] * Ker [?, ?, ?, ? ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MOpt Overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Data reuse in tiled matrix multiplication</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example to illustrate approach to multi-level tile size optimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>? 3 =</head><label>3</label><figDesc>min(? 3 (?)), ? 3 (?) ? ? 1 (?), ? 3 (?) ? ? 2 (?), ? lo &lt; ? &lt; ? hi and then selecting ? 123 = min(? 1 , ? 2 , ? 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Conceptual view of outer product scheme</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Input: 5 FixedTileSizes ? [] ; 6 while NotVisitedLvls ? ? do 7 MinCost ? INT_MAX; 8 for ObjLvl ? NotVisitedLvls do 9 [ 10 if MinCost &gt; CurCost then 11 MinTileSizes ? CurTileSizes; 12 MinLevel ? ObjLvl; 13 MinCost 23</head><label>567891011121323</label><figDesc>ProblemSize, HardwareSpec Output : LoopPermutation, TileSize 1 PrunedPermuSet ? GetPrunedPermutations (); 2 GlobalSoln.Cost ? INT_MAX; 3 for pm ? PrunedPermuSet do 4 NotVisitedLvls ? [Reg, L1, L2, L3]; CurCost, CurTileSizes] ? ArgMinSolve (ProblemSize, HardwareSpec, ObjLvl, pm, FixedTileSizes, NotVisitedLvls); IntegerSoln ? FloorToInteger (GlobalSoln); 24 FinalSolution.TileSizes ? LoadBalancer (finalIntegerSol); 25 return [FinalSolution.pm, FinalSolution.TileSizes];</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Model-predicted rank ordering versus actual measurement on i7-9700K. Left: Resnet9, Middle: Mobnet2, Right: Yolo5; Top: Performance (GFLOPs), followed by Reg. load/stores, L1 misses, L2 misses, L3 misses. Points are ordered along X-axis in decreasing order of predicted performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Performance (relative to TVM) and variance for Mobilenet, Yolo-9000, and Resnet-18 on i7-9700K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? ? ? ? ? ? ? ? ? ? DV Ker ...,rt = ? ? ? ? ? ? ? ? ? ? ? ? DV In ...,rt = ? ? ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1) DV ...,rt = DV Out ...,rt + DV Ker ...,rt + DV In ? ? ? ? ? ? ? ? ? ? for Out and</figDesc><table><row><cell>DV Out ...,rt = 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</cell><cell>? ? ? ? ? ? ? ? ? ? ?</cell></row><row><cell></cell><cell>...,rt</cell></row><row><cell cols="2">Innermost kt: In this case the data movement volume will be 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</cell></row></table><note><p>The best permutations are in set ?{nt, kt, ht, wt}, {ct, st}, rt?. For them, the data movement cost is as follows:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? ? ? ? ? ? ? ? ? ? DV Ker ...,wt,kt = ? ? ? ? ? ? ? ? ? ? ? ? DV In ...,wt,kt = ? ? ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1) DV ...,wt,kt = DV Out .? ? ? ? ? ? ? ? ? ? DV In ...,rt,kt = ? ? ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1) DV ...,rt,kt = DV Out .</figDesc><table><row><cell cols="2">permutation ?{nt, ct, ht, rt, st}, wt, kt?</cell></row><row><cell>DV Out ...,wt,kt = 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</cell><cell>? ? ? ?</cell></row><row><cell cols="2">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? DV Out ...,st,kt = 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? DV Ker ...,st,kt = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? DV In ...,st,kt = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? DV Out ...,rt,kt = 2 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? DV Ker ...,rt,kt = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</cell></row></table><note><p>..,wt,kt + DV Ker ...,wt,kt + DV In ...,wt,kt For permutation ?{nt, ct, wt, rt, st}, ht, kt? DV Out ...,ht,kt = 2 ? ? ? ? ? ? ? ? ? ? ? ? ? DV Ker ...,ht,kt = ? ? ? ? ? ? ? ? ? ? ? ? ? DV In ...,ht,kt = ? ? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1) DV ...,ht,kt = DV Out ...,ht,kt + DV Ker ...,ht,kt + DV In ...,ht,kt For permutation ?{nt, ct, ht, wt, rt}, st, kt? ? ? ? (? ? + ? ? -1)(? ? + ? ? -1) DV ...,st,kt = DV Out ...,st,kt + DV Ker ...,st,kt + DV In ...,st,kt For permutation ?{nt, ct, ht, wt, st}, rt, kt? ..,rt,kt + DV Ker ...,rt,kt + DV In ...,rt,kt</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Strengths/limitations of oneDNN, TVM and MOpt</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>To simplify the presentation, we do not show stride/dilation, but the methodology is applicable to the general case.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="14">ACKNOWLEDGMENTS</head><p>This work was supported in part by the <rs type="funder">U.S. National Science Foundation</rs> through awards 1946752 and 2018016.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The first row of charts shows that there is a strong correlation between actual performance and predicted performance.-code versions with higher performance generally also have higher modelpredicted scores. The other plots shows a strong correlation between data movement hardware counter measurement for the predicted bottleneck resource and the predicted performance. Since the predicted performance is based on the predicted bottleneck resource, we would expect correlation with hardware counter measurements for that resource. For both Resnet9 (left column) and Mobnet2 (middle column), the model predicts that the register level is the most constraining one. Indeed, the experimental measurements show a strong correlation with hardware measurements of load/stores. It is interesting to note that for both benchmarks there is no correlation with hardware counter measurements at some other levels, specifically L1 and L3. Both registers and L3 are predicted to be constraining resources for Yolo5 (right column) and this is also seen in the experimental data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">COMPARISON WITH STATE-OF-THE-ART LIBRARY AND AUTO-TUNING</head><p>In this section, we present a comparative experimental evaluation of the code generated by MOpt with a state-of-the-art library (Intel oneDNN <ref type="bibr" target="#b26">[27]</ref>) and a state-of-the-art auto-tuning system (AutoTVM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. The experiments were carried out on two systems: an 8-core Intel Core i7-9700K CoffeeLake processor, with 32KB L1 cache per core, 256KB L2 cache per core, and a shared 12MB L3 cache and an 18-core Intel i9-10980XE CascadeLake processor, with 32KB L1 cache per core, 1MB L2 cache per core, and a shared 24.75MB L3 cache.</p><p>We compare the performance of code generated by MOpt with two state-of-the-art frameworks: (i) Intel oneDNN (v1.5) library, and (ii) TVM (v0.6). TVM relies on auto-tuning and machine learning models to generate efficient code. All MOpt codes and oneDNN were compiled using the Intel ICC 2019 compiler with flags "-O3 -march=native -qopenmp". TVM recommends using the LLVM framework; hence we used LLVM-8. TVM tuning was based on their recommended template: "generic.schedule_conv2d_nchw"[2]. We used XGBTuner as the ML tuning model, and we set "LLVM -mcpu=core-avx2 or -mcpu=skylake-avx512" based on the target to ensure that the generated code was vectorized for the appropriate ISA (avx2 for i7, avx512 for i9). For each CNN benchmark, we ran TVM's auto-tuner with its internal ML model to find the best configuration over 1000 trials.</p><p>We compare TVM and oneDNN agaist two MOpt code versions (i) MOpt-1: A single code version generated with the configuration with minimum modeled cost and (ii) MOpt-5: Five code versions were synthesized based on the top 5 modeled configurations. The reason we also include MOpt-5 is to highlight the potential for performance improvement by inclusion of limited empirical autotuning to MOpt. Since the modeling in MOpt is based on an idealized fully associative cache, occasionally we find (e.g., Yolo9 and Yolo18)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automatic Kernel Optimization for Deep Learning on All Hardware Platforms</title>
		<ptr target="https://tvm.apache.org/2018/10/03/auto-opt-all" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tvm</forename><surname>Cnn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuning</forename><surname>Script</surname></persName>
		</author>
		<ptr target="https://github.com/apache/incubator-tvm/blob/v0.6/topi/python/topi/x86/conv2d.py" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analytical modeling of cache behavior for affine programs</title>
		<author>
			<persName><forename type="first">Wenlei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Noel</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ponnuswamy</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>POPL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Code generation in the polyhedral model is easier than you think</title>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Bastoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A practical automatic polyhedral parallelizer and locality optimizer</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
		<meeting>ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global communication optimization for tensor contraction expressions under memory constraints</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cociorva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhya</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Chung</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><surname>Ramanujam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Parallel and Distributed Processing Symposium</title>
		<meeting>International Parallel and Distributed Processing Symposium</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Some efficient solutions to the affine scheduling problem. I. One-dimensional time</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Feautrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="313" to="347" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A modeling language for mathematical programming</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fourer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Kernighan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="519" to="554" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistically Rigorous Java Performance Evaluation</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dries</forename><surname>Buytaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="57" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Polly-performing polyhedral optimizations on a low-level intermediate representation</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Grosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Groesslinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1250010</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast analytical model of fully associative caches</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Gysi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Grosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurin</forename><surname>Brandner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="816" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="https://github.com/oneapi-src/oneDNN" />
		<title level="m">Intel oneDNN</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to keras</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Ketkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning with Python</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The effect of cache models on iterative compilation for combined tiling and unrolling</title>
		<author>
			<persName><forename type="first">Toru</forename><surname>Peter Mw Knijnenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kisuki</surname></persName>
		</author>
		<author>
			<persName><surname>Gallivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fp O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="247" to="270" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient synthesis of out-of-core algorithms using a nonlinear optimization solver</title>
		<author>
			<persName><forename type="first">Sandhya</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Chung</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Choppella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="659" to="673" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uday</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacques</forename><surname>Pienaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">River</forename><surname>Riddle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11054[cs.PL]</idno>
		<title level="m">MLIR: A Compiler Infrastructure for the End of Moore&apos;s Law</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">XLA: TensorFlow, compiled</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TensorFlow Dev Summit</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing memory efficiency for deep convolutional neural networks on GPUs</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srimat</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;16: Proc. International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analytical cache modeling and tilesize optimization for tensor contractions</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sukumaran-Rajam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Veras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Rastello</surname></persName>
		</author>
		<author>
			<persName><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic loop tiling for direct memory access</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Renganarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huoding</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">O</forename><surname>'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Parallel &amp; Distributed Processing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="479" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing CNN Model Inference on CPUs</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference (USENIX ATC 19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1025" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analytical modeling is enough for high-performance BLIS</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">D</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><forename type="middle">M</forename><surname>Igual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Quintana-Orti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<ptr target="https://01.org/onednn" />
		<title level="m">oneDNN: oneAPI Deep Neural Network Library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Positivity, posynomials and tile size selection</title>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Renganarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rajopadhye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">IEEE</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An analytical model for loop tiling and its solution</title>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Megiddo</surname></persName>
		</author>
		<idno>ISPASS (Cat. No. 00EX422</idno>
	</analytic>
	<monogr>
		<title level="m">2000 IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="146" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Analytical bounds for optimal tile size selection</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shirako</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naznin</forename><surname>Fauzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-No?l</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Compiler Construction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="101" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="https://tiramisu-compiler.org/" />
		<title level="m">Tiramisu Compiler</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Likwid: A lightweight performance-oriented tool suite for x86 multicore environments</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Treibig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Wellein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 39th International Conference on Parallel Processing Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Performance-portable autotuning of OpenCL kernels for convolutional layers of deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yaohung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Kurzak</surname></persName>
		</author>
		<author>
			<persName><surname>Dongarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="9" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04730</idno>
		<title level="m">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Polyhedral parallel code generation for CUDA</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Juega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francky</forename><surname>Tenllado</surname></persName>
		</author>
		<author>
			<persName><surname>Catthoor</surname></persName>
		</author>
		<idno type="DOI">10.1145/2400682.2400713</idno>
		<ptr target="https://doi.org/10.1145/2400682.2400713" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>W?chter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><surname>Biegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="57" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic Creation of Tile Size Selection Models</title>
		<author>
			<persName><forename type="first">Tomofumi</forename><surname>Yuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshminarayanan</forename><surname>Renganarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rajopadhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><forename type="middle">E</forename><surname>Eichenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin O'</forename><surname>Brien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<meeting>the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="190" to="199" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
