<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HARMONY: Dynamic Heterogeneity-Aware Resource Provisioning in the Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<email>q8zhang@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Faten Zhani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raouf</forename><surname>Boutaba</surname></persName>
							<email>rboutaba@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HARMONY: Dynamic Heterogeneity-Aware Resource Provisioning in the Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7FEBB26A40BE900A9E95818F026806D3</idno>
					<idno type="DOI">10.1109/ICDCS.2013.28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data centers today consume tremendous amount of energy in terms of power distribution and cooling. Dynamic capacity provisioning is a promising approach for reducing energy consumption by dynamically adjusting the number of active machines to match resource demands. However, despite extensive studies of the problem, existing solutions for dynamic capacity provisioning have not fully considered the heterogeneity of both workload and machine hardware found in production environments. In particular, production data centers often comprise several generations of machines with different capacities, capabilities and energy consumption characteristics. Meanwhile, the workloads running in these data centers typically consist of a wide variety of applications with different priorities, performance objectives and resource requirements. Failure to consider heterogenous characteristics will lead to both sub-optimal energy-savings and long scheduling delays, due to incompatibility between workload requirements and the resources offered by the provisioned machines. To address this limitation, in this paper we present HARMONY, a Heterogeneity-Aware Resource Management System for dynamic capacity provisioning in cloud computing environments. Specifically, we first use the K-means clustering algorithm to divide the workload into distinct task classes with similar characteristics in terms of resource and performance requirements. Then we present a novel technique for dynamically adjusting the number of machines of each type to minimize total energy consumption and performance penalty in terms of scheduling delay. Through simulations using real traces from Google's compute clusters, we found that our approach can improve data center energy efficiency by up to 28% compared to heterogeneity-oblivious solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Data centers have recently gained significant popularity as a cost-effective platform for hosting large-scale service applications. While large data centers enjoy economies of scale by amortizing long-term capital investments over large number of machines, they also incur tremendous energy costs in terms of power distribution and cooling. In particular, it has been reported that energy-related costs account for approximately 12% percent of overall data center expenditures <ref type="bibr">[5]</ref>. For large companies like Google, a 3% reduction in energy cost can translate to over a million dollars in cost savings <ref type="bibr" target="#b13">[16]</ref>. On the other hand, governmental agencies continue to implement standards and regulations to promote energy-efficient computing <ref type="bibr" target="#b1">[2]</ref>. As a result, reducing energy consumption has become a primary concern for today's data center operators.</p><p>In recent years, there has been extensive research on improving data center energy efficiency <ref type="bibr" target="#b15">[18]</ref>, <ref type="bibr" target="#b19">[22]</ref>. One promising technique that has received significant attention is Dynamic Capacity Provisioning (DCP). The goal of this technique is to dynamically adjust the number of active machines in a data center in order to reduce energy consumption while meeting the Service Level Objectives (SLOs) of workloads. In the context of workload scheduling in data centers, a metric of particular importance is scheduling delay <ref type="bibr" target="#b18">[21]</ref>, <ref type="bibr" target="#b16">[19]</ref>, <ref type="bibr" target="#b12">[15]</ref>, <ref type="bibr" target="#b14">[17]</ref>, which is the time a request has to wait before it is scheduled on a machine. Task scheduling delay is a primary concern in data center environments for several reasons: (1) A user may need to immediately scale up an application to accommodate a surge in demand and hence requires the resource request to be satisfied as soon as possible. <ref type="bibr" target="#b1">(2)</ref> Even for lower-priority requests (e.g., background applications), long scheduling delay can lead to starvation, which can significantly hurt the performance of these applications. In practice, however, there is often a trade-off between energy savings and scheduling delay. Even though turning off a large number of machines can achieve high energy savings, at the same time, it reduces service capacity and hence leads to high scheduling delay.</p><p>However, despite the fact that a large number of DCP schemes have been proposed in the literature in recent years, a key challenge that often has been overlooked or considered difficult to address is heterogeneity, which is prevalent in production cloud data centers <ref type="bibr" target="#b14">[17]</ref>. We summarize the types of heterogeneity found in production environments as follows:</p><p>• Machine Heterogeneity. Production data centers often comprise several types of machines from multiple generations <ref type="bibr" target="#b16">[19]</ref>. They have heterogeneous processing capacities and capabilities, different hardware features, processor architecture, processor speed, memory and disk size. Consequently, they also have different energy consumption rates at run-time. • Workload Heterogeneity. Production data centers typically receive vast number of heterogeneous resource requests with diverse resource demand, durations, priorities and performance objectives <ref type="bibr" target="#b18">[21]</ref>, <ref type="bibr" target="#b16">[19]</ref>, <ref type="bibr" target="#b12">[15]</ref>. In particular, it has been reported that the difference in resource demand and duration can span several orders of magnitude <ref type="bibr" target="#b18">[21]</ref>, <ref type="bibr" target="#b16">[19]</ref>, <ref type="bibr" target="#b3">[6]</ref>. The heterogeneous nature of both machine and workload in production cloud environments has profound implications on the design of DCP schemes. In particular, given a surge of workload requests, a heterogeneous-oblivious DCP scheme can turn on wrong types of machines which are not capable of handling these requests (e.g., due to insufficient capacity), resulting in both resource wastage and high scheduling delays. However, designing a heterogeneity-aware DCP scheme is known to be difficult. In particular, designing such a scheme requires accurate characterization of both workload and machine heterogeneities. It also requires a heterogeneity-aware performance model that strikes a balance between energy savings and scheduling delay at run-time.</p><p>In this paper, we present HARMONY, a Heterogeneity-Aware Resource MONitoring and management sYstem that addresses the aforementioned challenges for DCP. Specifically, we first present a characterization of the heterogeneity found in one of Google's production compute clusters. Using standard K-means clustering, we show that the heterogeneous workload can be divided into multiple distinct task classes with similar characteristics in terms of resource and performance objectives. We then formulate the DCP as an optimization problem that considers machine and workload heterogeneity as well as run-time electricity prices. We then devise an online control algorithm based on the Model Predictive Control framework that dynamically adjusts the number of servers in order to minimize total energy cost and task scheduling delay, while taking into account the switching costs of machines.</p><p>The remainder of the paper is organized as follows. Section II surveys related work in the literature. Section III provides an analysis of a publicly available workload traces from Google to motivate our approach. Section IV provides an overview of HARMONY. In Section V, VI and VII we describe the design of HARMONY in details. Section VIII discusses the deployment of HARMONY in practice. Finally, we evaluate our proposed system using Google workload traces in Section IX, and draw our conclusions in Section X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section we provide a survey of existing studies on (1) understanding workload and machine characteristics in production clouds, and (2) dynamic capacity provisioning for balancing the trade-off between energy savings and application performance objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Machine and workload characterization</head><p>Both capacity planning and task scheduling require a deep analysis of the workload characteristics in terms of arrival rate, requirements, and duration <ref type="bibr" target="#b12">[15]</ref>. As a result, characterizing workload in production clouds has received much attention in recent years. For example, Mishra et. al. have analyzed the workload of a Google compute cluster, and proposed an approach to task classification using k-means clustering <ref type="bibr" target="#b12">[15]</ref>. Following the same line of research, Chen et. al. provided a characterization of Google cluster workload at joblevel applying the k-means algorithm <ref type="bibr" target="#b7">[10]</ref>. Sharma et. al. <ref type="bibr" target="#b16">[19]</ref> and Zhang et. al. <ref type="bibr" target="#b18">[21]</ref> studied the problem of finding accurate workload characterizations through benchmark generation and validation. Recently, Reiss et. al. <ref type="bibr" target="#b14">[17]</ref> provided a comprehensive analysis of the heterogeneity and dynamicity found in Google cluster traces. They have shown that both machine configurations and workload composition are highly heterogeneous and dynamic over time. They also pointed out the importance of considering workload heterogeneity for designing adaptive schedulers. However, the goal of these studies was to understand the workload composition in production clouds, rather than using workload characterization for resource allocation and capacity provisioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Energy-aware capacity provisioning</head><p>There is a large body of literature on energy-aware dynamic capacity provisioning in data centers. For example, pMapper <ref type="bibr" target="#b17">[20]</ref> is a migration-aware workload placement framework for optimizing application performance and power consumption in data centers. However, it does not consider the cost of turning on and off machines. Similarly, Mistral <ref type="bibr" target="#b11">[14]</ref> is a framework that dynamically adjusts VM placement to find a trade-off between power consumption, application performance, and reconfiguration costs. However, it does not consider the arrival rate of task requests in its formulation. More recently, Ren et. al. <ref type="bibr" target="#b15">[18]</ref> studied the problem of scheduling heterogenous batch workload across geographically distributed data centers. Different from out work, they assume workload has already been divided into distinct types. Furthermore, they do not consider schedulability issues, since for general cloud workloads, not every task can be divided arbitrarily and scheduled on any machine. To the best of our knowledge, no previous work has applied task classification to dynamic capacity provisioning problem in heterogenous data centers. Thus, we design HARMONY as a workload-aware DCP framework that can achieve both higher application performance and efficiency in terms of energy savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. WORKLOAD ANALYSIS</head><p>To understand the heterogeneity in production cloud data centers, we have conducted an analysis of workload traces for one of Google's production compute clusters <ref type="bibr" target="#b2">[3]</ref> <ref type="foot" target="#foot_0">1</ref> consisting of approximately 12, 000 machines. The workload traces contain scheduling events as well as resource demand and usage records for a total of 672, 003 jobs and 25, 462, 157 tasks over a time span of 29 days. Specifically, a job is an application that consists of one or more tasks. Each task is scheduled on a single physical machine. When a job is submitted, the user can specify the maximum allowed resource demand for each task in terms of required CPU and memory size. The values of the demand for each resource type were normalized between 0 and 1. Even though the dataset does not provide task size for other resource types such as disk, it is straightforward to extend our approach to consider additional resource types.</p><p>In addition to resource demand, the user can also specify a scheduling class, a priority and placement constraints for each task. The scheduling class captures the type of the task. Its value ranges from 0 to 3, with 0 corresponding to least latency-sensitive tasks (e.g., batch processing tasks) Priority group gratis (0-1) other (2-8) production <ref type="bibr" target="#b6">(9)</ref><ref type="bibr" target="#b7">(10)</ref><ref type="bibr" target="#b8">(11)</ref> Fig. <ref type="figure">4</ref>: CDF Scheduling delay and 3, the most latency-sensitive tasks (e.g., web servers). The scheduling class is used by every machine to determine the local resource allocation policy that should be applied to each task. The priority reflects the importance of each task. There are 12 priorities that are divided into three priority groups: gratis(0 -1), other(2 -8), production(9 -11) <ref type="bibr" target="#b14">[17]</ref>. In this paper, we primarily analyze task characteristics at the priority group-level, because priority groups already provide a coarse-grained classification of tasks. Furthermore, they also have strong correlation with task scheduling classes <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[17]</ref>. Nevertheless, our technical approach can be extended to handle any combination of task priority groups and task scheduling classes. Generally speaking, task priorities can be used for specifying the Quality of Service (QoS) in terms of desired task scheduling delay. During busy periods when demand approaches cluster capacity, task priorities can ensure that high priority tasks are scheduled earlier than low priority tasks, resulting in lower scheduling delay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Machine and Workload Dynamicity</head><p>In our analysis, we first plot the total demand for both CPU and memory over time. The results are shown in Figure <ref type="figure" target="#fig_3">1</ref> and 2, respectively. The total demand at a given time is determined by total resource requirement by all tasks in the system, including the tasks that are waiting to be scheduled. From both figures, it can be observed that the demand for each resource type can fluctuate significantly over time. Figure <ref type="figure">3</ref> shows the number of machines available and used in the cluster. Specifically, a machine is available if it can be turned on to execute tasks, and is used if there is at least one task running on it. Figure <ref type="figure">3</ref> also suggests that the capacity of the cluster is not adjusted according to resource demand, as the number of used machines is almost equal to the number of available machines. These observations suggest that a large number of machines can be turned off to save energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of Task Scheduling Delay</head><p>While turning off active machines can reduce total energy consumption, turning off too many machines can also hurt task performance in terms of scheduling delay. Figure <ref type="figure">4</ref> shows the Cumulative Distribution Function (CDF) of the scheduling delay for tasks with respect to their priority groups. It is apparent that tasks with production priority have better scheduling delay than the gratis ones. Indeed, more than 50% and 30% of the tasks in production and other priority groups respectively are scheduled immediately. However, on the other Priority group gratis (0-1) other (2-8) production <ref type="bibr" target="#b6">(9)</ref><ref type="bibr" target="#b7">(10)</ref><ref type="bibr" target="#b8">(11)</ref> Fig. <ref type="figure">6</ref>: CDF of task duration for different priority groups hand, some of the tasks were delayed significantly. During our analysis, we have also noticed that some tasks even with production priority were delayed for up to 21 days. Since the cluster is not constantly overloaded, the only possible explanation is that the task is difficult to schedule due to unrealistic resource requirement or there is a constraint that is difficult to satisfy. These results suggests that more efficient provisioning and scheduling methods are needed to reduce the scheduling delay for these difficult-to-schedule tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Understanding Machine Heterogeneity</head><p>The dataset also provides information about the types of machines running in the cluster. A machine is characterized by its capacity in terms of CPU, memory and disk size as well as a platform ID, which identifies the micro-architecture (e.g., vendor name and chipset version) and memory technology (e.g., DDR or DDR2) of the machine. Similar to tasks, machine capacities are normalized such that the largest machine has a capacity equal to 1. Figure <ref type="figure">5</ref> shows the different types of machines and their characteristics (capacity and platform ID (PFID)). We found 10 types of machines where more than 50% and 30% of the machines belong to machine types 1 and 2, respectively. On the other hand, machines types 3 and 4 have around 1000 machines each. The remaining machine types (5 to 10) constitute less than 100 machines. Unfortunately, the traces do not provide detailed information about hardware specifications, however, it is clear that such a heterogeneity will translate into different energy consumption models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Understanding Task Heterogeneity</head><p>In order to analyze the workload heterogeneity, we plotted tasks requirements and their durations for the three priority groups. Figure <ref type="figure" target="#fig_2">7</ref> shows the CPU and memory size of tasks belonging to each priority group. The coordinates of each In particular, we found that 43% of gratis tasks have the same CPU and memory requirements equal to 0.0125 and 0.0159, respectively. Furthermore, most of the large tasks are either CPUintensive or memory-intensive. There is usually no correlation between CPU requirement and memory requirements. Another key observation is that the difference in task size can span several orders of magnitude. For example, Figure <ref type="figure" target="#fig_2">7a</ref> shows that the largest task in the gratis priority group is almost 1000× bigger than the smallest task in the same group for both CPU and memory. Similar characteristics can also be found in Figure <ref type="figure" target="#fig_2">7b</ref> and 7c. Finally, by comparing Figure <ref type="figure">5</ref> and Figure <ref type="figure" target="#fig_2">7</ref>, it is easy to see that not every task (e.g., CPU size ≈ 1) can be scheduled on every type of machine (e.g., CPU capacity= 0.5).</p><p>Another important parameter that shows the heterogeneity of the tasks is the task duration. Figure <ref type="figure">6</ref> shows the CDF of task durations for tasks with different priority groups. From Figure <ref type="figure">6</ref>, it can be seen that production tasks (9-11) have long durations that can reach 17 days, whereas 90% of the remaining tasks (i.e., gratis and other) have shorter duration that ranges between 0 and 10 hours. The same observation can be done for production-priority tasks when compared to other priority groups (Figure <ref type="figure">6</ref>). Furthermore, it is worth noting that more than 50% of the tasks are short (less than 100 seconds). This concurs with the previous workload analysis studies <ref type="bibr" target="#b18">[21]</ref>, which showed that tasks are either short or long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Summary</head><p>The above analysis suggests that while the benefit of dynamic capacity provisioning is apparent for production data center environments, designing an effective and dynamic task scheduling and capacity provisioning scheme is challenging, as it involves finding a satisfactory compromise between energy savings and scheduling delay, given the heterogeneous characteristics of both machines and workload. In particular, we have found the heterogeneity in task size can span several orders of magnitude, and not every type of machine can schedule every task. Similar characteristics have also been recently reported in Microsoft and Facebook data centers <ref type="bibr" target="#b3">[6]</ref>.</p><p>Thus, it is a critical issue to design heterogeneity-aware DCP schemes for production data centers, as failing to consider these heterogeneous characteristics will result in sub-optimal performance for DCP.</p><p>IV. SYSTEM OVERVIEW As discussed previously, we aim at designing HARMONY as a DCP framework that considers both task and machine heterogeneity. This requires (1) an accurate characterization of both workload and machines, (2) effectively capture the dynamic workload composition at run time, and (3) using the captured information to control the number of machines in the compute cluster to achieve a balance between energy savings and scheduling delay. In practice, large cloud infrastructures such as Google compute clusters execute millions of tasks per day. Capturing heterogeneity at fine-grained (i.e., pertask) level is not a viable option due to the high overhead for monitoring and computation. Thus, a medium-grained characterization of the workload is necessary. Motivated by this observation, we present a workload characterization of Google traces by dividing tasks into task classes using the K-means algorithm. However, different from previous work on this topic <ref type="bibr" target="#b12">[15]</ref>, <ref type="bibr" target="#b7">[10]</ref> that focuses on only understanding workload characteristics, our goal is to ensure high accuracy of the characterization, while supporting efficient task classification (e.g., labeling) at run time. It should be mentioned that machines are naturally characterized (i.e., there are 10 types of machines in the cluster). Thus, in our solution we will mainly focus on task characterization.</p><p>Once we have obtained the workload characterization, we introduce a monitoring mechanism that allows HARMONY to capture the run-time workload composition in terms of arrival rate for each task class. To make resource allocation decisions, we then define a container as a logical allocation of resources to a task that belongs to a task class. In our approach, the task containers serve as reservations for helping the controller to make machine allocation decisions. It is also possible to directly use task containers for scheduling (to be described in Section VII). Finally, a heterogeneity-aware DCP controller is designed to adjust the number of active machines, based on the current machine availability and workload composition.</p><p>The architecture of HARMONY is shown in Figure <ref type="figure">8</ref>. It consists of the following components. The task analysis module is responsible for monitoring the arrival of every task class in order to identify the type to which it belongs. The scheduler is responsible for assigning incoming tasks to active machines in the cluster. The prediction module receives statistics of the arrival rate for each task class, and forecasts its future arrival rates. The container manager evaluates the number of containers required to cope with the current workload. These parameters are evaluated based on two factors, namely, the predicted arrival rate and the required SLO for each type of tasks expressed in terms of average scheduling delay. The container manager periodically notifies the capacity provisioning module about the number of required containers of each type. The capacity provisioning module decides which machine in particular should be switched on or off. Obviously, the goal is to select the right combination of machines that would be able to host the containers and, at the same time, minimizes the energy consumption. The monitoring module is responsible for collecting diverse statistics about tasks and machines, including CPU and memory usage, free resources and current task durations. It also reports any failures and anomalies to the management framework. It should be mentioned that currently we only focus on the cases where disk storage of a VM is either handled by a storage network or collocated with the VM itself. However, it is possible to extend HARMONY to consider distributed file systems such as the Google File System (GFS) <ref type="bibr" target="#b9">[12]</ref>. In the following sections, we describe the design of HARMONY in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TASK CHARACTERIZATION AND RUN-TIME CLASSIFICATION</head><p>The goal of task classification is to divide the workload into task classes that are accurate for DCP yet efficient for runtime task labeling. For the purpose of resource provisioning, it is necessary to consider task priority group, task size (CPU, memory) as well as task running time as the features for clustering. For run-time task labeling, we can observe the task characteristics and assign it to the class that has the highest similarity score. Following the common approach, the similarity score between a task and a task class is computed as the Euclidean distance between the task and the centroid of the task class in the feature space.</p><p>Generally speaking, achieving a high accuracy in characterization can be realized by properly controlling the number of task classes. On the other hand, performing accurate runtime task labeling is more difficult, because task running time is generally unknown to the system until the task finishes. In Harmony, we address this issue by realizing the fact that tasks are either short or long, and the majority of the tasks are short tasks. Thus, we can initially label all tasks as short tasks, and gradually update the labels to the correct ones as time passes. Since only a small fraction of tasks are long, the error caused by the incorrect labeling is both small and short-lived.</p><p>To provide better support for this mechanism, we adopt a two-step approach for workload clustering. In the first step, tasks are classified based on static characteristics (e.g., priority, CPU and memory size specified in the job request) using k-means algorithm. In the second step, each task class is further divided into sub-classes based on task running time. The advantage of this approach is that it not only simplifies the relabeling process, but also reduces the error introduced by the relabeling process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. WORKLOAD PREDICTION</head><p>As mentioned previously, the task analysis module is responsible for monitoring the arrival of tasks and determine their types based on their size and current running time. This also allows the task analysis module to predict the future arrival rate of tasks. Currently, we have implemented a time series-based predictor using the well-known ARIMA <ref type="bibr" target="#b4">[7]</ref> model. Once the predictions have been obtained, the next step is to determine the number of machines of each type needed in next control period. We address this problem by computing the number of containers required to support the workload for each task class. In HARMONY, this is performed by the container manager. The container manager evaluats the number of containers per type of tasks such that the desired scheduling delay is achieved. Let c i denote the number of containers for tasks type i such that the average scheduling delay is equal to di . We can model the queue of tasks of type i and its corresponding N i t containers at time t by M /G/N i t queue since a single container can process one task at a time.</p><p>Based on queuing theory, the average waiting time d i for type i tasks is given by <ref type="bibr" target="#b10">[13]</ref>:</p><formula xml:id="formula_0">d i ≈ π N i t 1 -ρ i • 1 + CV 2 i 2 • 1 N i t μ i<label>(1)</label></formula><p>where μ i is the execution rate of task type i, ρ i = λi N i t μi is the traffic intensity of tasks type i, CV 2  i is the squared coefficient of variation of the average duration, and π N i t is the probability that a task has to wait in the queue. It is expressed as:</p><formula xml:id="formula_1">π N i t = (N i t ρ) N i t N i t !(1 -ρ i ) ⎡ ⎣ N i t -1 k=0 (N i t ρ i ) k k! + (N i t ρ i ) N i t N i t !(1 -ρ i ) ⎤ ⎦ -1<label>(2)</label></formula><p>Given an average scheduling delay and using Eq. ( <ref type="formula" target="#formula_0">1</ref>), it is easy to estimate c i to ensure d i ≤ di and ρ i &lt; 1.</p><p>The number of containers computed using Eq. ( <ref type="formula" target="#formula_1">2</ref>) provides an estimation of the number of required containers to guarantee performance objectives. In the next section, we discuss how to use this number for machine selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONTAINER-BASED SCHEDULING FOR DCP</head><p>In this section. We describe our solution called Container-Based Scheduling (CBS), which aims at finding an allocation of containers that can be used for run-time task scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Modeling Container Size</head><p>One of the main challenges for using containers for scheduling is to select appropriate container size. On one hand, setting the container size equal to the maximum value in the class ensures every task can be scheduled without violating machine capacity constraints. However, it also causes resource wastage due to overestimation of true task resource demand in the average case. In contrast, setting the container size equal to the average task size will lead to under-estimation of task resource assumption, resulting in machine capacity violations.</p><p>To deal with this issue, we rely on the statistical multiplexing of task resource demand to ensure the probability of machine capacity violation to be low. Specifically, it is known that k-means algorithm tries to model the input as a mixture of Gaussian distributions with identity covariance matrix [4]. Thus, it is reasonable to assume tasks in each task class n follow an independent gaussian distribution N (μ D n , σ D n ) for each resource type r ∈ R, where R = {1, 2, ..., D} denotes the set of resource types. The problem of selecting the optimal container size can be stated as follows: For each task class n ∈ N and resource type r ∈ R, for any machine m with resource capacities C mr ∀r ∈ R and any group of tasks G where each task i ∈ G has actual demand s ir and container size c ir , find a value c nr such that if i c ir ≤ C mr ∀r ∈ R, then Pr( i s ir &gt; C mr ∀r ∈ R) ≤ , where is the error bound we wish to guarantee.</p><p>To solve this problem, we first translate into resource specific error bounds r for r ∈ R using the formula for joint probability. Assuming all tasks in G are independent, then the total task usage is also normally distributed, namely i s ir ∼ N( i μ r i , i (σ ir ) 2 ), ∀r ∈ R. Define Z r as the (1 -r )-percentile of the unit normal distribution, our goal is to ensure</p><formula xml:id="formula_2">C mr -i∈G μ r i i∈G (σ r i ) 2 ≥ Z r<label>(3)</label></formula><p>holds for all r ∈ R. In this case, we set c ir = μ ir + Z r σ ir , which satisfies the above inequality. Then, we can reserve extra machines to handle machine capacity violation by scheduling the tasks causing the violation in the reserved machines.</p><p>Finally, even though we assume task size follows Gaussian distribution, the results for other clustering methods and more general distributions (i.e., non-Gaussian) can be derived similarly using concentration bounds <ref type="bibr" target="#b8">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CBS Formulation</head><p>We now provide a formal model for CBS. In our model, time is divided into intervals of equal duration, and control decision is made at the beginning of each time interval. The cluster consists of M types of machines, each m ∈ M has N m t machines available at time t (i.e., the t'th time interval). Denote by C mr ∈ R + the capacity of a single machine of type m ∈ M for resource type r ∈ R. Similarly, there are N types of containers to schedule at time t, the number of containers of type n ∈ N is N n t . Let c nr ∈ R + denote the size of a type n ∈ N container for resource type r ∈ R.</p><p>Let y i t ∈ {0, 1} denote whether machine i is active at time t. Furthermore, define u i t ∈ {-1, 0, 1} as an integer variable that indicates whether the machine is turned on (u i t = 1) or off (u i t = -1), or unchanged (u i t = 0). We also define a in t ∈ N∪{0} as an integer variable that indicates the number of type n containers on machine i at time t, and γ in t as the change in z in t at time t. We thus have the following state equations:</p><formula xml:id="formula_3">y i t+1 = y i t + u i t (4) a in t+1 = a in t + γ in t (5)</formula><p>The utilization of machine i for type r resource at time t is</p><formula xml:id="formula_4">u ir t = 1 C mr n∈N z in t c nr .<label>(6)</label></formula><p>As total energy usage of a physical machine can be estimated by a linear function of resource utilization <ref type="bibr" target="#b19">[22]</ref>, let p t denote the energy price at time t, the energy cost at time t is:</p><formula xml:id="formula_5">E t = p t m∈M i∈N m t y i t E idle,m + r∈R α mr u ir t (<label>7</label></formula><formula xml:id="formula_6">)</formula><p>where E idle,m ∈ R + is the energy consumption of a type m machine when it is idle, and α nr ∈ R + is the slope of the linear energy consumption function. . Next, to model the scheduling performance, since it is not possible for all containers to be scheduled when demand is high, we assume there is a utility function f n (•) that models the monetary gain for scheduling containers. f n (•) is assumed to be a concave function that can be derived from SLO objectives (e.g., reduction in monetary cost due to scheduling delay). The total revenue becomes:</p><formula xml:id="formula_7">U perf t = n∈N f n ( m∈M i∈N m t a in t )<label>(8)</label></formula><p>The cost for turning machines on and off can be described by:</p><formula xml:id="formula_8">C sw t = m∈M i∈N m t q m |u i t | (9)</formula><p>where q m ∈ R + denotes the switching cost of a single type m machine. Finally, we require the following constraints:</p><formula xml:id="formula_9">y i t ≥ a in t ∀m ∈ M, i ∈ N m t , n ∈ N, t ∈ T (10) n∈N a ni t c nr ≤ C mr ∀m ∈ M, i ∈ N m t , t ∈ T<label>(11)</label></formula><p>Constraint <ref type="bibr" target="#b7">(10)</ref> states that a machine is active if it hosts at least one container. Constraint <ref type="bibr" target="#b8">(11)</ref> ensures that containers scheduled on the same machine do not exceed the resource capacity of the machine. Thus, the overall objective of CBS is control the number of active machines and adjust container placement in a way that minimizes the energy consumption, while minimizing the number of machines to be switched on and off over a time horizon T = {1, 2, ..., T }:</p><formula xml:id="formula_10">max a in t ,u i i ,y i t ,a in t R T = T t=1 U pref t -E t -C sw t</formula><p>subjects to constraints (4), ( <ref type="formula">5</ref>), ( <ref type="formula">10</ref>) and <ref type="bibr" target="#b8">(11)</ref>. CBS is N Phard to solve as it generalizes the vector bin-packing problem <ref type="bibr" target="#b6">[9]</ref>. Furthermore, linear programming based solutions cannot be applied to CBS due to the large number of variables involved. Given 80 task classes and over 10K machines, CBS contains at least 800K variables, making it difficult to solve in online settings. Finally, traditional bin-packing heuristics (e.g., First-Fit) do not apply directly to CBS as they do not consider machine switching and container reassignment costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Solution Algorithm</head><p>To address the above limitation, we first describe a relaxed version of CBS, where the number of machines (i.e., y it ) and container assignment (i.e., z in t ) no longer take integer values. This relaxation yields a simpler formulation, as we only need to maintain the container counts at an aggregate level (i.e., permachine type) rather than at per-machine level. This addresses the scalability issue associated with CBS. Specifically, denote by z m t ∈ R + the number of active type m machines at time t, and δ m t ∈ R the change in the number of machines at time t. Similarly, define x mn t ∈ R + as the number of type n containers assigned to machines of type m that is capable of hosting container type n, and σ mn t ∈ R + the change in x mn t at time t, we have the following equations:</p><formula xml:id="formula_11">z m t+1 = z m t + δ m t (12) x mn t+1 = x mn t + σ mn t (<label>13</label></formula><formula xml:id="formula_12">)</formula><p>It is straightforward to show the relaxed version of CBS can be rewritten as the following problem called CBS -RELAX:</p><formula xml:id="formula_13">max δ m t ,σ mn t T t=0 m∈M -p t z m t E idle,m + r∈R n∈N α mr c nr c mr • x mn t + n∈N f n ( m∈M x mn t ) - T -1 t=0 m∈M q m |δ m t |<label>(14)</label></formula><p>subject to</p><formula xml:id="formula_14">z m t ≤ N m t ∀m ∈ M, t ∈ T (15) n∈N c r n x mn t ≤ z m t C mr ∀m ∈ M, r ∈ R, t ∈ T (16) x mn t , z m t ∈ R + ∀n ∈ N, m ∈ M, t ∈ T</formula><p>along with constraints ( <ref type="formula">12</ref>) and ( <ref type="formula" target="#formula_11">13</ref>). This problem is a convex optimization problem that can be solved using standard methods <ref type="bibr" target="#b5">[8]</ref>. Before introducing the control algorithm, We provide some properties of CBS: Proof: We rely on the property that the First-Fit (F F ) algorithm produces a solution in which at most one machine i is less than "half-full" (i.e., utilization u ir t ≤ 1 2 ∀r ∈ R). To see this, suppose this statement is false, i.e., there are two nonempty i, j ∈ N m t that are less than "half-full" and i is filled before j. In this case, when F F tries to pack a container that belongs to j in the solution, it would pack it in i instead. As a result, machine j should hold no containers, which contradicts our assumption. Therefore, given a machine i with utilization u ir t for resource type r ∈ R, define the effective utilization of i as 1 |R| r∈R u ir t . Based on this "half-full" property, F F ensures every machine has effective utilization at least 1 2|R| except the last non-empty machine.</p><p>Given x mn * t type n containers for each n ∈ N that can be scheduled on z m * t type m machines, the sum of the total effective utilization must be less than z m * t as it is the maximum possible utilization for z m * t machines. Now, suppose we scale down the number of type n containers to</p><formula xml:id="formula_15">x mn * t 2|R|</formula><p>for each n ∈ N , the total utilization of machines is thus at most</p><formula xml:id="formula_16">z m * t 2|R| .</formula><p>Suppose there are still containers waiting to be scheduled after using z m * t + 1 machines. As F F ensures every machine has effective utilization at least 1 2|R| except the last one, the total utilization of the z m * t + 1 machines is at least</p><formula xml:id="formula_17">z m * t 2|R|</formula><p>, which contradicts the fact that the total utilization is at most</p><formula xml:id="formula_18">z m * t 2|R| .</formula><p>Lemma 1 provides a simple algorithm for rounding the fractional solution of CBS -RELAX to an integer one using F F , such that at least</p><formula xml:id="formula_19">x mn t 2|R|</formula><p>containers of type n ∈ N are scheduled on z m * t + 1 machines for each m ∈ M and n ∈ N . Algorithm 1 summarizes our controller algorithm. When the control interval t starts, the controller uses the predicted values N n t+i|t ∀n ∈ N, 1 ≤ i ≤ W<ref type="foot" target="#foot_1">2</ref> to solve CBS -RELAX, which gives z m * t|t , the number of active type m machines to be made available at time t. Then the controller computes an integer solution by first reducing the number of type n containers to at most x mn t 2|R| and then add containers using F F to ensure the number of type n containers is at least</p><formula xml:id="formula_20">x mn * t 2|R|</formula><p>for all n ∈ N . Container reassignment (i.e., migration) is then performed to ensure there are at most z m * t|t + 1 machines to be active. In our formulation, container reassignment cost is modeled as part of the machine switching cost, as it is only used to allow machines to be turned off. The average switching cost can be obtained through experiments. Once the container reassignment is completed and there is still room for more containers, the controller is free to schedule additional containers as long as the total number of containers for each n ∈ N is at most x mn t . Finally, the controller will realize the Algorithm 1 Controller Algorithm for CBS 1: Provide initial state z m 0 , x mn 0 , t ← 0 2: loop 3: At beginning of control period t: Compute a re-packing configuration for all selected active machines 11:</p><p>Turn on selected machines, perform re-parking, turn off other machines 12:</p><p>t ← t + 1 Theorem 1 provides a bound on the worst case performance of CBS. In experiments, we have observed Algorithm 1 typically performs much better than the worst case bound. Furthermore, there are heuristics for finding better trade-offs between performance objectives U pref t and resource costs (E t + C sw t ). In particular, realizing the bin-packing solutions often cannot fully utilize the machine capacities, we can define an over-provisioning factor ω n ∈ R + for container type n to account for the bin-packing inefficiencies. ω n essentially captures how much extra resource is required to fully pack a given set of type n containers. To account for ω m , it suffices to replace constraint <ref type="bibr" target="#b13">(16)</ref> by the following constraint:</p><formula xml:id="formula_21">n∈N ω n c r n x mn t ≤ z m t C mr ∀m ∈ M, r ∈ R, t ∈ T (17)</formula><p>and run Algorithm 1 to find a suitable container placement. Finding a suitable value for ω n can be done through various methods, such as uniformly sampling the range [1, 2|R|] and selecting the one that produces the best solution among the sampled values for ω n . However, using ω n does not lead to a better performance guarantee. To see this, consider an example where N m t of type m machines that are selected by CBS -RELAX to be active. All other machines are inactive and have E idle ≈ ∞. In this case, no matter how we adjust the value of ω n , the number of containers scheduled of the algorithm will not improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION</head><p>In this section we discuss considerations related to the deployment of HARMONY in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task Classification</head><p>It is should be mentioned that many public cloud providers today (e.g. Amazon EC2 <ref type="bibr" target="#b0">[1]</ref>) already offer VMs in distinct types. In such a case, our DCP algorithms can be applied directly to these public clouds. However, we argue that predefined VM sizes may not match the actual need of each customer in all cases. This is reflected by the fact that workload heterogeneity is prevalent in private clouds such as Google's compute clusters, where customers are given the flexibility to choose desired VM size. In these cases, our approach is more flexible and can provide highly efficient solution for DCP for arbitrary workload compositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scheduler Design</head><p>Although CBS provides a theoretically-sound solution for DCP, it requires the scheduler to adopt a container-based scheduling algorithm, which is not always available in practice. As many production cloud systems (e.g., Google's compute cluster) have also developed sophisticated scheduler algorithms, implementing CBS requires major change to the scheduler design. To address this issue, in this section we propose a simple solution called Contained-Based Provisioning (CBP) that works with existing scheduling algorithms. Specifically, we solve CBS -RELAX to compute the machines to be provisioned at run-time, and round the fractional values of δ m t and σ mn t to their nearest integer values as the number of machines available to the scheduler. At run-time, the scheduler can retain its current scheduling algorithm (e.g., first fit) as long as it ensures the number of type n tasks assigned to type m machines is less than x mn t . The main benefit of CBP is its simplicity and practicality for deployment in existing systems. However, due to lack of control of the scheduler, CBP does not provide performance guarantee in terms of task scheduling delay. As a solution that can be readily deployed in practice, we also evaluate the performance of CBP in our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. SIMULATION STUDIES</head><p>We simulate a heterogeneous cluster composed of a mixture of servers from multiple manufacturers and models. Table <ref type="table" target="#tab_3">II</ref> provides the characteristics of the simulated servers. We normalized the CPU core count and memory capacity to the largest machine size. Hence, HP DLG585 G7 has a capacity 1 CPU unit and 1 memory unit, which corresponds to 48 cores and 64 GB, respectively. We used Equation 7 to model the energy consumption of the different machines. The parameters E idle,m and α mr for each type of servers were estimated using energy measurements available in <ref type="bibr" target="#b1">[2]</ref>. Figure <ref type="figure">9</ref> shows the energy consumption as function of CPU usage. Indeed, this figure demonstrates the importance of considering the machine heterogeneity when scheduling tasks/containers in order to reduce energy consumption. For instance, a container requiring 0.2 CPU unit should be placed in a HP DL385 G7 since the PowerEdge R210 does not have enough CPU capacity, whereas the other types of servers are able to host it but will consume much more energy. Selecting the "right" machines to switch on becomes even more challenging when millions of heterogeneous tasks have to be scheduled in the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results for Task Classification</head><p>We performed task classification as described in Section V. For each priority group, we varied the value of k and evaluated the quality of the resulting clusters produced by the K-means algorithm. The best value of k for each priority group is selected as the one for which no significant benefit can be achieved by increasing the value of k. The results after the first step of our characterization for each priority group are shown in Figure <ref type="figure" target="#fig_3">13</ref>, 15, and 17, respectively. It is evident that the clustering algorithm captures the differences in task sizes and identifies cpu-intensive tasks and memory-intensive tasks. Furthermore, the standard deviation is much less than the mean value for both CPU and memory, confirming the accuracy of the characterization. The number of tasks in each task class is shown in Figure <ref type="figure" target="#fig_3">10</ref>, 11 and 12, respectively. It is clear that the number of tasks within each cluster can vary significantly. Most of the classes have between 10 4 and 10 6 tasks except cluster 4 for Gratis priority group, which has only 100 tasks. Lastly, we run the k-means algorithm with k = 2 to categorize tasks of each task class as either short or long. The results are shown in Figure <ref type="figure" target="#fig_3">14</ref>, 16 and 18, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Controller Performance</head><p>We have evaluated the performance of CBS and CBP algorithms using Google workload traces. For comparison purpose, we have also implemented a baseline (heterogeneityoblivious) algorithm that finds the best trade-off between energy savings and scheduling delay by maintaining an 80% utilization of the bottleneck resource. It provisions machines in a "greedy" fashion by turning them on in decreasing order of energy efficiency. As the Google workload contains many long running tasks that were scheduled before the start of the traces, in our current simulation, we mainly focus on simulating the arrival of new tasks. The sum of arrival rate of tasks belonging to each priority group is shown in Figure <ref type="figure" target="#fig_3">19</ref>. Figure <ref type="figure">20</ref> shows the sum of the total containers belonging to each priority group computed by HARMONY. The number of active servers provisioned by the baseline algorithm and CBS/CBP are shown in Figure <ref type="figure" target="#fig_3">21</ref> and Figure <ref type="figure">22</ref>, respectively. Note that CBS and CBP provision the same number of machines as On the other hand, CBP is able to outperform the baseline algorithm, but generally performs worse than CBS due to lack of coordination with the scheduling algorithm. An intuitive explanation is that the baseline algorithm was unable to take advantage of heterogenous machines CPU and memory capacities for scheduling, resulting in inefficient schedules and long scheduling delay for large tasks. Finally, Figure <ref type="figure" target="#fig_6">26</ref> shows the total energy consumption of all three approaches. It can be seen that CBS incurs the lowest energy costs, corresponding to a 28% reduction in energy cost compared to the baseline algorithm.</p><p>X. CONCLUSION Dynamic capacity provisioning has become a promising solution for reducing energy consumption in data centers in recent years. However, existing work on this topic has not addressed a key challenge, which is the heterogeneity of both workloads and physical machines. In this paper, we first provide a characterization of both workload and machine heterogeneity found in one of Google's production compute clusters. Then we present HARMONY, a heterogeneity-aware framework that dynamically adjusts the number of machines to strike a balance between energy savings and scheduling delay, while considering the reconfiguration cost. Through experiments using Google workload traces, we found HARMONY can lead to up to 28 % energy savings while significantly improving task scheduling delay.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1: Total CPU demand</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig</head><label></label><figDesc>Fig. 5: Machine heterogeneity in compute cluster</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Task Size Analysis Fig. 8: System architecture</figDesc><graphic coords="4,362.69,71.62,190.52,95.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 1 .</head><label>1</label><figDesc>Given a fractional solution of CBS -RELAX with z m * t type m machines and x mn * t type n containers, a greedy first-fit algorithm can place x mn t 2|R| of each type of container n in z m * t + 1 machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t+i|t , p t+i|t for horizons t = 1, • • • , W using a demand prediction model 5:Solve CBP -RELAX to obtain δ m t+i|t ,σ mn t+i|t for i = 0, • • • , W -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 18 :</head><label>18</label><figDesc>Fig. 9: Machine Energy consumption Rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 26 :</head><label>26</label><figDesc>Fig. 19: Aggregated Task Arrival Rates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Table of Notations Change in num. of type n containers in type m machines at t</figDesc><table><row><cell>Symbol</cell><cell>Meaning</cell></row><row><cell>d i</cell><cell>Average scheduling delay for task class i</cell></row><row><cell>R s r i c r i C mr</cell><cell>Resource types Size of task i for resource type r Size of a container of type i for resource type r Capacity of a type m machine resource type r</cell></row><row><cell>E idle,m α mr</cell><cell>Energy consumption of a type m machine when idle Energy efficiency ratio of a type m machine for type r</cell></row><row><cell>u ir t y i t u i t a in t γ in t z m t δ m t x mn t σ mn t</cell><cell>Util. of machine i for resource type r at time t Boolean variable indicating machine i is active at time t Change in machine i's state at time t Num. of type n containers assigned to machine i at time t Change in num. of type n containers on machine i at time t Number of type m machines active at time t Change in the num. of type m machines at time t Num. of type n containers assigned to machine m at time t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Since the number of machines used is determined by CBS -RELAX, it is clear that the C sw</figDesc><table><row><cell>13: end loop</cell></row><row><cell>new configuration by actually turning off unused machines and</cell></row><row><cell>making container allocations.</cell></row><row><cell>Theorem 1. The integer solution produced by Algorithm 1 ensures U pref t -E t -C sw t ≥ ( 1 2|R| -)U pref  *  t -(1 + )(E  *  t -C sw *  t ) when z m t is sufficiently large for all m ∈ M , t ∈ T .</cell></row><row><cell>Proof: t E idle t = E idle *  . As the number of type n containers scheduled = C sw *  and t t on type m machines is upper-bounded by x mn *  , we have t E util t ≤ E util *  t . Finally, by Lemma 1, it is easy to show that x mn *  t 2|R| • z m *  t -1 z m *  containers of each type n ∈ N can be t packed in z m *  t machines. As f (•) is convex function, it must hold that U pref t ≥ (max m { z m *  t -1 z m *  t } -) • 1 t •, where 2|R| U pref  *  = max m { x mn *  t 2|R| • z m t -1 z m t -x mn *  t 2|R| • z m t -1 z m } is the rounding t error. The theorem is proven by defining = max m { 1 z m } + t and summing the above equations.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Machine Configurations</figDesc><table><row><cell>Model</cell><cell>Num.</cell><cell cols="2">Num. Memory</cell><cell>Num.</cell></row><row><cell></cell><cell cols="4">of Processors of Cores Memory of Machines</cell></row><row><cell>Dell PowerEdge R210</cell><cell>1</cell><cell>4</cell><cell>4 GB</cell><cell>7000</cell></row><row><cell>Dell PowerEdge R515</cell><cell>2</cell><cell>6</cell><cell>32 GB</cell><cell>1500</cell></row><row><cell>HP DL385 G7</cell><cell>2</cell><cell>12</cell><cell>16 GB</cell><cell>1000</cell></row><row><cell>HP DL585 G7</cell><cell>4</cell><cell>12</cell><cell>64 GB</cell><cell>500</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It should be mentioned that the same dataset has been analyzed by Reiss et. al.<ref type="bibr" target="#b14">[17]</ref>. However, our analysis extends, and largely complements the results in<ref type="bibr" target="#b14">[17]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use (t + i|t) to denote future value for time t + i either predicted or computed at time t</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT This work was supported in part by the Natural Science and Engineering Council of Canada (NSERC) under the Smart Applications on Virtual Infrastructure (SAVI) Research Network, and in part by a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://aws.amazon.com/ec2/" />
		<title level="m">Amazon elastic computing cloud</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Energy star computer server qualified product list. energystar.gov/ia/products/prod lists/enterprise servers prod list.xls</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://code.google.com/p/googleclusterdata/" />
		<title level="m">Googleclusterdata traces of google workloads</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On cloud computational models and the heterogeneity challenge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Internet Services and App</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<title level="m">Time Series Analysis, Forecasting, and Control</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On multi-dimensional packing problems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Discrete algorithms</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Analysis and lessons from a publicly available Google cluster trace</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>UCB/EECS-2010-95</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A guide to concentration bounds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on Randomized Computing</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Google file system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fundamentals of queueing theory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="244" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mistral: Dynamically managing power, performance, and adaptation cost in cloud infrastructures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hiltunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Schlichting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDCS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards characterizing cloud backend workloads: insights from Google compute clusters. SIGMETRICS Perform</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cirne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eval. Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2010-03">March 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cutting the electric bill for Internet-scale systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heterogeneity and dynamicity of clouds at scale: Google trace analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Comp</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Provably-efficient job scheduling for energy and fairness in geographically distributed data centers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDCS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling and synthesizing task placement constraints in google compute clusters</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Comp</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">pmapper: power and migration cost aware application placement in virtualized systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<editor>Middleware</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Characterizing task usage shapes in Google&apos;s compute clusters</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LADIS Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic energy-aware capacity provisioning for cloud computing environments</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Zhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boutaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/ACM International Conference on Autonomic Computing (ICAC)</title>
		<meeting>of the IEEE/ACM International Conference on Autonomic Computing (ICAC)</meeting>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
