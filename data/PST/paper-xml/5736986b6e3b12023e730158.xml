<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Global Linear Convergence of Frank-Wolfe Optimization Variants</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA -SIERRA project-team École Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science ETH</orgName>
								<address>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Global Linear Convergence of Frank-Wolfe Optimization Variants</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">07ABFEBBA37C39DB6FB1CB2CC9838E9D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple lessknown fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that have been successfully applied in practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Frank-Wolfe algorithm <ref type="bibr" target="#b8">[9]</ref> (also known as conditional gradient) is one of the earliest existing methods for constrained convex optimization, and has seen an impressive revival recently due to its nice properties compared to projected or proximal gradient methods, in particular for sparse optimization and machine learning applications.</p><p>On the other hand, the classical projected gradient and proximal methods have been known to exhibit a very nice adaptive acceleration property, namely that the the convergence rate becomes linear for strongly convex objective, i.e. that the optimization error of the same algorithm after t iterations will decrease geometrically with O((1 -ρ) t ) instead of the usual O(1/t) for general convex objective functions. It has become an active research topic recently whether such an acceleration is also possible for Frank-Wolfe type methods.</p><p>Contributions. We clarify several variants of the Frank-Wolfe algorithm and show that they all converge linearly for any strongly convex function optimized over a polytope domain, with a constant bounded away from zero that only depends on the geometry of the polytope. Our analysis does not depend on the location of the true optimum with respect to the domain, which was a disadvantage of earlier existing results such as <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5]</ref>, and the newer work of <ref type="bibr" target="#b27">[28]</ref>, as well as the line of work of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> which rely on Robinson's condition <ref type="bibr" target="#b29">[30]</ref>. Our analysis yields a weaker sufficient condition than Robinson's condition; in particular we can have linear convergence even in some cases when the function has more than one global minima, and is not globally strongly convex. The constant also naturally separates as the product of the condition number of the function with a novel notion of condition number of a polytope, which might have applications in complexity theory. Related Work. For the classical Frank-Wolfe algorithm, <ref type="bibr" target="#b4">[5]</ref> showed a linear rate for the special case of quadratic objectives when the optimum is in the strict interior of the domain, a result already subsumed by the more general <ref type="bibr" target="#b11">[12]</ref>. The early work of <ref type="bibr" target="#b22">[23]</ref> showed linear convergence for strongly convex constraint sets, under the strong requirement that the gradient norm is not too small (see <ref type="bibr" target="#b10">[11]</ref> for a discussion). The away-steps variant of the Frank-Wolfe algorithm, that can also remove weight from 'bad' atoms in the current active set, was proposed in <ref type="bibr" target="#b33">[34]</ref>, and later also analyzed in <ref type="bibr" target="#b11">[12]</ref>. The precise method is stated below in Algorithm 1. <ref type="bibr" target="#b11">[12]</ref> showed a (local) linear convergence rate on polytopes, but the constant unfortunately depends on the distance between the solution and its relative boundary, a quantity that can be arbitrarily small. More recently, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref> have obtained linear convergence results in the case that the optimum solution satisfies Robinson's condition <ref type="bibr" target="#b29">[30]</ref>. In a different recent line of work, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref> have studied a variation of FW that repeatedly moves mass from the worst vertices to the standard FW vertex until a specific condition is satisfied, yielding a linear rate on strongly convex functions. Their algorithm requires the knowledge of several constants though, and moreover is not adaptive to the best-case scenario, unlike the Frank-Wolfe algorithm with away steps and line-search. None of these previous works was shown to be affine invariant, and most require additional knowledge about problem specific parameters.</p><p>Setup. We consider general constrained convex optimization problems of the form:</p><formula xml:id="formula_0">min x∈M f (x) , M = conv(A),</formula><p>with only access to:</p><formula xml:id="formula_1">LMO A (r) ∈ arg min x∈A r, x ,<label>(1)</label></formula><p>where A ⊆ R d is a finite set of vectors that we call atoms. <ref type="foot" target="#foot_0">1</ref> We assume that the function f is µstrongly convex with L-Lipschitz continuous gradient over M. We also consider weaker conditions than strong convexity for f in Section 4. As A is finite, M is a (convex and bounded) polytope. The methods that we consider in this paper only require access to a linear minimization oracle LMO A (.) associated with the domain M through a generating set of atoms A. This oracle is defined as to return a minimizer of a linear subproblem over M = conv(A), for any given direction r ∈ R d . <ref type="foot" target="#foot_1">2</ref>Examples. Optimization problems of the form (1) appear widely in machine learning and signal processing applications. The set of atoms A can represent combinatorial objects of arbitrary type. Efficient linear minimization oracles often exist in the form of dynamic programs or other combinatorial optimization approaches. As an example from tracking in computer vision, A could be the set of integer flows on a graph <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7]</ref>, where LMO A can be efficiently implemented by a minimum cost network flow algorithm. In this case, M can also be described with a polynomial number of linear inequalities. But in other examples, M might not have a polynomial description in terms of linear inequalities, and testing membership in M might be much more expensive than running the linear oracle. This is the case when optimizing over the base polytope, an object appearing in submodular function optimization <ref type="bibr" target="#b2">[3]</ref>. There, the LMO A oracle is a simple greedy algorithm. Another example is when A represents the possible consistent value assignments on cliques of a Markov random field (MRF); M is the marginal polytope <ref type="bibr" target="#b31">[32]</ref>, where testing membership is NP-hard in general, though efficient linear oracles exist for some special cases <ref type="bibr" target="#b16">[17]</ref>. Optimization over the marginal polytope appears for example in structured SVM learning <ref type="bibr" target="#b20">[21]</ref> and variational inference <ref type="bibr" target="#b17">[18]</ref>.</p><p>The Original Frank-Wolfe Algorithm. The Frank-Wolfe (FW) optimization algorithm <ref type="bibr" target="#b8">[9]</ref>, also known as conditional gradient <ref type="bibr" target="#b22">[23]</ref>, is particularly suited for the setup <ref type="bibr" target="#b0">(1)</ref> where M is only accessed through the linear minimization oracle. It works as follows: At a current iterate x (t) , the algorithm finds a feasible search atom s t to move towards by minimizing the linearization of the objective function f over M (line 3 in Algorithm 1) -this is where the linear minimization oracle LMO A is used. The next iterate x (t+1) is then obtained by doing a line-search on f between x (t) and s t (line 11 in Algorithm 1). One reason for the recent increased popularity of Frank-Wolfe-type algorithms is the sparsity of their iterates: in iteration t of the algorithm, the iterate can be represented as a sparse convex combination of at most t + 1 atoms S (t) ⊆ A of the domain M, which we write as</p><formula xml:id="formula_2">x (t) = v∈S (t) α (t) v v.</formula><p>We write S (t) for the active set, containing the previously discovered search atoms s r for r &lt; t that have non-zero weight α (t) sr &gt; 0 in the expansion (potentially also including the starting point x (0) ). While tracking the active set S (t) is not necessary for the original FW algorithm, the improved variants of FW that we discuss will require that S (t) is maintained.</p><p>Zig-Zagging Phenomenon. When the optimal solution lies at the boundary of M, the convergence rate of the iterates is slow, i.e. sublinear: f (x (t) ) -f (x * ) ≤ O 1/t , for x * being an optimal solution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>. This is because the iterates of the classical FW algorithm start to zig-zag</p><formula xml:id="formula_3">x ⇤ x (t) x (0) x (t+1) s t x ⇤ x (t) x (0) v t s t x (t+1) x ⇤ x (t) x (0) v t s t / /</formula><p>x (t+1) / / between the vertices defining the face containing the solution x * (see left of Figure <ref type="figure" target="#fig_0">1</ref>). In fact, the 1/t rate is tight for a large class of functions: Canon and Cullum <ref type="bibr" target="#b5">[6]</ref>, Wolfe <ref type="bibr" target="#b33">[34]</ref> showed (roughly) that f (x (t) ) -f (x * ) ≥ Ω 1/t 1+δ for any δ &gt; 0 when x * lies on a face of M with some additional regularity assumptions. Note that this lower bound is different than the Ω 1/t one presented in [15, <ref type="bibr">Lemma 3]</ref> which holds for all one-atom-per-step algorithms but assumes high dimensionality d ≥ t.</p><p>1 Improved Variants of the Frank-Wolfe Algorithm Algorithm 1 Away-steps Frank-Wolfe algorithm: AFW(x (0) , A, )</p><p>1: Let x (0) ∈ A, and S (0) := {x (0) } (so that α</p><formula xml:id="formula_4">(0) v = 1 for v = x (0)</formula><p>and 0 otherwise)</p><formula xml:id="formula_5">2: for t = 0 . . . T do 3: Let s t := LMO A ∇f (x (t) ) and d FW t := s t -x (t) (the FW direction) 4: Let v t ∈ arg max v∈S (t) ∇f (x (t) ), v and d A t := x (t) -v t (the away direction) 5: if g FW t := -∇f (x (t) ), d FW t ≤ then return x (t)</formula><p>(FW gap is small enough, so return) Line-search:</p><formula xml:id="formula_6">6: if -∇f (x (t) ), d FW t ≥ -∇f (x (t) ), d A</formula><formula xml:id="formula_7">γ t ∈ arg min γ∈[0,γmax] f x (t) + γd t 12:</formula><p>Update x (t+1) := x (t) + γ t d t (and accordingly for the weights α (t+1) , see text)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Update S (t+1) := {v ∈ A s.t. α (t+1) v &gt; 0} 14: end for Algorithm 2 Pairwise Frank-Wolfe algorithm: PFW(x (0) , A, ) 1: . . . as in Algorithm 1, except replacing lines 6 to 10 by: d t = d PFW t := s t -v t , and γ max := α vt .</p><p>Away-Steps Frank-Wolfe. To address the zig-zagging problem of FW, Wolfe <ref type="bibr" target="#b33">[34]</ref> proposed to add the possibility to move away from an active atom in S (t) (see middle of Figure <ref type="figure" target="#fig_0">1</ref>); this simple modification is sufficient to make the algorithm linearly convergent for strongly convex functions. We describe the away-steps variant of Frank-Wolfe in Algorithm 1. <ref type="foot" target="#foot_2">3</ref> The away direction d A t is defined in line 4 by finding the atom v t in S (t) that maximizes the potential of descent given by</p><formula xml:id="formula_8">g A t := -∇f (x (t) ), x (t) -v t .</formula><p>Note that this search is over the (typically small) active set S (t) , and is fundamentally easier than the linear oracle LMO A . The maximum step-size γ max as defined on line 9 ensures that the new iterate x (t) + γd A t stays in M. In fact, this guarantees that the convex representation is maintained, and we stay inside conv(S (t) ) ⊆ M. When M is a simplex, then the barycentric coordinates are unique and x (t) + γ max d A t truly lies on the boundary of M. On the other hand, if |A| &gt; dim(M) + 1 (e.g. for the cube), then it could hypothetically be possible to have a step-size bigger than γ max which is still feasible. Computing the true maximum feasible step-size would require the ability to know when we cross the boundary of M along a specific line, which is not possible for general M. Using the conservative maximum step-size of line 9 ensures that we do not need this more powerful oracle. This is why Algorithm 1 requires to maintain S (t) (unlike standard FW). Finally, as in classical FW, the FW gap g FW t is an upper bound on the unknown suboptimality, and can be used as a stopping criterion:</p><formula xml:id="formula_9">g FW t := -∇f (x (t) ), d FW t ≥ -∇f (x (t) ), x * -x (t) ≥ f (x (t) ) -f (x * ) (by convexity).</formula><p>If γ t = γ max , then we call this step a drop step, as it fully removes the atom v t from the currently active set of atoms S (t) (by settings its weight to zero). The weight updates for lines 12 and 13 are of the following form: For a FW step, we have S (t+1) = {s t } if γ t = 1; otherwise S (t+1) = S (t) ∪{s t }. Also, we have α</p><formula xml:id="formula_10">(t+1) st := (1-γ t )α (t) st +γ t and α (t+1) v := (1-γ t )α (t) v for v ∈ S (t) \{s t }.</formula><p>For an away step, we have S (t+1) = S (t) \ {v t } if γ t = γ max (a drop step); otherwise S (t+1) = S (t) . Also, we have α</p><formula xml:id="formula_11">(t+1) vt := (1 + γ t )α (t) vt -γ t and α (t+1) v := (1 + γ t )α (t) v for v ∈ S (t) \ {v t }.</formula><p>Pairwise Frank-Wolfe. The next variant that we present is inspired by an early algorithm by Mitchell et al. <ref type="bibr" target="#b24">[25]</ref>, called the MDM algorithm, originally invented for the polytope distance problem. Here the idea is to only move weight mass between two atoms in each step. More precisely, the generalized method as presented in Algorithm 2 moves weight from the away atom v t to the FW atom s t , and keeps all other α weights un-changed. We call such a swap of mass between the two atoms a pairwise FW step, i.e. α</p><formula xml:id="formula_12">(t+1) vt = α (t) vt -γ and α (t+1) st = α (t) st + γ for some step-size γ ≤ γ max := α (t)</formula><p>vt . In contrast, classical FW shrinks all active weights at every iteration. The pairwise FW direction will also be central to our proof technique to provide the first global linear convergence rate for away-steps FW, as well as the fully-corrective variant and Wolfe's minnorm-point algorithm.</p><p>As we will see in Section 2.2, the rate guarantee for the pairwise FW variant is more loose than for the other variants, because we cannot provide a satisfactory bound on the number of the problematic swap steps (defined just before Theorem 1). Nevertheless, the algorithm seems to perform quite well in practice, often outperforming away-steps FW, especially in the important case of sparse solutions, that is if the optimal solution x * lies on a low-dimensional face of M (and thus one wants to keep the active set S (t) small). The pairwise FW step is arguably more efficient at pruning the coordinates in S (t) . In contrast to the away step which moves the mass back uniformly onto all other active elements S (t) (and might require more corrections later), the pairwise FW step only moves the mass onto the (good) FW atom s t . A slightly different version than Algorithm 2 was also proposed by Ñanculef et al. <ref type="bibr" target="#b25">[26]</ref>, though their convergence proofs were incomplete (see Appendix A.3). The algorithm is related to classical working set algorithms, such as the SMO algorithm used to train SVMs <ref type="bibr" target="#b28">[29]</ref>. We refer to <ref type="bibr" target="#b25">[26]</ref> for an empirical comparison for SVMs, as well as their Section 5 for more related work. See also Appendix A.3 for a link between pairwise FW and <ref type="bibr" target="#b9">[10]</ref>.</p><p>Fully-Corrective Frank-Wolfe, and Wolfe's Min-Norm Point Algorithm. When the linear oracle is expensive, it might be worthwhile to do more work to optimize over the active set S (t) in between each call to the linear oracle, rather than just performing an away or pairwise step. We give in Algorithm 3 the fully-corrective Frank-Wolfe (FCFW) variant, that maintains a correction polytope defined by a set of atoms A (t) (potentially larger than the active set S (t) ). Rather than obtaining the next iterate by line-search, x (t+1) is obtained by re-optimizing f over conv(A (t) ). Depending on how the correction is implemented, and how the correction atoms A (t) are maintained, several variants can be obtained. These variants are known under many names, such as the extended FW method by Holloway <ref type="bibr" target="#b13">[14]</ref> or the simplicial decomposition method <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>. Wolfe's min-norm point (MNP) algorithm <ref type="bibr" target="#b34">[35]</ref> for polytope distance problems is often confused with FCFW for quadratic objectives. The major difference is that standard FCFW optimizes f over conv(A (t) ), whereas MNP implements the correction as a sequence of affine projections that potentially yield a different update, but can be computed more efficiently in several practical applications <ref type="bibr" target="#b34">[35]</ref>. We describe precisely in Appendix A.1 a generalization of the MNP algorithm as a specific case of the correction subroutine from step 7 of the generic Algorithm 3. The original convergence analysis of the FCFW algorithm <ref type="bibr" target="#b13">[14]</ref> (and also MNP algorithm <ref type="bibr" target="#b34">[35]</ref>) only showed that they were finitely convergent, with a bound on the number of iterations in terms of the cardinality of A (unfortunately an exponential number in general). Holloway <ref type="bibr" target="#b13">[14]</ref> also argued that FCFW had an asymptotic linear convergence based on the flawed argument of Wolfe <ref type="bibr" target="#b33">[34]</ref>. As far as we know, our work is the first to provide global linear convergence rates for FCFW and MNP for Algorithm 3 Fully-corrective Frank-Wolfe with approximate correction: FCFW(x (0) , A, ) 1: Input: Set of atoms A, active set S (0) , starting point</p><formula xml:id="formula_13">x (0) = v∈S (0) α (0)</formula><p>v v, stopping criterion .</p><p>2: Let A (0) := S (0) (optionally, a bigger A (0) could be passed as argument for a warm start) 3: for t = 0 . . . T do 4:</p><p>Let s t := LMO A ∇f (x (t) )</p><p>(the FW atom)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Let</p><formula xml:id="formula_14">d FW t := s t -x (t) and g FW t = -∇f (x (t) ), d FW t (FW gap) 6: if g FW t ≤ then return x (t)</formula><p>7:</p><p>(x (t+1) , A (t+1) ) := Correction(x (t) , A (t) , s t , )</p><p>(approximate correction step)</p><p>8: end for Algorithm 4 Approximate correction: Correction(x (t) , A (t) , s t , )</p><p>1: Return (x (t+1) , A (t+1) ) with the following properties:</p><p>2:</p><p>S (t+1) is the active set for x (t+1) and A (t+1) ⊇ S (t+1) .</p><p>3:</p><formula xml:id="formula_15">f (x (t+1) ) ≤ min γ∈[0,1] f x (t) + γ(s t -x (t)</formula><p>)</p><p>(make at least as much progress as a FW step) 4:</p><formula xml:id="formula_16">g A t+1 := max v∈S (t+1) -∇f (x (t+1) ), x (t+1) -v ≤</formula><p>(the away gap is small enough)</p><p>general strongly convex functions. Moreover, the proof of convergence for FCFW does not require an exact solution to the correction step; instead, we show that the weaker properties stated for the approximate correction procedure in Algorithm 4 are sufficient for a global linear convergence rate (this correction could be implemented using away-steps FW, as done for example in <ref type="bibr" target="#b17">[18]</ref>).</p><p>2 Global Linear Convergence Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intuition for the Convergence Proofs</head><p>We first give the general intuition for the linear convergence proof of the different FW variants, starting from the work of Guélat and Marcotte <ref type="bibr" target="#b11">[12]</ref>. We assume that the objective function f is smooth over a compact set M, i.e. its gradient is Lipschitz continuous with constant L. Also let M := diam(M). Let d t be the direction in which the line-search is executed by the algorithm (Line 11 in Algorithm 1). By the standard descent lemma [see e.g. (1.2.5) in 27], we have:</p><formula xml:id="formula_17">f (x (t+1) ) ≤ f (x (t) + γd t ) ≤ f (x (t) ) + γ ∇f (x (t) ), d t + γ 2 2 L d t 2 ∀γ ∈ [0, γ max ].<label>(2)</label></formula><p>We let r t := -∇f (x (t) ) and let h t := f (x (t) ) -f (x * ) be the suboptimality error. Supposing for now that γ max ≥ γ * t := r t , d t /(L d t 2 ). We can set γ = γ * t to minimize the RHS of (2), subtract f (x * ) on both sides, and re-organize to get a lower bound on the progress:</p><formula xml:id="formula_18">h t -h t+1 ≥ r t , d t 2 2L d t 2 = 1 2L r t , dt 2 ,<label>(3)</label></formula><p>where we use the 'hat' notation to denote normalized vectors: dt := d t / d t . Let e t := x * -x (t) be the error vector. By µ-strong convexity of f , we have:</p><formula xml:id="formula_19">f (x (t) + γe t ) ≥ f (x (t) ) + γ ∇f (x (t) ), e t + γ 2 2 µ e t 2 ∀γ ∈ [0, 1]. (<label>4</label></formula><formula xml:id="formula_20">)</formula><p>The RHS is lower bounded by its minimum as a function of γ (unconstrained), achieved using γ := r t , e t /(µ e t 2 ). We are then free to use any value of γ on the LHS and maintain a valid bound. In particular, we use γ = 1 to obtain f (x * ). Again re-arranging, we get:</p><formula xml:id="formula_21">h t ≤ r t , êt 2 2µ</formula><p>, and combining with (3), we obtain:</p><formula xml:id="formula_22">h t -h t+1 ≥ µ L r t , dt 2 r t , êt 2 h t .</formula><p>(5) The inequality ( <ref type="formula">5</ref>) is fairly general and valid for any line-search method in direction d t . To get a linear convergence rate, we need to lower bound (by a positive constant) the term in front of h t on the RHS, which depends on the angle between the update direction d t and the negative gradient r t . If we assume that the solution x * lies in the relative interior of M with a distance of at least δ &gt; 0 from the boundary, then r t , d t ≥ δ r t for the FW direction d FW t , and by combining with d t ≤ M , we get a linear rate with constant 1 -µ L ( δ M ) 2 (this was the result from <ref type="bibr" target="#b11">[12]</ref>). On the other hand, if x * lies on the boundary, then rt , dt gets arbitrary close to zero for standard FW (the zig-zagging phenomenon) and the convergence is sublinear.</p><p>Proof Sketch for AFW. The key insight to prove the global linear convergence for AFW is to relate r t , d t with the pairwise FW direction d PFW t := s t -v t . By the way the direction d t is chosen on lines 6 to 10 of Algorithm 1, we have:</p><formula xml:id="formula_23">2 r t , d t ≥ r t , d FW t + r t , d A t = r t , d FW t + d A t = r t , d PFW t . (<label>6</label></formula><formula xml:id="formula_24">)</formula><p>We thus have r t , d t ≥ r t , d PFW t /2. Now the crucial property of the pairwise FW direction is that for any potential negative gradient direction r t , the worst case inner product rt , d PFW</p><formula xml:id="formula_25">t d FW t d A t d pFW t x v t s t r t</formula><p>can be lower bounded away from zero by a quantity depending only the geometry of M (unless we are at the optimum). We call this quantity the pyramidal width of A. The figure on the right shows the six possible pairwise FW directions d PFW t for a triangle domain, depending on which colored area the r t direction falls into. We will see that the pyramidal width is related to the smallest width of pyramids that we can construct from A in a specific way related to the choice of the away and towards atoms v t and s t . See ( <ref type="formula" target="#formula_31">9</ref>) and our main Theorem 3 in Section 3. This gives the main argument for the linear convergence of AFW for steps where γ * t ≤ γ max . When γ max is too small, AFW will perform a drop step, as the line-search will truncate the step-size to γ t = γ max . We cannot guarantee sufficient progress in this case, but the drop step decreases the active set size by one, and thus they cannot happen too often (not more than half the time). These are the main elements for the global linear convergence proof for AFW. The rest is to carefully consider various boundary cases. We can re-use the same techniques to prove the convergence for pairwise FW, though unfortunately the latter also has the possibility of problematic swap steps. While their number can be bounded, so far we only found the extremely loose bound quoted in Theorem 1.</p><p>Proof Sketch for FCFW. For FCFW, by line 4 of the correction Algorithm 4, the away gap satisfies g A t ≤ at the beginning of a new iteration. Supposing that the algorithm does not exit at line 6 of Algorithm 3, we have g FW t &gt; and therefore 2 r t , d FW t ≥ r t , d PFW t using a similar argument as in <ref type="bibr" target="#b5">(6)</ref>. Finally, by line 3 of Algorithm 4, the correction is guaranteed to make at least as much progress as a line-search in direction d FW t , and so the progress bound (5) applies also to FCFW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convergence Results</head><p>We now give the global linear convergence rates for the four variants of the FW algorithm: awaysteps FW (AFW Alg. 1); pairwise FW (PFW Alg. 2); fully-corrective FW (FCFW Alg. 3 with approximate correction Alg. 4); and Wolfe's min-norm point algorithm (Alg. 3 with MNP-correction as Alg. 5 in Appendix A.1). For the AFW, MNP and PFW algorithms, we call a drop step when the active set shrinks |S (t+1) | &lt; |S (t) |. For the PFW algorithm, we also have the possibility of a swap step where γ t = γ max but |S (t+1) | = |S (t) | (i.e. the mass was fully swapped from the away atom to the FW atom). A nice property of FCFW is that it does not have any drop step (it executes both FW steps and away steps simultaneously while guaranteeing enough progress at every iteration). Theorem 1. Suppose that f has L-Lipschitz gradient<ref type="foot" target="#foot_3">4</ref> and is µ-strongly convex over M = conv(A). Let M = diam(M) and δ = P Width(A) as defined by <ref type="bibr" target="#b8">(9)</ref>. Then the suboptimality h t of the iterates of all the four variants of the FW algorithm decreases geometrically at each step that is not a drop step nor a swap step (i.e. when γ t &lt; γ max , called a 'good step'), that is</p><formula xml:id="formula_26">h t+1 ≤ (1 -ρ) h t ,</formula><p>where</p><formula xml:id="formula_27">ρ := µ 4L δ M 2 .</formula><p>Let k(t) be the number of 'good steps' up to iteration t. We have k(t) = t for FCFW; k(t) ≥ t/2 for MNP and AFW; and k(t) ≥ t/(3|A|! + 1) for PFW (because of the swap steps). This yields a global linear convergence rate of h t ≤ h 0 exp(-ρ k(t)) for all variants. If µ = 0 (general convex), then h t = O(1/k(t)) instead. See Theorem 8 in Appendix D for an affine invariant version and proof.</p><p>Note that to our knowledge, none of the existing linear convergence results showed that the duality gap was also linearly convergent. The result for the gap follows directly from the simple manipulation of (2); putting the FW gap to the LHS and optimizing the RHS for γ ∈ [0, 1]. Theorem 2. Suppose that f has L-Lipschitz gradient over M with M := diam(M). Then the FW gap g FW t for any algorithm is upper bounded by the primal error h t as follows:</p><formula xml:id="formula_28">g FW t ≤ h t + LM 2 /2 when h t &gt; LM 2 /2, g FW t ≤ M 2h t L otherwise .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pyramidal Width</head><p>We now describe the claimed lower bound on the angle between the negative gradient and the pairwise FW direction, which depends only on the geometric properties of M. According to our argument about the progress bound (5) and the PFW gap (6), our goal is to find a lower bound on</p><formula xml:id="formula_29">r t , d PFW t / r t , êt . First note that r t , d PFW t = r t , s t -v t = max s∈M,v∈S (t)</formula><p>r t , sv where S (t) is a possible active set for x (t) . This looks like the directional width of a pyramid with base S (t) and summit s t . To be conservative, we consider the worst case possible active set for x (t) ; this is what we will call the pyramid directional width P dirW (A, r t , x (t) ). We start with the following definitions. Directional Width. The directional width of a set A with respect to a direction r is defined as dirW (A, r) := max s,v∈A r r , sv . The width of A is the minimum directional width over all possible directions in its affine hull. Pyramidal Directional Width. We define the pyramidal directional width of a set A with respect to a direction r and a base point x ∈ M to be P dirW (A, r, x) := min</p><formula xml:id="formula_30">S∈Sx dirW (S ∪ {s(A, r)}, r) = min S∈Sx max s∈A,v∈S r r , s -v ,<label>(8)</label></formula><p>where S x := {S | S ⊆ A such that x is a proper<ref type="foot" target="#foot_4">5</ref> convex combination of all the elements in S}, and s(A, r) := arg max v∈A r, v is the FW atom used as a summit.</p><p>Pyramidal Width. To define the pyramidal width of a set, we take the minimum over the cone of possible feasible directions r (in order to avoid the problem of zero width). A direction r is feasible for A from x if it points inwards conv(A), (i.e. r ∈ cone(A -x)).</p><p>We define the pyramidal width of a set A to be the smallest pyramidal width of all its faces, i.e. P Width(A) := min</p><formula xml:id="formula_31">K∈faces(conv(A)) x∈K r∈cone(K-x)\{0} P dirW (K ∩ A, r, x).<label>(9)</label></formula><p>Theorem 3. Let x ∈ M = conv(A) be a suboptimal point and S be an active set for x. Let x * be an optimal point and corresponding error direction ê = (x * -x)/ x * -x , and negative gradient r := -∇f (x) (and so r, ê &gt; 0). Let d = s-v be the pairwise FW direction obtained over A and S with negative gradient r. Then r, d r, ê ≥ P Width(A).</p><p>(10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Properties of Pyramidal Width and Consequences</head><p>Examples of Values. The pyramidal width of a set A is lower bounded by the minimal width over all subsets of atoms, and thus is strictly greater than zero if the number of atoms is finite. On the other hand, this lower bound is often too loose to be useful, as in particular, vertex subsets of the unit cube in dimension d can have exponentially small width O(d -d 2 ) [see <ref type="bibr">Corollary 27 in 36]</ref>. On the other hand, as we show here, the pyramidal width of the unit cube is actually 1/ √ d, justifying why we kept the tighter but more involved definition <ref type="bibr" target="#b8">(9)</ref>. See Appendix B.1 for the proof. For the probability simplex with d vertices, the pyramidal width is actually the same as its width, which is 2/ √ d when d is even, and 2/ d-1/d when d is odd <ref type="bibr" target="#b1">[2]</ref> (see Appendix B.1). In contrast, the pyramidal width of an infinite set can be zero. For example, for a curved domain, the set of active atoms S can contain vertices forming a very narrow pyramid, yielding a zero width in the limit.</p><p>Condition Number of a Set. The inverse of the rate constant ρ appearing in Theorem 1 is the product of two terms: L/µ is the standard condition number of the objective function appearing in the rates of gradient methods in convex optimization. The second quantity (M/δ) 2 (diameter over pyramidal width) can be interpreted as a condition number of the domain M, or its eccentricity. The more eccentric the constraint set (large diameter compared to its pyramidal width), the slower the convergence. The best condition number of a function is when its level sets are spherical; the analog in term of the constraint sets is actually the regular simplex, which has the maximum widthto-diameter ratio amongst all simplices [see Corollary 1 in 2]. Its eccentricity is (at most) d/2. In contrast, the eccentricity of the unit cube is d 2 , which is much worse.</p><p>We conjecture that the pyramidal width of a set of vertices (i.e. extrema of their convex hull) is non-increasing when another vertex is added (assuming that all previous points remain vertices). For example, the unit cube can be obtained by iteratively adding vertices to the regular probability simplex, and the pyramidal width thereby decreases from 2/ √ d to 1/ √ d. This property could provide lower bounds for the pyramidal width of more complicated polytopes, such as 1/ √ d for the d-dimensional marginal polytope, as it can be obtained by removing vertices from the unit cube.</p><p>Complexity Lower Bounds. Combining the convergence Theorem 1 and the condition number of the unit simplex, we get a complexity of O(d L µ log( <ref type="formula" target="#formula_1">1</ref>)) to reach -accuracy when optimizing a strongly convex function over the unit simplex. Here the linear dependence on d should not come as a surprise, in view of the known lower bound of 1/t for t ≤ d for Frank-Wolfe type methods <ref type="bibr" target="#b14">[15]</ref>.</p><p>Applications to Submodular Minimization. See Appendix A.2 for a consequence of our linear rate for the popular MNP algorithm for submodular function optimization (over the base polytope).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Non-Strongly Convex Generalization</head><p>Building on the work of Beck and Shtern <ref type="bibr" target="#b3">[4]</ref> and Wang and Lin <ref type="bibr" target="#b32">[33]</ref>, we can generalize our global linear convergence results for all Frank-Wolfe variants for the more general case where f (x) := g(Ax) + b, x , for A ∈ R p×d , b ∈ R d and where g is µ g -strongly convex and continuously differentiable over AM. We note that for a general matrix A, f is convex but not necessarily strongly convex. In this case, the linear convergence still holds but with the constant µ appearing in the rate of Theorem 1 replaced with the generalized constant μ appearing in Lemma 9 in Appendix F. We illustrate the performance of the presented algorithm variants in two numerical experiments, shown in Figure <ref type="figure" target="#fig_3">2</ref>. The first example is a constrained Lasso problem ( 1 -regularized least squares regression), that is min x∈M f (x) = Axb</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Illustrative Experiments</head><p>2 , with M = 20 • L 1 a scaled L 1 -ball. We used a random Gaussian matrix A ∈ R 200×500 , and a noisy measurement b = Ax * with x * being a sparse vector with 50 entries ±1, and 10% of additive noise. For the L 1 -ball, the linear minimization oracle LMO A just selects the column of A of best inner product with the residual vector. The second application comes from video co-localization. The approach used by <ref type="bibr" target="#b15">[16]</ref> is formulated as a quadratic program (QP) over a flow polytope, the convex hull of paths in a network. In this application, the linear minimization oracle is equivalent to finding a shortest path in the network, which can be done easily by dynamic programming. For the LMO A , we re-use the code provided by <ref type="bibr" target="#b15">[16]</ref> and their included aeroplane dataset resulting in a QP over 660 variables. In both experiments, we see that the modified FW variants (away-steps and pairwise) outperform the original FW algorithm, and exhibit a linear convergence. In addition, the constant in the convergence rate of Theorem 1 can also be empirically shown to be fairly tight for AFW and PFW by running them on an increasingly obtuse triangle (see Appendix E).</p><p>Discussion. Building on a preliminary version of our work <ref type="bibr" target="#b19">[20]</ref>, Beck and Shtern <ref type="bibr" target="#b3">[4]</ref> also proved a linear rate for away-steps FW, but with a simpler lower bound for the LHS of (10) using linear duality arguments. However, their lower bound [see e.g. Lemma 3.1 in 4] is looser: they get a d 2 constant for the eccentricity of the regular simplex instead of the tighter d that we proved. Finally, the recently proposed generic scheme for accelerating first-order optimization methods in the sense of Nesterov from <ref type="bibr" target="#b23">[24]</ref> applies directly to the FW variants given their global linear convergence rate that we proved. This gives for the first time first-order methods that only use linear oracles and obtain the "near-optimal" Õ(1/k 2 ) rate for smooth convex functions, or the accelerated Õ( L/µ) constant in the linear rate for strongly convex functions. Given that the constants also depend on the dimensionality, it remains an open question whether this acceleration is practically useful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (left) The FW algorithm zig-zags when the solution x * lies on the boundary. (middle) Adding the possibility of an away step attenuates this problem. (right) As an alternative, a pairwise FW step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>t then 7 : 1 ( 9 :</head><label>719</label><figDesc>d t := d FW t , and γ max := d t := d A t , and γ max := α vt /(1 -α vt ) (choose away direction; maximum feasible step-size)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 4 .</head><label>4</label><figDesc>The pyramidal width of the unit cube in R d is 1/ √ d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Duality gap g FW t vs iterations on the Lasso problem (top), and video co-localization (bottom). Code is available from the authors' website.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The atoms do not have to be extreme points (vertices) of M.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>All our convergence results can be carefully extended to approximate linear minimization oracles with multiplicative approximation guarantees; we state them for exact oracles in this paper for simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The original algorithm presented in<ref type="bibr" target="#b33">[34]</ref> was not convergent; this was corrected by Guélat and Marcotte<ref type="bibr" target="#b11">[12]</ref>, assuming a tractable representation of M with linear inequalities and called it the modified Frank-Wolfe (MFW) algorithm. Our description in Algorithm 1 extends it to the more general setup of (1).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>For AFW and PFW, we actually require that ∇f is L-Lipschitz over the larger domain M + M -M.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>By proper convex combination, we mean that all coefficients are non-zero in the convex combination.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank J.B. Alayrac, E. Hazan, A. Hubard, A. Osokin and P. Marcotte for helpful discussions. This work was partially supported by the MSR-Inria Joint Center and a Google Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linear convergence of a modified Frank-Wolfe algorithm for computing minimum-volume enclosing ellipsoids</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Ahipaaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Methods and Software</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="19" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The width and diameter of a simplex</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geometriae Dedicata</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="94" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with submodular functions: A convex optimization perspective</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="145" to="373" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Linearly convergent away-step conditional gradient for non-strongly convex functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shtern</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05002v1</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A conditional gradient method with linear rate of convergence for solving convex linear systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Methods of Operations Research (ZOR)</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="247" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tight upper bound on the rate of convergence of Frank-Wolfe algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Canon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Cullum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="516" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On pairwise costs for network flow multi-object tracking</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rates of convergence for conditional gradient algorithms near singular and nonsingular extremals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="211" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.4666v5</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster rates for the Frank-Wolfe method over strongly-convex sets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some comments on Wolfe&apos;s &apos;away step</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guélat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marcotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Restricted simplicial decomposition: Computation and extensions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawphongpanich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation Mathematical Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="99" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An extension of the Frank and Wolfe method of feasible directions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Holloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="27" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting Frank-Wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with Frank-Wolfe algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="159" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Barrier Frank-Wolfe for marginal inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A linearly convergent linear-time first-order algorithm for support vector classification with a core set result</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Yildirim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An affine invariant linear convergence analysis for Frank-Wolfe algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.7864v2</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Block-coordinate Frank-Wolfe optimization for structural SVMs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pletscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The complexity of large-scale convex programming under a linear optimization oracle</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.5550v2</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Levitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<title level="m">Constrained minimization methods. USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<date type="published" when="1966-01">Jan. 1966</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="787" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A universal catalyst for first-order optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Finding the point of a polyhedron closest to the origin</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Demyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Malozemov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel Frank-Wolfe algorithm. Analysis and applications to large-scale SVM training</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ñanculef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sartori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Allende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Introductory Lectures on Convex Optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On the von Neumann and Frank-Wolfe algorithms with away steps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Soheili</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04073v2</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in kernel methods: support vector learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Generalized Equations and their Solutions, Part II: Applications to Nonlinear Programming</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simplicial decomposition in nonlinear programming algorithms</title>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Hohenbalken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="68" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Iteration complexity of feasible descent methods for convex optimization</title>
		<author>
			<persName><forename type="first">P.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1523" to="1548" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convergence theory in nonlinear programming</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Integer and Nonlinear Programming</title>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Finding the nearest point in a polytope</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="149" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:math/9909177v1</idno>
		<title level="m">Lectures on 0/1-polytopes</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
