<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
							<email>czhang11@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
							<email>huaxiuyao@psu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
							<email>chaohuang75@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">JD Finance American Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
							<email>mjiang2@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
							<email>nchawla@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs (KGs) serve as useful resources for various natural language processing applications. Previous KG completion approaches require a large number of training instances (i.e., head-tail entity pairs) for every relation. The real case is that for most of the relations, very few entity pairs are available. Existing work of one-shot learning limits method generalizability for few-shot scenarios and does not fully use the supervisory information; however, few-shot KG completion has not been well studied yet. In this work, we propose a novel few-shot relation learning model (FSRL) that aims at discovering facts of new relations with few-shot references. FSRL can effectively capture knowledge from heterogeneous graph structure, aggregate representations of few-shot references, and match similar entity pairs of reference set for every relation. Extensive experiments on two public datasets demonstrate that FSRL outperforms the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Large-scale knowledge graphs (KGs) such as YAGO <ref type="bibr" target="#b10">(Suchanek, Kasneci, and Weikum 2007)</ref>, NELL <ref type="bibr" target="#b0">(Carlson et al. 2010), and</ref><ref type="bibr">Wikidata (Vrandečić and</ref><ref type="bibr" target="#b10">Krötzsch 2014)</ref> usually represent facts in the form of relations (edges) between (head-tail) entity pairs (nodes). This kind of graphstructured knowledge is essential for many downstream applications such as search, question answering, and semantic web. However, KGs are known for their incompleteness. In order to automate the KG completion process, many work <ref type="bibr" target="#b8">(Nickel, Tresp, and Kriegel 2011;</ref><ref type="bibr" target="#b0">Bordes et al. 2013;</ref><ref type="bibr" target="#b9">Socher et al. 2013;</ref><ref type="bibr" target="#b11">Yang et al. 2015;</ref><ref type="bibr" target="#b10">Trouillon et al. 2016;</ref><ref type="bibr" target="#b8">Schlichtkrull et al. 2018;</ref><ref type="bibr" target="#b0">Dettmers et al. 2018</ref>) have been proposed to infer missing relations by learning existing ones. For example, RESCAL <ref type="bibr" target="#b8">(Nickel, Tresp, and Kriegel 2011)</ref> employs tensor factorization to capture inherent structure of multi-relational data in KGs. TransE <ref type="bibr" target="#b0">(Bordes et al. 2013)</ref> interprets relations as translation operation on the low-dimensional embeddings of entities. And recently, G-GCN <ref type="bibr" target="#b8">(Schlichtkrull et al. 2018</ref>) models relational structure by graph neural network.</p><p>The above methods need a good number of entity pairs for every relation. However, the frequency distributions of relations in real datasets often have long tails. A large portion of relations have only few entity pairs in KGs. It is important and challenging to deal with the relations with limited (fewshot) number of entity pairs. The few-shot scenario incurs the infeasibility of previous models which assume available, sufficient training instances for all relations.</p><p>In light of the above issue, <ref type="bibr" target="#b10">Xiong et al. (2018)</ref> proposed GMatching which introduces a local neighbor encoder to learn entity embeddings. It achieves considerable performance in one-shot relation inference yet still has some limitations. First, GMatching assumes all local neighbors contribute equally to the entity embedding, whereas heterogeneous neighbors could have different impacts. For example, the embedding of "Nadella" may have more influence on the embedding of "Microsoft" than "Apple" as the company has only one CEO and a number of competitors. Thus the neighbor encoder of GMatching learns insufficient graph structure representation and impairs the model performance. Second, GMatching is designed under one-shot learning setting. Although it can be modified to few-shot case by adding a pooling layer over reference set, the general operation ignores the interaction among few-shot reference instances and limits the representation capability of reference set. Therefore, it is crucial to design a model to effectively complete relations with limited reference entity pairs. To address the above weak points, we propose a Few-Shot Relation Learning model (FSRL) with the purpose of learning a matching function that can effectively infer the true entity pairs given the set of few-shot reference entity pairs for each relation. To be more specific, first, we propose a relation-aware heterogeneous neighbor encoder to learn entity embeddings based on the heterogeneous graph structure and attention mechanism. It captures both different relation types and impact differences of local neighbors. Next, we design a recurrent autoencoder aggregation network to model interactions of few-shot reference entity pairs and accumulate their expression capabilities for each relation. With the aggregated embedding of reference set, we finally employ a matching network to discover similar entity pairs of reference set. The meta-training based gradient <ref type="bibr">de-arXiv:1911.11298v1 [cs.CL]</ref> 26 Nov 2019 scent approach is employed to optimize model parameters. The learned model can be further applied to infer true entity pairs for any new relation without any fine-tuning step.</p><p>To summarize, our main contributions are: Results demonstrate that our model outperforms state-ofthe-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Here we survey two topics relevant to this work: few-shot learning and relation learning for KGs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>In this section, we formally define the few-shot knowledge graph completion problem and detail the corresponding fewshot learning settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><formula xml:id="formula_0">A KG G is represented as a collection of triples {(h, r, t)} ⊆ E × R × E,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot Learning Settings</head><p>The purpose of this work is to design a machine learning model which could be utilized to predict the new facts with few-shot reference instances. </p><formula xml:id="formula_1">(h k , t k ) ∈ R r . Besides, P test r = {(h i , t i , C hi,r )|(h i , r, t i ) ∈ G} contains all test-</formula><p>ing entity pairs of r, including true tail entities t i of each query (h i , r) and the remaining candidate entities t j ∈ C hi,r where t j is an entity in G. The proposed model thus could be tested on this set by ranking all candidate entities given the test query (h i , r) and the few-shot reference pairs in P train r . We denote the ranking loss of relation r as L Θ (h i , t i |C hi,r , P train r ), where Θ is the set of model parameters. Thus, the objective of model training is defined as: These relations form a meta-testing set which is denoted as T mte . In addition, we leave out a subset of relations in T mtr as the meta-validation set T mtv . Furthermore, the model can access to a background KG G , which is a subset of G that excludes all the relations in T mtr , T mte and T mtv .</p><formula xml:id="formula_2">min Θ E Tmtr   (hi,ti,C h i ,r )∈P test r L Θ (h i , t i |C hi,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>In this section, we present the detail of FSRL. FSRL consists of three major parts: (1) encoding heterogeneous neighbors for each entity; (2) aggregating few-shot reference entity pairs for each relation; (3) matching query pairs with reference set for relation prediction. Figure <ref type="figure" target="#fig_2">1</ref> shows the framework of FSRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding Heterogeneous Neighbors</head><p>Although many work <ref type="bibr" target="#b8">(Nickel, Tresp, and Kriegel 2011;</ref><ref type="bibr" target="#b0">Bordes et al. 2013;</ref><ref type="bibr" target="#b11">Yang et al. 2015)</ref> have been proposed to learn entity embeddings by using relational information, Xiong et al. <ref type="bibr" target="#b10">(Xiong et al. 2018</ref>) demonstrated that explicitly encoding graph local structure (i.e., one-hop neighbors) can benefit relation prediction. The proposed neighbor encoder takes the average of feature representations of all relational neighbors as the embedding of given entity. Despite the desirable performance, it neglects the different impacts of heterogeneous neighbors which may help improve entity embedding <ref type="bibr">(Zhang et al. 2019)</ref>. In light of this issue, we design a relation-aware heterogeneous neighbor encoder. Specifically, we denote the set of relational neighbors (relation, entity) of given head entity h as</p><formula xml:id="formula_3">N h = {(r i , t i )|(h, r i , t i ) ∈ G },</formula><p>where G is the background knowledge graph, r i and t i represent the i-th relation and corresponding tail entity of h, respectively. The heterogeneous neighbor encoder should be able to encode N h and output a feature representation of h by considering different impacts of relational neighbors (r i , t i ) ∈ N h . To achieve this goal, we introduce an attention module and formulate the embedding of h as follows:</p><formula xml:id="formula_4">f θ (h) = σ i α i e ti α i = exp u T rt W rt (e ri ⊕ e ti ) + b rt j exp u T rt W rt (e rj ⊕ e tj ) + b rt (2)</formula><p>where σ denotes activation unit (we use Tanh), ⊕ represents concatenation operator, e ti , e ri ∈ R d×1 are pre-trained embeddings of t i and r i . Besides, u rt ∈ R d×1 , W rt ∈ R d×2d and b rt ∈ R d×1 (d: pre-trained embedding dimension) are learnable parameters. Figure <ref type="figure" target="#fig_2">1</ref>(b) illustrates the detail of heterogeneous neighbor encoder. According to Eq. 2, the formulation of f θ (h) considers the different impacts of heterogeneous relational neighbors via attention weight α i and leverages both embeddings of entity t i and relation r i to compute α i .</p><p>Aggregating Few-Shot Reference Set</p><p>The current models (e.g., GMatching) are not able to model the interactions of few-shot instances in reference set, which limits model capability. Thus, we need to design a module to effectively formulate the aggregated embedding of reference set R r for each relation r. By applying the neighbor encoder f θ (h) to each entity pair</p><formula xml:id="formula_5">(h k , t k ) ∈ R r , we can obtain the representation of (h k , t k ) in the form E h k ,t k = [f θ (h k ) ⊕ f θ (t k )].</formula><p>Learning the representation of a reference set R r with few-shot entity pairs is challenging as it requires modeling interactions among different entity pairs and accumulating their expression capability. Inspired by the common practices in learning sentence embeddings <ref type="bibr" target="#b0">(Conneau et al. 2017</ref>) in natural language processing and aggregating node embeddings <ref type="bibr" target="#b4">(Hamilton, Ying, and Leskovec 2017)</ref> in graph neural networks, we tackle the challenge and formulate the embedding of R r by aggregating representations of all entity pairs in R r :</p><formula xml:id="formula_6">f (R r ) = AG (h k ,t k )∈Rr E h k ,t k<label>(3)</label></formula><p>where AG is an aggregation function which can be pooling operation, feed-forward neural network, etc. Motivated by the recent success of recurrent neural network aggregator in order-invariant problems such as graph embedding <ref type="bibr" target="#b4">(Hamilton, Ying, and Leskovec 2017)</ref>, we design a recurrent autoencoder aggregator which achieves good capability. Specifically, the entity pair embeddings E h k ,t k ∈ R r are sequentially fed into a recurrent autoencoder by:</p><formula xml:id="formula_7">E h1,t1 → m 1 → • • • → m K → d K → • • • → d 1 (4)</formula><p>where K is the size of reference set (i.e., few-shot size). The hidden states m k and d k of encoder and decoder are computed by:</p><formula xml:id="formula_8">m k = RNN encoder (E h k ,t k , m k−1 ) d k−1 = RNN decoder (d k )<label>(5)</label></formula><p>where RNN encoder and RNN decoder represent recurrent encoder and decoder (e.g., LSTM (Hochreiter and Schmidhuber 1997)), respectively. The reconstruction loss for optimizing autoencoder is defined as:</p><formula xml:id="formula_9">L re (R r ) = k d k − E h k ,t k 2 2<label>(6)</label></formula><p>L re will be incorporated to the relational ranking loss for refining the representation of each entity pair, as we will describe later. In order to formulate the embedding of reference set, we aggregate all hidden states of encoder and   The formulation of f (R r ) aggregates all representations of E h k ,t k ∈ R r and each component in this module will make effect for better performance, as we will show in the ablation study experiment.</p><formula xml:id="formula_10">m k = m k + E h k ,t k β k = exp u T R W R m k + b R k exp u T R W R m k + b R f (R r ) = k β k m k (7) where u R ∈ R d×1 , W R ∈ R d×2d and b R ∈ R d×1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching Query and Reference Set</head><p>With the heterogeneous neighbor encoder f θ and the reference set aggregator f , we now present how to effectively match each query entity pair (h l , t l ) ∈ Q r (Q r is the set of all query pairs of relation r) with the reference set R r .</p><p>By applying f θ and f to the query entity pair (h l , t l ) and the reference set R r , we can obtain two embedding vectors E h l ,t l = [f θ (h l ) ⊕ f θ (t l )] and f (R r ), respectively. In order to measure the similarity between two vectors, we employ a recurrent processor <ref type="bibr" target="#b10">(Vinyals et al. 2016</ref>) f µ to perform multiple steps matching. The t-th process step is formulated as:</p><formula xml:id="formula_11">g t , c t = RNN match (E h l ,t l , [g t−1 ⊕ f (R r )], c t−1 ) g t = g t + E h l ,t l (8)</formula><p>where RNN match is LSTM cell (Hochreiter and Schmidhuber 1997) with input E h l ,t l , hidden state g t and cell state c t . The last hidden state g T after T "processing" step is the refined embedding of query pair (h l , t l ): E h l ,t l = g T . We use the inner product between E h l ,t l and f (R r ) as the similarity score for later ranking optimization procedure. Figure <ref type="figure" target="#fig_2">1</ref>(d) shows the detail of matching processor. This module is effective for improving model performance, as we will demonstrate in the ablation study experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective and Model Training</head><formula xml:id="formula_12">= {(h l , t l )|(h l , r, t l ) ∈ G ∩ (h l , t l ) /</formula><p>∈ R r } are utilized as positive query pairs. Besides, we construct a group of negative (false) entity pairs</p><formula xml:id="formula_13">N E r = {(h l , t − l )|(h l , r, t − l ) /</formula><p>∈ G} by polluting the tail entities. Therefore the ranking loss is formulated as:</p><formula xml:id="formula_14">L rank = r (h l ,t l )∈PEr (h l ,t − l )∈N Er ξ + s (h l ,t − l ) − s (h l ,t l ) + (9) where [x] + = max[0, x]</formula><p>is standard hinge loss and ξ is safety margin distance, s (h l ,t l ) and s (h l ,t − l ) are similarity scores between query pairs (h l , t l /t − l ) and reference set R r . By leveraging the reconstruction loss L re of reference set aggregator, we define the final objective function as:</p><formula xml:id="formula_15">L joint = L rank + γL re (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where γ is trade-off factor between L rank and L re . To minimize L joint and optimize model parameters, we take each relation as a task and design a batch sampling based metatraining procedure. The detail of this process is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we conduct extensive experiments to evaluate the performance of proposed model and verify the effectiveness of each component in the model. Few-shot size impact analysis and embedding visualization are also provided. to obtain the general embedding of reference set. Moreover, we also consider taking the maximum of similarity scores between a query and all K references as the final ranking score of this query. Thus in total, this type of model includes three baseline methods which are denoted as GMatching (MaxP), GMatching (MeanP), and GMatching (Max).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility Settings</head><p>The above relational embedding methods can be utilized to pre-train KG embeddings, which are further used as the input for GMatching and FSRL. We select ComplEx for pre-training as GMatching and FSRL with it achieve best performances in most cases.</p><p>For the proposed model, we tune hyper-parameters on the validation dataset. The embedding dimension is set to 100 and 50 for NELL and Wiki dataset, respectively. The maximum number of local neighbors in heterogeneous neighbor encoder is set to 30 for both datasets. In addition, we use LSTM as the reference set aggregator and matching processor. The dimension of LSTM's hidden state is set to 200 and 100 for NELL and Wiki dataset, respectively. The number of recurrent steps equals 2 in matching network. We use the Adam optimizer (Kingma and Ba 2015) to update model parameters. The initial learning rate equals 0.001 and the weight decay is 0.25 for each 10k training steps. The margin distance and trade-off factor in the objective function are set to 5.0 and 0.0001, respectively. In entity candidate set construction, we set the maximum size to 1000 for both datasets. We employ Pytorch 1 to implement our model and further conduct it on a server with GPU machines.</p><p>Evaluation Metrics Relations and their entity pairs in training data are utilized to train the model while those of validation and test data are respectively used to tune and evaluate model. We use the top-k hit ratio (Hits@k) and the mean reciprocal rank (MRR) to evaluate performances of different methods. The k is set to 1, 5, and 10. The few-shot size K is set to 3 for the following experiments. In addition, we also conduct experiment to analyze the impact of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Comparison</head><p>Overall Comparison with Baselines The performances of all models are reported in    Embedding Visualization To show a better performance comparison between our model and GMatching, we visualize the 2D embeddings of positive and negative candidate entity pairs for each relation. Figure <ref type="figure" target="#fig_6">3</ref> shows the visualization results of our model and GMatching for two test relations of NELL data, i.e., "produced by" and "team coach", which vary from each other in semantic meaning and size of positive/negative candidate set. According to the figure, both methods can distinguish embeddings of positive and negative candidates well. However, it is clear that our model achieves better performance and embeddings of two classes are clearly discriminated from each other, which further demonstrates the superior performance of our model in terms of visualization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we presented a new few-shot KG completion problem and proposed an innovative few-shot relation learning model, i.e., FSRL, to solve the problem. FSRL performs joint optimization of relation-aware heterogeneous neighbor encoder, recurrent autoencoder aggregation network and matching network. The extensive experiments on two public datasets demonstrate that FSRL can outperform stateof-the-art baseline methods. In addition, the ablation studies verify the effectiveness of each model component. As a new research problem, there are many opportunities for the next steps. The future work might consider utilizing a better model training process such as model-agnostic metalearning or incorporating contextual information such as entity attributes or text description to improve the quality of entity embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Following the standard few-shot learning settings (Ravi and Larochelle 2016; Snell, Swersky, and Zemel 2017), we can access to a set of training tasks. In the problem, each training task corresponds to a KG relation r ∈ R with its own training/testing entity pairs data: D r = P train r , P test r . We denote this kind of task set as meta-training set, T mtr . To imitate the fewshot relation prediction in evaluation period, each P train r only contains few-shot entity pairs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>|</head><label></label><figDesc>represents the number of tuples in P test r . In next section, we will detail how to formulate and optimize the above objective function.After sufficient training, the learned model can be utilized to predict facts of each new relation r ∈ R . This step is called the meta-testing. The relations in meta-testing are unseen from meta-training, i.e., R ∩ R = φ. The same as meta-training relations, each relation r in meta-testing has its own few-shot training data P train r and testing data P test r .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The framework of FSRL: it first generates entity embedding via heterogeneous neighbor encoder, then aggregates few-shot reference entity pairs and generate reference set embedding, finally employs a matching network to compute similarity score between query pair and reference set; (b) the relation-aware heterogeneous neighbor encoder for entity; (c) the recurrent autoencoder aggregation network for reference set; (d) the recurrent matching network for query pair and reference set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(d: aggregated embedding dimension) are learnable parameters. Figure 1(c) illustrates the detail of recurrent autoencoder aggregator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Impact of few-shot size K. Our model consistently outperforms GMatching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Embedding visualization of positive and negative candidates of two selected relations. Our model can clearly discriminate embeddings of these two types of candidates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>We introduce a new few-shot KG completion problem which is different from previous work and more suitable for practical scenarios. • We propose a few-shot relation learning model to solve the problem. The model performs joint optimization of several learnable neural network modules. • We conduct extensive experiments on two public datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Pollute the tail entity of (h l , t l ) to get (h l , t − l ) Optimal model parameters θ * , * and µ * and regard them as the reference set R r . The remaining positive entity pairs PE r</figDesc><table><row><cell cols="2">input : Meta-training task (relation) set Tmtr</cell></row><row><cell></cell><cell>Pre-trained KG embeddings</cell></row><row><cell></cell><cell>Initial model parameters θ, and µ</cell></row><row><cell cols="2">1 while not done do</cell></row><row><cell>2</cell><cell>Shuffle tasks (relations) in Tmtr</cell></row><row><cell>3</cell><cell>for Tr ∈ Tmtr do</cell></row><row><cell>4</cell><cell>Sample few-shot entity pairs as reference set</cell></row><row><cell>5</cell><cell>Sample a batch of query entity pairs (h l , t l )</cell></row><row><cell>7</cell><cell>Accumulate the loss by Eq. 10</cell></row><row><cell>8</cell><cell>Update parameters by Adam optimizer</cell></row><row><cell>9</cell><cell>end</cell></row><row><cell>10 end</cell><cell></cell></row><row><cell cols="2">11 return</cell></row></table><note>For the query relation r, we randomly sample a set of few positive (true) entity pairs{(h k , t k )|(h k , r, t k ) ∈ G} Algorithm 1: FSRL Meta-Training 6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets. # Ent. denotes the number of all unique entities and # triples denotes the number of all relational triples. # Rel. represents the number of all relations and # Tasks represents the number of relations selected as few-shot tasks. Graph neighbor encoder methods. This type of model joints graph local neighbor encoder and matching network to learn entity embeddings and predict facts of new relations. We employ state-of-the-art model GMatching (Xiong et al. 2018) for comparison. Note that there are few-shot embeddings of entity pairs in reference set, we use max/mean pooling (denoted as MaxP and MeanP)</figDesc><table><row><cell>Datasets We use two public datasets for experiments. The</cell></row><row><cell>first one is based on NELL (Mitchell et al. 2018), a sys-</cell></row><row><cell>tem that continuously collects structured knowledge from</cell></row><row><cell>webs. The second one is based on Wikidata (Vrandečić and</cell></row><row><cell>Krötzsch 2014). Table 1 lists the statistics of two datasets.</cell></row><row><cell>The same as (Xiong et al. 2018), we select the relations</cell></row><row><cell>with less than 500 but more than 50 triples as few-shot</cell></row><row><cell>tasks. There are 67 and 183 tasks in NELL and Wiki data,</cell></row><row><cell>respectively. In addition, we use 51/5/11 task relations for</cell></row><row><cell>training/validation/testing in NELL and the division is set to</cell></row><row><cell>133/16/34 in Wiki.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>The overall results of all methods. GMatching is the best baseline. Our model has the best performances in all cases.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Data: NELL</cell><cell></cell><cell></cell><cell cols="2">Data: Wiki</cell><cell></cell></row><row><cell>Model</cell><cell>Hits@1</cell><cell>Hits@5</cell><cell>Hits@10</cell><cell>MRR</cell><cell>Hits@1</cell><cell>Hits@5</cell><cell>Hits@10</cell><cell>MRR</cell></row><row><cell>RESCAL</cell><cell cols="8">.069/.141 .160/.313 .204/.383 .119/.223 .259/.057 .297/.090 .309/.126 .279/.081</cell></row><row><cell>TransE</cell><cell cols="8">.056/.119 .112/.256 .189/.320 .104/.193 .186/.069 .352/.134 .431/.176 .273/.111</cell></row><row><cell>DistMult</cell><cell cols="8">.066/.164 .123/.306 .178/.375 .109/.231 .271/.069 .419/.156 .459/.195 .339/.112</cell></row><row><cell>ComplEx</cell><cell cols="8">.049/.129 .092/.223 .112/.273 .079/.185 .226/.085 .315/.117 .397/.145 .282/.106</cell></row><row><cell>GMatching (MaxP)</cell><cell cols="8">.244/.198 .418/.370 .524/.464 .331/.279 .313/.095 .402/.235 .468/.324 .346/.171</cell></row><row><cell>GMatching (MeanP)</cell><cell cols="5">.257/.186 .455/.360 .542/.453 .341/.267 .290/.128</cell><cell>407/.274</cell><cell cols="2">.484/.350 .352/.203</cell></row><row><cell>GMatching (Max)</cell><cell cols="8">.179/.152 .391/.335 .476/.445 .273/.241 .279/.135 .396/.284 .477/.374 .342/.214</cell></row><row><cell>FSRL (Ours)</cell><cell cols="8">.345/.211 .502/.433 .570/.507 .421/.318 .338/.155 .430/.327 .486/.406 .390/.241</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>relations (e.g., relation 1) with small candidate set (or are</cell></row><row><cell>easily to be predicted) have relative large scores and two</cell></row><row><cell>models in these cases are compared.</cell></row><row><cell>• FSRL has better performances than GMatching in most</cell></row><row><cell>cases. It demonstrates that our model is robust for differ-</cell></row><row><cell>ent relations and outperforms GMatching for most rela-</cell></row><row><cell>tions.</cell></row><row><cell>, where the best results</cell></row><row><cell>are highlighted in bold and the best baseline results are indi-</cell></row><row><cell>cated by underline. The former/later score denotes result in</cell></row><row><cell>validation/test dataset. According to this table:</cell></row><row><cell>• The graph neighbor encoder methods (GMatching) out-</cell></row><row><cell>perform the relational embedding methods, showing that</cell></row><row><cell>incorporating graph local structure and matching network</cell></row><row><cell>is effective for learning entity embeddings and predicting</cell></row><row><cell>facts of new relations.</cell></row><row><cell>• FSRL achieves the best performances in all cases. The</cell></row><row><cell>average relative improvement (%) over the best baseline</cell></row><row><cell>method is up to 34% and 15% in NELL and Wiki data, re-</cell></row><row><cell>spectively. It demonstrates the effectiveness of our model.</cell></row><row><cell>The heterogeneous neighbor encoder and recurrent au-</cell></row><row><cell>toencoder aggregation network benefit few-shot relation</cell></row><row><cell>prediction in KGs.</cell></row><row><cell>Comparison Over Different Relations Besides the over-</cell></row><row><cell>all performance for all relations, we also conduct experi-</cell></row><row><cell>ments to evaluate model performance for each relation in</cell></row><row><cell>NELL test data. Table 3 reports the results of FSRL and</cell></row><row><cell>GMatching. The better result for each case is highlighted</cell></row><row><cell>in bold. According to this table:</cell></row><row><cell>• The results of both models on different relations are of</cell></row><row><cell>high variance. It is reasonable since different relations</cell></row><row><cell>have different sizes of candidate set for evaluation. The</cell></row><row><cell>1 https://pytorch.org/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>The results of GMatching and FSRL for each relation (RId) in NELL test data.FSRL is a joint learning framework of several neural network modules. To investigate the contributions of different components, we conduct the following ablation studies from three perspectives in Table4, where the results in NELL data are reported and best results are highlighted in bold: We further analyze the effectiveness of matching network. We remove the LSTM cell and use inner-product between query embedding and reference embedding as similarity (ranking) score. From the results in table, our model largely outperforms the variant (in AS 3), showing that recurrent matching network has good capability in computing relevance between query and reference.</figDesc><table><row><cell>RId</cell><cell>Model</cell><cell cols="4">Hits@1 Hits@5 Hits@10 MRR</cell></row><row><cell>1</cell><cell>GMatching FSRL</cell><cell>0.946 0.972</cell><cell>1.000 0.986</cell><cell>1.000 1.000</cell><cell>0.970 0.981</cell></row><row><cell>2</cell><cell>GMatching FSRL</cell><cell>0.279 0.972</cell><cell>0.390 0.972</cell><cell>0.451 0.986</cell><cell>0.360 0.975</cell></row><row><cell>3</cell><cell>GMatching FSRL</cell><cell>0.069 0.466</cell><cell>0.115 0.418</cell><cell>0.152 0.357</cell><cell>0.108 0.398</cell></row><row><cell>4</cell><cell>GMatching FSRL</cell><cell>0.017 0.044</cell><cell>0.033 0.215</cell><cell>0.070 0.343</cell><cell>0.035 0.132</cell></row><row><cell>5</cell><cell>GMatching FSRL</cell><cell>0.069 0.044</cell><cell>0.115 0.215</cell><cell>0.151 0.343</cell><cell>0.107 0.132</cell></row><row><cell>6</cell><cell>GMatching FSRL</cell><cell>0.192 0.342</cell><cell>0.515 0.549</cell><cell>0.581 0.617</cell><cell>0.338 0.442</cell></row><row><cell>7</cell><cell>FSRL GMatching</cell><cell>0.478 0.438</cell><cell>0.692 0.716</cell><cell>0.804 0.842</cell><cell>0.575 0.562</cell></row><row><cell>8</cell><cell>GMatching FSRL</cell><cell>0.151 0.201</cell><cell>0.502 0.543</cell><cell>0.669 0.681</cell><cell>0.312 0.347</cell></row><row><cell>9</cell><cell>GMatching FSRL</cell><cell>0.449 0.163</cell><cell>0.707 0.750</cell><cell>0.737 0.838</cell><cell>0.564 0.408</cell></row><row><cell>10</cell><cell>GMatching FSRL</cell><cell>0.043 0.069</cell><cell>0.129 0.192</cell><cell>0.206 0.291</cell><cell>0.098 0.139</cell></row><row><cell>11</cell><cell>GMatching FSRL</cell><cell>0.076 0.104</cell><cell>0.708 0.631</cell><cell>0.736 0.781</cell><cell>0.341 0.416</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results of model variants in NELL data. Our model has better performance than all model variants. According to the figure: • With the increment of K, performances of both models increase. It indicates that larger reference set can produce better reference set embedding for the relation. • Our model consistently outperforms GMatching in different K, demonstrating the stability of the proposed model for few-shot relation completion in KGs.</figDesc><table><row><cell>Model</cell><cell>Hits@1</cell><cell>Hits@5</cell><cell>Hits@10</cell><cell>MRR</cell></row><row><cell>(AS 1)</cell><cell cols="4">.121/.179 .312/.379 .432/.464 .212/.272</cell></row><row><cell>(AS 2a)</cell><cell cols="4">.281/.191 .463/.414 .538/.504 .368/.297</cell></row><row><cell>(AS 2b)</cell><cell cols="4">.298/.219 .480/.420 .556/.498 .382/.315</cell></row><row><cell>(AS 2c)</cell><cell cols="4">.333/.226 .478/.418 .552/.499 .401/.313</cell></row><row><cell>(AS 3)</cell><cell cols="4">.282/.205 .463/.394 .548/.478 .370/.299</cell></row><row><cell>Ours</cell><cell cols="4">.345/.211 .502/.433 .570/.507 .421/.318</cell></row><row><cell>Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Impact of Few-Shot Size This work studies few-shot rela-</cell></row><row><cell cols="5">tion learning in KGs, thus we conduct experiment to analyze</cell></row><row><cell cols="5">the impact of few-shot size K. Figure 2 reports the perfor-</cell></row><row><cell cols="5">mances of our model and GMatching (MaxP) in NELL test</cell></row><row><cell cols="3">data with different settings of K.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053 and the National Science Foundation awards #1925607,  #1629914, #1652525, #1618448 and #1849816.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2010">2013. 2013. 2010. 2010. 2017. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One-shot imitation learning</title>
		<author>
			<persName><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Abbeel</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Levine ; Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Levine ; Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradientbased meta-learning with learned layerwise metric and subspace</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Leskovec ; Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
				<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">2017. 2017. 2016. 1997. 1997. 2015. 2015. 2015. 2015. 2018. 2018</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transferable end-to-end aspect-based sentiment analysis with selective adversarial learning</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner. ICLR</title>
		<author>
			<persName><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2018">2018. 2018. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Never-ending learning</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Tresp</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011. 2011. 2016. 2018</date>
		</imprint>
	</monogr>
	<note>ESWC</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Swersky</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zemel ; Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2017. 2017. 2013</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot relational learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Kasneci</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Weikum ; Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2007">2007. 2007. 2016. 2016. 2014. 2014. 2018. 2018</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning from multiple cities: A meta-learning approach for spatial-temporal prediction</title>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018. 2018. 2019a</date>
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchically structured meta-learning</title>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019b. 2019b. 2019</date>
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
