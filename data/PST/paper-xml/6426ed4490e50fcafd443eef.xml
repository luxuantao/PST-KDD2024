<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PIMFlow: Compiler and Runtime Support for CNN Models on Processing-in-Memory DRAM</title>
				<funder ref="#_u2hkfGn">
					<orgName type="full">National IT Industry Promotion Agency</orgName>
					<orgName type="abbreviated">NIPA</orgName>
				</funder>
				<funder>
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
				<funder ref="#_EpWRtex #_E8Fz3Bm #_KXAa4YM">
					<orgName type="full">Institute for Information &amp; communications Technology Promotion</orgName>
					<orgName type="abbreviated">IITP</orgName>
				</funder>
				<funder ref="#_Dj7vTCn">
					<orgName type="full">National Research Foundation</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yongwon</forename><surname>Shin</surname></persName>
							<email>ywshin@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering POSTECH</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">South</forename><surname>Korea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering POSTECH</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering POSTECH</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juseong</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering POSTECH</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sungjun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering POSTECH</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PIMFlow: Compiler and Runtime Support for CNN Models on Processing-in-Memory DRAM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3579990.3580009</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Processing-in-memory, CNN models</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Processing-in-Memory (PIM) has evolved over decades into a feasible solution to addressing the exacerbating performance bottleneck with main memory by placing computational logic in or near memory. Recent proposals from DRAM manufacturers highlighted the HW constraint-aware design of PIM-enabled DRAM with specialized MAC logic, providing an order of magnitude speedup for memory-intensive operations in DL models. Although the main target for PIM acceleration did not initially include convolutional neural networks due to their high compute intensity, recent CNN models are increasingly adopting computationally lightweight implementation. Motivated by the potential for the software stack to enable CNN models on DRAM-PIM hardware without invasive changes, we propose PIMFlow, an end-to-end compiler and runtime support, to accelerate CNN models on a PIM-enabled GPU memory. PIMFlow transforms model graphs to create inter-node parallelism across GPU and PIM, explores possible task-and data-parallel execution scenarios for optimal execution time, and provides a code-generating back-end and execution engine for DRAM-PIM. PIMFlow achieves up to 82% end-to-end speedup and reduces energy consumption by 26% on average for CNN model inferences.</p><p>CCS Concepts: ? Software and its engineering ? Compilers; ? Hardware ? Emerging architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main memory has become an increasingly critical performance and energy bottleneck in modern computing systems, as the performance gap between compute units and memories widens <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and the demand for efficient large-scale data processing grows. Among many architectural efforts to address this "memory wall" <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>, processing-inmemory (PIM) places computational logic in or near memory devices to perform in-memory operations, thus effectively eliminating the data movement overhead for PIM-offloaded computations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>While the idea of PIM is not new <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>, recent advances in memory technologies motivated major DRAM manufacturers to explore its potential as a commercial DRAM solution <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. These efforts discovered that the area and power constraints on the number and complexity of PIM compute units are much more stringent than those assumed by previous PIM approaches, and focused on integrating multiply-accumulate (MAC) units in the logic layer of 3D-stacked memory <ref type="bibr" target="#b36">[37]</ref>, or after bit-line sense amplifier (BLSA) <ref type="bibr" target="#b25">[26]</ref> while meeting fab-level energy and area constraints. The resulting "DRAM-PIM" achieved an order of magnitude acceleration for memory-intensive fully-connected (FC) layers in various DNN models.</p><p>On the other hand, convolutional layers, one of the two major building blocks of DNN models along with FC layers, were not considered main targets for PIM due to their high computational intensity and data reuse, with little prospect for PIM logic with limited computational power to beat massively parallel GPUs. Nevertheless, we see a strong potential for PIM acceleration with a type of separable convolutional layers, e.g., pointwise or 1x1 convolutional layers. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, these layers have relatively low arithmetic intensity as they are equivalent to FC layers after convolution lowering, and are increasingly used in modern CNN models to reduce dimensionality (ResNet50 <ref type="bibr" target="#b24">[25]</ref>) or combine with depthwise (DW) separable layers to replace regular convolutional layers to reduce computation load while retaining accuracy (EfficientNetB0 <ref type="bibr" target="#b56">[57]</ref> and MobileNetV2 <ref type="bibr" target="#b50">[51]</ref>). Taking a step forward, we envision a holistic deep learning (DL) software stack for DRAM-PIM with optimizing compiler and runtime support. Prior work on software support for PIM focused mainly on providing user interfaces to explicitly specify and schedule PIM-offloadable code regions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>, while recent proposals include more sophisticated placement and scheduling mechanisms with cost models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b57">58]</ref> and GPU-PIM parallel execution support <ref type="bibr" target="#b47">[48]</ref>. Our key insight is that compilers can exploit PIM-offloading opportunities inherent in input DL models, but also can systematically create them by transforming model graphs so that they have more nodes to accelerate on PIM and overlap execution with GPU. This will significantly extend the scope of PIM target software and improve PIM utilization. Automating the offloading process can be challenging due to the complexity of code generation, but we observed that configuring the DRAM-PIM as both GPU memory and PIM device can simplify the task.</p><p>Thus, we propose PIMFlow, a compiler and runtime solution that accelerates CNN models on a PIM-enabled GPU memory based on <ref type="bibr" target="#b25">[26]</ref>, with PIM-aware graph transformations and PIM command generation support. PIMFlow takes model inference graphs from DL frameworks as input and searches the space of possible placement and scheduling options for convolution layers. PIMFlow supports multiple mixed-parallel execution models, where a convolution layer can be distributed across GPU and DRAM-PIM (multi-device data parallel) or multiple convolution layers partially overlap execution (pipelined), in addition to traditional device offloading (heterogeneous parallel). The search result is used to transform the graphs accordingly, which are then compiled and executed by our TVM back-end for DRAM-PIM.</p><p>In the DRAM-PIM back-end, PIMFlow performs a memory layout optimization to minimize data movement overheads introduced by parallelization, and optimizes DRAM-PIM code generation for more efficient command sequences. PIMFlow can also be viewed as an SW/HW co-design effort; our target DRAM-PIM architecture is extended with new DRAM-PIM commands to allow more fine-grained data reuse specifically needed to efficiently handle convolutional layer matrices, and the code generator supports them. Our experimental results showed up to 82% and 33% (34% and 23%, on average) speedup for all evaluated CNN models against the GPU and DRAM-PIM baselines without PIMFlow, respectively. The reduced execution time led to lower energy consumption by 26% on average against the GPU baseline.</p><p>The contributions of the paper are as follows: ? We systematically analyze performance trends of convolution layers on the DRAM-PIM hardware, and define the PIMFlow execution model accordingly. To our knowledge, this paper concerns the first effort to expand the scope of PIM-offloadable computations and increase PIM utilization by transforming computations to exploit GPU-PIM mixed-parallel execution. ? We propose an end-to-end compiler and runtime solution that enables all types of convolution layers on DRAM-PIM. The resulting PIMFlow provides up to 82% speedup over GPU for evaluated CNN models, showing the strong potential for PIMFlow as an optimizing compiler toolchain for industrial DRAM-PIM. ? We extend the DRAM-PIM memory architecture to reduce PIM command latencies, support convolution more efficiently, and facilitate mixed-parallel execution across GPU and DRAM-PIM.</p><p>In the rest of the paper, we first provide background information for DRAM-PIM architectures and convolution operation. Section 3 describes our preliminary analysis, which motivates the design and implementation of PIMFlow in Section 4. Section 5 and 6 show the experimental result for PIMFlow with several CNN models. Section 7 discusses the overheads incurred by PIMFlow implementation. Section 8 presents related work, and Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Digital DRAM-PIM Architecture</head><p>We assume Newton and its sister architecture <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref> as our baseline PIM-enabled DRAM. They present a DRAM manufacturer's constraint-aware design for commercially viable DRAM-PIM. Newton defines its target operation as memorybound matrix-vector multiplication of one large operand with low data reuse and one small operand possibly with high data reuse. Fig. <ref type="figure" target="#fig_1">2</ref> shows a structural overview of the architecture on the right. With the large operand in a memory cell array (1) and the small operand in a global buffer per memory channel (2), Newton performs bank-level parallel matrix-vector multiplication of these two operands using MAC units placed after BLSA. MAC logic consists of the reduction tree after column I/O (3), where fetched matrices are multiplied with input data in the global buffer and then summed up, and the result latches (4) to accumulate MAC results. Other operations (e.g., element-wise operations) are not supported. Although Newton features a small number of compute units (16 multipliers and a reduction tree per bank) compared to previous digital DRAM-PIM approaches <ref type="bibr" target="#b14">[15]</ref>, it produces approximately 20x speedup for memory-bound DL models such as BERT <ref type="bibr" target="#b15">[16]</ref> and recommendation models (DLRM) <ref type="bibr" target="#b43">[44]</ref>, showing the feasibility of commercial DRAM-PIM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Convolution Operation and Implementation</head><p>Convolutional neural networks (CNN) are widely used in various vision and graphics tasks such as image classification, object detection, and image/video segmentation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. CNN stacks multiple convolutional (CONV) layers to extract image features. Shallow stacks detect simple geometric shapes like edges, while deeper stacks extract more complex and higher-level features. Convolution operation works by sliding "filters" across spatial dimensions (width and height) of input images and computing output values as the dot product between the filter and input element. There exist many convolution algorithms optimized for specific data shapes and hardware <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>, and "convolution lowering, " which implements convolution as matrix-matrix multiplication of rearranged image and filter matrices, is widely used for data-parallel accelerators <ref type="bibr" target="#b12">[13]</ref>.</p><p>To map CONV layers to the DRAM-PIM hardware, we apply convolution lowering first and iteratively perform matrixvector multiplications. For example, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, after convolution lowering, convolution becomes the multiplication of input tensor (green) and filter weight (blue) matrices. To map the input tensor to the DRAM-PIM global buffer, the input tensor matrix is broken down into multiple vectors. Similar to the tiling approach in <ref type="bibr" target="#b25">[26]</ref>, we place the filters in the memory cell array in advance, and then fetch the input tensor from GPU memory to the global buffer and activate the PIM compute unit. We assume the NHWC, i.e., channelslast, format for input data layout as it guarantees contiguous memory access in the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary Analysis</head><p>Prior work on software support for DL models on DRAM-PIM assumed the default heterogeneous parallelism, where a host CPU schedules computational graph nodes on the host or device(s) (including GPU and PIM) and serially launches each node in a graph traversal order that is then executed, exploiting data parallelism within the node. While PIM can provide significant runtime acceleration for memory-intensive operations in this mode of parallelism <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref>, support for both intra-and inter-node parallelism across multiple devices could bring out additional speedup for a wider range of computations, and recent research explored its potential <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref>. Thus, we conducted a preliminary analysis of how convolution layers perform on GPU-PIM systems with varying configurations to examine the prospect of the performance gain from parallelizing them across devices and guide our compiler and architecture design in Section 4. 1. The majority of DNN inference models including CNN do not have enough inherent inter-node parallelism to fully utilize PIM units in parallel with GPU. Many DNN models are known to not offer much inter-node parallelism since data simply flow through a straight-lined sequence of layers without branches. We found that zero or less than 17% of the graph nodes have nodes without data-flow dependency in 75% of the Torchvision <ref type="bibr" target="#b39">[40]</ref> CNN models. Enabling independent nodes to execute in parallel on GPU and PIM introduces software complexity to solve the placement and scheduling problems with some form of GPU and PIM performance models. It would be challenging to achieve a meaningful parallelization speedup and justify the complexity if target graphs had few independent nodes. 2. The PIM performance of many convolutional layers does not dominate the GPU performance, and vice versa; parallelization across GPU and PIM can further reduce the critical execution path. Even in graphs with little internode parallelism, each node has abundant intra-node data parallelism over input or output tensors. GPU and PIM are specifically designed to exploit such data parallelism to accelerate compute-intensive and memory-intensive operations, respectively. For example, memory-intensive FC layers are an order of magnitude faster on PIM than on GPU <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, while CONV layers often fully utilize GPU cores with cached activations and filters. However, for layers with moderate data reuse and arithmetic intensity, e.g., pointwise CONV layers with deep input/output channels, neither hardware outperforms the other by a crushing margin, i.e., PIM and GPU performance are within a close range. Thus, supporting mixed-parallel execution, i.e., task-parallelism for dataparallel kernels, can achieve a further speedup by overlapping the execution of GPU and PIM kernels. 3. GPU memory can be configured to act as both a regular DRAM and a PIM device to minimize data movement overheads for inter-node parallel execution while maintaining GPU kernel performance. Inter-node parallel execution introduces synchronization and data movement overheads between devices. If devices connected through PCIe operate independently, communication overheads can easily offset performance gain from computation acceleration. However, if a part of GPU memory is enabled with PIM compute units, a GPU kernel can run in parallel with a PIM kernel sharing the same physical memory and moving data only between channels. This configuration also simplifies PIM command generation as it does not require a separate device driver for address mapping. Dedicating a subset of channels to PIM could slow down GPU kernels compared to when all channels are accessible to GPU, but our preliminary experiments showed that compute-intensive models are not noticeably impacted, even when the number of memory channels is halved (Fig. <ref type="figure" target="#fig_2">3</ref>), due to its high computation-communication ratio. By offloading memoryintensive layers to PIM-enabled channels, this GPU-PIM dual configuration can achieve PIM acceleration without sacrificing GPU performance and increasing DRAM size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design and Implementation</head><p>Guided by the motivating observations in the previous section, we propose PIMFlow, compiler and runtime mechanisms that enable mixed-parallel GPU-PIM acceleration for CNN models. We also introduce a PIM-enabled GPU memory architecture based on <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PIM-Enabled GPU Memory</head><p>We extended the DRAM-PIM architecture in prior work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref> to support GPU-PIM parallel execution with minimal overheads and optimize PIM operation latencies. GPU/PIM memory channel grouping. In order to efficiently execute GPU and PIM workloads on a single memory, independently and in parallel while minimizing software changes, we configure a single DRAM to serve as both GPU memory and PIM device by dividing the memory channels into two contiguous sets: regular channels for GPU data and PIM-enabled channels. There is a trade-off between performance and area/power with regard to the number of PIMenabled channels in the memory. Having all channels PIMcapable will maximize the PIM computing power, but the added complexity with area and power overheads is hard to justify as many GPU kernels with unsupported operations on PIM cannot use PIM features at all. Thus, we augment only a subset of memory channels with PIM hardware so that we can achieve close-to-ideal PIM acceleration while  minimizing underutilized PIM resources.</p><formula xml:id="formula_0">Host Core Main Memory GPU L2 Cache Global Memory ch1 ch2 ch3 ch4 ch5 ch6 ch7 ch8 ch9 ch10 ch11 ch12 ch13 ch14 ch15 ch16 GPU Memory Channel ch1 ch2 ch3 ch4 ch5 ch6 ch7 ch8 ch9 ch10 ch11 ch12 ch13 ch14 ch15</formula><p>For GPU kernels that do not issue DRAM-PIM commands, the memory behaves the same way as traditional GPU memories. When GPU kernels activate PIM units, offloaded data are mapped to the PIM-enabled memory channels and then used as operands for PIM compute units. The memory controller of the PIM-enabled DRAM is updated to place data in the proper channels and move them between the channels, if needed. Fig. <ref type="figure" target="#fig_3">4</ref> shows how data is transferred between the host memory and distinct sets of channels in the PIMenabled GPU memory. The data is initially transferred from the host memory to the GPU memory channels. When GPU and PIM kernels are launched to execute in parallel <ref type="bibr" target="#b0">(1)</ref>, data is moved to the PIM channels before PIM kernels execute <ref type="bibr" target="#b1">(2)</ref>. Once a PIM kernel finishes its execution, data is transferred back to the host memory for CPU kernels (3) (e.g., activation functions following FC or CONV layers) or back to GPU memory if the data is requested by another kernel <ref type="bibr" target="#b3">(4)</ref>. We assume all GPU and PIM memory channels are connected to each other forming memory networks <ref type="bibr" target="#b32">[33]</ref>, since direct memory interconnect incurs much less contention than the GPU L2 cache crossbar, as shown in <ref type="bibr" target="#b62">[63]</ref>. PIM command and global buffer extension. The DRAM-PIM hardware supports a set of PIM commands to move data for PIM compute units and activate them; GWRITE and READRES commands push input data to the global buffer and read out computed results respectively, while COMP triggers PIM computation and G_ACT activates multiple banks. These commands are usually issued in the following order: GWRITE-G_ACT-COMP-READRES. We extended the GWRITE command as follows to accelerate CONV operations on PIM more efficiently, while reusing the command semantics as described in <ref type="bibr" target="#b25">[26]</ref> for the rest:</p><p>? Multiple global buffers: We observed that CONV filters are often lowered into small matrices that cannot fully utilize PIM units. As G_ACT fetches the same filter elements from memory to be multiplied by data brought in by different GWRITE commands, we use four global buffers instead of one <ref type="bibr" target="#b25">[26]</ref> or two <ref type="bibr" target="#b37">[38]</ref> to reuse G_ACTs for higher PIM utilization and data reuse (the command reuse optimization is implemented in the DRAM-PIM back-end). We also added GWRITE_2 and GWRITE_4 commands to access two or four global buffers with a single PIM command. ? Strided GWRITE: We extend GWRITE to accept three additional arguments so that input tensor elements in noncontiguous memory addresses can be pushed into the global buffer with a single GWRITE command. This command helps simplify PIM command sequences to compute non-pointwise CONV layers. ? GWRITE latency hiding: We hide GWRITE latency by asynchronously issuing a following G_ACT command, which significantly reduces PIM cycles. These commands cannot be activated simultaneously when all memory channels are involved in data fetch as in <ref type="bibr" target="#b25">[26]</ref>, but in our DRAM-PIM architecture with separate GPU and PIM memory channels, data can be fetched from GPU channels while PIM channels activate memory rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PIMFlow Compiler and Runtime Support</head><p>PIMFlow is implemented as a compiler and runtime extension in DL frameworks, and consists of three main components as shown in Fig. <ref type="figure">5</ref>. PIM-aware graph transformations identify candidate ONNX graph nodes for PIM acceleration, determine the execution mode, e.g., full/no offloading, multi-device data-parallel, or pipeline-parallel, and transform them accordingly. We use the execution mode and task size search engine to search for the optimal execution mode for each node. The resulting ONNX graphs are compiled and executed by the TVM back-end for DRAM-PIM. This includes memory and command optimization passes to reduce data communication overheads and improve PIM utilization, as well as a PIM command generation pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">PIM-aware Graph</head><p>Transformations. We extend the current heterogeneous-parallel execution model in DL frameworks to execute independent graph nodes in parallel on GPU and DRAM-PIM. We support the following mixedparallel execution modes.</p><p>? Multi-Device Data-Parallel (MD-DP) execution: GPU and PIM kernels execute the same task but with a disjoint portion of input data. Each kernel exploits internal dataparallelism. ? Pipelined execution: GPU and PIM kernels with data dependency overlap the execution of their pipeline stages across GPU and PIM. Each pipeline stage kernel exploits internal data-parallelism.</p><p>PIMFlow implements two ONNX graph transformation passes that create inter-node parallelism to activate the mixed-parallel execution modes (Fig. <ref type="figure">5</ref>). For both passes, we assume the PIM candidate nodes to be FC and CONV layers (except for DW CONV), while all the other layers are GPU-executable only. These passes work with the code generator and the execution engine so that proper GPU and DRAM-PIM kernel codes are generated and scheduled to execute in parallel while respecting data dependency.</p><p>Multi-device parallelization pass. This pass splits a single PIM-candidate node into two nodes to activate the MD-DP execution mode. For example, node 2 in Fig. <ref type="figure">5</ref> is split into 2(A) and 2(B), connected to the producer node of 2. The input data flowing into 2 is sliced into two subsets, each of which becomes input data for 2(A) and 2(B). Lastly, the output data of 2(A) and 2(B) are concatenated to produce a single output tensor equivalent to the original output of 2.</p><p>Pipelining pass. This pass takes a subgraph of two or more consecutive nodes and splits each node into multiple pipeline stage nodes to generate inter-node parallelism between pipeline stage nodes processing different data. In Fig. <ref type="figure">5</ref>, nodes 3 and 4 are pipelined with two pipeline stages. Node 3(A) and 4(B) are prologue and epilogue nodes, which can be executed on GPU or DRAM-PIM, while 3(B) and 4(A) are executed in parallel. The "concat" node before 4(B) is inserted to enforce data dependency for boundary elements from 3(A) when filters are bigger than 1x1. Finally, the outputs of 4(A) and 4(B) are concatenated to produce the subgraph output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Execution</head><p>Mode and Task Size Search. PIMFlow performs a hardware-measurement-based search prior to model compilation to determine which execution mode and task splitting ratio to use for each node in the input graph.</p><p>To model node performance in the MD-DP mode, PIMFlow generates and profiles samples with different GPU-PIM task splitting ratios (at an interval of 10%) for each PIM-candidate layer (FC and CONV). Thus, the search generates 11 samples for each target layer, including 0/100 and 100/0 ratios for full GPU and DRAM-PIM execution, respectively. PIMFlow uses the multi-device parallelization pass to generate samples with splitting ratios between 10/90 and 90/10, while using the original graph for full GPU or PIM execution. for ? ? 1 to ? do ? Solve by dynamic programming 24:</p><p>for ? ? 1 to ? do 25:</p><p>for ? ? 1 to ? -1 do 26:</p><p>if ? + ? &gt; ? then 27: continue 28:</p><formula xml:id="formula_1">? [? ] [? ] ? min(? [? ] [? ],? [? ] [? ] + ? [? + ? ] [? -? ] ) 29: return ? [1] [? ]</formula><p>PIMFlow also recursively traverses the model graph to identify all pipelining candidate subgraphs and executes the pipelining pass in Section 4.2.1 to transform them. We extract promising subgraphs with a PIM-candidate node followed or preceded by a non-PIM node (unsupported operator) from the graph. Subgraphs with non-PIM nodes that are computationally very lightweight, e.g., element-wise multiplication/addition and max pooling, compared to CONV and FC layers, or that have a data-flow dependency with multiple nodes that make pipelining impossible or very complicated, are excluded from candidates, since the cost of pipelining in these cases is likely to exceed the potential performance gain. We analyzed the model architectures of CNN models and identified a sequence of 1x1 and DW CONV layers as a frequent and promising subgraph pattern. These layers often follow each other and have similar arithmetic intensity, but DW CONV layers are only executed on GPU because it is not straightforward to offload DW CONV to current DRAM-PIM as it requires the global buffer to be flushed for each input channel.</p><p>Algorithm 1 illustrates the overall search process. The algorithm searches for optimal runtime at 10% ratio intervals (lines 3-6), and measures all pipelining candidates starting from each node and expanding subgraphs one by one (lines 11-15). The above search results are recorded in ? (lines <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, and we compute an optimal solution with dynamic programming (lines 23-28).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TVM Back-End for DRAM-PIM</head><p>TVM back-end for DRAM-PIM implements two optimization passes to increase command-level parallelism and reduce overheads. It also includes a code generator that generates DRAM-PIM commands for offloaded PIM nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">DRAM-PIM Command</head><p>Generator. The DRAM-PIM back-end is designed to take transformed model graphs as input, generate PIM commands for PIM-offloaded nodes, and execute GPU and PIM kernels in parallel. We mark PIMoffloaded nodes by prefixing the node names and passing them as Relay IR attribute to trigger the DRAM back-end. We extend the TVM execution engine to launch GPU and PIM kernels in parallel while reusing the existing TVM mapping for GPU nodes to cuDNN, cuBLAS, or CUTLASS library calls. The command generator includes a command scheduling pass to distribute PIM commands across channels to fully utilize all PIM compute units. This prevents channels from being idle when matrices to be placed in memory are too small, which is often the case for 1x1 CONV layers. Our scheduling mechanism distributes PIM commands at G_ACT (1), READRES (2), and COMP (3) granularity as shown in Fig. <ref type="figure">6</ref>, which progressively increases channel-level parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Memory</head><p>Optimizer. Splitting and pipelining nodes require input tensors to be sliced. When the kernel width or height is greater than one, additional input tensor elements around the splitting edges are needed to convolve with the kernel elements. Thus, we need an extra "Pad" operator before each input tensor. Also, after nodes are executed in the MD-DP mode, computation results on GPU and DRAM-PIM must be combined into a single tensor as input for subsequent layers. This requires an additional "Concat" operator to join split kernels. We found that "Slice", "Pad", and "Concat" operators incur significant data copy overheads, making most splitting attempts futile. Therefore, we devised a memory layout optimization to eliminate the overheads. We assume the memory layout for CONV layers in NHWC format, and inference is done in a single batch size. As shown in Fig. <ref type="figure" target="#fig_4">7</ref>, slicing/concatenating tensors in the input height dimension (? ) is a no-op if two split input/output tensors are located in contiguous memory space. Additionally, if we pre-allocate the memory space of the size of the input tensor plus padding and initialize them to 0, we can eliminate the "Pad" operator by writing input data from a padding offset. Thus, during code generation, we allocate contiguous memory space including padding for input/output of CONV layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methodology</head><p>We implemented ONNX graph transformation passes by using ONNX opset version 13 <ref type="bibr" target="#b0">[1]</ref>, and extended the TVM compiler version 0.8 (commit 7e376e2) <ref type="bibr" target="#b10">[11]</ref> to support DRAM-PIM as a back-end using the Bring-Your-Own-Codegen (BYOC) interface <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b60">61]</ref>. GPU kernels are based on CUDA 11.3.1 and cuDNN 8.2 libraries. PIMFlow allows an individual application to choose whether to enable the PIM capability in the GPU memory. If disabled, applications are compiled and executed to use all memory channels as regular load/store units for GPU. The execution mode and task size search phase is executed once per kernel prior to compilation to decide the execution mode, and search results are stored as a metadata log file for later compilations. We implemented the memory optimizer by modifying codes to lay out input matrices for cuDNN library calls and PIM kernels so that their addresses are contiguous when split. We plan to move the implementation to the compiler back-end and automate the memory address generation. Evaluated models. We evaluated the single-batch inference time of five CNN models: EfficientNetB0 (ENetB0) <ref type="bibr" target="#b56">[57]</ref>, MnasNet <ref type="bibr" target="#b55">[56]</ref>, MobileNetV2 (MBNetV2) <ref type="bibr" target="#b50">[51]</ref>, ResNet50 <ref type="bibr" target="#b24">[25]</ref>, and VGG16 <ref type="bibr" target="#b52">[53]</ref>. PIM candidate layers in these models include 1x1, 3x3, 5x5, and 7x7 CONV layers and FC layers. For pipelined execution, we used three subgraph patterns: 1x1-DW, DW-1x1, and 1x1-DW-1x1, where DW layers are executed on GPU while 1x1 layers are on DRAM-PIM. GPU and DRAM-PIM simulators. We implemented a simulator for the Newton-based DRAM-PIM architecture by extending Ramulator (commit 4edcb0d) <ref type="bibr" target="#b33">[34]</ref>. We modified the DRAM controller to process both GPU memory commands and PIM commands. We set the latency of the PIM commands based on parameter descriptions in <ref type="bibr" target="#b25">[26]</ref>, and adapted them for GDDR6 DRAM, as shown in Table <ref type="table" target="#tab_4">1</ref>. TVM DRAM-PIM back-end interfaces with this simulator to generate PIM command traces for PIM-offloaded layers and measures the trace execution time. We used Accel-Sim <ref type="bibr" target="#b31">[32]</ref> (commit 000be7f) to generate and simulate GPU traces on NVIDIA GeForce RTX 2060 GPU. We enabled the "write-through" mode for GPU caches, which guarantees data coherence at the memory level for PIM commands and memory accesses from GPU. <ref type="foot" target="#foot_2">2</ref> We used AccelWatch <ref type="bibr" target="#b29">[30]</ref> integrated with Accel-Sim to measure GPU energy consumption, and CACTI 7 <ref type="bibr" target="#b3">[4]</ref> with the energy parameters adapted from <ref type="bibr" target="#b53">[54]</ref> to measure PIM energy consumption. Simulator validation. We validated our simulator using the matrix-vector kernel benchmarks evaluated in <ref type="bibr" target="#b25">[26]</ref>. Fig. <ref type="figure" target="#fig_5">8</ref> is a reproduced version of Fig. <ref type="figure" target="#fig_1">12</ref> in <ref type="bibr" target="#b25">[26]</ref> on our simulator, comparing the PIM and GPU performance with different batch sizes. We matched software and hardware configurations to the best of our knowledge; we used the HBM timing parameters based on <ref type="bibr" target="#b25">[26]</ref> and CUTLASS v1.3 <ref type="bibr" target="#b30">[31]</ref> for GPU kernels and NVIDIA Titan V GPU with 24 memory channels for GPU configurations. The experiment shows that our simulator performs 20.4x better than GPU (batch size = 1), which is a conservative speedup compared to 50x in <ref type="bibr" target="#b25">[26]</ref> but closer to 10x in follow-up research <ref type="bibr" target="#b37">[38]</ref>. While PIM performance scales consistently with matrix sizes, we found that GPU kernels show widely varying behaviors, depending on matrix sizes and library versions. Considering the inherently limited capacity for validation against a proprietary architecture, we focus on simulating a realistic version of PIM-enabled memory and using a widely-used cuDNN library for a fair evaluation. Evaluated PIM offloading mechanisms.</p><p>? Baseline: GPU-only execution with a 32-channel memory.</p><p>? Newton+: The baseline Newton <ref type="bibr" target="#b25">[26]</ref> with offloading support for CONV and FC layers (no mixed-parallel execution support) and command scheduling for multiple channels. ? Newton++: Newton+ with the PIM command optimizations.</p><p>? PIMFlow-md and PIMFlow-pl: Newton++ with mixedparallel execution support for MD-DP (PIMFlow-md) and pipelining only (PIMFlow-pl). ? PIMFlow: PIMFlow with full optimizations and execution model support as proposed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>In this section, we show how much performance and energy improvement PIMFlow can achieve by enabling CNN models on PIM-enabled GPU memory. We also present sensitivity  analysis results with different memory configurations, software parameters, and model sizes/types, to investigate the feasibility and performance impact of our design decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">CNN Model Performance</head><p>We measured the inference time of five CNN models compiled by PIMFlow and the other offloading mechanisms on simulated hardware configurations as listed in Section 5. For stable results, each simulation is repeated three times. Fig. <ref type="figure">9</ref> presents (1) the execution time of all PIM-candidate CONV layers and (2) end-to-end model inference time for all evaluated models, normalized to the GPU baseline. In summary, PIMFlow with full MD-DP and pipelined execution support provides a 30% speedup on average for CONV layers against the GPU baseline, while outperforming all of the other offloading mechanisms. The performance gain is more significant -up to 48% with ENetB0, MBNetV2, and MnasNet than ResNet50 and VGG16 -since more computeintensive CONV layers in the latter models do not provide as much speedup on DRAM-PIM as the former models. For  end-to-end inference time, ENetB0 and VGG16 gain 27% and 5% additional speedups on top of 43% and 17% for CONV layers, respectively, by offloading FC layers to the DRAM-PIM. Overall, the results show that compiler and runtime support by PIMFlow can enable CNN models on the DRAM-PIM hardware with substantial performance gain for both FC and CONV layers without impacting the hardware. Newton+ vs. Newton++. Newton++ outperforms Newton+ by 20% on average for all CONV layers (up to 37% for MBNetV2), showing that optimizing PIM commands alone can boost PIM capabilities (more details in Section 6.2). Newton++ vs. PIMFlow-md. Comparing the layerwise and modelwise execution times of Newton++ and PIMFlow-md highlights the performance improvement enabled by the MD-DP execution mode (Fig. <ref type="figure">9</ref> and<ref type="figure" target="#fig_6">10</ref>). The splitting ratio between GPU and DRAM-PIM varies depending on how memory-intensive a layer is and how it performs on GPU. Table <ref type="table" target="#tab_6">2</ref> shows that 58% of the PIM-candidate layers are split across GPU and DRAM-PIM and executed in parallel, while 41% fully offload to DRAM-PIM (some of the layers were considered for pipelined execution as well, but executed in parallel based on search results). We observed that the MD-DP execution mode enables many layers that were not offloading candidates for Newton++ to gain speedups with parallel execution on GPU and DRAM-PIM (e.g., layers  Newton++, on average. These are the target CONV layers identified in Section 3 whose performance on DRAM-PIM is not significantly better or worse than GPU. While full PIM offloading could also make a difference for these layers, parallel execution further reduced the critical execution path length with minimal parallelization overhead. PIMFlow-md vs. PIMFlow-pl. In our current implementation, the MD-DP execution mode identifies all CONV layers except for DW as candidates while the pipelined execution mode uses pre-defined subgraph patterns to find a limited set of candidate layers. Their candidate layers partially overlap, so we analyzed how PIMFlow-md and PIMFlow-pl perform comparatively for the PIMFlow-pl patterns with a layerwise and stagewise breakdown. Fig. <ref type="figure" target="#fig_0">11</ref> presents pipelined subgraphs that show &gt;10% speedup or &lt;25% slowdown with PIMFlow-pl compared to PIMFlow-md in all evaluated models. We used three subgraph patterns for pipelining as shown on the top of Fig. <ref type="figure" target="#fig_0">11</ref>, and only the Type 1 pattern outperforms the same nodes parallelized in MD-DP mode. This pattern can effectively overlap GPU kernels (G.1 and G.2) with PIM kernels (Pa.2 and Pb.1), while prologue and epilogue GPU kernel latencies are too high in Type 2 and 3 patterns. PIMFlow-md/PIMFlow-pl vs. PIMFlow. PIMFlow with full MD-DP and pipelining support outperforms the PIMFlow-md and PIMFlow-pl variations by 1% and 12% on average, respectively. For ENetB0, MBNetV2, and MnasNet, PIMFlow-md and PIMFlow-pl separately achieve significant speedups: 38% and 41% on average against the baseline (Fig. <ref type="figure">9</ref>). When combined, PIMFlow improves the inference time by 43% on average for these models. The speedup numbers do not strictly add up due to having common candidate layers, as described above. For ResNet50 and VGG16 with a few to zero pipelining pattern matches, PIMFlow performs the same as PIMFlow-md. Energy consumption. Fig. <ref type="figure" target="#fig_1">12</ref> shows that both Newton++ and PIMFlow consume significantly less energy than the GPU baseline by 18% and 26%, respectively. The result aligns with prior work that the fixed-function MAC logic in memory requires less power for the same computation than dense GPU cores and additionally saves power by reducing data transfers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref>. Newton++ uses 17% less energy than Newton+, as the PIM-command optimizations reduces bank activation. The models with relatively small speedup on PIM-Flow (Resnet50 and VGG16) show limited or negative energy gains due to the increased GPU static power for computeintensive layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sensitivity Study</head><p>GPU/PIM memory channel ratio. We experimented with different ratios of GPU-only and PIM-enabled memory channels to investigate how they impact overall model performance. Fig. <ref type="figure" target="#fig_2">13</ref> shows that as the number of PIM channels increases (and the number of GPU channels decreases) in a 32-channel memory, PIMFlow consistently improves model performance thanks to PIM acceleration, up to a certain point (16 PIM channels), then slows down as GPU kernel performance suffers from too few memory channels. The 16-16 channel division in our PIM-enabled GPU memory is derived from this experimental result. The positive impact of PIM acceleration is weaker and the negative impact of reduced GPU memory is more severe for Newton+ and Newton++, especially for ResNet50 with more compute-intensive layers than ENetB0, which reaffirms the contribution of PIMFlow to extending the scope of PIM utilization. PIM-command optimizations. We isolated the performance impact of two PIM-command optimizations, GWRITE latency hiding and multiple global buffers. Fig. <ref type="figure" target="#fig_3">14</ref> shows that GWRITE latency hiding provides a 9% speedup on its own while multiple global buffers provide a 14% speedup against Newton+. PIMFlow achieves a 22% speedup on average by combining them; thus, we see that neither optimization absorbs or interferes with the effect of the other and independently contributes to the performance. Pipeline stage granularity. Increasing the number of pipeline stages will reduce prologue and epilogue overheads, but increase kernel launch and communication overheads. We examined how the number of pipeline stages may impact performance, and found that having more than two stages leads to larger overheads than the performance gain from overlapped execution (Fig. <ref type="figure" target="#fig_0">15</ref>).</p><p>Model type and size. We evaluated BERT <ref type="bibr" target="#b15">[16]</ref> and scaledup versions of ENet, MBNetV2, and MnasNet) to see how PIMFlow features work for DNN models of different types and sizes. For BERT, while PIMFlow performs the same as Newton++ for small input (1x3) as used in <ref type="bibr" target="#b25">[26]</ref>, PIMFlow provides a 32% extra speedup over Newton++ for 1x64 input by executing FC layers in the MD-DP mode. The PIMFlow acceleration for the mobile CNN models decreases as the model size increases, going down to 7% for ENetB6 against the GPU baseline. This is because even with 1x1 CONV layers, the arithmetic intensity and data reuse increase, favoring GPU execution over PIM as the model size grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Area overhead. The baseline DRAM-PIM architecture reported the area overhead for PIM-related logic to be 0.19 mm 2 <ref type="bibr" target="#b37">[38]</ref> per bank. Our PIM-enabled memory introduced additional area overheads for a crossbar interconnect between channels and larger global buffers (4KB per channel). We estimate the area of global buffers to be 0.33 mm 2 based on <ref type="bibr" target="#b3">[4]</ref>, and the area of the crossbar and long links to be 1.53 mm 2 in 32-channel memory referring to <ref type="bibr" target="#b62">[63]</ref>, which are around 0.72% of the GPU die area in total.</p><p>Contention at memory controller. The GPU memory controller is extended to handle PIM commands too, which can increase contention at the controller. While a PIM channel reads activation data from GPU channels, the GPU memory controller cannot accept GPU memory commands. We simulated this contention by interleaving Accel-Sim memory commands with PIM command sequences to the DRAM-PIM simulator, and the slowdown due to the contention was negligible at 0.15% for MBNetV2 and 0.22% for Resnet50 compared to no-contention cases. Compilation overhead. Compilation time is dominated by the hardware measurement time during the execution mode and task size search phase, which is proportional to the number of FC and CONV layers in a model. Though profiling on our simulators takes several hours, measurements on actual hardware are expected to finish within several minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Digital Processing-in-Memory DRAM. Many researchers have proposed PIM architectures for DL model acceleration <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">60]</ref>. <ref type="bibr" target="#b14">[15]</ref> performed matrix multiplication in a systolic-array accelerator per MAC bank, which takes activations from Broadcast bank. <ref type="bibr" target="#b13">[14]</ref> observed that small batch sizes result in memory-bound matrix operations during model inference, and processed these operations near memory while handling complex memory address mapping. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref> accelerated matrix-vector multiplication with in-memory MAC logic and global buffers, while <ref type="bibr" target="#b36">[37]</ref> introduced an HBM-based PIM architecture that supports MAC and elementwise operations at the bank level. Recent work tends to limit PIM compute capability to MAC or ele-mentwise computations considering hardware constraints. PIMFlow is designed with such PIM architectures in mind, and thus it can be readily adapted to support them. Software support for PIM hardware. To make PIM hardware accessible and fully utilized, it is important to provide software interfaces to mitigate the burden on programmers. <ref type="bibr" target="#b42">[43]</ref> provided compiler support to generate PIM binaries, but required programmers to annotate programs with directives to identify PIM computations. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b57">58]</ref> built a cost model using profiling results and decided which computation to offload to PIM, while <ref type="bibr" target="#b61">[62]</ref> profiled a subset of loop iterations to dynamically offload loops to PIM. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref> implemented a software stack for GPU-PIM systems that uses a cost-benefit analysis of memory bandwidth consumption for offloading decisions. <ref type="bibr" target="#b58">[59]</ref> allowed programmers to restructure target programs to optimize memory access patterns for PIM using a runtime memory traffic monitor. While these efforts could automate the offloading process, they did not proactively transform programs to generate more offloading opportunities as PIMFlow does.</p><p>Placement and scheduling mechanisms for heterogeneous systems. Scheduling workloads on heterogeneous systems is an open problem with intractable complexity <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>. <ref type="bibr" target="#b47">[48]</ref> proposed a way to utilize the GPU with PIM-enabled memory. It analyzed GPU kernels by using metrics such as memory intensity and scheduled each kernel concurrently on both GPU and PIM cores. <ref type="bibr" target="#b40">[41]</ref> proposed a hierarchical task scheduler for heterogeneous systems that divided a coarse-grained task into subtasks and scheduled them on accelerators. On CPU-GPU heterogeneous systems, <ref type="bibr" target="#b28">[29]</ref> used online profiling to reduce load imbalance and increase parallelism for scheduled workloads across devices. While PIMFlow shares the goal of maximizing parallelism on heterogeneous systems with prior work, we focus specifically on PIM-aware compiler transformations and runtime support to maximize PIM utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>This paper presented our effort to expand the scope of PIM acceleration to CNN models, widely used as a backbone but out of the spotlight for PIM target applications. With the PIM-aware graph transformations, command-generating back-end, and mixed-parallel execution runtime specifically designed to accelerate convolutional layers on DRAM-PIM, PIMFlow showed strong potential for an optimizing software stack for commercial DRAM-PIM. For future work, we plan to apply an auto-tuning approach to our execution mode and task size search for more optimized code generation.</p><p># execute docker container docker run -it --gpus=all --rm \ yongwonshin/pimflow:v0.1 # install prerequisites ./install.sh A.4.2 Local Installation (Zenodo). The Zenodo archive provides Git patch files for TVM, Accel-Sim, GPGPUSIM, and Ramulator, and a compressed PIMFlow repository that includes all execution and profiling scripts, ONNX transformation passes, and GPU traces. Git patch files can be applied to each repository by using the following command:</p><p># from commit: 000be7f git am PIMFlow_accel-sim-framework.patch # from commit: 13c6711 git am PIMFlow_gpgpu-sim_distribution.patch # from commit: 4edcb0d git am PIMFlow_ramulator.patch # from commit: 7e376e2 git am PIMFlow_tvm.patch</p><p>The PIMFlow repository can be extracted by using the following command: unzip PIMFlow_b30b0b8.zip An installation guide is included in each repository, and an overall environment setup guide for evaluation is provided by the README file in the PIMFlow GitHub repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Experiment Workflow</head><p>The overall experiment workflow consists of three steps: candidate profiling, candidate search and selection, and end-toend execution. You can jump to Step 2 if you reuse previously profiled data, and to Step 3 if you have already computed the optimal graph. We use the Toy network for the following installation sequence. The &lt;net&gt; option can be efficientnet-v1-b0, mobilenet-v2, mnasnet-1.0, resnet-50, or vgg-16.</p><p>Step 1: Profile each CONV layer using MD-DP or pipelining transformation pass.</p><p>./pimflow -m=profile -t=split -n=&lt;net&gt; ./pimflow -m=profile -t=pipeline -n=&lt;net&gt;</p><p>Step 2: Compute the optimal ONNX graph based on the result from Step 1.</p><p>./pimflow -m=solve -n=&lt;net&gt;</p><p>Step 3: Execute the transformed model. Use the --gpu_only option for GPU execution time.</p><p>./pimflow -m=run --gpu_only -n=&lt;net&gt; ./pimflow -m=run -n=&lt;net&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Evaluation and Expected Result</head><p>After steps 1 and 2, the resulting ONNX graph and GPU/DRAM-PIM traces for the graph are created in PIMFlow. Also, profiling results are saved in PIMFlow/layerwise and PIMFlow/pipeline for MD-DP and pipelined executions, respectively. Executing the following command will run the traces and generate an execution time graph for all PIMcandidate CONV layers with four offloading mechanisms (Fig. <ref type="figure" target="#fig_9">17</ref>). The &lt;policy&gt; option can be Newton+, Newton++, Pipeline, MDDP, or PIMFlow. A.7 Experiment Customization Profiling scripts are customizable with MD-DP split ratios and pipeline patterns that are different from what is used in the paper. The number of memory channels used for DRAM-PIM hardware can also be customizable for hardware design space exploration. The main execution script can take as input other CNN/DNN models that were not evaluated in the paper and optimize them with PIMFlow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Runtime breakdown of CNN models on NVIDIA RTX 2080 Ti GPU (left) and arithmetic intensity (# of MAC divided by # of LD/ST) of convolutional layers in the models (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Convolution lowering and mapping to the baseline DRAM-PIM architecture [26].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Model inference time on GPU with different number of memory channels, normalized to runtime on GPU with 24 memory channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. PIM-enabled GPU memory architecture with data movement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. An example of memory optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Simulator validation based on batch size sensitivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Layerwise performance breakdown for nodes executed in the MD-DP mode (normalized to the GPU baseline).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Figure 11. Layerwise performance breakdown for pipelining candidate subgraphs (left bars for nodes when executed in the MD-DP mode, right bars when pipelined).0.2 0.4 0.6 0.8 1 1.2 1.4 1.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Model type and size sensitivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>#Figure 17 .</head><label>17</label><figDesc>Figure 17. Example output graph (normalized to the GPU baseline).</figDesc><graphic url="image-2.png" coords="12,317.96,274.51,240.05,87.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1</head><label></label><figDesc>Algorithm 1 Execution Mode and Task Size Search</figDesc><table><row><cell cols="2">Require: Graph G</cell><cell></cell></row><row><cell cols="3">Ensure: Nodes in graph G are topologically sorted</cell></row><row><cell cols="3">1: function optimal_split(?, ????, ? )</cell></row><row><cell>2:</cell><cell>???? _??????? ? ?</cell><cell></cell></row><row><cell>3:</cell><cell>for ratio for every 10% do</cell><cell></cell></row><row><cell>4:</cell><cell cols="2">??????? ? ??_????? (?, ????, ????? )</cell></row><row><cell>5:</cell><cell cols="2">if ???? _??????? &gt; ??????? then</cell></row><row><cell>6:</cell><cell cols="2">???? _??????? ? ???????</cell></row><row><cell>7:</cell><cell cols="2">? [????.????? ] [1] ? ???? _???????</cell></row><row><cell cols="2">8: function pipeline(?, ????, ? )</cell><cell></cell></row><row><cell>9:</cell><cell cols="2">???? _???? ? ????_?????????? (?, ???? )</cell></row><row><cell>10:</cell><cell>? ? 2</cell><cell></cell></row><row><cell>11:</cell><cell cols="2">while ???? _????.??_? ??? = Conv do</cell></row><row><cell>12:</cell><cell cols="2">??????? ? ??_???????? (?, ????, ? )</cell></row><row><cell>13:</cell><cell cols="2">? [????.????? ] [? ] ? ???????</cell></row><row><cell>14:</cell><cell cols="2">???? _???? ? ????_?????????? (?, ???? _???? )</cell></row><row><cell>15:</cell><cell>? ? ? + 1</cell><cell></cell></row><row><cell cols="3">16: function execution_mode_search(?)</cell></row><row><cell>17:</cell><cell>? ? ? .???? ()</cell><cell>? The number of nodes in the ?</cell></row><row><cell>18:</cell><cell>? ? ??? [? ] [? ]</cell><cell>? Allocate table ?</cell></row><row><cell>19:</cell><cell>for ???? in ? do</cell><cell></cell></row><row><cell>20:</cell><cell cols="2">if ????.??_? ??? = Conv then</cell></row><row><cell>21:</cell><cell cols="2">optimal_split(?, ????, ? )</cell></row><row><cell>22:</cell><cell>pipeline(?, ????, ? )</cell><cell></cell></row><row><cell>23:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>DRAM configuration ? ?? : 2, ? ?? : 11, ? ?? : 11, ? ??? : 11, ? ??? : 2, ? ??? : 25</figDesc><table><row><cell>Num of Ranks</cell><cell>1</cell><cell cols="2">Num of Column I/Os per row 32</cell></row><row><cell>Num of Banks</cell><cell>16</cell><cell>Column I/O bit width</cell><cell>256b</cell></row><row><cell cols="3">Global buffer size 4 KB Num of Multipliers per bank</cell><cell>16</cell></row><row><cell cols="3">Timing Parameters (in clock cycles)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>The distribution of MD-DP splitting ratios</figDesc><table><row><cell></cell><cell></cell><cell cols="7">Split ratio to GPU (0: total offload)</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell cols="10">41% 5% 6% 9% 3% 10% 5% 6% 8% 6%</cell><cell>0%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>More fine-grained samples with</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2% ratio intervals provided a 1.13% speedup for EfficientNetB0, so we use 10% ratio intervals for simulation efficiency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>We observed a 2.8% slowdown compared to the "write-back" mode (Mo-bileNet), which may impact offloading decisions and splitting ratios but is tolerable considering the overall performance gain from PIM acceleration.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://github.com/NVlabs/NVBit</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://pytorch.org/vision/stable/models.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valuable feedback. This work was supported by <rs type="funder">Institute for Information &amp; communications Technology Promotion (IITP)</rs> (<rs type="grantNumber">2019-0-01906</rs>,<rs type="grantNumber">2021-0-00871</rs>,<rs type="grantNumber">2021-0-00310</rs>), <rs type="funder">National Research Foundation (NRF)</rs> (<rs type="grantNumber">2020M3H6A1084853</rs>), and <rs type="funder">National IT Industry Promotion Agency (NIPA)</rs> (<rs type="grantNumber">R-20210319-010567</rs>), funded by the <rs type="funder">Korean government (MSIT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EpWRtex">
					<idno type="grant-number">2019-0-01906</idno>
				</org>
				<org type="funding" xml:id="_E8Fz3Bm">
					<idno type="grant-number">2021-0-00871</idno>
				</org>
				<org type="funding" xml:id="_KXAa4YM">
					<idno type="grant-number">2021-0-00310</idno>
				</org>
				<org type="funding" xml:id="_Dj7vTCn">
					<idno type="grant-number">2020M3H6A1084853</idno>
				</org>
				<org type="funding" xml:id="_u2hkfGn">
					<idno type="grant-number">R-20210319-010567</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We also included DockerFile for manual docker build. The README at the archive has detailed installation instructions. A.4.3 Local Installation (GitHub). The following four GitHub repositories must be cloned in order to compile and run PIMFlow: https://github.com/yongwonshin/PIMFlow_tvm.git https://github.com/yongwonshin/PIMFlow_accel-simframework.git https://github.com/yongwonshin/PIMFlow_gpgpu-sim _distribution.git https://github.com/yongwonshin/PIMFlow_ramulator.git https://github.com/yongwonshin/PIMFlow.git</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-Availability Statement</head><p>The data that support the findings of this study are openly available in Zenodo (DOI: 10.5281/zenodo.7639153) <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Artifact Appendix</head><p>A.1 Abstract Our artifact consists of four parts: (1) ONNX transformation passes, (2) hardware measurement scripts for the execution mode and task size search, (3) an extended TVM compiler with DRAM-PIM back-end, and (4) GPU and DRAM-PIM simulators. For pre-generated input data, we provide GPU traces for the CNN models evaluated in the paper. These traces can also be generated by using NVBit. 3 We provide modified binaries and source codes for the TVM compiler extended with the DRAM-PIM back-end.</p><p>Artifact evaluation and testing are streamlined with a top-level script (pimflow) that controls different features of PIMFlow with lower-level scripts (details in Section A.5). The reproduction of the results in this paper can be conducted on any platform that can run the simulators, as long as all traces are generated and executed with the same simulator configurations as the paper.</p><p>A.2 Artifact Check-list (Meta-information) ? Algorithm: Methods for (1) PIM-aware graph transformation passes to enable MD-DP (Multi-Device Data-Parallel) and pipelined execution of CONV layers, (2) layer-wise profiling of MD-DP and pipelining candidates on GPU and DRAM-PIM, (3) dynamic-programming based algorithm to obtain the optimal execution mode and task size, and (4) code-generating back-end for DRAM-PIM. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Open Neural Network Exchange</title>
		<ptr target="https://onnx.ai/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<idno type="DOI">10.5281/zenodo.7639153</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7639153" />
		<title level="m">PIMFlow: Compiler and Runtime Support for CNN Models on Processing-in-Memory DRAM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PIM-enabled instructions: A low-overhead, locality-aware processingin-memory architecture</title>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoung</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2750385</idno>
		<ptr target="https://doi.org/10.1145/2749469.2750385" />
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="336" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories</title>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Kahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnav</forename><surname>Srinivas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3085572</idno>
		<ptr target="https://doi.org/10.1145/3085572" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2017-06">2017. jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks</title>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berkin</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldo</forename><forename type="middle">F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Shiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1109/PACT52795.2021.00019</idno>
		<ptr target="https://doi.org/10.1109/PACT52795.2021.00019" />
	</analytic>
	<monogr>
		<title level="m">2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="159" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks</title>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngsok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachata</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Shiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aki</forename><surname>Kuusela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Knies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173177</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173177" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Williamsburg, VA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="316" to="331" />
		</imprint>
	</monogr>
	<note>ASPLOS &apos;18). Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The fast Fourier transform</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Brigham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Morrow</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSPEC.1967.5217220</idno>
		<ptr target="https://doi.org/10.1109/MSPEC.1967.5217220" />
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="63" to="70" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online Scheduling of Task Graphs on Heterogeneous Platforms</title>
		<author>
			<persName><forename type="first">Louis-Claude</forename><surname>Canon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loris</forename><surname>Marchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?d?ric</forename><surname>Vivien</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2019.2942909</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2019.2942909" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="721" to="732" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding and Improving the Latency of DRAM-Based Memory Systems</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1712.08304</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1712.08304" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis, and Optimization. SIGMETRICS Perform</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijith</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2964791.2901453</idno>
		<ptr target="https://doi.org/10.1145/2964791.2901453" />
	</analytic>
	<monogr>
		<title level="j">Eval. Rev</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="323" to="336" />
			<date type="published" when="2016-06">2016. jun 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 13th USENIX Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bring Your Own Codegen to Deep Learning Compiler</title>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorn</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hsiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Delaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/2105.03215</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accelerating Bandwidth-Bound Deep Learning Inference with Main-Memory Accelerators</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeageun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><surname>Erez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis<address><addrLine>St. Louis, Missouri; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
	<note>) (SC &apos;21)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge</title>
		<author>
			<persName><forename type="first">Seunghwan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haerang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3011265</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2020.3011265" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><surname>Chame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Lacoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Granacki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewook</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">Woo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ihn</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>Daglikoca</surname></persName>
		</author>
		<idno type="DOI">10.1145/514191.514197</idno>
		<ptr target="https://doi.org/10.1145/514191.514197" />
		<title level="m">The Architecture of the DIVA Processing-in-Memory Chip</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Processing in memory: the Terasys massively parallel PIM array</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iobst</surname></persName>
		</author>
		<idno type="DOI">10.1109/2.375174</idno>
		<ptr target="https://doi.org/10.1109/2.375174" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix computations</title>
		<imprint>
			<publisher>JHU press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DLUX: A LUT-based nearbank accelerator for data center deep learning training workloads</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinfeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">T</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance analysis of multi-level parallelism: inter-node, intra-node and hardware accelerators</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Juckeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Brunst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CAIRO: A compiler-assisted technique for enabling instruction-level offloading of processing-in-memory</title>
		<author>
			<persName><forename type="first">Ramyad</forename><surname>Hadidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyojong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SIMDRAM: A Framework for Bit-Serial SIMD Processing Using DRAM</title>
		<author>
			<persName><forename type="first">Nastaran</forename><surname>Hajinazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldo</forename><forename type="middle">F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Gregorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo?o Dinis</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nika</forename><surname>Mansouri Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>G?mez-Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3445814.3446749</idno>
		<ptr target="https://doi.org/10.1145/3445814.3446749" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (Virtual, USA) (ASP-LOS &apos;21)</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (Virtual, USA) (ASP-LOS &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="329" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Near-Data Processing in Memory Expander for DNN Acceleration on GPUs</title>
		<author>
			<persName><forename type="first">Hyungkyu</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunuk</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jueon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongmin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyojin</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Euicheol</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwangsun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/LCA.2021.3126450</idno>
		<ptr target="https://doi.org/10.1109/LCA.2021.3126450" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="171" to="174" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A DRAM-maker&apos;s Accelerator-in-Memory (AiM) Architecture for Machine Learning</title>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choungki</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilkon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunseok</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mithuna</forename><surname>Thottethodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO50266.2020.00040</idno>
		<ptr target="https://doi.org/10.1109/MICRO50266.2020.00040" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE 48th Annu. Int. Symp. Microarchit</title>
		<meeting>ACM/IEEE 48th Annu. Int. Symp. Microarchit<address><addrLine>Newton</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent near-Data Processing in GPU Systems</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwangsun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladrish</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Keckler</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2016.27</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2016.27" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
		<meeting>the 43rd International Symposium on Computer Architecture<address><addrLine>Seoul, Republic</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="204" to="216" />
		</imprint>
	</monogr>
	<note>of Korea) (ISCA &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Maskrnn: Instance level video object segmentation. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive heterogeneous scheduling for integrated GPUs</title>
		<author>
			<persName><forename type="first">Rashid</forename><surname>Kaleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajkishore</forename><surname>Barik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Shpeisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunling</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
		<idno type="DOI">10.1145/2628071.2628088</idno>
		<ptr target="https://doi.org/10.1145/2628071.2628088" />
	</analytic>
	<monogr>
		<title level="m">2014 23rd International Conference on Parallel Architecture and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AccelWattch: A Power Modeling Framework for Modern GPUs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Kandiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Peverelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junrui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amogh</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Hardavellas</surname></persName>
		</author>
		<idno type="DOI">10.1145/3466752.3480063</idno>
		<ptr target="https://doi.org/10.1145/3466752.3480063" />
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO &apos;21). Association for Computing Machinery</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="738" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haicheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustyn</forename><surname>Blasig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ramini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Shivam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Majcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hohnerbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Nicely</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/cutlass" />
	</analytic>
	<monogr>
		<title level="j">CUTLASS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling</title>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhesheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA45697.2020.00047</idno>
		<ptr target="https://doi.org/10.1109/ISCA45697.2020.00047" />
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Toward Standardized Near-Data Processing with Unrestricted Data Placement for GPUs</title>
		<author>
			<persName><forename type="first">Gwangsun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladrish</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126908.3126965</idno>
		<ptr target="https://doi.org/10.1145/3126908.3126965" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis<address><addrLine>Denver, Colorado; New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
	<note>SC &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ramulator: A Fast and Extensible DRAM Simulator</title>
		<author>
			<persName><forename type="first">Yoongu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LCA.2015.2414456</idno>
		<ptr target="https://doi.org/10.1109/LCA.2015.2414456" />
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Archit. Lett</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1509.09308</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1509.09308" />
		<title level="m">Fast Algorithms for Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MVP: An Efficient CNN Accelerator with Matrix, Vector, and Processing-Near-Memory Units</title>
		<author>
			<persName><forename type="first">Sunjung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonkyung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hweesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<idno type="DOI">10.1145/3497745</idno>
		<ptr target="https://doi.org/10.1145/3497745" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Des. Autom. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2022-06">2022. jun 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kyomin Sohn, and Nam Sung Kim. 2021. Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology : Industrial Product</title>
		<author>
			<persName><forename type="first">Sukhan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Haeng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eojin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungwoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hosang</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyounghwan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Seongil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA52012.2021.00013</idno>
		<ptr target="https://doi.org/10.1109/ISCA52012.2021.00013" />
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A 1ynm 1.25V 8Gb, 16Gb/s/pin GDDR6-based Accelerator-in-Memory supporting 1TFLOPS MAC Operation and Various Activation Functions for Deep-Learning Applications</title>
		<author>
			<persName><forename type="first">Seongju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gimoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Ka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyudong</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongje</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyeongpil</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyeol</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nahsung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkee</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornijcuk</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woojae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoon</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunha</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haerang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donguc</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younggun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keewon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choungki</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunseok</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daehan</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joohwan</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Solid-State Circuits Conf</title>
		<meeting>IEEE Int. Solid-State Circuits Conf</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical Task Scheduler for Interleaving Subtasks on Heterogeneous Multiprocessor Platforms</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francky</forename><surname>Catthoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Vounckx</surname></persName>
		</author>
		<idno type="DOI">10.1145/1120725.1120765</idno>
		<ptr target="https://doi.org/10.1145/1120725.1120765" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Asia and South Pacific Design Automation Conference</title>
		<meeting>the 2005 Asia and South Pacific Design Automation Conference<address><addrLine>Shanghai, China; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="952" to="955" />
		</imprint>
	</monogr>
	<note>ASP-DAC &apos;05). Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Torchvision the Machine-Vision Package of Torch</title>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Rodriguez</surname></persName>
		</author>
		<idno type="DOI">10.1145/1873951.1874254</idno>
		<idno>MM &apos;10</idno>
		<ptr target="https://doi.org/10.1145/1873951.1874254" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia<address><addrLine>Firenze, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1485" to="1488" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Hierarchical Task Scheduler for Heterogeneous Computing</title>
		<author>
			<persName><forename type="first">Narasinga</forename><surname>Rao Miniskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">R</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwaipayan</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78713-4_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78713-4_4" />
	</analytic>
	<monogr>
		<title level="m">High Performance Computing: 36th International Conference</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2021-06-24">2021. June 24 -July 2, 2021</date>
			<biblScope unit="page" from="57" to="76" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Active Memory Cube: A processing-in-memory architecture for exascale systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Antao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bertolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Brunheroto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Cher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H A</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evangelinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Fleischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gunnels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohmacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Prener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sallenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D M</forename><surname>Siegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sugavanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sura</surname></persName>
		</author>
		<idno type="DOI">10.1147/JRD.2015.2409732</idno>
		<ptr target="https://doi.org/10.1147/JRD.2015.2409732" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Active Memory Cube: A processing-in-memory architecture for exascale systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Antao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bertolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Brunheroto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Cher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H A</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Evangelinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Fleischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gunnels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karkhanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohmacht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Prener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sallenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D M</forename><surname>Siegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sugavanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sura</surname></persName>
		</author>
		<idno type="DOI">10.1147/JRD.2015.2409732</idno>
		<ptr target="https://doi.org/10.1147/JRD.2015.2409732" />
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansha</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjie</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Smelyanskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00091</idno>
		<ptr target="http://arxiv.org/abs/1906.00091" />
		<title level="m">Deep Learning Recommendation Model for Personalization and Recommendation Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Ataberk</forename><surname>Olgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">G?mez</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Salami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasan</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O?uz</forename><surname>Ergin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00082[cs.AR]</idno>
		<title level="m">PiDRAM: A Holistic End-to-end FPGA-based Framework for Processing-in-DRAM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimal task scheduling for partially heterogeneous systems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Sinnen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.parco.2021.102815</idno>
		<ptr target="https://doi.org/10.1016/j.parco.2021.102815" />
	</analytic>
	<monogr>
		<title level="j">Parallel Comput</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">102815</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multilevel Granularity Parallelism Synthesis on FPGAs</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Stratton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Gururaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen-Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><surname>Cong</surname></persName>
		</author>
		<idno type="DOI">10.1109/FCCM.2011.29</idno>
		<ptr target="https://doi.org/10.1109/FCCM.2011.29" />
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scheduling techniques for GPU architectures with processing-inmemory capabilities</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Pattnaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xulong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adwait</forename><surname>Jog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Kayiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asit</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mahmut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chita</forename><forename type="middle">R</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.1145/2967938.2967940</idno>
		<ptr target="https://doi.org/10.1145/2967938.2967940" />
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00474</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00474" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">McDRAM: Low Latency and Energy-Efficient Matrix Computations in DRAM</title>
		<author>
			<persName><forename type="first">Hyunsung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCAD.2018.2857044</idno>
		<ptr target="https://doi.org/10.1109/TCAD.2018.2857044" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput.-Aided Design Integr. Circuits Syst</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1409.1556</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1409.1556" />
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Maestro: Data Orchestration and Tuning for OpenCL Devices</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Spafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Meredith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro-Par 2010 -Parallel Processing, Pasqua D&apos;Ambra</title>
		<editor>
			<persName><forename type="first">Mario</forename><surname>Guarracino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Domenico</forename><surname>Talia</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Optimal Latency-Throughput Tradeoffs for Data Parallel Pipelines</title>
		<author>
			<persName><forename type="first">Jaspar</forename><surname>Subhlok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Vondran</surname></persName>
		</author>
		<idno type="DOI">10.1145/237502.237508</idno>
		<ptr target="https://doi.org/10.1145/237502.237508" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures</title>
		<meeting>the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures<address><addrLine>Padua, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
	<note>SPAA &apos;96)</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00293</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00293" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<ptr target="http://arxiv.org/abs/1905.11946" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PIMProf: An Automated Program Profiler for Processing-in-Memory Offloading Decisions</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korakit</forename><surname>Seemakhupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tajana</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.23919/DATE54114.2022.9774560</idno>
		<ptr target="https://doi.org/10.23919/DATE54114.2022.9774560" />
	</analytic>
	<monogr>
		<title level="m">2022 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="855" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Tuning applications for efficient GPU offloading to in-memory processing</title>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM International Conference on Supercomputing</title>
		<meeting>the 34th ACM International Conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spacea: Sparse matrix vector multiplication on processing-in-memory accelerator</title>
		<author>
			<persName><forename type="first">Xinfeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abanti</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="570" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bolt: Bridging the Gap between Autotuners and Hardware-native Performance</title>
		<author>
			<persName><forename type="first">Jiarong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/paper/2022/file/38" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="204" to="216" />
		</imprint>
	</monogr>
	<note>b3eff8baf56627478ec76a704e9b52-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">CoPIM: A Concurrency-aware PIM Workload Offloading Architecture for Graph Applications</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rujia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-He</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISLPED52811.2021.9502483</idno>
		<ptr target="https://doi.org/10.1109/ISLPED52811.2021.9502483" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Selective Replication in Memory-Side GPU Caches</title>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO50266.2020.00082</idno>
		<ptr target="https://doi.org/10.1109/MICRO50266.2020.00082" />
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="967" to="980" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
