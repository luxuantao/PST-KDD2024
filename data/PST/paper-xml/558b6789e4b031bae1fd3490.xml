<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-11-27">November 27, 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>MÃ¸gelmose</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Analysis of Peo-ple Laboratory</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<postCode>9220</postCode>
									<settlement>Aalborg East</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohan</forename><forename type="middle">Manubhai</forename><surname>Trivedi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Analysis of Peo-ple Laboratory</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<postCode>9220</postCode>
									<settlement>Aalborg East</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Robotics Research Labora-tory</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0434</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Analysis of Peo-ple Laboratory</orgName>
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<postCode>9220</postCode>
									<settlement>Aalborg East</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vision-Based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-11-27">November 27, 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">84338E0CDA48B6404EC38FDD455207AE</idno>
					<idno type="DOI">10.1109/TITS.2012.2209421</idno>
					<note type="submission">received January 30, 2012; revised April 27, 2012; accepted June 19, 2012. Date of publication October 19, 2012; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active safety</term>
					<term>human-centered computing</term>
					<term>machine learning</term>
					<term>machine vision</term>
					<term>object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we provide a survey of the traffic sign detection literature, detailing detection systems for traffic sign recognition (TSR) for driver assistance. We separately describe the contributions of recent works to the various stages inherent in traffic sign detection: segmentation, feature extraction, and final sign detection. While TSR is a well-established research area, we highlight open research issues in the literature, including a dearth of use of publicly available image databases and the overrepresentation of European traffic signs. Furthermore, we discuss future directions of TSR research, including the integration of context and localization. We also introduce a new public database containing U.S. traffic signs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N THIS paper, we provide a survey of traffic sign detec- tion for driver assistance. State-of-the-art research utilizes sophisticated methods in computer vision for traffic sign detection, which has been an active area of research over the past decade. On-road applications of vision have included lane detection, driver distraction detection, and occupant pose inference. As described in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, it is crucial to not only consider the car's surrounding and external environment when designing an assist system but also consider the internal environment and take the driver into account. Fusing other types of information with the sign detector, as described in <ref type="bibr" target="#b3">[4]</ref>, can make the overall system even better.</p><p>When the system is considered a distributed system where the driver is an integral part, it allows for the driver to contribute what he is good at (e.g., seeing speed limit signs, as we shall see later), while the TSR part can present information from other signs. In addition, other surround sensors can also have an influence on what is presented.</p><p>In recent years, speed limit detection systems have been included in top-of-the-line models from various manufacturers, TABLE I SIGNIFICANT RESULTS FROM <ref type="bibr" target="#b8">[9]</ref> REGARDING ATTENTION TO VARIOUS SIGN TYPES but a more general sign detection solution and an integration into other vehicle systems have not yet materialized. Current state-of-the-art TSR systems utilize neither information about the driver nor input from the driver to enhance performance.</p><p>Extensive studies in human-machine interactivity are necessary to present the TSR information in a careful way to inform the driver without causing distraction or confusion. The literature features just two surveys on TSR: In <ref type="bibr" target="#b4">[5]</ref>, there is a good introduction, but it is not very comprehensive. In <ref type="bibr" target="#b5">[6]</ref>, any improvements in the field from the past five years are not presented because the paper is several years old. A very good comparison of various segmentation methods is offered in <ref type="bibr" target="#b6">[7]</ref>, but given that it only covers segmentation, it is not a comprehensive overview of detection methods. Likewise, <ref type="bibr" target="#b7">[8]</ref> provides good comparison of Hough transform derivatives. In this paper, our emphasis is on framing the TSR problem in the context of human-centered driver assistance systems. We provide a comparative discussion of papers published mostly within the last five years and an overview of the recent work in the area of sign detection, which is a subset of the TSR problem. We provide a critical review of traffic sign detection and offer suggestions for future research areas in this challenging problem domain. The next section establishes the driver assistance context and covers TSR systems in general. Section III provides a problem description and a gentle introduction to traffic sign detection. Section IV deals with segmentation for traffic sign detection. Section V details models and feature extraction. Section VI deals with the detection itself. In the final section, analysis and insight on future research directions in the field are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HUMAN-CENTERED TRAFFIC SIGN RECOGNITION FOR DRIVER ASSISTANCE: ISSUES AND CONSIDERATIONS</head><p>Traffic sign recognition (TSR) research needs to take into account the visual system of the driver. This can include factors Here, all signs must be detected and processed. (b) and (c) System that tracks the driver's attention. In (b), the driver is attentive and spots all signs. Therefore, the system just highlights the sign that is known to be difficult for people to notice. In (c), the driver is distracted by a passing car and thus misses two signs. In this case, the system should inform the driver about the two missed signs. such as visual saliency of signs, driver focus of attention, and cognitive load. According to <ref type="bibr" target="#b8">[9]</ref> (see Table <ref type="table">I</ref> for a summary of the main results), not all signs are equal in their ability to capture the attention of the driver. For example, a driver may fixate his gaze on a sign but neither notice the sign nor remember its informational content. While drivers invariably fixate on speed limit signs and recall their information, they are less likely to notice game crossing and pedestrian signs. This can endanger pedestrians, as it may not leave enough reaction time to stop.</p><p>The implications of use of TSR in human-in-the-loop system are clear; instead of focusing on detection and perfectly recognizing all signs of some class, which would be the objective for an autonomous car, the task is now to detect and highlight signs that the driver has not seen. This gives way to various models of TSR, which take into account the driver's focus of attention, and interactivity issues. Driver attention tracking is covered in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> shows examples on how TSR can be used for driver assistance. Fig. <ref type="figure" target="#fig_0">1</ref>(a) shows how a system should act in an autonomous car. It simply recognizes all signs present. In Fig. <ref type="figure" target="#fig_0">1</ref>(b), there is a driver in the loop, and while the system may see all the signs, it should avoid presenting them to avoid driver confusion. Instead, it simply highlights the sign type that is easy to overlook, such as the pedestrian crossing warnings in the research. Fig. <ref type="figure" target="#fig_0">1(c</ref>) shows how a driver is distracted by a passing car. This causes him to miss two signs. His car has a TSR system for driver assistance, which informs him of the signs as he returns his attention to the road ahead of him. This could, for example, be done using a heads-up display, as suggested in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Even though this paper is mostly concerned with using TSR for driver assistance, TSR has various well-defined applications nicely summarized here <ref type="bibr" target="#b12">[13]</ref>.</p><p>1) Highway maintenance: Check the presence and condition of signs along major roads. 2) Sign inventory: Similar to the preceding task, create an inventory of signs in city environments. 3) Driver-assistance systems: Assist the driver by informing of current restrictions, limits, and warnings. 4) Intelligent autonomous vehicles: Any autonomous car that is to drive on public roads must have a means of obtaining the current traffic regulations. This can be done through TSR. This paper uses the term TSR to refer to the entire chain from detection of signs to their classification and potential presentation to the driver. Generally, TSR is split into two stages: detection and classification (see Fig. <ref type="figure" target="#fig_1">2</ref>). Detection is concerned with locating signs in input images, whereas classification is about determining what type of sign the system is looking at. The two tasks can often be treated as completely separate, but in some cases, the classifier relies on the detector to supply information, such as the sign shape or sign size. In a full system, the two stages depend on each other, and it does not make sense to have a classifier without a detection stage. Later, we divide the detection stage into three substages, but these should not be confused with the two main stages of a full TSR system: detection and classification.</p><p>Apart from shape and color, another aspect may be used in TSR: temporal information. Most TSR systems are designed with a video feed from a vehicle in mind; therefore, signs can be tracked over time. The simplest way of using tracking is to accept sign candidates as signs only if they have shown up on a number of consecutive frames. Sign candidates that only show up once are usually a result of noise. Employing a predictive method, such as a Kalman filter, allows for the system to predict where a sign candidate should show up in the next frame, and if its position is too far away from this prediction, the sign candidate is discarded. A predictive tracking system has the additional benefit of handling occlusions, hence, preventing signs that were occluded from being classified as new signs. This is very important in a driver-assistance system where signs should only be presented once and in a consistent way. Imagine a scenario where a sign is detected in a few frames and occluded for a short time before being detected again. For an autonomous car, it is not likely to be a problem to be presented with the same information twice: If the first sign prompted the speed to be set at 55 mi/h, there is no problem in the system being told once again that the speed limit is 55 mi/h. In a driver-assistance system, the system must not present more information than absolutely necessary at any given moment, so the driver is not overwhelmed with information, e.g., forcing the driver to pay attention to a sign he has already seen should be avoided.</p><p>Many TSR systems are tailored to a specific sign type. Due to the vast differences in sign design from region to region (see the next section) and the differences in sign design based on their purpose, many systems narrow their scope down to a specific sign type in a specific country.</p><p>There is a wide span in speeds of the systems. For use in driver assistance and autonomous vehicles, real-time performance is necessary. This does not necessarily mean a speed of 30 Hz, but the signs must be read quickly enough to still be relevant to act on. Depending on the exact application, a few hertz is required.</p><p>Instead of treating the entire TSR process in what could easily become a cursory manner, we have opted to thoroughly look on the detection stage. The line between detection and classification is a bit blurry since some detectors provide more information to the classifier than others. It is normal for the detector to inform the classifier of the general category of signs since that is often defined by either the overall sign shape or its color, which is something that the detector itself may use to localize the sign.</p><p>Even though this paper is targeted toward the problem of detecting traffic signs, one must not forget that, without a subsequent classification stage, the systems are useless. Thus, even though we encourage a decoupling of the two tasks, this does not mean that the classification is a solved problem. It is a crucial part of a full system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TRAFFIC SIGNS</head><p>Traffic signs are markers placed along roads to inform drivers about either road conditions and restrictions or which direction to go. They communicate a wealth of information but are designed to do so efficiently and at a glance. This also means that they are often designed to stand out from their surroundings, making the detection task fairly well defined.</p><p>The designs of traffic signs are standardized through laws but differ across the world. In Europe, many signs are standardized via the Vienna Convention on Road Signs and Signals <ref type="bibr" target="#b13">[14]</ref>. There, shapes are used to categorize different types of signs: Circular signs are prohibitions including speed limits, triangular signs are warnings, and rectangular signs are used for recommendations or subsigns in conjunction with one of the other shapes. In addition to these, octagonal signs are used to signal a full stop, and downward-pointing triangles signal a yield. Countries have other different types, e.g., to inform about city limits. Examples of these signs can be seen in Fig. <ref type="figure" target="#fig_2">3</ref>. In the U.S., traffic signs are regulated by the Manual on Uniform Traffic Control Devices (MUTCD) <ref type="bibr" target="#b14">[15]</ref>. It defines which signs exist and how they should be used. It is accompanied by the Standard Highway Signs and Markings (SHSM) book, which describes the exact designs and measurements of signs. At the time of writing, the most recent MUTCD was from 2009, whereas the SHSM book had not been updated since 2004. Thus, it described the MUTCD from 2003. The MUTCD contains a few hundred different signs divided into 13 categories.</p><p>To further complicate matters, each U.S. state can decide whether it wishes to follow the MUTCD. A state has three options.</p><p>1) Adopt the MUTCD fully as is.</p><p>2) Adopt the MUTCD but add a State Supplement.</p><p>3) Adopt a State MUTCD that is "in substantial conformance with" the national MUTCD. In the U.S., 19 states have adopted the national MUTCD without modifications, 23 states have adopted the national MUTCD with a state supplement, and ten states have opted to create a State MUTCD (the count includes the District of Columbia and Puerto Rico). Examples of U.S. signs can be seen in Fig. <ref type="figure">4</ref>.</p><p>New Zealand uses a sign standard with warning signs that are yellow diamonds, as in the U.S., but regulatory signs are round with a red border, like those from the Vienna Convention countries. Japan uses signs that are generally in compliance with the Vienna Convention, as are Chinese regulatory signs. Chinese warning signs are triangular with a black/yellow color scheme. Central and South American countries do not participate in any international standard but often use signs somewhat like the American standard.</p><p>While signs are well defined through laws and designed to be easy to spot, there are still plenty of challenges for TSR systems.</p><p>1) Signs are similar within or across categories (see Fig. <ref type="figure">5</ref>).</p><p>2) Signs may have faded or are dirty so they are no longer their specified color. 3) The sign post may be bent, and therefore, the sign is no longer orthogonal to the road. 4) Lighting conditions may make color detection unreliable. 5) Low contrast may make shape detection hard. 6) In cluttered urban environments, other objects may look very similar to signs. 7) There may be varying weather conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Assessing Performance of Sign Detectors</head><p>When comparing sign detectors, some comparison metrics must be set up. The straightforward and most important measure is the true positive rate. However, even if all signs are detected, the system is not necessarily perfect. The number of false positives must also be taken into account. If the amount of false positives is too high, the classifier will have to handle a lot more data than it should, degrading the overall system speed. For cases when a system must work in real time in a car, obviously, the detection must be fast. In general, the faster the detection runs, the more time left over for the classification stage. Adjusting these goals is a tradeoff. Often, the target will be to create a system that is just fast enough for a given application while keeping the receiver operating characteristic acceptable. Another interesting performance characteristic is which sign types a given system works for.</p><p>Even with the parameters in mind and a clear idea of the performance metrics, comparing the performance of different systems is not a straightforward task. Unlike other computer vision areas, until recently, no standardized training and test data set existed, so no two systems were tested with the same data. The image quality varies from high-resolution still images (as in <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>) to low-resolution frames from in-car video cameras (such as <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>). That, combined with the facts that signs wildly vary between countries and many papers limit their scope to specific sign types, makes for a quite uneven playing field.</p><p>For a discussion of the performance of the papers presented in this survey, see Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Public Sign Databases</head><p>A few publicly available traffic sign data sets exist: 1) German TSR Benchmark (GTSRB) <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>; 2) KUL Belgium Traffic Signs Data set (KUL Data set) <ref type="bibr" target="#b23">[24]</ref>; 3) Swedish Traffic Signs Data set (STS Data set) <ref type="bibr" target="#b24">[25]</ref>; 4) RUG Traffic Sign Image Database (RUG Data set) <ref type="bibr" target="#b25">[26]</ref>; 5) Stereopolis Database <ref type="bibr" target="#b26">[27]</ref>. Information on these databases can be found in Table <ref type="table" target="#tab_0">II</ref>. Most of the databases have emerged within the last two years (except for the very small RUG Data set) and are not yet widely used. One of the most widespread databases is the GTSRB, which has been presented in <ref type="bibr" target="#b21">[22]</ref> and created for the competition "The German Traffic Sign Recognition Benchmark." The competition was held at the International Joint Conference on Neural Networks (IJCNN) 2011. It is a large data set containing German signs and is thus very suitable for training and testing systems aimed at signs adhering to the Vienna Convention. A sample image from the GTSRB database can be found in Fig. <ref type="figure" target="#fig_4">6</ref>(a). The GTSRB is primarily geared toward classification, rather than detection, since each image contains exactly one sign without much background. For detection, images of complete scenes are necessary. In addition, many detection systems rely on a tracking scheme to make detection more robust, and without video of the tracks (in GTSRB parlance, a "track" is a set of images of the same physical sign), this will not properly work. Since the data set is created for the classification task, this is not so much a problem of that database, as it is a testament to its target. In conjunction with the competition, five interesting papers <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b31">[32]</ref> were released. They all focus on classification rather than detection.</p><p>Two other data sets should be highlighted: The STS Data set and the KUL Data set. They are both very large, although not as large as the GTSRB, and they contain full images. This means that they can both be used for detection purposes. The STS Data set does not have all images annotated, but it does include all frames from the videos used to obtain the data. This means that tracking systems can be used on this data set, but it can only be verified with ground truth every five frames. An example from the STS Data set can be seen in Fig. <ref type="figure" target="#fig_4">6(b</ref>). The KUL Data set also includes four recorded sequences, which can be used for tracking experiments. KUL also includes a set of sign-free images, which can be used as negative training images, and it has pose information for the cameras for each image.  From the research, it was evident that there was a lack of databases with U.S. traffic signs, and therefore, in conjunction with this paper, we have assembled one. Its details are also listed in Table <ref type="table" target="#tab_0">II</ref>. One novel feature of this data set is that it includes video tracks of all the annotated signs. Many systems already use various tracking schemes to minimize the number of false positives, and it is quite likely that, in the future, detectors using temporal data will emerge even more. Therefore, the LISA data set includes video and standalone frames. Not all frames have been extracted for annotation, but all annotated frames can be traced back to the source video; therefore, so the annotations can also be used to verify systems using tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIGN DETECTION</head><p>The approaches in this stage have traditionally been divided into two kinds:</p><p>1) color-based methods; 2) shape-based methods.</p><p>Color-based methods take advantage of the fact that traffic signs are designed to be easily distinguished from their surroundings, often colored in highly visible contrasting colors. These colors are extracted from the input image and used as a base for the detection. Just as signs have specific colors, they also have very well-defined shapes that can be searched for. Shape-based methods ignore the color in favor of the characteristic shape of signs.</p><p>Each method has its pros and cons. Color of signs, while well defined in theory, varies much with available lighting, as well as with the age and condition of the sign. On the other hand, searching for specific colors in an image is fairly straightforward. Sign shapes are invariant to lighting and age, but parts of the sign can be occluded, making the detection harder or the sign may be located at a background of a similar color, ruining edge detection, on which most shape detectors rely.</p><p>The division of systems in this way can be problematic. Almost all color-based approaches take shape into account after having looked at colors. Others use shape detection as their main method but integrate some color aspects as well. Instead, the detection can be split into two steps, as proposed in <ref type="bibr" target="#b6">[7]</ref>, i.e., segmentation and detection. In this paper, we go one step further and split the detection step into a feature extraction step and the actual detection, which acts on the features that are extracted. Many shape-only-based methods have no segmentation step. The flow is outlined in Fig. <ref type="figure" target="#fig_5">7</ref>.</p><p>An overview of all surveyed papers and their methods is listed in Table <ref type="table" target="#tab_1">III</ref>. It contains each of the systems and lists which segmentation method, feature type, and detection method are used. The author group numbers are used to mark the papers that are part of an ongoing effort from the same group of authors. They do not constitute a ranking in any way. In Tables IV and V, some of their more detailed properties are listed. The systems are split into two tables. Table IV displays those that do not use any tracking. Table V contain those that do use tracking, something we find crucial when using TSR in a driver-assistance context, as mentioned earlier. Apart from this division, the two tables are structured in the same way: Sign type in paper describes which sign types the authors of the paper have attempted to find, whereas sign type possible are the types of signs the method could be extended to include, which is usually a very broad group. Real time is about how fast the system runs, if that information is available. Any system with a frame rate faster than 5 frame/s is considered to have real-time potential. Rotation invariance tells whether the used technique is robust to rotation of signs. Model versus training describes if the detection system relies on a theoretical model of signs (such as a predefined shape), if it uses a learned type of classifier, or if it uses a combination of the two. Test image type is the image resolution that the system is designed to work with. Low-resolution images are usually video frames, whereas high-resolution images are still images.</p><p>The detection performance of the surveyed papers is presented in Table <ref type="table" target="#tab_3">VI</ref>. As mentioned earlier, very few papers use common databases to test their performance, and the papers detect various types and numbers of signs. Thus, the numbers should not be directly compared; nevertheless, they give an idea of performance. Not all papers report all the measures reported in the table (detection rate, false positives per frame, etc.), so some fields in the table could not be filled. In other cases, these exact measures were not given but could be calculated from other given numbers. Where figures are available, the best detection rate that the system obtained is reported, along with the corresponding measure of false positives. The detection rate is per frame, meaning that 100% detection is only achieved if a sign is found in every frame present. It is not sufficient to just detect the sign in a few frames. This is the way results are presented in most papers, and therefore, this is the measure chosen here, even if a real-world system works well enough if each sign is just detected once. Papers that only report the per-sign detection rate as opposed to the per-frame detection rate are marked with a triangle in the rightmost column of the table.</p><p>Different papers report the false positives in different ways, so a few different measures, which are not directly comparable, are presented in the table: FPPF) False positives per frame: F P P F = F P/f, where F P is the number of false positives, and f is the number of frames analyzed. FPR) False positive rate: F P R = F P/N, where N is the number of negatives in the test set. This measure is rarely used in detection since the number of negatives does not always make much sense (how many negatives exist in a full frame). PPV) Positive predictive value: P P V = T P/T P + F P , where T P is the number of true positives. FPTP) False/true positive ratio: F P T P = F P/T P . WPA) Wrong pixels per area: W P A = W P/AP , where W P is the number of wrongly classified pixels, and AP is the total number of pixels classified. When papers present results for different sign types, the mean detection performance is also presented in the table. In many cases, that will give a better view of the true performance of the approach.</p><p>Five papers stick out, claiming 100% detection rate. The first <ref type="bibr" target="#b32">[33]</ref> is only tested on synthetic data. It is possible that the synthetic data do not fully encapsulate real-world variations, so the performance of that approach is not guaranteed to be as good in real-world scenarios. At first glance, <ref type="bibr" target="#b33">[34]</ref> achieves 100% detection rate, but that is only the case for one of their sign types. The mean performance is a more accurate (and still promising) gauge of the actual performance. The same is the case for <ref type="bibr" target="#b24">[25]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, all signs in the test set are detected, but at the cost of a large number of false positives per frame. In <ref type="bibr" target="#b35">[36]</ref>, the per-sign detection rate is all that is presented, and therefore, the figure cannot be compared with other systems.</p><p>Generally, systems achieve detection rates well into the 90% range, whereas some achieve very low false detection rates. From the table, no "best system" can be chosen since the test sets are very different in both size and content. A system that can detect several different sign types at low detection rate may, in some applications, be considered better than a system that can only detect one specific sign type but does that very well. A few papers that should be highlighted are <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. They have all been tested on large data sets and report detection rates above 90% with a decent low number of false positives. Now that the basics about sign detection are in place, the succeeding sections go in depth with how recent papers perform each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SEGMENTATION</head><p>The purpose of the segmentation step is to achieve a rough idea about where signs might be and thus narrow down the search space for the next steps. Not all authors make use of this step. Since the segmentation is traditionally done based on colors, authors who believe this should not be part of sign   detection often have no segmentation step but directly go to the detection.</p><p>Of the papers that do use segmentation, all, except <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b39">[40]</ref>, use colors to some extent. Normally, segmentation is done with colors, and subsequently, shape detection is run in a later stage. In <ref type="bibr" target="#b37">[38]</ref>, the usual order is reversed; therefore, they use radial symmetry voting (see Section VII) for segmentation and a color-based approach for the detection. In <ref type="bibr" target="#b39">[40]</ref>, radial symmetry voting as preprocessing is also run, but it was followed up with a cascaded classifier using Haar wavelets (see Section VII).</p><p>Generally, color-based segmentation relies on a thresholding of the input image in some color space. Since many believe that the RGB color space is very fragile with regard to changes in lighting, these methods are spearheaded by the hue, saturation, and intensity (HIS) space (or its close sibling, the hue, saturation, and value (HSV) space). HSI/HSV is used by <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b45">[46]</ref>. The HIS space models human vision better than RGB and allows some variation in the lighting, most notably in the intensity of light. Some papers, like those in the series starting with <ref type="bibr" target="#b15">[16]</ref> and followed by <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b46">[47]</ref>, and <ref type="bibr" target="#b47">[48]</ref>, augment the HSI thresholding with a way to find white signs. Hue and saturation are not reliable for detecting white since it can be at any hue they use an achromatic decomposition of the image proposed by <ref type="bibr" target="#b48">[49]</ref> (see Fig. <ref type="figure" target="#fig_6">8</ref>).</p><p>Some authors are not satisfied with the performance of HSI since it does not model the change in color temperature in different weather but only helps in changing light intensity. References <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b49">[50]</ref> instead threshold in the luminosity, chroma, and hue (LCH) color space, which is obtained using the CIECAM97 model. This allows them to take variations in color temperature into account. The RGB space is used by <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b50">[51]</ref>, but they use an adaptive threshold in an attempt to combat instabilities caused by lighting variations.</p><p>Of special interest in this color space discussion is the excellent paper <ref type="bibr" target="#b6">[7]</ref>, which has shown that HSI-based segmentation offers no significant benefit over normalized RGB and that methods that use color segmentation generally perform much better than shape-only methods. They do, however, have trouble with white signs. For a long time, it has simply been assumed that the RGB color space was a bad choice for segmentation, but through rigorous testing, they show that there is nothing to gain from switching to the HSI color space, instead of a normalized RGB space. As the authors write: "Why use a nonlinear and complex transformation if a simple normalization is good enough?"</p><p>A color-based model not relying on thresholding was put forward in <ref type="bibr" target="#b51">[52]</ref>, which uses a cascaded classifier trained with AdaBoost, which is similar to that proposed by <ref type="bibr" target="#b52">[53]</ref> but on Local Rank Pattern features, instead of Haar wavelets. In addition, <ref type="bibr" target="#b33">[34]</ref> used a color-based search method that, while closely related to, is not directly thresholding based. Here, the image is discretized into colors that may exist on signs. The discretization process is less destructive than thresholding in that it does not directly discard pixels; instead, it maps them into the closest sign-relevant color. In a more recent contribution <ref type="bibr" target="#b19">[20]</ref>, they replace the color discretization method with a Quadtree interest-region-finding algorithm, which finds interesting areas using an iterative search method for colored signs. In the same realm lies <ref type="bibr" target="#b7">[8]</ref>, which uses learned probabilistic color preprocessing.</p><p>In <ref type="bibr" target="#b20">[21]</ref>, a unique approach is proposed: Using a biologically inspired attention system, it produces a heat map that denotes areas where signs are likely to be found. An example can be seen in Fig. <ref type="figure" target="#fig_7">9</ref>. A somewhat similar system was put forth by <ref type="bibr" target="#b18">[19]</ref>, who uses a saliency measure to find possible areas of interests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. FEATURES AND MODELING</head><p>While various features are available from the vision literature, the choice of feature set is often closely coupled with the detection method, although some feature sets can be used with a selection of different detection methods. The most popular feature is edges: sometimes edges directly obtained from the raw picture and sometimes edges from presegmented images. Edges are practically always found using Canny edge detection or some method that is very similar, and they are used as the only feature in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b51">[52]</ref>, and <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b60">[61]</ref>. Edges with Haar-like features are combined in <ref type="bibr" target="#b50">[51]</ref>, and <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b61">[62]</ref> looked only at certain color-filtered edges.</p><p>Even though edges comprise the most popular feature choice, there are other options. Histogram of Oriented Gradients (HOG) is one. It was first used to detect people in images but has been used in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b62">[63]</ref>, and <ref type="bibr" target="#b63">[64]</ref> to detect signs. HOG is based on creating histograms of gradient orientations on patches of the image and comparing them to known histograms for the sought-after objects. HOG is also used by Creusen et al. <ref type="bibr" target="#b64">[65]</ref>, but they augment the HOG feature vectors with color information to make them even more robust.</p><p>A number of papers <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b65">[66]</ref> use Haar waveletlike features only on certain colors <ref type="bibr" target="#b65">[66]</ref> and in the form of so-called dissociated dipoles with wider structure options than traditional Haar wavelets <ref type="bibr" target="#b36">[37]</ref>.</p><p>More esoteric choices are distance to bounding box (DtB), fast Fourier transform (FFT) of shape signatures, tangent functions, simple image patches, and combinations of various simple features. DtB, as used in <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr" target="#b47">[48]</ref>, are a measure of distances from the contour of a sign candidate to its bounding box. Similarly, the FFT of shape signatures used in <ref type="bibr" target="#b32">[33]</ref> is based on the distance from the shape center to its contour at different angles. Tangent functions, which are used in <ref type="bibr" target="#b43">[44]</ref>, calculate the angles of the tangents at various points around the contour. Simple image patches (although in the YCbCr color space) are championed by <ref type="bibr" target="#b41">[42]</ref>, and a combination of simple features, such as corner positions and color, is used in <ref type="bibr" target="#b20">[21]</ref>, which is an area that warrants further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DETECTION</head><p>The detection stage is where the signs are actually found. This is, in many ways, the most critical step and is often also the most complicated. The selection of detection method is a bit more constrained than the previous two stages since the method must work with the features from the previous stage. The decision is therefore often made the other way around: A desired detection method is chosen, and the feature extraction stage is designed to deliver what is necessary to perform the detection. As we know from the previous section, the most popular feature is the edges, and this reflects on the most popular choice in the detection method. Using Hough transforms to process the edges is one option, as done by <ref type="bibr" target="#b42">[43]</ref> and <ref type="bibr" target="#b57">[58]</ref>- <ref type="bibr" target="#b59">[60]</ref>. In <ref type="bibr" target="#b59">[60]</ref>, a proprietary and undisclosed algorithm is used for the detection of rectangles, in addition to the Hough transform used for circles. That said, Hough transforms are computationally expensive and not suited for systems with realtime requirements. Because of that, the most popular methods are derivatives of the radial symmetry detector that was first proposed in <ref type="bibr" target="#b66">[67]</ref> and first put to use for sign detection in <ref type="bibr" target="#b67">[68]</ref>. The algorithm votes for the most likely sign centers in an image based on symmetric edges and is itself inspired by the Hough transform. The basic principle can be seen in Fig. <ref type="figure" target="#fig_8">10</ref>. In a circle, all edge gradients intersect at the center. The algorithm finds gradients with a magnitude above a certain threshold. In the direction pointed out by the gradient, it casts a vote in a separate vote image. It looks for circles of a specific radius and thus votes only in the distance from the edge that is equivalent to the radius. The places with most votes are most likely to be the center of circles. This algorithm was later extended to regular polygons by <ref type="bibr" target="#b34">[35]</ref>, and a faster implementation for sign detection use was proposed by <ref type="bibr" target="#b53">[54]</ref>. It is also used in some form by <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, and <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>. An example of votes from a system that is extended to work for rectangular signs can be seen in Fig. <ref type="figure" target="#fig_9">11</ref>.</p><p>An alternate edge-based voting system is proposed by <ref type="bibr" target="#b60">[61]</ref>.</p><p>The HOG features can be used with a support vector machine (SVM), as in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b64">[65]</ref>, or be compared by calculating a similarity coefficient, as in <ref type="bibr" target="#b16">[17]</ref>. Another option with regard to HOG is to use a cascaded classifier trained with some type of boosting. This is done in <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b63">[64]</ref>. Cascaded classifiers are traditionally used with Haar wavelets, and sign detection is no exception, as used in <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b50">[51]</ref>, and <ref type="bibr" target="#b65">[66]</ref>.</p><p>Finally, neural networks and genetic algorithms are represented in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b48">[49]</ref>, respectively.</p><p>The detection stage reflects the philosophical difference that was also seen in the feature extraction stage: Either reliance on a simple theoretical model of sign shapes-at this stage, it is nearly always shapes that are searched for-or reliance on training data and, then, a more abstract detection method is preferred. Since it is extremely hard to compare systems tested across different data sets, it is not clear which methods perform the best; therefore, this is clearly an area that needs to be studied further. Both ways can be fast enough for realtime performance, and most of them could also work with signs of any shape. There are outliers using different methods, but there is no compelling argument that they should perform significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION AND FUTURE DIRECTIONS</head><p>In the previous sections, different methods and philosophies for each stage are presented. This section discusses the current state of the art and outlines ideas for future directions of research.</p><p>At the moment, the problem in TSR is the lack of use of standardized sign image databases. This makes comparisons between contributions very hard. To obtain meaningful advances in the field, the development of such databases is crucial. Until now, research teams have only implemented a method that they believe has potential or perhaps tested a few solutions. Without a way to compare performance with other systems, it is not clear which approaches work best; therefore, every new team starts back at square one, implementing what they think might work best. Two efforts to remedy this situation deserve to be mentioned: The sign databases presented earlier and the segmentation evaluation in <ref type="bibr" target="#b6">[7]</ref>. As mentioned earlier (see Section III-B), a few public sign databases have recently emerged but have not yet been widely used. In <ref type="bibr" target="#b6">[7]</ref>, the authors compare various segmentation methods on the same data set containing a total of 552 signs in 313 images. They also propose a way to evaluate the performance of segmentation methods. That paper provides a very good starting point for determining which segmentation method to use.</p><p>These two efforts notwithstanding, public databases covering signs from non-Vienna Convention regions are necessary. Databases that include video tracks of signs would also be very beneficial to the development of TSR systems since many detectors employ a tracking system for signs. This is, to some extent, included in the KUL Data set. In relation to the work on this present survey, we have assembled such a database for U.S. traffic signs, one that includes full video tracks of signs. It is our hope that the GTSRB database will also be extended to include video and full frames and that more U.S. databases will be created.</p><p>The absence of usage of public database may not explain in entirety why very few comparative studies of methods exist. Another reason is that TSR systems are long complex chains of various methods, where it is not always possible to swap individual modules. When it is not feasible to swap, for example, the detection method for something else, it is naturally hard to determine whether other solutions may be better. This is solved, if more papers divide their work more clearly into stages, ideally as fine grained as those used in this survey, plus a similar set of stages for classification. This is done with success in <ref type="bibr" target="#b6">[7]</ref>, as they test different segmentation methods while keeping the feature extraction, detection, and classification stages fixed.</p><p>Another problem is the need for work on TSR in regions not adhering to the Vienna Convention. The bulk of the existing work comes out of Europe, Australia, and Japan. Japan and Australia did not participate in the Vienna Convention, but they use similar signs, for example, to convey speed limits. Of the surveyed papers here, only two are concerned with U.S. traffic signs <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b59">[60]</ref>, and even they only look at speed limit signs.</p><p>When looking at sign detection from a driver-in-the-loop perspective, it is also unfortunate that the bulk of research now focuses on speed limit signs. A wealth of papers cites driver assistance as their main application but carries on focusing on speed limit signs. Detection of speed limits is highly relevant for an autonomous vehicle, but as it turns out, humans are already very good at seeing speed limit signs themselves <ref type="bibr" target="#b8">[9]</ref>. As such, recognition of signs other than speed limit is actually more interesting.</p><p>The final problem we wish to highlight in this section is the relation of signs to the surroundings. TSR has seen significant work, as is evident from this paper, but little work has been done on ensuring that the detected signs are relevant for the ego car (with the notable exception of <ref type="bibr" target="#b57">[58]</ref>). In many situations, it can occur that a detected sign is not connected to the road the car is on. An example from our own collected data can be seen in Fig. <ref type="figure" target="#fig_10">12</ref>. In this case, two stop signs can be seen, but only the rightmost one pertains to the current road. Similar situations occur often on freeways, where some signs may only be relevant for exit lanes. Related to this problem is that, when the driver changes to a different road, most often, restrictions from earlier detected signs no longer apply. This should be detected and relayed to the system. It is very likely that research in other areas, such as lane detection can be of benefit here. Another idea with regard to the surroundings would be to link knowledge of weather and current lighting conditions to enhance the robustness of the detector, similar to what is done for detection of people in <ref type="bibr" target="#b68">[69]</ref>. It is also possible that vehicle dynamics can be taken into account and used in the tracking of detected signs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUDING REMARKS</head><p>This paper has provided an overview of the state of sign detection. Instead of treating the entire TSR flow, focus has been solely on the detection of signs. In recent years, a lot of effort has gone into TSR, mainly from Europe, Japan, and Australia, and the developments have been described.</p><p>The detection process has been split into segmentation, feature extraction, and detection. Many segmentation approaches exist, mostly based on evaluating colors in various color spaces. For features, there is also a wealth of options. The choice is made in conjunction with the choice of detection method. By far, the most popular features are edges and gradients, but other options such as HOG and Haar wavelets have been investigated. The detection stage is dominated by the Hough transform and its derivatives, but for HOG and Haar wavelet features, SVMs, neural networks, and cascaded classifiers have also been used.</p><p>Arguably, the biggest issue with sign detection is currently the lack of use of public image databases to train and test systems. Currently, every new approach presented uses a new data set for testing, making comparisons between papers hard. This gives the TSR effort a somewhat scattered look. Recently, a few databases have been made available, but they are still not widely used and cover only Vienna Convention-compliant i.e., signs. We have contributed with a new database, the LISA Data set, which contains U.S. traffic signs.</p><p>This issue leads to the main unanswered question in sign detection: Is a model-based shape detector superior to a learned approach, or vice versa? Systems using both approaches exist but are hard to compare since they all use different data sets.</p><p>Many contributions cite driver assistance systems as their main motivation for creating the system, but so far, only little effort has gone into the area of combining TSR systems with other aspects of driver assistance, and notably, none of the studies include knowledge about the driver's behavior to tailor the performance of the TSR system to the driver.</p><p>Other open issues include the lack of research into finding non-European style signs and the fact that detected signs are hard to relate to their surroundings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Different detection scenarios. The circle is the ego car, and three signs are distributed along the road. The area highlighted in red illustrates the driver's area of attention. (a) Standard scenario used for autonomous cars.Here, all signs must be detected and processed. (b) and (c) System that tracks the driver's attention. In (b), the driver is attentive and spots all signs. Therefore, the system just highlights the sign that is known to be difficult for people to notice. In (c), the driver is distracted by a passing car and thus misses two signs. In this case, the system should inform the driver about the two missed signs.</figDesc><graphic coords="2,302.75,268.56,246.12,99.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Basic flow in most TSR systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of European signs. These are Danish, but many countries use similar signs. (a) Speed limit. Sign C55. (b) End speed limit. Sign C56. (c) Start of freeway. Sign E55. (d) Right turn. Sign A41.</figDesc><graphic coords="3,344.95,70.14,171.96,187.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Examples of signs from the U.S. national MUTCD. Image source: [15]. (a) Stop. Sign R1-1. (b) Yield. Sign R1-2. (c) Speed limit. Sign R2-1. (d) Turn warning with speed recommendation. Sign W1-2a.</figDesc><graphic coords="4,75.73,69.70,173.88,189.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example sign images from (a) the GTSRB and (b) the STS Data set, with the sign bounding boxes superimposed.</figDesc><graphic coords="5,44.94,328.93,246.12,313.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. General flow followed by typical sign detection algorithms.</figDesc><graphic coords="6,54.73,70.07,216.12,101.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Example of thresholding, looking for red hues. (a) Before thresholding. (b) After thresholding.</figDesc><graphic coords="9,147.95,70.30,303.00,119.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Biologically inspired detection stage from<ref type="bibr" target="#b20">[21]</ref>. Image source:<ref type="bibr" target="#b20">[21]</ref>.</figDesc><graphic coords="9,307.95,225.42,246.12,207.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Basic principle behind the radial symmetry detector. Image inspired by [55]. (a) Possible circles for a gradient. (b) Intersecting vote lines.</figDesc><graphic coords="10,79.23,69.71,166.68,84.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Votes from a radial symmetry system superimposed on the original image. The brightest spot coincides with the center of the sign. This image is from a system developed in conjunction with this paper and is a radial symmetry voting algorithm extended to work for rectangles.</figDesc><graphic coords="10,308.74,69.90,234.12,175.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Example of sign relevancy challenges in a crop from our own collected data set. The signs have been manually highlighted, and while both signs would likely be detected, only the one to the right is relevant to the driver. The sign to the left belongs to another road, where the black and white cars come from.</figDesc><graphic coords="11,322.95,70.50,216.12,136.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,47.44,102.57,504.12,347.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,47.44,489.05,504.12,269.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,42.23,93.33,504.12,154.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,42.23,299.26,504.12,333.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II INFORMATION</head><label>II</label><figDesc>ON THE PUBLICLY AVAILABLE SIGN DATABASES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III OVERVIEW</head><label>III</label><figDesc>OF DETECTION METHODS IN 41 RECENT PAPERS. PAPERS WITH THE SAME BACKGROUND COLOR ARE PAPERS WRITTEN BY THE SAME GROUP. WHITE BACKGROUND INDICATE STANDALONE PAPERS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV OVERVIEW</head><label>IV</label><figDesc>OF DETAILED PROPERTIES OF THE 27 PAPERS THAT DO NOT USED TRACKING</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V OVERVIEW</head><label>V</label><figDesc>OF DETAILED PROPERTIES OF THE 14 PAPERS THAT USED TRACKINGTABLE VI OVERVIEW OF THE PERFORMANCE OF THE PAPERS INCLUDED IN THIS SURVEY. FOR THOSE PAPERS WHERE THE NUMBERS ARE AVAILABLE, THE BEST AND MEAN DETECTION RATES ARE PRESENTED, ALONG WITH THE CORRESPONDING FALSE POSITIVE MEASURE. NOTE THAT THE SYSTEMS HAVE ALL BEEN TESTED IN DIFFERENT WAYS. THEREFORE, A DIRECT COMPARISON IS NOT FEASIBLE (SEE SECTION IV FOR FURTHER DETAILS)</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank our colleagues at the LISA-CVRR Laboratory, particularly S. Sivaraman, M. Van Ly, S. Martin, and E. Ohn-Bar for their comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Looking-in and looking-out of a vehicle: Computer-vision-based enhanced vehicle safety</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mccall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="120" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Holistic sensing and active displays for intelligent driver support systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="60" to="68" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision for driver assistance: Looking at people in a vehicle</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Guide to Visual Analysis of Humans: Looking at People</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Krueger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vehicle iconic surround observer: Visualization platform for intelligent driver support applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IV Symp</title>
		<meeting>IEEE IV Symp</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="168" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of traffic sign recognition</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICWAPR</title>
		<meeting>ICWAPR</meeting>
		<imprint>
			<date type="published" when="2010-07">Jul. 2010</date>
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Road and traffic sign detection and recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fleyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th EWGT Meet./16th Mini-EURO Conf</title>
		<meeting>10th EWGT Meet./16th Mini-EURO Conf</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="644" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Goal evaluation of segmentation algorithms for traffic sign recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gomez-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-Bascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gil-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="930" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A single target voting scheme for traffic sign detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IV Symp</title>
		<meeting>IEEE IV Symp</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Shinar</surname></persName>
		</author>
		<title level="m">Traffic Safety and Human Behaviour</title>
		<meeting><address><addrLine>Bingley, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Emerald</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention estimation by simultaneous observation of viewer and view</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. CVPRW, 2010</title>
		<meeting>IEEE Comput. Soc. Conf. CVPRW, 2010</meeting>
		<imprint>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Head pose estimation for driver assistance systems: A robust algorithm and experimental evaluation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ITSC</title>
		<meeting>IEEE ITSC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel active heads-up display for driver assistance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="93" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Traffic sign recognition and analysis for intelligent vehicles</title>
		<author>
			<persName><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Armingol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="258" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m">Convention on Road Signs and Signals of 1968, United Nations Economic Commission for Europe</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">California Manual on Uniform Traffic Control Devices for Streets and Highways, State of California</title>
	</analytic>
	<monogr>
		<title level="j">Dept. Transp</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Sacramento, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Traffic sign shape classification based on correlation techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>VÃ¡zquez-Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-BascÃ³n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Acevedo-RodrÃ­guez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th WSEAS Int. Conf. Signal Process</title>
		<meeting>5th WSEAS Int. Conf. Signal ess</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="149" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognition of traffic signs based on their colour and shape features extracted using human vision models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Podladchikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shaposhnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shevtsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view traffic sign detection, recognition, and 3D localisation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unifying visual saliency with HOG feature learning for traffic sign detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">In-vehicle camera traffic sign detection and recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00138-009-0231-x</idno>
		<ptr target="http://dx.doi.org/10.1007/s00138-009-0231-x" />
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="359" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention-based traffic sign recognition with an array of weak classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Michalke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goerick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IV Symp</title>
		<meeting>IEEE IV Symp</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="333" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: A multi-class classification competition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<ptr target="http://benchmark.ini.rub.de/?section=gtsrb" />
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608012000457" />
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-view traffic sign detection, recognition, and 3D localisation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00138-011-0391-3</idno>
		<ptr target="http://dx.doi.org/10.1007/s00138-011-0391-3" />
	</analytic>
	<monogr>
		<title level="m">Machine Vision and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using Fourier descriptors and spatial models for traffic sign recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Image Anal</title>
		<meeting>Image Anal</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="238" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distance sets for shape filters and shape recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1274" to="1286" />
			<date type="published" when="2003-10">Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Road sign detection in images: A case study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belaroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Soheilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paparoditis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="484" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coherence vector of oriented gradients for traffic sign recognition using neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suchithra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lekhesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ragesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011-05-31">Aug. 5-31, 2011</date>
			<biblScope unit="page" from="907" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Traffic sign classification using K-d trees and Random Forests</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zaklouta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hamdoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011-05-31">Aug. 5-31, 2011</date>
			<biblScope unit="page" from="2151" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A support vector machines network for traffic sign recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Boi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gagliardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2210" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Traffic sign shape classification and localization based on the normalized FFT of the signature of blobs and 2D homographies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Gil</forename><surname>JimÃ©nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>BascÃ³n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2943" to="2955" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time traffic sign recognition from video by class-specific discriminative features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="416" to="430" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast shape-based road sign detection for a driver assistance system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. IROS</title>
		<meeting>IEEE/RSJ Int. Conf. IROS</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards reliable traffic sign recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hoferlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="324" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Traffic sign recognition using evolutionary adaboost detection and forest-ECOC classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Traffic sign detection in dual-focal active camera system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yendo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tehrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IV Symp</title>
		<meeting>IEEE IV Symp</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large scale sign detection using HOG feature variants</title>
		<author>
			<persName><forename type="first">G</forename><surname>Overett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IV Symp</title>
		<meeting>IEEE IV Symp</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Realtime recognition of U.S. speed signs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sprunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baratoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IV Symp</title>
		<meeting>IEEE IV Symp</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stage road sign detection and recognition</title>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2007-07">Jul. 2007</date>
			<biblScope unit="page" from="1427" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detection and classification of road signs in natural environments</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Nguwi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kouzani</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-007-0120-z</idno>
		<ptr target="http://dx.doi.org/10.1007/s00521-007-0120-z" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="289" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">General traffic sign recognition by feature matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. IVCNZ</title>
		<meeting>24th Int. Conf. IVCNZ</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov. 2009</date>
			<biblScope unit="page" from="409" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust traffic sign shape recognition using geometric matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Road speed sign recognition using edge-voting principle and learning vector quantization network</title>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICS</title>
		<meeting>ICS</meeting>
		<imprint>
			<date type="published" when="2010-12">Dec. 2010</date>
			<biblScope unit="page" from="246" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A detection and recognition method for prohibition traffic signs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qingsong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tiantian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. IASP</title>
		<meeting>Int. Conf. IASP</meeting>
		<imprint>
			<date type="published" when="2010-04">Apr. 2010</date>
			<biblScope unit="page" from="583" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Road-sign detection and recognition based on support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-Bascon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gil-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gomez-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>LÃ³pez-Ferreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="278" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A decision support system for the automatic management of keep-clear signs based on support vector machines and geographic information systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lafuente-Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salcedo-Sanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maldonado-BascÃ³n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Portilla-Figueras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>LÃ³pez-Sastre</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1628324.1628558" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="767" to="773" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Real-time recognition of road traffic sign in motion image based on genetic algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. Cybern</title>
		<meeting>Int. Conf. Mach. Learn. Cybern</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="83" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Colour vision model-based approach for segmentation of traffic signs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Passmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Podladchikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shaposhnikov</surname></persName>
		</author>
		<idno type="DOI">10.1155/2008/386705</idno>
		<ptr target="http://dx.doi.org/10.1155/2008/386705" />
	</analytic>
	<monogr>
		<title level="j">J. Image Video Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2008-01">2008. Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Integrating object detection with 3D tracking towards a better driver assistance system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ICPR</title>
		<meeting>20th ICPR</meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="3344" to="3347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Intelligent traffic sign detector: Adaptive learning based on online gathering of training samples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shirasuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IV Symp</title>
		<meeting>IEEE IV Symp</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Real-time regular polygonal sign detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Field and Service Robotics</title>
		<imprint>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2006">2006</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Real-time speed sign detection using the radial symmetry detector</title>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="322" to="332" />
			<date type="published" when="2008-06">Jun. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A two stage detection module for traffic signs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kummert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muller-Schneiders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICVES</title>
		<meeting>IEEE ICVES</meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="248" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A decision fusion and reasoning module for a traffic sign recognition system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Gormer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muller-Schneiders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kummert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1126" to="1134" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust traffic signs detection by means of vision and V2I communications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Garcia-Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ocana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Llorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Llamazares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. IEEE ITSC</title>
		<meeting>14th Int. IEEE ITSC</meeting>
		<imprint>
			<date type="published" when="2011-10">Oct. 2011</date>
			<biblScope unit="page" from="1003" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Automatic traffic signs and panels inspection system using computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Llorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gavilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Revenga De Toro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="485" to="499" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Robust onvehicle real-time visual detection of American and European speed limit signs, with a modular traffic signs recognition system</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bargeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1122" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Angle vertex and bisector geometric model for triangular road sign detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belaroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards real-time traffic sign recognition by class-specific discriminative features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Brit. Mach. Vis. Conf</title>
		<meeting>18th Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Road sign detection from edge orientation histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alefs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Eschemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="993" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The histogram feature-A resource-efficient weak classifier</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="678" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Color exploitation in hog-based traffic sign detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Creusen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wijnhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herbschleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De With</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th IEEE ICIP</title>
		<meeting>17th IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="2669" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A system for traffic sign detection, tracking, and recognition using color, shape, and motion information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellkofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fast radial symmetry for detecting points of interest</title>
		<author>
			<persName><forename type="first">G</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="959" to="973" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Real-time radial symmetry for speed sign detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intell. Veh. Symp</title>
		<meeting>IEEE Intell. Veh. Symp</meeting>
		<imprint>
			<date type="published" when="2004-06">Jun. 2004</date>
			<biblScope unit="page" from="566" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Satellite imagery based adaptive background models and shadow suppression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image Video Process</title>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="119" to="132" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
