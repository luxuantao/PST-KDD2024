<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting API Documentation with Insights from Stack Overflow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Treude</surname></persName>
							<email>christoph.treude@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Adelaide Adelaide</orgName>
								<address>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
							<email>martin@cs.mcgill.ca</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University Montréal</orgName>
								<address>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Augmenting API Documentation with Insights from Stack Overflow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EED5A3191374E615F7584C0D4C402691</idno>
					<idno type="DOI">10.1145/2884781.2884800</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Information systems → Information extraction</term>
					<term>•Software and its engineering → Documentation</term>
					<term>•Computing methodologies → Supervised learning</term>
					<term>API documentation, Stack Overflow, insight sentences</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Software developers need access to different kinds of information which is often dispersed among different documentation sources, such as API documentation or Stack Overflow. We present an approach to automatically augment API documentation with "insight sentences" from Stack Overflowsentences that are related to a particular API type and that provide insight not contained in the API documentation of that type. Based on a development set of 1,574 sentences, we compare the performance of two state-of-the-art summarization techniques as well as a pattern-based approach for insight sentence extraction. We then present SISE, a novel machine learning based approach that uses as features the sentences themselves, their formatting, their question, their answer, and their authors as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on the development set. In a comparative study with eight software developers, we found that SISE resulted in the highest number of sentences that were considered to add useful information not found in the API documentation. These results indicate that taking into account the meta data available on Stack Overflow as well as part-of-speech tags can significantly improve unsupervised extraction approaches when applied to Stack Overflow data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>While much of the information needed by software developers is captured in some form of documentation, it is often Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. not obvious where a particular piece of information is stored. Different documentation formats, such as wikis or blogs, contain different kinds of information, written by different individuals and intended for different purposes <ref type="bibr" target="#b37">[38]</ref>. For instance, API documentation captures information about functionality and structure, but lacks other types of information, such as concepts or purpose <ref type="bibr" target="#b17">[18]</ref>. Some of the most severe obstacles faced by developers learning a new API are related to its documentation <ref type="bibr" target="#b31">[32]</ref>, in particular because of scarce information about the API's design, rationale <ref type="bibr" target="#b30">[31]</ref>, usage scenarios, and code examples <ref type="bibr" target="#b31">[32]</ref>.</p><p>On the other hand, "how-to" questions <ref type="bibr" target="#b34">[35]</ref> (also referred to as "how-to-do-it" questions <ref type="bibr" target="#b9">[10]</ref>) are the most frequent question type on the popular Question and Answer (Q&amp;A) site Stack Overflow, and the answers to these questions have the potential to complement API documentation in terms of concepts, purpose, usage scenarios, and code examples. While a lot of research has focused on finding code examples for APIs (e.g., <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b32">[33]</ref>), less work has been conducted on improving or augmenting the natural language descriptions contained in API documentation.</p><p>To fill this gap, we compare techniques for automatically extracting sentences from Stack Overflow that are related to a particular API type and that provide insight not contained in the API documentation. We call these sentences insight sentences. The idea is related to update summarization <ref type="bibr" target="#b3">[4]</ref>, which attempts to summarize new documents assuming that the reader is already familiar with certain old documents. Update summarization is often applied to summarizing overlapping news stories. Applied to API documentation and content from Stack Overflow, the idea is to create a summary of the discussions on Stack Overflow as they relate to a given API type, assuming that the reader is already familiar with the type's API documentation.</p><p>Our research is guided by two main questions:</p><formula xml:id="formula_0">RQ1:</formula><p>To what extent are unsupervised and supervised approaches able to identify meaningful insight sentences?</p><p>RQ2: Do practitioners find these sentences useful?</p><p>To answer the first research question, we applied two state-of-the-art extractive summarization techniques-LexRank <ref type="bibr" target="#b12">[13]</ref> and Maximal Marginal Relevance (MMR) <ref type="bibr" target="#b3">[4]</ref>-to a set of API types, their documentation, and related Stack Overflow threads. These techniques assign a numeric value to each sentence and return the top-ranked sentences as a summary. We found these summarization techniques to perform poorly on our data, mainly because sentences on Stack Overflow are often not 2016 IEEE/ACM 38th IEEE International Conference on Software Engineering  We also applied a pattern-based approach that has been successfully used to detect and recommend fragments of API documentation potentially important to a programmer who has already decided to use a certain API element <ref type="bibr" target="#b6">[7]</ref>, with similar results. We then developed a novel machine learning approach called SISE (Supervised Insight Sentence Extractor), which uses as features the sentences themselves, their formatting, their question, their answer, and their authors as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on our development set. 1 In addition to the similarity of a sentence to its corresponding API documentation, characteristics of the user asking the question, the score and age of the answer, question characteristics, and the part-of-speech tags at the beginning of a sentence were among the features with the highest information gain.</p><p>To answer the second research question on whether practitioners find these sentences useful, we conducted a comparative study in which we asked eight software developers to rate sentences extracted with all four approaches (LexRank, MMR, patterns, and SISE). These sentences were related to 20 Java API types. The study showed that sentences 1 Precision measures the fraction of sentences extracted by an approach that are meaningful while coverage measures the ratio of API types for which the approach extracts at least one sentence. extracted by SISE were considered significantly more meaningful and resulted in the most sentences that added useful information not contained in the API documentation. These results indicate that taking into account Stack Overflow meta data as well as part-of-speech tags can significantly improve existing unsupervised approaches when applied to Stack Overflow data.</p><p>The main contributions of this work are the conceptualization of insight sentences as sentences from one documentation source that provide insight to other documentation sources and a comparative study of four different approaches for the extraction of such insight sentences, as well as a list of factors that can be used to distinguish insight sentences from sentences that are not meaningful or do not add useful information to another documentation source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MOTIVATING EXAMPLES</head><p>Table <ref type="table" target="#tab_0">1</ref> shows, for three Java API types, a sentence taken from Stack Overflow that contains useful information about this type that is not stated in the type's API documentation. The goal of our work is to automatically extract such sentences and use them to augment API documentation. The first example is taken from our development set while the other two examples were automatically identified by SISE.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows the source of the sentence in the first example. In an answer to a question about "DataSource and DriverManager on J2SE", the Stack Overflow user first cites a statement from the API documentation, but then elaborates on it further than the API documentation does. This is an example of the API documentation lacking information on purpose <ref type="bibr" target="#b17">[18]</ref> since it only states which alternative is preferred without explaining why. In contrast, the sentence added by the Stack Overflow user clearly distinguishes the roles of the API types discussed and explains which one should be used in which situation. The second example shows a case where a Stack Overflow user clarifies the relationship between types with similar names in an answer to "HTTP URL Address Encoding in Java". In the third example, a user again compares two alternatives and explains which one to use in a particular situation, this time in an answer to "Thread sleep and thread join".</p><p>In the next sections, we describe our investigation of the means to automatically identify sentences on Stack Overflow that are meaningful and add useful information not contained in the API documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LINKING DOCUMENTATION</head><p>First, we describe how we identify Stack Overflow threads related to a given API type as well as our development set used for comparing different extraction approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linking</head><p>Our linking approach identifies Stack Overflow threads that are related to an API type in a two-step process: (1) we perform a full-text query for the non-qualified type name using the Stack Overflow API relying on its "relevance" ordering, and (2) we reject all results that do not match at least one of a number of regular expressions that are applied to the question that started the thread (see Table <ref type="table" target="#tab_1">2</ref>). The advantage of using the Stack Overflow API over the Stack Overflow data dump used in previous research such as that of Bacchelli et al. <ref type="bibr" target="#b1">[2]</ref> is that sentences extracted by our linking approach always reflect the latest content available on Stack Overflow. However, the querying mechanism offered by the Stack Overflow API is limited, thus warranting the additional filtering in step <ref type="bibr" target="#b1">(2)</ref>.</p><p>We manually created a benchmark to measure the relevance of the threads that the filter identifies in step <ref type="bibr" target="#b1">(2)</ref>. For 40 randomly selected types of the Java SE 7 API (20 types with one-word identifiers, such as List, and 20 types with multi-word identifiers, such as ArrayList), we selected the first five threads that the Stack Overflow API returned for each non-qualified type, and we manually annotated whether the thread actually mentioned the API type. We evaluated our linking approach separately for one-word types and multi-word types. For one-word types, precision was 0.82 and recall was 1.0 (F1-measure: 0.90), and for multi-word types, precision was 0.92 and recall was 0.97 (F1-measure: 0.94).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotated Data</head><p>We created a development set by manually annotating all 1,574 sentences that belong to the top ten answers (as indicated by their scores) from the first ten threads that our linking approach associated with ten Java API types. We did not remove false positives (threads not mentioning a target type) to ensure that the development set is a realistic representation of the sentences used as input to the extraction approaches. To sample API types, we identified the three most commonly used types from each of the ten most commonly used Java packages, as indicated by the Apatite tool <ref type="bibr" target="#b11">[12]</ref>, see Table <ref type="table" target="#tab_2">3</ref>. We chose this stratified sampling strategy to ensure a wide coverage of commonly used types while not focusing on rarely used ones. We then divided the data into the development set and the test set for the comparative study (see Section 6) as follows: The second most commonly used type from each package was used to construct the development set, and the most commonly used type as well as the third most commonly used type was used for the comparative study. The first author annotated each of the 1,574 sentences belonging to the second most commonly used type in each of the ten packages with a yes/no rating to indicate whether it was meaningful on its own.<ref type="foot" target="#foot_0">2</ref> 196 sentences were rated as meaningful. Table <ref type="table">4</ref> shows the number of sentences considered meaningful for each of the ten API types.</p><p>As an example, the first author annotated the following three sentences related to Java's ArrayList as being meaningful: "The list returned from asList has fixed size", "There is one common use case in which LinkedList outperforms ArrayList: that of a queue", and "It's worth noting that Vector also implements the List interface and is almost identical to ArrayList". Examples for sentences that are not meaningful and related to the same API type are: "See the next step if you need a mutable list", "They serve two different purposes", and "Use the Javadocs". All of these sentences make sense in the context of an entire question-and-answer thread, but they do not convey meaningful information on their own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">UNSUPERVISED APPROACHES</head><p>In this section, after outlining our preprocessing steps, we present the results of using state-of-the-art text summarization and pattern-based approaches for the extraction of insight sentences. We had developed a set of techniques for preprocessing software documentation in previous work <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. We summarize them here for completeness.</p><p>We remove markup from the HTML files and preprocess the resulting text files to account for the unique characteristics of software documentation not found in other texts, such as the systematic use of incomplete sentences and the presence of code terms. In particular, we prefix sentences that start with a verb in present tense, third person singular, such as "returns" or "computes", with the word "this" to ensure correct parsing of partial sentences, such as "Returns the next page number". In addition, we prefix sentences that start with a verb in present participle or gerund, such as "adding" or "removing", immediately followed by a noun, with the word "for" to ensure correct parsing of partial sentences, such as "Displaying data from another source". <ref type="foot" target="#foot_1">3</ref> We further configure the Stanford NLP parser <ref type="bibr" target="#b18">[19]</ref> to automatically tag all code elements as nouns. In addition to code elements tagged with tt or code tags in the original source, all words that match one of about 30 hand-crafted regular expressions are treated as code elements. <ref type="foot" target="#foot_2">4</ref> The resulting sentences are then parsed using the Stanford NLP toolkit.</p><p>These preprocessing steps are identical for all the approaches described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LexRank</head><p>LexRank is a text summarization technique that conceptualizes a text as a graph where each node represents a sentence and the weight of each edge corresponds to the cosine similarity of the sentences it connects. <ref type="foot" target="#foot_3">5</ref> The importance of a sentence is then given by the eigenvector centrality of the corresponding node <ref type="bibr" target="#b12">[13]</ref>. We chose LexRank because it is the best-known graph-based method for summarization <ref type="bibr" target="#b23">[24]</ref>.</p><p>We re-implemented LexRank in Java and calculated the eigenvector centrality of each sentence in our development set, separately for each API type. <ref type="foot" target="#foot_4">6</ref> For each API type, we considered all related sentences in our development set as the text to be summarized. The result is a numeric score for each sentence. There is no clear rule as to how many sentences should be considered for a summary or what a Table <ref type="table" target="#tab_3">5</ref> shows the results in terms of precision and coverage. We define coverage as the ratio of API types for which there is at least one sentence. We focus on coverage instead of recall because our goal is the extraction of useful insight sentences and not the identification of all possible insight sentences. If we only consider the sentence with the highest eigenvector centrality to be the insight sentence for each API type, the average precision across the ten API types in our development set is 0.40. The precision drops if we consider more sentences as insight sentences. We also explored the effects of different eigenvector centrality thresholds. As Table <ref type="table" target="#tab_3">5</ref> shows, the precision remains low and coverage drops as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Update Summarization</head><p>The goal of update summarization is to produce a summary of a new document under the assumption that the reader is already familiar with the content of a given set of old documents. Applied to insight sentence extraction, the idea is to create a summary of Stack Overflow threads related to an API type under the assumption that the reader is already familiar with the type's API documentation.</p><p>As previously done by Boudin et al. <ref type="bibr" target="#b3">[4]</ref>, we adopted the concept of Maximal Marginal Relevance (MMR) in our implementation of update summarization, using the LexRank score as a baseline for calculating the MMR scores (see previous section). We subtracted from each sentence's LexRank score the maximum cosine similarity between that sentence and any sentence in the API type's documentation. In other words, if the similarity between a sentence and each sentence in the API documentation is 0, the MMR score is identical to the one assigned by LexRank. However, sentences that are similar to at least one sentence in the API documentation receive a score lower than the one assigned by LexRank.</p><p>Table <ref type="table" target="#tab_4">6</ref> shows precision and coverage for different configurations of our MMR implementation when applied to the development set. The results are worse than those for LexRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Knowledge Patterns</head><p>Previous research has successfully used knowledge patterns to detect and recommend fragments of API documentation potentially important to a programmer using an API element. In their work, Chhetri and Robillard <ref type="bibr" target="#b6">[7]</ref> categorize text fragments in API documentation based on whether they contain information that is indispensable, valuable, or neither. From the fragments that contain potentially im- numeric whether the answer user is registered boolean relative rank of the answer among all answers to that question by score and age numeric similarity cosine similarity between sentence and most similar sentence in API documentation numeric average cosine similarity between sentence and all sentences in API documentation numeric portant knowledge, they extract word patterns to automatically find new fragments that contain similar knowledge in unseen documentation. In a study involving independent human participants, indispensable knowledge items recommended for API types were judged useful 57% of the time and potentially useful an additional 30% of the time.</p><p>Each knowledge pattern consists of a small number of words, such as "specify, should, argument" or "not, exceed, number, should". To match a knowledge pattern, a sentence must contain all of the words in the pattern, but not necessarily in the order specified by the pattern. Instead of a word, a pattern can contain a special wildcard for code elements which indicates that only sentences containing a code element can match the pattern. An example is given by the pattern "must, not, &lt;code element&gt;, null".</p><p>We applied the 361 patterns for indispensable and valuable knowledge items extracted from the reference documentation of the Java 6 SE SDK to our development set and calculated precision and coverage. To ensure that the matching of sentences to patterns is not sensitive to different grammatical forms, we applied stemming to each word in the patterns and the sentences using Porter's stemmer <ref type="bibr" target="#b26">[27]</ref> before calculating the matches. Similar to the attempts of using text summarization techniques for the extraction of insight sentences, the application of knowledge patterns to our development set did not produce encouraging resultswe obtained a precision of 0.15 and a coverage of 0.8. This can be explained by the patterns' reliance on the systematic writing style of reference documentation which is not used in informal documentation formats such as Stack Overflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SISE: A SUPERVISED APPROACH</head><p>Considering the poor results achieved with traditional summarization approaches, we developed SISE (Supervised Insight Sentence Extractor), a novel approach specifically for extracting insight sentences from Stack Overflow. A supervised approach efficiently supports considering a large amount of potential factors available for each sentence on the Q&amp;A website, such as the reputation of the user authoring a sentence or the score of the corresponding answer.</p><p>After the preprocessing steps described in Section 4, we used the features shown in Table <ref type="table" target="#tab_5">7</ref> for each sentence as input for machine learning algorithms. We designed the feature set to cover all meta data available on Stack Overflow as well as basic syntactic and grammatical features. In addition, we included two features for the similarity of a sentence to the corresponding API documentation, following the idea of update summarization <ref type="bibr" target="#b3">[4]</ref>. Most of the features are either boolean or numeric. For the two string features, we used WEKA's <ref type="bibr" target="#b14">[15]</ref> StringToWordVector filter to turn the corresponding text into separate features for single words, bigrams, and trigrams. For example, the simple sentence "List is slower than Arrays" would result in one feature for each lemmatized word ("&lt;code element&gt;", "be", etc.), four features for the bigrams ("&lt;code element&gt; be", "be slow", etc.), and three features for the trigrams ("&lt;code element&gt; be slow", "be slow than", etc.). For the part-of-speech tags feature set, the same number of features would be produced based on the part-of-speech text that corresponds to the sentence, which in the example is "NN VBZ JJR IN NN" (a noun followed by a verb in third person singular, a comparative adjective, a preposition, and another noun).</p><p>We tested different machine learning algorithms that are commonly used for text classification in software engineering (e.g., <ref type="bibr" target="#b9">[10]</ref>) on our development set: k-nearest neighbour (Ibk) <ref type="bibr" target="#b0">[1]</ref>, decision trees (J48) <ref type="bibr" target="#b27">[28]</ref>, Naive Bayes <ref type="bibr" target="#b15">[16]</ref>, random forest <ref type="bibr" target="#b4">[5]</ref>, and support vector machines (SMO, the sequential minimal optimization implementation in WEKA) <ref type="bibr" target="#b22">[23]</ref>. Apart from J48 and random forest, all classifiers belong to different classes, ensuring a wide coverage of different pos- of occurrences of the verb "be" in sentence sible algorithms. We used WEKA's default setting for each classifier except for the k-nearest neighbour classifier which we instantiated with values of one, five, and ten for k. 7 Because of the large number of features generated by our treatment of string features, we calculated the information gain of each feature and used attribute selection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40]</ref> to reduce the data set in order to improve classifier performance.</p><p>To calculate the precision and coverage of each classifier, we applied what we call "ten-type cross validation", i.e., we trained the algorithms on sentences belonging to nine API types in our development set, and we tested them on the tenth type. Each type was used once as the test type. We used five different settings for filtering out features based on low information gain: no filtering as well as filtering at a 0.01, 0.02, 0.03, and 0.04 threshold, respectively.</p><p>Only four cases achieved a precision of above 0.5: the random forest classifier without attribute selection achieved a precision of 0.60 with a coverage of 0.7 (i.e., the classifier produced at least one sentence for seven out of the ten types in our development set). The support vector machine classifier showed a similar performance in terms of precision at a value of 0.64 when combined with an information gain filter for features at thresholds of 0.02 and 0.03, with the same coverage. Finally, the random forest classifier with an attribute selection threshold of 0.03 achieved perfect precision, but only covered a single type in the development set. Balancing precision and coverage using the harmonic mean, we conclude that the most promising performance was shown by the support vector machine classifier at information gain thresholds of 0.02 and 0.03. We use the 0.02 setting for the remainder of the paper. 7 Tuning machine learning parameters is part of our future work. Table <ref type="table" target="#tab_6">8</ref> shows the features in this setting. The features are diverse, ranging from a sentence's similarity to the API documentation and part-of-speech tags to attributes of the answer, question, and the user asking the question.</p><p>In answering our first research question regarding the ability of different approaches to identify meaningful insight sentences, we conclude that only the supervised approach was able to identify insight sentences with reasonable precision and coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">COMPARATIVE STUDY</head><p>To investigate our second research question, i.e., whether practitioners find these insight sentences useful, we conducted a comparative study. We selected the most commonly used type and the third most commonly used type from each of the ten most commonly used Java packages, as indicated by Apatite <ref type="bibr" target="#b11">[12]</ref> (cf. Table <ref type="table" target="#tab_2">3</ref>). The motivation for this stratified sampling was to cover a wide range of types while avoiding ones that are rarely used.</p><p>We then used all four approaches (LexRank, MMR, patterns, and SISE) to extract insight sentences for these 20 API types. For LexRank and MMR, we used the configuration that achieved the highest precision on the development set (i.e., we considered only the sentence with the highest score). Table <ref type="table" target="#tab_7">9</ref> shows the number of sentences that each approach extracted. LexRank and MMR extracted exactly one sentence per API type, while patterns extracted between 1 and 21 sentences per API type and SISE extracted between 0 and 22 sentences per API type. <ref type="foot" target="#foot_5">8</ref> As the last column shows, the overlap between sentences extracted by different approaches was very low: 213 of the 222 sentences selected by all approaches for all API types were unique. Seven of the nine overlaps occurred between LexRank and MMR, one occurred between LexRank and patterns, and one involved patterns and SISE. To keep the number of sentences man- ageable for a study, we randomly selected up to four sentences per approach and per API type. This resulted in at most ten sentences per API type (one for LexRank, one for MMR, at most four for patterns, and at most four for SISE). Duplicate sentences (selected by more than one approach) were only shown to participants once. We recruited eight participants from GitHub, randomly selecting from the 68,949 GitHub users who had made at least one contribution in the previous twelve months, used Java in at least one of their projects, and had published their email address. We randomly selected email addresses in batches of ten. It took 40 emails to recruit these eight participants (response rate 20%). The study was conducted using Google Forms and there were no time constraints. To minimize bias, we did not explain the research goal to participants. Each participant was shown sentences belonging to five API types, leading to a maximum of 50 sentences per participant. All sentences were rated by exactly two participants. We asked each participant whether developing software was part of their job, about their job title, for how many years they had been developing software, and what their area of software development was. Table <ref type="table" target="#tab_8">10</ref> shows the answers. All participants indicated that developing software was part of their job.</p><p>For each pair of an API type and a sentence, we asked the participants to choose one of the following options:</p><p>• The sentence is meaningful and adds useful information not found in the API documentation.</p><p>• The sentence is meaningful, but does not add useful information to the API documentation.</p><p>• The sentence requires more context to understand.</p><p>• The sentence does not make sense to me.</p><p>These options were motivated by Binkley et al.'s <ref type="bibr" target="#b2">[3]</ref> observation that summaries should be judged based on their usefulness rather than their quality alone.</p><p>Table <ref type="table" target="#tab_0">11</ref> shows the results of the comparative study. SISE resulted in most ratings indicating a meaningful sentence, both in absolute numbers and relatively. In total, 70% of the sentences identified by SISE were considered meaningful (the first two answer options), compared to 44% for patterns, 43% for MMR, and 35% for LexRank. When comparing SISE to each of the other approaches, the difference between meaningful sentences and not meaningful sentences (the last two answer options) is statistically significant (Pearson's chi square, p &lt; 0.005). In addition, SISE resulted in the highest number of sentences which were considered to add useful information not found in the API documentation.</p><p>Table <ref type="table" target="#tab_9">12</ref> shows the inter-rater agreement. Out of a total of 134 pairs of ratings, 58 (43%) were in perfect agreement. The highest number of disagreements <ref type="bibr" target="#b21">(22)</ref> was related to sentences that either require more context to understand or make no sense.</p><p>In answering our second research question on the usefulness of insight sentences as perceived by practitioners, we conclude that our participants found more sentences which contained useful information in the output of SISE compared to the output of other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION</head><p>This section discusses the implications of our work, in particular related to user interface design for insight sentence presentation and to the role that meta data on Stack Overflow can play for the extraction of insight sentences. In addition, we discuss the inter-rater agreement from our comparative study in more detail and review the threats to validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Sentence Meta Data</head><p>This work shows that the large amount of meta data on Stack Overflow can be used for the extraction of insight sentences. Compared to state-of-the-art summarization techniques or pattern-based techniques which do not take any meta data into account, SISE achieved higher precision and usefulness. Out of the 22 features with the highest information gain used in the classifier, half of them (and eight out of the first ten) represent Stack Overflow meta data, such as the number of views on a question or the score of an answer (cf. Table <ref type="table" target="#tab_6">8</ref>). Interestingly, the features with the highest information gain also suggest that the meta data of the person asking the question is possibly more important than the meta data of the person authoring the answer. For example, the feature with the third highest information gain is the acceptance rate of the person asking the question. We hypothesize that the acceptance rate of a user reflects the kinds of questions that such a user would ask, and that insight sentences are more likely to come from answers to questions that ask about basic information instead of specific use cases. Future work will have to be conducted to investigate this hypothesis.</p><p>Another interesting finding is that the two features that represent the similarity of a potential insight sentence to the corresponding API documentation were the features with the highest information gain. This finding suggests that there is an advantage to interpreting sentences on Stack Overflow in the context of other documentation sources. Our current hypotheses regarding the positive correlation between sentence similarity and meaningful insights is that a "somewhat similar" sentence combines content from the API documentation with new information, while less similar sentences contain information completely unrelated to an API type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">User Interface</head><p>The results of our evaluation suggest that the context of sentences will play an important role when complementing API documentation with sentences from Stack Overflow. In fact, only 15% of the ratings for sentences extracted by SISE indicated that the sentence did not make sense. Another 15% of the ratings indicated that more context was required for the sentence to be understandable. Since this context (e.g., surrounding code snippets, the complete answer, or the corresponding question) is available on Stack Overflow, it would be possible to display it along with an insight sentence. For example, each insight sentence could be accompanied by an expandable widget which shows the entire thread on Stack Overflow from which the insight sentence originated. In addition, user input similar to the one we gathered as part of our comparative study could be used to continuously improve the extraction of insight sentences.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the current version of our interface for SISE. In the top left corner of the API documentation, a widget is added that shows up to five insight sentences for the API type. Upon selection of one sentence, the sentence is expanded to show the surrounding paragraph from the original source, along with a link to the corresponding Stack Overflow thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Inter-rater Agreement</head><p>Since the inter-rater agreement in our comparative study was relatively low, we analyzed the disagreements in more detail.</p><p>For the twelve cases where both raters agreed that the sentence was meaningful but disagreed as to whether it added useful information not contained in the API documentation, we manually verified whether that information could indeed be found in the API documentation. In nine out of the twelve cases, the information in the sentence was available in the API documentation. However, the insight sentence often summarized information in a more succinct way than the API documentation did, e.g., "List is an ordered sequence of elements whereas Set is a distinct list of elements which is unordered", which was extracted by SISE for Java's List. In some contexts, such sentences could still be useful since they provide a more succinct version of content that is available in the API documentation. There were 28 cases where one rater indicated that more context was required to understand the sentence while the other rater indicated that the sentence was meaningful. In many of these cases, the background of the users seems to determine whether they understand a sentence or not. We found a similar situation in our previous work <ref type="bibr" target="#b35">[36]</ref> when we asked developers to rate the meaningfulness of task descriptions that we had automatically extracted from their software documentation. In those cases, we argue that displaying such sentences does little harm if some users do not understand them while other users find them useful. An example for such a sentence from our data set is "Yes you should close a ResultSet", which was extracted by the pattern-based approach for Java's ResultSet. Arguably, this sentence should be accompanied by a question to be more meaningful, yet the message from the sentence is understandable without the specific question.</p><p>In 14 cases, one participant indicated that a sentence did not make sense while the other participant found it meaningful. A manual inspection of those 14 cases suggests that in most cases, the problem was missing context. An example is "I don't know what's your problem, but if you have some problems to run this code, you can try to close connection and open other to make the second query", a sentence that was extracted by LexRank and MMR and is related to Java's ResultSet. While the sentence does require more context about the questioner's problem to be understandable, it might be helpful without such context if a user is troubleshooting a connection issue related to a Result-Set. As mentioned before, we are addressing the context issue with a user interface that shows more context when requested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Threats to Validity</head><p>A threat to the validity of our results is the manual construction of the development set since we did not attempt to validate the annotated data. However, it would have been practically impossible for us to annotate sentences in a way that would favour specific approaches. When the development set was constructed, we were not aware that SISE would be based on machine learning, thus our development set was not biased towards certain features.</p><p>The size of the development set is another limitation since the sentences were related to only ten Java API types. However, for each API type, we considered ten different questions on Stack Overflow, and for each question, we considered up to ten answers. In total, the 1,574 sentences originated from 309 different answers. In addition, our finding that SISE outperformed state-of-the-art summarization techniques and a pattern-based approach was confirmed in a comparative study with sentences related to another twenty Java API types.</p><p>The agreement about insight sentences between our study participants was relatively low. It is natural for software developers with different backgrounds and different experience to disagree on what information is useful. Despite the disagreements, the comparative study clearly showed that SISE produced the most meaningful and useful insight sentences.</p><p>The evaluation of the usefulness of the insight sentences was based on subjective assessment from the study participants. Although all sentences were judged by two participants to eliminate the threat of individual bias, it is nevertheless possible that the responses may be affected by collective bias. There was no mechanism to ensure participants read the API documentation before rating sentences, and we acknowledge this threat to validity.</p><p>We cannot claim that SISE generalizes beyond Java. However, none of the features used in SISE are specific to Java, and we are optimistic that we can achieve similar results for other programming languages in future work.</p><p>We also cannot make claims regarding generalizability beyond Stack Overflow. However, with more than 17 million answers, Stack Overflow is a big enough data source to war- rant specialized tools to utilize its data. SISE will only work if a topic is discussed on Stack Overflow. Since all insight sentences used in this paper were obtained from sets of ten Stack Overflow threads associated with an API type, we would expect comparable results for any API type with at least ten threads on Stack Overflow. As we found in our previous work <ref type="bibr" target="#b20">[21]</ref>, 77% of the types of the Java API are discussed on Stack Overflow (Android: 87%) -thus, we do not expect library popularity to be a major limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>Work related to our approach for insight sentence extraction can be divided into work on harnessing Stack Overflow data and work on improving API documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Harnessing Stack Overflow data</head><p>Seahawk by Bacchelli et al. <ref type="bibr" target="#b1">[2]</ref> is an Eclipse plug-in that integrates Stack Overflow content into an integrated development environment (IDE). Seahawk automatically formulates queries from the current context in the IDE and presents a ranked and interactive list of results. The tool lets users identify individual discussion pieces and import code samples through drag &amp; drop. In addition, users can link Stack Overflow discussions and source code. An evaluation of Seahawk showed that the tool can produce surprising insights that aid a developer in program comprehension and software development <ref type="bibr" target="#b24">[25]</ref>. A related tool called Prompter was later proposed by Ponzanelli et al. <ref type="bibr" target="#b25">[26]</ref>. Given the IDE context, Prompter automatically retrieves pertinent discussions from Stack Overflow, evaluates their relevance and notifies developers about the available help if a given threshold is surpassed.</p><p>Other approaches have focused on harnessing Stack Overflow data for explaining stack traces in the IDE. Cordeiro et al. <ref type="bibr" target="#b7">[8]</ref> developed a tool that integrates the recommendation of Q&amp;A web resources related to stack traces into Eclipse. Their preliminary evaluation showed that their ap-proach outperformed a simple keyword-based approach. In a similar line of work, Rahman et al. Arguably the work that is most similar to ours is Au-toComment, the automatic comment generation approach introduced by Wong et al. <ref type="bibr" target="#b38">[39]</ref>, since it also harnesses the natural language text available on Stack Overflow. Auto-Comment extracts code-descriptions mappings, which are code segments together with their descriptions, from Stack Overflow, and leverages this information to automatically generate descriptive comments for similar code segments in open-source projects. The authors applied AutoComment to Java and Android projects, and they were able to automatically generate 102 comments for 23 projects. In a user study, the majority of participants found the generated comments to be accurate, adequate, concise, and useful in helping them understand the code. Our work differs from that by Wong et al. in that we focus on single sentences from Stack Overflow that are relevant to an API type instead of a code snippet.</p><p>Related to our solution for linking Stack Overflow threads to API types is the work by Rigby and Robillard <ref type="bibr" target="#b29">[30]</ref>. Their traceability recovery approach discovers essential code elements in informal documentation such as Stack Overflow. Our linking approach for linking Stack Overflow threads to API types works the other way around. We start from an API type, and then use the Stack Overflow API as well as a number of regular expressions to find threads that are related to that API type.</p><p>In terms of using machine learning to discover content on Stack Overflow, there are some common themes between SISE and the work of de Souza et al. <ref type="bibr" target="#b9">[10]</ref>. They developed an improved search engine for content on Stack Overflow which recommends question-and-answer pairs (as opposed to entire Q&amp;A threads) based on a query. The ranking criteria used by their approach consists of the textual similarity of the question-and-answer pairs to the query and the quality of these pairs. In addition, their search focuses on "how-to" threads. In an evaluation of their work, the authors found that their approach was able to recommend at least one useful question-and-answer pair for most queries, many of which included a reproducible code snippet. In comparison, the goal of SISE is the extraction of insight sentences from Stack Overflow that add useful information to API documentation. Our catalogue of machine learning features is also more extensive and includes features that bridge the gap between different documentation formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Improving API documentation</head><p>Several researchers have contributed efforts for the improvement of API documentation. Stylos et al. <ref type="bibr" target="#b32">[33]</ref> introduced Jadeite, which displays commonly used classes more prominently and automatically identifies the most common ways to construct an instance of any given class. eMoose by Dekel and Herbsleb <ref type="bibr" target="#b10">[11]</ref> improves API documentation by decorating method invocations whose targets have associated usage directives, such as rules or caveats, of which authors of invoking code must be aware. In a similar effort, Pandita et al. <ref type="bibr" target="#b19">[20]</ref> proposed to infer formal specifications from natural language text.</p><p>More closely related to SISE is the proposal for integrating crowdsourced FAQs into API documentation by Chen and Zhang <ref type="bibr" target="#b5">[6]</ref>. They propose to connect API documentation and informal documentation through the capture of developers' Web browsing behaviour. In contrast, we connect different forms of documentation through heuristics that match Stack Overflow threads to API types, and instead of FAQs, SISE produces insight sentences.</p><p>Other work has focused on detecting and preventing API documentation errors. Zhong and Su <ref type="bibr" target="#b40">[41]</ref> introduced DOCREF, an approach that combines natural language processing techniques and code analysis to detect and report inconsistencies in documentation. The authors successfully used DOCREF to detect more than one thousand documentation errors. Dagenais and Robillard <ref type="bibr" target="#b8">[9]</ref> introduced AdDoc, a technique that automatically discovers documentation patterns, i.e., coherent sets of code elements that are documented together, and that reports violations of these patterns as the code and the documentation evolve.</p><p>Previous work has successfully identified natural language text that is potentially important for a programmer using a given API type. Chhetri and Robillard <ref type="bibr" target="#b6">[7]</ref> categorized text fragments in API documentation based on whether they contain information that is indispensable, valuable, or neither, using word patterns. When we applied their patterns to content on Stack Overflow, we were not able to repeat their positive results in terms of precision and usefulness (see Section 4.3). Petrosyan et al. <ref type="bibr" target="#b21">[22]</ref> proposed an approach to discover tutorial sections that explain a given API type. They classified fragmented tutorial sections using supervised text classification based on linguistic and structural features and they were able to achieve high precision and recall on dif-ferent tutorials. Their work differs from ours in that we use Stack Overflow's meta data for SISE. In addition, unlike Petrosyan et al., we focus on the extraction of single sentences instead of entire documentation fragments.</p><p>Several researchers have focused on augmenting API documentation with code examples. For example, Kim et al. <ref type="bibr" target="#b16">[17]</ref> proposed a recommendation system that returns API documents embedded with code example summaries mined from the Web. Their evaluation results showed that the approach provides code examples with high precision and boosts programmer productivity. In a similar effort, Subramanian et al. <ref type="bibr" target="#b33">[34]</ref> introduced Baker, an iterative, deductive method for linking source code examples to API documentation. In contrast to these tools, SISE links natural language sentences from Stack Overflow to API documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSION AND FUTURE WORK</head><p>While the rise of social media and Q&amp;A sites such as Stack Overflow has resulted in a plethora of information for software developers available in many different formats on the Web, it can be difficult to determine where a particular piece of information is stored. In an effort to bring documentation from different sources together, we presented an evaluation of different techniques for extracting insight sentences from Stack Overflow. We define insight sentences as those sentences on Stack Overflow that are related to a particular API type and that provide insight not contained in the API documentation of the type.</p><p>In a comparative study with eight software developers to evaluate the meaningfulness and usefulness of the insight sentences, we found that our supervised approach (SISE) resulted in the highest number of sentences which were considered to add useful information not found in the API documentation. We conclude that considering the meta data available on Stack Overflow along with natural language characteristics can improve existing approaches when applied to Stack Overflow data.</p><p>We believe that we are the first to investigate augmenting natural language software documentation from one source with that from another source. Our work is a first step towards a vision of presenting users with combined documentation from various sources rather than users having to look through different sources to find a piece of information. We plan to extend this work beyond the Java API and we plan to experiment with more features that capture the grammatical structure of sentences on Stack Overflow. Determining whether a sentence is meaningful on its own is non-trivial, and while our evaluation showed that a supervised approach can detect such sentences based on part-of-speech tags with a higher precision than summarization or pattern-based approaches, we expect that the precision can further be improved with a deeper understanding of each sentence and its dependencies on other sentences or code snippets. In addition, we intend on applying the idea of insight sentence extraction to other textual artifacts produced by software developers, such as bug reports, commit messages, or code comments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>ICSE ' 16 ,</head><label>16</label><figDesc>May 14-22, 2016, Austin, TX, USA c 2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00 DOI: http://dx.doi.org/10.1145/2884781.2884800</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Stack Overflow Example meaningful on their own without their surrounding code snippets or the question that prompted a given answer.We also applied a pattern-based approach that has been successfully used to detect and recommend fragments of API documentation potentially important to a programmer who has already decided to use a certain API element<ref type="bibr" target="#b6">[7]</ref>, with similar results. We then developed a novel machine learning approach called SISE (Supervised Insight Sentence Extractor), which uses as features the sentences themselves, their formatting, their question, their answer, and their authors as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on our development set.1 In addition to the similarity of a sentence to its corresponding API documentation, characteristics of the user asking the question, the score and age of the answer, question characteristics, and the part-of-speech tags at the beginning of a sentence were among the features with the highest information gain.To answer the second research question on whether practitioners find these sentences useful, we conducted a comparative study in which we asked eight software developers to rate sentences extracted with all four approaches (LexRank, MMR, patterns, and SISE). These sentences were related to 20 Java API types. The study showed that sentences</figDesc><graphic coords="2,53.80,321.27,239.09,63.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screenshot of Java's Map documentation, with insight sentences produced by SISE</figDesc><graphic coords="9,53.80,53.80,502.12,232.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b28">[29]</ref> developed a contextaware IDE-based meta search engine for recommendations about programming errors and exceptions. The Stack Overflow API is one of the search APIs used in their work, and their approach captures the context in a similar fashion to the work by Cordeiro et al. In an evaluation, the authors found that the inclusion of different types of contextual information associated with an exception can enhance the accuracy of recommendations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Motivating Examples API type Stack Overflow sentence Stack Overflow id java.sql.DriverManager Normally you use DriverManager when you just want a connection 10868758 for one time while with DataSource you get other features such as connection pooling and distributed transactions.</figDesc><table><row><cell>java.net.URL</cell><cell>URLEncoder is meant for passing data as parameters, not for</cell><cell>724043</cell></row><row><cell></cell><cell>encoding the URL itself.</cell><cell></cell></row><row><cell>java.lang.Thread</cell><cell>join() waits for something meaningful while sleep() just sits there</cell><cell>4561951</cell></row><row><cell></cell><cell>doing nothing.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Regular Expressions for filtering Stack Overflow Threads question part pattern</figDesc><table><row><cell>body</cell><cell>.*(^|[a-z]+ |[\.!?] |[\(&lt;])TypeName([&gt;\)\.,!?$]| [a-z]+).* (non-qualified API type surrounded by lower case words or punctuation marks)</cell></row><row><cell>title or body</cell><cell></cell></row></table><note><p>(?i).*\bPackageName\.TypeName\b.* (fully-qualified API type, case-insensitive) body .*&lt;code&gt;.*\bTypeName\b.*&lt;/code&gt;.* (non-qualified API type in code) body .*&lt;a.*href.*PackageName/TypeName\.html.*&gt;.*&lt;/a&gt;.* (link to the official API documentation) title (?i).*\b(a |an )TypeName\b.* (non-qualified API type prefixed with "a" or "an", case-insensitive)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Three most commonly used types in the ten most commonly used Java packages. Types in bold were used for sampling sentences in the development set and the remaining types were used to sample sentences for the comparative study.</figDesc><table><row><cell>package</cell><cell>type</cell><cell></cell></row><row><cell>java.applet</cell><cell cols="3">Applet, AudioClip, AppletContext</cell></row><row><cell>java.awt</cell><cell cols="2">Event, Image, Component</cell></row><row><cell>java.beans</cell><cell cols="2">PropertyChangeListener,</cell></row><row><cell></cell><cell cols="2">PropertyChangeEvent,</cell></row><row><cell></cell><cell>PropertyDescriptor</cell><cell></cell></row><row><cell>java.io</cell><cell cols="2">File, Serializable, InputStream</cell></row><row><cell>java.lang</cell><cell cols="2">Object, String, Thread</cell></row><row><cell>java.net</cell><cell cols="2">URL, URLClassLoader, Socket</cell></row><row><cell cols="4">java.security AccessController, SecureClassLoader,</cell></row><row><cell></cell><cell>Principal</cell><cell></cell></row><row><cell>java.sql</cell><cell cols="2">Connection, DriverManager,</cell></row><row><cell></cell><cell>ResultSet</cell><cell></cell></row><row><cell>java.util</cell><cell cols="2">List, ArrayList, Map</cell></row><row><cell>javax.swing</cell><cell cols="2">JComponent, JPanel, JFrame</cell></row><row><cell cols="3">Table 4: Development Set</cell></row><row><cell>type</cell><cell cols="3">meaningful not meaningf.</cell></row><row><cell>ArrayList</cell><cell></cell><cell>58</cell><cell>235</cell></row><row><cell>AudioClip</cell><cell></cell><cell>7</cell><cell>45</cell></row><row><cell>DriverManager</cell><cell></cell><cell>14</cell><cell>68</cell></row><row><cell>Image</cell><cell></cell><cell>12</cell><cell>151</cell></row><row><cell>JPanel</cell><cell></cell><cell>8</cell><cell>129</cell></row><row><cell cols="2">PropertyChangeEvent</cell><cell>3</cell><cell>101</cell></row><row><cell cols="2">SecureClassLoader</cell><cell>3</cell><cell>62</cell></row><row><cell>Serializable</cell><cell></cell><cell>48</cell><cell>111</cell></row><row><cell>String</cell><cell></cell><cell>35</cell><cell>437</cell></row><row><cell cols="2">URLClassLoader</cell><cell>8</cell><cell>39</cell></row><row><cell>sum</cell><cell cols="2">196</cell><cell>1,378</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Precision and Coverage for different</figDesc><table><row><cell>LexRank Configurations</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">precision coverage</cell></row><row><cell>only first sentence</cell><cell>0.40</cell><cell>1.0</cell></row><row><cell>first two sentences</cell><cell>0.25</cell><cell>1.0</cell></row><row><cell>first three sentences</cell><cell>0.23</cell><cell>1.0</cell></row><row><cell>first four sentences</cell><cell>0.23</cell><cell>1.0</cell></row><row><cell>first five sentences</cell><cell>0.24</cell><cell>1.0</cell></row><row><cell>score at least 0.005</cell><cell>0.14</cell><cell>0.9</cell></row><row><cell>score at least 0.010</cell><cell>0.13</cell><cell>0.8</cell></row><row><cell>score at least 0.015</cell><cell>0.16</cell><cell>0.4</cell></row><row><cell>score at least 0.020</cell><cell>0.21</cell><cell>0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Precision and Coverage for different MMR</figDesc><table><row><cell>Configurations</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">precision coverage</cell></row><row><cell>only first sentence</cell><cell>0.20</cell><cell>1.0</cell></row><row><cell>first two sentences</cell><cell>0.10</cell><cell>1.0</cell></row><row><cell>first three sentences</cell><cell>0.10</cell><cell>1.0</cell></row><row><cell>first four sentences</cell><cell>0.15</cell><cell>1.0</cell></row><row><cell>first five sentences</cell><cell>0.16</cell><cell>1.0</cell></row><row><cell>score at least 0.005</cell><cell>0.13</cell><cell>0.8</cell></row><row><cell>score at least 0.010</cell><cell>0.14</cell><cell>0.5</cell></row><row><cell>score at least 0.015</cell><cell>0.11</cell><cell>0.3</cell></row><row><cell>score at least 0.020</cell><cell>0.14</cell><cell>0.1</cell></row><row><cell cols="3">good threshold for eigenvector centrality is. Thus, we ex-</cell></row><row><cell cols="3">perimented with different settings and evaluated the perfor-</cell></row><row><cell cols="3">mance of different configurations on our development set.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Features</figDesc><table><row><cell>used for SISE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Features with highest Information Gain</figDesc><table><row><cell># feature</cell></row><row><cell>1 cosine similarity between sentence and most similar</cell></row><row><cell>sentence in API documentation</cell></row><row><cell>2 average cosine similarity between sentence and all</cell></row><row><cell>sentences in API documentation</cell></row><row><cell>3 question user acceptance rate</cell></row><row><cell>4 answer score</cell></row><row><cell>5 answer age</cell></row><row><cell>6 answer time difference to question</cell></row><row><cell>7 question score</cell></row><row><cell>8 question favorites</cell></row><row><cell>9 question user reputation</cell></row><row><cell>10 question views</cell></row><row><cell>11 number of nouns followed by verb in present tense,</cell></row><row><cell>third person singular in sentence</cell></row><row><cell>12 question age</cell></row><row><cell>13 sentence starts with noun followed by verb in</cell></row><row><cell>present tense, third person singular</cell></row><row><cell>14 number of tokens in sentence</cell></row><row><cell>15 position of API element in sentence (or 0)</cell></row><row><cell>16 number of occurrences of API element in sentence</cell></row><row><cell>17 answer score</cell></row><row><cell>18 answer size</cell></row><row><cell>19 number of nouns in sentence</cell></row><row><cell>20 sentence starts with noun</cell></row><row><cell>21 number of characters that are code</cell></row><row><cell>22 number</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Sentences in the Comparative Study Lex MMR KP SISE (unique)</figDesc><table><row><cell>Applet</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>(3/4)</cell></row><row><cell>AppletCont.</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>(4/5)</cell></row><row><cell>Event</cell><cell>1</cell><cell>1</cell><cell>6</cell><cell>0</cell><cell>(8/8)</cell></row><row><cell>Component</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>7</cell><cell>(11/12)</cell></row><row><cell>PropertyCh.</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>(6/6)</cell></row><row><cell>File</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>4</cell><cell>(10/10)</cell></row><row><cell>PropertyD.</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>(2/3)</cell></row><row><cell>Object</cell><cell>1</cell><cell>1</cell><cell>12</cell><cell>4</cell><cell>(18/18)</cell></row><row><cell>InputStream</cell><cell>1</cell><cell>1</cell><cell>7</cell><cell>4</cell><cell>(13/13)</cell></row><row><cell>URL</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>2</cell><cell>(8/8)</cell></row><row><cell>Thread</cell><cell>1</cell><cell>1</cell><cell>11</cell><cell>5</cell><cell>(18/18)</cell></row><row><cell>AccessContr.</cell><cell>1</cell><cell>1</cell><cell>7</cell><cell>2</cell><cell>(10/11)</cell></row><row><cell>Socket</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>(6/6)</cell></row><row><cell>Connection</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>1</cell><cell>(5/6)</cell></row><row><cell>Principal</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>1</cell><cell>(7/7)</cell></row><row><cell>List</cell><cell>1</cell><cell>1</cell><cell>11</cell><cell>15</cell><cell>(28/28)</cell></row><row><cell>ResultSet</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>0</cell><cell>(5/6)</cell></row><row><cell>JComponent</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>(3/3)</cell></row><row><cell>Map</cell><cell>1</cell><cell>1</cell><cell>21</cell><cell>22</cell><cell>(43/45)</cell></row><row><cell>JFrame</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>0</cell><cell>(5/5)</cell></row><row><cell>sum</cell><cell>20</cell><cell cols="2">20 109</cell><cell cols="2">73 (213/222)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Participants in the Study</figDesc><table><row><cell>job title</cell><cell></cell><cell>exp.</cell><cell cols="2">area</cell></row><row><cell cols="3">Technical Consultant 4 years</cell><cell cols="3">automation, mobile</cell></row><row><cell>Software Engineer</cell><cell></cell><cell>4 years</cell><cell>web</cell><cell></cell></row><row><cell cols="2">Research Assistant</cell><cell cols="4">10+ years embedded, system</cell></row><row><cell cols="2">Postdoc Researcher</cell><cell cols="3">10+ years web, systems</cell></row><row><cell>Software Engineer</cell><cell></cell><cell>5 years</cell><cell cols="3">data engineering</cell></row><row><cell>Student</cell><cell></cell><cell>2 years</cell><cell>web</cell><cell></cell></row><row><cell cols="2">Android Developer</cell><cell>3 years</cell><cell cols="2">mobile</cell></row><row><cell cols="2">Software Developer</cell><cell>3 years</cell><cell cols="2">web, systems</cell></row><row><cell cols="5">Table 11: Comparative Study Results</cell></row><row><cell cols="2">meaningf.,</cell><cell>meaningf.,</cell><cell></cell><cell>more</cell><cell>no</cell></row><row><cell cols="6">added inf. no added inf. context sense</cell></row><row><cell>Lex</cell><cell>9</cell><cell></cell><cell>5</cell><cell>15</cell><cell>11</cell></row><row><cell cols="2">22.5%</cell><cell cols="2">12.5%</cell><cell cols="2">37.5% 27.5%</cell></row><row><cell>MMR</cell><cell>13</cell><cell></cell><cell>4</cell><cell>17</cell><cell>6</cell></row><row><cell cols="2">32.5%</cell><cell cols="2">10.0%</cell><cell cols="2">42.5% 15.0%</cell></row><row><cell>KP</cell><cell>34</cell><cell cols="2">21</cell><cell>46</cell><cell>23</cell></row><row><cell cols="2">27.4%</cell><cell cols="2">16.9%</cell><cell cols="2">37.1% 18.6%</cell></row><row><cell>SISE</cell><cell>38</cell><cell cols="2">18</cell><cell>12</cell><cell>12</cell></row><row><cell cols="2">47.5%</cell><cell cols="2">22.5%</cell><cell cols="2">15.0% 15.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table><row><cell cols="4">Inter-rater Agreement</cell><cell></cell></row><row><cell></cell><cell cols="4">(1) (2) (3) (4)</cell></row><row><cell>(1) meaningf., added inf.</cell><cell cols="3">27 12 16</cell><cell>8</cell></row><row><cell>(2) meaningf., no added inf.</cell><cell>-</cell><cell cols="2">8 12</cell><cell>6</cell></row><row><cell>(3) req. more context</cell><cell>-</cell><cell cols="3">-17 22</cell></row><row><cell>(4) no sense</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The complete coding guide is available in our online appendix at http://cs.mcgill.ca/˜swevo/insight/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Using other prepositions, such as "by", does not significantly change the results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p><ref type="bibr" target="#b3">4</ref> The regular expressions are available in our online appendix at http://cs.mcgill.ca/˜swevo/insight</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>/. 5 similarity(A, B) = |A ∩ B|/( |A| × |B|), where A and B are the tokens of the respective</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>sentence.<ref type="bibr" target="#b5">6</ref> We implemented our own version of LexRank to be able to modify it if needed. However, all results in this paper are based on an unmodified implementation following Erkan and Radev<ref type="bibr" target="#b12">[13]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>This variation is explained by the length of the Stack Overflow threads linked to each API type. Devising algorithms for ranking sentences is part of our future work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the study participants. This research was funded by NECSIS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instance-based learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="37" to="66" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Harnessing Stack Overflow for the IDE</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bacchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ponzanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Int&apos;l. Workshop on Recommendation Systems for Software Engineering</title>
		<meeting>of the 3rd Int&apos;l. Workshop on Recommendation Systems for Software Engineering</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="26" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Task-driven software summarization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Binkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hebig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Keszocze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slankas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 29th Int&apos;l. Conf. on Software Maintenance</title>
		<meeting>of the 29th Int&apos;l. Conf. on Software Maintenance</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="432" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A scalable MMR approach to sentence scoring for multi-document update summarization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Bèze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Torres-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Int&apos;l. Conf. on Computational Linguistics (Companion volume: Posters and Demonstrations)</title>
		<meeting>of the 22nd Int&apos;l. Conf. on Computational Linguistics (Companion volume: Posters and Demonstrations)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Who asked what: Integrating crowdsourced FAQs into API documentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proc. of the 36th Int&apos;l. Conf. on Software Engineering</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="456" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recommending reference API documentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Chhetri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1558" to="1586" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context-based recommendation to support problem solving in software development</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Int&apos;l. Workshop on Recommendation Systems for Software Engineering</title>
		<meeting>of the 3rd Int&apos;l. Workshop on Recommendation Systems for Software Engineering</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using traceability links to recommend adaptive changes for documentation evolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dagenais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1126" to="1146" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ranking crowd knowledge to assist software development</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B L</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D A</forename><surname>Maia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Int&apos;l. Conf. on Program Comprehension</title>
		<meeting>of the 22nd Int&apos;l. Conf. on Program Comprehension</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="72" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving API documentation usability with knowledge pushing</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Herbsleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 31st Int&apos;l. Conf. on Software Engineering</title>
		<meeting>of the 31st Int&apos;l. Conf. on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Apatite: A new interface for exploring APIs</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stylos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Human Factors in Computing Systems</title>
		<meeting>of the Conf. on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1331" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An extensive empirical study of feature selection metrics for text classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1289" to="1305" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<title level="m">The WEKA data mining software: An update. SIGKDD Explorations</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating continuous distributions in bayesian classifiers</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conf. on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="338" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enriching documents with examples: A corpus mining approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Information Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Patterns of knowledge in API reference documentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1264" to="1282" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inferring method specifications from natural language API descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pandita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paradkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 34th Int&apos;l. Conf. on Software Engineering</title>
		<meeting>of the 34th Int&apos;l. Conf. on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="815" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring API documentation on the web</title>
		<author>
			<persName><forename type="first">C</forename><surname>Parnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Treude</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Int&apos;l. Workshop on Web 2.0 for Software Engineering</title>
		<meeting>of the 2nd Int&apos;l. Workshop on Web 2.0 for Software Engineering</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering information explaining API types using text classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Petrosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37th Int&apos;l. Conf. on Software Engineering</title>
		<meeting>of the 37th Int&apos;l. Conf. on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="869" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating the use of different positional strategies for sentence selection in biomedical literature summarization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>De Albornoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">71</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leveraging crowd knowledge for software comprehension and development</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ponzanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bacchelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th European Conf. on Software Maintenance and Reengineering</title>
		<meeting>of the 17th European Conf. on Software Maintenance and Reengineering</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining StackOverflow to turn the IDE into a self-confident programming prompter</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ponzanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Working Conf. on Mining Software Repositories</title>
		<meeting>of the 11th Working Conf. on Mining Software Repositories</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Snowball: A language for stemming algorithms</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<ptr target="http://snowball.tartarus.org/texts/introduction.html" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards a context-aware IDE-based meta search engine for recommendation about programming errors and exceptions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeasmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Software Evolution Week: Conf. on Software Maintenance, Reengineering and Reverse Engineering</title>
		<meeting>of the Software Evolution Week: Conf. on Software Maintenance, Reengineering and Reverse Engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering essential code elements in informal documentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Rigby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 35th Int&apos;l. Conf. on Software Engineering</title>
		<meeting>of the 35th Int&apos;l. Conf. on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="832" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What makes APIs hard to learn? Answers from developers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A field study of API learning obstacles</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="703" to="732" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jadeite: Improving API documentation using usage information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stylos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="4429" to="4434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Live API documentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Inozemtseva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 36th Int&apos;l. Conf. on Software Engineering</title>
		<meeting>of the 36th Int&apos;l. Conf. on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="643" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How do programmers ask and answer questions on the web? (NIER track)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Treude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Storey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd Int&apos;l. Conf. on Software Engineering</title>
		<meeting>of the 33rd Int&apos;l. Conf. on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Extracting development tasks to navigate software documentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Treude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dagenais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="565" to="581" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TaskNav: Task-based navigation of software documentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Treude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sicard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Robillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 37th Int&apos;l. Conf. on Software Engineering</title>
		<meeting>of the 37th Int&apos;l. Conf. on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective communication of software development knowledge through community portals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Treude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Storey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th joint meeting of the European Software Engineering Conf. and the Symp. on the Foundations of Software Engineering</title>
		<meeting>of the 8th joint meeting of the European Software Engineering Conf. and the Symp. on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Autocomment: Mining question and answer sites for automatic comment generation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th Int&apos;l. Conf. on Automated Software Engineering</title>
		<meeting>of the 28th Int&apos;l. Conf. on Automated Software Engineering</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th Int&apos;l. Conf. on Machine Learning</title>
		<meeting>of the 14th Int&apos;l. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detecting API documentation errors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int&apos;l. Conf. on Object Oriented Programming Systems Languages and Applications</title>
		<meeting>of the Int&apos;l. Conf. on Object Oriented Programming Systems Languages and Applications</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="803" to="816" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
