<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-07">7 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
								<address>
									<addrLine>2 Peking University 3 MIT</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-07">7 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2009.03294v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Normalization plays an important role in the optimization of deep neural networks. While there are standard normalization methods in computer vision and natural language processing, there is limited understanding of how to effectively normalize neural networks for graph representation learning. In this paper, we propose a principled normalization method, Graph Normalization (GraphNorm), where the key idea is to normalize the feature values across all nodes for each individual graph with a learnable shift. Theoretically, we show that GraphNorm serves as a preconditioner that smooths the distribution of the graph aggregation's spectrum, leading to faster optimization. Such an improvement cannot be well obtained if we use currently popular normalization methods, such as BatchNorm, which normalizes the nodes in a batch rather than in individual graphs, due to heavy batch noises. Moreover, we show that for some highly regular graphs, the mean of the feature values contains graph structural information, and directly subtracting the mean may lead to an expressiveness degradation. The learnable shift in GraphNorm enables the model to learn to avoid such degradation for those cases. Empirically, Graph neural networks (GNNs) with GraphNorm converge much faster compared to GNNs with other normalization methods, e.g., BatchNorm. GraphNorm also improves generalization of GNNs, achieving better performance on graph classification benchmarks. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there has been a surge of interest in Graph Neural Networks (GNNs) for learning with graph-structured data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>. GNNs learn node and graph features by following a neighbor aggregation (or message passing) scheme <ref type="bibr" target="#b9">[10]</ref>, where node features are recursively aggregated from their neighbours. One major theme of existing works is the design of GNN architecture variants, e.g., neighbor aggregation modules, that learn good graph representations <ref type="bibr" target="#b36">[37]</ref>. To that end, many theoretical aspects of GNNs have been studied, including their representation power <ref type="bibr" target="#b36">[37]</ref>, generalization ability <ref type="bibr" target="#b38">[39]</ref>, and infinite-width asymptotic behavior <ref type="bibr" target="#b6">[7]</ref>. These theoretical understandings lead to GNN architectures that enjoy good representation power and generalization. However, an important problem remains: the optimization of GNNs is often unstable, and the convergence is slow <ref type="bibr" target="#b36">[37]</ref>. This raises the question:</p><p>Can we provably improve the optimization for GNNs?</p><p>We give an affirmative answer. Specifically, we study the optimization of GNNs through the lens of normalization. Feature normalization is an orthogonal direction to feature aggregation or architecture design, and it has been shown crucial when a neural network gets deeper, wider, and more sophisticated <ref type="bibr" target="#b11">[12]</ref>. Normalization methods that shift and scale feature values have proven to help the optimization of deep neural networks. Curiously, different domains require specialized normalization methods. In computer vision, batch normalization <ref type="bibr" target="#b15">[16]</ref> is a standard component. While in natural language processing (NLP), layer normalization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> is more popularly used. Empirically, common normalization methods from other domains, e.g., BatchNorm and LayerNorm, often lead to unsatisfactory performance when applied to GNNs. Theoretically, there is limited understanding of what kind of normalization provably helps optimization of GNNs.</p><p>In this paper, we propose a theoretically motivated normalization method for GNNs, Graph Normalization (GraphNorm). GraphNorm normalizes the feature values across all nodes in each individual graph with a learnable shift. We derive GraphNorm from understanding how different components or steps of a normalization method influence the optimization (Figure <ref type="figure" target="#fig_0">1</ref>). In particular, we identify the importance of appropriate shift steps for GNNs, an under-explored topic in normalization methods for other domains.</p><p>First, we show that applying normalization to each individual graph instead of to the whole mini-batch, is beneficial according to a theoretical understanding of the shift operation. We prove that when applying the normalization to each individual graph, the shift operation (Step 1a in Figure <ref type="figure" target="#fig_0">1</ref>) serves as a preconditioner of the graph aggregation operation (Theorem 3.1). Empirically, the preconditioning makes the optimization curvature smoother and makes the training more efficient (Figure <ref type="figure">2</ref>). Such an improvement cannot be well achieved if we apply the normalization across graphs in a batch, i.e., using BatchNorm. This is because the variation of the batch-level statistics on graph data is much larger (Figure <ref type="figure" target="#fig_1">3</ref>). Therefore using noisy statistics during training may make the optimization even more unstable.</p><p>Second, we show that the standard shift that simply subtracts the mean of feature values may lead to an expressiveness degradation. Specifically, we prove that for some highly regular graphs, the mean statistics of feature values contains graph structural information which may be crucial for classification (Proposition 4.1 and 4.2). Therefore, directly removing them from the features will consequently hurt the performance (Figure <ref type="figure" target="#fig_4">5</ref>). Based on this analysis, we propose the learnable shift (Step 2 in Figure <ref type="figure" target="#fig_0">1</ref>) to control how much information in mean statistics to preserve. Together, our proposed GraphNorm normalizes the feature values further scaling to unit norm enjoys "scale-invariant" property <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. In comparison, BatchNorm in the lower branch suffers from heavy batch noise. Overall, GraphNorm significantly surpasses BatchNorm in training speed (Figure <ref type="figure" target="#fig_2">4</ref>) and enjoys good generalization performance (Table <ref type="table" target="#tab_1">1</ref>). across nodes in each graph using a learnable shift to avoid expressiveness degradation and enjoy effective optimization.</p><p>To validate the effectiveness of GraphNorm, we conduct extensive experiments on eight popular graph classification benchmarks. Empirical results confirm that GraphNorm consistently improves the optimization for GNNs, e.g., convergence speed and stability of training, by a large margin compared to BatchNorm (Figure <ref type="figure" target="#fig_2">4</ref>). Furthermore, GraphNorm helps GNNs achieve better generalization performance on most benchmarks (Table <ref type="table" target="#tab_1">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Normalization is important in optimizing deep neural networks, and different normalization techniques have been proposed to improve the training process in different applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. The reason behind the effectiveness of normalization has been intensively studied. Most of the works focus on the "scale-invariant" property: by using a normalization layer right after a linear (or convolutional) layer, the output values will not change when the weights of the parameters in the layer are scaled. Using this property, <ref type="bibr" target="#b19">[20]</ref> suggests that normalization decouples the optimization of direction and length of the parameters; <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> show that the normalization implicitly tunes the learning rate. <ref type="bibr" target="#b27">[28]</ref> reveals that normalization smooths the optimization landscape. The "scale-invariant" property is a consequence of the scaling operation in normalization. However, the effect of the shift operation remains highly unexplored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we introduce the notations and the basics of GNNs. Let G = (V, E) denote a graph where V = {v 1 , v 2 , • • • , v n }, n is the number of nodes. Let the feature vector of node v i be X i . We denote the adjacency matrix of a graph as A ∈ R n×n with A ij = 1 if (v i , v j ) ∈ E and 0 otherwise. The degree matrix associated with A is defined as D = diag (d 1 , d 2 , . . . , d n ) where d i = n j=1 A ij . Graph Neural Networks. GNNs use the graph structure and node features to learn the representations of nodes and graphs. Modern GNNs follow a neighborhood aggregation strategy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, where the representation of a node is iteratively updated by aggregating the representation of its neighbors. To be concrete, we denote h (k) i as the representation of v i at the k-th layer and define h (0) i = X i . We use AGGREGATE to denote the aggregation function in the k-th layer. Formally,</p><formula xml:id="formula_0">h (k) i = AGGREGATE (k) h (k−1) i , h (k−1) j : v j ∈ N (v i ) , i = 1, 2, • • • , n,<label>(1)</label></formula><p>where N (v i ) is the set of nodes adjacent to v i . Different graph neural networks can be obtained by choosing different AGGREGATE functions. We introduce two popularly used networks in detail, Graph Convolutional Networks (GCN) <ref type="bibr" target="#b18">[19]</ref> and Graph Isomorphism Network (GIN) <ref type="bibr" target="#b36">[37]</ref>. In GCN, the AGGREGATE function is defined as:</p><formula xml:id="formula_1">h (k) i = ReLU W (k) • MEAN h (k−1) j , ∀v j ∈ N (v i ) ∪ {v i } ,<label>(2)</label></formula><p>where MEAN denotes the average pooling operation over each feature dimension and W (k) is the parameter matrix in layer k. Taking the matrix form, Eq. 2 can be rewritten as</p><formula xml:id="formula_2">H (k) = ReLU W (k) H (k−1) Q GCN ,<label>(3)</label></formula><p>where k) denotes the feature dimension at the k-th</p><formula xml:id="formula_3">H (k) = h (k) 1 , h (k) 2 , • • • , h (k) n ∈ R d (k) ×n , d<label>(</label></formula><formula xml:id="formula_4">layer. Q GCN = D− 1 2 Â D− 1 2</formula><p>, where Â = A + I n and D is the degree matrix of Â. I n is the identity matrix.</p><p>In GIN, the AGGREGATE function is defined as</p><formula xml:id="formula_5">h (k) i = MLP (k)   W (k)   1 + ξ (k) • h (k−1) i + v j ∈N (v i ) h (k−1) j     ,<label>(4)</label></formula><p>which in matrix form is</p><formula xml:id="formula_6">H (k) = MLP (k) W (k) H (k−1) Q GIN ,<label>(5)</label></formula><p>where ξ (k) is a learnable parameter and</p><formula xml:id="formula_7">Q GIN = A + I n + ξ (k) I n .</formula><p>For a K-layer GNN, the outputs of the final layer, i.e., h </p><formula xml:id="formula_8">(K) i ,i = 1, • • • , n,</formula><formula xml:id="formula_9">σ 2 = 1 m m i=1 (x i − µ) 2 .</formula><p>The major difference among different existing normalization methods is which set of feature values the normalization is applied to. For example, in computer vision, BatchNorm <ref type="bibr" target="#b15">[16]</ref> is the de facto method that normalizes the feature values in the same channel across different samples in the batch. In NLP, LayerNorm <ref type="bibr" target="#b3">[4]</ref> is more popularly used, which normalizes the feature values at each position in a sequence separately. In GNN literature, as the aggregation function is similar to the convolutional operation, BatchNorm is usually used. <ref type="bibr" target="#b36">[37]</ref> uses BatchNorm in the GIN model, where the BatchNorm is applied to all values in the same feature dimension across the nodes of all graphs in the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Understanding Normalization for GNNs</head><p>In this section, we start from analyzing why and how normalization can help the optimization procedure of GNNs, and then use such a theoretical understanding to develop GraphNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Advantage of the Shift in Normalization</head><p>As mentioned previously, the scale-invariant property of the normalization has been investigated and considered as one of the ingredients that make the optimization efficient. However, as far as we know, the effectiveness of the shift is not well understood. Compared to the image and sequential data, the graph is explicitly structured, and the neural networks exploit the structural information directly in the aggregation of the neighbours, see Eq. <ref type="bibr" target="#b0">(1)</ref>. Such uniqueness of GNNs makes it possible to study how the shift operation interplays with the graph data in detail. We first consider the following general GNN structure equipped with a normalization layer:</p><formula xml:id="formula_10">H (k) = F (k) Norm W (k) H (k−1) Q ,<label>(6)</label></formula><p>where F (k) is a function that applies to each node separately, Q is an n×n matrix representing the neighbor aggregation, and W (k) is the weight/parameter matrix in layer k. We apply the normalization after the linear transformation as in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. We can instantiate Eq. ( <ref type="formula" target="#formula_10">6</ref>) as GCN and GIN, by setting proper F (k) and matrix Q. For example, if we set F (k) to be ReLU and set Q to be Q GCN (Eq. ( <ref type="formula" target="#formula_2">3</ref>)), then Eq. ( <ref type="formula" target="#formula_10">6</ref>) becomes GCN with normalization; Similarly, by setting F (k) to be MLP (k) and Q to be Q GIN (Eq. ( <ref type="formula" target="#formula_6">5</ref>)), we recover GIN with normalization. We are interested in how this normalization layer affects the optimization of graph neural networks. Towards this goal, we first consider applying the normalization over each individual graph separately. Mathematically, for a graph of n nodes, denote N = I n −<ref type="foot" target="#foot_0">1</ref> n 11 . N is the matrix form of the shift operation, i.e., for any vector z</p><formula xml:id="formula_11">= [z 1 , z 2 , • • • , z n ] ∈ R n , z N = z − 1 n n i=1 z i 1 .</formula><p>Then the normalization together with the aggregation can be represented as</p><formula xml:id="formula_12">1 Norm W (k) H (k−1) Q = S W (k) H (k−1) Q N,<label>(7)</label></formula><p>where</p><formula xml:id="formula_13">S = diag 1 σ 1 , 1 σ 2 , • • • , 1 σ d (k)</formula><p>is the scaling. Each σ i is the standard deviation of the values of the i-th features among the nodes in the graph we consider. We can see that, in matrix form, shifting feature values on a single graph is equivalent to multiplying N as in Eq. ( <ref type="formula" target="#formula_12">7</ref>). Therefore, we further check how this operation affects optimization. In particular, we examine the singular value distribution of QN . The following theorem shows that QN has a smoother singular value distribution than Q, i.e., N serves as a preconditioner of Q. Theorem 3.1 (Shift Serves as a Preconditioner of Q). Let Q, N be defined as in Eq. ( <ref type="formula" target="#formula_12">7</ref>), 0 ≤ λ 1 ≤ • • • ≤ λ n be the singular values of Q. We have µ n = 0 is one of the singular values of QN , and let other singular values of QN be 0</p><formula xml:id="formula_14">≤ µ 1 ≤ µ 2 ≤ • • • ≤ µ n−1 . Then we have λ 1 ≤ µ 1 ≤ λ 2 ≤ • • • ≤ λ n−1 ≤ µ n−1 ≤ λ n ,<label>(8)</label></formula><p>where λ i = µ i or λ i = µ i−1 only if there exists one of the right singular vectors α i of Q associated with λ i satisfying 1 α i = 0.</p><p>Classic wisdom in optimization shows that preconditioning can accelerate the convergence of iterative methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, and similar ideas are also used to accelerate the optimization of deep neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. In the case of optimizing the weight matrix W (k) , we can see from Eq. ( <ref type="formula" target="#formula_12">7</ref>) that after applying normalization, the term Q in the gradient of W (k) will become QN which makes the optimization curvature of W (k) smoother, see Appendix A.4 for more discussions.</p><p>To check how much the matrix N improves the distribution of the spectrum of matrix Q in real practice, we sample graphs from different datasets for illustration, as showed in Figure <ref type="figure">2</ref> (more visualizations for different types of graph can be found in Appendix D.1). We can see that the singular value distribution of QN is much smoother, and the condition number is improved. Note that for a multi-layer GNN, the normalization will be applied in each layer. Therefore, the overall improvement of such preconditioning can be more significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Disadvantages of Batch Normalization for Graph</head><p>The above analysis shows the benefits of using normalization on the nodes in a single graph. Then a natural question is whether using a batch-level normalization for GNNs <ref type="bibr" target="#b36">[37]</ref> can lead to similar advantages. In batch normalization (BatchNorm), the mean and standard deviation in a sampled batch are random variables which try to provide accurate estimations for the mean and standard deviation over the whole dataset <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32]</ref>. During testing, the estimated dataset-level statistics are used instead of the batch-level statistics <ref type="bibr" target="#b15">[16]</ref>.</p><p>In GNNs, for each feature dimension, the BatchNorm normalizes the feature values of the dimension over all nodes across different graphs in the batch. Note that one can view all graphs in the dataset as isolated subgraphs in a super graph. If the batch-level statistics are well-concentrated around dataset-level statistics, we can use Eq. ( <ref type="formula" target="#formula_12">7</ref>) to this super graph, and thus Theorem 3.1 can be applied. Then BatchNorm can be considered as normalizing isolated parts in the super graph, which will enjoy the preconditioning in the theorem. However, the concentration of batch-level statistics is heavily domain-specific. <ref type="bibr" target="#b28">[29]</ref> find that in computer vision, the variation of batch-level statistics in typical networks is quite small while in natural language processing, this variation is large. In GNNs, how the batch-level statistics are concentrated is still unknown. If those values poorly concentrate around the dataset-level statistics, we cannot expect the preconditioning property of the shift operation holds for batch normalization.</p><p>To study this, we train a 5-layer GIN with BatchNorm as in <ref type="bibr" target="#b36">[37]</ref> on the PROTEINS dataset and train a ResNet18 <ref type="bibr" target="#b11">[12]</ref> on the CIFAR10 dataset for comparison. The batch size of all experiments are set to 128. For each model checkpoint, we record the maximum/minimum batch-level statistics (mean and standard deviation) for the first (layer 0) and the last (layer 3) BatchNorm layer on a randomly picked dimension across different batches. We also calculate the dataset-level statistics. In Figure <ref type="figure" target="#fig_1">3</ref>, pink line denotes the dataset-level statistics, and green/blue line denotes the maximum/minimum value of the batch-level statistics respectively. We observe that for image tasks, the batch-level statistics well concentrate around the dataset-level statistics during training. On the contrary, on the graph tasks, the variation of batch-level statistics is rather large. We hypothesize this is due to that the graph structure is quite different between each other and the statistics of a batch is hard to reflect the statistics of the whole dataset. Such heavy noise brings instabilities to the optimization when using BatchNorm, and the preconditioning property also may not hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph Normalization</head><p>Although we provide evidence on the indispensability and advantages to apply the normalization in a graph-wise manner, simply normalizing the values in each feature dimension within a graph does not consistently lead to improvement. We show that in some situations, e.g., for regular graphs, the standard shift (e.g., shifting by subtracting the mean) may cause information loss on graph structures. We also show in the experimental section that some graphs in real-world datasets are highly regular.</p><p>We consider r-regular graphs, i.e., each node has a degree r. We first look into the case that there are no available node features, then X i is set to be the one-hot encoding of the node degree <ref type="bibr" target="#b36">[37]</ref>. In a r-regular graph, all nodes have the same encoding, and thus the columns of H (0) are the same. We study the output of the standard shift operation in the first layer, i.e., k = 1 in Eq. <ref type="bibr" target="#b6">(7)</ref>. From the following proposition, we can see that when the standard shift operation is applied to GIN for a r-regular graph described above, the information of degree is lost: Proposition 4.1. For a r-regular graph with features described above, we have for GIN, Norm W (1) H (0) Q GIN = S W (1) H (0) Q GIN N = 0, i.e., the output of normalization layer is a zero matrix without any information of the graph structure. Such information loss not only happens when there are no node features. For complete graphs, we can further show that even each node has different features, the graph structural information, i.e., adjacency matrix A, will always be ignored after the standard shift operation in GIN: Proposition 4.2. For a complete graph (r = n − 1), we have for GIN, Q GIN N = ξ (k) N , i.e., graph structural information in Q will be removed after multiplying N .</p><p>The proof of these two propositions can be found in Appendix A. Similar results can be easily derived for other architectures like GCN.</p><p>As we can see from the above analysis, in graph data, the mean statistics after the aggregation sometimes contain structural information. Discarding the mean will degrade the expressiveness of the neural networks. Note that the problem may not happen in image domain. The mean statistics of image data contains global information such as brightness. Removing such information in images will not change the semantics of the objects and thus will not hurt the classification performance.  Graph-agnostic MLP <ref type="bibr" target="#b14">[15]</ref> 68.19 ± 0.71 GCN <ref type="bibr" target="#b14">[15]</ref> 76.06 ± 0.97 GIN <ref type="bibr" target="#b14">[15]</ref> 75 This analysis inspires us to modify the current normalization method with a learnable parameter to automatically control how much the mean to preserve in the shift operation. Combined with the graph-wise normalization, we call our new method GraphNorm. For each graph G, we generally denote value ĥi,j as the inputs to GraphNorm, e.g., the j-th feature value of node</p><formula xml:id="formula_15">v i , i = 1, • • • , n, j = 1, • • • , d. GraphNorm takes the following form: GraphNorm ĥi,j = γ j ĥi,j − α j • µ j σj + β j ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_16">µ j = 1 n n i=1 ĥi,j , σ2 j = 1 n n i=1 ĥi,j − α j • µ j 2</formula><p>, and γ j , β j are the affine parameters as in other normalization methods. By introducing the learnable parameter α j for each feature dimension j, we are able to learn how much the information we need to keep in the mean. In Section 5.3, we show that using this parameter consistently boosts the convergence speed, and makes a significant improvement on the datasets consisting of "regular" graphs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on different scales of graph classification benchmark datasets to show the training efficiency and the generalization ability of using GraphNorm. Ablation study is also provided to show how the design choices in GraphNorm affect the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>We use eight popularly used benchmark datasets of different scales in the experiments <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref>, including four medium-scale bioinformatics datasets (MUTAG, PTC, PROTEINS, NCI1), three medium-scale social network datasets (IMDB-BINARY, COLLAB, REDDIT-BINARY), and one large-scale bioinformatics dataset ogbg-molhiv, which is recently released on Open Graph Benchmark (OGB). Dataset statistics are summarized in Table <ref type="table" target="#tab_1">1</ref>. We evaluate our proposed GraphNorm on two typical graph neural networks GIN <ref type="bibr" target="#b36">[37]</ref> and GCN <ref type="bibr" target="#b18">[19]</ref> and compare it with BatchNorm<ref type="foot" target="#foot_1">2</ref> .. Specifically, we use a five-layer GCN/GIN. For GIN, the number of sub-layers in MLP is set to 2. Normalization is applied to each layer. To aggregate global features on top of the network, we use SUM readout for MUTAG, PTC, PROTEINS and NCI1 datasets, and use MEAN readout for other datasets, as in <ref type="bibr" target="#b36">[37]</ref>. Details of the experimental settings are presented in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We plot the training curve of GIN/GCN with GraphNorm and BatchNorm on different tasks in Figure <ref type="figure" target="#fig_2">4</ref>. First, from the curve, we can see that GraphNorm is significantly better than BatchNorm in terms of the convergence speed. For example, GIN/GCN with GraphNorm converges in roughly 5000/500 iterations on NCI1 and PTC datasets, while the two models using BatchNorm does not even converge in 10000/1000 iterations. Second, the majority of modern deep learning models are shown to be able to interpolate the data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr">41</ref>]. But we found that GIN and GCN with BatchNorm are slow to fit the training set well, and the training performance is not very stable, which may due to the large noise induced by the batch-level statistics. However, when using GraphNorm, in most datasets, the model can fit the training data easily.</p><p>Besides the training performance, we report the test (validation) accuracy on the datasets in Table <ref type="table" target="#tab_1">1</ref>. From the table, we can see that by using GraphNorm, we can achieve better performance on five tasks, which shows that better optimization leads to better test performance. On the large-scale ogbg-molhiv dataset, the improvements are more impressive. We achieve state-of-the-art performance, and GraphNorm is 2.1/1.1 points better than BatchNorm with GCN/GIN, respectively.</p><p>As a summary, the experimental results show that using GraphNorm is a better choice for GNNs in terms of both optimization and generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>As mentioned in Section 4, the mean statistics of the feature values in a graph contains structural information. In GraphNorm, we use a learnable shift with parameter α (see Eq. ( <ref type="formula" target="#formula_15">9</ref>)) to preserve such useful information automatically. We conduct experiments to show whether such a learnable α is essential. We use two typical datasets, PROTEINS and IMDB-BINARY, which exhibit irregular-type and regular-type graphs. Sampled cases are visualized in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>We follow the same experimental setting as above and train GIN/GCN using GraphNorm. In the first setting, we train the model with a learnable α, and in the second setting, we train the model without α, i.e., by fixing α = 1. The training curves are presented in Figure <ref type="figure" target="#fig_4">5</ref>. The figure shows that using a learnable α slightly improves the convergence on PROTEINS while significantly boost the training on IMDB-BINARY. This observation shows that shifting the feature values by subtracting the mean losses information, especially for regular graphs. Such results are consistent with our theoretical analysis in Section 4 and verify the necessity of the learnable shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a principled normalization method, called Graph Normalization (GraphNorm), where the key idea is to normalize all nodes for each individual graph with a learnable shift. Theoretically, we show that GraphNorm serves as a preconditioner that smooths the distribution of the graph aggregation's spectrum, and the learnable shift is used to improve the expressiveness of the networks. Experimental results show GNNs with GraphNorm achieve better generalization performance on several benchmark datasets. In the future, we will apply our method to more scenarios and explore other aspects of the optimization for GNNs.</p><p>[41] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.</p><p>[42] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. pages 4438-4445, 2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>A.1 Proof of Theorem 3.1</p><p>We first introduce the Cauchy interlace theorem:</p><p>Lemma A.1 (Cauchy interlace theorem (Theorem 4.3.17 in <ref type="bibr" target="#b13">[14]</ref>)). Let S ∈ R (n−1)×(n−1) be symmetric, y ∈ R n and a ∈ R be given, and let</p><formula xml:id="formula_17">R = S y y a ∈ R n×n . Let λ 1 ≤ λ 2 ≤ • • • ≤ λ n be the eigenvalues of R and µ 1 ≤ µ 2 ≤ • • • ≤ µ n−1 be the eigenvalues of S.</formula><p>Then</p><formula xml:id="formula_18">λ 1 ≤ µ 1 ≤ λ 2 ≤ • • • ≤ λ n−1 ≤ µ n−1 ≤ λ n ,<label>(10)</label></formula><p>where λ i = µ i only when there is a nonzero z ∈ R n−1 such that Sz = µ i z and y z = 0; if λ i = µ i−1 then there is a nonzero z ∈ R n−1 such that Sz = µ i−1 z, y z = 0.</p><p>Using Lemma A.1, the theorem can be proved as below.</p><p>Proof. For any matrices P, R ∈ R n×n , we use P ∼ R to denote that the matrix P is similar to the matrix R. Note that if P ∼ R, the eigenvalues of P and R are the same. As the singular values of P are equal to the square root of the eigenvalues of P P , we have the eigenvalues of Q Q and that of N Q QN are {λ 2 i } n i=1 and {µ 2 i } n i=1 , respectively. Note that N is a projection operator onto the orthogonal complement space of the subspace spanned by 1, and N can be decomposed as</p><formula xml:id="formula_19">N = U diag   1, • • • , 1 ×n−1 , 0   U</formula><p>where U is an orthogonal matrix. Since 1 is the eigenvector of N associated with eigenvalue 0, we have</p><formula xml:id="formula_20">U = U 1 1 √ n 1 ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_21">U 1 ∈ R n×(n−1) satisfies U 1 1 = 0 and U 1 U 1 = I n−1 . Then we have N Q QN = U diag (1, • • • , 1, 0) U Q QU diag (1, • • • , 1, 0) U ∼ diag (1, • • • , 1, 0) U Q Q Let D = diag (1, • • • , 1, 0) = I n−1 0 0 0 ,<label>(12)</label></formula><formula xml:id="formula_22">B = I n−1 0 ,<label>(13)</label></formula><formula xml:id="formula_23">C = Q Q,<label>(14)</label></formula><p>A.2 Proof of Proposition 4.1</p><p>Proof. For r-regular graph, A = r • I n and Q GIN = r + 1 + ξ (1) I n . Since H (0) is given by one-hot encodings of node degrees, the row of H (0) can be represented as c • 1 where c = 1 for the r-th row and c = 0 for other rows. By the associative property of matrix multiplication, we only need to show H (0) Q GIN N = 0. This is because, for each row</p><formula xml:id="formula_24">c • 1 Q GIN N = c • 1 (r + 1 + ξ (1) )I n I n − 1 n 11<label>(27)</label></formula><formula xml:id="formula_25">= c r + 1 + ξ (1) 1 − 1 • 1 n 11 = 0.<label>(28)</label></formula><p>A.3 Proof of Proposition 4.2</p><p>Proof.</p><formula xml:id="formula_26">Q GIN N = (A + I n + ξ (k) I n )N == (11 + ξ (k)In )N = ξ (k) N,<label>(29)</label></formula><p>A.4 Gradient of W (k)</p><p>We first calculate the gradient of W (k) when using normalization. Denote Z (k) = Norm W (k) H (k−1) Q and L as the loss. Then the gradient of L w.r.t. the weight matrix</p><formula xml:id="formula_27">W (k) is ∂L ∂W (k) = H (k−1) QN ⊗ S ∂L ∂Z (k) ,<label>(30)</label></formula><p>where ⊗ represents the Kronecker product, and thus H (k−1) QN ⊗ S is an operator on matrices.</p><p>Analogously, the gradient of W (k) without normalization consists a H (k−1) Q ⊗ I n term. As suggested by Theorem 3.1, QN has a smoother distribution of spectrum than Q, so that the gradient of W (k) with normalization enjoys better optimization curvature than that without normalizaiton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets</head><p>Detailed of the datasets used in our experiments are presented in this section. Brief statistics of the datasets are summarized in Table <ref type="table" target="#tab_3">2</ref>. Those information can be also found in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b14">[15]</ref>. Social networks datasets. IMDB-BINARY is a movie collaboration dataset. Each graph corresponds to an ego-network for each actor/actress, where nodes correspond to actors/actresses and an edge is drawn betwen two actors/actresses if they appear in the same movie. Each graph is derived from a pre-specified genre of movies, and the task is to classify the genre graph it is derived from. REDDIT-BINARY is a balanced dataset where each graph corresponds to an online discussion thread and nodes correspond to users. An edge was drawn between two nodes if at least one of them responded to another's comment. The task is to classify each graph to a community or a subreddit it belongs to. COLLAB is a scientific collaboration dataset, derived from 3 public collaboration datasets, namely, High Energy Physics, Condensed Matter Physics and Astro Physics. Each graph corresponds to an ego-network of different researchers from each field. The task is to classify each graph to a field the corresponding researcher belongs to.</p><p>Large-scale Open Graph Benchmark: ogbg-molhiv. Ogbg-molhiv is a molecular property prediction dataset, which is adopted from the the MOLECULENET <ref type="bibr" target="#b34">[35]</ref>. Each graph represents a molecule, where nodes are atoms and edges are chemical bonds. Both nodes and edges have associated diverse features. Node features are 9-dimensional, containing atomic number and chirality, as well as other additional atom features. Edge features are 3-dimensional, containing bond type, stereochemistry as well as an additional bond feature indicating whether the bond is conjugated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The Experimental Setup</head><p>Network architecture. For the medium-scale bioinformatics and social network datasets, we use 5-layer GIN/GCN with a linear output head for prediction followed <ref type="bibr" target="#b36">[37]</ref> with residual connection. The hidden dimension of GIN/GCN is set to be 64. For the large-scale ogbgmolhiv dataset, we also use 5-layer GIN/GCN <ref type="bibr" target="#b36">[37]</ref> architecture with residual connection. Following <ref type="bibr" target="#b14">[15]</ref>, we set the hidden dimension as 300.</p><p>Hyper-parameter configurations. We use Adam <ref type="bibr" target="#b17">[18]</ref> optimizer with a linear learning rate decay schedule. For the drawing of the training curves in Figure <ref type="figure" target="#fig_2">4</ref>, we set batch size to be 128, dropout ratio to be 0.5, weight decay to be 0.0, learning rate to be 1e-2, and train the models for 400 epochs for all settings. We select the batch size ∈ {64, 128}, the dropout ratio ∈ {0, 0.5}, weight decay ∈ {5e − 2, 5e − 3, 5e − 4, 5e − 5} ∪ {0.0}, the learning rate ∈ {1e − 4, 1e − 3, 1e − 2}. We follow previous work <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b14">[15]</ref> to select the best hyper-parameter based on validation performance.</p><p>Baselines. For the medium-scale bioinformatics and social network datasets, we compare several competitive baselines as in xu2018how, including the WL subtree kernel model <ref type="bibr" target="#b29">[30]</ref>, diffusion-convolutional neural networks (DCNN) <ref type="bibr" target="#b1">[2]</ref>, Deep Graph CNN (DGCNN) <ref type="bibr">[42]</ref> and Anonymous Walk Embeddings (AWL) <ref type="bibr" target="#b16">[17]</ref>. We report the accuracies reported in the original paper <ref type="bibr" target="#b36">[37]</ref>. For the large-scale ogbg-molhiv dataset, we use the baselines in <ref type="bibr" target="#b14">[15]</ref>, including the Graph-agnostic MLP model, GCN <ref type="bibr" target="#b18">[19]</ref> and GIN <ref type="bibr" target="#b36">[37]</ref>. We also report the roc-auc values reported in the original paper <ref type="bibr" target="#b14">[15]</ref>.</p><p>Evaluation, Using the chosen hyper-parameter, we report the averaged test performance over different random seeds (or cross-validation). For the medium-scale datasets, following <ref type="bibr" target="#b36">[37]</ref>, we perform a 10-fold cross-validation as these datasets do not have a clear train-validatetest splitting format. The mean and standard deviation of the validation accuracies across the 10 folds are reported. For the ogbg-molhiv dataset, we follow the official setting <ref type="bibr" target="#b14">[15]</ref>. We repeat the training process with 10 different random seeds, and average the roc-auc value on the test set. We also report the standard deviation of the roc-auc values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Visualization of the singular value distributions</head><p>As stated in Theorem 3.1, the shift operation N serves as a preconditioner of Q which makes the singular value distribution of Q smoother. To check the improvements, we sample graphs from 6 median-scale datasets (PROTEINS, NCI1, MUTAG, PTC, IMDB-BINARY, COLLAB) for visualization, as in Figure <ref type="figure">6</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Overview. Our proposed GraphNorm is shown along the upper branch. Each step in this branch can boost the performance of GNNs: subtracting graph mean has preconditioning effect; introducing a learnable shift avoids the expressiveness degradation; further scaling to unit norm enjoys "scale-invariant" property<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. In comparison, BatchNorm in the lower branch suffers from heavy batch noise. Overall, GraphNorm significantly surpasses BatchNorm in training speed (Figure4) and enjoys good generalization performance (Table1).</figDesc><graphic url="image-1.png" coords="3,435.58,178.62,103.37,73.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Batch-level statistics are noisy for GNNs. We plot the batch-level/datasetlevel mean/standard deviation of the first (layer 0) and the last (layer 3) BatchNorm layers of different model checkpoints for a five-layer GIN on PROTEINS and a ResNet18 on CIFAR10. The batch size of all experiments are set to 128. More visualizations for different types of graphs can be found in Appendix D.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training performance of GIN/GCN with GraphNorm and BatchNorm on different tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation study of the parameter α. Left panel: Sampled graphs with different topological structures on PROTEINS (bioinformatics) and IMDB-BINARY (social). Right panel: training curves of GIN/GCN using GraphNorm with or without α (α = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Training performance of GIN/GCN with GraphNorm, BatchNorm , LayerNorm and without normalization on PROTEINS, NCI1, PTC MUTAG datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Singular value distribution of Q and QN for sampled graphs in different datasets using GIN. More visualizations for different types of graphs can be found in Appendix D.1Normalization. Generally, given a set of values {x 1 , x 2 , • • • , x m }, a normalization operation first shifts each x i by the mean µ, and then scales them down by standard deviation σ: x i → γ x i −µ σ +β, where γ and β are learnable parameters, µ = 1</figDesc><table><row><cell>3527(,16</cell><cell>1&amp;,</cell><cell>087$*</cell></row><row><cell>6LQJXODU9DOXH</cell><cell>6LQJXODU9DOXH</cell><cell>6LQJXODU9DOXH</cell></row><row><cell cols="2">VLQJXODUYDOXHRIQ</cell><cell>VLQJXODUYDOXHRIQN</cell></row><row><cell cols="2">Figure 2: m</cell><cell>m i=1 x i and</cell></row><row><cell></cell><cell></cell><cell>will be used</cell></row><row><cell cols="3">for prediction. For graph classification tasks, we can apply a READOUT function, e.g.,</cell></row><row><cell cols="3">summation, to aggregate node features h (K) i h G = READOUT h (K) i v i ∈ V . A classifier can be applied upon h G to predict the to obtain the entire graph's representation</cell></row><row><cell>labels.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test performance of GIN/GCN with GraphNorm and BatchNorm on different tasks. Left Panel: 10-fold cross validations are performed and test accuracies are reported.</figDesc><table><row><cell>Datasets</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>IMDB-B</cell><cell>RDT-B</cell><cell>COLLAB</cell><cell>Datasets</cell><cell>Ogbg-molhiv</cell></row><row><cell># graphs</cell><cell>188</cell><cell>344</cell><cell>1113</cell><cell>4110</cell><cell>1000</cell><cell>2000</cell><cell>5000</cell><cell># graphs</cell><cell>41,127</cell></row><row><cell># classes Avg # nodes</cell><cell>2 17.9</cell><cell>2 25.5</cell><cell>2 39.1</cell><cell>2 29.8</cell><cell>2 19.8</cell><cell>2 429.6</cell><cell>2 74.5</cell><cell># classes Avg # nodes</cell><cell>2 25.5</cell></row><row><cell>WL subtree [30]</cell><cell>90.4 ± 5.7</cell><cell>59.9 ± 4.3</cell><cell>75.0 ± 3.1</cell><cell cols="2">86.0 ± 1.8 73.8 ± 3.9</cell><cell>81.0 ± 3.1</cell><cell>78.9 ± 1.9</cell><cell></cell><cell></cell></row><row><cell>DCNN [2]</cell><cell>67.0</cell><cell>56.6</cell><cell>61.3</cell><cell>62.6</cell><cell>49.1</cell><cell>-</cell><cell>52.1</cell><cell></cell><cell></cell></row><row><cell>DGCNN [42]</cell><cell>85.8</cell><cell>58.6</cell><cell>75.5</cell><cell>74.4</cell><cell>70.0</cell><cell>-</cell><cell>73.7</cell><cell></cell><cell></cell></row><row><cell>AWL [17]</cell><cell>87.9 ± 9.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.5 ± 5.9</cell><cell>87.9 ± 2.5</cell><cell>73.9 ± 1.9</cell><cell></cell><cell></cell></row><row><cell cols="2">GIN+BatchNorm ([37]) 89.4 ± 5.6</cell><cell>64.6 ± 7.0</cell><cell>76.2 ± 2.8</cell><cell>82.7 ± 1.7</cell><cell>75.1 ± 5.1</cell><cell>92.4 ± 2.5</cell><cell>80.2 ± 1.9</cell><cell></cell><cell></cell></row><row><cell>GIN+GraphNorm</cell><cell cols="7">91.6 ± 6.5 64.9 ± 7.5 77.4 ± 4.9 81.4 ± 2.4 76.0 ± 3.7 93.5 ± 2.1 80.2 ± 1.0</cell><cell></cell><cell></cell></row></table><note>Right Panel: 10-seed runs are performed and test ROC-AUC values are reported. The best results are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Summary of statistics of benchmark datasets.Bioinformatics datasets. PROTEINS is a dataset where nodes are secondary structure elements (SSEs) and there is an edge between two nodes if they are neighbors in the aminoacid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet or turn. NCI1 is a dataset made publicly available by the National Cancer Institute (NCI) and is a subset of balanced datasets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines, having 37 discrete labels. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels. PTC is a dataset of 344 chemical compounds that reports the carcinogenicity for male and female rats and it has 19 discrete labels.</figDesc><table><row><cell>Datasets</cell><cell cols="8">MUTAG PTC PROTEINS NCI1 IMDB-B RDT-B COLLAB ogbg-molhiv</cell></row><row><cell># graphs</cell><cell>188</cell><cell>344</cell><cell>1113</cell><cell>4110</cell><cell>1000</cell><cell>2000</cell><cell>5000</cell><cell>41127</cell></row><row><cell># classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Avg # nodes</cell><cell>17.9</cell><cell>25.5</cell><cell>39.1</cell><cell>29.8</cell><cell>19.8</cell><cell>429.6</cell><cell>74.5</cell><cell>25.5</cell></row><row><cell>Avg # edges</cell><cell>57.5</cell><cell>72.5</cell><cell>184.7</cell><cell>94.5</cell><cell>212.8</cell><cell>1425.1</cell><cell>4989.5</cell><cell>27.5</cell></row><row><cell>Avg # degrees</cell><cell>3.2</cell><cell>3.0</cell><cell>4.7</cell><cell>3.1</cell><cell>10.7</cell><cell>3.3</cell><cell>66.9</cell><cell>2.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Standard normalization has an additional affine operation after shifting and scaling. Here we omit it in Eq. 7 for easier understanding. Note that adding this operation will not affect the theoretical analysis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We did not include LayerNorm as a baseline in the main body due to that we observe it usually leads to unsatisfactory performance, see Figure10in Appendix. Similar phenomena are also observed in<ref type="bibr" target="#b8">[9]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We thank Zhiyuan Li and Kaifeng Lyu for discussions on the normalization literature; Chen Xing, Ruiqi Gao and Yiping Lu for suggestions on the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have</p><p>Using Lemma A.1 and taking R = U CU and S = U 1 CU 1 , we have the eigenvalues of U 1 CU 1 are interlacing between the eigenvalues of U CU . Note that the eigenvalues of</p><p>n−1 and µ 2 n = 0, and by Eq. ( <ref type="formula">19</ref>), the eigenvalues of DU CU D contain the eigenvalues of U 1 CU 1 and 0. Since the eigenvalues of U CU are λ 2 1 ≤ λ 2 2 ≤ • • • ≤ λ 2 n (By similarity of U CU and C), we then have</p><p>Moreover, the equality holds only when there is a nonzero z ∈ R n−1 that satisfies</p><p>where µ is one of µ 2 i s. Since U 1 forms an orthogonal basis of the orthogonal complement space of 1 and Eq. ( <ref type="formula">22</ref>) is equivalent to " CU 1 z lies in the orthogonal complement space", we have that there is a vector y ∈ R n−1 such that</p><p>Substituting this into Eq. ( <ref type="formula">21</ref>), we have</p><p>Since U 1 U 1 = I n−1 , the equation above is equivalent to</p><p>which means</p><p>i.e., U 1 z is the eigenvector of C associated with µ. By noticing U 1 z lies in the orthogonal complement space of 1 and the eigenvector of C is right singular vector of Q, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Visualization of noise in the batch statistics</head><p>We show the noise of the batch statistics on the PROTEINS task in the main body. Here we provide more experiment details and results. For graph tasks (PROTEINS, PTC, NCI1, MUTAG, IMDB-BINARY datasets), we train a 5-layer GIN with BatchNorm as in <ref type="bibr" target="#b36">[37]</ref> and the number of sub-layers in MLP is set to 2. For image task (CIFAR10 dataset), we train a ResNet18 <ref type="bibr" target="#b11">[12]</ref>. Note that for a 5-layer GIN model, it has four graph convolution layers (indexed from 0 to 3) and each graph convolution layer has two BatchNorm layers; for a ResNet18 model, except for the first 3×3 convolution layer and the final linear prediction layer, it has four basic layers (indexed from 0 to 3) and each layer consists of two basic blocks (each block has two BatchNorm layers). For image task, we set the batch size as 128, epoch as 100, learning rate as 0.1 with momentum 0.9 and weight decay as 5e-4. For graph tasks, we follow the setting of Figure <ref type="figure">4</ref> (described in Appendix C).</p><p>The visualization of the noise in the batch statistics is obtained as follows. We first train the models and dump the model checkpoints at the end of each epoch; Then we randomly sample one feature dimension and fix it. For each model checkpoint, we feed different batches to the model and record the maximum/minimum batch-level statistics (mean and standard deviation) of the feature dimension across different batches. We also calculate dataset-level statistics.</p><p>As Figure <ref type="figure">3</ref> in the main body, pink line denotes the dataset-level statistics, and green/blue line denotes the maximum/minimum value of the batch-level statistics respectively. First, we provide more results on PTC, NCI1, MUTAG, IMDB-BINARY tasks, as in Figure <ref type="figure">7</ref>. We visualize the statistics from the first (layer-0) and the last (layer-3) BatchNorm layers in GIN for comparison. Second, we further visualize the statistics from different BatchNorm layers (layer 0 to layer 3) in GIN on PROTEINS and ResNet18 in CIFAR10, as in Figure <ref type="figure">8</ref>. Third, we conduct experiments to investigate the influence of the batch size. We visualize the statistics from BatchNorm layers under different settings of batch sizes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32</ref>, 64], as in Figure <ref type="figure">9</ref>. We can see that the observations are consistent and the batch statistics on graph data are noisy, as in Figure <ref type="figure">3</ref> in the main body.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Theoretical analysis of auto rate-tuning by batch normalization</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03981</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1993">1993-2001, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of preconditioned iterative methods for linear systems of algebraic equations</title>
		<author>
			<persName><forename type="first">Owe</forename><surname>Axelsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="187" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">To understand deep learning we need to understand kernel learning</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumik</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="541" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Applied numerical linear algebra</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">56</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName><forename type="first">Kangcheng</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruosong</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5724" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Norm matters: efficient and accurate normalization schemes in deep networks</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2160" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
				<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2191" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Daneshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Neymeyr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="806" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An exponential learning rate schedule for deep learning</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07454</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Just interpolate: Kernel&quot; ridgeless&quot; regression can generalize</title>
		<author>
			<persName><forename type="first">Tengyuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rakhlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00387</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards understanding regularization in batch normalization</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanglin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How does batch normalization help optimization?</title>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2483" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking batch normalization in transformers</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07845</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian uncertainty estimation for batch normalized deep networks</title>
		<author>
			<persName><forename type="first">Mattias</forename><surname>Teye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4907" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Moleculenet: A benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<idno>CoRR, abs/1703.00564</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04745</idno>
		<title level="m">On layer normalization in the transformer architecture</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What can neural networks reason</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
