<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Victima: Drastically Increasing Address Translation Reach by Leveraging Underutilized Cache Resources</title>
				<funder>
					<orgName type="full">Microsoft</orgName>
				</funder>
				<funder>
					<orgName type="full">Semiconductor Research Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">VMware</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-06">6 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><forename type="middle">Chul</forename><surname>Nam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">F</forename><forename type="middle">Nisa</forename><surname>Bostanci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Sadrosadati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Norwegian University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Davide</forename><forename type="middle">Basilio</forename><surname>Bartolini</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Zurich Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Victima: Drastically Increasing Address Translation Reach by Leveraging Underutilized Cache Resources</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-06">6 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2310.04158v1[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Address translation is a performance bottleneck in data-intensive workloads due to large datasets and irregular access patterns that lead to frequent high-latency page table walks (PTWs). PTWs can be reduced by using (i) large hardware TLBs or (ii) large softwaremanaged TLBs. Unfortunately, both solutions have significant drawbacks: increased access latency, power and area (for hardware TLBs), and costly memory accesses, the need for large contiguous memory blocks, and complex OS modifications (for software-managed TLBs).</p><p>We present Victima, a new software-transparent mechanism that drastically increases the translation reach of the processor by leveraging the underutilized resources of the cache hierarchy. The key idea of Victima is to repurpose L2 cache blocks to store clusters of TLB entries, thereby providing an additional low-latency and high-capacity component that backs up the last-level TLB and thus reduces PTWs. Victima has two main components. First, a PTW cost predictor (PTW-CP) identifies costly-to-translate addresses based on the frequency and cost of the PTWs they lead to. Leveraging the PTW-CP, Victima uses the valuable cache space only for TLB entries that correspond to costly-to-translate pages, reducing the impact on cached application data. Second, a TLB-aware cache replacement policy prioritizes keeping TLB entries in the cache hierarchy by considering (i) the translation pressure (e.g., last-level TLB miss rate) and (ii) the reuse characteristics of the TLB entries.</p><p>Our evaluation results show that in native (virtualized) execution environments Victima improves average end-to-end application performance by 7.4% (28.7%) over the baseline four-level radix-treebased page table design and by 6.2% (20.1%) over a state-of-the-art software-managed TLB, across 11 diverse data-intensive workloads. Victima delivers similar performance as a system that employs an optimistic 128K-entry L2 TLB, while avoiding the associated area and power overheads. Victima (i) is effective in both native and virtualized environments, (ii) is completely transparent to application and system software, (iii) unlike large software-managed TLBs, does not require contiguous physical allocations, (iv) is compatible with modern large page mechanisms and (iv) incurs very small area and power overheads of 0.04% and 0.08%, respectively, on a modern high-end CPU. The source code of Victima is freely available at https://github.com/CMU-SAFARI/Victima.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Address translation is a significant performance bottleneck in modern data-intensive workloads <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. To enable fast address translation, modern processors employ a two-level translation look-aside buffer (TLB) hierarchy that caches recently used virtual-to-physical address translations. However, with the very large data footprints of modern workloads, the last-level TLB (L2 TLB) experiences high miss rate (misses per kilo instructions; MPKI), leading to highlatency page table walks (PTWs) that negatively impact application performance. Virtualized environments exacerbate the PTW latency as they impose two-level address translation (e.g., up to 24 memory accesses can occur during a PTW in a system with nested paging <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>), resulting in even higher address translation overheads compared to native execution environments. Therefore, it is crucial to increase the translation reach (i.e., the maximum amount of memory that can be covered by the processor's TLB hierarchy) to improve the effectiveness of TLBs and thus minimize PTWs. Doing so becomes increasingly important as PTW latency continues to rise with modern processors' deeper multi-level page table (PT) designs (e.g., 5-level radix PT in the latest Intel processors <ref type="bibr" target="#b3">[4]</ref>).</p><p>Previous works have proposed various solutions to reduce the high cost of address translation and increase the translation reach of the TLBs such as employing (i) large hardware TLBs <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> or (ii) backing up the last-level TLB with a large software-managed TLB <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Unfortunately, both solutions have significant drawbacks: increased access latency, power, and area (for hardware TLBs), and costly memory accesses, the need for large contiguous memory blocks, and complex OS modifications (for softwaremanaged TLBs).</p><p>Drawback of Large Hardware TLBs. First, a larger TLB has larger access latency (e.g., 1.4x larger latency for every 2x increase in size as reported by CACTI 7.0 <ref type="bibr" target="#b25">[26]</ref>), which may partially or entirely offset the performance gains due to fewer TLB misses. Second, a larger TLB leads to larger chip area and higher power consumption, resulting in higher costs and challenges in managing power constraints within the system. Third, a larger TLB may only benefit a specific subset of workloads, making it challenging to justify its applicability in a general-purpose system where some workloads are not sensitive to address translation performance. Section 3.1 provides a detailed quantitative analysis of (i) increasing the size of a conventional last-level TLB and (ii) expanding the TLB hierarchy with a hardware L3 TLB, considering both realistic and optimistic (i.e, ideal) TLB designs.</p><p>Drawbacks of Large Software-Managed TLBs. First, to look up a software-managed TLB (STLB), the processor fetches STLB entries from the main memory into the cache hierarchy, resulting in a translation latency comparable to that of a PTW. Hence, an STLB is more effective when PTW latency is higher than the STLB access latency (e.g., as in virtualized environments). Second, storing an STLB in memory requires allocating large contiguous memory blocks during runtime (on the order of 10's of MB <ref type="bibr" target="#b16">[17]</ref>). Third, an STLB introduces complex hardware/software interactions (e.g., evicting data from a hardware TLB to an STLB) and requires modifications in OS software. Section 3.2 provides a detailed quantitative analysis of STLBs.</p><p>Opportunity: Leveraging the Cache Hierarchy. Rather than expanding hardware TLBs or introducing large software-managed TLBs, a cost-effective method to drastically increase translation reach is to store the existing TLB entries within the existing cache hierarchy. For example, a 2MB L2 cache can fit 128? the TLB entries a 2048-entry L2 TLB holds. When a TLB entry resides inside the L2 cache, only one low-latency (e.g., ? 16 cycles) L2 access is needed to find the virtual-to-physical address translation instead of performing a high-latency (e.g., ? 137 cycles as shown in ?3) PTW. One potential pitfall of this approach is the potential reduction of caching capacity for application data, which could ultimately harm end-to-end performance. However, as we show in ?3 and as shown in multiple prior works <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>, modern data-intensive workloads, tend to (greatly) underutilize the cache hierarchy, especially the large L2/L3/L4 caches. This is because many modern working sets exceed the capacity of the cache hierarchy and many data accesses exhibit low spatial and temporal locality <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. Therefore, the underutilized cache blocks can likely be repurposed to store TLB entries without replacing useful program data and harming end-to-end application performance.</p><p>Our goal in this work is to increase the translation reach of the processor's TLB hierarchy by leveraging the underutilized resources in the cache hierarchy. We aim to design such a practical technique that: (i) is effective in both native and virtualized execution environments, (ii) does not require or rely on contiguous physical allocations, (iii) is transparent to both application and OS 2 Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Virtual Memory Abstraction</head><p>Virtual memory is a cornerstone of most modern computing systems that eases the programming model by providing a convenient abstraction to manage the physical memory <ref type="bibr" target="#b21">[22,</ref>. The operating system (OS), transparently to application software, maps each virtual memory address to its corresponding physical memory address. Doing so provides a number of benefits, including: (i) application-transparent memory management, (ii) sharing data between applications, (iii) process isolation, and (iv) page-level memory protection. Conventional virtual memory designs allow any virtual page to map to any free physical page. Such a flexible address mapping enables two important key features of virtual memory: (i) efficient memory utilization, and (ii) sharing pages between applications. However, such a flexible address mapping mechanism has a critical downside: it creates the need to store a large number of virtual-to-physical mappings, as for every process, the OS needs to store the physical location of every virtual page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Page Table (PT)</head><p>The PT is a per-process data structure that stores the mappings between virtual and physical pages. In modern x86-64 processors, the PT is organized as a four-level radix-tree <ref type="bibr" target="#b72">[73]</ref>. Even though the radix-tree-based PT optimizes for storage efficiency, it requires multiple pointer-chasing operations to discover the virtual-to-physical mapping. To search for a virtual-to-physical address mapping, the system needs to sequentially access each of the four levels of the page table. This process is called page table walk (PTW).</p><p>Figure <ref type="figure">1</ref> shows the PTW assuming (i) an x86-64 four-level radixtree PT whose base address is stored in the CR3 register, and (ii) 4KB pages. As shown in Figure <ref type="figure">1</ref>, a single PTW requires four sequential memory accesses 1 -4 to discover the physical page number. The processor uses the first 9-bits of the virtual address as offset (Page Map Level4; PML4) to index the appropriate entry of the PT within the first level of the PT 1 . The processor then reads the pointer stored in the first level of the PT to access the second-level of the PT 2 . It uses the next 9-bit set (Page Directory Page table; PDP) from the virtual address to locate the appropriate entry within the second level. This process continues iteratively for each subsequent level of the PT (Page Directory; PD 3 and Page Table ; PT 4 ). Eventually, the processor reaches the leaf level of the PT, where it finds the final entry containing the physical page number corresponding to the given virtual address 5 . ARM processors use a similar approach, with the number of levels varying across different versions of the ISA <ref type="bibr" target="#b73">[74]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Virtualized Environments</head><p>In virtualized environments, each memory request requires a twolevel address translation: (i) from guest-virtual to guest-physical, and (ii) from guest-physical to host-physical. The dominant technique to perform address translation in virtualized environments is Nested Paging (NP) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. In NP, the system uses two page tables: the guest page </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Memory Management Unit (MMU)</head><p>When a user process generates a memory (i.e., instruction or data) request, the processor needs to translate the virtual address to its corresponding physical address. Address translation is a critical operation because it sits on the critical path of the memory access flow: no memory access is possible unless the requested virtual address is first translated into its corresponding physical address. Given that frequent PTWs lead to high address translation overheads, modern cores comprise of a specialized memory management unit (MMU) responsible for accelerating address translation. Figure <ref type="figure" target="#fig_0">2</ref> shows an example structure of the MMU of a modern processor <ref type="bibr" target="#b74">[75]</ref>, consisting of three key components: (i) a two-level hierarchy of translation lookaside buffers (TLBs), (ii) a hardware page table walker, and (iii) page walk caches (PWCs). L1 TLBs are highly-or fully-associative caches that directly provide the physical address for recently-accessed virtual pages at very low latency (i.e., typically within 1 cycle). There are two separate L1 TLBs, one for instructions (L1 I-TLB) and one for data (L1 D-TLB). Modern TLBs make use of multiple page sizes beyond 4KB in order to (i) cover large amounts memory with a single entry and (ii) maintain compatibility with modern OSes that transparently allocate large pages <ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref>. For example, an Intel Cascade Lake core <ref type="bibr" target="#b74">[75]</ref> employs 2 L1 D-TLBs, one for 2MB pages and one for 4KB pages. Translation requests that miss in the L1 TLBs 1 are forwarded to a unified L2 TLB, that stores translations for both instructions and data. In case of an L2 TLB miss, the MMU triggers a PTW 2 . PTW is performed by a dedicated hardware page table walker capable of performing multiple concurrent PTWs. In order to reduce PTW latency, page table walkers are equipped with page walk caches (PWC) 3 , which are small dedicated caches for each level of the PT (for the first three levels in x86-64). In case of a PWC miss, the MMU issues the request(s) for the corresponding level of the PT to the conventional memory hierarchy 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 I-TLB L1 D-TLB (4KB)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2 Unified TLB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page Table Walker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page Walk Caches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Management Unit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Miss</head><p>To accelerate address translation in virtualized execution environments that use Nested Paging <ref type="bibr" target="#b11">[12]</ref>, as shown in Figure <ref type="figure" target="#fig_1">3</ref>, the MMU is additionally equipped with (i) a nested TLB that stores guest-physical-to-host-physical mappings and (ii) an additional hardware page table walker that walks the host PT (while the other one walks the guest PT). Upon an L2 TLB miss, the MMU triggers a guest PTW to retrieve the guest-physical address 1 . On a PWC miss, the guest Page Table Walker must retrieve the guest PT entries from the cache hierarchy. However, to access the cache hierarchy that operates on host-physical addresses, the guest PTW must first translate the host-virtual address to the host-physical address using a host PTW. To avoid the host PTW, the MMU probes the nested TLB to search for the host-virtual-to-host-physical translation 2 . Only in case of a nested TLB miss the MMU triggers the host PTW 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 D-TLB (4KB)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2 Unified TLB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Guest Page Table Walker</head><p>Page Walk Caches Nested TLB </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Host Page Table Walker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Management Unit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Miss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>As shown in multiple prior academic works and industrial studies <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, various modern data-intensive workloads experience severe performance bottlenecks due to address translation. For example, a system that (i) employs a 1.5K-entry L2 TLB and (ii) uses both 4KB and 2MB pages, experiences a high MPKI of 39, averaged across all evaluated workloads (see Fig. <ref type="figure">5</ref>). <ref type="foot" target="#foot_0">1</ref> At the same time, as we show in Figure <ref type="figure" target="#fig_2">4</ref>, the average latency of a PTW is 137 cycles. <ref type="foot" target="#foot_1">2</ref> Based on our evaluation results, frequent L2 TLB misses in combination with high-latency PTWs lead to an average of 30% of total execution cycles spent on address translation.  Previous works propose various solutions to reduce the high cost of address translation and increase the translation reach of the TLBs such as employing (i) large hardware TLBs <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> or (ii) backing up the last-level TLB with a large software-managed TLB <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. We examine these solutions and their shortcomings in ?3.1 and ?3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean (137 cycles)</head><formula xml:id="formula_0">0.0 K 1.0 K 2.0 K 3.0 K 4.0 K 5.0 K 20-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Large Hardware TLBs</head><p>We evaluate the effectiveness of increasing the size of the TLB. Our methodology and workloads are described in detail in ?8. Figure <ref type="figure">5</ref> demonstrates the L2 TLB MPKI as we increase the size of the L2 TLB from 1.5K up to 64K entries. We observe that increasing the number of L2 TLB entries from 1.5K to 64K (i.e., by 42?) results in reducing the MPKI from 39 to 24 (i.e., by 44%). To better understand the potential performance of increasing the size of the L2 TLB, Figure <ref type="figure" target="#fig_4">6</ref> shows the execution time speedup of L2 TLB configurations with increasing sizes but equal access latencies (i.e., 12 cycles) compared to the baseline system (1.5K-entry L2 TLB). We evaluate an optimistic setting where the access latency is set to 12 cycles regardless of the TLB size. We observe that the optimistic 64K-entry configuration (that reduces MPKI by 44%) leads to a 4.0% higher performance on average compared to the baseline 1.5K-entry L2 TLB configuration.   Unfortunately, increasing the TLB size does not come for free: it leads to larger access latency (as well as area and power), which counteracts the potential performance benefits due to fewer PTWs. For instance, according to CACTI 7.0 <ref type="bibr" target="#b25">[26]</ref>, the latency of accessing a 64K-entry large TLB is as high as 39 cycles. Figure <ref type="figure">7</ref> shows the execution time speedup of realistic L2 TLB configurations with increasing sizes, while the access latency is adjusted based on the size of the TLB (based on CACTI 7.0 modeling <ref type="bibr" target="#b25">[26]</ref>), compared to the baseline system (1.5K-entry L2 TLB with 12-cycle access latency). We observe that in this realistic setting, the performance benefits of increasing the L2 TLB size are significantly lower compared to the optimistic setting (Fig. <ref type="figure" target="#fig_4">6</ref>). The realistic 64K-entry configuration (that reduces MPKI by 44%, but comes with a 39-cycle access latency) leads to only 0.8% higher average performance over the baseline configuration. We conclude that although increasing the L2 TLB size reduces PTWs, it comes with increased access latency (as well as power and area), which leads to small performance benefits realistically.</p><p>Increasing the size of the L2 TLB has a negative impact on the translation latency of requests that hit in the L2 TLB. Therefore, to keep the access latency of the L2 TLB small, we also explore a scenario where the TLB hierarchy is extended with a large hardware L3 TLB. Figure <ref type="figure" target="#fig_5">8</ref> shows the execution time speedup achieved by a system with a 64K-entry L3 TLB with increasing access latencies, ranging from 15 cycles up to 39 cycles (which is the latency Figure <ref type="figure">7</ref>: Speedup provided by larger L2 TLBs over the baseline system (1.5K-entry L2 TLB). L2 TLB access latency is adjusted based on the size of the TLB (modeled using CACTI 7.0 <ref type="bibr" target="#b25">[26]</ref>).</p><p>suggested by CACTI 7.0 <ref type="bibr" target="#b25">[26]</ref>), compared to the baseline system that employs a two-level TLB hierarchy (with a 1.5K-entry 12-cycle L2 TLB). We observe that a large 64K-entry L3 TLB with a very aggressive 15-cycle access latency leads to a 2.9% performance increase compared to the baseline system. The performance gains are lower compared to employing a 64K-entry L2 TLB (4.0%). This is because, for applications that experience low L2 TLB hit rates, employing an L3 TLB results in a higher L3 TLB hit latency (L2 TLB miss latency + L3 TLB hit latency) compared to using a large L2 TLB. We conclude that employing a large L3 TLB is not universally beneficial, and the performance gains heavily depend on the L2 TLB hit rates and L3 TLB access latencies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Large Software-Managed TLBs</head><p>Previous works <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> propose using large software-managed TLBs to reduce PTWs. However, software-managed TLBs suffer from four key disadvantages. First, to look up a software-managed TLB (STLB), the processor fetches STLB entries from the main memory into the cache hierarchy. At the same time, the hit rate of STLBs likely does not justify the cost of fetching STLB entries from the main memory. Hence, the total latency of accessing STLB entries and performing PTWs is comparable to the latency of performing PTWs in the baseline system. To validate our claim, Figure <ref type="figure" target="#fig_6">9</ref> shows the average L2 TLB miss latency in (i) the baseline system in native execution, (ii) a system with a state-of-the-art L3 STLB <ref type="bibr" target="#b16">[17]</ref> in native execution, (iii) the baseline system that employs nested paging (NP) <ref type="bibr" target="#b11">[12]</ref> in virtualized execution and (iv) a system with a stateof-the-art L3 STLB <ref type="bibr" target="#b16">[17]</ref> and NP <ref type="bibr" target="#b11">[12]</ref> in virtualized execution. We observe that the average L2 TLB miss latency in a system with an STLB is 122 cycles, which is comparable to the baseline system (128 cycles). However, the average L2 TLB miss latency in the system with NP in virtualized execution is 275 cycles, which is higher than the average L2 TLB miss latency in a system with an L3 STLB (220 cycles) in virtualized execution, making the STLB a more attractive solution in virtualized execution environments. Second, allocating an STLB in software requires contiguous physical address space (on the order of 10's of MB), which is difficult to find in environments where memory is heavily fragmented, such as : L2 TLB miss latency of (i) baseline system in native execution, (ii) system with STLB <ref type="bibr" target="#b16">[17]</ref> in native execution, (iii) baseline system in virtualized execution and, (iv) STLB in virtualized execution.</p><p>data centers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81]</ref> and in cases where memory capacity pressure is high <ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref>. Third, resizing an STLB throughout the execution of the program to match the program's needs is challenging due to the large data movement cost of migrating the TLB entries betweeen different software data structures <ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref>. Fourth, integrating a software-managed TLB in the address translation pipeline requires OS and hardware changes to support (i) flushing and updating software STLB entries during a TLB shootdown <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, (ii) handling evictions from the hardware TLB to the STLB <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Opportunity: Storing TLB Entries Inside the Cache Hierarchy</head><p>Instead of expanding hardware TLBs or introducing large softwaremanaged TLBs, we posit that a cost-effective method to drastically increase the translation reach of the TLB hierarchy is to store the existing TLB entries within the existing cache hierarchy. For example, a 2MB L2 cache can fit 128? the TLB entries a 2048-entry L2 TLB holds. When a TLB entry resides inside the L2 cache, only one low-latency (i.e., ? 16 cycles) L2 access is needed to find the virtualto-physical address translation instead of performing a high-latency (i.e., ? 137 cycles on average) PTW.</p><p>To better understand the potential of caching TLB entries in the cache hierarchy, we conduct a study where for every L2 TLB miss, the translation request is always served from the L1 cache (TLBhit-L1), L2 cache (TLB-hit-L2) or the LLC (TLB-hit-LLC). Figure <ref type="figure" target="#fig_7">10</ref> shows the reduction in address translation latency provided by TLB-hit-{L1, L2, LLC} compared to the baseline system. We observe that, even when servicing every L2 TLB miss from the LLC (which takes ?35 cycles to access), L2 TLB miss latency is reduced by 71.9% on average across 11 workloads. We conclude that caching TLB entries inside the cache hierarchy can potentially greatly reduce the address translation latency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cache Underutilization</head><p>One potential pitfall of storing TLB entries inside the cache hierarchy is the potential reduction of caching capacity for application data, which could ultimately harm end-to-end performance. However, as shown in prior works <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>, many modern data-intensive workloads, tend to (greatly) underutilize the cache hierarchy, especially the large L2/L3/L4 caches. This is because modern working sets exceed the capacity of the cache hierarchy and data accesses exhibit low spatial and temporal locality <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>.</p><p>Figure <ref type="figure" target="#fig_8">11</ref> shows the reuse-level distribution of blocks in the L2 cache across our evaluated data-intensive workloads (note that y-axis starts from 75%). We observe that on average 92% of the cache blocks experience no reuse (i.e., 0 reuse) after being brought to the L2 cache (i.e., these blocks are not accessed while they reside inside the L2 cache). In contrast, only 8% of blocks experience reuse higher than 1 (i.e., they are accessed more than once while they reside inside the L2 cache). We conclude that a large fraction of the underutilized cache blocks can be repurposed to store TLB entries without replacing useful program data and harming end-to-end application performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Our Goal</head><p>Our goal is to increase the translation reach of the processor's TLB hierarchy by leveraging the underutilized resources in the cache hierarchy. We aim to design such a practical technique that: (i) is effective in both native and virtualized execution environments, (ii) does not require or rely on contiguous physical allocations, (iii) is transparent to both application and OS software and (iv) has low area, power, and energy costs. To this end, our key idea is to store TLB entries in the cache hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Victima: Design Overview</head><p>We present Victima, a new software-transparent mechanism that drastically increases the translation reach of the TLB by leveraging the underutilized resources of the cache hierarchy. The key idea of Victima is to repurpose L2 cache blocks to store clusters of TLB entries. Doing so provides an additional low-latency and high-capacity component to back up the last-level TLB and thus reduces PTWs. Victima has two main components. First, a PTW cost predictor (PTW-CP) identifies costly-to-translate addresses based on the frequency and cost of the PTWs they lead to. Leveraging the PTW-CP, Victima uses the valuable cache space only for TLB entries that correspond to costly-to-translate pages, reducing the impact on cached application data. Second, a TLB-aware cache replacement policy prioritizes keeping TLB entries in the cache hierarchy by taking into account (i) the translation pressure (e.g., high last-level TLB miss rate) and (ii) the reuse characteristics of the TLB entries.</p><p>Figure <ref type="figure" target="#fig_9">12</ref> shows the translation flow in Victima compared to the one in a conventional baseline processor <ref type="bibr" target="#b49">[50]</ref>. In the baseline system (Fig. <ref type="figure" target="#fig_9">12</ref> top) , (i) whenever an entry is evicted from the L2 TLB 1 , the evicted TLB entry is not cached anywhere. Hence, (i) the TLB entry is dropped 2 and (ii) a high-latency PTW is required to fetch it when it is requested again 3 . In contrast, Victima (Fig. <ref type="figure" target="#fig_9">12</ref> bottom) stores into the L2 cache (i) entries that get evicted from the L2 TLB and (ii) the TLB entries of memory accesses that cause L2 TLB misses. Victima gets triggered on last-level TLB misses and evictions 4 . On a last-level TLB miss, if PTW-CP predicts that the page will be costly-to-translate in the future 5 , Victima transforms the data cache block that contains the last-level PT entries (PTEs) (fetched during the PTW) into a cluster of TLB entries 6 to enable direct access to the corresponding cluster of PTEs using a virtual address without walking the PT. Storing a cluster of TLB entries for contiguous virtual pages inside the L2 cache can be highly beneficial for applications whose memory accesses exhibit high spatial locality. On a last-level TLB eviction 7 , if PTW-CP makes a positive prediction, Victima issues a PTW in the background to bring the PTEs of the evicted address into the L2 cache, and Victima transforms the fetched PTEs into a TLB entry. This way, if the evicted TLB entry is accessed again in the future 8 , Victima can directly access the corresponding PTE from the L2 cache without walking the PT 9 . Storing evicted TLB entries in the L2 cache can be highly beneficial for applications that experience a high number of capacity misses in the TLB hierarchy. Victima's functionality seamlessly applies to virtualized environments as well. In virtualized execution, where Victima stores into the L2 cache both (i) conventional TLB entries that store direct guest-virtual-to-host-physical mappings as well as (ii) nested TLB entries that store guest-physical-to-host-physical mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Victima: Detailed Design</head><p>We describe in detail (i) how the L2 cache is modified to store TLB entries, (ii) how Victima inserts TLB entries into the L2 cache, (iii) how address translation flow changes in the presence of Victima, (iv) how Victima operates in virtualized environments and (v) how Victima maintains TLB entries coherent. We use as the reference design point a modern x86-64 system that employs 48-bit virtual addresses (VA) and 52-bit physical adddresses (PA) <ref type="bibr" target="#b72">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Modifications to the L2 Cache</head><p>We minimally modify the L2 cache to (i) support storing TLB entries and (ii) enable a TLB-aware replacement policy that favors keeping TLB entries inside the L2 cache taking into account address translation pressure (e.g., L2 TLB MPKI) and the reuse characteristics of TLB entries. TLB Blocks. We introduce a new cache block type to store TLB entries in the data store of the L2 cache, called the TLB block. Figure <ref type="figure" target="#fig_10">13</ref> shows how the same address maps to (i) a conventional L2 data cache block and (ii) an L2 cache block that contains TLB entries for 4KB or 2MB pages. Each cache entry can potentially store a data block or a TLB block. A conventional data block is (typically) accessed using the PA while a TLB block is accessed using the VA. Victima modifies the cache block metadata layout to enable storing TLB entries. First, an additional bit is needed to distinguish between a data block versus a TLB block. Second, in a conventional data block, the size of the tag of a 1MB, 16-way associative L2 cache consists of 52 -??? 2 (1024) -??? 2 (64) = 36 bits. However, in a TLB block, the tag consists of only 23 bits and is computed as 48 -??? 2 (4??) -??? 2 (1024) -??? 2 (8) = 23 bits which is smaller than the tag needed for a data block. <ref type="foot" target="#foot_2">3</ref> We leverage the unused space in TLB blocks to (i) avoid aliasing and (ii) store page size information.  To prevent aliasing between the virtual addresses (VAs) of different processes, 11 unused bits of the tag are reserved for storing the address-space identifier (ASID) or the virtual-machine identifier (VMID) of each process. The rest of the bits are used to store page size information. Given a 48-bit VA and 52-bit PA, we can spare 11 bits for the ASID/VMID. As the VA size becomes larger, e.g., 57 bits, fewer bits can be spared for the ASID/VMID (4 bits in case of 57-bit VA and 52-bit PA). However, modern operating systems do not use more than 12 ASIDs/core <ref type="bibr" target="#b88">[89]</ref> in order to avoid expensive lookups in the ASID table. Hence, when using 57-bit VAs and 52-bit PAs, even with only 4 bits left for the ASID, there is no risk of aliasing.</p><p>For a cache with 64-byte cache lines, it is possible to uniquely tag and avoid aliasing between TLB entries (without increasing the size of the cache's hardware tag entries) only if (?? ?????? &gt; ? ? ?????? -9). <ref type="foot" target="#foot_3">4</ref> In cases where this condition is not met, an alternative approach is to reduce the number of TLB entries in the TLB Block (e.g., by storing 7 PTEs instead of 8 PTEs) and use the remaining bits for the tag/ASID/VMID. Previous works (e.g., <ref type="bibr" target="#b89">[90]</ref>) propose such solutions to enable efficient sub-block tagging in data caches. TLB-aware Cache Replacement Policy. We extend the conventional state-of-the-art SRRIP cache replacement policy <ref type="bibr" target="#b90">[91]</ref> to prioritize storing TLB entries of an application for longer time periods if the application experiences high address translation overheads (i.e., L2 TLB MPKI greater than 5). Listing 5.1 shows the pseudocode of the block insertion function, replacement candidate function, and cache hit function for SRRIP in the baseline system and Victima (changes compared to baseline SRRIP are marked in blue). Upon insertion of a TLB entry inside the L2 cache (insertBlockInL2(block) Line 1), the re-reference interval (analogous to reuse distance) is set to 0 (Line 6), marking the TLB entry as a block with a small reuse distance. This way, TLB entries are unlikely to be evicted soon after their insertion. Upon selection of a replacement candidate (chooseReplacementCandidate() Line 10), if the selected replacement candidate is a TLB block (Line 23) and translation pressure is high (Line 23), SRRIP makes one more attempt to find a replacement candidate that is not a TLB block (Line 23). If no such candidate is found, the TLB block is evicted from the L2 cache and is dropped (i.e., not written anywhere else). Upon a cache hit to a TLB entry (updateOnL2CacheHit(index) Line 28), the re-reference interval is reduced by three instead of one (Line 32) to provide higher priority to the TLB entry compared to other data blocks (Line 34). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inserting TLB Blocks into the L2 Cache</head><p>Victima allocates a block of 8 TLB entries (64 bytes) that correspond to 8 contiguous virtual pages inside the L2 cache upon an L2 TLB miss or an L2 TLB eviction, if the corresponding page is deemed to be costly-to-translate in the future. To predict whether a page will be costly-to-translate, Victima employs a Page Table Walk cost predictor (PTW-CP). Figure <ref type="figure" target="#fig_11">14</ref> depicts Victima's operations on an L2 TLB miss or eviction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core</head><p>Virtual Address Inserting a TLB Block into the L2 Cache upon an L2 TLB Miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1 TLB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Management Unit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2 TLB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PTW Cost Estimator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unified L2 Cache</head><note type="other">Page Table Walker</note><p>When an L2 TLB miss occurs, the MMU consults the PTW-CP to find out if the page is predicted to be costly-to-translate in the future ( 1 in Fig. <ref type="figure" target="#fig_11">14</ref>). If the prediction is positive, the MMU checks if the corresponding TLB block already resides inside the L2 cache 2 . If it does, no further action is needed. If not, the MMU first waits until the PTW is completed 3b . When the last level of the PT is fetched, the MMU transforms the cache block that contains the PTEs to a TLB block by updating the metadata of the block 4 . The MMU (i) replaces the existing tag with the tag of the virtual page region, (ii) sets the TLB bit to mark the cache block as TLB block, and (iii) updates the ASID and the page size information associated with the TLB block. This way, the TLB block containing the consecutive PTE entries is directly accessible using the corresponding virtual page numbers and the ASID of the application without walking the PT. Storing several (e.g., 8 in our implementation) TLB entries for consecutive virtual pages inside the same L2 cache TLB block can be highly beneficial for applications whose memory acccesses exhibit high spatial locality and frequently access neighboring pages.</p><p>Inserting a TLB Block into the L2 Cache upon an L2 TLB Eviction. When an L2 TLB eviction occurs, the MMU consults the PTW-CP to find out if the page is predicted to be costly-to-translate in the future ( 1 in Fig. <ref type="figure" target="#fig_11">14</ref>). If the outcome of the prediction is positive, the MMU checks if the corresponding TLB block already resides in the L2 cache 2 . If it does, no further action is needed. If it does not, the MMU issues in the background a PTW for the corresponding TLB entry 3a . When the last level of the page table is fetched 3b , the MMU follows the same procedure as the L2 TLB miss-based insertion (i.e., transforms the cache block that contains the PTEs to a TLB block) 4 . This way, if the evicted TLB entry (or any other TLB entry in the block) is accessed again in the future, Victima can directly access the corresponding PTE without walking the PT.</p><p>Page Table <ref type="table">Walk</ref> Cost Predictor: Functionality. The PTW cost predictor (PTW-CP) is a small comparator-based circuit that estimates whether the page is among the top 30% most costly-totranslate pages. Using it, Victima predicts if a page will cause costly PTWs in the future and decides whether the MMU should store the corresponding TLB block inside the L2 cache. To make this decision, PTW-CP uses two metrics associated with a page: (i) PTW frequency and (ii) PTW cost, both of which are embedded inside the PTE of the corresponding page. Figure <ref type="figure" target="#fig_12">15</ref> shows the structure and the functionality of PTW-CP. PTW frequency is stored as a 3-bit counter in the unused bits of the PTE and is incremented after every PTW that fetches the corresponding PTE. PTW cost is also stored as a 4-bit counter in the unused bits of the PTE and is incremented every time the PTW leads to at least one DRAM access. Both counters are updated by the MMU after every PTW that fetches the corresponding PTE. If any of the two counters overflows, its value remains at the maximum value throughout the rest of the program's execution. On an L2 TLB miss or eviction 1 , the PTW-CP waits until the corresponding PTE is fetched inside the L2 TLB. PTW-CP fetches the two counters 2 from the TLB entry that contains the PTE, passes them through a tree of comparators, and calculates the result 3 . If the L2 cache experiences high MPKI (i.e., data exhibits low locality, meaning that caching data is not that beneficial), the PTW-CP is bypassed and the TLB entry is inserted inside the L2 cache without consulting the PTW-CP 4 . Page Table Walk Cost Predictor: Feature Selection. Our development of PTW-CP's architecture involves a systematic and empirical approach to (i) identify the most critical features for making high-accuracy predictions and (ii) create an effective predictor while minimizing hardware overhead and inference latency. Initially, we collect a set of 10 per-page features related to address translation, as shown in Table <ref type="table" target="#tab_8">1</ref>. From these 10 features, we methodically identify a small subset that would maximize accuracy while minimizing prediction time and storage overhead. Table <ref type="table" target="#tab_9">2</ref> shows the architectural characteristics and the performance of three different multi-layer perceptron-based neural networks (NN) <ref type="bibr" target="#b91">[92]</ref> and of our final comparator-based model. First, we evaluate three different NN architectures with different feature sets to gain insights about the most critical features (for accuracy). The first NN (NN-10) uses all 10 features, the second NN (NN-5) uses a set of 5 features (PTW cost, PTW frequency, PWC hits, L2 TLB evictions, and accesses to the page), and the third (NN-2) uses only 2 features, the PTW frequency and the PTW cost. We use four metrics to evaluate the performance of each model: accuracy, precision, recall, and F1-score. Accuracy is the fraction of correct predictions, precision is the fraction of correct positive (i.e., costly-to-translate) predictions, and recall is the fraction of correct negative predictions. F1-score is the harmonic mean of precision and recall. In the context of PTW-CP, making negative predictions when the page is actually costly-to-translate leads to performance degradation, while making positive predictions when the page is actually not costly-to-translate leads to L2 cache pollution. From Table <ref type="table" target="#tab_9">2</ref>, we observe that NN-10 achieves the highest performance, with an F1score of 90.42%. By reducing the number of features to 5, NN-5 still achieves high performance reaching 89.89% F1-score while NN-2 leads to an F1-score of 80.66%. At the same time, NN-2 is 7.75x smaller than NN-10 and 90.5x smaller than NN-5 which makes it an attractive solution for PTW-CP as it achieves reasonable accuracy with small hardware overhead. To gain a better understanding of the prediction pattern of NN-2, Fig. <ref type="figure" target="#fig_13">16</ref> shows the predictions of the network for all possible PTW frequency and PTW cost value pairs. We observe that the network exhibits a clear prediction pattern that separates costly-to-translate pages from non-costly-to-translate pages: PTW frequency-cost value pairs that fall inside the boundaries of the bounding box (rectangle spanning from the bottom-left corner (1,1) to the top-right corner (12,7) as drawn on Fig. <ref type="figure" target="#fig_13">16</ref>) are classified as costly-to-translate by NN-2, while PTW frequency-cost value pairs that fall outside the bounding box are classified as non-costly-to-translate. Many of the PTW frequency-cost value pairs never occur during the execution of the applications we evaluate and are not classified by NN-2. Table 2 demonstrates that a simple comparator approach that mimics the functionality the bounding box shown in Fig. <ref type="figure" target="#fig_13">16</ref>, achieves an F1-score of 80.66% without any performance loss compared to NN-2. The comparator-based model requires only 24 bytes of storage, 251x less than NN-10, 2923x less than NN-5 and 32x less than NN-2. The comparator-based model requires only (i) four comparators to compare the two counters with the edges of the bounding box, i.e., (1,1) and (12,7) and (ii) can make a prediction in a single cycle. The comparator-based model is the PTW-CP architecture that we use in Victima. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Address Translation Flow with Victima</head><p>Figure <ref type="figure" target="#fig_14">17</ref> demonstrates the address translation flow in a system that employs Victima. When an L2 TLB miss occurs, the MMU in parallel (i) initiates the PTW 1 and (ii) looks up the corresponding TLB block in the L2 cache 2 . In contrast to regular L2 data block lookups, which are performed using the physical address, a TLB block lookup is performed using the virtual page number (VPN) and the address-space identifier (ASID) of the translation request. The size of the VPN is not known a priori, so Victima probes the L2 cache twice in parallel, once assuming a 4KB VPN and once assuming a 2MB VPN. If the tag (either the tag of the 4KB VPN or the tag of the 2MB VPN) and the ASID matches with a block that has the TLB-entry bit set, the translation request is served by the L2 cache 3a , the PTW is aborted, and the TLB entry is inserted into the L2 TLB. If the TLB entry is not found in the L2 cache, the PT walker runs to completion and resolves the translation 3b . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual Address</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Management Unit Unified L2 Cache</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PS TAG TLB-E ASID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P0 ? P7</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLB Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page Table Walker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Victima in Virtualized Environments</head><p>We demonstrate how Victima improves address translation in virtualized environments. The key idea is to insert both (i) TLB entries and (ii) nested TLB entries into the L2 cache to increase the translation reach of the processor's TLB hierarchy for both guestvirtual-to-guest-physical and guest-physical-to-host-physical address translations and avoid both (i) guest-PTWs and (ii) host-PTWs.</p><p>A nested TLB block is a block of 8 nested TLB entries that correspond to 8 contiguous host-virtual pages. To distinguish between conventional TLB blocks and nested TLB blocks, Victima extends the cache block metadata with an additional bit to mark a block as a nested TLB block. Figure <ref type="figure" target="#fig_5">18</ref> shows how Nested TLB blocks are inserted into the L2 cache in a system that employs Victima and nested paging <ref type="bibr" target="#b11">[12]</ref> in virtualized execution. (conventional TLB blocks are allocated as described in ?5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual Address</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Management Unit Unified L2 Cache Host Page Table Walker</head><p>Host PTE Data Block  <ref type="figure" target="#fig_5">18</ref> shows the address translation flow of a system that employs Victima and nested paging <ref type="bibr" target="#b11">[12]</ref> in virtualized execution. If a nested TLB miss occurs 1 , the MMU probes the L2 cache to search for the nested TLB entry 2 . If the nested TLB entry is found inside the L2 cache, the host-PTW gets skipped 3a . If it is not found, the host-PTW is performs the guestphysical-to-host-physical address translation 3b . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual Address</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Management Unit Unified L2 Cache</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PS TAG TLB-E ASID P0 ? P7</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLB-Entry</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Host Page Table Walker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">TLB Maintainance Operations</head><p>Modern ISAs provide specific instructions used by the OS to invalidate TLB entries and maintain correctness in the presence of (i) context switches and (ii) modifications of virtual-to-physical address mappings (called TLB shootdowns) that occur due to physical page migration, memory de-allocation etc. Different ISAs provide different instructions for TLB invalidations. For example, the ARM v8 architecture <ref type="bibr">[74, D5.10.2]</ref> defines multiple special instructions to invalidate TLB entries with each instruction handling a distinct case (e.g., invalidating a single TLB entry vs invalidating all TLB entries with a specific ASID). x86-64 provides a single instruction, INVLPG, which corresponds to invalidating one single TLB entry <ref type="bibr" target="#b92">[93]</ref>. In Victima, whenever a TLB invalidation is required, the corresponding TLB entries in the L2 cache need to be invalidated. In this section, following the example of the ARM specification, which is a superset of other specifications we know of, we discuss in detail how Victima supports TLB invalidations due to context-switches and TLB shootdowns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Context Switches</head><p>TLB flushing occurs when the OS switches the hardware context and schedules another process (or thread) to the core. In this case, the OS makes a decision on whether or not the TLB entries across the TLB hierarchy should be invalidated, which depends on the ASIDs of the current and to-be-executed processes (in practice Linux uses only 12 different ASIDs per core even though the processor can support up to 4096 ASIDs). In Victima, if the OS flushes the entire TLB hierarchy, all the TLB blocks in L2 cache need to be invalidated as well. If the OS performs a partial flush based on the ASID, all the TLB blocks in L2 cache with the corresponding ASID need to be evicted. In the corner case that Victima uses fewer bits for the ASID, i.e., when L2 cache tag is not large enough to store enough ASID bits to cover the ASID of the process, all the TLB blocks inside the L2 cache get invalidated during a context switch (the L1 and L2 TLB entries can still be invalidated using the ASID). Based on our evaluation setup, for a 2MB L2 cache which is occupied by 50% by TLB blocks, the total time to complete the invalidation procedure is on the order of 100 ns. The invalidation procedure happens in parallel with the L2 TLB invalidation and is negligible compared to context switch completion times (order of ?s <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b94">95]</ref>). (i) Invalidating all TLB entries. To invalidate all the TLB blocks inside the L2 cache, the L2 TLB first sends an invalidation command to the L2 cache controller. The cache controller probes in parallel all cache banks to invalidate all the TLB blocks of every L2 cache set. For each way, if the TLB entry bit is set, the TLB block is invalidated. (ii) Invalidating all TLB entries with a specific ASID. To invalidate all TLB blocks with a specific ASID, the L2 TLB first sends an invalidation command to the L2 cache controller with the corresponding ASID. For every cache block, if the TLB entry bit is set and the ASID matches the ASID of the invalidation request, the TLB block is invalidated. If the size of the ASID of the invalidation command is larger (e.g., 4 bits) than the supported ASID (e.g., 3 bits), then all the TLB blocks inside L2 cache are flushed. However, we believe this is an uncommon case, because, e.g., Linux uses only 12 ASIDs/core <ref type="bibr" target="#b88">[89]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">TLB Shootdowns</head><p>A TLB shootdown occurs when the CPU needs to invalidate stale TLB entries on local and remote cores. It is caused by various memory management operations that modify page table entries, such as de-allocating pages (unmap()), migrating pages, page permission changes, deduplication, and memory compaction. As shown in previous works <ref type="bibr" target="#b95">[96]</ref>, TLB shootdowns take order of ?s time to complete due to expensive inter-processor interrupts (IPIs). In Victima, if the system performs a TLB shootdown, the corresponding TLB blocks need to be invalidated in the L2 cache. We explain how for two different TLB shootdown-based invalidations:</p><p>(i) Invalidating a single TLB entry given VA and ASID. Invalidating a specific TLB entry by VA and ASID only requires sending an invalidation command with the VA and the ASID to the L2 cache controller. Since each TLB block contains eight contiguous TLB entries, invalidating one TLB entry of the TLB block leads to invalidating all eight corresponding TLB entries.</p><p>(ii) Invalidating all TLB entries given a range of VAs. Invalidating a range of VAs requires sending multiple invalidation commands with different VAs to the L2 cache controller. The L2 cache controller accordingly invalidates all the corresponding TLB blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Area &amp; Power Overhead</head><p>Victima requires three additions to an existing high-performance core design: (i) two new TLB Entry bits in every L2 cache block (one of TLB entries and one for nested TLB entries) ( ?5.1), (ii) the PTW cost estimator ( ?5.2) and (iii) the necessary logic to perform tag matching and invalidation of TLB blocks using the TLB Entry bit, the VPN, and the ASID ( ?6). Extending each L2 cache block with two TLB Entry bits results in a 0.4% storage overhead for caches with 64B blocks (e.g., in total 8KB for a 2MB L2 cache). PTW-CP requires only (i) 4 comparators to compare the PTE counters with the corresponding thresholds and (ii) 4 registers to store the thresholds. To support tag matching/invalidation operations for TLB blocks, we extend the tag comparators of the L2 cache with a bitmask to distinguish between tag matching/invalidation for TLB blocks and tag matching/invalidation for conventional data blocks. Based on our evaluation with McPAT <ref type="bibr" target="#b96">[97]</ref>, all additional logic requires 0.04% area overhead and 0.08% power overhead on top of the high-end Intel Raptor Lake processor <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation Methodology</head><p>We evaluate Victima using an extended version of the Sniper Multicore Simulator <ref type="bibr" target="#b41">[42]</ref>. This simulator and its documentation are freely available at https://github.com/CMU-SAFARI/Victima. We extend Sniper to accurately model: (i) TLBs that support multiple page sizes, (ii) the conventional radix page table walk, (iii) page walk caches, (iv) nested TLBs and nested paging <ref type="bibr" target="#b11">[12]</ref> and, (vi) the functionality and timing of all the evaluated systems. Table <ref type="table" target="#tab_11">3</ref> shows the simulation configuration of (i) the baseline system and (ii) all evaluated systems. Workloads. Table <ref type="table" target="#tab_12">4</ref> shows all the benchmarks we use to evaluate Victima and the systems we compare Victima to. We select applications with high L2 TLB MPKI (&gt; 5), which are also used in previous works <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b101">102]</ref>. We evaluate our design using seven workloads from the GraphBig <ref type="bibr" target="#b43">[44]</ref> suite, XSBench <ref type="bibr" target="#b45">[46]</ref>, the Random access workload from the GUPS suite <ref type="bibr" target="#b44">[45]</ref>, Sparse Length Sum from DLRM <ref type="bibr" target="#b46">[47]</ref> and kmer-count from GenomicsBench <ref type="bibr" target="#b47">[48]</ref>. We extract the page size information for each workload from a real system that uses Transparent Huge Pages <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b99">100]</ref> with both 4KB and 2MB pages. Each benchmark is executed for 500M instructions. Evaluated Systems in Native Execution. We evaluate six different systems in native execution environments: (i) Radix: Baseline x86-64 system that uses the conventional (1) two-level TLB hierarchy and (2) four-level radix-based page table, (ii) POM-TLB: a system equipped with a large 64K-entry software-managed L3 TLB <ref type="bibr" target="#b16">[17]</ref> and the TLB-aware SRRIP policy ( ?5.1) at the L2 cache, (iii) Opt. L3TLB-64K: a system equipped with a 64K-entry L3 TLB with an optimistic 15-cycle access latency, (iv) Opt. L2TLB-64K: a  GraphBIG <ref type="bibr" target="#b43">[44]</ref> Betweeness Centrality (BC), Bread-first search (BFS), Connected components (CC), Graph coloring (GC), PageRank (PR), Triangle counting (TC), Shortest-path (SP)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">GB</head><p>XSBench <ref type="bibr" target="#b45">[46]</ref> Particle Simulation (XS) 9 GB GUPS <ref type="bibr" target="#b44">[45]</ref> Random-access (RND) 10 GB DLRM <ref type="bibr" target="#b46">[47]</ref> Sparse-length sum (DLRM) 10.3 GB GenomicsBench <ref type="bibr" target="#b47">[48]</ref> k-mer counting (GEN) 33 GB system equipped with a 64K-entry L2 TLB with an optimistic 12cycle access latency, (v) Opt. L2TLB-128K: a system equipped with a 128K-entry L2 TLB with an optimistic 12-cycle access latency, and (vi) Victima: a system that employs Victima and the TLB-aware SRRIP policy ( ?5.1) at the L2 cache. Evaluated Systems in Virtualized Execution. We evaluate four different systems in virtualized execution environments: (i) Nested Paging (NP): Baseline x86-64 system that uses (1) a two-level TLB hierarchy and (2) a 64-entry Nested TLB and employs Nested Paging <ref type="bibr" target="#b11">[12]</ref>, (ii) POM-TLB: a system equipped with a large 64K-entry software-managed L3 TLB <ref type="bibr" target="#b16">[17]</ref> and the TLB-aware SRRIP policy ( ?5.1) at the L2 cache, (iii) I-SP: a system that employs an ideal version of shadow paging <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b48">49]</ref> where <ref type="bibr" target="#b0">(1)</ref> only a four-level radix shadow page table walk is needed to discover the virtual-to-physical translation and (2) the updates to the shadow page table are performed without incurring performance overhead, and (iv) Victima: a system that employs Victima and caches both TLB and nested TLB entries in the L2 cache which is equipped with the TLB-aware SRRIP policy at the L2 cache.</p><p>9 Evaluation Results   Figures 21 shows the reduction in PTWs achieved by POM-TLB, L2 TLB-64K, L2 TLB-128K and Victima over Radix, in a native execution environment, across 11 workloads. We observe that Victima reduces the number of PTWs by 50%, POM-TLB by 37%, L2 TLB-64K by 37% and L2 TLB-128K by 48% on average across all workloads. L2 TLB-128K and Victima lead to similar reductions in PTWs, which explains the similar performance gains of the two mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Native Execution Environments</head><p>Figure <ref type="figure" target="#fig_0">22</ref> shows the reduction in L2 TLB miss latency for POM-TLB and Victima over Radix. Victima and POM-TLB respectively reduce L2 TLB miss latency by 22% and 3% over Radix. We observe that the latency of accessing POM-TLB nearly nullifies the potential performance gains of reducing PTWs. We conclude that Victima delivers significant performance gains compared to all evaluated systems due to the reduction in the number of PTWs which in turn leads to a reduction in the total L2 TLB miss latency.   <ref type="figure" target="#fig_20">23</ref> shows the translation reach of a processor that uses Victima averaged across 500K execution epochs. 5 We observe that the average translation reach provided by Victima is 36x larger (220 MBs) than the maximum reach offered by the L2 TLB of the baseline system that uses a two-level TLB hierarchy. This is due to the fact that each cache block can cover 32KBs (16MB) of memory while each L2 TLB block covers 4KB (2MB) per entry and the L2 cache typically has significantly more 5 Each epoch consists of 1K instructions and we assume 4KB pages for simplicity.  9.2.2 Reuse of TLB Blocks. Figure <ref type="figure" target="#fig_2">24</ref> shows the reuse distribution of the TLB blocks in the L2 cache (we measure a block's reuse once the block gets evicted from the L2 cache). We observe that the majority of TLB blocks (65%) experience high reuse (i.e., accessed more than 20 times before getting evicted from the L2 cache) due to (i) the accuracy of the PTW-CP (82% average accuracy across all workloads) and (ii) the prioritization of the TLB blocks by the TLB-aware replacement policy used in the L2 cache. We conclude that Victima effectively utilizes underutilized L2 cache resources to store high-reuse TLB blocks. In a system without Victima, accessing these TLB blocks would lead to high-latency PTWs.  employing the TLB-aware SRRIP leads to 1.8% higher performance compared to the conventional SRRIP. We conlude that Victima can deliver high performance with both TLB-aware and TLB-agnostic replacement policies.  Figure <ref type="figure" target="#fig_24">28</ref> shows the reduction in guest and host PTWs for all the configurations. We observe that Victima leads to significant reductions in both guest PTWs (50%) and host PTWs (99%). The host PTW is the major bottleneck in NP, and Victima almost eliminates it by caching nested TLB blocks inside the L2 cache.  Figure <ref type="figure" target="#fig_25">29</ref> shows the L2 TLB miss latency for all the configurations normalized to NP. We observe that Victima minimizes host PTW latency to as low as 1% of the baseline while reducing the guest translation by 60%, 6% more than I-SP, which performs only four PT accesses to find out the guest-virtual to host-physical translation. We conclude that caching both nested and conventional TLB entries in the L2 cache allows Victima to achieve high performance in both native and virtualized environments. 10 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Diving Deeper into Victima</head><p>To our knowledge, Victima is the first software-transparent mechanism that proposes caching TLB entries in the cache hierarchy to increase the translation reach of the processor. We have already comprehensively compared Victima to (i) systems that employ large hardware TLBs and large software-managed TLBs <ref type="bibr" target="#b16">[17]</ref> in native execution environments and (ii) systems that employ nested paging <ref type="bibr" target="#b11">[12]</ref>, large software-managed TLBs <ref type="bibr" target="#b16">[17]</ref> and ideal shadow paging <ref type="bibr" target="#b48">[49]</ref> in virtualized environments in ?9.1 and ?9.3. In this section, we qualitatively compare Victima to other related prior works that propose solutions to reduce address translation overheads.</p><p>Efficient TLBs and Page Walk Caches (PWCs). Many prior works focus on reducing address translation overheads through efficient TLB and PWC designs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b102">[103]</ref><ref type="bibr" target="#b103">[104]</ref><ref type="bibr" target="#b104">[105]</ref><ref type="bibr" target="#b105">[106]</ref><ref type="bibr" target="#b106">[107]</ref><ref type="bibr" target="#b107">[108]</ref><ref type="bibr" target="#b108">[109]</ref><ref type="bibr" target="#b109">[110]</ref><ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref><ref type="bibr" target="#b113">[114]</ref><ref type="bibr" target="#b114">[115]</ref><ref type="bibr" target="#b115">[116]</ref>. Such techniques involve: (i) prefetching TLB and page table entries <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b110">[111]</ref><ref type="bibr" target="#b111">[112]</ref><ref type="bibr" target="#b112">[113]</ref><ref type="bibr" target="#b113">[114]</ref><ref type="bibr" target="#b114">[115]</ref>, (ii) TLB-specific replacement policies <ref type="bibr" target="#b102">[103,</ref><ref type="bibr" target="#b116">117]</ref>, (iii) employing software-managed TLBs <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, (iv) sharing TLBs across cores <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, (v) employing efficient PWCs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b117">118]</ref>, and (vi) PT-aware cache management <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b119">120</ref>] (e.g., pinning PTEs in the LLC <ref type="bibr" target="#b118">[119]</ref>). Although such techniques may offer notable performance improvements, as the page table size increases, their effectiveness reduces. This is because they rely on (i) the existing TLB hierarchy that is unable to accommodate the large number of TLB entries required by data-intensive applications or (ii) new hardware/software translation structures that pose a significant trade-off between performance and area/energy efficiency ( ?3). In contrast, Victima repurposes the existing underutilized resources of the cache hierarchy to drastically increase address translation reach and thus does not require additional structures to store translation metadata. For example, as we show in ?3.2, employing a software-managed TLB to back up the L2 TLB is not effective in native environments as the latency of the PTW is similar to the latency of accessing the software-managed TLB. In ?9. . For example, in <ref type="bibr" target="#b0">[1]</ref>, the authors propose pre-allocating arbitrarily-large contiguous physical regions (10-100's of GBs) to drastically increase the translation reach for specific data structures of the application. Karakostas et al. <ref type="bibr" target="#b143">[144]</ref> propose the use of multiple dynamicallyallocated contiguous physical regions, called ranges, to provide efficient address translation for a small number of large memory objects used by the application. Alverti et al. <ref type="bibr" target="#b80">[81]</ref> propose an OS mechanism that enables efficient allocation of large contiguous physical regions. These works can significantly increase translation reach, but, in general have two drawbacks: (i) they require system software modifications and (ii) their effectiveness heavily depends on the availability of free contiguous memory blocks. In contrast to these works, Victima increases the translation reach of the processor without requiring (i) contiguous physical memory allocations or (ii) modifications to the system software. Address Translation in Virtualized Environments. Various works propose techniques to reduce address translation overheads in virtualized environments <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b133">134,</ref><ref type="bibr" target="#b147">[148]</ref><ref type="bibr" target="#b148">[149]</ref><ref type="bibr" target="#b149">[150]</ref><ref type="bibr" target="#b150">[151]</ref><ref type="bibr" target="#b150">[151]</ref><ref type="bibr" target="#b151">[152]</ref><ref type="bibr" target="#b152">[153]</ref><ref type="bibr" target="#b153">[154]</ref>. For example, Ghandi et al. <ref type="bibr" target="#b48">[49]</ref> propose a hybrid address translation design for virtualized environments that combines shadow paging and nested paging. In ?9. Data-intensive workloads experience frequent and long-latency page table walks. This paper introduces Victima, a software-transparent technique that stores TLB entries in the cache hierarchy to drastically increase the translation reach of the processor and thus reduces the occurence of page table walks. Our evaluation shows that Victima provides significant performance improvements in both native and virtualized environments. Victima presents a practical opportunity to improve the performance of data-intensive workloads with small hardware changes, modest area and power overheads, and no modifications to software, by repurposing the underutilized resources of the cache hierarchy.</p><p>? tar (GNU tar) 1.34 Data Sets. The Sniper traces required to evaluate Victima will be downloaded automatically using the supplied scripts. All traces are uploaded in Google Cloud Storage under this link: https://storage. googleapis.com/traces_virtual_memory/traces_victima</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Experiment Workflow</head><p>This section describes steps to install all required software and execute necessary experiments. We recommend the reader to follow the README file to know more about each script used in this section.</p><p>Installation. The following command line instructions will install all software packages for Docker or Podman.</p><p>(1) Specify Podman or Docker:</p><p>$ ~/Victima$ sh install_container.sh podman | docker</p><p>Launching Experiments. The following script downloads all traces and launches all experiments required to reproduce the key results. The following command directly executes neural network inference to reproduce the results shown in Table <ref type="table" target="#tab_9">2</ref>. We strongly recommend using a compute cluster with Slurm support to efficiently launch experiments in bulk. We have set the maximum memory usage of each slurm job as 10GB and the maximum timeout as 3 days, in order to make sure that all experiments will run correctly.</p><p>(1) To launch your experiments execute : $ cm pull repo mlcommons@ck &amp;&amp; cm pull repo CMU-SAFARI@Victima</p><p>The CM scripts for Victima will be available under: /CM/repos/CMU-SAFARI@Victima/script/. Perform the following steps to evaluate Victima with MLCommons:</p><p>(1) $ cm run script micro-2023-461:install_dep \ -env.CONTAINER_461="docker"</p><p>(2) $ cm run script micro-2023-461:run-experiments \ -env.EXEC_MODE_461="-slurm" | native \ -env.CONTAINER_461="docker" | "podman"</p><p>(3) $ cm run script micro-2023-461:produce-plots \ -env.CONTAINER_461="docker" | "podman"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Evaluation &amp; Expected Results</head><p>The experiments evaluate (i) a system with different L2 TLB sizes, a system that employs an L3 TLB, POM-TLB <ref type="bibr" target="#b16">[17]</ref> and Victima in native execution environments as well as (ii) nested paging <ref type="bibr" target="#b11">[12]</ref>, POM-TLB <ref type="bibr" target="#b16">[17]</ref>, ideal shadow paging <ref type="bibr" target="#b48">[49]</ref> and Victima in virtualized environments.</p><p>? Increasing the L2 TLB size up to 64K entries should lead to 4.0% higher performance compared to the baseline system (Fig. <ref type="figure" target="#fig_4">6</ref>) and L2 TLB MPKI reduction from 39.4 to 24.3 (Fig. <ref type="figure">5</ref>).</p><p>Using an L3 TLB should lead to 2.9% higher performance compared to the baseline system (Fig. <ref type="figure" target="#fig_5">8</ref>). ? 92% of L2 data blocks should experience a reuse of 0 and 8% should experience a reuse higher than 1 (Fig. <ref type="figure" target="#fig_8">11</ref>). ? The comparator-based PTW-CP should achieve 89% Recall, 82% Accuracy, 73% Precision and 80% F1-Score (Table <ref type="table" target="#tab_9">2</ref>). ? Victima should outperform Baseline, POM-TLB, Optimistic L3 TLB 64K, Optimistic L2 TLB 64K, Optimistic L2 TLB 128K, by 7.4%, 6.2%, 4.4%, 3.3%, and 0.3% respectively in native execution environments (Fig. <ref type="figure" target="#fig_16">20</ref>). ? Victima should reduce the number of PTWs by 50% compared to the baseline system (Fig. <ref type="figure" target="#fig_18">21</ref>). ? Victima should provide 220 MBs of translation reach (Fig. <ref type="figure" target="#fig_20">23</ref>).</p><p>? Victima should outperform Nested Paging, POM-TLB, and Ideal Shadow Paging by 28.7%, 20.1%, and 4.9% respectively in virtualized environments (Fig. <ref type="figure" target="#fig_22">27</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Methodology</head><p>Submission, reviewing and badging methodology:</p><p>? https://www.acm.org/publications/policies/artifact-review-badging ? http://cTuning.org/ae/submission-20201122.html ? http://cTuning.org/ae/reviewing-20201122.html</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structure of the Memory Management Unit (MMU) of a modern processor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MMU extensions to support address translation in virtualized environments using Nested Paging [12].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of PTW latency.Previous works propose various solutions to reduce the high cost of address translation and increase the translation reach of the TLBs such as employing (i) large hardware TLBs<ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> or (ii) backing up the last-level TLB with a large software-managed TLB<ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. We examine these solutions and their shortcomings in ?3.1 and ?3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Speedup provided by larger L2 TLBs with equal access latencies (i.e., 12 cycles) over the baseline system (1.5Kentry L2 TLB).Unfortunately, increasing the TLB size does not come for free: it leads to larger access latency (as well as area and power), which counteracts the potential performance benefits due to fewer PTWs. For instance, according to CACTI 7.0<ref type="bibr" target="#b25">[26]</ref>, the latency of accessing a 64K-entry large TLB is as high as 39 cycles. Figure7shows the execution time speedup of realistic L2 TLB configurations with increasing sizes, while the access latency is adjusted based on the size of the TLB (based on CACTI 7.0 modeling<ref type="bibr" target="#b25">[26]</ref>), compared to the baseline system (1.5K-entry L2 TLB with 12-cycle access latency). We observe that in this realistic setting, the performance benefits of increasing the L2 TLB size are significantly lower compared to the optimistic setting (Fig.6). The realistic 64K-entry configuration (that reduces MPKI by 44%, but comes with a 39-cycle access latency) leads to only 0.8% higher average performance over the baseline configuration. We conclude that although increasing the L2 TLB size reduces PTWs, it comes with increased access latency (as well as power and area), which leads to small performance benefits realistically.Increasing the size of the L2 TLB has a negative impact on the translation latency of requests that hit in the L2 TLB. Therefore, to keep the access latency of the L2 TLB small, we also explore a scenario where the TLB hierarchy is extended with a large hardware L3 TLB. Figure8shows the execution time speedup achieved by a system with a 64K-entry L3 TLB with increasing access latencies, ranging from 15 cycles up to 39 cycles (which is the latency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Speedup provided by adding a 64K-entry L3 TLB with different access latencies over the baseline system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9</head><label>9</label><figDesc>Figure9: L2 TLB miss latency of (i) baseline system in native execution, (ii) system with STLB<ref type="bibr" target="#b16">[17]</ref> in native execution, (iii) baseline system in virtualized execution and, (iv) STLB in virtualized execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Reduction in L2 TLB miss latency when L1/L2/LLC serve all L2 TLB misses over the baseline system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Reuse-level distribution of L2 cache blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>VictimaFigure 12 :</head><label>12</label><figDesc>Figure12: Address translation flow in a conventional baseline processor<ref type="bibr" target="#b49">[50]</ref> and Victima.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Conventional data block layout (top) and conventional TLB block layout for the same address (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Insertion of a TLB block into the L2 cache upon (i) an L2 TLB miss and (ii) an L2 TLB eviction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: PageTable Walk Cost Predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Prediction pattern of NN-2. The bounding box separates the PTW cost-frequency pairs that lead to positive predictions (inside the box) from the ones that lead to negative predictions (outside the box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Address translation flow in a system with Victima.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Address translation flow in a system with Victima in a virtualized execution environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 20 shows</head><label>20</label><figDesc>Figure 20  shows the execution time speedup provided by POM-TLB, Opt. L3TLB-64K, Opt. L2TLB-64K, Opt. L2TLB-128K and Victima compared to Radix. We make two key observations: First, Victima on average respectively outperforms Radix, POM-TLB, Opt. L3TLB-64K, Opt. L2TLB-64K, by 7.4%, 6.2%, 4.4%, 3.3%. In RND, which follows highly irregular access patterns, Victima improves performance by 28% over Radix. Second, Victima achieves similar performance gains as Opt.L2-TLB 128K without the latency/area/power overheads associated with an 128K-entry TLB. To better understand the performance benefits achieved by Victima, we examine the impact of Victima on (i) the number of PTWs and (ii) the L2 TLB miss latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Speedup provided by POM-TLB, Opt. L3TLB-64K, Opt. L2TLB-64K, Opt. L2TLB-128K and Victima over Radix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>9. 2 . 1</head><label>21</label><figDesc>Translation Reach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Translation reach provided by TLB blocks stored in L2 cache (assuming 4KB page size).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: Victima's reduction in PTWs across different L2 cache sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 27 :</head><label>27</label><figDesc>Figure 27: Speedup provided by POM-TLB, I-SP and Victima in a virtualized system with NP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>Guest PTW POM-TLB Host PTW Victima Guest PTW Victima Host PTW</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 28 :</head><label>28</label><figDesc>Figure 28: Reduction in host and guest PTWs provided by POM-TLB and Victima in a virtualized system with NP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 29 :</head><label>29</label><figDesc>Figure 29: L2 TLB miss latency in POM-TLB, I-SP and Victima normalized to NP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>table that stores guest-virtual to guest-physical address mappings and the host page table that stores guest-physical to host-physical address mappings. To search for the mapping between a guest-virtual page to a host-physical page, NP performs a two-dimensional walk, since a host page table walk is required for each level of the guest page table walk. Therefore, in a virtualized environment with a four-level radix-tree-based PT, NP-based address translation can cause up to 24 sequential memory accesses (a 6? increase in memory accesses compared to the native execution environment).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 :</head><label>1</label><figDesc>Per-Page Feature Set</figDesc><table><row><cell>Feature (per PTE)</cell><cell cols="2">Bits Description</cell></row><row><cell>Page Size</cell><cell>1</cell><cell>The size of the page (4KB or 2MB)</cell></row><row><cell>Page Table Walk Frequency</cell><cell>3</cell><cell># of PTWs for the page</cell></row><row><cell>Page Table Walk Cost</cell><cell>4</cell><cell># of DRAM accesses during all PTWs</cell></row><row><cell>PWC Hits</cell><cell>5</cell><cell># of times the PTW led to PWC hit</cell></row><row><cell>L1 TLB Misses</cell><cell>5</cell><cell># of times the page experienced L1 TLB miss</cell></row><row><cell>L2 TLB Misses</cell><cell>5</cell><cell># of times the page experienced L2 TLB miss</cell></row><row><cell>L2 Cache Hits</cell><cell>5</cell><cell># of times the page experienced L2 cache hits</cell></row><row><cell>L1 TLB Evictions</cell><cell>5</cell><cell># of times the TLB entry got evicted from L1 TLB</cell></row><row><cell>L2 TLB Evictions</cell><cell>6</cell><cell># of times the TLB entry got evicted from L2 TLB</cell></row><row><cell>Accesses</cell><cell>6</cell><cell># of accesses to the page</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Different Types of PTW-CP</figDesc><table><row><cell>Model Parameters</cell><cell>NN-10</cell><cell>NN-5</cell><cell>NN-2</cell><cell>Comparator</cell></row><row><cell># of Features</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>2</cell></row><row><cell>Number of Layers</cell><cell>4</cell><cell>4</cell><cell>6</cell><cell>N/A</cell></row><row><cell>Size of Hidden Layers</cell><cell>16</cell><cell>64</cell><cell>4</cell><cell>N/A</cell></row><row><cell>Size (B)</cell><cell>6024</cell><cell>70152</cell><cell>776</cell><cell>24</cell></row><row><cell>Recall</cell><cell>93.34%</cell><cell>92.44%</cell><cell>89.62%</cell><cell>89.61%</cell></row><row><cell>Accuracy</cell><cell>92.13%</cell><cell>91.72%</cell><cell>82.90%</cell><cell>82.90%</cell></row><row><cell>Precision</cell><cell>87.68%</cell><cell>87.47%</cell><cell>73.33%</cell><cell>73.34%</cell></row><row><cell>F1-score</cell><cell>90.42%</cell><cell>89.89%</cell><cell>80.66%</cell><cell>80.66%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>When a Nested TLB miss occurs, the MMU consults the PTW-CP to find out if the host-virtual page will be costly-totranslate in the future 1 . If the prediction is positive, the MMU checks if the corresponding nested TLB block already resides inside the L2 cache 2 . If it does, no further action is needed. If not, the MMU first waits until the host-PTW is completed. When the last level of the host-PT is fetched 3b , the MMU transforms the cache block that contains the host-PTEs to a nested TLB block by updating the metadata of the block 4 . The MMU (i) replaces the existing tag with the tag of the host-virtual page region, (ii) sets the nested TLB bit to mark the cache block as a nested TLB block, and (iii) updates the ASID (or VMID) and the page size information. Inserting a Nested TLB Block into the L2 Cache upon a Nested TLB Eviction. When a Nested TLB eviction occurs, the MMU consults the PTW-CP to find out if the host-virtual page will be costly-to-translate in the future 1 . If the outcome of the prediction is positive, the MMU checks if the corresponding nested TLB block already resides in the L2 cache 2 . If it does, no further action is needed. If it does not, the MMU issues in the background a host-PTW for the corresponding TLB entry 3a . When the last level of the host-PT is fetched 3b , the MMU transforms the cache block that contains the host-PTEs to a nested TLB block. 4 . Address Translation Flow. Figure</figDesc><table><row><cell>Nested TLB</cell><cell>1</cell><cell cols="2">PTW Cost</cell><cell>Nested TLB Block</cell><cell></cell></row><row><cell></cell><cell>Miss | Eviction</cell><cell cols="2">Estimator</cell><cell></cell><cell></cell></row><row><cell cols="2">(only for eviction) Start nested PTW</cell><cell>3a Costly</cell><cell>2</cell><cell>PS NestedTLB Block TAG TLB-E ASID</cell><cell>?</cell><cell>4</cell></row></table><note><p>3b Figure 18: Insertion of a nested TLB block into the L2 cache upon (i) a nested TLB miss and (ii) a nested TLB eviction Inserting a Nested TLB Block into the L2 Cache upon a Nested TLB Miss.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3 :</head><label>3</label><figDesc>Simulation Configuration and Simulated Systems</figDesc><table><row><cell></cell><cell>Baseline System</cell></row><row><cell>Core</cell><cell>4-way OoO x86-64 2.6GHz</cell></row><row><cell></cell><cell>L1 I-TLB: 128-entry, 8-way assoc, 1-cycle latency</cell></row><row><cell></cell><cell>L1 D-TLB (4KB): 64-entry, 4-way assoc, 1-cycle latency</cell></row><row><cell>MMU</cell><cell>L1 D-TLB (2MB): 32-entry, 4-way assoc, 1-cycle latency</cell></row><row><cell></cell><cell>L2 TLB: 1536-entry, 12-way assoc, 12-cycle latency</cell></row><row><cell></cell><cell>3 Split Page Walk Caches: 32-entry, 4-way assoc, 2-cycle latency</cell></row><row><cell></cell><cell>L1 I-Cache: 32 KB, 8-way assoc, 4-cycle access latency</cell></row><row><cell>L1 Cache</cell><cell>L1 D-Cache: 32 KB, 8-way assoc, 4-cycle access latency</cell></row><row><cell></cell><cell>LRU replacement policy; IP-stride prefetcher [98]</cell></row><row><cell></cell><cell>2 MB, 16-way assoc, 16-cycle latency</cell></row><row><cell>L2 Cache</cell><cell>SRRIP replacement policy [91]; Stream prefetcher [99]</cell></row><row><cell>L3 Cache</cell><cell>2 MB/core, 16-way assoc, 35-cycle latency</cell></row><row><cell>Transparent</cell><cell>Debian 9 4.14.2. 10-node cluster</cell></row><row><cell>Huge Pages [77, 100]</cell><cell>Memory per node: 256GB-1TB</cell></row><row><cell></cell><cell>Evaluated Systems</cell></row><row><cell></cell><cell>64K-entry L3 software-managed TLB, 16-way assoc</cell></row><row><cell>POM-TLB [17]</cell><cell>TLB-aware SRRIP replacement policy ( ?5.1)</cell></row><row><cell></cell><cell>1.5K-entry L2 TLB, 12-cycle latency</cell></row><row><cell>Opt. L3 TLB-64K</cell><cell>64K-entry L3 TLB, optimistic 15-cycle latency</cell></row><row><cell>Opt. L2 TLB-64K</cell><cell>64K-entry L2 TLB, 16-way assoc, optimistic 12-cycle latency</cell></row><row><cell>Opt. L2 TLB-128K</cell><cell>128K-entry L2 TLB, 16-way assoc, optimistic 12-cycle latency</cell></row><row><cell>Nested Paging [12]</cell><cell></cell></row></table><note><p>2D PTW; Guest PT: Four-level Radix, Host PT: Four-level Radix 64-entry Nested TLB, 1-cycle latency Ideal Shadow Paging (I-SP) [49] 1D Shadow PTW instead of 2D PTW Updates to shadow page table cause no performance overheads Victima MMU consults PTW-CP only if L2 cache MPKI &lt; 5 ( ?5.2) TLB-aware SRRIP replacement policy ( ?5.1)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 :</head><label>4</label><figDesc>Evaluated Workloads</figDesc><table><row><cell>Suite</cell><cell>Workload</cell><cell>Dataset size</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Figure27shows the execution time speedup of POM-TLB, I-SP and Victima over Nested Paging, in a virtualized execution environment, across 11 workloads. We observe that Victima outperforms Nested Paging on average by 28.7%, I-SP by 4.9%, and POM-TLB by 20.1%, across all workloads. To better understand the performance speedup achieved by Victima, we examine the impact of Victima on (i) the number of guest and host PTWs and (ii) the L2 TLB miss latency and the nested TLB miss latency.</figDesc><table><row><cell>Speedup over SRRIP</cell><cell>0.90 1.00 1.10</cell><cell>BC</cell><cell>BFS</cell><cell>CC</cell><cell>DLRM</cell><cell>GEN</cell><cell>GC</cell><cell>PR</cell><cell>RND</cell><cell>SSSP</cell><cell>TC</cell><cell>XS</cell><cell>GMEAN</cell></row><row><cell cols="14">Figure 26: Performance improvement provided by Victima</cell></row><row><cell cols="14">with the TLB-aware SRRIP replacement policy over Victima</cell></row><row><cell cols="7">with TLB-agnostic SRRIP.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">9.3 Virtualized Environments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Speedup over NP</cell><cell>0.50 0.75 1.00 1.25 1.50</cell><cell></cell><cell></cell><cell>POM-TLB</cell><cell></cell><cell></cell><cell cols="3">Ideal Shadow Paging</cell><cell>1.57x</cell><cell cols="2">Victima</cell><cell></cell></row><row><cell></cell><cell></cell><cell>BC</cell><cell>BFS</cell><cell>CC</cell><cell>DLRM</cell><cell>GEN</cell><cell>GC</cell><cell>PR</cell><cell>RND</cell><cell>SSSP</cell><cell>TC</cell><cell>XS</cell><cell>GMEAN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>Victima is complementary to these techniques as it reduces PTWs while these techniques reduce PTW latency. Employing Large Pages. Many works propose hardware and software mechanisms for efficient and transparent support for pages of varying sizes<ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b128">[129]</ref><ref type="bibr" target="#b129">[130]</ref><ref type="bibr" target="#b130">[131]</ref><ref type="bibr" target="#b131">[132]</ref><ref type="bibr" target="#b132">[133]</ref><ref type="bibr" target="#b133">[134]</ref><ref type="bibr" target="#b134">[135]</ref><ref type="bibr" target="#b135">[136]</ref><ref type="bibr" target="#b136">[137]</ref><ref type="bibr" target="#b137">[138]</ref><ref type="bibr" target="#b138">[139]</ref><ref type="bibr" target="#b139">[140]</ref><ref type="bibr" target="#b140">[141]</ref><ref type="bibr" target="#b141">[142]</ref>. For example, Ram et al.<ref type="bibr" target="#b75">[76]</ref> propose harnessing memory resources to provide 1GB pages to applications in an application-transparent manner. Guvenilir et al.<ref type="bibr" target="#b129">[130]</ref> propose modifications to the existing radix-based page table design to support a wide range of different page sizes. As we discuss in ?5.1, Victima is able to cache TLB entries for any page size and thus is compatible with large pages. Contiguity-Aware Address Translation. Many prior works enable and exploit virtual-to-physical address contiguity to perform low-latency address translation<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b142">[143]</ref><ref type="bibr" target="#b143">[144]</ref><ref type="bibr" target="#b144">[145]</ref><ref type="bibr" target="#b145">[146]</ref><ref type="bibr" target="#b146">[147]</ref></figDesc><table><row><cell>3 and  ?9.1, we</cell></row><row><cell>compare Victima against state-of-the-art software-managed TLB,</cell></row><row><cell>POM-TLB [17] and show that Victima outperforms POM-TLB by</cell></row><row><cell>6.2% (20.1%) in native (virtualized) environments by storing TLB</cell></row><row><cell>entries in the high-capacity and low-latency L2 cache.</cell></row><row><cell>Alternative Page Table Designs. Various prior works focus on</cell></row><row><cell>alternative page table designs [9, 85-87, 101, 121-127] to acceler-</cell></row><row><cell>ate PTWs. For example, Skarlatos et al. [85] propose replacing the</cell></row></table><note><p><p><p><p><p>radix-tree-based page table with a Cuckoo hash table</p><ref type="bibr" target="#b127">[128]</ref> </p>to parallelize accesses to the page table and reduce PTW latency. Park et al.</p><ref type="bibr" target="#b86">[87]</ref> </p>propose a flat page table design in combination with a pagetable-aware replacement policy to reduce PTW latency.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc><ref type="bibr" target="#b2">3</ref>, we show that Victima is effective in virtualized environments and outperforms an ideal shadow paging design by 4.9% by storing both TLB entries and nested TLB entries in the cache hierarchy. Virtual Caching &amp; Intermediate Address Spaces. Another class of works focuses on delaying address translation by using techniques such as virtual caching<ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b154">[155]</ref><ref type="bibr" target="#b155">[156]</ref><ref type="bibr" target="#b156">[157]</ref><ref type="bibr" target="#b157">[158]</ref><ref type="bibr" target="#b158">[159]</ref><ref type="bibr" target="#b159">[160]</ref><ref type="bibr" target="#b160">[161]</ref> and intermediate address spaces<ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b161">[162]</ref><ref type="bibr" target="#b162">[163]</ref><ref type="bibr" target="#b163">[164]</ref>. Virtually-indexed caches reduce address translation overheads by performing address translation only after a memory request misses in the LLC<ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b156">157,</ref><ref type="bibr" target="#b157">158,</ref><ref type="bibr" target="#b164">165]</ref>. Hajinazar et al.<ref type="bibr" target="#b162">[163]</ref> propose the use of virtual blocks mapped to an intermediate address space to delay address translation until an LLC miss. Victima is orthogonal to these techniques and can operate with both (i) virtually-indexed caches 6 and (ii) intermediate address spaces by storing TLB blocks with intermediate-to-physical address mappings in the cache hierarchy.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>? Kernel: 5.15.0-56-generic ? Dist: Ubuntu SMP 22.04.1 LTS (Jammy Jellyfish)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>All results are stored under ./results. Execute the following command to: (1) Parse the results of the experiments. (2) Generate Figures 5, 6, 8, 11, 20-21, 23-25, 27, 28. All figures can be found under: /path/to/Victima/plots/ $ ~/Victima$ sh ./scripts/produce_plots.sh docker | podman B Reusability using MLCommons We added support to evaluate Victima using the MLCommons CM automation language: https://github.com/mlcommons/ck. Make sure you have installed CM. Follow the guide under https://github. com/mlcommons/ck/blob/master/docs/installation.md to install it. Next, install reusable MLCommons automations and pull this repository via CM::</figDesc><table /><note><p>$ ~/Victima$ sh artifact.sh --slurm docker | podman Parse Results &amp; Generate Figures.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>?8 describes our evaluation methodology in detail.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The x-axis of Figure4is cut off (at 190 cycles) since only 0.2% of the PTWs take more than 190 cycles to complete. Maximum observed PTW latency is 608 cycles.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Each 64-byte TLB block can store up to 8 8-byte PTEs. Victima uses the 3 least significant bits of the virtual page number to identify and access a specific PTE.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>If ?? ?????? ? (? ? ?????? -9), a single VA can map to different TLB blocks. This is because the tag of the TLB block does not fit inside the hardware tag entry of the L2 cache.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Victima distinguishes between data blocks and TLB entries by using a tag bit in the cache block, regardless of whether the cache is virtually-or physically-indexed.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the anonymous reviewers of MICRO 2023 for their encouraging feedback. We thank the <rs type="institution">SAFARI Research Group members</rs> for providing a stimulating intellectual environment. We acknowledge the generous gifts from our industrial partners: Google, <rs type="person">Huawei</rs>, <rs type="funder">Intel</rs>, <rs type="funder">Microsoft</rs>, and <rs type="funder">VMware</rs>. This work is supported in part by the <rs type="funder">Semiconductor Research Corporation</rs> and the <rs type="institution">ETH Future Computing Laboratory</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Artifact Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Abstract</head><p>We implement Victima using the Sniper simulator <ref type="bibr" target="#b41">[42]</ref>. In this artifact, we provide the source code of Victima and necessary instructions to reproduce its key performance results. We identify four key results to demonstrate and analyze Victima's novelty and effectiveness:</p><p>? Execution time speedup and MPKI reduction achieved by increasing L2 TLB size and by using an L3 TLB. The artifact can be executed in any machine with a generalpurpose CPU and 10 GB disk space. However, we strongly recommend running the artifact on a compute cluster with slurm <ref type="bibr" target="#b165">[166]</ref> support for bulk experimentation.</p><p>A.2 Artifact Check-list (Meta-information)</p><p>? Compilation: Container-based compilation.</p><p>? Data set: Download traces using the supplied script. ? The experiments were executed using Slurm <ref type="bibr" target="#b165">[166]</ref>.</p><p>We strongly suggest executing the experiments using such an infrastructure for bulk experimentation. However, we provide support for non-slurm-based, native execution. ? Each experiment takes ?8-10 hours to finish and requires about ?5-13GB of free memory (depends on the experiment). ? The workload traces require ? 10GB of storage space.</p><p>? Hardware infrastructure used to run the experiments:</p><p>(i) Nodes: Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GH and (ii) Slurm <ref type="bibr" target="#b165">[166]</ref>  To execute experiments with a container, we need the following software (all packages will be automatically downloaded using the provided scripts): </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient Virtual Memory for Big Memory Servers</title>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Performance Analysis of the Memory Management Unit Under Scale-out Workloads</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Unsal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IISWC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translation Caching: Skip, Don&apos;t Walk (the Page Table)</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://docs.kernel.org/x86/x86_64/5level-paging.html" />
		<title level="m">Linux. 5 Level Paging</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contiguitas: the Pursuit of Physical Memory Contiguity in Datacenters</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Schatzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonis</forename><surname>Manousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rik Van Riel, Bikash Sharma, Chunqiang Tang, and Dimitrios Skarlatos</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Radiant: Efficient Page Table Management for Tiered Memory Systems</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinda</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Smruti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Sarangi</surname></persName>
		</author>
		<author>
			<persName><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Characterizing the TLB Behavior of Emerging Parallel Workloads On Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devirtualizing Memory in Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Haria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hash, Don&apos;t Cache (the Page Table)</title>
		<author>
			<persName><forename type="first">Idan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Performance ImplicatiOns of Extended Page Tables On Virtualized X86 Processors</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Merrifield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Taheri</surname></persName>
		</author>
		<editor>VEE</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Study of Virtual Memory Usage and Implications for Large Memory</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hornyack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Gribble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hank</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AMD-V Nested Paging</title>
		<ptr target="http://developer.amd.com/wordpress/media/2012/10/NPT-WP-1%201-final-TM.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advanced Micro Devices</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Inc</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/compute/docs/instances/enable-nested-virtualization-vm-instances" />
		<title level="m">Compute Engine: Enabling Nested Virtualization for VM Instances</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shared Last-Level TLBs for Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scalable Distributed Last-Level TLBs Using Low-Latency Interconnects</title>
		<author>
			<persName><forename type="first">Srikant</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving GPU Multi-tenancy with Page Walk Stealing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pratheek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Jawalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RethInking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB</title>
		<author>
			<persName><forename type="first">Jee</forename><surname>Ho Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CSALT: Context Switch Aware Large TLB</title>
		<author>
			<persName><forename type="first">Yashwant</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Gulur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jee</forename><surname>Ho Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparisons of Memory Virtualization Solutions for Architectures with Software-Managed TLBs</title>
		<author>
			<persName><forename type="first">Yunfang</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving Virtualization in the Presence of Software Managed Translation Lookaside Buffers</title>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubertus</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimi</forename><surname>Xenidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Design Tradeoffs for Software-Managed TLBs</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Sechrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOCS</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Look At Several Memory Management Units, TLB-Refill Mechanisms, and Page Table OrganizAtions</title>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Software-Controlled Caches in the VMP Multiprocessor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Slavenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Design Tradeoffs for Software-managed TLBs</title>
		<author>
			<persName><forename type="first">David</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Sechrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">N</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Software Prefetching and Caching for Translation Lookaside Buffers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kavita Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">E</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><surname>Weihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<title level="m">CACTI 7.0: A Tool to Model Large Caches. HP laboratories</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Criticality Aware Tiered Cache Hierarchy: A Fundamental Relook At Multi-Level Cache Hierarchies</title>
		<author>
			<persName><forename type="first">Anant</forename><surname>Vithal Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Clearing the Clouds: A Study of Emerging Scale-Out Workloads On Modern Hardware</title>
		<author>
			<persName><forename type="first">Almutaz</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cansu</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">Daniel</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Harvesting L2 Caches in Server Processors</title>
		<author>
			<persName><forename type="first">Majid</forename><surname>Jalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks</title>
		<author>
			<persName><forename type="first">Geraldo</forename><forename type="middle">F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>G?mez-Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lois</forename><surname>Orosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sadrosadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain-specialized Cache Management for Graph Analytics</title>
		<author>
			<persName><forename type="first">Priyank</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Analysis and Optimization of the Memory Hierarchy for Graph Processing Workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Many-Core Graph Workload Analysis</title>
		<author>
			<persName><forename type="first">Stijn</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wim</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Bois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Fryman</surname></persName>
		</author>
		<author>
			<persName><surname>Hur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Accelerating Long-Latency Load Requests Via Perceptron-Based Off-Chip Load Prediction</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Shankar Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ataberk</forename><surname>Novo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Olgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Sadrosadati</surname></persName>
		</author>
		<author>
			<persName><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Hermes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Line Distillation: Increasing Cache Capacity By Filtering Unused Words in Cache Lines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Aater Suleman</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive Spill-Receive for Robust high-performance Caching in CMPs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive Insertion Policies for High Performance Caching</title>
		<author>
			<persName><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Scalable Processing-in-Memory Accelerator for Parallel Graph Processing</title>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungpack</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoung</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stream-based Memory Access Specialization for General Purpose Processors</title>
		<author>
			<persName><forename type="first">Zhengrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Dynamic Granularity Memory System</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<ptr target="https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html" />
		<title level="m">The Linux Kernel</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sniper: Exploring the Level of Abstraction for Scalable and Accurate Parallel Multi-Core Simulations</title>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wim</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<ptr target="https://github.com/CMU-SAFARI/Victima" />
		<title level="m">Victima -Github Repository</title>
		<imprint/>
		<respStmt>
			<orgName>SAFARI Research Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">GraphBIG: Understanding Graph Computing in the Context of Industrial Solutions</title>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Tanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Simple Synchronous Distributed-Memory Algorithm for the HPCC RandomAccess Benchmark</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Plimpton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Brightwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtenay</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">XSBench -The Development and Verification of a Performance Abstraction for Monte Carlo Reactor Analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>John R Tramm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanzima</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PHYSOR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Learning Recommendation Model for Personalization and Recommendation Systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansha</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjie</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Smelyanskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">GenomicsBench: A Benchmark Suite for Genomics</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Subramaniyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somnath</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Vasimuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanchit</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blaauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satish</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reetuparna</forename><surname>Das</surname></persName>
		</author>
		<editor>ISPASS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Agile Paging: Exceeding the Best of Nested and Shadow Paging</title>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Wiki</forename><surname>Chip</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/raptor_lake" />
		<title level="m">Intel Raptor Lake</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Breaking the Address Translation Wall By Accelerating Memory Replays</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Micro</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-Paging in the Nemesis Operating System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Memory Coherence in Shared Virtual Memory Systems</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hudak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOCS</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Virtual Memory Primitives for User Programs</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Machine-independent Virtual Memory Management for Paged Uniprocessor and Multiprocessor Architectures</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avadis</forename><surname>Tevanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bolosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSR</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lightweight Recoverable Virtual Memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">H</forename><surname>Mashburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Steere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Kistler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generic Virtual Memory Management for Operating System Kernels</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abrossimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rozier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">WSCLOCK -A Simple and Effective Algorithm for Virtual Memory Management</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">W</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CRAMM: Virtual Memory Support for Garbage-Collected Applications</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">F</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Eliot</surname></persName>
		</author>
		<author>
			<persName><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Virtual Memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Denning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSUR</title>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ahearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Capowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arlin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Liptay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virtual Memory System</title>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Survey of Virtual Machine Research</title>
		<author>
			<persName><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Comparative Study of Set Associative Memory Mapping Algorithms and Their Use for Cache and Main Memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An In-Cache Address Translation Mechanism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pendleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A Simulation Based Study of TLB Performance</title>
		<author>
			<persName><forename type="first">Anita</forename><surname>Bradley Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Architecture Support for Single Address Space Operating Systems</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Koldinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The Grand Unified Theory of Address Spaces</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Lindstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Dearle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Virtual Memory in Contemporary Microprocessors</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Mudge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">AVM: Application-Level Virtual Memory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Engler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Architectural Support for Translation Table Management in Large Address Space Machines</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The Interaction of Architecture and Operating System Design</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">N</forename><surname>Bershad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">D</forename><surname>Lazowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Introduction and Overview of the Multics System</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Corbat?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vyssotsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AFIPS</title>
		<imprint>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Intel? 64 and ia-32 architectures software developer&apos;s manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Arm Architecture Reference Manual for A-profile Architecture</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/documentation/ddi0487/latest/,2021" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<ptr target="https://en.wikichip.org/wiki/intel/cores/cascade_lake_sp" />
		<title level="m">WikiChip. Intel Cascade Lake</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Trident: Harnessing Architectural Resources for All Page Sizes in X86 Processors</title>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Transparent Huge Pages in 2</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/423584/" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="6" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Practical, Transparent Operating System Support for Superpages</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitaram</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Hawkeye: Efficient Fine-grained Os Support for Huge Pages</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Translation Ranger: Operating System Support for Contiguity-Aware TLBs</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Konstantinos Nikas, Georgios Goumas, and Nectarios Koziris. Enhancing and Exploiting Contiguity for Fast Memory Virtualization</title>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Alverti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratos</forename><surname>Psomadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">TMO: Transparent Memory Offloading in Datacenters</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Weiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Schatzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Sanouillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bikash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejun</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Memtrade: Marketplace for Disaggregated Memory Clouds</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName><surname>Waldspurger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Software-Defined Far Memory in Warehouse-Scale Computers</title>
		<author>
			<persName><forename type="first">Andres</forename><surname>Lagar-Cavilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suleiman</forename><surname>Souhlal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radoslaw</forename><surname>Burny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakeel</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Chaugule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaid</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Thelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yurtsever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parthasarathy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Kokolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Memory-Efficient Hashed Page Tables</title>
		<author>
			<persName><forename type="first">Jovan</forename><surname>Stojkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Mantri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Every Walk&apos;s a Hit: Making Page Walks Single-Access Cache Hits</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Parallel Virtualized Memory Translation with Nested Elastic Cuckoo Page Tables</title>
		<author>
			<persName><forename type="first">Jovan</forename><surname>Stojkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><surname>Kokolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><surname>Elixir</surname></persName>
		</author>
		<ptr target="https://elixir.bootlin.com/linux/latest/-source/arch/x86/include/asm/tlbflush.h" />
		<title level="m">Linux</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Graphfire: Synergizing Fetch, Insertion, and Replacement Policies for Graph Analytics</title>
		<author>
			<persName><forename type="first">Aninda</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Arag?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TC</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP)</title>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Neural Networks: a Comprehensive Foundation</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Intel? 64 and ia-32 architectures software developer&apos;s manual volume 2a: Instruction set reference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A Case Against (Most) Context Switches</title>
		<author>
			<persName><forename type="first">Jack Tigar</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostis</forename><surname>Kaffes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mazi?res</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">XPC: Architectural Support for Secure and Efficient Cross Process Call</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Latr: Lazy Translation Coherence</title>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kumar Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?n</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hewlett</forename><surname>Packard</surname></persName>
		</author>
		<author>
			<persName><surname>Mcpat</surname></persName>
		</author>
		<ptr target="https://github.com/HewlettPackard/mcpat" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Stride Directed Prefetching in Scalar Processors</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janak</forename><forename type="middle">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><forename type="middle">L</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Janssens</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Effective Hardware-based Data Prefetching for High-performance Processors</title>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TC</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Transparent Hugepage Support</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Arcangeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KVM Forum</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Compendia: Reducing Virtual-Memory Costs Via Selective Densification</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Rebooting Virtual Memory with Midgard</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunho</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Payer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">CHiRP: Control-Flow History Reuse Prediction</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Mirbagher-Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elba</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Prediction-Based Superpage-Friendly TLB Designs</title>
		<author>
			<persName><forename type="first">Misel-Myrto</forename><surname>Papadopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Reducing TLB Power Requirements</title>
		<author>
			<persName><forename type="first">Toni</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISLPED</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Reducing TLB and Memory Overhead Using Online Superpage Promotion</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Romer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Ohlrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Bershad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Compiler-directed Physical Address Generation for Reducing dTLB Power</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kadayif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">SpecTLB: A Mechanism for Speculative Address Translation</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">SIPT: Speculatively Indexed, Physically Tagged Caches</title>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haishan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Concurrent Support of Multiple Page Sizes on a Skewed Associative TLB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TC</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Exploiting Page Table Locality for Agile TLB Prefetching</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Vavouliotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluc</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Nikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nectarios</forename><surname>Koziris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Casas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Morrigan: A Composite Instruction TLB Prefetcher</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Vavouliotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluc</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Casas</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Artemiy</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Prefetched Address Translation. In MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Going the Distance for TLB Prefetching: An Application-driven Study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gokul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Kandiraju</surname></persName>
		</author>
		<author>
			<persName><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Recency-based TLB Preloading</title>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Saulsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Large-Reach Memory Management Unit Caches</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Dead Page and Dead Block Predictors: Cleaning TLBs and Caches Together</title>
		<author>
			<persName><forename type="first">Chandrashis</forename><surname>Mazumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prachatos</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Exploiting Parallelization On Address Translation: Shared Page Walk Cache</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Esteve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Engracia</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Robles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OMHI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Pinning Page Structure Entries to Last-Level Cache for Fast Address Translation</title>
		<author>
			<persName><forename type="first">Osang</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokin</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Access</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Address Translation Conscious Caching and Prefetching for High Performance Cache Hierarchy</title>
		<author>
			<persName><forename type="first">Vasudha</forename><surname>Vasudha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biswabandan</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPASS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Devirtualizing Virtual Memory for Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Haria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Mosaic Pages: Big TLB Reach with Small Pages</title>
		<author>
			<persName><forename type="first">Jaehyun</forename><surname>Krishnan Gosakan</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(massachusetts</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><surname>Inst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Of Technology) Kuszmaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nirjhar</forename><surname>Nael Mubarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Tagliavini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Farach-Colton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudarsun</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS 2023</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Utopia: Fast and Efficient Address Translation via Hybrid Flexible &amp; Restrictive Virtual-to-Physical Address Mappings</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosta</forename><surname>Stojiljkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Nisa</forename><surname>Bostanci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Firtina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachata</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nastaran</forename><surname>Hajinazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Sadrosadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Near-Memory Address Translation</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Accelerating Pointer Chasing in 3D-stacked Memory: Challenges, Mechanisms, Evaluation</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<editor>ICCD</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory Machines</title>
		<author>
			<persName><forename type="first">Reto</forename><surname>Achermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Do-It-Yourself Virtual Memory Translation</title>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Space Efficient Hash Tables with Worst Case Constant Access Time</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Fotakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Spirakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STACS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Perforated Page: Supporting Fragmented Memory Allocation for Large Pages</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokyeong</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Tailored Page Sizes</title>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Guvenilir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Coordinated and Efficient Huge Page Management with Ingens</title>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmett</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Tradeoffs in Supporting Two Page Sizes</title>
		<author>
			<persName><forename type="first">Madhusudhan</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Making Huge Pages Actually Useful</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinda</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Large Pages and Lightweight Memory Management in Virtualized Environments: Can You Have It Both Ways?</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?n</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Mosaic: A GPU Memory Manager with Application-Transparent Support for Multiple Page Sizes</title>
		<author>
			<persName><forename type="first">Rachata</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Landgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vance</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Reevaluating Online Superpage Promotion with Hardware Support</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Increasing TLB Reach Using Superpages Backed By Shadow Memory</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Supporting Superpages in Non-Contiguous Physical Memory</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">R</forename><surname>Childers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Moss?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Melhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Surpassing the TLB Performance of Superpages with Less Operating System Support</title>
		<author>
			<persName><forename type="first">Madhusudhan</forename><surname>Talluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Supporting Superpage Allocation Without Additional Hardware Support</title>
		<author>
			<persName><forename type="first">Mel</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">Predicting Execution Times with Partial Simulations in Virtual Memory Research: Why and How</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Agbarya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">General Purpose Operating System Support for Multiple Page Sizes</title>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curt</forename><surname>Schimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ATC</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Energy-Efficient Address Translation</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri?n</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><forename type="middle">S</forename><surname>Unsal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Redundant Memory Mappings for Fast Access to Large Memories</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Karakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furkan</forename><surname>Ayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri?n</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osman</forename><surname>?nsal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Hybrid TLB Coalescing: Improving TLB Translation Coverage Under Diverse Fragmented Memory Allocations</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taekyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungi</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyuk</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">FlexPointer: Fast Address TranslatiOn Based On Range TLB and Tagged Pointers</title>
		<author>
			<persName><forename type="first">Dongwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangfang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACO</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Aamer Jaleel, and Abhishek Bhattacharjee. CoLT: Coalesced Large-Reach TLBs</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viswanathan</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Efficient Memory Virtualization: Reducing Dimensionality of Nested Page Walks</title>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Using TLB Speculation to Overcome Page Splintering in Virtual Machines</title>
		<author>
			<persName><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
		<idno>DCS-TR-713</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Rutgers Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Accelerating Two-Dimensional Page Walks for Virtualized Systems</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Serebrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Spadini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srilatha</forename><surname>Manne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Hardware Translation Coherence for Virtualized Systems</title>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?n</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">BabelFish: Fusing Address Translations for Containers</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Skarlatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umur</forename><surname>Darbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargava</forename><surname>Gopireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">PTEMagnet: FIne-graIned Physical Memory Reservation for Faster Page Walks in Public Clouds</title>
		<author>
			<persName><forename type="first">Artemiy</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amna</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Fast Local Page-tables for Virtualized Numa Servers with vmitosis</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reto</forename><surname>Achermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayneel</forename><surname>Gandhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">A New Perspective for Efficient Virtual-Cache Coherence</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">SEESAW: Using Superpages to Improve VIPT Caches</title>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Parasar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Reducing Memory Reference Energy with Opportunistic Virtual Caching</title>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Virtual-Address Caches Part 1: Problems and Solutions in Uniprocessors</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Cekleov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Dubois</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Coherency for Multiprocessor Virtual Address Caches</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Consistency Management for Virtually Indexed Caches</title>
		<author>
			<persName><forename type="first">Bob</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">N</forename><surname>Bershad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Organization and Performance of a Two-Level Virtual-Real Cache Hierarchy</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Enigma: Architectural and Operating System Support for Reducing the Impact of Address Translation</title>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Speight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Rajamony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework</title>
		<author>
			<persName><forename type="first">Nastaran</forename><surname>Hajinazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachata</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geraldo</forename><forename type="middle">F</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Appavoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">PowerPC Architecture Book</title>
		<author>
			<persName><surname>Frey</surname></persName>
		</author>
		<ptr target="www.ibm.com/developerworks/eserver/articles/archguide.html" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Virtual-Address Caches Part 2: Multiprocessor Issues</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Cekleov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Dubois</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Slurm: Simple Linux Utility for Resource Management</title>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morris</forename><forename type="middle">A</forename><surname>Jette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Grondona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Job Scheduling Strategies</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
