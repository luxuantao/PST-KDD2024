<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New Constraints on Data-Closeness and Needle Map Consistency for Shape-from-Shading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">L</forename><surname>Worthington</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>Y01 5DD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>Y01 5DD</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">New Constraints on Data-Closeness and Needle Map Consistency for Shape-from-Shading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2919FCF574A60DA95D908244231A0CCD</idno>
					<note type="submission">received 22 Dec. 1998; revised 24 Aug. 1999. Recommended for acceptance by M. Shah.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms±Shape-from-shading</term>
					<term>hard constraints</term>
					<term>curvature consistency</term>
					<term>gradient consistency</term>
					<term>robust statistics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AbstractÐThis paper makes two contributions to the problem of needle-map recovery using shape-from-shading. First, we provide a geometric update procedure which allows the image irradiance equation to be satisfied as a hard constraint. This not only improves the data closeness of the recovered needle-map, but also removes the necessity for extensive parameter tuning. Second, we exploit the improved ease of control of the new shape-from-shading process to investigate various types of needle-map consistency constraint. The first set of constraints are based on needle-map smoothness. The second avenue of investigation is to use curvature information to impose topographic constraints. Third, we explore ways in which the needle-map is recovered so as to be consistent with the image gradient field. In each case we explore a variety of robust error measures and consistency weighting schemes that can be used to impose the desired constraints on the recovered needle-map. We provide an experimental assessment of the new shape-from-shading framework on both real world images and synthetic images with known ground truth surface normals. The main conclusion drawn from our analysis is that the data-closeness constraint improves the efficiency of shape-from-shading and that both the topographic and gradient consistency constraints improve the fidelity of the recovered needle-map.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S HAPE-FROM-SHADING (SFS) has been a topic of sustained research activity for over two decades. The process is concerned with recovering local surface orientation from variations in image radiance. In Marr's seminal work detailing a framework for computer vision <ref type="bibr" target="#b19">[21]</ref>, shapefrom-shading is identified as providing a useful route to understanding 3D surface shape from a single image. Moreover, psychophysical motivation for the use of SFS has recently been demonstrated through experiments on the human vision system <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b16">[18]</ref>.</p><p>From a computational viewpoint, SFS involves solving the image irradiance equation (IIR) to recover a set of surface normals, often described as the needle-map. However, since the IIR is under-constrained, additional constraints such as local smoothness of the needle-map must be invoked for reasons of computational tractability.</p><p>One of the most popular approaches is to adopt a regularization framework <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[12]</ref> and iteratively recover the needle-map using an update equation derived by applying variational calculus. The regularization functional must ensure data-closeness by penalizing departures from the IIR, while imposing smoothness on the recovered needle-map. A fundamental obstacle to progress in energy minimization approaches to shape-from-shading is the failure, to-date, to develop a constraint function which is uniquely minimized by a surface which closely matches the true surface. The search for such a functional is hindered by the nature of the minimization framework, which is usually based upon regularization theory <ref type="bibr" target="#b25">[27]</ref>. This requires careful selection of constraint functions and, most significantly, time-consuming experimentation to establish regularization parameters and determine whether or not a given scheme is numerically stable. In many cases, the conditions to ensure numerical stability lead to severe model dominance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Literature</head><p>Many different computational approaches to recovering shape from shading have been proposed, falling into two broad categories; local <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and global <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b26">[28]</ref>. Local shading analysis involves taking small windows of the image and recovering surface patches which are subsequently quilted together. Local approaches tend to be fast, but often require a priori surface height information, such as the elevation of singular points <ref type="bibr" target="#b20">[22]</ref>, or the linearization of the reflectance map <ref type="bibr" target="#b23">[25]</ref>. On real world images, the results of local SFS techniques are often noisy.</p><p>In contrast, global approaches attempt to recover the entire surface, either through propagation of height values from singular points of the surface, or by minimization of some energy functional associated with the estimated surface. Global propagation techniques again require a priori height data for initialization, whereas global minimization techniques may be developed which require only the image and a light source direction estimate as input. Moreover, global minimization techniques have been shown to be more generally applicable to different types of input images, and more robust to noise than either local techniques or global propagation approaches <ref type="bibr" target="#b34">[36]</ref>. With these factors in mind, this paper focuses upon global minimization techniques.</p><p>A major criticism of many global approaches has been their tendency to oversmooth the recovered surface; that is, to lose detail, particularly in regions where the true surface is discontinuous. Various approaches have been proposed to address this problem. For instance, Ferrie and Legarde <ref type="bibr" target="#b5">[6]</ref> have used curvature consistency to augment the more usual constraint on smoothness. Zheng and Chellappa <ref type="bibr" target="#b35">[37]</ref> apply an intensity gradient constraint which penalizes discrepancies between the rate of change of reflectance and the rate of change of image intensity. Lee and Kuo <ref type="bibr" target="#b27">[29]</ref> describe a scheme based on a triangular-element surface model, in which the smoothness constraint takes the form of a surface stiffness.</p><p>Recently, Kimmel and Bruckstein <ref type="bibr" target="#b13">[15]</ref> have shown how the apparatus of level-set theory <ref type="bibr" target="#b27">[29]</ref> can be used to solve the Euler equations developed by Horn <ref type="bibr" target="#b8">[9]</ref> to describe the SFS problem. In this approach, a 3D function is propagated on a grid. A level set of this function tracks the height contours of the recovered shape. This builds upon earlier work by Bruckstein <ref type="bibr" target="#b4">[5]</ref>, which propagated a single equalheight contour in an image (e.g., a shoreline or horizon in a landscape picture) to find the set of equal-height contours defining the surface. In the level-set approach, the 3D function evolves according to constraints which are analogous to Huygen's wavefront propagation principle, an idea borrowed from recent advances in computational fluid dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Paper Outline</head><p>Existing work on shape-from-shading can be criticized on two counts. First, the data-closeness constraint invariably plays a relatively weak role in the recovery of the needlemap, while the smoothness error dominates the process. Second, the modeling of the smoothness constraint is extremely simplistic. Most practical schemes opt for a quadratic regularizer <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[12]</ref>. This has the effect not only of over-smoothing fine surface detail, but also of failing to capture the differential or topographic structure of realistic surfaces.</p><p>Several pieces of work aim explicitly to address these criticisms. Ferrie and Lagarde <ref type="bibr" target="#b5">[6]</ref> impose curvature consistency on the surface as a postprocessing step. Their method attempts to use topographic surface structure to improve the global properties of the surface reconstruction, without resorting to smoothing.</p><p>Horn <ref type="bibr" target="#b10">[12]</ref> attempts to improve data-closeness by annealing the weight assigned to the smoothness regularizer as convergence is approached. In a more recent paper <ref type="bibr" target="#b30">[32]</ref>, we have applied the apparatus of robust statistics to modify the strength of the smoothness constraint in a principled manner. Robust error kernels are used in place of a quadratic regularizer. Applying variational calculus to the resulting regularized functional yields needle-map update equations in which data-closeness plays a more significant role.</p><p>The novel contribution of this paper is to furnish new constraints on the recovery of the needle-map. The first contribution is to impose data-closeness as a hard constraint. Our starting point is the observation that the IIR constrains the surface normal to fall on a cone of ambiguity. The axis of the cone is the light-source direction. This leads to a simple geometric picture of iterative needle-map recovery which can be visualized as remapping the surface normals onto a cone of possible solutions. The second contribution is to exploit this new framework to explore more complex constraints on needle-map consistency. Here, we show that both curvature consistency and gradient consistency provide better constraints on the recovery of faithful needle maps than the conventional quadratic smoothness constraint of Horn and Brooks.</p><p>The structure of this paper is as follows: As an introduction to the global recovery of shape from shading, we briefly summarize the variational approach of Horn and Brooks <ref type="bibr" target="#b9">[10]</ref>. By considering its weaknesses, we identify the main deficiencies of existing global minimization techniques. We then present our framework for SFS and argue for its validity and advantages in terms of addressing these deficiencies. Subsequently, we develop several new constraints for SFS, including image gradient and curvature consistency. Placing these constraints within the new framework, we develop iterative update schemes to recover the needle-map from an image. We compare the performance of our new techniques against the Horn and Brooks variational approach to SFS and other schemes from the literature. Through detailed analysis using synthetic data, we demonstrate the advantages of the new framework. Improved performance on real images is also empirically demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE VARIATIONAL APPROACH TO SFS</head><p>Central to shape-from-shading is the idea that local regions in an image ixY y correspond to illuminated patches of a piecewise continuous surface, zxY y. The measured brightness ixY y will depend on the material properties of the surface, the orientation of the surface at the coordinates xY y, and the direction and strength of illumination.</p><p>The reflectance map, pY q characterizes these properties, and provides an explicit connection between the image and the surface orientation. Surface orientation is described by the components of the surface gradient in the x and y direction, i.e., p dz dx and q dz dy . The shape from shading problem is to recover the surface zxY y from the intensity image ixY y. As an intermediate step, we may recover the needle-map, or set of estimated local surface normals, nxY y.</p><p>Needle-map recovery from a single intensity image is an under-determined problem <ref type="bibr" target="#b20">[22]</ref>, <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b1">[2]</ref> which requires a number of constraints and assumptions to be made. The common assumptions are that the surface has ideal Lambertian reflectance, constant albedo, and is illuminated by a single point source at infinity. A further assumption is that there are no interreflections, i.e., the light reflected by one portion of the surface does not impinge on any other part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Horn and Brooks Algorithm</head><p>The local surface normal may be written as n ÀpY ÀqY I Y where p dz dx and q dz dy . For a light source at infinity, we can similarly write the light source direction as s Àp l Y Àq l Y I . If the surface is Lambertian the reflectance map is given by pY q n Á sX I</p><p>The image irradiance equation <ref type="bibr">[11]</ref> states that the measured brightness of the image is proportional to the radiance at the corresponding point on the surface; that is, just the value of pY q for pY q corresponding to the orientation of the surface. Normalizing both the image intensity, ixY y, and the reflectance map, the constant of proportionality becomes unity, and the image irradiance equation is simply ixY y pY qX P</p><p>Although the image irradiance equation succinctly describes the mapping between the xY y coordinate space of the image and the the pY q gradient space of the surface, it provides insufficient constraints for the unique recovery of the needle-map. To overcome this problem, a further constraint must be applied. Usually, the needle-map is assumed to vary smoothly.</p><p>The process of smooth surface recovery is posed as a variational problem in which a global error functional is minimized through the iterative adjustment of the needle map. Surface normals are updated with a step size dictated by Euler's equation. Here we consider the formulation of Brooks and Horn <ref type="bibr" target="#b5">[6]</ref> which is couched in terms of unit surface normals. Their error functional is defined to be The terms dn dx and dn dy above are the directional derivatives of the needle-map in the x and y directions respectively. The magnitudes of these quantities are used to measure the smoothness of the surface, with a large value indicating a highly curved region. However, it should be noted that a planar surface has dn dx dn dy H in this case. The first term of (3) is the brightness error, which encourages data-closeness of the measured image intensity and the reflectance function. The regularizing term imposes the smoothness constraint on the recovered surface normals, penalizing large local changes in surface orientation. The constant ! is a Lagrange multiplier. For numerical stability, ! must often be large, resulting in the smoothness term dominating.</p><p>Minimization of the functional defined in (3) is accomplished by applying the calculus of variations <ref type="bibr" target="#b6">[7]</ref>   <ref type="formula">5</ref>) is discretized using the following approximation to the Laplacian</p><formula xml:id="formula_0">n r P n o iYj % R P " n iYj À n iYj Y T</formula><p>where is the spacing of pixel-sites on the lattice and " n iYj is the local 4-neighborhood mean of the surface normals around pixel position iY j,</p><formula xml:id="formula_1">" n iYj I R n iYjI n iYjÀI n iIYj n iÀIYj X U</formula><p>Discretizing <ref type="bibr" target="#b4">(5)</ref> and rearranging, we obtain an update equation to estimate the needle-map at epoch k I using the estimate at epoch k</p><formula xml:id="formula_2">n kI iYj " n k iYj P P! i iYj À n k iYj Á s sX V 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Weaknesses of the Horn and Brooks Algorithm</head><p>The principal criticism of the Horn and Brooks algorithm, and of similar approaches <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b28">[30]</ref>, is its tendency to oversmooth the recovered needle-map. Specifically, in ( <ref type="formula">8</ref>), the regularizing term dominates the data term. Since the smoothness constraint is formulated in terms of the directional derivatives of the needle-map, it is trivially minimized by a flat surface. Thus, the conflict between the data and the model leads to a strongly smoothed needlemap and the loss of detail. The problem is exacerbated by the need to select a conservative value for the Lagrange multiplier in order to ensure numerical stability of the scheme <ref type="bibr" target="#b10">[12]</ref>.</p><p>A further criticism of the Brooks and Horn algorithm stems from the use of the Lagrange multiplier, which entails parameter searching. To be confident that the scheme will remain numerically stable for all expected input images, it is often necessary to use a conservative value of the parameter, exacerbating the oversmoothing problem.</p><p>In <ref type="bibr" target="#b29">[31]</ref>, <ref type="bibr" target="#b31">[33]</ref>, we introduced a novel application of robust M-estimators to SFS in order to address the oversmoothing problem. However, the improvements resulting from this approach are obtained at the expense of introducing an additional parameter to control the kernel width.</p><p>Hence, we see clear problems of model domination and time consuming parameter selection in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A NOVEL FRAMEWORK FOR SFS</head><p>In this paper, we aim to address the criticisms detailed in the previous section. Specifically, we wish to reduce model dependence without requiring parameters or any information in addition to the image and the light source direction. Note that Pentland <ref type="bibr" target="#b24">[26]</ref> developed a method to estimate the light source direction from the image, so in fact only the image is required.</p><p>In the variational approach to SFS due to Brooks and Horn <ref type="bibr" target="#b3">[4]</ref>, the aim is to minimize a dual functional (3). However, the aims of the terms of the functional are different and competing. The smoothness term is minimized by any planar surface. Thus, if the scheme is initialized with the true needle-map, it will tend to ªwalkawayº as brightness error is traded for smoothness. Horn <ref type="bibr" target="#b10">[12]</ref> attempts to address this problem by increasing the relative importance of the brightness error over successive iterations. Here, however, we opt to treat the data-closeness and constraint terms separately <ref type="bibr" target="#b32">[34]</ref>, <ref type="bibr" target="#b33">[35]</ref>, in such a way that data-closeness is always guaranteed; that is, the image irradiance equation is treated as a hard constraint on the problem. Hence, we have a valid needle-map which satisfies the image irradiance equation at every iteration. Subject to this constraint, the task becomes iteration toward the true needle-map, using minimization of the smoothness error or some other constraint functional. The need for a weighting parameter is removed entirely.</p><p>Geometrically, we consider the image irradiance equation to define a cone of ambiguity about the light source direction for each surface normal. The normals forming the needle-map can only assume directions defined by this cone, as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. At each iteration the updated normal is free, as a result of the smoothing process, to lie outside the cone, but is subsequently projected back onto the closest vector lying on the cone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Using the Image Irradiance Equation as a Hard Constraint</head><p>The new framework requires us to minimize the constraint functional</p><formula xml:id="formula_3">s 2 nxY yY xxY y dxdyY W</formula><p>while satisfying the hard constraint, imposed by the image irradiance equation i À n Á s dxdy HX IH</p><p>Here, xxY y is the set of local neighborhood vectors about location xY y. For example, in terms of lattice coordinates iY j, the 4-neighborhood of n iYj is defined as:</p><formula xml:id="formula_4">x fn iIYj Y n iÀIYj Y n iYjI Y n iYjÀI gX II</formula><p>The function 2 nxY yY xxY y is a localized function of the current surface normal estimates. The size of the neighborhood may be varied according to the nature of 2. Clearly, it is possible to incorporate the hard data-closeness constraint directly into 2, but this needlessly complicates the mathe-matics. Instead, we choose to impose the constraint after each iteration by mapping the updated normals back to the most similar normal lying on the cone.</p><p>If we take the smoothness constraint of Horn and Brooks as an example, then</p><formula xml:id="formula_5">2 nY x dn dx P dn dy P X IP</formula><p>Discretizing 2 nxY yY xxY y directly in terms of forward differences yields the following equation in terms of n iYj and its neighboring normals</p><formula xml:id="formula_6">2 n iYj Y x iYj À Á n iIYj À n iYj À Á P n iYjI À n iYj À Á P X IQ</formula><p>In practice, to maintain symmetry, we take the mean of the forward and backward difference results. We cannot use the central difference because this does not include the current normal, n iYj . Taking the mean yields</p><formula xml:id="formula_7">2 n iYj Y x iYj À Á I P n iIYj À n iYj À Á P n iYjI À n iYj À Á P n iYj À n iÀIYj À Á P n iYj À n iYjÀI À Á P X</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IR</head><p>We can again apply the calculus of variations, as described by <ref type="bibr" target="#b3">(4)</ref>. Note that, having discretized prior to differentiating, we have only the s n component. However, the same result can be obtained by performing the discretization after differentiation. The operations are shown in this order to motivate the definition of 2 in terms of the neighborhood normals, xxY y. Thus, the discrete Euler equation becomes</p><formula xml:id="formula_8">I P ÀP n iIYj À n iYj À Á À P n iYjI À n iYj À Á À P n iYj À n iÀIYj À Á P n iYj À n iYjÀI À Á Á HX</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IS</head><p>Rearranging, and substituting (7), yields</p><formula xml:id="formula_9">n kI iYj " n k iYj X IT</formula><p>In other words, we obtain a simple neighborhood averaging, or smoothing action, which if left unchecked will  <ref type="formula">8</ref>). In contrast, the new framework forces the brightness error to be satisfied by using the rotation matrix Â to map the smoothness update to the closest point on the cone.</p><p>eventually lead to a flat surface. Note that this update equation could be obtained from the Horn and Brooks update equation simply by setting the brightness error to zero in <ref type="bibr" target="#b7">(8)</ref>. Equally, we could obtain an identical update equation using the constraint function</p><formula xml:id="formula_10">2 nY x n À " n P X IU</formula><p>Applying the hard constraint, for instance, in terms of a rotation of the update back onto the cone of ambiguity, ( <ref type="formula">16</ref>) becomes</p><formula xml:id="formula_11">n kI iYj Â" n k iYj Y IV</formula><p>where Â is a rotation matrix to map the updated normal to the closest normal lying on the cone of ambiguity. Another way to look at this, is that we allow the smoothness constraint to select the direction of the normal estimate in the image plane only, while fixing the angle between the normal estimate and the light source direction.</p><p>To achieve the rotation, we define an axis perpendicular to the intermediate update normal, " n k iYj , and the light source direction. The axis of rotation is found by taking the cross product of the intermediate update with the light source direction</p><formula xml:id="formula_12">uY vY w " n k iYj Â sX IW</formula><p>The angle of rotation is the difference between the angle subtended by the intermediate update and the light source, and the apex angle of the cone of ambiguity. Since the image is normalized, the latter angle is simply os ÀI i, giving a rotation angle of Fig. <ref type="figure" target="#fig_1">1</ref> illustrates the update process, and compares it with the Horn and Brooks update of (8).</p><formula xml:id="formula_13">À os ÀI " n k iYj Á s " n k iYj s k k H f d I g e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initialization</head><p>The new framework forces the surface normals to lie on the cone defined by the image irradiance equation at each iteration. Therefore, we choose an initialization which ensures that the IIR ( <ref type="formula">1</ref>) is satisfied at the outset. This differs from the Horn and Brooks algorithm, which is usually initialized by estimating the occluding boundary normals, with all other normals set to point in the light source direction.</p><p>We choose to initialize each normal such that its projection onto the image plane lies in the opposite direction to the image gradient direction, as shown in Fig. <ref type="figure">2</ref>. This results in an initialization with an implicit bias toward convex rather than concave surfaces. In other words, bright regions are assumed to correspond to peaks, and the image gradient direction points toward these peaks. Fig. <ref type="figure" target="#fig_3">3</ref> illustrates this bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ALTERNATIVE CONSTRAINTS</head><p>The novel framework developed in the preceding section, using the smoothness constraint of Horn and Brooks, reduces the problem of model dominance, and removes the need to perform exhaustive parameter searches. Our second contribution in this research is to develop new methods of enforcing needle-map consistency. This is motivated by previous work which has shown that the traditional quadratic smoothness regularizer is ill-suited to the SFS problem <ref type="bibr" target="#b31">[33]</ref>. Remarkably little attention has been focused upon alternatives to smoothness constraints <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b35">[37]</ref>.</p><p>Here, we describe three promising approaches. The first is simply to modify the action of needle-map smoothness to more adequately model the properties of real surfaces. Second, we consider the topographic properties of surfaces to develop consistency constraints based upon surface curvature. Finally, we use higher order consistency between the needle-map and image to ensure satisfaction of the image irradiance equation in terms of image gradient as well as intensity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Surface Geometry Constraints</head><p>This class of constraints includes any which make direct assumptions about the form of the surface. For instance, the quadratic smoothness constraint of Horn and Brooks falls into this category, since it assumes that the surface to be recovered is the smoothest possible surface satisfying the image irradiance equation, (1) subject to the criticisms of Section 2.2. Similarly, the robust regularizer developed in <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b31">[33]</ref> uses a modified version of this assumption. In essence, it considers that the recovered surface should be smooth, except where there is a high probability that a Fig. <ref type="figure">2</ref>. The set of surface normals at a point which satisfy the Image Irradiance equation define a cone such that i-nXs HX A normal from this set is chosen such that the direction of its projection to the image plane is opposite to the maxium intensity gradient direction, gX discontinuity is present, in which case the smoothing is reduced.</p><p>We define the robust regularizer constraint function as While the contents of the parentheses on the right hand side of <ref type="bibr" target="#b21">(23)</ref> appear at first glance to be a scalar, in fact the differentiation of the robust kernel by a vector results in another vector. Hence, ( <ref type="formula">23</ref>) is dimensionally correct, as demonstrated below using a specific instance of &amp; ' .</p><formula xml:id="formula_14">2</formula><p>In <ref type="bibr" target="#b31">[33]</ref> we experimented with several robust error kernels, including Li's Adaptive Potential Functions <ref type="bibr" target="#b18">[20]</ref>, and the Tukey <ref type="bibr" target="#b7">[8]</ref> and Huber <ref type="bibr" target="#b11">[13]</ref> estimators. However, the sigmoidal derivative M-estimator, a continuous version of Huber's estimator, proved to possess the best properties for handling surface discontinuities, and is defined to be </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PT</head><p>The robust regularizer approach provides significantly improved results over the simple Horn and Brooks smoothness constraint, but at the expense of introducing the parameter, '.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Curvature Consistency</head><p>Needle-map smoothness appears to be an over strong and inappropriate constraint for shape from shading. This is primarily because real surfaces are more likely to be piecewise smooth; in other words, formed of smooth regions separated by sharp discontinuities in depth or orientation. The oversmoothing problem is exacerbated by the difficulty of formulating the continuous concept of smoothness on a A bright patch corresponds to a region presenting a large area toward the light source direction. Initializing the surface normals to point in the same direction as the negative gradient means that the bright region is always assumed to be a peak, as shown in the first diagram. However, the bottom two diagrams also show valid surfaces which will result in the same intensities, but for which the initialization will not work well. Given the bas-relief ambiguity, there is no way to distinguish between these possibilities, so we define our convention to assume the first case.</p><p>discrete pixel lattice, as clearly illustrated by the fact that the Horn and Brooks smoothness constraint is trivially minimized by a needle-map corresponding to a planar surface.</p><p>In contrast, consider the well defined curvature characteristics of most real world surfaces. Although the curvature classes either side of a depth discontinuity may be completely unrelated, this is not the case for an orientation discontinuity. Orientation discontinuities usually correspond to ruts or ridges. Furthermore, the curvature classes for locations either side of a rut or a ridge should be the most similar classes, either trough or saddle rut for a rut, or dome or saddle ridge for a ridge. This property of smooth variation in class suggests that curvature consistency may be a more appropriate constraint for SFS than smoothness, which strongly penalizes legitimate orientation discontinuities.</p><p>The use of a curvature consistency measure was introduced to SFS by Ferrie and Lagarde <ref type="bibr" target="#b5">[6]</ref>. They use global consistency of principal curvatures <ref type="bibr" target="#b22">[24]</ref> to refine the surface estimate returned by local shading analysis. Curvature consistency is formulated in terms of rotating the local Darboux frame to ensure that the principal curvature directions are locally consistent. Recently, Angelopoulou <ref type="bibr" target="#b0">[1]</ref> has recovered estimates of r À u labels using photometric stereo under three different illumination conditions.</p><p>One way of representing curvature information is to use r À u labels, but these require us to set four thresholds to define the classes in terms of the mean and Gaussian curvatures. However, we propose to use curvature consistency based upon the shape index of Koenderink and van Doorn <ref type="bibr" target="#b14">[16]</ref>. This is a measure which encodes the same curvature class information as r À u labels in a single, continuous representation, and has the further advantage of not requiring any thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">The Shape Index</head><p>We reformulate the definition of the shape index in terms of the needle-map. This allows us to use the needle-map directly, rather than needing to reconstruct the surface.</p><p>The differential structure of a surface is captured by the Hessian matrix </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Weighted Mean Using Curvature Consistency</head><p>A practical method of using curvature consistency is to use a weighted mean process of the local normals in the update. This follows logically from the update equation obtained using the quadratic smoothness constraint and the robust regularizer above. The idea is to weight in favor of ªgoodº normals in the neighborhood. The selection of a criterion for what constitutes a good normal is an open problem. In this section, we investigate the use of curvature consistency to determine the goodness of each neighborhood normal, while later we investigate the use of image edge strength measurements.</p><p>As stated above, since the shape index is a continuous, angular measure, we expect it to vary gradually over a smooth surface. For instance, with reference to Fig. <ref type="figure" target="#fig_4">4</ref>, we would not expect the shape index at adjacent pixels to differ by more than one curvature class unless they lie on opposite sides of a surface discontinuity. Since the over smoothing effect of the quadratic smoothness constraint stems directly from the indiscriminate averaging of normals lying across a discontinuity, we anticipate that weighting according to curvature consistency will reduce the problem in a physically principled manner. We note once again that (18) may be obtained using the constraint function</p><formula xml:id="formula_15">2 nY x n À " n P QI</formula><p>instead of the constraint on the magnitudes of the directional derivatives of the needle-map used in <ref type="bibr" target="#b10">(12)</ref>. From this observation, we propose a similar constraint as follows:</p><formula xml:id="formula_16">2 nY x n À ( x P Y QP</formula><p>where (x is a weighted mean process defined on the neighborhood normals as</p><formula xml:id="formula_17">(x lPx w I n I lPx w I X QQ</formula><p>To weight on the basis of curvature consistency we choose weights of the form</p><formula xml:id="formula_18">w l exp À 0 l À " 0 À Á P P' P 0 2 3 Y QR</formula><p>where " 0 and ' 0 are, respectively, the mean and variance of the shape index over the neighborhood. Hence, a neighborhood normal with a shape index similar to the neighborhood mean gains greater weight in the averaging process than one with an outlying shape index. The latter normal is assumed to lie on the opposite side of a discontinuity from the majority of the neighborhood, and therefore does not contribute significantly to the weighted mean vector.</p><p>There is no need to apply the calculus of variations in this case, as the functional based on the constraint of ( <ref type="formula">32</ref>) is trivially minimized, within the new framework, by the update equation</p><formula xml:id="formula_19">n kI iYj Â( x k iYj Y QS</formula><p>where, once again, Â is a rotation matrix to map the intermediate update normal back onto the cone of solutions to the IIR <ref type="bibr" target="#b19">(21)</ref>. Normals which are associated with a large difference from the neighborhood mean shape index are given small weights in the update, reflecting the fact that they are expected to lie on the opposite side of a discontinuity from the majority of the neighborhood.</p><p>An alternative formulation uses weights calculated using the Median Absolute Deviation (MAD) <ref type="bibr" target="#b7">[8]</ref> in place of the mean of the neighborhood shape index values</p><formula xml:id="formula_20">w l exp À 0 l À medin 0 À Á P Pweh P 0 2 3 X QT</formula><p>This is intended to deal with the potential problem that, if the neighborhood is evenly split by a discontinuity, the mean shape index may not be representative of either set of normals. Use of the median in this case forces the comparison value to be representative of the set of normals upon one side of the discontinuity or the other, but at the expense of introducing a nonlinear process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Adaptive Robust Regularizer Using Curvature Consistency</head><p>We have also investigated the use of an adaptive robust regularizer in conjunction with curvature consistency <ref type="bibr" target="#b33">[35]</ref>.</p><p>Here, instead of determining the individual weightings of each normal in the neighborhood, we use the shape index statistics to adaptively set the width of the robust kernel, '</p><p>, to be applied over the neighborhood. The kernel width determines the level of smoothing applied to the neighborhood. If the shape index of the neighborhood varies greatly from the shape index at the center, then strong smoothing is applied, whereas an already smoothly varying shape index pattern receives less attention. Using the robust kernel of ( <ref type="formula" target="#formula_21">22</ref>) reduces the problem of oversmoothing if large differences of shape index are present.</p><p>Once again, we have</p><formula xml:id="formula_22">2 nY x &amp; ' dn dx &amp; ' dn dy Y QU</formula><p>where, again, x is the set of local neighborhood normals used to calculate the finite difference approximations to dn dx and dn dy . Instead of a fixed kernel width, ', used in <ref type="bibr" target="#b30">[32]</ref>, <ref type="bibr" target="#b31">[33]</ref>, we use the adaptive value</p><formula xml:id="formula_23">' ' H exp À I x lPx 0 l À 0 P Á0 P d @ A I P P R Q S Y QV</formula><p>where 0 is the shape index associated with the central normal of the neighborhood, n iYj , and Á0 d is the difference in shape index between the center values of adjacent curvature classes. The number of neighborhood normals used in calculating the finite difference approximations to dn dx and dn dy is denoted x, and ' H is a reference kernel width which we set to unity. Using the scale of Fig. <ref type="figure" target="#fig_5">4,</ref><ref type="figure">Á0 d I</ref> V . If the shape index varies significantly over the neighborhood, a small value of ' results, and the robust regularizer saturates to produce a heavy smoothing effect. In contrast, when the shape index values are already similar, the kernel is widened so that little smoothing occurs.</p><p>The update equation is identical to <ref type="bibr" target="#b21">(23)</ref>, but with the adaptive width parameter, ', of (38) substituted in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Gradient Consistency</head><p>Zheng and Chellappa <ref type="bibr" target="#b35">[37]</ref> used such an intensity gradient constraint in conjunction with integrability. This constraint ensured that the intensity gradient of the image reconstructed from the needle-map matched the gradient of the input image. However, this constraint is not useful in the new framework since the reconstructed image always precisely matches the input due to the application of the image irradiance equation as a hard constraint. We therefore attempt to design constraints which use the needlemap structure directly.</p><p>To formulate a gradient consistency constraint, we desire to ensure that the image gradients in the x and y directions match those of the needle-map. In other words, differentiating the image irradiance ( <ref type="formula">1</ref>)</p><formula xml:id="formula_24">di dx d dx n Á s nd di dy d dy n Á s X QW</formula><p>Since the light source direction is constant with respect to x and y, these equations simplify to di dx dn dx Ás nd di dy dn dy ÁsY RH respectively. We incorporate these equations into constraint functions of similar forms to those investigated for curvature consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Weighted Mean Using Gradient Consistency</head><p>Using (40), we develop a similar constraint function to <ref type="bibr" target="#b30">(32)</ref>, but with weights defined as</p><formula xml:id="formula_25">w l exp À di dx À dn dx Á s P di dy À dn dy Á s P @ A 4 5</formula><p>X RI Hence, a neighborhood normal which has low gradient consistency is weighted less favorably than one which matches the image gradient closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Adaptive Robust Regularizer Using Gradient Consistency</head><p>Incorporating the gradient constraint into the robust regularizer constraint function described in <ref type="bibr" target="#b20">(22)</ref>, we define the adaptive kernel width to be the mean of the exponentials of the gradient consistency</p><formula xml:id="formula_26">' ' H I x lPx exp À di dx À dn l dx Á s P di dy À dn l dy Á s P @ A 4 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RP</head><p>If the image and needle-map gradients correspond well, ' is large and little smoothing is applied. However, if some of the neighborhood normals exhibit large deviations from image consistency, this causes the kernel to narrow, reducing the influence of the inconsistent normals. If the mean inconsistency is very large, the kernel narrows to the point where all the neighborhood normals are in the saturation region of the kernel, causing the algorithm to revert to the Horn and Brooks smoothing process.</p><p>We experiment with two additional methods for adapting the value of '. In the first case, we use the root mean value of the exponential consistencies as follows:</p><formula xml:id="formula_27">' ' H I x IPx exp Àw I s Y RQ</formula><p>where w l is defined as in (41). This approach is motivated by the same considerations as above, i.e., to widen the kernel when the image and needle-map gradients correspond well, and narrow it to increase the smoothing when this is not the case. By taking the root mean instead of the simple mean, we reduce the influence of outlying values of w l on the kernel width.</p><p>We also experiment with a scheme which uses the second derivatives of the image irradiance equation. Using this approach, we relate the kernel width to the Laplacian of the image. Thus, we attempt to obtain consistency on the basis of local edge strength rather than the image gradient. This yields an adaptive kernel width involving the Laplacian of the image</p><formula xml:id="formula_28">' ' H I x lPx exp À r P i À d P n l dx P d P n l dy P &amp; ' Á s ! X RR</formula><p>This edge-based approach has intuitive appeal as an image edge is likely to correspond to a surface discontinuity in many cases. If the mismatch between the local edge strength and the needle-map second derivatives is large, the kernel is narrowed to increase the smoothing, whereas if the image and needle-map are consistent, little smoothing occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary of Investigated Constraints</head><p>Table <ref type="table">1</ref> summarizes the constraint functions described in the preceding sections, along with the update equations which result from them. The new schemes are labeled DD1-9 for Data Driven. Clearly, Table <ref type="table">1</ref> is far from exhaustive. Many more constraint functions are possible, both within the categories considered here, and using other properties of surfaces and images. However, the rapid development of several alternatives to the smoothness constraint shows the power of the new framework for experimenting with different constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We have performed extensive experiments upon synthetic and real world images, using the SFS schemes summarized in Table <ref type="table">1</ref>. We also compare with the Horn and Brooks algorithm and two recent SFS schemes. These are the local shape-from-shading algorithms of Bichsel and Pentland <ref type="bibr" target="#b2">[3]</ref> and Tsai and Shah <ref type="bibr" target="#b26">[28]</ref>. The experimental work is divided into three parts. We commence by showing surface reconstructions and needle-maps delivered by the different shape-from-shading algorithms. Next, we investigate the iteration dependence of various performance measures. Finally, we show some results on real-world images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Data</head><p>Fig. <ref type="figure" target="#fig_7">5</ref> illustrates the surfaces which can be recovered from an image of two con-joined spheres. The figure compares the SFS schemes developed in this paper with the alternatives described above. Note that the Bichsel and Pentland algorithm and the Tsai and Shah algorithm recover a surface directly, while the Horn and Brooks algorithm and the new framework recover needle-maps. The latter must be integrated into a surface, which is an open problem in itself. Here, we take the simplest possible approach, summing the needle-map terms in the x and y directions to yield two surfaces, then taking the mean of these. This process is extremely sensitive to noise, so surfaces recovered by this method are often poor. However, we include surfaces in this paper because it is easier to compare surfaces than needle-maps. Conversely, if we calculate the needle-map from the surfaces generated by Tsai and Sha or Bichsel and Pentland and compare on a    surface is important given that our primary motivations for developing new constraint functions is to address the problem of oversmoothing. For instance, in Fig. <ref type="figure" target="#fig_7">5</ref>, the important feature is the curve defined by the intersection of the two spheres. The Horn and Brooks algorithm loses this detail, while Tsai and Shah is noisy. Bichsel and Pentland preserves the junction well, but distorts the spheres a little.</p><p>Of the new constraint functions, DD2 (robust smoothness error), DD5 (robust use of curvature consistency with adaptive kernel width), and, particularly, DD9 (robust use of gradient consistency with adaptive kernel width) appear to perform well. Empirically, it appears that the robust kernel approach is more effective than the weighted mean process. Also, the use of gradient consistency seems to yield slightly better results than curvature consistency. We now turn to the needle-maps delivered by the different algorithms. In Fig. <ref type="figure" target="#fig_8">6</ref>, we compare the needle-maps recovered for three objects. These are a pair of co-joined cones and a sphere impaled on an ellipsoid. We compare the new framework with the traditional Horn and Brooks algorithm. For simplicity, we use a quadratic constraint function (i.e., DD1) rather than one of the more complicated curvature consistency or gradient consistency constraints. Also included in the comparison are the Tsai and Shah and Bichsel and Pentland algorithms. For completeness, we investigate the effect of using the Horn and Brooks algorithm with the new initialization described in Section 3.2.</p><p>We note that the Tsai and Shah algorithm results in a noisy needle-map despite its relatively good surface recovery, demonstrated in Fig. <ref type="figure" target="#fig_7">5</ref>. The traditional Horn and Brooks algorithm, using the occluding boundary intialization, yields needle-maps which lack detail due to oversmoothing, whereas, with the new intialization, it recovers more structure. The new framework captures much more detail, especially around the discontinuity in the cone object. This extra detail is clear, even when using the simple smoothness constraint of Horn and Brooks, although it is also obvious that the quadratic smoothness constraint, i.e., DD1, introduces some errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Measures</head><p>In this section, we consider the iterative properties of the new shape-from-shading scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Accuracy and Convergence</head><p>Figs. 8 and 9 provide an empirical comparision of the normal-error and constraint functional values. We compare for the SFS algorithms which use the Horn and Brooks quadratic smoothness constraint. Fig. <ref type="figure" target="#fig_10">8</ref> is the most important since it shows how the normal error varies with iteration number. This plot demonstrates that the datacloseness constraint gives the smallest normal error. The standard Horn and Brooks algorithm starts from a large initial error and converges slowly. The new initialization process improves the starting point, but convergence is still much slower than the new shape-from-shading method. However, it is worth noting that the minimum normal error is still relatively high, of the order of 0.3 radians per normal on average, or 20 .</p><p>Fig. <ref type="figure" target="#fig_11">9</ref> shows the value of the functional used in each case, which is a measure often used in judging convergence in the absence of ground truth data. However, we see that not only do the functional values not possess distinct minima in some cases, but there is little correspondence between the functional value plots and the normal error plot of Fig. <ref type="figure" target="#fig_10">8</ref>. Although they begin with the same initialization, the new framework reduces the functional value far below the minimum value achieved by the modified Horn and Brooks algorithm; indeed, even the traditional Horn and Brooks algorithm performs better, in terms of minimizing the constraint functional, than the modified version using gradient initialization. This discrepancy is due to the fact that the occluding boundary initialization yields zero smoothness error everywhere except at the boundary, where the smoothness error is large. The traditional Horn and Brooks algorithm then propagates this boundary error into the object interior, dissipating it along the way. Starting with the new initialization gives nonzero smoothnes error in the interior of the object, so the propagation/dissipation   Second, even when using this constraint, the new framework succeeds in minimizing the constraint more effectively than the traditional Horn and Brooks algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Algorithm Comparison</head><p>We now turn our attention to the iterative behavior of the different algorithms listed in Table <ref type="table">1</ref>. The results are summarized in Fig. <ref type="figure" target="#fig_12">10</ref>. Several quantities are plotted against iteration number for the Horn and Brooks algorithm, the Robust Regularizer approach of <ref type="bibr" target="#b31">[33]</ref>, and each of the new framework schemes. In each case, the new initialization of Section 3.2 is used. The measures used are the normal-error, the surface reconstruction error, and the value of the constraint is functional.</p><p>Essentially, there are two primary criteria for a suitable SFS constraint function. The first is that it leads to a low minimum value of normal error, while the second, often overlooked, citerion is that there must be some method to stop the algorithm. Ideally, we wish termination to occur as closely as possible to the minimum of the normal error, despite the fact that we will not ordinarily know at which </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of update process for Horn and Brooks (left) and the new framework with the same smoothness constraint (right). Horn and Brooks allows the updated normal to move away from the cone of ambiguity, sacrificing brightness error for smoothness. The movement in the light source direction is sin the left-hand diagram, where P P! i À n Á s from Equation (8). In contrast, the new framework forces the brightness error to be satisfied by using the rotation matrix Â to map the smoothness update to the closest point on the cone.</figDesc><graphic coords="4,137.37,69.17,291.69,164.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 3 .</head><label>3</label><figDesc>Fig 3. A 1D bright image patch surrounded by darker regions causes the image gradient at surrounding locations to point toward the bright patch (top).A bright patch corresponds to a region presenting a large area toward the light source direction. Initializing the surface normals to point in the same direction as the negative gradient means that the bright region is always assumed to be a peak, as shown in the first diagram. However, the bottom two diagrams also show valid surfaces which will result in the same intensities, but for which the initialization will not work well. Given the bas-relief ambiguity, there is no way to distinguish between these possibilities, so we define our convention to assume the first case.</figDesc><graphic coords="6,55.73,69.17,192.08,437.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig.4shows the range of shape index values, the type of curvature which they represent, and the gray levels used to display different shape index values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The shape index scale ranges from ÀI to 1 as shown. The shape index values are encoded as a continuous range of grey level values between 1 and 255, with grey level 0 being reserved for background and flat regions (for which the shape index is undefined).</figDesc><graphic coords="7,297.13,73.08,235.28,59.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>TABLE 1</head><label>1</label><figDesc>Summary of the Constraint Functions Developed Using the New Framework DD1 and DD2 are surface geometric constraints. DD3-5 use curvature consistency, while DD6-9 are based upon gradient consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Surface reconstruction results for a synthetic image of two co-joined spheres.</figDesc><graphic coords="11,80.11,69.17,406.20,594.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of the various Horn and Brooks-based techniques with Tsai and Shah and Bichsel and Pentland algorithms and of the validity of the new framework and initialization. Top row: Synthetic input images. Second row: Results of applying Bichsel and Pentland algorithm. Third row: Tsai and Shah algorithm. Fourth row: Traditional Horn and Brooks algorithm with occluding boundary initialization. Fifth row: Horn and Brooks algorithm using new, gradient-based initialization. Bottom row: Results of using the smoothness constraint of Horn and Brooks within the new, datadriven framework (DD1).</figDesc><graphic coords="12,61.12,69.17,444.19,626.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of differences between the recovered needle-maps and ground truth data for the results of Fig. 6. Top row: Synthetic input images. Second row: Difference between ground truth and normals recovered using Bichsel and Pentland algorithm. Third row: Tsai and Shah algorithm. Fourth row: Traditional Horn and Brooks algorithm. Fifth row: Horn and Brooks with new initialization. Bottom row: Horn and Brooks smoothness constraint within the new, data-driven framework (DD1).</figDesc><graphic coords="13,63.04,69.17,440.33,628.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Surface normal error for synthetic images, using DD1, the traditional Horn and Brooks algorithm, and Horn and Brooks with the new intialization.</figDesc><graphic coords="14,31.69,69.17,240.09,186.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Constraint functional for synthetic images, using DD1, the traditional Horn and Brooks algorithm, and Horn and Brooks with the new initialization.</figDesc><graphic coords="14,294.69,69.17,240.09,186.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Comparision of the behavior of various measures for Horn and Brooks, the robust regularizer algorithm of<ref type="bibr" target="#b31">[33]</ref>, and the new framework using different constraint functions. In each case, the scales of the different measures have been scaled to give a maximum value of unity. Comparison of the form and location of the minima of each measure allows us to qualitatively consider questions of termination (see text).</figDesc><graphic coords="15,31.97,69.17,502.47,591.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Needle-maps recovered from toy duck image.</figDesc><graphic coords="16,95.36,69.17,375.70,529.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>os ÀI iX PH</figDesc><table><row><cell cols="4">Hence, the rotation matrix is given by</cell><cell></cell></row><row><cell>Â</cell><cell>H d</cell><cell>u P H ws uv H</cell><cell cols="2">Àws uv H vs uw H v P H Àus vw H</cell><cell>I e Y</cell></row><row><cell></cell><cell></cell><cell cols="2">Àvs uw H us vw H</cell><cell>w P H</cell></row><row><cell>where</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>os</cell><cell>H I À</cell><cell>s sinX</cell><cell>PI</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>The in depth analysis of the SFS problem presented here explicitly exposes the inadequacy of the traditional quadratic smoothness constraint to the task of SFS recovery, since finding the smoothest possible surface is demonstrably not an appropriate goal. With the new framework, we have demonstrated the development of principled, novel constraint functions modeling the properties of real surfaces. It is hoped that this research will stimulate the search for still better constraints, possessing global minima which more closely correspond to true surfaces.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>iteration this will occur. In the absence of ground truth, the obvious termination criterion is the minimization of the functional. However, it is clear from Fig. <ref type="figure">10</ref> that the minimization of the constraint functional is not an appropriate stopping criterion. Neither, it appears, is the average change in normals between iterations.</p><p>However, it is worth noting that, in the cases of most of the new framework constraint functions, the termination point is not critical since the normal error has only a shallow minimum or continues to fall for many iterations at a diminishing rate. Only curvature consistency measured using the median absolute deviation, i.e., DD4, breaks this pattern, exhibiting strange behavior wherein the normal error increases after initialization. The poor performance of DD4, which walks away from the initial needle-map estimate, is explained by the nonlinear median process incorporated in it, which appears to make it very susceptible to local noise. This effect introduces orientation errors at scales too small to be obvious in the surface reconstruction of Fig. <ref type="figure">5</ref>.</p><p>In the remaining cases, 100-200 iterations usually results in a good approximation to the minimum of the normal error. When the curvature and gradient consistency schemes DD3, DD5, and DD9 are used, the minimum of the functional value and the behavior of the curves suggest that using the minimization of the functional as a termination criterion may be adequate.</p><p>Since the normal error is an absolute measure unaffected by which constraint function is used and as all schemes use the same initialization and, so, start with identical values of normal error, we can compare these normalized values directly. Doing so, we see that when the normal smoothness and gradient consistency constraints are modeled using a robust error kernel, i.e., schemes DD2 and DD6, are used, then there is a reduction in the normal error by approximately 57 percent of the initial value. This value is lower than the normal error achieved by the traditional Horn and Brooks algorithm after 1,000 iterations. Hence, use of a robust kernel appears more appropriate than a weighted mean process and gradient consistency performs better, in this case, than curvature consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Real World Data</head><p>In Fig. <ref type="figure">11</ref>, we illustrate the application of several of the SFS schemes considered in this paper to real data. The image is from the Columbia Object Image Library and contains real world imperfections such as albedo changes, regions of brightness saturation, and deviations from the Lambertian assumption. Only four of the new constraint functons are used, one each from the surface geometric and gradient consistency categories and two curvature consistency techniques. With the exception of DD3, they are all based upon the use of the robust kernel approach.</p><p>In Fig. <ref type="figure">11</ref>, the needle-map recovered by the Horn and Brooks algorithm is seen to be extremely smooth, even losing large-scale details such as the wing of the toy duck. Both Bichsel and Pentland and Tsai and Shah algorithms yield noisy needle-maps when these are calculated from the recovered surfaces. The new framework schemes all perform well, recovering the wing while behaving well over the smooth regions of the surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We have presented a novel framework for the shape-fromshading problem which addresses the problem of model dominance encountered using many traditional global algorithms. Specifically, we ensure the fullest use of the data available, namely the input image, by incorporating the image irradiance equation as a hard constraint.</p><p>Our second contribution is to develop novel constraints on needle-map consistency. These range from simple modifications to the traditional smoothness constraint, to topographic curvature and gradient consistency. Using extensive experimentation upon synthetic and real shaded images, we have demonstrated that several of these novel constraints perform significantly better than various existing algorithms. Specifically, we find that incorporating an adaptive robust kernel to control local smoothing yields excellent results, especially when used in conjunction with gradient consistency constraints.</p><p>Gradient consistency makes only relatively weak assumptions about the surface physics. This is in contrast to the curvature consistency techniques developed here as a result of modeling the topographic properties of surfaces. The slight advantage of the gradient consistency schemes over the more complicated curvature consistency methods suggests that we can obtain good results without recourse to complex arguments about the behavior of real surfaces. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Angelopoulou</surname></persName>
		</author>
		<title level="m">ªGaussian Curvature from Photometric Scatter Plots,º Proc. IEEE Workshop Photometric Modeling for Computer Vision and Graphics</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<title level="m">ªWhat is the Set of Images of an Object Under All Possible Lighting Conditions?º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="270" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ªA Simple Algorithm for Shape from Shading</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bichsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="459" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ªShape and Source from Shading</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Joint Conf. Artifical Intelligence</title>
		<meeting>Int&apos;l Joint Conf. Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="932" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<title level="m">ªOn Shape from Shading,º Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Ferrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lagarde</surname></persName>
		</author>
		<title level="m">ªCurvature Consistency Improves Local Shading Analysis,º Proc. IEEE Int&apos;l Conf. Pattern Recognition</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="70" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Hilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Courant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods of Math. Physics</title>
		<imprint>
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Understanding Robust and Exploratory Data Analysis</title>
		<editor>D.C. Hoaglin, F. Mosteller, and J.W. Tukey</editor>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ªObtaining Shape from Shading Information.º The Psychology of Computer Vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<editor>P.H. Winston</editor>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>McGraw Hill</publisher>
			<biblScope unit="page" from="115" to="155" />
			<pubPlace>New York.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Brooks</surname></persName>
		</author>
		<title level="m">ªThe Variational Approach to Shape from Shading,º Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="174" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ªHeight and Gradient from Shading</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int&apos;l Joint Conf. Artifical Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="75" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Robust Statistics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K P</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªNumerical Shape from Shading and Occluding Boundaries</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="141" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ªTracking Level Sets by Level Sets: A Method for Solving the Shape from Shading Problem</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="58" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
		<title level="m">ªSurface Shape and Curvature Scales,º Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
		<title level="m">ªRelief Pictorial and Otherwise,º Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="321" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªPerturbation Study of Shading in Pictures</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ªShape from Shading with a Linear Triangular Element Surface Model</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Analysis and Machine</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="815" to="822" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">ªDiscontinuous MRF Prior and Robust Statistics: A Comparative Study,º Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="227" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Freeman</publisher>
			<pubPlace>San Francisco, Calif</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Oliensis</surname></persName>
		</author>
		<title level="m">ªShape from Shading as a Partially Well-Constrained Problem,º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="559" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ªA Global Algorithm for Shape from Shading</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oliensis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dupuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="692" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ªCurvature Consistency and Curve Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Parent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. America A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
		<title level="m">ªLocal Shading Analysis,º IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="170" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">ªFinding the Illuminant Direction,º J</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Optical Society of America</publisher>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ªComputational Vision and Regularization Theory</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">317</biblScope>
			<biblScope unit="page" from="314" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">ªShape from Shading Using Linear Approximation,º Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="487" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sethian</surname></persName>
		</author>
		<title level="m">Level Set Methods</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<title level="m">ªFast Shape from Shading,º Computer Vision, Graphics, and Image Processing: Image Understanding</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="129" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<title level="m">ªShape-from-Shading using Robust Statistics,º Proc. IEEE Int&apos;l Conf. Digital Signal Processing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<title level="m">ªNeedle Map Recovery Using Robust Regularizers,º Proc. British Machine Vision Conf</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<title level="m">ªNeedle Map Recovery using Robust Regularizers,º Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="545" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<title level="m">ªData Driven Shape-from-Shading using Curvature Consistency,º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<title level="m">ªSurface Topography from Intensity Images,º Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="911" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">ªAnalysis of Shape from Shading Techniques,º Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ªEstimation of Illuminant Direction, Albedo, and Shape from Shading</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="680" to="702" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
