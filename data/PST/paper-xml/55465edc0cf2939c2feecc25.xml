<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
							<email>dongzhen@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Beijing Laboratory of Intelligent Information Tech-nology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
							<email>wuyuwei@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Beijing Laboratory of Intelligent Information Tech-nology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Beijing Laboratory of Intelligent Information Tech-nology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Beijing Laboratory of Intelligent Information Tech-nology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yunde</forename><surname>Jia</surname></persName>
							<email>jiayunde@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Beijing Laboratory of Intelligent Information Tech-nology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1DAE2F65A848C6E8F698835BF8577EF</idno>
					<idno type="DOI">10.1109/TITS.2015.2402438</idno>
					<note type="submission">received June 27, 2014; revised October 21, 2014 and December 26, 2014; accepted February 2, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature learning</term>
					<term>filter learning</term>
					<term>multitask learning</term>
					<term>neural network</term>
					<term>vehicle type classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a vehicle type classification method using a semisupervised convolutional neural network from vehicle frontal-view images. In order to capture rich and discriminative information of vehicles, we introduce sparse Laplacian filter learning to obtain the filters of the network with large amounts of unlabeled data. Serving as the output layer of the network, the softmax classifier is trained by multitask learning with small amounts of labeled data. For a given vehicle image, the network can provide the probability of each type to which the vehicle belongs. Unlike traditional methods by using handcrafted visual features, our method is able to automatically learn good features for the classification task. The learned features are discriminative enough to work well in complex scenes. We build the challenging BIT-Vehicle dataset, including 9850 high-resolution vehicle frontal-view images. Experimental results on our own dataset and a public dataset demonstrate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>features (e.g., SIFT <ref type="bibr" target="#b9">[10]</ref>, Sobel edges <ref type="bibr" target="#b10">[11]</ref>) to represent the vehicle for classification. Most of these methods are based on vehicle side view images. Currently, large numbers of vehicle frontal view images are captured by traffic surveillance cameras, so we focus on vehicle type classification from vehicle frontal view images.</p><p>There has been less effort on methods based on vehicle frontal view images. Petrovic and Cootes <ref type="bibr" target="#b11">[12]</ref> modeled the vehicle appearance from vehicle frontal view images by extracting many features, such as Sobel edge response, edge orientation, direct normalized gradients, locally normalized gradients, and Harris corner response. Negri et al. <ref type="bibr" target="#b12">[13]</ref> presented a voting algorithm based on oriented-contour points for their multiclass vehicle type recognition system. Psyllos et al. <ref type="bibr" target="#b13">[14]</ref> used SIFT features to recognize the logo, manufacturer, and model of a vehicle. Zhang <ref type="bibr" target="#b14">[15]</ref> fused the PHOG feature and the Gabor transform feature to represent the vehicle and proposed a cascade classifier scheme to recognize the type of the vehicle. Peng et al. <ref type="bibr" target="#b15">[16]</ref> represented a vehicle by license plate color, vehicle front width, and type probabilities for vehicle type classification. However, these methods use multiple hand-crafted features which might not be discriminative enough in complex scenes.</p><p>In this paper, we propose a novel framework of vehicle type classification from vehicle frontal view images using a convolutional neural network. The convolutional neural network is a multi-layer feed-forward neural network which is biologicallyinspired. Unlike traditional methods by using hand-crafted features, the convolutional neural network is able to automatically learn multiple stages of invariant features for the specific task <ref type="bibr" target="#b16">[17]</ref>. It has been used to learn good features in face detection <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, facial point detection <ref type="bibr" target="#b19">[20]</ref>, pedestrian detection <ref type="bibr" target="#b20">[21]</ref>, human attribute inference <ref type="bibr" target="#b21">[22]</ref>, image quality assessment <ref type="bibr" target="#b22">[23]</ref>, image classification <ref type="bibr" target="#b23">[24]</ref>, and video classification <ref type="bibr" target="#b24">[25]</ref>.</p><p>The convolutional neural network in our method takes an original vehicle image as the input and outputs the probability of each vehicle type to which the vehicle belongs. The network contains two stages, and each stage consists of the convolutional layer, the absolute rectification layer, the local contrast normalization layer, the average pooling layer, and the subsampling layer. The convolutional layer computes the convolutions between the input and a set of filters (filter bank), and provide a nonlinear representation of the input signal by using a point-wise nonlinear function. The absolute rectification layer and local contrast normalization layer perform a nonlinear transformation on the output of the convolutional layer. The average pooling layer and subsampling layer reduce the spatial resolution of the representation to achieve the robustness to both geometric distortions and small shifts. The flowchart of the proposed method. Our convolutional neural network is semi-supervised. In order to capture rich and discriminative information of vehicles, the sparse Laplacian filter learning is employed to learn the filters of the network with a large number of unlabeled vehicles. The softmax classifier layer which is trained by multi-task learning with a small number of labeled vehicles is used as the output layer. For a given vehicle image, the network is able to automatically learns good features to represent the vehicle and outputs the probability of each type which the vehicle belongs to. The type of the vehicle can be predicted by picking the label which makes the probability achieving maximum.</p><p>Our network is semi-unsupervised, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The filter bank of the convolutional layer is learned in an unsupervised manner with large amounts of unlabeled data, and the parameters of the output layer are learned in a supervised manner with a certain amount of labeled data. Motivated by great success of unsupervised pre-training for multi-layer neural networks <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b28">[29]</ref>, we introduce the sparse Laplacian filter learning (SLFL), an unsupervised learning method, to learn the filter bank of the convolutional layer. Different from traditional sparsity constraints like l0, l1 or l2 norms, the SLFL uses the sparse function sps(•) with properties of population sparsity, high dispersal, and lifetime sparsity to measure the sparsity of representations. During learning filters, the manifold assumption <ref type="bibr" target="#b29">[30]</ref> is considered to ensure that similar input image patches have similar high-level representations. The learned filters are able to capture rich and discriminative information of vehicles for improving the classification performance.</p><p>We adopt a softmax classifier as the output layer to calculate the probability of each vehicle type. A supervised learning method is introduced to learn the parameters of the classifier. We observe that many vehicle types share large number of common appearance patterns. For example, the vehicles of "truck" and "minivan" have the similar parts layouts, and both of them have a hopper. The multi-task learning <ref type="bibr" target="#b30">[31]</ref> is used to learn the shared common appearance patterns. The appearance pattern is regarded as a latent task, and the parameter of each type model is reconstructed by the linear combination of the latent tasks. The constraints of the latent tasks and combination coefficients are important to obtain a robust model for classification. To enhance the discriminative power of the latent task, we represent the important appearance patterns by employing the l1-norm. The l1-norm of each category's combination coefficient vector is able to reduce the noise of reconstruction.</p><p>The rest of the paper is organized as follows. In Section II, we show the architecture of the convolutional neural network and its implementation. In Section III, the learning methods of the network parameters are described, including learning the filer bank and the parameters of the softmax layer. Experimental results and discussions are reported in Sections IV and V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. ARCHITECTURE OF THE CONVOLUTIONAL NEURAL NETWORK</head><p>The architecture of our convolutional neural network is shown in Fig. <ref type="figure">2</ref>. The network contains two stages which generate low-level local features and high-level global features, respectively. The high-level global features provide holistic descriptions of vehicles, and the low-level features aim to characterize vehicle parts precisely. In order to take full advantage of both global features and local features, the network is with layer-skipping which integrates the features learned in both the 1st and the 2nd stage for classification. There are five layers in each stage, i.e., the convolutional layer, the absolute value rectification layer, the local contrast normalization layer, the average pooling layer, and the subsampling layer. In the convolutional layer, the sparse Laplacian filter learning (SLFL) provides effective filters for the network. The absolute rectification layer and local contrast normalization layer provide a nonlinear transformation for the output of the convolutional layer. The pooling and subsampling layers use the average pooling operator to reduce the spatial resolution of the representation. The representations can thus be robust to geometric distortions and small shifts. We employ a softmax classifier as the output layer of the network to compute the probability of each vehicle type. For simplicity, we use x and y to represent the input and output of each layer, respectively. They are both 3D arrays where x are with the size of s1 × s2 × s3 and y with the size of t1 × t2 × t3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Layer</head><p>In the convolutional layer, convolutions between the input and a series of filters are first computed. An element-wise nonlinear activation function is then executed on the convolutions. Fig. <ref type="figure">2</ref>. The architecture of our semi-supervised convolutional neural network. The network contains two stages. Each stage consists of the convolutional layer, the absolute rectification layer, the local contrast normalization layer, the average pooling layer, and the subsampling layer. The number in brackets behind "convolution" is the size of the filters. The number in brackets behind "average pooling" is the size of the average filter, and the number in brackets behind "subsampling" reflects the step of subsampling. The numbers around cuboids describe the size of 3D feature arrays. The network is with layer-skipping which integrates the features learned in both 1st and 2nd stage together for classification. The softmax classifier is applied as the output layer to compute the type probability of the input vehicle.</p><p>The layer provides a non-linear mapping from the low level image representation to high level semantic understanding, which simulates the "simple cells" in the standard models of the visual cortex <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The input x is a 3D array with the size of s1 × s2 × s3, where s3 is the number of 2D feature maps, and s1 × s2 is the size of the 2D feature map which is represented by x i . The output y is also a 3D array whose size is t1 × t2 × t3. Similar to the 2D feature map x i of the input, y j is defined as the j-th 2D feature map of the output. The element-wise sigmoid function sig(•) is chosen as the nonlinear activation function. Hence, y j is computed by</p><formula xml:id="formula_0">y j = sig i k ij ⊗ x i ,<label>(1)</label></formula><p>where ⊗ denotes the convolution operation, and k ij is a 2D filter learned by the sparse Laplacian filter learning described in Section III-A. Suppose that the filter size is l1 × l2, we have t1 = s1 -l1 + 1 and t2 = s2 -l2 + 1 due to the board effects. The size of input 2D feature map is an important factor. A large size is beneficial for learning good features of vehicles, but the computation time cost will be high. A small size saves time, but may lose too much information, which leads to low classification accuracy. As shown in Fig. <ref type="figure">2</ref>, in the 1st stage of the network, the size of the input 2D feature map is set as 143 × 143 to take a balance between the computation time cost and the accuracy. The size of output 2D feature map is 135 × 135 since the filters are with the size of 9 × 9 which is set according to the size of the input 2D feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Absolute Value Rectification Layer</head><p>In this module, all the elements are passed into the absolute value rectification function</p><formula xml:id="formula_1">y i,j,k = |x i,j,k |, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where x i,j,k and y i,j,k represent each element of x and y, respectively. The absolute value rectification layer is inspired by the fact that the relationship between two items in real world is always positive or zero, but not negative. It is clear that the sizes of the input and the output of the absolute value rectification layer are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Local Contrast Normalization Layer</head><p>The purpose of the local contrast normalization layer is to enforce local competitions between one neuron and its neighbors, which is motivated by the computational neuroscience <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. The neighbors include nearby neurons in the same feature map and the ones at the same 2D location in different feature maps. To do this, two normalization operations are performed, i.e., subtractive and divisive. For the element x i,j,k in the input 3D array size of s1 × s2 × s3, the subtractive normalization operator is given by</p><formula xml:id="formula_3">z i,j,k = x i,j,k - 4 p=-4 4 q=-4 s3 r=1 ω p,q x i+p,j+q,r,<label>(3)</label></formula><p>where ω p,q is a normalized Gaussian filter with the size of 9 ×9, z is the output of the subtractive normalization and the input of the divisive normalization. The divisive normalization operator is defined as</p><formula xml:id="formula_4">y i,j,k = z i,j,k max (M, M (i, j)) ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">M (i, j) = 4 p=-4 4 q=-4 s3 r=1 ω p,q z 2 i+p,j+q,r , (5) M = s1 i=1 s2 j=1 M (i, j) /(s1 × s2). (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>In both operations, the filtering by ω p,q is computed with the zero-padded edges. It is obvious that the size of the output is the same as the input in the local contrast normalization layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Average Pooling and Subsampling Layers</head><p>The pooling and subsampling layers aim to make the representation robust to both geometric distortions and small shifts. Their roles are essentially equivalent to the "complex cells" in the standard models of the visual cortex <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. We adopt the average pooling method in the pooling layer. The convolution between the 2D feature map and the average filter is formulated as</p><formula xml:id="formula_7">y i,j,k = p,q α p,q x i+p,j+q,k, (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where α p,q = 1/(f 1 × f 2) is the average filter with the size of f 1 × f 2, x and y are the input and output 3D arrays of the average pooling layer, respectively. The subsampling procedure is performed on the output of the average pooling layer with the rate of p1 horizontally and p2 vertically. Suppose that the input 2D feature maps of these two layers are size of s1 × s2, and the size of the output 2D feature map is t1 × t2, we have</p><formula xml:id="formula_9">t1 = s1 -f 1 p1 + 1, t2 = s2 -f 2 p2 + 1,<label>(8)</label></formula><p>where δ denotes the maximum integer less or equal than δ.</p><p>For example, the input 3D array of the pooling layer in the 1st stage is with the width and height of s1 = s2 = 135. The appropriate average filter size and subsampling rate are set for simplicity. The average filter size is f 1 = f 2 = 10, and the subsampling rates are 5 in both the horizontal direction and the vertical direction. Therefore, the size of the output 2D array is t1 = t2 = (135 -10)/5 + 1 = 26.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Softmax Classifier Layer</head><p>In order to calculate the probability of each vehicle type, the softmax classifier is employed as the output layer of the convolutional neural network. As shown in Fig. <ref type="figure">2</ref>, the input of the softmax classifier layer is the feature vector learned by previous layers, and the output is the type probability vector. A linear function is applied to model the relationship between the feature and the probability distribution of the vehicle type</p><formula xml:id="formula_10">v = W x + b,<label>(9)</label></formula><p>where x ∈ R D×1 represents the input feature, v ∈ R C×1 is a intermediate variable for describing the distribution, and C is the number of vehicle types. For simplicity, Eq. ( <ref type="formula" target="#formula_10">9</ref>) is rewritten as</p><formula xml:id="formula_11">v = W x + b = [W b] x 1 = W x 1 , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where</p><formula xml:id="formula_13">W = [w 1 , w 2 , • • • , w C ] ∈ R (D+1)×C</formula><p>, and each column of W is the corresponding vehicle type model parameter. Because the probability has the properties of nonnegativity and unitarity, v is normalized as</p><formula xml:id="formula_14">y i = 1 V e v i , i = 1, 2, • • • , C, V = C i=1 e v i ,<label>(11)</label></formula><p>where v i is the i-th element of v, and y = [y 1 , y 2 , • • • , y C ] is the output of the softmax classifier layer. The parameter W can be learned by the multi-task learning described in Section III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PARAMETERS LEARNING</head><p>As discussed in Section II, two kinds of parameters of the network should be learned, i.e., the filter bank of the convolutional layer and the parameter of the softmax classifier. In this section, we elaborate how to learn these two parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse Laplacian Filter Learning</head><p>We introduce the sparse Laplacian filter learning (SLFL), an unsupervised learning method, to learn the filter bank of the convolutional neural network. Define a data matrix as</p><formula xml:id="formula_15">U = [u 1 , u 2 , • • • , u n ] ∈ R d×n</formula><p>, where the columns are data points. Our goal is to learn the filter bank K ∈ R d×t which consists of t filters. Using this filter bank, the input data points U can be mapped to sparse representations A. The nonlinear map function is given by</p><formula xml:id="formula_16">A = sig(K U ),<label>(12)</label></formula><p>where sig(•) is the element-wise sigmoid function which is commonly used as the activation function of the neural network.</p><p>A ∈ R t×n is the feature distribution matrix over U , where the row is a feature and the column is an example. The element A i,j represents the activation of the i-th feature on the j-th example of A. We formulate the sparse Laplacian filter learning as</p><formula xml:id="formula_17">min B,A,K U -BA 2 F + αsps(A) + βtr(ALA ) + γ A -sig(K U ) 2 F , (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where • F denotes the Frobenius norm of the matrix, tr(•) represents the trace of a matrix, L is the Laplacian matrix, α, β, and γ are the regularized parameters. Similar to the sparse coding, the first term pursues accurate reconstruction by the dictionary B, in other words, each data point u i can be linearly represented by the bases of the dictionary B, meanwhile keeping the reconstruction error as small as possible.</p><p>The sps(•) function in Eq. ( <ref type="formula" target="#formula_17">13</ref>) is optimized for the sparsity in the feature distribution <ref type="bibr" target="#b35">[36]</ref>. It avoids modeling the data distribution explicitly, which gives rise to a simple formulation and permits the effectiveness of learning. Let</p><formula xml:id="formula_19">A (i, ) ∈ R 1×n (i = 1, 2, • • • , t) be the i-th row of A, and A ( ,j) ∈ R t×1 (j = 1, 2, • • • , n) the j-th column of A.</formula><p>The sps(•) function is computed in three steps: normalizing the feature distribution matrix by rows, normalizing the feature distribution matrix by columns, and summing up the absolute values of all entries. In the first step, each feature is divided by its 2-norm across all examples, i.e., A (i, ) = A (i, ) / A (i, ) 2 . In this way, the feature is equally active. In the second step, each example is divided by its 2-norm across all features, Â( ,j) = A ( ,j) / A ( ,j) 2 , to ensure that all examples lie on the unit 2-ball. In the third step, we sum up the absolute values of all the entries in Â. The sps(•) function is given by</p><formula xml:id="formula_20">min K Â * = t i=1 n j=1 | Âi,j | = n j=1 Â( ,j) 1 = n j=1 A ( ,j) A ( ,j) 2 1 , (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>where M * denotes summing up the absolute values of all the elements in the matrix M. The minimization of the sparse function makes the feature distribution A have three properties, i.e., population sparsity, high dispersal, and lifetime sparsity <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Population Sparsity: The population sparsity requires that the example should only have a few active (non-zero) features, and it is considered to be an efficient coding method in early vision cortex. The term Â( ,j)</p><formula xml:id="formula_22">1 = A ( ,j) A ( ,j) 2 1</formula><p>in Eq.( <ref type="formula" target="#formula_20">14</ref>) reflects the population sparsity property of the features on the j-th example. Since Â( ,j) has been constrained to lie on the unit 2-ball, the objective function is minimized when the features are sparse. In other words, the objective tends to place the examples close to feature axes, and the example which has similar values on the features would have a high penalty.</p><p>High Dispersal: The high dispersal property denotes that the statistics of the features should be similar, which implies that the contributions of all features are similar. Here, the statistics are taken as the mean squared activations by averaging the squared values in the feature matrix across all the examples</p><formula xml:id="formula_23">T i = 1 n n j=1 A 2 ij = 1 n A (i, ) 2 2 . (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>Each feature is divided by its 2-norm across all examples in the first step of computing the objective function, which makes the features equally active. Therefore, the objective function is optimized for high dispersal.</p><p>Lifetime Sparsity: The property that the feature should be active only on a few examples is called lifetime sparsity. This property guarantees that the feature is discriminative enough to distinguish different examples. Specifically, each row of the feature distribution matrix should only have a few active (non-zero) entries. In the sparse filtering algorithm, the lifetime sparsity property is ensured by the population sparsity property and the high dispersal property. The feature distribution matrix should have a great many non-active (zero) elements due to the population sparsity property. These non-active elements could not be placed in a few specific rows, otherwise it would be against the high dispersal property. Therefore, the feature would have a significant number of non-active elements and thus be lifetime sparse.</p><p>The third term of Eq. ( <ref type="formula" target="#formula_17">13</ref>) incorporates the manifold assumption into the objective function. The manifold assumption implies that close-by data points tend to have similar representations and distant ones are less likely to take similar representations. This can be achieved by approximating the structure of the manifold with a graph. Each point of the graph represents a data point x i , and the edge weight matrix R is defined as</p><formula xml:id="formula_25">R ij = u i u j u i u j if u i ∈ N ε (u j ) or u j ∈ N ε (u i ) 0 otherwise,<label>(16)</label></formula><p>where N ε (u i ) represents the set of ε nearest neighbors of u i . The edge weight matrix satisfies that large values R ij is corresponding to nearby data points. The manifold assumption is formulated as the minimization of</p><formula xml:id="formula_26">1 2 n i,j=1 A ( ,i) -A ( ,j) 2 = T r(ALA ),<label>(17)</label></formula><p>where L = D -R is the Laplacian matrix, and D is a diagonal matrix whose elements are column (or row) sums of R.</p><p>The last term of Eq. ( <ref type="formula" target="#formula_17">13</ref>) pursuits the minimal error between A and the nonlinear mapping of U . The term is added into the objective function to optimize B, A, and K jointly. The optimization procedure contains two alternating steps.</p><p>STEP 1: Keeping parameters B and K fixed, learn the representation A by solving the optimization problem:</p><formula xml:id="formula_27">min A U -BA 2 F + α sps(A) + βT r(ALA ) + γ A -sig(K U ) 2 F . (<label>18</label></formula><formula xml:id="formula_28">)</formula><p>In our implementation, the L-BFGS optimization method <ref type="bibr" target="#b38">[39]</ref> is used. The gradient of the objective function in Eq. ( <ref type="formula" target="#formula_27">18</ref>) with respect to A is easy to solve. The gradient of sps(•) function is computed by the back propagation algorithm. As the sps(•) contains absolute value operators which are nondifferentiable, we ignore the absolute value operators when calculating the gradient to get an approximation. STEP 2: With the optimal value of A from STEP 1, minimize Eq. ( <ref type="formula" target="#formula_17">13</ref>) with respect to B and K. The optimization problem is rewritten as</p><formula xml:id="formula_29">min B,K U -BA 2 F + γ A -sig(K U ) 2 F . (<label>19</label></formula><formula xml:id="formula_30">)</formula><p>Because two terms of the objective function are not correlated, they can be solved independently. The optimal dictionary B can be achieved by minimizing</p><formula xml:id="formula_31">min B U -BA 2 F . (<label>20</label></formula><formula xml:id="formula_32">)</formula><p>An analytical solution of B is obtained that B = UA (AA ) -1 . The columns of B are then re-scaled to unit norm to avoid trivial solutions that are due to the ambiguity of the linear reconstruction. Unlike the first term, K cannot be solved analytically due to the element wise function. Instead, IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS the L-BFGS optimization method <ref type="bibr" target="#b38">[39]</ref> is used to minimize the second term with respect to K:</p><formula xml:id="formula_33">min K A -sig(K U ) 2 F . (<label>21</label></formula><formula xml:id="formula_34">)</formula><p>The overall optimization procedure of the sparse Laplacian filter learning is summarized in Algorithm 1, where the "convergence" is the value difference of the objective function in Eq. ( <ref type="formula" target="#formula_16">12</ref>) smaller than a threshold, or the iterative times exceeds another threshold. Since the Step 2 and 3 of Algorithm 1 are both based on L-BFGS, the computation complexity of the algorithm is O(kt</p><formula xml:id="formula_35">2 (n 2 + d 2 ))</formula><p>where k is the iterative times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Task Learning</head><p>We provide a supervised learning procedure for the parameters of the softmax layer. The learning method is based on the observation that many vehicle types share a great many common appearance patterns. We use the multi-task learning method <ref type="bibr" target="#b30">[31]</ref> to learn the shared knowledge between different vehicle types. The shared knowledge is corresponding to the latent tasks, and the model of each vehicle type can be combined by the latent tasks.</p><p>Specifically, each column of W is a vehicle type classifier and can be represented by the linear combination of the latent tasks. Define T ∈ R (D+1)×K as the shared latent task matrix with each column characterizing a latent task, and K is the number of latent tasks. The linear combination weight matrix is defined as C ∈ R K×C with each column representing the combination coefficients of the corresponding vehicle type. We thus have</p><formula xml:id="formula_36">W = T C. (<label>22</label></formula><formula xml:id="formula_37">)</formula><p>We will learn T and C simultaneously instead of learning W directly. Denote the training samples as {(x (i) , d (i) )|i = 1, 2, • • • , N} where x (i) ∈ R (D+1)×1 is the input feature vector (with the additional constant dimension), d (i) is the probability distribution of the type of x (i) . If x (i) belongs to the j-th type (1 ≤ j ≤ C), the j-th element of d (i) will be 1 and others will be 0. In order to learn T and C, we introduce the Kullback-Leibler (KL) divergence as the optimization principle min</p><formula xml:id="formula_38">T ,C N i=1 KL(d (i) y (i) ) = min T ,C N i=1 C j=1 d (i) j ln 1 y (i) j - C j=1 d (i) j ln 1 d (i) j = min T ,C - N i=1 C j=1 d (i) j ln y (i) j , (<label>23</label></formula><formula xml:id="formula_39">)</formula><p>where</p><formula xml:id="formula_40">d (i)</formula><p>j and y (i) j represent the j-th element of d (i) and y (i) , respectively.</p><p>The constraints to T and C are also very important to learn a robust model for vehicle type classification. Discriminative information may be lost if vehicle types share too much holistic information. We expect that the latent tasks focus on the basic visual patterns that can be shared by vehicle types. To achieve this goal, the 1-norm of the latent task is employed. We assume that the vehicle type model can be reconstructed by only a small number of latent tasks. Latent tasks are thus shared only among related vehicle types and hold high discriminative power. This can be achieved by minimizing the 1-norm of each row of C. The Frobenius-norm regularization of T is used to avoid overfitting. Taking these three constraints into account, the objective function for learning T and C is formulated as <ref type="bibr" target="#b23">(24)</ref> where • * denotes summing up the absolute values of all the elements in the matrix.</p><formula xml:id="formula_41">min T ,C - N i=1 C j=1 d (i) j ln y (i) j + λ T 2 F + μ T * + η C * ,</formula><p>The objective function of Eq. ( <ref type="formula">24</ref>) is not jointly convex in T and C, but it is convex in C with fixed T and convex in T with fixed C. Therefore, we adopt the alternating optimization strategy to solve Eq.( <ref type="formula">24</ref>). The two steps of the optimization method are as follows.</p><p>STEP 1: With fixed T , the optimal combination weight matrix C can be obtained by solving</p><formula xml:id="formula_42">min C - N i=1 C j=1 d (i) j ln y (i) j + η C * . (<label>25</label></formula><formula xml:id="formula_43">)</formula><p>The objective function is not smooth with respect to C as the existence of • * . Fortunately, the first term is a differentiable convex function, and the second term is convex but non-smooth. The accelerated proximal gradient method (APG) <ref type="bibr" target="#b39">[40]</ref> is able to solve the optimization problem. For simplicity, we define</p><formula xml:id="formula_44">f (C) = - N i=1 C j=1 d (i) j ln y (i) j , g(C) = η C * . (<label>26</label></formula><formula xml:id="formula_45">)</formula><p>Following the update scheme in <ref type="bibr" target="#b39">[40]</ref>, the APG uses the linear combination of previous two points C i-1 and C i-2 as the next search point C i :</p><formula xml:id="formula_46">C i = r i-1 + r i-2 -1 r i-1 C i-1 - r i-2 -1 r i-1 C i-2 , C i = h C i - 1 ξ ∇ C f (C i ); η ξ , (<label>27</label></formula><formula xml:id="formula_47">)</formula><p>where ξ is the Lipschitz constant calculated by the backtracking search method, r is initialized as 1 and updated as r i = (1 +</p><p>1 + 4r 2 i-1 )/2, and h(x; α) = max(|x|α, 0) sgn(x) is the shrinkage operator.</p><p>STEP 2: Keeping C fixed, the optimal latent task matrix T is learned by solving</p><formula xml:id="formula_48">min T - N i=1 C j=1 d (i) j ln y (i) j + λ T 2 F + μ T * . (<label>28</label></formula><formula xml:id="formula_49">)</formula><p>Similar to STEP 1, Eq. ( <ref type="formula" target="#formula_48">28</ref>) is also solved by using the APG algorithm <ref type="bibr" target="#b39">[40]</ref>, where f (T ) and g(T ) are defined as</p><formula xml:id="formula_50">f (T ) = - N i=1 C j=1 d (i) j ln y (i) j + λ T 2 F , g(T ) = μ T * . (<label>29</label></formula><formula xml:id="formula_51">)</formula><p>The overall algorithm for learning W is summarized in Algorithm 2. The algorithm is converged when the value difference of the objective function in Eq. ( <ref type="formula">24</ref>) is under a small threshold. Since the second and third step of Algorithm 2 are both based on APG, their convergence rates are O(k -2 1 ) and</p><formula xml:id="formula_52">O(k -2 2 )</formula><p>where k 1 and k 2 are the iterative times of the two steps, respectively.</p><p>Obtaining the optimized parameters, we calculate the type probability of the test vehicle image by using the convolutional neural network. The type of the test vehicle can be predicted by picking the label which makes the probability achieving maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets, Settings, and Preprocessing</head><p>We constructed a complex and challenging vehicle dataset called BIT-Vehicle Dataset<ref type="foot" target="#foot_0">1</ref> which includes 9,850 vehicle images to test the proposed method. The proportion of nightlight images in the whole dataset is about 10%. Fig. <ref type="figure" target="#fig_1">3</ref> shows the example images of the dataset, there are images with sizes of 1600 × 1200 and 1920 × 1080 captured from two cameras at different time and places. The images contain changes in the illumination condition, the scale, the surface color of vehicles, and the viewpoint. The top or bottom parts of some vehicles are not included in the images due to the capturing delay and the size of the vehicle. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, there may be one or two vehicles in one image, so the location of each vehicle is pre-annotated. The dataset can also be used for evaluating the performance of vehicle detection. All vehicles in the dataset We also test our method on the dataset in <ref type="bibr" target="#b40">[41]</ref>. There are totally 3,618 daylight and 1,306 nightlight images with the image size of 1600 × 1264 in the dataset. All the images are captured in highways with a fixed camera. The vehicles in the images fall into five categories: Truck, Minivan, Bus, Passenger car, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on BIT-Vehicle Dataset</head><p>We test our method on BIT-Vehicle dataset and report the performance. Our approach achieves 88.11% accuracy. The confusion matrix is shown in Table <ref type="table">I</ref>. From the matrix, we find that most of the misclassifications are between "SUV" and "Sedan". This is because they have quite similar appearances. Fig. <ref type="figure" target="#fig_3">5</ref> shows some of the classified real word images together with their classification results. Our model can precisely classify vehicle types in some challenging situations, such as different lighting conditions, vehicle parts invisible, and viewpoint changes. The primary reason is that our convolutional neural network is able to learn discriminative features for vehicle type classification. As shown in the last row of Fig. <ref type="figure" target="#fig_3">5</ref>, most of the misclassifications are due to visually similar appearance patterns in different vehicle types (e.g., "Sedan" and "SUV") and significant image blurring.</p><p>To evaluate the effect of filters learned by the sparse Laplacian filter learning (SLFL), we replace them by random values. The classification accuracy is displayed in Table <ref type="table" target="#tab_0">II</ref>. It shows that the network with learned filters outperforms that with random filters. The performance is significantly improved by using the  SLFL as the sparse filters learned from unlabeled vehicles are able to capture rich discriminative information of vehicles. The effect of the manifold assumption involving in the SLFL is also verified. We simply set β in Eq. ( <ref type="formula" target="#formula_17">13</ref>) as 0 to remove the Laplacian term. The filters are learned and applied in the convolutional neural network. The classification accuracy is also shown in Table <ref type="table" target="#tab_0">II</ref>. The performance difference between the "SLFL" and the "SLFL without Laplacian" demonstrates the effectiveness of the manifold assumption during learning filters. The sparse function sps(•) in Eq. ( <ref type="formula" target="#formula_17">13</ref>) is the same with the objective function of sparse filtering <ref type="bibr" target="#b35">[36]</ref>. We use the sparse filtering to learn filters for classification and compare the accuracy with the SLFL. The high performance of "SLFL" shows that the SLFL is more effective in learning filters. The reason is that the SLFL takes the reconstruction, the sparse property, and the manifold assumption all into account, while the sparse filtering method only considers the sparse property. We further investigate the contribution of the criterion for learning the parameters W of the softmax layer. We use the simple KL divergence which is the same with the objective function of Eq. ( <ref type="formula" target="#formula_38">23</ref>) as the criterion for learning W. The effectiveness of the constraints for the latent task matrix and the combination coefficient matrix is also evaluated. We remove T * and C * from Eq. ( <ref type="formula">24</ref>) and report the classification performances, respectively. The results are all described in Table <ref type="table" target="#tab_1">III</ref>. As shown in the table, all the three constraints are beneficial for learning the softmax parameter. The benefits of the observation that many vehicle types are highly correlated can be clearly seen from the accuracy difference between the "KL divergence" and the "Multi-task learning". The learned   <ref type="table" target="#tab_1">III</ref>. Furthermore, the effect of the network depth and the benefit of the layer-skipping strategy are verified. We evaluate two types of features. For the first one, only the 1st stage is used to learn vehicle features, and the dimensionality of the final feature is 2304. For the second one, the 2nd stage is added but without layer-skipping strategy, and the dimensionality of the final feature is 4096. Their average classification accuracies are shown in Table <ref type="table" target="#tab_2">IV</ref>, and the performance difference emphasizes that multi-stage is better than one stage for vehicle feature learning. The layer-skipping strategy connects the features learned from the 1st stage and the 2nd stage. The features learned from the 1st stage are low-level and local, and the outputs of the 2nd stage are high-level global features. Our model uses the high-level global features to capture rich and discriminative information. With low-level local features, our model is able to describe the details of the vehicle precisely. Therefore, these two types of features can be effectively used by adding the layer-skipping strategy into the network, as illustrated in Table <ref type="table" target="#tab_2">IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison Results</head><p>We test our method on the dataset in <ref type="bibr" target="#b40">[41]</ref>. Similar to <ref type="bibr" target="#b40">[41]</ref>, the experiments on daylight images and nightlight images are performed respectively. Since there is only one vehicle in an image of the dataset, the image is directly used as the input of the convolutional neural network to learn features without vehicle detection. The reported results of the dataset are the averages of 20 independent experiments for a better estimation of the generalization performance. Our method achieves 96.1% classification accuracy on daylight images and 89.4% on nightlight images, better than the results of previous methods, as demonstrated in Table <ref type="table" target="#tab_3">V</ref>. The underlying reason is that the convolutional neural network we use is able to learn discriminative and reliable features for vehicle type classification. The unsupervised pre-trained filters can capture rich and discrimina-  <ref type="bibr" target="#b40">[41]</ref> tive information of vehicles. The multi-task learning is able to learn the robust model for classification. In addition, the layerskipping strategy allows the classifier use both high-level global and low-level local features. It should be noted that our method outperforms other methods even without vehicle detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have proposed a vehicle type classification method from vehicle frontal view images by using a semi-supervised convolutional neural network. The filters of the network are learned by the proposed sparse Laplacian filter learning method to capture rich and discriminative information of vehicles. Serving as the output layer, the softmax classifier is trained by the multitask learning. The network takes the vehicle image as the input and outputs the probability of each type to which the vehicle belongs. The features learned by the network are discriminative enough to work well in complex scenes. Experimental results on our own BIT-Vehicle dataset and a public dataset demonstrate the effectiveness of the proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. The flowchart of the proposed method. Our convolutional neural network is semi-supervised. In order to capture rich and discriminative information of vehicles, the sparse Laplacian filter learning is employed to learn the filters of the network with a large number of unlabeled vehicles. The softmax classifier layer which is trained by multi-task learning with a small number of labeled vehicles is used as the output layer. For a given vehicle image, the network is able to automatically learns good features to represent the vehicle and outputs the probability of each type which the vehicle belongs to. The type of the vehicle can be predicted by picking the label which makes the probability achieving maximum.</figDesc><graphic coords="2,107.25,69.40,384.12,180.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The example images of BIT-Vehicle Dataset. All vehicles in our dataset fall into 6 types: Bus, Microbus, Minivan, Sedan, SUV, and Truck.</figDesc><graphic coords="7,302.55,443.19,246.12,112.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The example images of the dataset in [41]. This dataset consists of 5 types of vehicles: Bus, Minivan, Passenger car, Sedan, and Truck.</figDesc><graphic coords="8,44.74,70.16,246.12,132.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Some real world images with classification results.</figDesc><graphic coords="8,307.76,69.76,246.12,176.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,42.04,69.43,504.12,184.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,42.27,217.89,250.53,201.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II THE</head><label>II</label><figDesc>CLASSIFICATION ACCURACIES VERSUS DIFFERENT FILTER LEARNING METHODS ON BIT-VEHICLE DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>ACCURACIES VERSUS DIFFERENT CLASSIFIERS</figDesc><table /><note><p>ON BIT-VEHICLE DATASET</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>ACCURACIES VERSUS DIFFERENT FEATURES</figDesc><table /><note><p>ON BIT-VEHICLE DATASET shared knowledge between vehicle types is demonstrated to be effective for classification. The constraints of latent task matrix and coefficient matrix are also beneficial to generate robust model, as illustrated in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V COMPARISON</head><label>V</label><figDesc>RESULTS OF DIFFERENT METHODS ON THE DATASET IN</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The BIT-Vehicle Dataset can be accessed for research purpose by the link of http://iitlab.bit.edu.cn/mcislab/vehicledb.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61203291, by the Specialized Research Fund for the Doctoral Program of Higher Education of China under Grant 20121101120029, and by the Specialized Fund for Joint Building Program of Beijing Municipal Education Commission. The Associate Editor for this paper was P. Grisleri.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vehicle type classification from visual-based dimension estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detection and classification of vehicles</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="47" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic traffic surveillance system for vehicle tracking and classification</title>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="187" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three-dimensional deformable-model-based localization and recognition of road vehicles</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Edge-based rich representation for vehicle classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A PCA-based vehicle classification framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 22nd Int. Conf. Data Eng. Workshops</title>
		<meeting>IEEE 22nd Int. Conf. Data Eng. Workshops</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="17" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-based vehicle type classification using partial Gabor filter bank</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Logist</title>
		<meeting>IEEE Int. Conf. Autom. Logist</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1037" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of discriminative edge measures for vehicle matching between nonoverlapping cameras</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="700" to="711" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vehicle classification based on hierarchical support vector machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Engineering and Networking</title>
		<meeting><address><addrLine>Dordrecht, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Camera models and machine perception</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<pubPlace>Stanford, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford Univ. Artif. Intell. Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. AIM-21</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analysis of features for rigid structure vehicle type recognition</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An oriented-contour point based voting algorithm for vehicle type classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Clady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poulenard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Pattern Recog</title>
		<meeting>IEEE Int. Conf. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="574" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vehicle model recognition from frontal view image measurements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Psyllos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Std. Interfaces</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="151" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reliable classification of vehicle types based on cascade classifier ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="322" to="332" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vehicle type classification using data mining techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Era of Interactive Media</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="325" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A neural architecture for fast and robust face detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 16th Int. Conf. Pattern Recog</title>
		<meeting>IEEE 16th Int. Conf. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="44" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1197" to="1215" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PANDA: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for no-reference image quality assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1733" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Greedy layerwise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1090" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning task grouping and overlap in multi-task learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1383" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962-01">Jan. 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="455" to="469" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nonlinear image representation using divisive normalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Why is real-world visual object recognition hard?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1125" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What is the goal of sensory coding?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="601" />
			<date type="published" when="1994-07">Jul. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Characterizing the sparseness of neural codes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Willmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Tolhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Netw., Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="270" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Updating quasi-Newton matrices with limited storage</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comput</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">151</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="1980-07">Jul. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vehicle type classification using PCA with self-clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo Workshops</title>
		<meeting>IEEE Int. Conf. Multimedia Expo Workshops</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="384" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vehicle type classification using distributions of structural and appearance-based features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Process</title>
		<meeting>Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="4321" to="4324" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
