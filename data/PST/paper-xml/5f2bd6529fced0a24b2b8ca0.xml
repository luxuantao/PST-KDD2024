<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Qi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Junyu</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>li@nwpu.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Center for Optical Imagery Analysis and Learn-ing (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an, Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BB85019341D1C22E8051C495C54C100D</idno>
					<idno type="DOI">10.1109/TPAMI.2020.3013269</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3013269, IEEE Transactions on Pattern Analysis and Machine Intelligence This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3013269, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Crowd counting</term>
					<term>crowd localization</term>
					<term>crowd analysis</term>
					<term>benchmark website</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last decade, crowd counting and localization attract much attention of researchers due to its wide-spread applications, including crowd monitoring, public safety, space design, etc. Many Convolutional Neural Networks (CNN) are designed for tackling this task. However, currently released datasets are so small-scale that they can not meet the needs of the supervised CNN-based algorithms. To remedy this problem, we construct a large-scale congested crowd counting and localization dataset, NWPU-Crowd, consisting of 5, 109 images, in a total of 2, 133, 375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0 ∼ 20, 033). Besides, a benchmark website is developed for impartially evaluating the different methods, which allows researchers to submit the results of the test set. Based on the proposed dataset, we further describe the data characteristics, evaluate the performance of some mainstream state-of-the-art (SOTA) methods, and analyze the new problems that arise on the new data. What's more, the benchmark is deployed at https://www.crowdbenchmark.com/, and the dataset/code/models/results are available at https://gjy3035.github.io/NWPU-Crowd-Sample-Code/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C ROWD analysis is an essential task in the field of video surveillance. Accurate analysis for crowd motion, human behavior, population density is crucial to public safety, urban space design, etc. Crowd counting and localization are fundamental tasks in the field of crowd analysis, which serve high-level tasks, such as crowd flow estimation <ref type="bibr" target="#b0">[1]</ref> and pedestrian tracking <ref type="bibr" target="#b1">[2]</ref>. Due to the importance of crowd counting, many researchers <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> pay attention to it and achieve quite a few significant improvements in this field. Especially, benefiting from the development of deep learning in computer vision, the counting performance on the datasets <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> is continuously refreshed by Convolutional Neural Networks (CNN)-based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>The CNN-based methods need to learn discriminate features from a multitude of labeled data, so a large-scale dataset can effectively promote the development of visual technologies. It is verified in many existing tasks, such as object detection <ref type="bibr" target="#b12">[13]</ref> and semantic segmentation <ref type="bibr" target="#b13">[14]</ref>. However, the currently released crowd counting datasets are so small-scale that most deeplearning-based methods are prone to overfit the data. According to the statistics, UCF-QNRF <ref type="bibr" target="#b8">[9]</ref> is the largest released congested crowd counting dataset. Still, it contains only 1, 535 samples, in a total of 1.25 million annotated instances, which is still unable to meet the needs of current deep learning methods. Moreover, some works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref> focus on the crowd localization task that produces point-wise predictions for each instance. However, the traditional datasets do not contain box-level labels, which makes it hard to evaluate the localization performance using a uniform metric. Furthermore, there is not an impartial evaluation benchmark, which potentially restricts further development of crowd counting. By the way, some methods 1 may use mistaken labels to evaluate models, which is also not accurate. Reviewing some benchmarks in other fields, CityScapes <ref type="bibr" target="#b15">[16]</ref> and Microsoft COCO <ref type="bibr" target="#b12">[13]</ref>, they allow the researchers to submit their results of the test set and impartially evaluate them, which facilitates the study of methodology. Thus, an equitable evaluation platform is important for the community.</p><p>Considering the problems mentioned above, in this paper, we construct a large-scale crowd counting and localization dataset, named as NWPU-Crowd, and develop a benchmark website to boost the community of crowd analysis. Compared with the existing congested datasets, the proposed NWPU-Crowd has the following main advantages: 1) This is the largest crowd counting and localization dataset, consisting of 5, 109 images and containing 2, 133, 375 annotated instances; 2) It introduces some negative samples like high-density crowd images to assess the robustness of models; 3) In NWPU-Crowd, the number of annotated objects range, 0 ∼ 20, 033. More concrete features are described in Section 3.3. Table <ref type="table" target="#tab_0">1</ref> illustrates the detailed statistics of ten mainstream real-world datasets and the proposed NWPU-Crowd.</p><p>Based on the proposed NWPU-Crowd, several experiments of some classical and state-of-the-art methods are conducted. After further analyzing their results, an interesting phenomenon on the proposed dataset is found: diverse data makes it difficult for counting networks to learn useful and distinguishable features, which does not appear or is ignored in the previous datasets. Specifically, 1) there are many error estimations on negative samples; 2) the data of different scene attributes (density level and luminance) have a significant influence on each other. Therefore, it is a research trend on how to alleviate the above two problems.  <ref type="bibr" target="#b19">[20]</ref> 15,212 1080 × 1920 7,625,843 0 501 3,995 JHU-CROWD++ <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> 4,372 What's more, for localization task, we design a reasonable metric and provide some simple baseline models.</p><p>In summary, we believe that the proposed large-scale dataset will promote the application of crowd counting and localization in practice and attract more attention to tackling the aforementioned problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>The existing crowd counting datasets mainly contain two types: surveillance-scene datasets and general-scene datasets. The former commonly records crowd in particular scenarios, of which the data consistency is obvious. For the latter, the crowd samples are collected from the Internet. Thus, there are more perspective variations, occlusions, and extreme congestion in these datasets. Tabel 1 demonstrates a summary of the basic information of the mainstream crowd counting datasets, and in the following parts, their unique characters are briefly introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Surveillance-scene Dataset</head><p>Surveillance view. Surveillance-view datasets aim to collect the crowd images in specific indoor scenes or small-area outdoor locations, such as marketplace, walking street, and station. The number of people usually ranges from 0 to 600. UCSD is a typical dataset for crowd analysis. It contains 2, 000 image sequences, which records a pedestrian walk-way at the University of California at San Diego (UCSD). Mall <ref type="bibr" target="#b16">[17]</ref> is captured in a shopping mall with more perspective distortion. However, these two datasets contain only a single scene, lacking data diversity. Thus, Zhang et al. <ref type="bibr" target="#b7">[8]</ref> build a multi-scene crowd counting dataset, WorldExpo'10, consisting of 108 surveillance cameras with different locations in Shanghai 2010 WorldExpo, e.g., entrance, ticket office. Considering the poor resolution of traditional surveillance cameras, Zhang et al. <ref type="bibr" target="#b6">[7]</ref> construct a high-quality crowd dataset, ShanghaiTech Part B, containing 782 images captured in some famous resorts of Shanghai, China. To remedy the occlusion problem in congested scenes, a multi-view dataset is designed by Zhang and Chan <ref type="bibr" target="#b22">[23]</ref>. By equipping 5 cameras at different positions for a specific view, the data can be recorded synchronously. For getting rid of the manually labeling process, Wang et al. <ref type="bibr" target="#b19">[20]</ref> construct a largescale synthetic dataset (GCC). By simulating the perspective of a surveillance camera, they capture 400 crowd scenes in a computer game (Grand Theft Auto V, GTA V), a total of 15, 212 images. The main advantage of GCC is that it can provide accurate labels (point and mask) and diverse environments. However, there are many domain shifts/gaps between synthetic and real data, limiting their practical values. Therefore, it is necessary to build a largescale real-world dataset. Compared with GCC, the advantages of NWPU-Crowd are: more natural person models, crowd scenes and environment (weathers, light, etc.).</p><p>In addition to the aforementioned datasets, there are also other crowd counting datasets with their specific characteristics. SmartCity <ref type="bibr" target="#b23">[24]</ref> focuses on some typical scenes, such as sidewalk and subway. ShanghaiTechRGBD <ref type="bibr" target="#b24">[25]</ref> records the RGBD crowd images with a stereo camera for concentrating on pedestrian counts and localization. Fudan-ShanghaiTech <ref type="bibr" target="#b25">[26]</ref> and Venice <ref type="bibr" target="#b26">[27]</ref> capture the video sequences for temporal crowd counting. Drone view. For some big scenes (such as stadium, plaza) or some large rally events (ceremony, hajj, etc.), the above traditional fixed surveillance camera is not suitable due to its small field of view. To tackle this problem, some other datasets are collected through the Drone or Unmanned Aerial Vehicle (UAV). Benefiting from their higher altitudes, more flexible view and free flight, more large scenes can be recorded compared with the traditional surveillance camera. There are two crowd counting datasets with the drone view, DLR-ACD Dataset <ref type="bibr" target="#b27">[28]</ref> and DroneCrowd Dataset <ref type="bibr" target="#b28">[29]</ref>. The former consists of 33 images with 226, 291 annotated persons, including some mass events: sports, concerts, trade fair, etc. The latter consists of 70 crowd scenes , with a total of 33, 600 droneview image sequences. Due to the Bird's-Eye View (BEV), the whole body of pedestrians can not be seen except their heads, so the perspective change rarely appears in the above two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">General-scene Dataset</head><p>In addition to the above crowd images captured in specific scenes, there are also many general-scene crowd counting datasets, which are collected from the Internet. A remarkable aspect of generalscene is that the crowd density varies significantly, which ranges from 0 to 20, 000. Besides, diversified scenarios, light and shadow conditions, and uneven crowd distribution in one single image are also distinctive attributes of these datasets.</p><p>The first general-scene dataset for crowd counting, UCF CC 50 <ref type="bibr" target="#b18">[19]</ref>, is presented by <ref type="bibr">Idrees et</ref>   For the former level, they label the scenario (mall, stadium, etc.) and weather conditions. For the head level, the annotation information includes not only head locations but also occlusion, size, and blur attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NWPU-CROWD DATASET</head><p>This section describes the proposed NWPU-Crowd from four perspectives: data collection/specification, annotation tool, statistical analysis, data split and evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection and Specification</head><p>Data Source. Our data are collected from self-shooting and the Internet. For the former, ∼ 2, 000 images and ∼ 200 video sequences are captured in some populous Chinese cities, including Beijing, Shanghai, Chongqing, Xi'an, and Zhengzhou, containing some typical crowd scenes, such as resort, walking street, campus, mall, plaza, museum, station. However, extremely congested crowd scenes are not the norm in real life, which is hard to capture via self-shooting. Therefore, we also collect ∼ 8, 000 samples from some image search engines (Google, Baidu, Bing, Sougou, etc.) via the typical query keywords related to the crowd. Table <ref type="table">2</ref> lists the primary data source websites and the corresponding keywords. The third row in the table records some Chinese websites and keywords. Finally, by the above two methods, 10, 257 raw images are obtained. Data Deduplication and Cleaning. We employ four individuals to download data from the Internet on non-overlapping websites. Even so, there are still some images that contain the same content. Besides, some congested datasets (UCF CC 50, Shanghai Tech Part A, and UCF-QNRF), are also crawled from the Internet, e.g., Flickr, Google, etc. For avoiding the problem of data duplication, we perform an effective strategy to measure the similarity google, baidu, bing, pxhere, pixabay... crowd, congestion, hundreds/thousands of people, speech, conference, ceremony, stadium, gathering, parade, demonstration, protest, carnival, beer festival, hajj, NBA, WorldCup, NFL, EPL, Super Bowl, etc.</p><p>dense, migration, fish school, empty scenes, flowers, etc. baidu, weibo, sogou, so, wallhere...</p><formula xml:id="formula_0">人群，拥挤，春运，军训，典礼，祭祀，庙会， 游客，万人，千人，大赛，运动会，候车厅， 音乐会/节，见面会，人从众，黄金周，招聘会， 万人空巷，人山人海，摩肩接踵，水泄不通…… 礼堂，动物迁徙， 花海，动漫人物， 餐厅，空无一人， 密集排列……</formula><p>between two images, which is inspired by Perceptual Loss <ref type="bibr" target="#b29">[30]</ref>. Specifically, for each image, the layer-wise VGG-16 <ref type="bibr" target="#b30">[31]</ref> features (from conv1 to conv5 3 layer) are extracted. Given two resized samples i x and i y with the resolution of 224 × 224, the similarity is defined as follows:</p><formula xml:id="formula_1">D (ix, iy) = j∈L 1 Cj Hj Wj ψj (ix) -ψj (iy) 2 2 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where L is the set of the last activation layer in five groups of VGG-16 network, namely L = {ψ j |j = 1, 2, ..., 5} = {relu1 2, relu2 2, relu3 3, relu4 3, relu5 3}. ψ j (i x ) and ψ j (i y ) denote layer ψ j 's outputs (feature maps) for sample i x and i y , respectively. C j , H j and W j are the size of ψ j (i x ) at three axes: channel, height and width. If D (i x , i y ) &lt; 5, these two samples are considered to have similar contents. As a result, one of the two is removed from the dataset. Then remove excess similar images by computing the distance of the feature between any two samples. Furthermore, some blurred images that are difficult to recognize the head location are also removed. Consequently, we obtain 5, 109 valid images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Annotation</head><p>Annotation tools: For conveniently annotating head points in the crowd images, an online efficient annotation tool is developed based on HTML5 + Javascript + Python. This tool supports two types of label form, namely point and bounding box. During the annotation process, each image is flexibly zoomed in/out to annotate head with different scales, and it is divided into 16 × 16 small blocks at most, which allows annotators to label the head under five scales: 2 i (i=0,1,2,3,4) times size of the original image. It effectively prompts annotation speed and quality. The more detailed description is shown in the video demo of our provided supplementary material. Point-wise annotation: The entire annotation process has two stages: labeling and refinement. Firstly, there are 30 annotators involved in the initial labeling process, which costs 2, 100 hours totally to annotate all collected images. After this, 6 individuals are employed to refine the preliminary annotations, which takes 150 hours per refiner. In total, the entire annotation process costs 3, 000 human hours. Box-level annotation and generation: There are three steps to annotate box labels: 1) for each image, manually select ∼ 10% typical points to draw their corresponding boxes, which can represent the scale variation in the whole scene; 2) for each point without box label, adopt a linear regression algorithm to obtain its box size based on its 8-nearest box-labeled neighbors; 3) manually refine the prediction box labels. Step 1) and 2) takes 1, 000 human hours in total.</p><p>Here, the step 2) is described as below: For a head point P 0 without box label, its 8-nearest box-labeled neighbors (P 1 8 ) are utilized to fit a linear regression algorithm <ref type="bibr" target="#b31">[32]</ref>, in which the vertical axis coordinates are variable, and the box size is the dependent variable. According to the linear function and the vertical axis coordinate of P 0 , the box size corresponding to P 0 can be obtained. We assume each box has a shape of a square, and the point coordinates are its center, and then the box can be obtained. Obviously, the linear regression is not reliable, so we should manually refine the predicted box labels again in Step 3). Then the linear regression and the manually refine will loop continuously until all boxes seem qualified. In the annotation stage, Step 2) and 3) are repeated four times. Discussion on annotation quality In the field of crowd counting and localization, it is important how to ensure high-quality annotation, especially in some extremely congested scenes. In this work, we attempt to alleviate it from the two aspects: 1) the proposed tools support zooming in or out on an image with 1x 16x online. For congested region, the annotator can easily draw a box on a tiny or occluded object using zooming operation; 2) we conduct two stages of refinement in the point annotation, and repeat four times for linear estimation and refinement to minimize labeling errors in the box annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Characteristic</head><p>NWPU-Crowd dataset consists of 5, 109 images, with 2, 133, 375 annotated instances. Compared with the existing crowd counting datasets, it is the largest from the perspective of image and instance level. Fig. <ref type="figure" target="#fig_0">1</ref> respectively demonstrates four groups of typical samples from Row 1 to 4 in the dataset: normal-light, extremelight, dark-light, and negative samples. Fig. <ref type="figure" target="#fig_3">2</ref>(a) compares the number distribution of different counting range on four datasets: NWPU-Crowd, JHU-CROWD++ <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, UCF-QNRF <ref type="bibr" target="#b8">[9]</ref>, and ShanghaiTech Part A <ref type="bibr" target="#b6">[7]</ref>. Except the bin of (0, 100], the number of images on NWPU-Crowd is much larger than that on the other three datasets. Fig. <ref type="figure" target="#fig_3">2(b)</ref> shows the distributions of the box area in NWPU-Crowd and JHU-CROWD. From the orange bars, more than 50% of boxes areas are in the range of (10 2 , 10 3 ] pixels. Since the average resolution of NWPU-Crowdis is higher than that of JHU-CROWD, the numbers of large-scale   heads are more. The larger scale provides more detailed headstructure information, which will aid the model to achieve better performance.</p><p>In addition to data volume and scale distribution, there are four more advantages in NWPU-Crowd: 1) Negative Samples. NWPU-Crowd introduces 351 negative samples (namely nobody scenes), which are similar to congested crowd scenes in terms of texture features. It effectively improves the generalization of counting models while applied in the real world. These samples contain animal migration, fake crowd scenes (sculpture, Terra-Cotta Warriors, 2-D cartoon figure, etc.), empty hall, and other scenes with densely arranged objects that are not the person. 2) Fair Evaluation. For a fair evaluation, the labels of the test set are not public. Therefore, we develop an online evaluation benchmark website that allows researchers to submit their estimation results of the test set. The benchmark can calculate the error between presented results and ground truth, and list them on a scoreboard. 3) Higher Resolution. The proposed dataset collects highquality and high-resolution scenes, which is entailed for extremely congested crowd counting. From Table <ref type="table" target="#tab_0">1</ref>, the average resolution of NWPU-Crowd is 2191 × 3209, which is larger than that of other datasets. Specifically, the maximum image size is 4028 × 19044. 4) Large Appearance Variation. The number of people ranges from 0 to 20, 033, which means large appearance variations within the data. Notably, the smallest head occupies only 4 pixels, but the largest head covers 1.2×10 7 pixels. In the whole dataset, the ratio of the area of the largest and smallest head in the same image is 3.8×10 5 .</p><p>In summary, NWPU-Crowd is one of the largest and most challenging crowd counting/localization datasets at present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Split and Evaluation Protocol</head><p>NWPU-Crowd Dataset is randomly split into three parts, namely training, validation and test sets, which respectively contain 3, 109, 500 and 1, 500 images. To be specific, each image is randomly assigned to a specific set with the corresponding probability (followed by 0.6, 0.1 and 0.3 for the three subsets) until the number reaches the upper bound. This strategy ensures that the statistics (such as data distribution, the average value of resolutions/counts) of the subset are almost the same. Counting Metrics Following some previous works, we adopt three metrics to evaluate the counting performance, which are Mean Absolute Error (MAE), Mean Squared Error (MSE), and mean Normalized Absolute Error (NAE). They can be formulated as follows:</p><formula xml:id="formula_3">M AE = 1 N N i=1 y i -ŷi , M SE = 1 N N i=1 y i -ŷi 2 , N AE = 1 N N i=1 y i -ŷi y i , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where N is the number of images, y i is the counting label of people and ŷi is the estimated value for the i-th test image. Since NWPU-Crowd contains quite a few negative samples, NAE's calculation does not contain them to avoid zero denominators.</p><p>In addition to the aforementioned overall evaluation on the test set, we further assess the model from different perspectives: scene level and luminance. The former have five classes according to the number of people: 0, (0, 100], (100, 500], (500, 5000], and more than 5000. The latter have three classes based on luminance value in the YUV color space: [0, 0.25], (0.25, 0.5],and (0.5, 0.75]. The two attribute labels are assigned to each image according to their annotated counting number and image contents. For each class in a specific perspective, MAE, MSE, and NAE are applied to the corresponding samples in the test set. Take the luminance attribute as an example, the average values of MAE, MSE, and NAE at the three categories can reflect counting models' sensitivity to the luminance variation. Similar to the overall metrics, the negative samples are excluded during the calculation of NAE. Localization Metrics For the crowd localization task, we adopt the box-level Precision, Recall and F1-measure to evaluate the localization performance. Given two point sets from prediction results P p and ground truth P g , we firstly construct a Bipartite Graph G p,s for the two sets. Secondly, we compute the distance matrix of P p and P p . If the distance between p p ∈ P p and p g ∈ P g is less than the predefined distance threshold σ, we think p p and p g are successfully matched. Corresponding to each element of the distance matrix, we obtain a boolean match matrix (True and False denote matched and non-matched). Finally, we can get a Maximum Bipartite Matching for G p,s by implementing the Hungarian algorithm<ref type="foot" target="#foot_2">2</ref> the match matrix and count the number of True Positive (TP), False Positive (FP) and False Negative (FN). In our evaluation, for each head with the size of width w and height h, we define two threshold σ s = min(w, h) and σ l = √ w 2 + h 2 . The former is a stricter criterion than the latter. Similar to the category-wise counting evaluation at the image level, we propose a scale-sensitive evaluation scheme at the box level for the localization task. To be specific, all heads are divided into six categories according to their corresponding box areas: [10 0 , 10 1 ], (10 1 , 10 2 ], (10 2 , 10 3 ], (10 3 , 10 4 ], (10 4 , 10 5 ],</p><p>and more than 10 5 . For each category, the Recall is calculated separately.</p><p>Different from the previous localization metrics <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>, the σ in this paper is adaptive, which is defined by the real head area. In addition, the performance on different scale classes are reported, which helps researchers analyze the model more deeply. In summary, our evaluation is more reasonable than the traditional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS ON COUNTING</head><p>In this section, we train ten mainstream open-sourced methods on the proposed NWPU-Crowd and submit their results on the evaluation benchmark. Besides, the further experimental analysis and visualization results on the validation set are discussed. SANet <ref type="bibr" target="#b32">[33]</ref>: Scale Aggregation Network. SANet is an efficient encoder-decoder network with Instance Normalization for crowd counting, which combines the MSE loss and SSIM loss to output the high-quality density map. PCC Net <ref type="bibr" target="#b33">[34]</ref>: Perspective Crowd Counting Network. It is a multi-task network, which tackles the following tasks: densitylevel classification, head region segmentation, and density map regression. The authors provide two versions, a lightweight from scratch and VGG-16 backbone. Reg+Det Net <ref type="bibr" target="#b34">[35]</ref>: a subnet of DecideNet. It consists of two branches: Regression and Detection Network. The former is a light-weight network for density estimation, and the latter focuses on head detection via on Faster R-CNN (ResNet-101) <ref type="bibr" target="#b35">[36]</ref>. C3F-VGG <ref type="bibr" target="#b36">[37]</ref>: A simple baseline based on VGG-16 backbone for crowd counting. C3F-VGG consists of the first 10 layers of VGG-16 <ref type="bibr" target="#b30">[31]</ref> as image feature extractor and two convolutional layers with a kernel size of 1 for regressing the density map. CSRNet <ref type="bibr" target="#b9">[10]</ref>: Congested Scene Recognition Network. CSRNet is a classical and efficient crowd counter, proposed by Li et al. in 2016. The authors design a Dilatation Module and add it to the top of the VGG-16 backbone. This network significantly improves performance in the field of crowd counting. CANNet <ref type="bibr" target="#b37">[38]</ref>: Context-Aware Network. CANNet combines the features of multiple streams using different respective field sizes. It encodes the multi-scale contextual information of the crowd scenes and yields a new record on the mainstream datasets. SCAR <ref type="bibr" target="#b38">[39]</ref>: Spatial-/Channel-wise Attention Regression Networks. SCAR utilizes the self-attention module <ref type="bibr" target="#b39">[40]</ref> on the spatial and channel axis to encode the large-range contextual information. The well-designed attention models effectively extracts discriminative features and alleviates mistaken estimations. BL <ref type="bibr" target="#b40">[41]</ref>: Bayesian Loss for Crowd Count Estimation. Different from the traditional strategy for the generation of ground truth, BL design a loss function to directly using head point supervision. It achieves state-of-the-art performance on the UCF-QNRF dataset. SFCN † <ref type="bibr" target="#b19">[20]</ref> Spatial Fully Convolutional Network with ResNet-101 <ref type="bibr" target="#b41">[42]</ref>. SFCN † is the only crowd counting model that uses ResNet-101 as a backbone, which shows the powerful capacity of density regression on the congested crowd scenes. Fig. <ref type="figure">3</ref>: The eight groups of visualization results of some selected methods on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mainstream Methods Involved in Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In the experiments, for PCC Net 3 and BL 4 , the models are trained using the official codes and the default parameters. For SANet, we implement the C 3 Framework <ref type="bibr" target="#b36">[37]</ref> and follow the corresponding parameters to train them on NWPU-Crowd dataset. For DetNet, we train a head detector using this code 5 .</p><p>For other models, namely MCNN, RegNet, CSRNet, C3F-VGG, CANNet, SCAR, and SFCN †, they are reproduced in our counting experiments, which is developed based on C 3 Framework <ref type="bibr" target="#b36">[37]</ref>, an open-sourced crowd counting project using PyTorch <ref type="bibr" target="#b42">[43]</ref>. In the data pre-processing stage, the high-resolution images are resized to the 2048-px scale with the original aspect ratio. The density map is generated by a Gaussian kernel with a fixed size of 15 and the σ of 4. For augmenting the data, during the training process, all images are randomly cropped with the size of 576 × 768, flipped horizontally, transformed to gray-scale images, and gamma corrected with a random value in [0.4, 2]. To optimize the above counting networks, Adam algorithm <ref type="bibr" target="#b43">[44]</ref> is employed. Other parameters (such as learning rate, batch size) are reported in https://github.com/gjy3035/NWPU-Crowd-Sample-Code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results Analysis on the Validation Set</head><p>Quantitative Results. Here, we list the counting performance and density quality of all participation methods in Table <ref type="table" target="#tab_4">3</ref>. For evaluating the quality of the density map, two popular criteria are adopted, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity in Image (SSIM) <ref type="bibr" target="#b44">[45]</ref>. Since BL <ref type="bibr" target="#b40">[41]</ref> is supervised by point locations instead of density maps, PSNR and SSIM are not reported. In the calculation of PSNR, the negative samples are excluded to avoid zero denominators.  From the table, we find SCAR <ref type="bibr" target="#b38">[39]</ref> attains the best counting performance, MAE of 81.57 and MSE of 397.92. SFCN † <ref type="bibr" target="#b19">[20]</ref> produces the most high-quality density maps, PSNR of 30.591 and SSIM of 0.952. For the three light models (MCNN, SANet, PCC-Net-light), we find that the last achieves the best SSIM (0.937), which even surpasses the SSIMs of some other VGGbased algorithms, such as C3F-VGG, CSRNet, CANNet, and SCAR. Similarly, PCC-Net-VGG is the best SSIM in the VGGbackbone methods. Visualization Results. Fig. <ref type="figure">3</ref> demonstrates some predicted density maps of the eight methods. The first two columns are negative samples, and others are crowd scenes with different density levels. From the first two columns, almost all models perform poorly for negative samples, especially densely arranged objects. For humans, we can easily recognize that the two samples are mural and stones. But for the counting models, they cannot understand them. For the third column, although the predictions of these methods are good, there are still many mistaken errors in background regions. For the last two images that are extremely congested scenes, the estimation counts are far from the ground truth. SCAR is the most 0162-8828 (c) 2020 IEEE. Personal use permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. accurate method on the validation set, but it is about 1, 900 and 8, 000 people away from the labels, respectively. For the extremeluminance scenes (Image 3367, 3250, and 3353), there are quite a few estimation errors in the high-light or dark-light regions. In general, the ability of the current models to cope with the above hard samples needs to be further improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Leaderboard</head><p>Table <ref type="table" target="#tab_6">4</ref> reports the results of five methods on the test set. It lists the overall performance (MAE, MSE, and NAE), category-wise MAE on the attribute of scene level and luminance, model size, speed (inference time) and floating-point operations per second (FLOPs) 6 . Compared with the results of the validation set, we find that the ordering has changed significantly. Although SCAR attains the best results of MAE and MSE on the validation set, the performance on the test set is not good. For the primary key (overall MAE), BL, SFCN † and CANNet occupy the top three on the test set.</p><p>From the category-wise results of Scene Level, we find that all methods perform poorly in S0 (negative samples), S3 ((500, 5000]) and S4 (≥ 5000), which causes that the average value of category-wise MAE is larger than the overall MAE (SFCN †: 712.7 v.s. 105.7). Besides, this phenomenon shows that negative samples and congested scenes are more challenging than sparse crowd images. Similarly, for the luminance classes, the MAE of L0 ([0, 0.25]) is larger than that of L1 and L2. In other words, the counters work better under the luminance than under the low-luminance scenes. More detailed results are shown in https://www.crowdbenchmark.com/nwpucrowd.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance Impact between Different Scenes</head><p>From Section 4.3 and 4.4, we find that two interesting phenomena worth attention: 1) Negative samples are prone to be mistakenly estimated; 2) The data with different scene attributes (namely density level) significantly affect each other. In this section, we conduct two experiments using a simple baseline model, C3F-VGG <ref type="bibr" target="#b36">[37]</ref>, to explore the above problems. Phenomenon 1. For the firth problem, the main reason is that the negative samples contain densely arranged objects, which is similar to the congested crowd scenes. As we all know, most existing counting models focus on texture information and local patterns for congested regions. To verify our thoughts, we design three groups of experiments to explore which samples affect the performance of negative samples. To be specific, we train three C3F-VGG counters on different combination training data: S0+S1, S0+S2, and S0+S3+S4 (considering that the number of S4 is small, so we integrate S3 and S4). Then the evaluation is performed on the validation set. Finally, the corresponding performance is listed in Table <ref type="table" target="#tab_7">5</ref>. From it, the MAE on Negative Sample (S0) increases from 18.54 to 147.53 as the density of positive samples increases. Phenomenon 2. For the second issue, we train the counting models only using the data with a single category, S1, S2, and S3 + S4 respectively. Removing the impacts of the negative samples, the model is trained on the data of S1 + S2 + S3 + S4. The concrete performance is illustrated in Table <ref type="table" target="#tab_7">5</ref>. According to the results, training each class individually is far better than training 6. For PCC-Net, we remove the useless layers (classification and segmentation modules) to compute the last three items: model size, speed and FLOPs.  together. To be specific, MAE decreases by 36.6%, 25.7%, 22.2% and 12.7% on the four classes, respectively. The main reason is that NWPU-Crowd contains more diverse crowd scenes than the previous datasets. There are large appearance variations in the dataset, especially the scales of the head. At present, the existing models can not tackle this problem well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Effectiveness of Negative Samples</head><p>In Section 3.3, we mention that the Negative Samples ("NS" for short) can effectively improve the generalization ability of the model. Here, we conduct four groups of comparative experiments using C3F-VGG <ref type="bibr" target="#b36">[37]</ref> to verify this opinion. To be specific, there are four types of training data: S1, S2, S3+S4 and S1+...+S4.</p><p>We respectively train the models for them using NS and without NS. In other words, we add S0 to the above four types of training data. The concrete results are reported in Table <ref type="table" target="#tab_7">5</ref>. After introducing NS, the category-wise MAEs are significantly reduced. Take the last six rows as the examples, the MAE is respectively decreased by 21.7%, 2.0%, 6.1% and 17.4% on the categorywise evaluation. The main reason is that NS contains diverse background objects with different structured information, which can prompt the counting models to learn more discriminative features than ever before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Impact of Data Volume on Performance</head><p>Generally speaking, large amounts of diverse training data will prompt the model to learn more robust features, and then perform better in the wild. This is also our original intention to build a large-scale crowd counting and localization dataset. In this section, we explore the impact of different data volumes on counting performance. To be specific, we train ten C3F-VGG models using 10%, 20%, 30%, ..., 100% training data, respectively. Then evaluate them on the validation set. The performances (MAE and MSE) are demonstrated in Fig. <ref type="figure" target="#fig_6">4</ref>. From the figure, with the gradual increase of training data, the errors on the validation set also gradually decrease overall. By comparing the MAEs when using the 10% and 100% data, the error is significantly reduced from 158.35 to 105.79 (relative decrease of 33.2%). Therefore, a large-scale dataset is very necessary for the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS ON LOCALIZATION</head><p>Considering that the box-level labels are provided, we evaluate four crowd localization methods in this section. What's more, we analyze the quantitative and qualitative results of them.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The display of the proposed NWPU-Crowd dataset. Column 1 shows some typical samples with normal lighting. The second and third column demonstrate the crowd scenes under the extreme brightness and low-luminance conditions, respectively. The last column illustrates the negative samples, including some scenes with densely arranged other objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>b e r o f I m a g e s N u m b e r R a n g e N W P U -C r o w d J H U -C R O W D + + U C F -Q N R F S h a n g H a i T e c h A (a) The distribution of counts on the four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>b e r o f B o x e s A r e a R a n g e N W P U -C r o w d J H U -C O R W D + + (b) The distribution of the area (pixels) of head region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The statistical histogram of image-level counts and boxlevel area. The number in x axis of Fig.(b) denotes 10 x . For example, [0, 1] represents the range of box area is [10 0 , 10 1 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>MCNN [ 7 ]</head><label>7</label><figDesc>: Multi-Column Convolutional Neural Network. It is a classical and lightweight counting model, proposed by Zhang et al. in 2016. Different from the original MCNN, the RGB images are fed into the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2020.3013269, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XXX, NO. XXX, XXX XXX</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The results under different volumes of the training data on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Statistics of the ten mainstream crowd counting datasets and NWPU-Crowd.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Number Avg. Resolution of Images (H × W )</cell><cell>Total</cell><cell cols="3">Count Statistics Min Ave</cell><cell>Extreme Max Congestion Test Labels Unseen</cell><cell>Category-wise Box-level Evaluation Label</cell></row><row><cell>UCSD [6]</cell><cell>2,000</cell><cell>158 × 238</cell><cell cols="2">49,885</cell><cell>11</cell><cell>25</cell><cell>46</cell></row><row><cell>Mall [17]</cell><cell>2,000</cell><cell>480 × 640</cell><cell cols="2">62,325</cell><cell>13</cell><cell>31</cell><cell>53</cell></row><row><cell>WorldExpo'10 [8]</cell><cell>3,980</cell><cell>576 × 720</cell><cell cols="2">199,923</cell><cell>1</cell><cell>50</cell><cell>253</cell></row><row><cell>ShanghaiTech Part B [7]</cell><cell>716</cell><cell>768 × 1024</cell><cell cols="2">88,488</cell><cell>9</cell><cell>123</cell><cell>578</cell></row><row><cell>Crowd Surv [18]</cell><cell>13,945</cell><cell>840 × 1342</cell><cell cols="2">386,513</cell><cell>2</cell><cell>35</cell><cell>1,420</cell></row><row><cell>UCF CC 50 [19]</cell><cell>50</cell><cell>2101 × 2888</cell><cell cols="2">63,974</cell><cell cols="3">94 1,279 4,543</cell></row><row><cell>ShanghaiTech Part A [7]</cell><cell>482</cell><cell>589 × 868</cell><cell cols="5">241,677 33 501 3,139</cell></row><row><cell>UCF-QNRF [9]</cell><cell>1,535</cell><cell cols="6">2013 × 2902 1,251,642 49 815 12,865</cell></row><row><cell>GCC (synthetic)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>It is composed of 1, 525 images with more than 1, 251, 642 label points. The average number of pedestrians per image is 815, and the maximum number reaches 12, 865. Aiming at the small size of crowd images, Crowd Surveillance<ref type="bibr" target="#b17">[18]</ref> build</figDesc><table><row><cell>al. in 2013. It only</cell></row><row><cell>contains 50 images, which is so small to train a robust deep learn-</cell></row><row><cell>ing model. Consequently, a larger crowd counting dataset becomes</cell></row><row><cell>more significant nowadays. Zhang et al. propose ShanghaiTech</cell></row><row><cell>Part A [7], which is constructed of 482 images crawled from the</cell></row><row><cell>Internet. Although its average number of labeled heads in each</cell></row><row><cell>image is smaller than UCV CC 50, it contains more pictures and</cell></row><row><cell>larger number of labeled head points. For further research on the</cell></row><row><cell>extremely congested crowd counting, UCF-QNRF [9] is presented</cell></row><row><cell>by Idrees et al.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>The performance of different models on the val set.</figDesc><table><row><cell>Method</cell><cell>MAE</cell><cell cols="2">the validation set MSE PSNR</cell><cell>SSIM</cell></row><row><cell>MCNN</cell><cell>218.53</cell><cell>700.61</cell><cell>28.558</cell><cell>0.875</cell></row><row><cell>SANet</cell><cell>171.16</cell><cell>471.51</cell><cell>29.228</cell><cell>0.886</cell></row><row><cell>Reg+Det Net</cell><cell>245.8</cell><cell>700.3</cell><cell>28.862</cell><cell>0.751</cell></row><row><cell>PCC-Net-light</cell><cell>141.37</cell><cell>630.72</cell><cell>29.745</cell><cell>0.937</cell></row><row><cell>C3F-VGG</cell><cell>105.79</cell><cell>504.39</cell><cell>29.977</cell><cell>0.918</cell></row><row><cell>CSRNet</cell><cell>104.89</cell><cell>433.48</cell><cell>29.901</cell><cell>0.883</cell></row><row><cell>PCC-Net-VGG</cell><cell>100.77</cell><cell>573.19</cell><cell>30.565</cell><cell>0.941</cell></row><row><cell>CANNet</cell><cell>93.58</cell><cell>489.90</cell><cell>30.428</cell><cell>0.870</cell></row><row><cell>SCAR</cell><cell>81.57</cell><cell>397.92</cell><cell>30.356</cell><cell>0.920</cell></row><row><cell>BL</cell><cell>93.64</cell><cell>470.38</cell><cell>-</cell><cell>-</cell></row><row><cell>SFCN †</cell><cell>95.46</cell><cell>608.32</cell><cell>30.591</cell><cell>0.952</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>158.35 150.01 150.01 150.01 142.02 142.02 136.72 136.72 136.72 132.34 132.34 132.34 130.46 130.46 130.46 125.21 125.21 125.21 118.12 118.12 118.12 110.82 110.82 110.82 105.79 105.79 105.79 649.46 649.46 649.46 615.99 615.99 615.99 610.66 610.66 610.66 588.52 588.52 588.52 572.6 572.6 572.6 544.23 544.23 539.14 539.14 539.14 524.12 524.12 515.57 515.57 515.57 504.39 504.39 504.39 MSE</head><label></label><figDesc></figDesc><table><row><cell>650.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>175.00</cell></row><row><cell>620.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>160.00</cell></row><row><cell>590.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>145.00</cell></row><row><cell>560.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>130.00</cell></row><row><cell>530.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>115.00</cell></row><row><cell>500.00</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell><cell>100.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>The leaderboard of the counting performance on the NWPU-Crowd test set. In the ranking strategy, the Overall MAE is the primary key. "FS" represents that the model is trained From Scratch. S0 ∼ S4 respectively indicates five categories according to the different number range: 0, (0, 100], ..., ≥ 5000. L0 ∼ L2 respectively denotes three luminance levels on the test set: [0, 0.25], (0.25, 0.5], and (0.5, 0.75]. Limited by the paper length, only MAE are reported in the category-wise results. The speed and FLOPs are computed on the input size of 576 × 768. The bold and underline fonts respectively represent the first and second place. .6 1.063 1171.9 356.0/72.1/103.5/509.5/4818.2 220.9 472.9/230.1/181.6</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Overall MAE MSE NAE</cell><cell>Avg.</cell><cell>Scene Level (only MAE) S0 ∼ S4</cell><cell>Luminance (only MAE) Avg. L0 ∼ L2</cell><cell>Model Size (M)</cell><cell>Speed (fps)</cell><cell>GFLOPs</cell></row><row><cell>MCNN</cell><cell>FS</cell><cell cols="5">232.5 7140.133</cell><cell>129.0</cell><cell>11.867</cell></row><row><cell>SANet</cell><cell>FS</cell><cell cols="2">190.6 491.4 0.991 716.3</cell><cell cols="2">432.0/65.0/104.2/385.1/2595.4 153.8 254.2/192.3/169.7</cell><cell>1.389</cell><cell>10.8</cell><cell>40.195</cell></row><row><cell>Reg+Det Net</cell><cell>Hybrid</cell><cell cols="4">264.9 759.0 1.770 1242.5 443.0/125.5/140.5/461.5/5036.6 313.6 464.2/267.4/209.1</cell><cell>189.6</cell><cell>4.2</cell><cell>263.079</cell></row><row><cell>PCC-Net-light</cell><cell>FS</cell><cell cols="2">167.4 566.2 0.444 944.9</cell><cell>85.3/25.6/80.4/424.2/4108.9</cell><cell>141.2 253.1/167.9/144.9</cell><cell>0.504</cell><cell>12.6</cell><cell>72.797</cell></row><row><cell>C3F-VGG</cell><cell>VGG-16</cell><cell cols="2">127.0 439.6 0.411 666.9</cell><cell>140.9/26.5/58.0/307.1/2801.8</cell><cell>127.9 296.1/125.3/91.3</cell><cell>7.701</cell><cell>47.2</cell><cell>123.524</cell></row><row><cell>CSRNet</cell><cell>VGG-16</cell><cell cols="2">121.3 387.8 0.604 522.7</cell><cell>176.0/35.8/59.8/285.8/2055.8</cell><cell>112.0 232.4/121.0/95.5</cell><cell>16.263</cell><cell>26.1</cell><cell>182.695</cell></row><row><cell>PCC-Net-VGG</cell><cell>VGG-16</cell><cell cols="2">112.3 457.0 0.251 777.6</cell><cell>103.9/13.7/42.0/259.5/3469.1</cell><cell>111.0 251.3/111.0/82.6</cell><cell>10.207</cell><cell>24.0</cell><cell>145.157</cell></row><row><cell>CANNet</cell><cell>VGG-16</cell><cell cols="2">106.3 386.5 0.295 612.2</cell><cell>82.6/14.7/46.6/269.7/2647.0</cell><cell>102.1 222.1/104.9/82.3</cell><cell>18.103</cell><cell>22.0</cell><cell>193.580</cell></row><row><cell>SCAR</cell><cell>VGG-16</cell><cell cols="2">110.0 495.3 0.288 718.3</cell><cell>122.9/16.7/46.0/241.7/3164.3</cell><cell>102.3 223.7/112.7/73.9</cell><cell>16.287</cell><cell>24.5</cell><cell>182.856</cell></row><row><cell>BL</cell><cell>VGG-19</cell><cell cols="2">105.4 454.2 0.203 750.5</cell><cell>66.5/8.7/41.2/249.9/3386.4</cell><cell>115.8 293.4/102.7/68.0</cell><cell>21.449</cell><cell>34.7</cell><cell>182.186</cell></row><row><cell>SFCN †</cell><cell cols="3">ResNet-101 105.7 424.1 0.254 712.7</cell><cell>54.2/14.8/44.4/249.6/3200.5</cell><cell>106.8 245.9/103.4/78.8</cell><cell>38.597</cell><cell>8.8</cell><cell>272.763</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>The MAE of the different training data on the val set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: UNIVERSITY OF ROCHESTER. Downloaded on August 26,2020 at 13:36:47 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XXX, NO. XXX, XXX XXX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Note that Hungarian algorithm's matching result is not unique. However, the number of TP, FP and FN is the same for different matching results. Considering that for saving computation time, we perform Hungarian algorithm on match matrix instead of the distance matrix.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods and Implementation Details</head><p>Faster RCNN <ref type="bibr" target="#b35">[36]</ref>: a general object detection framework, based on ResNet-101. It directly detects the head boxes, of which center is the prediction head location. We follow the original training parameters of this code 7 . In the forward process, the thresholds of confidence and nms are set as 0.8 and 0.3, respectively. TinyFaces <ref type="bibr" target="#b45">[46]</ref>: a tiny object detection framework, which focuses on tiny faces detection. We implement the third-party code 8 to train a detector using the default parameters. The thresholds of confidence and nms are set as 0.8 and 0.3, respectively. VGG+GPR <ref type="bibr" target="#b46">[47]</ref>: a two-stage method that consists of density map regression and point reconstruction based on Gaussian-kernel priors. C3F-VGG's <ref type="bibr" target="#b36">[37]</ref> training is the same as Section 4 and GPR using standard Gaussian kernel with a size of 15.</p><p>RAZ Loc <ref type="bibr" target="#b14">[15]</ref>: the localization branch of RAZNet, which consists of localization map classification and point post-processing based on finding high-confidence peaks. The training details follows RAZNet, and classification threshold is set as 0.5.   To intuitively understand the performance of crowd localization, Fig. <ref type="figure">5</ref> demonstrates the visualization results of four methods on some typical samples. For the first sample, containing differentscale heads, Faster RCNN almost misses all small objects. The other three methods perform better for this scene than it. For largescale objects (such as the second sample), Faster RCNN produces the perfect results of 100% precision and 100% recall. TinyFaces is the second, and the other two methods miss quite a few heads to different extents. In congested crowd scenes (e.g., Sample 3), VGG+GPR and RAZ Loc obtain good results though they produce some false positives in the background regions. Faster RCNN and TinyFaces work not well for this case: the former miss 95.4% heads, and the latter yields many false positives. Besides, we also find TinyFaces produces more false positives than other <ref type="bibr">TABLE 7:</ref> The leaderboard of the localization performance on the NWPU-Crowd test set. In the ranking strategy, the Overall F1measure is the primary key under σ l , which is bold font in Row 2 of the table. A0 ∼ A4 respectively indicates six categories according to the different head area ranges: [10 0 , 10 1 ], (10 1 , 10 2 ], (10 2 , 10 3 ], (10 3 , 10 4 ], (10 4 , 10 5 ], and &gt; 10 5 . More detailed results will be reported in https://www.crowdbenchmark.com/nwpucrowdloc.html, which is under deployment. *: Since the counts is transformed to integer data, the performance is slightly different from methods. In summary, there is no method to tackle the crowd localization problem well: 1) the traditional general object detection methods can not detect small-scale objects; 2) TinyFaces outputs quite a few false positives; 3) the existing regression-/classification methods can not handle large-range scale variations and misestimations on the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results Analysis on the Validation Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Leaderboard</head><p>Table <ref type="table">7</ref> reports the results of four methods on the test set. It lists the overall localization performance (F1-measure, Precision, and Recall) under σ l and counting performance (MAE, MSE, NAE), category-wise Recall on the different head scales (Box Level). Compared with the results of the validation set, we find that the rankings are consistent. For the primary key (overall F1measure), RAZ Loc attains the first place. From the category-wise results of Box Level, we find that all methods perform poorly for tiny heads (area range is <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>). The main reason is that current feature extractor loses spatial information (usually, 8× or 16× downsampling are adopted). For extremely large-scale heads (area is more than 10 5 ), detection-based methods is better than regression-/classification methods. The main reason is the latter's label is so small (VGG+GPR: 15×15, RAZ Loc: 3×3) that can not cover enough semantic head regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND OUTLOOK</head><p>In this paper, a large-scale NWPU-Crowd counting dataset is constructed, which has the characteristics of high resolution, negative samples, and large appearance variation. At the same time, we develop an online benchmark website to fairly evaluate the performance of counting models. Based on the proposed dataset, we perform the fourteen typical algorithms and rank them from the perspective of the counting and localization performance, the density map quality, and the time complexity.</p><p>According to the quantitative and qualitative results, we find some interesting phenomena and some new problems that need to be addressed on the proposed dataset:</p><p>1) How to improve the robustness of the models? In the real world, the counters may encounter many unseen data, giving incorrect estimation for background regions. Thus, the performance on negative samples is vital in the counting, which represents the models' robustness. 2) How to remedy the performance impact between different scenes? Due to the large appearance variations, the training with all data results in an obvious performance reduction compared with the individual training for each category. Hence, it is essential to prompt the counting model's capacity for appearance representations.</p><p>3) How to reduce the estimation errors in the extremely congested crowd scenes? Because of head occlusions, small objects, and lack of structured information, the existing models can not work well in the high-density regions. 4) How to accurately locate the tiny-size and largescale heads together? The current detection-/regression-/classification-based methods can not handle the problem of large-range scale variation in real crowd scenes. Perhaps researchers need to design scale-aware models or hybrid methods to locate the head position accurately.</p><p>In the future, we will continue to focus on handling the above issues and dedicate to improving the performance of crowd counting and localization in the real world. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A lagrangian particle dynamics approach for crowd flow segmentation and stability analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The solution path algorithm for identity-aware multi-object tracking</title>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3871" to="3879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1879" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data in cnns by self-supervised learning to rank</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Residual regression with semantic prior for crowd counting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4036" to="4045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-driven crowd understanding: a baseline for a large-scale crowd dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multi</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1048" to="1061" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01050</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Iterative crowd counting</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09959</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowd counting with deep structured scale integration network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1774" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent attentive zooming for joint crowd counting and precise localization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1217" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perspective-guided convolution networks for crowd counting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="952" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from synthetic data for crowd counting in the wild</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8198" to="8207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained crowd counting: New dataset and benchmark method</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1221" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03597</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wide-area crowd counting via ground-plane density maps and multi-view fusion cnns</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8297" to="8306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crowd counting via scale-adaptive convolutional neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Appl. Comput. Vis</title>
		<meeting>IEEE Winter Conf. Appl. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1113" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Density map regression guided detection network for rgb-d crowd counting and localization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Locality-constrained spatial transformer network for video crowd counting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multi. Expo</title>
		<meeting>IEEE Int. Conf. Multi. Expo</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="814" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mrcnet: Crowd counting and density map estimation in aerial and ground imagery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12743</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Drone-based joint density map estimation, localization and tracking with space-time multi-scale attention network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01811</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Applied regression analysis</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pcc net: Perspective crowd counting via spatial convolutional network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decidenet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">C 3 framework: An open-source pytorch code for crowd counting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02724</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scar: Spatial-/channel-wise attention regression networks for crowd counting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6142" to="6151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1522" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Domain-adaptive crowd counting via inter-domain features segregation and gaussian-prior reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03677</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
