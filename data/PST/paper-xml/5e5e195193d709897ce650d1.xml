<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving malicious URLs detection via feature engineering: Linear and nonlinear space transformation methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-01-15">15 January 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Management and Economics</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gang</forename><surname>Kou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Business Administration</orgName>
								<orgName type="institution">Southwestern University of Finance and Economics</orgName>
								<address>
									<postCode>610074</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yi</forename><surname>Peng</surname></persName>
							<email>pengyi@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Management and Economics</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
						</author>
						<title level="a" type="main">Improving malicious URLs detection via feature engineering: Linear and nonlinear space transformation methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-01-15">15 January 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">E9D1B3B8107871CB96177603444BCE4C</idno>
					<idno type="DOI">10.1016/j.is.2020.101494</idno>
					<note type="submission">Received 10 November 2017 Received in revised form 1 December 2019 Accepted 12 January 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Feature engineering Malicious URLs detection Nyström method Distance metric learning Singular value decomposition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In malicious URLs detection, traditional classifiers are challenged because the data volume is huge, patterns are changing over time, and the correlations among features are complicated. Feature engineering plays an important role in addressing these problems. To better represent the underlying problem and improve the performances of classifiers in identifying malicious URLs, this paper proposed a combination of linear and non-linear space transformation methods. For linear transformation, a two-stage distance metric learning approach was developed: first, singular value decomposition was performed to get an orthogonal space, and then a linear programming was used to solve an optimal distance metric. For nonlinear transformation, we introduced Nyström method for kernel approximation and used the revised distance metric for its radial basis function such that the merits of both linear and non-linear transformations can be utilized. 33,1622 URLs with 62 features were collected to validate the proposed feature engineering methods. The results showed that the proposed methods significantly improved the efficiency and performance of certain classifiers, such as k-Nearest Neighbor, Support Vector Machine, and neural networks. The malicious URLs' identification rate of k-Nearest Neighbor was increased from 68% to 86%, the rate of linear Support Vector Machine was increased from 58% to 81%, and the rate of Multi-Layer Perceptron was increased from 63% to 82%. We also developed a website to demonstrate a malicious URLs detection system which uses the methods proposed in this paper. The system can be accessed at: http://url.jspfans.com.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Internet is frequently used by criminals for illegal activities, such as financial fraud, phishing, online gambling, fake TV shopping, fraudulent prize winning and spam SMS in social networks <ref type="bibr" target="#b0">[1]</ref>. The dark side of Internet has emerged and bedeviled the world <ref type="bibr" target="#b1">[2]</ref>. The issue of malicious Uniform Resource Locators (URLs) detection is not well addressed yet and still causes great loss every year, which is particularly prominent in China. In recent years, mainland Chinese citizens suffered more than 20 billion Yuan loss per year from all kinds of phishing, most of which were with the aid of fake websites located outside China. Besides, omnipresent usage of smart phones also stimulates the rapid increase of mobile and Quick Response (QR) code phishing activities, which encode fake URLs in QR codes, to deceive people, especially seniors. With the widespread use of QR codes in almost every aspect of life in China, safety is a top concern for QR code payments. Fig. <ref type="figure" target="#fig_0">1</ref> shows the number of malicious URLs reported to Anti-phishing Alliance of China (APAC 1 ) from Aug 2011 to May 2016. More than 10,000 phishing URLs were reported to APAC per month on average. The curve on the bottom of Fig. <ref type="figure" target="#fig_0">1</ref> shows the trend of the number of malicious websites after eliminating the impacts of seasons.</p><p>URL is the infrastructure for all online activities, and detecting malicious URLs is usually viewed as a classification problem that discriminates the malicious ones from the benign ones <ref type="bibr" target="#b2">[3]</ref>. In fact, it involves far more complicated systematic tasks including continuous data collection, feature extraction, data pre-processing, and classification. Besides, specialized online systems are also needed to support these tasks <ref type="bibr" target="#b3">[4]</ref>. New characteristics of websites data (such as large scale, high dimension, sparsity, and changing patterns) keep challenging the traditional detecting methods. Many winners of competitions, such as Kaggle, admitted that the biggest gains usually came from being smart about representing data <ref type="bibr" target="#b4">[5]</ref>. Feature engineering is a process of representing features that benefits downstream machine learning algorithms. The general purpose of feature engineering is to feed in the original data and come up with new feature representations to solve specific problems in the data <ref type="bibr" target="#b5">[6]</ref>. Though the use of feature engineering in machine learning dates back to the nineties, it is still one of the most time-consuming and challenging steps in applied machine learning today, and is still a hot topic in computer science <ref type="bibr" target="#b6">[7]</ref>.</p><p>In malicious URLs detection, most previous works focused on the improvements of classifiers. A few involved feature selection and feature extraction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, but not dedicated to feature transformation, which produces new features based on the existing ones to facilitate downstream algorithms. Based on the belief that feature engineering plays an important role in improving malicious URLs detection, this paper unifies several linear and non-linear space transformation methods to improve the performance and efficiency of widely used classifiers. The contributions of this paper are twofold:</p><p>(1) It integrates several linear, non-linear, and integrated space transformation methods (namely singular value decomposition, distance metric learning, Nyström method, and the integrations) to help classifiers handle large scale URLs datasets efficiently. The integrated approach is different from existing works and has a strong theoretical background concerning the complementarity between distance metric learning and kernel methods. This approach provides a new direction for how to represent features of datasets that are large in size and have complex data distributions.</p><p>(2) This study builds a system that provides continuous support for features extraction, space transformation, and fast URLs classification, with cutting-edge technologies such as distributed cache, Map-reduce computing, and NoSQL database. The system collected 331,622 URLs and extracted 62 features. Experiments on these data showed that the proposed feature engineering methods significantly improved the performances of classifiers in discriminating malicious URLs.</p><p>The remainder of this paper is organized as follows. Section 2 reviews related works. Section 3 describes the data collection, summarizes the characteristics of the data, and analyzes the challenges of URL classification. Section 4 proposes linear and nonlinear transformation methods for malicious URLs detection and Section 5 presents the experiments and discusses the results. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Many approaches have been developed to detect malicious URLs and they can be categorized into four types: blacklists, content-based classification, URLs-based classification, and feature engineering approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Blacklists</head><p>''Blacklists'' refers to the methods that use records of known malicious URLs to filter the incoming URLs. C. Whittaker et al. described the design of a phishing classifier that was used to automatically maintain Google's phishing blacklist <ref type="bibr" target="#b9">[10]</ref>. Blacklisting services can be encapsulated in toolbars, apps, and search engines to detect fake websites and they rely on blacklists that are comprised of URLs taken from manual reporting databases maintained by backend databases. Such online services can be got easily, such as ''Google Safe Browsing service'' <ref type="bibr" target="#b10">[11]</ref>, ''Microsoft Smart Screen service'' <ref type="bibr">[12]</ref>, and ''360 Safety Center''. Though blacklists can be updated frequently, this approach misses new malicious sites that are constantly created <ref type="bibr" target="#b11">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Content-based classification</head><p>Content-based classification tries to identify malicious sites by analyzing contents or layouts of the pages <ref type="bibr" target="#b12">[14]</ref>. Y. Zhang et al. proposed a content-based approach to detect phishing websites <ref type="bibr" target="#b13">[15]</ref>. Ko used the contents layout of pages to classify websites <ref type="bibr" target="#b14">[16]</ref>. Unfortunately, fake website detection is a rivalry game. Fraudsters constantly employ new strategies and utilize newer, more sophisticated counter measures <ref type="bibr" target="#b15">[17]</ref>. With the aid of IP looking-up programming tools, fraudulent sites can recognize the IPs of search engine crawlers and auditing institutions, and then show different contents to the auditing institutions and the public <ref type="bibr" target="#b16">[18]</ref>. For search engines and auditing institutions, the contents of the fraudulent sites would seem normal and regular, which makes content-based detecting measures fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">URLs-based classification</head><p>As a basic infrastructure, URL plays an important role in online business. The ''background'' information of a URL, i.e., the domain name information, host information, indexing information from search engines, is more reliable to identify malicious site than content because such information is hard to be manipulated by criminals <ref type="bibr" target="#b17">[19]</ref>.</p><p>S. Garera et al. described several obfuscation techniques that phishing URLs used, and selected features including lexical features and external features, such as Google PageRank and Google page quality, to detect malicious URLs <ref type="bibr" target="#b18">[20]</ref>. The reason behind the usage of ''Google PageRank'' is that websites highly ranked by search engines have a lower chance of being fraudulent. In a follow-up work, J. Ma <ref type="bibr" target="#b3">[4]</ref> proposed a light-weighted detecting approach based on websites URLs and compared the results of four classifiers. <ref type="bibr" target="#b3">[4]</ref> compared the performance of batch-based algorithms to online algorithms, and argued that their methods are better suited to URLs classification and can process a large number of instances far more efficiently than batch methods.</p><p>The classification part of this study is also URLs-based. The difference between this study and previous URLs-based classification is that we aim to improve malicious URLs detection via feature engineering, rather than designing new classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Feature engineering approach</head><p>The key idea of ''feature engineering'' is representing or creating features that make data mining algorithms work better. It is somewhat similar to ''Extract-Transform-Load'' (ETL) in data warehouses with regards to the transformation of the original data <ref type="bibr" target="#b19">[21]</ref>.</p><p>One frequently used feature engineering approach is to select important features for specific algorithms. W. Zhang used genetic algorithm to divide features into critical features and non-critical features first, then projected non-critical features to a new feature, and took the critical features and the new feature as input data to construct a detection model <ref type="bibr" target="#b7">[8]</ref>. W. Dhifli proposed a feature selection algorithm for big data based on subgraph selection <ref type="bibr" target="#b20">[22]</ref>. R. Rakesh proposed a modified C4.5 algorithm that used only the important features for malicious URLs detection <ref type="bibr" target="#b21">[23]</ref>. Zhang et al. used Chi2 (k-χ 2 ) to gain the most relative features for e-commerce websites classification <ref type="bibr" target="#b22">[24]</ref>. Barddal used boosting decision stumps to extract features from data streams <ref type="bibr" target="#b23">[25]</ref>. The main limitation of feature selection is that it does not take into consideration the interactions between features.</p><p>Another feature engineering approach transforms the original features into new ones to represent the original data, and the transformation can be accomplished through linear or nonlinear methods. Most linear transformation methods are based on matrix factorization, such as Principal Component Analysis (PCA), Singular Value Decomposition (SVD) and Non-negative Matrix Factorization (NMF). For instance, Khare et al. used NMF to generate new features, namely ''random walk term weights'', to improve URLs classification <ref type="bibr" target="#b24">[26]</ref>. The main limitation of linear methods is that they are not capable of learning non-linear dependencies between features. To capture the non-linear interactions between features, deep neural networks, which perform non-linear transformations, were used recently for similar tasks. Wang et al. used Auto-Encoder (AE) to eliminate noises and improve malicious JavaScript detection <ref type="bibr" target="#b25">[27]</ref>. Thaler et al. proposed to use Variational Auto-Encoder (VAE) to learn the most evident hidden variables in information security <ref type="bibr" target="#b26">[28]</ref>. Nevertheless, both AE and VAE are general-purpose neural networks, and none of them was dedicated to the problems arising from malicious URLs detection. Moreover, neural network is a black box and it is hard to position the problems and explain how the problems were addressed. How to use non-linear transformation of features to facilitate the downstream algorithms is still a hot topic in machine learning <ref type="bibr" target="#b27">[29]</ref>.</p><p>Driven heavily by the particular problems arising from malicious URLs detection, this study aims to propose a combination of linear and nonlinear space transformation methods to better present the features such that the performances of malicious URLs detection classifiers can be improved. This study also has a strong theoretical background and can explain why the proposed transformation is needed and how the problems are addressed.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> describes the characteristics of the four types of malicious URLs detection approaches, plus the proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data collection and characteristics</head><p>As discussed in the previous section, this paper focuses on URLs-based approaches to detect malicious URLs. Since the characteristics of URLs have important impacts on the design of feature engineering approaches, this section describes the data collection process and analyzes the characteristics of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data sources</head><p>Malicious URLs were collected from ''PhishTank'' 2 and ''360 Safety Center''. 3 PhishTank is a collaborative clearing house for data and information about phishing on the Internet. It also provides an open API for developers and researchers to integrate anti-phishing data into applications. ''360 Safety'' is a major Chinese network security company, and they periodically publish Chinese malicious websites reports, including fishing sites, fraud sites, and hacking sites. Benign URLs were collected from ''Hao123'' <ref type="foot" target="#foot_0">4</ref> and ''DMOZ''.<ref type="foot" target="#foot_1">5</ref> Hao123 is a Chinese yellow page website. All sites indexed by Hao123 are reliable because they have been manually verified and registered by Chinese BEIAN system, which is a censorship mechanism developed by the Chinese government. DMOZ is an acronym for Directory Mozilla. It is an open web directory whose entries are constantly examined by volunteer editors, who go through a vetting process.</p><p>These data sources have been used by previous studies and proven to be legitimate <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Our dataset is not exactly the same as the preceding ones since it is collected at different time, plus the features extracted are different. The details of the data collection are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Features extraction</head><p>The features used in this paper are extracted from the URLs and their background information. They can be categorized into four types.</p><p>(1) Domain-based features This type is mainly about ''Who Is'' (WHOIS) information of the domains the websites use, and includes the following features:</p><p>TLDs of the domain names. The Top Level Domain names (TLDs) of the URLs. Phishing websites with domain names of .COM, .TK, .LT and .CC accounted for 76.86% of all the handled phishing websites, in which ''.COM'' ranked the first among all domain name types.</p><p>Expire days. Number of days left before the day the domain expires.</p><p>Register days. Number of days since the domain was registered.</p><p>Domain and its sections. They include server section and domain section. One of the distinct characteristics of malicious websites is that their server sections are much longer than benign websites'. For example, ''paypal.login.updateyouraccountiniqsitub .itbae.com'' looks like that it is related to PayPal, but its domain is ''itbae.com'' and has nothing to do with PayPal. The server section accounts for most of its disguise. Thus, domain and server section features, such as ''server section length'' and ''domain length'', are useful in malicious URLs classification.</p><p>(2) Host-based features Country. Different countries have different censorship policies and they have influences on the classification.</p><p>Sponsor of host. Sine frauds or hackers tend to buy hosts from negligent sponsors who do not keep eyes on their hosts' contents, sponsors can provide useful information for malicious URLs classification. This feature is of nominal type.</p><p>Technologies. It indicates what technologies the site uses. Typical technologies include php, asp, jsp, cgi, and static html. Phishing sites may prefer certain technologies.</p><p>(3) Reputation-based features It is important to examine the reputation of a website while judging whether a website is malicious. A website with good  reputation is more likely to be benign. There are many tools on Internet to check reputations, such as Alexa, Google, and Yahoo. Baidu index. The number of pages indexed by Baidu. Parts of the URLs are collected in China and they can be hardly indexed by search engines outside China. So, the number of pages indexed by Baidu was taken into consideration.</p><p>Baidu inverted index. Inverse index means the pages which link to the objective URL. Actually, inverted index is more important than index. It is not very difficult for a website to make many pages indexed by search engine. But it is really hard for a website to make other sites link to its pages and make the pages of those sites be indexed by the search engine.</p><p>Alexa rank. Alexa has a database of global websites ranking information.</p><p>Alexa linkin. Alexa records how many sites linking to the specific site, and provides open access for the public.</p><p>Google page rank. The famous Google Page Ranking system. (4) Lexical Features URL characteristics. They indicate the characteristics of URL, such as the number of dots in URL, and number of parameters.</p><p>Keywords. We built a keyword corpus for the words contained in the URLs. Keywords in a URL contain important information that can be used to judge whether a URL is malicious. The selected keywords include: alipay, jd, safety, paypal, google, apple, facebook, amazon, porn, gamble, and awarding. Such keywords are frequently used to counterfeit famous websites and deceive the users.</p><p>We extracted 62 features in total. The number of features in each type is listed in Table <ref type="table" target="#tab_1">2</ref>. The descriptions of the features are list in Appendix A.</p><p>Feature extraction of URLs costs more that 60% of the total time of this work. Most of the efforts were devoted to the retrieval of ''WHOIS'', ''Host'', and ''Reputation base'' information. This job cannot be accomplished without the support of a dedicated online system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Characteristics of URLs dataset</head><p>While collecting the data and extracting the features, we observed the following characteristics of the URLs data.</p><p>(1) Patterns in the data are changing over time Fig. <ref type="figure" target="#fig_2">3</ref> compares the top four categories of counterfeit objects reported by APAC in 2012 and 2016. Mobile fraud, which was not one of the top four categories in 2012, was responsible for more than 10% of the total fraud in 2016. Media fraud dropped from 10.64% in 2012 to 1.64% in 2016, and Instant Messages (IM) fell out of the top four categories. It indicates that the counterfeit objects of the malicious URLs were changing from 2012 to 2016.</p><p>In fact, the patterns of URLs are susceptible to the evolving external economic and social environments. In addition, since the relationship between fraudulent websites and inspection systems is adversarial, the fraud and the anti-fraud strategies are evolving over time. Similar phenomenon has also been observed in many industries, such as finance, network intrusion detection, and it is also a hotspot topic in game theory research <ref type="bibr" target="#b28">[30]</ref>. All of these factors result in changing and shifting patterns (locally or globally) in the data, and require adaptive algorithms and models.</p><p>(2) Structural differences of URLs URLs are not ''identically distributed'' because of their structural differences. For example, newly created webpages hardly have ''reputation based'' features, and they only have ''Host based'', ''Domain based'' and lexical features. ''TinyURLs'' even does not have ''Domain based'' and ''Lexical'' features, so they can only be represented by ''Host based'' features. Moreover, websites in the world are geo-distributed and they have geographical differences. China has BEIAN system, which is an important mechanism that can be used to identify fraud websites in China, but URLs outside China do not have this feature. Another special feature of URLs in China is Baidu Index. The nature of URLs' structural differences result in changing and shifting patterns inside the data <ref type="bibr" target="#b29">[31]</ref>.</p><p>To make the characteristics described in (2) and (3) easier to understand, we reduce the dimension of the dataset to 2 using t-Distributed Stochastic Neighbor Embedding (t-SNE) techniques and illustrate them in Fig. <ref type="figure" target="#fig_3">4</ref>. It shows that data points with same labels cluster at lots of different local areas, and they are not linearly separable.</p><p>(3) The data scale is large The explosion of the Web brings about a corresponding surge in the sheer volume of URLs. Malicious URLs detection approach must be capable of handling datasets of huge size. This study  collected 331,622 URLs, which only present an ordinary volume in real applications. However, it is very challenging for many existing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Challenges in malicious URLs detection and proposed space transformation methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Challenges in malicious URLs detection</head><p>An assumption of statistical learning models is that variables are ''Independent and Identically Distributed'' <ref type="bibr" target="#b30">[32]</ref>. In malicious URLs detection, it is far more complicated. As it is stated in Section 3.3, given the complexity of data sources, uniform patterns do not always exist across the entire dataset and specific patterns only fit in certain parts of the data at certain time. So, it is necessary for data mining algorithms to learn using the surrounding information. Past researches on such issue <ref type="bibr" target="#b31">[33]</ref> showed that there are three representative types of classifiers that can handle the problems above: (1) Instance-based methods such as k-Nearest Neighbor (k-NN), (2) Linear models with kernel transformation, such as kernelized Support Vector Machine (SVM), and (3) deep neural networks, which are recognized as all-purpose approximator for any pattern <ref type="bibr" target="#b32">[34]</ref>. Both instancebased models and linear models still face a number of challenges before applying them to malicious URLs detection, and we will analyze them later. As for deep neural networks, they are black box models and it is hard to explain the knowledge hidden in them <ref type="bibr" target="#b32">[34]</ref>. They are not the main focus of this work. We will compare the experimental results of neural networks with instance-based and linear classifiers in the experiment section, and discuss the significance of the proposed feature engineering methods for neural networks.</p><p>Instance-based classifiers, such as k-NN, can overcome the challenge of ''linearly inseparable'' in malicious URLs detection. k-NN does not pursuit linear separability and only uses the labels of the surrounding points. But unlike SVM, which can learn a vector that rescales the original features linearly, k-NN does not have a built-in mechanism to assign weights to features <ref type="bibr" target="#b33">[35]</ref>. This can result in severely distorted distance metrics and the measurement of the distance between data points will be unconvincing.</p><p>Generalized linear models, such as Linear Support Vector Machine (L-SVM), Linear Discriminant Analysis (LDA), and Logistic Regression, are not suitable for distributions in which data points are not linearly separable <ref type="bibr" target="#b34">[36]</ref>, such as the situation in Fig. <ref type="figure" target="#fig_3">4</ref>, and they must resort to kernel methods (such as radial basis function kernel) in such circumstance. But kernel methods involve intensive computation of the kernel matrix, whose time cost is O(n 2 ) <ref type="bibr" target="#b35">[37]</ref>. The scale of URLs datasets makes kernel methods hard to find solutions. But detection systems require fast response to new data which arrive in real-time, even in big data circumstance <ref type="bibr" target="#b36">[38]</ref>.</p><p>To further analyze the potential challenges in instance-based models and linear models, we use Correlogram <ref type="bibr" target="#b37">[39]</ref> to show parts of the inter-correlations among URLs features in Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>In Fig. <ref type="figure" target="#fig_4">5</ref>, five features are selected as examples (if_html, dot_num, register_days, google_page_rank, google_indexed_ num). The names and the maximum and minimum values of the features are shown in the diagonal. The lower triangle is shaded to represent the magnitude and direction of the correlations. The upper triangle contains smoothed best fit lines and confidence ellipses. Based on the ''Correlogram'' and previous analysis, we propose 3 main problems lying in the URLs dataset:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem (1).</head><p>There are linear correlations between features. For instance, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>, ''if_Html'' and ''DotNum'' are strongly correlated, so are ''G.PR'' and ''G. Ind''. Correlation is not a big issue for models like neural network which can automatically recombine features. However, both k-NN and SVM do not have built-in mechanisms to handle correlations. The correlations should be eliminated before applying classifiers like k-NN and SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem (2).</head><p>Manually selected features may be not relevant to the classification problem, the importance/weight of each feature is unknown, and the scales of the features are very different. For instance, ''DotNum'' ranges from 0 to 27, while ''Baidu. Ind'' ranges from 0 to 1.0E8. Computing the distance between two data points in k-NN can be easily dominated by a single largely scaled attribute feature. Moreover, whether these features are relevant to classification is unknown. Although this issue is trivial for neural network and even linear models such as SVM which can learn the weights of the features automatically, it is critical to k-NN and Radial Basis Function (RBF) kernel, which do not learn the weights of features. The distance metrics should be revised before applying any distance based models (such as k-NN and RBF) <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b39">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem (3).</head><p>The scale of the URLs dataset is too huge to apply kernel methods for linear classifiers. For instance, the size of a kernel matrix is n × n, where n is the number of data points, and it involves n(n + 1)/2 times computation of the inner products of the data points. In this study, the memory cost of the kernel matrix is about 331, 622 2 ≈ 1.1E11, and the computational cost is about 5.5E10. That is not a trivial problem. In fact, the scale of the dataset is a challenge for all models with high computational costs in malicious URLs detection.</p><p>The objective of the following sub-sections is to propose feasible solutions to deal with these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Correlations between features and the counter measure: Linear orthogonal transformation via SVD</head><p>We use SVD to solve Problem <ref type="bibr" target="#b0">(1)</ref>. SVD can be used to eliminate correlations among features. Moreover, SVD can find a low-rank subspace to approximate the original high-dimension linear space <ref type="bibr" target="#b40">[42]</ref> to partially tackle the large-scale problem in malicious URLs detection.</p><p>Suppose that P H is a projection matrix to subspace H ∈ R r×n , then the optimization problem is: min</p><formula xml:id="formula_0">P H E{∥x -P H x∥ 2 },<label>(1)</label></formula><p>where P H = W (W T W ) -1 W T . P H is an idempotent matrix and the solution to P H is unique. It turns out that the solution of Eq. ( <ref type="formula" target="#formula_0">1</ref>)</p><p>is W = U, where U is the singular vectors matrix <ref type="bibr" target="#b41">[43]</ref>.</p><p>If we set the rank of the subspace to r, then the solution of Eq. ( <ref type="formula" target="#formula_0">1</ref>) is W = U r , where U r is the singular vectors matrix corresponding to the r top eigenvalues of the autocorrelation matrix XX T . The projection matrix for the best approximate rranksubspace is:</p><formula xml:id="formula_1">P H = U r U T</formula><p>r . If we convert the form of the singular value decomposition of data matrix X as follows:</p><formula xml:id="formula_2">X m×n = U m×m Σ m×n V T n×n ,<label>(2)</label></formula><formula xml:id="formula_3">U T m×m X m×n = U T m×m U m×m Σ m×n V T n×n = Σ m×n V T n×n ,</formula><p>the orthogonal projection is:</p><formula xml:id="formula_4">x → U T m×m x. (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>If we keep the r top singular values, such that:</p><formula xml:id="formula_6">X m×n ≈ U m×r Σ r×r V T r×n ,<label>(4)</label></formula><formula xml:id="formula_7">U T r×m X m×n ≈ U T r×m U m×r Σ r×r V T r×n = Σ r×r V T r×n = Xr×n ,</formula><p>where every single column of X m×n represents a data point x, and</p><p>Xr×n is the new transformed data matrix.</p><p>The orthogonal projection to the low rank optimal approximation subspace can be denoted as:</p><p>x → U T r×m x.</p><p>(</p><p>With the projection in Eq. ( <ref type="formula" target="#formula_4">3</ref>), the features can be kept linearly orthogonal. With the projection in Eq. ( <ref type="formula" target="#formula_8">5</ref>), the model is supposed to have an effect of ''noises eliminating'' by keeping the r top singular values <ref type="bibr" target="#b40">[42]</ref>. The essence of the derivation above is the same as PCA. In large scale setting, SVD can be solved via Lanczos algorithms or other power methods <ref type="bibr" target="#b42">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Assigning weights to orthogonally transformed features: LPbased distance metric learning</head><p>We propose a new Distance Metric Learning (DML) approach to solve Problem (2) presented in 4.1. The original objective of DML is to learn a distance metric that is consistent with a given set of constraints, namely minimizing the distances between pairs of instances from the same class and maximizing the distances between pairs of instances from different classes <ref type="bibr" target="#b33">[35]</ref>. Let the distance metric denoted by matrix A ∈ R m×m . In distance metric learning, the form of the distance function is typically presented as follows <ref type="bibr" target="#b43">[45]</ref>:</p><formula xml:id="formula_9">d A (x i , x j ) = ∥x i -x j ∥ A = √ (x i -x j ) T A(x i -x j ) = √ (x i -x j ) T P T P(x i -x j ) = √ (Px i -Px j ) T (Px i -Px j ) = d E (Px i , Px j )<label>(6)</label></formula><p>With appropriate matrices A and P, a transformed space can be obtained via linear projection x → Px, where A is semi-definite and P can be solved via the Cholesky decomposition or Eigen decomposition to A. The purpose of the projection is to make the points with same labels closer to each other.</p><p>To denote the data points, optimization objective, and constraints, we introduce the form of triplets <ref type="bibr" target="#b39">[41]</ref>: (x i , x j , x k ), in which x i is the closest instance to x j with the same label and x k is the closest instance to x j with a different label. Given the basic idea of DML, distance between x i and x j should be constrained, while distance between x j and x k should be maximized. So, the DML can be formulated by the following convex programming problem <ref type="bibr" target="#b33">[35]</ref>:</p><formula xml:id="formula_10">max A ∑ (x j ,x k ) ∥x j -x k ∥ A 2 (7) s.t. ∑ (x i ,x j ) ∥x i -x j ∥ A 2 ≤ 1, A ⪰ 0 (7) is usually expressed in a trace form: max A tr(C T A) (8) s.t. tr(F T A) ≤ 1, A ⪰ 0</formula><p>The positive semi-definitive constraint A ⪰ 0 is needed to ensure that the triangle inequality and there is no negative distance between any two data points. Two major challenges of applying DML to malicious URLs detection are also related to the data scale. The first one is that DML involves the search of the neighboring points with same labels and different labels. At worst case it takes O(n 2 ). Lots of methods, such as indexing technologies <ref type="bibr" target="#b44">[46]</ref> and Local Sensitive Hashing (LSH) <ref type="bibr" target="#b45">[47]</ref>, have been proposed to solve this problems and proven to be effective. This study will use the LSH method to search neighboring points. The second challenge is to solve the distance metric matrix A in Eqs. ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>), which involves semidefinite programming <ref type="bibr" target="#b33">[35]</ref> or projected gradient methods <ref type="bibr" target="#b39">[41]</ref>. These methods are hard to be applied to large scale learning setting, especially when the dimension is high. So, we will focus on the amelioration of the second challenge in this study.</p><p>After performing SVD, features of the original data have been linearly recombined. Now the columns are kept orthogonal and there is no need to learn another matrix to exert linear recombination of the features <ref type="bibr" target="#b46">[48]</ref>. What we need is to learn a diagonal matrix, which assigns weights to scale the independent features so that they reflect better linear relations between the features and the labels. These are exactly Problem (2) presented in 4.1.</p><p>DML can now be viewed as a two-stage learning process. The linear projection matrix of DML can be decomposed into two parts: P = P A P B , where P A ∈ R r×r , and P B ∈ R r×m . P B involves orthogonal projection into a new space and eliminates the correlations between the features, and P A involves scaling the new space (i.e., assigning weights to the independent features) to reflect better linear relations between the features and the labels.</p><p>In Eq. ( <ref type="formula" target="#formula_8">5</ref>), the projection x → U T r×m x is actually the solution to P B , i.e., P B = U T r×m . Based on the transformation of P B , the solution to P A can be simplified as a Linear Programming (LP).</p><p>We propose a new LP based DML problem in Eq. ( <ref type="formula">9</ref>). min</p><formula xml:id="formula_11">P A ∑ (x i ,x j ) ∥P A x i -P A x j ∥ 2 + µ ∑ ξ j (9) s.t. ∥P A x j -P A x k ∥ 2 -∥P A x i -P A x j ∥ 2 ≥ 1 -ξ j , ξ j ≥ 0, P A = diag( √ λ 1 , √ λ 2 , . . . , √ λ m ), λ 1 , λ 2 , . . . , λ m ≥ 0.</formula><p>where ξ j are nonnegative slack variables and P A is diagonal. The objective is to minimize the distance between the closest points with the same labels. The constraints indicate that for every x j in the triplets, the distance to the closest point x i with the same label is less than the distance to x k with a different label. The scalar ''1'' on the right side is arbitrary because all elements in P A can be scaled up and down in exact proportion. With slack variables, the optimization problem is more robust and the risk of ''no solution'' is reduced. Now Eq. ( <ref type="formula">9</ref>) is a linear programming problem, can be solved quickly, and is free from the semi-definite constraint, which involves Eigen decomposition and projection to the semi-definite core at each iterative optimization step.</p><p>The linear independence of the features in this method is guaranteed. The original projection x → Px of DML now can be denoted as:</p><formula xml:id="formula_12">x → P A P B x. (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>After the projection of P A and P B , the challenges presented in 4.1 for malicious URLs detection using k-NN can be overcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Approximating kernel matrix: Nyström method for nonlinear transformation</head><p>We use Nyström method to approximate kernel matrix <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b47">49]</ref> and solve Problem (3) presented in 4.1. Suppose a kernel matrix</p><formula xml:id="formula_14">A: A = [ A 11 A 12 A 21 A 22 ] ,<label>(11)</label></formula><p>and C = [</p><formula xml:id="formula_15">A 11 A 21</formula><p>] T . Nyström method uses A 11 and C to approx- imate matrix A. Suppose C is a uniform sampling of the columns, Nyström method generates a rank-k approximation of A(k ≤ n)</p><p>and is defined by:</p><formula xml:id="formula_16">A nys k = CA 11 + C T = [ A 11 A 21 A 21 A 21 A 11 + A 21 T ] ≈ A,<label>(12)</label></formula><p>where A 11 + denotes the generalized pseudo inverse of A 11 . There exists an Eigen decomposition A 11</p><formula xml:id="formula_17">+ = V Λ -1 V T , such that each element A nys k ij in A nys k</formula><p>can be decomposed as:</p><formula xml:id="formula_18">A nys k ij = (C T i V Λ -1 V T C j ) = (Λ -1/2 V T C i ) T (Λ -1/2 V T C j ) = (Λ -1/2 V T (κ(x i , x 1 ), . . . κ(x i , x m ))) T • (Λ -1/2 V T (κ(x j , x 1 ), . . . κ(x j , x m ))),<label>(13)</label></formula><p>where κ(x i , x j ) is the base kernel function, x 1 , x 2 , . . . , x m are representative data points and can be obtained by uniform sampling or clustering methods <ref type="bibr" target="#b47">[49]</ref>. Let:</p><formula xml:id="formula_19">φm (x) = Λ -1/2 V T (κ(x, x 1 ), . . . κ(x, x m )) T (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>such that:</p><formula xml:id="formula_21">A nys k ij = φm (x i ) T φm (x j ) = κ(x i , x j )<label>(15)</label></formula><p>So, with Nyström method, we can get an explicit approximation of the nonlinear projection φ(x), which is:</p><formula xml:id="formula_22">x → φm (x). (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>One problem in Nyström method is how to obtain the representative points x 1 , x 2 , . . . , x m in Eq. ( <ref type="formula" target="#formula_19">14</ref>). The simplest way to get these points is uniform sampling. However, in this way the results are random and make the Nyström method unstable.</p><p>To overcome this shortcoming, large size of samples is needed, which greatly increases the computation cost. Take our case for example: if we sample 10% of the instances (about 33,000), it means that the dimension of the transformed dataset is 33,000.</p><p>Inspired by the methods in RBF Networks, we can first cluster the original points and then use the centroids of the clusters as the representative points. Actually, the effectiveness of this method has been proven in another research <ref type="bibr" target="#b47">[49]</ref>. We introduce Self-Organizing Features Map (SOFM) here as the clustering algorithm, which can find the k clusters centroids at linear time cost <ref type="bibr" target="#b48">[50]</ref>.</p><p>Nyström method alleviates the time cost and provides a feasible approximate solution for kernel methods in large scale circumstance. It should be very useful for linear classifiers such as linear SVM to solve the problem of ''linearly inseparable''.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Integrated linear and non-linear transformation</head><p>As described in previous sections, P B is a linear and unsupervised process, P A is a linear and supervised process, and Φn (x) is a nonlinear and unsupervised process. It should be a better solution if we can integrate the merits of linear, nonlinear, supervised, and unsupervised transformation because the integrated model is expected to overcome the shortcomings of each single one.</p><p>(1) DML for Radial Basis Function in Nyström method Radial Basis Function (RBF) is the most frequently used function in kernel methods, which has the following form:</p><formula xml:id="formula_24">κ(x i , x j ) = exp(-γ ∥x i -x j ∥ 2 ), (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>where γ is a tuning parameter. This type of kernel is independent of the original positions of data points and only utilizes the distances between them. We can see from Eq. ( <ref type="formula" target="#formula_24">17</ref>) that RBF relies on the distance metric between x i and x j . So learning a good metric before applying a kernel function in Nyström method should have a positive influence on the overall performance. The integrated space transformation is as follows:</p><p>x → Φn (P A P B x)</p><formula xml:id="formula_26">(18)</formula><p>where Φn (P A P B x i ) T Φn (P A P B x j ) ≈ κ(P A P B x i , P A P B x j ).</p><p>(2) Kernelized DML: DML after Nyström method The basic idea of kernel methods is to learn a model in a transformed space (i.e., reproducing kernel Hilbert space). Theoretically it makes sense if we perform DML on a dataset that has been transformed by Nyström method, because that is exactly what ''kernel tricks'' refer to. Furthermore, one possible shortcoming of Nyström method is that the dimension of the transformed data may be high. Many models based on Euclidean distance cannot work effectively on datasets with high dimensions <ref type="bibr" target="#b49">[51]</ref>. This study will explore the effects of dimension reduction with SVD and DML on dataset transformed by Nyström method. The integrated form of space transformation now is as follows:</p><formula xml:id="formula_27">x → P A P B Φn (x),<label>(19)</label></formula><p>(3) Integrated space transformation framework We outline the preceding methods in an integrated space transformation framework in Fig. <ref type="figure" target="#fig_5">6</ref>, which includes SVD, DML, Nyström methods, and the integrations of these basic models. SVD and Nyström method are unsupervised, while DML is supervised. In another aspect, SVD and DML are linear models, while Nyström is a non-linear model. The integrations of them are also proposed to utilize all of the merits of supervised, unsupervised, linear, and non-linear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and system development</head><p>We implemented the proposed models using Java and Scala language, with the help of third-party Java libraries including ''common-math3'', 6 ''weka'', 7 ''joptimizer'', 8 and ''lp_solve''. 9 The configuration of the computer that conducted the experiments was i7-4560U (4CPU, 2 GHz), 16 GB RAM, and Ubuntu OS. As noted in Section 3, we collected 331,622 instances of URLs. The malicious URLs were defined as positive and the benign URLs were defined as negative. The details of the dataset used for experiments were listed in Table <ref type="table" target="#tab_0">1</ref>.    DML is based on SVD, NYS-DML is based on NYS, and DML-NYS is based on SVD and DML. The cumulative time refers to the total time cost of the process and its preceding processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Grid search for the optimal parameter in kernel function</head><p>The parameters of the kernel function κ in Eq. ( <ref type="formula" target="#formula_24">17</ref>) are unknown. A small subset of 10,000 instances, which were selected by uniform sampling from the whole dataset, was used to evaluate the optimal parameters. Then a grid search was performed by kernelized SVM to evaluate the performance of each γ in RBF. The search was conducted on [2 -12 , 2 -11 , . . . , 2 12 ] and the accuracy of the kernelized SVM was shown in Fig. <ref type="figure" target="#fig_6">7</ref>. Fig. <ref type="figure" target="#fig_6">7</ref> showed the accuracy of kernelized SVM with parameter γ on each value in [2 -12 , 2 -11 , . . . , <ref type="table" target="#tab_1">2 12</ref> ]. After the search, the optimal value of γ in Eq. ( <ref type="formula" target="#formula_24">17</ref>) turned out to be 2 -6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Singular values decomposition</head><p>There are two procedures in Fig. <ref type="figure" target="#fig_5">6</ref> involving singular value decomposition: ''SVD'' on the original dataset and ''NYS-DML'' on the dataset after Nyström transformation (the number of clusters, i.e., the dimension, was set to 500). The singular values distributions were shown in Fig. <ref type="figure" target="#fig_7">8</ref>.</p><p>Fig. <ref type="figure" target="#fig_7">8</ref> showed that our datasets were low-rank because the number of the big singular values was small. Noises can be partially eliminated by keeping the big singular values <ref type="bibr" target="#b46">[48]</ref>. Dimension reduction also can be realized using the method in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Space transformation</head><p>We used the models in Section 4 to perform the space transformation. The total sizes of the datasets are the same but their numbers of features vary. The number of the features kept in each method is listed in Table <ref type="table" target="#tab_3">3</ref>.</p><p>The real time cost of each method was recorded in Table <ref type="table" target="#tab_4">4</ref>. Table <ref type="table" target="#tab_4">4</ref> showed that the time cost of the Nyström method was 377 s, which is a dramatic reduction of computational cost compared to exact kernel methods (As it is shown in Table <ref type="table" target="#tab_8">8</ref>, the time cost of SVM with kernel method was 6.085E5 s, i.e. 7.043 days). The time cost of DML was 8,728 s (i.e., more than 2 hours), most of which was taken by the search of the closest points in DML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Performance evaluation</head><p>After the transformations, 7 different datasets were generated using these feature engineering methods. To evaluate the effectiveness of the proposed methods, we first applied sixteen classifiers to the datasets to evaluate the impacts of these methods, then we used other 5 feature selection models and 6 feature transformation models to evaluate the comparative performance of our methods <ref type="bibr" target="#b50">[52]</ref>. Besides, an extensive experiment was also conducted to compare the performance of textual contents based method with the URLs' based method. The results are recorded in Appendix B.</p><p>(1) TPR and accuracy evaluation based on 16 classifiers The purpose of the models is to improve malicious URLs detection, we evaluated both True-Positive Rate (TPR) and Accuracy.</p><p>TPR is defined as TPR = TP/(TP + FN) and indicates the ability of identifying the malicious URLs. In each test, 3-fold cross validation was used for the accuracy evaluation because it is well balanced and suitable for our datasets. Table <ref type="table" target="#tab_5">5</ref> summarizes the TPR and overall accuracy of the selected classifiers with different transformed datasets, respectively (TPR in the first line and accuracy in the second line of each cell). The ''Original'' column in Table <ref type="table" target="#tab_5">5</ref> records the TPR and accuracy of the classifiers using the original dataset. The other six columns indicate the results of applying the corresponding classifiers to the datasets transformed by the six different methods. The standard deviation between folds were also recorded to show the stability of the classifiers.</p><p>The sixteen classifiers are divided into four groups in Table <ref type="table" target="#tab_5">5</ref>. The first group is distance-based classifiers, which use Euclidean distance or radial basis function. The second group includes generalized linear classifiers, the third group contains classifiers based on neural networks, and the fourth group contains other classifiers for comparative purpose. Since the proposed approach integrates the DML and the non-linear transformations with kernel methods, theoretically, DML should work for distance-based methods (such as k-NN, RBF and RBF based kernel), and the Nyström method should work for linear models (such as SVM, LDA, and Logistic). To emphasize this situation in Table <ref type="table" target="#tab_5">5</ref>, TPRs and accuracies that are theoretically supposed to be improved by our approaches are shown in italics. For each classifier, the best TPR and accuracy are denoted in bold. ''\'' indicates that the computer ran out of memory using the corresponding classifier on the datasets.</p><p>Based on the experimental results listed in Table <ref type="table" target="#tab_5">5</ref>, the effectiveness of each feature engineering method is analyzed as follows.</p><p>SVD and SVDR. Table <ref type="table" target="#tab_5">5</ref> showed that the improvements of the TPR and overall accuracy of most classifiers were not evident after applying SVD, as a result of ''keeping features orthogonal''. Theoretically SVD should have no impact on the result of k-NN because the projection matrix U T is orthogonal and the distance between points should be identical to the original one after projection. Slight precision loss was observed during the computation of the SVD. SVDR <ref type="bibr" target="#b23">(25)</ref>, which keeps 25 features corresponding to the 25 largest singular values, improved the TPR and overall accuracy of most classifiers due to the partial noise elimination <ref type="bibr" target="#b46">[48]</ref>. Although Naïve Bayes had the highest TPR among all classifiers, its overall accuracy was 41.3%, which indicated that it basically classified all data points as malicious.</p><p>Nyström method. Nyström method is the approximation of kernel matrix. The results show that it significantly improved the TPR of generalized linear models, such as L-SVM, LDA, and Logistic regression, which run fast but pursue data points being linearly separable on the whole. With Nyström method, the TPR of these models were promoted by more than 15%. The TPR and accuracy of L-SVM were increased to 79.32% and 90.32%, respectively. They were very close to the ones of K-SVM on the original dataset, which were 79.41% and 90.91%. As an approximate kernel, the Nyström method did not lose too much accuracy. Therefore, it is an effective and feasible approach for linear classifiers to realize kernel tricks on large scale datasets. Besides, the Nyström method also improved the performances of MLPs. The TPR of MLPs (shallow or deep) on the original dataset did not exceed 65%, while the TPR was were increased to about 75%. It indicated that kernel methods have positive impacts on neural networks, and the improvement by the kernel method is more significant than simply adding more layers to MLP (from 1 hidden layer to 7 hidden layers). Non-linear transformation with kernel methods provides novel features that are not easy for neural networks to learn. CNN's results indicated that although convolution kernel is effective on images, it does not apply to the URLs dataset in this study. Finally, it should be noted that the Nyström method raised the dimension of the datasets (In our experiments it was raised from 62 to 500). The memory and computational cost increased. ''Voted Perceptron'' ran out of memory on all datasets processed by the Nyström method (NYS, DML-NYS) since either the volume or the dimension was too big for this classifier.</p><p>Distance Metric Learning. DML had prominent effect on k-NN, RBF Network, RBF Classifier, K-SVM, and slight effect on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrated methods.</head><p>The DML-NYS model achieved the highest TPR and overall accuracies for linear models and MLP, because DML learned better distance metrics for the Radial Basis Function (RBF) in the Nyström method. The second integration model NYS-DML is the kernelization of DML and did not have evident effects on most classifiers. The kernel function used in this paper was Radial Basis Function, whose merit is local learning ability. This kind of kernelization makes sense for linear and global models such as linear SVM, but not necessarily makes sense to DML. DML generally applies to k-NN, which is a typical local model itself. But it may have significance for other applications with proper kernel functions. This problem is beyond the scope of this paper.</p><p>Finally, we should notice that no evident effect of the proposed methods were observed on Naïve Byes and RILB. The proposed space transformation methods do not apply to these classifiers.</p><p>(2) TPR and accuracy comparison with other feature selection models Feature selection is a sub-category of features engineering methods, and many researches used such methods to improve malicious URLs detection. For instance, <ref type="bibr" target="#b22">[24]</ref> used Chi2 (k-χ 2 ) to gain the relative features for classifiers. <ref type="bibr" target="#b21">[23]</ref> used Information Gain (InfoGain) to generate a pruned set of the most important features and test how different classifiers perform with the pruned feature set.</p><p>We used 5 feature selection methods, i.e., Variance Threshold (VT), k-Best Chi2 (k-χ 2 ), Recursive Feature Elimination (RFE), L 1 based Linear Support Vector Machine (L-SVM-L 1 ), and Information Gain (InfoGain), for comparison. Among the 5 feature selection models, VT are unsupervised, while the other 4 models are supervised. InfoGain is a tree-based feature selection method. The TPR and accuracy of each classifier on the pruned features by each method are listed in each cell of Table <ref type="table" target="#tab_6">6</ref>.</p><p>As shown in Table <ref type="table" target="#tab_6">6</ref>, most classifiers performed best on the features selected by L-SVM-L 1 and InfoGain. Feature selection has positive effects on classifiers that are not capable of learning features' weights automatically because the feature selection models The best TPR and accuracy of each classifier are denoted in bold. All feature selection models use their default parameters. Number in parentheses is the number of selected features for that model.</p><p>eliminated the unrelated information and improved the distance metric. For example, k-NN's performances on the features selected by L-SVM-L 1 and InfoGain were better than the results on the original dataset in Table <ref type="table" target="#tab_5">5</ref>. However, its performances were not as good as the results on the dataset transformed by the DML. The reason is that feature selection cannot guarantee that features' weights are proper for k-NN. By contrast, the DML model proposed in this paper can rotate (aka linear recombination) the features into an advantageous angle and scale the rotated features into the most proper status such that the k-NN's accuracy can be maximized. In addition, the feature selection models do not consider the linear or non-linear interactions between features.</p><p>As for classifiers that have built-in feature weighting mechanisms, such as L-SVM, LDA, Logistic, and neural networks, feature selection does not bring much benefits to them, in terms of accuracy. The results in Table <ref type="table" target="#tab_6">6</ref> showed that the performances of SVM, Logistic, and MLPs on the selected features were not as good as their performances on the features transformed by NYS or DML-NYS (Table <ref type="table" target="#tab_5">5</ref>). The non-linear transformation methods proposed in this paper can bring novel features that are better than the original features selected by the feature selection models.</p><p>Finally, a common merit of the feature selection methods is that the size of the pruned dataset is much smaller than the original one, and this property can facilitate classifiers of high time complexity, such as K-SVM, CNN, and Voted Perceptron. All of these 3 classifiers converged in reasonable time on the features selected by the models in Table <ref type="table" target="#tab_6">6</ref>, while they did not finish in 7 days on datasets transformed by NYS or DML-NYS.</p><p>(3) TPR and accuracy comparison with feature transformation models Feature transformation is another important sub-category of feature engineering. Different from the previous feature selection models, feature transformation models linearly or non-linearly re-generate new features. We selected 6 feature transformation methods (Principal Component Analysis (PCA), Factor Analysis (FA), Independent Component Analysis (ICA), Non-negative Matrix Factorization (NMF), Auto-Encoder (AE), and Variational Auto-Encoder (VAE)) that were frequently used, to compare with the proposed methods. Among these models, NMF was also used by <ref type="bibr" target="#b24">[26]</ref> to improve URL classification, AE was used by <ref type="bibr" target="#b25">[27]</ref> to obtain a reduced dimension vector representation and eliminate noises, and VAE was introduced by <ref type="bibr" target="#b26">[28]</ref> as a representation learning method for similar information security tasks. The TPR and accuracy of each classifier on the transformed datasets by each method are listed in each cell of Table <ref type="table" target="#tab_7">7</ref>.</p><p>As shown in Table <ref type="table" target="#tab_7">7</ref>, k-NN's performances on most datasets transformed by the above feature transformation models were better than its performances on the pruned features selected by the feature selection model. Most classifiers performed best on the dataset transformed by FA, and the performances of most classifiers were slightly improved compared to the original dataset. Notably, none of the classifiers performed well on the dataset transformed by VAE. Although VAE has been proven to be effective on images and intensively studied as a generative deep learning model in recent years, it did not work well on the URLs datasets. VAE forces the hidden variables to follow a normal distribution by adding the Kullback-Leibler divergence to the objective function. The experiments showed that the learned most evident patterns did not discriminate malicious URLs from benign ones. As a result, most classifiers predicted the URLs as benign on the dataset transformed by VAE.</p><p>Although most of those feature engineering models were effective, they did not outperform the proposed features engineering methods, which were designed to solve the problems presented, and particularly useful for Euclidean distance-based and linear models. It can be observed from Table <ref type="table" target="#tab_8">8</ref> that K-SVM using exact kernel method took 6.085E5 s (i.e., about 7 days) to learn the original dataset, while the Nyström method in Table <ref type="table" target="#tab_4">4</ref> only took 377 s to perform transformation. The total time cost was about 540 s (plus the training time of L-SVM, which was 162.23 s). Compared to traditional kernel methods, the time cost of the Nyström method is significantly small. In practice, kernelized SVM can be accelerated by the GPU architecture which reduces the run time. In real industrial applications, we recommend that both exact kernel and the Nyström method should utilize GPU if possible.</p><p>It is also notable that k-NN took more time on testing than training (learns a KDTree). It took 355.6 s on the original dataset and 2,893.1 s on the Nyström transformed dataset in Table <ref type="table" target="#tab_8">8</ref>, because the computation of Euclidean distance on dataset of high dimension involves much more work. In practice, predicting using k-NN should be accelerated with indexing methods or LSH <ref type="bibr" target="#b45">[47]</ref>. Another direction of reducing the time cost is to use big data technologies such as distributed Map-Reduce computation or distributed in-memory processing <ref type="bibr" target="#b51">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Discussion</head><p>(1) Difference between TPR and overall accuracy It can be observed from Table <ref type="table" target="#tab_5">5</ref> that the proposed space transformation methods can improve the performance of classifiers on  the TPR and overall accuracy in general, and the improvement of the overall accuracy was not as notable as the TPR. The main reason is that the number of the malicious sites is much smaller than the benign sites. To further explain this issue, ''Margin curve'' <ref type="bibr" target="#b50">[52]</ref> was introduced in Fig. <ref type="figure" target="#fig_9">9</ref>. The margin is defined as the difference between the probability predicted for the actual class and the highest probability predicted for the other classes <ref type="bibr" target="#b50">[52]</ref>. Fig. <ref type="figure" target="#fig_9">9</ref> was generated based on the classification results of k-NN on the original dataset. It showed that about 80% of the data points are far away from the decision border and are almost impossible to be predicted as wrong classes. It means that only 20% of the data points with different labels are interweaved with each other (This phenomenon can also be seen in Fig. <ref type="figure" target="#fig_3">4</ref>.). We name this area as ''decision area''. The proposed transformation methods have little impact on points that are far away from the decision area. In the detection of malicious URLs, more than 70% sites are benign, which are different from the malicious ones and located far away from the decision area. Therefore, the effectiveness of the proposed models could be observed more evidently in the detection of the malicious data points, i.e., the TPR.</p><p>Besides, the TPR has more information about our interested side of the classification. For example, the TPRs of RBF Network on NYS, NaïveBayes on NYS, and RILB on NYS-DML, were under 6%. But their overall accuracies were still about 71%. This phenomenon indicated that they predicted almost all instances to be benign URLs (Negative), which account for 70% of the whole instances. These three classifiers actually did not have the abilities to detect malicious URLs with the corresponding datasets. We cannot get this knowledge without the TPR.</p><p>(2) Computational complexity analysis Generally speaking, SVD in Section 4.1 requires O(rn 2 ) time, where r is the rank of the dataset. In the LP-based distance metrics learning in Section 4.2, the computational complexity of searching the neighboring points with same and different labels using LSH is O(nd), where d is the number of the hash bins in LSH. The computational complexity of the linear programming in Eq. ( <ref type="formula">9</ref>) completely relies on the solving algorithms. With simplex algorithm in ''lp_solve'', the average time cost to solve Eq. ( <ref type="formula">9</ref>)</p><formula xml:id="formula_28">is O(m 1.5 ) ∼ O(m 2 )</formula><p>, where m is the number of variables <ref type="bibr" target="#b52">[54]</ref>.  Since the DML is based on SVD, so the total time cost of DML is O(rn 2</p><formula xml:id="formula_29">+ nd + (m 1.5 ∼ m 2 )).</formula><p>In Nyström method, the time cost of SOFM is O(kn), where k is the cluster number of SOFM and the target dimension of Nyström method. The time cost of SVD on A 11 + in Eq. ( <ref type="formula" target="#formula_16">12</ref>) is O(r a k 2 ), where r a is the rank of A 11 + <ref type="bibr" target="#b35">[37]</ref>, and the multiplication with the vector in Eq. ( <ref type="formula" target="#formula_19">14</ref>) also takes O(kn). So the total time cost is O(r a k 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ 2kn).</head><p>As for the integrated methods, NYS-DML requires O(r a k (3) Performance comparison of the classifiers using multiple criteria.</p><p>To further compare the performances of the classifiers, we selected the classifiers with good performance in Tables <ref type="table" target="#tab_5">5</ref><ref type="table" target="#tab_6">6</ref><ref type="table" target="#tab_7">7</ref><ref type="table" target="#tab_8">8</ref>as the candidates and compared their performance in terms of F-Measure, Matthews Correlation Coefficient (MCC), and Area Under ROC Curve (AUC) <ref type="bibr" target="#b53">[55]</ref> using the same URL datasets. The indices are list in Table <ref type="table" target="#tab_9">9</ref>.</p><p>There is a trade-off between performance and learning time. k-NN based on DML had the best results in all five accuracy related measures, but its ''testing time cost'' was the highest, which means that k-NN is slow in predicting the labels of the instances and this is fatal for the online systems of URLs detection. The performance of SVM, LDA, and Logistic based on Nyström and DML-NYS were also satisfactory. The selection of appropriate combination of transformation methods and classifiers is actually a multiple-criterion decision making problem and should be chosen according to the specific demands in practice.</p><p>(4) The significance of the proposed feature engineering methods in deep learning era Deep neural networks achieve great improvements in recent years. As the results in Tables 5-7 and 11 showed, with the help of the proposed feature engineering approaches, traditional classifiers (such k-NN and SVM) outperformed some deep learning based methods (MLP-H1, MLP-H3, MLP-H7, CNN, AE, and VAE). Moreover, it is observed in Table <ref type="table" target="#tab_5">5</ref> that the proposed non-linear transformation approaches also have positive impacts on neural networks. The proposed feature engineering methods still make sense in the current deep learning era for the following reasons:</p><p>(a) First, non-linear transformation with proper kernel methods can provide novel features that are not easy for neural networks to learn. Taking MLP for example, there is a dilemma in setting its hidden layer number. Deeper MLP is indeed more capable of fitting complex non-linear patterns, but also susceptible to over-fitting. Shallower MLP is not as versatile as deeper ones and it does not necessarily have the ability to learn the complex patterns that non-linear kernel approximation methods (Eq. ( <ref type="formula" target="#formula_19">14</ref>)) can capture. Therefore, as shown in Table <ref type="table" target="#tab_5">5</ref>, non-linear transformations using the Nyström method (NYS and DML-NYS) also benefit MLPs.</p><p>(b) Second, deep learning techniques are mainly developed for computer vision, audio, graphs, and sequential data, such as texts and time series. These techniques are not necessarily suitable to malicious URLs detection. The experiments in Appendix B showed that malicious URLs detection using URLs' textual contents, which were vectorized by GolVe and BERT, were not as effective as the URL-based approach used in this study. Enhancement of traditional classifiers with feature engineering methods is still needed in numerous data mining domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">System development</head><p>Many jobs in this study, such as feature extraction, space transformation, and fast URLs classification, are supported by an  information system, which contains six main functional modules (as shown Fig. <ref type="figure" target="#fig_10">10</ref>).</p><p>The system was developed using Java EE, with cutting-edge technologies such as distributed cache (Hazelcast), Map-reduce computing (Hazelcast), and NoSQL database (Cassandra). The technical architecture of the system is shown in Fig. <ref type="figure" target="#fig_11">11</ref>. All four classifiers (k-NN, L-SVM, LDA and Logistic) selected in Table <ref type="table" target="#tab_9">9</ref> were used.</p><p>The homepage of the system is shown in Fig. <ref type="figure" target="#fig_12">12</ref>. This page can predict the type of inputted URLs with selected classifier in real-time. Interpretations of the detection, such as the probability distribution of each class, values of the features, instance vector, and parameters of the classifier, are also provided on the page.</p><p>More details about the development of this system are introduced on page ''About''. The online system can be accessed at: http://url.jspfans.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Feature engineering is an important step in malicious URLs detection. This paper used five space transformation models (singular value decomposition, distance metric learning, Nyström methods, DML-NYS, and NYS-DML) to generate new features that disentangle the linear and non-linear interactions between features in malicious URLs data. Each model focuses on one aspect of the space revision and the integrated ones leveraged the merits of nonlinear, linear, unsupervised, and supervised models. The experiments using 331,622 instances of URLs showed that the proposed feature engineering models are effective and can significantly improve the performances of certain classifiers in identifying malicious URLs.</p><p>Detecting malicious URLs needs continuous data collection, feature collecting and extraction, and model training, this paper is the beginning of our effort in identifying malicious URLs. As </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Phishing URLs number reported to APAC from Aug-2011 to May-2016.</figDesc><graphic coords="2,37.15,53.65,240.14,162.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Malicious URLs detection approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Counterfeit objects of phishing websites at different times.</figDesc><graphic coords="5,46.04,217.34,240.14,187.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of high dimensional URLs dataset by t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Correlogram of the original dataset.</figDesc><graphic coords="5,321.39,219.99,231.47,117.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Framework of the proposed methods and processing flow. (a) Keep features orthogonal. (b) Project the data points to a space of high dimension. (c) Reflect good relationship between features and labels. (d) Learn a good distance metric for Radial Basis Function. (e) Perform DML in the transformed space as all kernel tricks do.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Grid search of the parameter γ in RBF.</figDesc><graphic coords="8,305.96,53.45,240.14,141.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Singular values distribution.</figDesc><graphic coords="9,92.28,55.32,420.62,151.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( 4 )</head><label>4</label><figDesc>Time cost evaluation of classifiers Speed is also a key factor in large scale malicious URLs detection. Both ''Training time cost'' and ''Testing time cost'' of each classifier were recorded in Table 8. ''Training time cost'' indicates the time cost in training the classifiers or building the models, and ''Testing time cost'' indicates the time cost in predicting the labels of testing dataset using the classifiers, or in another word, testing the classifiers. ''Testing time cost'' is especially important in practice since users have very low latency tolerance when browsing pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Margin curve of k-NN.</figDesc><graphic coords="13,49.44,442.51,240.14,150.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. System modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Technical architecture of the learning system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Homepage of the system.</figDesc><graphic coords="15,139.78,339.08,325.72,195.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Data sources of the collected URLs dataset.</figDesc><table><row><cell>Source</cell><cell>Number</cell><cell>Type</cell><cell>Total</cell><cell></cell></row><row><cell>PhishTank 360.com</cell><cell>66,154 31,771</cell><cell>Malicious</cell><cell>97,925</cell><cell>331,622</cell></row><row><cell>Hao123 DMOZ</cell><cell>36,406 197,291</cell><cell>Benign</cell><cell>233,697</cell><cell></cell></row></table><note><p><p>2 PhishTank, http://www.phishtank.com.</p>3 360 safety center. http://webscan.360.cn.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Extracted features from the URLs.</figDesc><table><row><cell>Domain-based</cell><cell>Host-based</cell><cell>Reputation-based</cell><cell>Lexical</cell></row><row><cell>features</cell><cell>features</cell><cell>features</cell><cell>features</cell></row><row><cell>7</cell><cell>21</cell><cell>6</cell><cell>28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>6</head><label></label><figDesc>Commons-math, http://commons.apache.org/proper/commons-math.7 Weka, http://www.cs.waikato.ac.nz/ml/weka/.</figDesc><table /><note><p>8 JOptimizer, http://www.joptimizer.com. 9 Ip_Solve, https://sourceforge.net/projects/lpsolve.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Number of features kept by each method.</figDesc><table><row><cell></cell><cell>Original</cell><cell>SVD</cell><cell>SVDR</cell><cell>NYS</cell><cell>NYS-DML</cell><cell>DML-NYS</cell></row><row><cell>Attribute</cell><cell>62</cell><cell>62</cell><cell>25</cell><cell>500</cell><cell>50</cell><cell>500</cell></row><row><cell cols="4">SVDR refers dimension reduction after SVD.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Time cost (seconds) of each space transformation method.</figDesc><table><row><cell></cell><cell>SVD</cell><cell>SVDR</cell><cell>NYS</cell><cell>DML</cell><cell>NYS-DML</cell><cell>DML-NYS</cell></row><row><cell>Time</cell><cell>143</cell><cell>139</cell><cell>377</cell><cell>8,585</cell><cell>11,298</cell><cell>402</cell></row><row><cell>Cumulative time</cell><cell>143</cell><cell>139</cell><cell>377</cell><cell>8,728</cell><cell>11,675</cell><cell>9,130</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>TPR and accuracy of each classifier on the datasets transformed by different methods. It promoted the overall accuracy of k-NN from 88.88% to 93.05%, and K-SVM from 90.91% to 92.34%. k-NN based on DML had the best overall accuracy in the experiments. The DML used in this paper is the integration of unsupervised and supervised methods, namely SVD and LP-DML. It can improve the accuracies of several models because it grasps both the supervised and the unsupervised characteristics of the data.</figDesc><table><row><cell></cell><cell>Original</cell><cell>SVD</cell><cell>SVDR(25)</cell><cell>NYS</cell><cell>DML</cell><cell>NYS-DML</cell><cell>DML-NYS</cell></row><row><cell>k-NN</cell><cell>67.62 ± 0.23 88.88 ± 0.07</cell><cell>67.58 ± 0.22 88.34 ± 0.07</cell><cell>68.03 ± 0.22 88.75 ± 0.05</cell><cell>74.64 ± 0.20 88.81 ± 0.10</cell><cell>86.40 ± 0.20 93.05 ± 0.03</cell><cell>74.88 ± 0.20 89.76 ± 0.06</cell><cell>76.22 ± 0.20 89.20 ± 0.05</cell></row><row><cell>RBF Network</cell><cell>57.59 ± 0.20 86.15 ± 0.06</cell><cell>52.95 ± 1.20 84.00 ± 0.14</cell><cell>67.70 ± 0.40 80.57 ± 0.05</cell><cell>5.28 ± 0.33 71.79 ± 0.22</cell><cell>63.11 ± 1.60 86.82 ± 0.33</cell><cell>81.39 ± 0.34 86.54 ± 0.21</cell><cell>15.64 ± 27.10 72.34 ± 3.24</cell></row><row><cell>RBF Classifier</cell><cell>62.01 ± 0.23 87.02 ± 0.17</cell><cell>62.09 ± 0.22 86.51 ± 0.16</cell><cell>62.03 ± 0.23 86.47 ± 0.16</cell><cell>19.20 ± 0.39 72.86 ± 0.35</cell><cell>69.54 ± 1.28 88.54 ± 0.28</cell><cell>78.36 ± 0.28 85.33 ± 0.24</cell><cell>27.93 ± 7.32 71.82 ± 1.53</cell></row><row><cell>K-SVM</cell><cell>79.41 ± 7.70 90.91 ± 0.40</cell><cell>79.41 ± 7.60 90.91 ± 0.40</cell><cell>79.65 ± 7.60 90.95 ± 0.41</cell><cell>\</cell><cell>84.30 ± 6.20 92.34 ± 0.52</cell><cell>\</cell><cell>\</cell></row><row><cell>L-SVM</cell><cell>58.41 ± 9.60 87.14 ± 0.32</cell><cell>68.10 ± 7.52 88.25 ± 0.30</cell><cell>71.61 ± 7.40 89.39 ± 0.30</cell><cell>79.32 ± 6.80 90.73 ± 0.22</cell><cell>77.87 ± 6.55 90.35 ± 0.28</cell><cell>74.02 ± 9.30 88.29 ± 0.26</cell><cell>80.87 ± 6.50 91.28 ± 0.30</cell></row><row><cell>LDA</cell><cell>62.58 ± 0.51 88.07 ± 0.13</cell><cell>62.58 ± 0.50 88.08 ± 0.13</cell><cell>68.64 ± 0.50 88.41 ± 0.13</cell><cell>80.75 ± 0.40 90.52 ± 0.14</cell><cell>72.97 ± 0.30 84.28 ± 0.11</cell><cell>74.82 ± 0.47 80.58 ± 0.13</cell><cell>82.69 ± 0.60 90.75 ± 0.11</cell></row><row><cell>Logistic</cell><cell>67.80 ± 4.51 88.16 ± 0.83</cell><cell>68.10 ± 0.20 88.25 ± 0.18</cell><cell>71.66 ± 0.18 89.04 ± 0.18</cell><cell>80.89 ± 0.10 91.02 ± 0.28</cell><cell>69.03 ± 3.30 88.45 ± 0.35</cell><cell>78.07 ± 0.36 90.26 ± 0.23</cell><cell>82.14 ± 0.90 91.51 ± 0.32</cell></row><row><cell>MLP-H1</cell><cell>62.53 ± 2.39 87.19 ± 0.04</cell><cell>71.37 ± 5.66 88.27 ± 0.33</cell><cell>72.08 ± 6.41 87.83 ± 0.21</cell><cell>73.25 ± 5.47 88.47 ± 1.17</cell><cell>68.86 ± 7.08 86.33 ± 1.53</cell><cell>71.40 ± 4.73 87.01 ± 0.57</cell><cell>81.85 ± 3.85 89.36 ± 0.35</cell></row><row><cell>MLP-H3</cell><cell>64.28 ± 2.13 87.34 ± 0.09</cell><cell>72.24 ± 5.88 88.39 ± 0.32</cell><cell>72.04 ± 5.85 88.01 ± 0.22</cell><cell>74.09 ± 2.63 88.64 ± 0.85</cell><cell>70.93 ± 5.45 87.02 ± 0.85</cell><cell>73.05 ± 3.92 87.83 ± 0.42</cell><cell>78.39 ± 0.92 90.78 ± 0.17</cell></row><row><cell>MLP-H7</cell><cell>64.75 ± 2.83 87.73 ± 0.52</cell><cell>71.77 ± 5.71 88.44 ± 0.35</cell><cell>72.14 ± 5.99 87.93 ± 0.26</cell><cell>75.77 ± 1.77 88.96 ± 0.53</cell><cell>71.40 ± 6.83 86.62 ± 1.34</cell><cell>73.32 ± 3.61 87.96 ± 0.39</cell><cell>78.96 ± 2.01 90.94 ± 0.16</cell></row><row><cell>CNN</cell><cell>9.10 ± 4.72 72.85 ± 0.16</cell><cell>0.00 ± 0.00 70.24 ± 0.00</cell><cell>0.00 ± 0.00 70.24 ± 0.00</cell><cell>\</cell><cell>0.00 ± 0.00 70.24 ± 0.00</cell><cell>9.12 ± 3.87 71.13 ± 0.15</cell><cell>\</cell></row><row><cell>QDA</cell><cell>58.82 ± 0.47 86.37 ± 0.11</cell><cell>58.37 ± 0.49 87.46 ± 0.20</cell><cell>60.45 ± 0.49 86.38 ± 0.19</cell><cell>58.06 ± 0.69 85.22 ± 0.23</cell><cell>61.94 ± 0.45 87.28 ± 0.15</cell><cell>62.71 ± 0.66 84.36 ± 0.18</cell><cell>59.33 ± 0.68 85.71 ± 0.23</cell></row><row><cell>NaïveBayes</cell><cell>98.41 ± 0.10 41.30 ± 0.05</cell><cell>19.66 ± 0.21 74.19 ± 0.11</cell><cell>92.00 ± 0.20 68.06 ± 0.10</cell><cell>4.93 ± 0.32 71.24 ± 0.09</cell><cell>30.42 ± 0.30 76.75 ± 0.15</cell><cell>50.00 ± 0.28 76.41 ± 0.07</cell><cell>98.12 ± 0.00 40.67 ± 0.08</cell></row><row><cell>Voted Perceptron</cell><cell>48.85 ± 6.90 58.60 ± 3.26</cell><cell>48.93 ± 6.90 58.55 ± 3.26</cell><cell>60.80 ± 7.10 84.68 ± 8.86</cell><cell>\</cell><cell>58.64 ± 6.80 74.27 ± 5.32</cell><cell>55.16 ± 7.20 69.31 ± 4.72</cell><cell>\</cell></row><row><cell>AdaBoost</cell><cell>66.71 ± 0.11 86.62 ± 0.05</cell><cell>73.25 ± 0.34 88.98 ± 0.11</cell><cell>75.41 ± 0.34 89.19 ± 0.12</cell><cell>49.73 ± 1.00 81.95 ± 0.18</cell><cell>70.82 ± 0.31 88.43 ± 0.05</cell><cell>57.03 ± 0.97 83.34 ± 0.07</cell><cell>73.87 ± 0.30 87.61 ± 0.05</cell></row><row><cell>RILB</cell><cell>78.92 ± 2.50 88.28 ± 0.86</cell><cell>79.84 ± 2.51 90.43 ± 1.11</cell><cell>79.59 ± 2.50 90.35 ± 1.12</cell><cell>73.65 ± 0.61 87.50 ± 4.15</cell><cell>79.95 ± 2.40 90.33 ± 0.37</cell><cell>1.30 ± 0.50 70.63 ± 5.32</cell><cell>74.03 ± 2.20 88.21 ± 0.35</cell></row></table><note><p><p>KDTree was used as a searching algorithm in k-NN and the k was set to 3, which is the most frequently used value. L-SVM refers to linear SVM and was implemented in LibLinear. K-SVM refers to kernelized SVM, and was implemented in LibSVM. Radical Basis Function was used as the kernel function, and K-SVM was not applied to any datasets transformed by the Nyström method because the kernel transformation cannot be applied twice. QDA refers to Quadratic Discriminant Analysis. MLP-H1, MLP-H3 and MLP-H7 refer to Multilayer Perceptrons of 1, 3, and 7 hidden layers, respectively. RILB refers to Raced Incremental LogitBoost. CNN refers to Convolutional Neural Networks, and it consists of a convolutional layer of four 3*1 filter, a max-pooling layer, two dense layers of 16 neurons, and a softmax output layer.</p>L-SVM, Naïve Bayes, MLP, and AdaBoost. DML can learn better distance metric for the distance function in k-NN and radial basis function in RBF Network, RBF Classifier and K-SVM.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>TPR and accuracy of each classifier on a pruned set of features using five feature selection methods.</figDesc><table><row><cell></cell><cell>VT (7)</cell><cell>k-χ 2 (15)</cell><cell>RFE (15)</cell><cell>L-SVM-L 1 (24)</cell><cell>InfoGain (11)</cell></row><row><cell>k-NN</cell><cell>56.93 ± 0.35 84.68 ± 0.17</cell><cell>62.81 ± 0.19 86.54 ± 0.12</cell><cell>52.27 ± 0.52 85.36 ± 0.12</cell><cell>71.81 ± 0.38 89.04 ± 0.16</cell><cell>76.62 ± 0.74 89.09 ± 0.08</cell></row><row><cell>RBF Network</cell><cell>51.13 ± 1.66 84.37 ± 0.22</cell><cell>55.11 ± 0.42 85.30 ± 0.19</cell><cell>52.63 ± 0.18 85.26 ± 0.16</cell><cell>57.39 ± 0.33 86.03 ± 0.16</cell><cell>63.91 ± 0.59 85.16 ± 0.54</cell></row><row><cell>RBF Classifier</cell><cell>56.97 ± 0.38 84.69 ± 0.18</cell><cell>54.62 ± 0.16 85.67 ± 0.09</cell><cell>52.91 ± 0.34 85.24 ± 0.16</cell><cell>61.61 ± 0.28 86.95 ± 0.19</cell><cell>56.69 ± 1.19 86.14 ± 0.22</cell></row><row><cell>K-SVM</cell><cell>56.95 ± 0.35 84.68 ± 0.17</cell><cell>57.39 ± 0.24 86.24 ± 0.13</cell><cell>52.11 ± 0.32 85.34 ± 0.10</cell><cell>69.08 ± 0.38 88.53 ± 0.11</cell><cell>70.36 ± 0.78 89.07 ± 0.24</cell></row><row><cell>L-SVM</cell><cell>49.99 ± 0.74 84.24 ± 0.26</cell><cell>55.01 ± 0.17 85.71 ± 0.16</cell><cell>52.11 ± 0.32 85.33 ± 0.11</cell><cell>59.48 ± 0.26 87.06 ± 0.06</cell><cell>56.51 ± 0.37 86.12 ± 0.16</cell></row><row><cell>LDA</cell><cell>49.99 ± 0.74 84.24 ± 0.25</cell><cell>53.00 ± 0.48 85.28 ± 0.20</cell><cell>52.11 ± 0.32 85.31 ± 011</cell><cell>56.21 ± 0.42 86.28 ± 0.14</cell><cell>57.91 ± 0.55 86.57 ± 0.25</cell></row><row><cell>Logistic</cell><cell>53.06 ± 0.40 84.57 ± 0.14</cell><cell>56.85 ± 0.41 85.96 ± 0.08</cell><cell>52.11 ± 0.32 85.41 ± 0.11</cell><cell>63.76 ± 0.68 87.64 ± 0.20</cell><cell>72.47 ± 1.08 87.90 ± 0.44</cell></row><row><cell>MLP-H1</cell><cell>57.32 ± 3.54 84.50 ± 0.30</cell><cell>59.52 ± 5.18 85.25 ± 1.11</cell><cell>52.28 ± 0.49 85.25 ± 0.15</cell><cell>63.29 ± 2.26 87.32 ± 0.16</cell><cell>62.43 ± 0.80 87.14 ± 0.18</cell></row><row><cell>MLP-H3</cell><cell>55.63 ± 1.57 84.64 ± 0.11</cell><cell>60.06 ± 5.40 85.65 ± 0.79</cell><cell>52.28 ± 0.49 85.25 ± 0.15</cell><cell>65.05 ± 2.60 87.53 ± 0.33</cell><cell>68.80 ± 4.94 87.95 ± 0.24</cell></row><row><cell>MLP-H7</cell><cell>55.63 ± 1.57 84.64 ± 0.11</cell><cell>59.93 ± 5.26 85.53 ± 0.86</cell><cell>52.26 ± 0.47 85.24 ± 0.15</cell><cell>65.95 ± 3.55 87.85 ± 0.14</cell><cell>70.16 ± 5.49 88.27 ± 0.35</cell></row><row><cell>CNN</cell><cell>15.64 ± 1.68 74.59 ± 0.42</cell><cell>31.57 ± 2.37 79.29 ± 0.83</cell><cell>31.00 ± 1.15 79.37 ± 0.46</cell><cell>9.19 ± 0.60 72.85 ± 0.23</cell><cell>38.14 ± 1.27 81.12 ± 0.54</cell></row><row><cell>QDA</cell><cell>49.99 ± 0.74 ± 84.24 ± 0.26</cell><cell>55.81 ± 0.29 85.07 ± 0.19</cell><cell>52.91 ± 0.34 85.27 ± 0.15</cell><cell>58.03 ± 0.04 85.69 ± 0.16</cell><cell>63.34 ± 0.49 85.89 ± 0.15</cell></row><row><cell>NaïveBayes</cell><cell>49.99 ± 0.74 84.24 ± 0.26</cell><cell>99.57 ± 0.01 41.52 ± 0.26</cell><cell>100 ± 0.00 35.43 ± 0.20</cell><cell>98.59 ± 0.22 42.75 ± 0.25</cell><cell>97.87 ± 1.86 51.10 ± 7.61</cell></row><row><cell>Voted Perceptron</cell><cell>53.70 ± 3.29 84.44 ± 0.29</cell><cell>53.65 ± 0.39 85.44 ± 0.21</cell><cell>52.33 ± 0.73 85.44 ± 0.13</cell><cell>60.69 ± 0.10 87.03 ± 0.19</cell><cell>57.21 ± 0.88 86.17 ± 0.28</cell></row><row><cell>AdaBoost</cell><cell>48.89 ± 0.58 84.08 ± 0.23</cell><cell>67.40 ± 0.30 85.49 ± 0.16</cell><cell>48.86 ± 0.58 84.26 ± 0.20</cell><cell>67.40 ± 0.30 85.49 ± 0.16</cell><cell>58.18 ± 1.57 86.73 ± 0.42</cell></row><row><cell>RILB</cell><cell>54.52 ± 3.78 84.49 ± 0.35</cell><cell>63.23 ± 6.71 85.99 ± 0.19</cell><cell>52.24 ± 0.49 85.38 ± 0.11</cell><cell>67.57 ± 2.40 87.77 ± 0.54</cell><cell>75.98 ± 4.00 88.77 ± 0.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>TPR and accuracy of each classifier on the transformed features using six feature transformation methods.</figDesc><table><row><cell></cell><cell>PCA</cell><cell>FA</cell><cell>ICA</cell><cell>NMF</cell><cell>AE</cell><cell>VAE</cell></row><row><cell>k-NN</cell><cell>76.75 ± 0.44 89.82 ± 0.12</cell><cell>78.01 ± 0.47 90.29 ± 0.07</cell><cell>76.24 ± 0.45 89.67 ± 0.17</cell><cell>76.77 ± 0.48 89.83 ± 0.05</cell><cell>77.16 ± 0.31 90.06 ± 0.12</cell><cell>21.97 ± 0.29 61.84 ± 0.16</cell></row><row><cell>RBFNetwork</cell><cell>63.38 ± 2.23 85.27 ± 0.29</cell><cell>57.28 ± 3.69 82.59 ± 0.85</cell><cell>47.46 ± 5.51 83.03 ± 1.26</cell><cell>54.63 ± 1.80 84.91 ± 0.17</cell><cell>65.42 ± 1.37 79.80 ± 1.09</cell><cell>0.00 ± 0.00 70.11 ± 0.00</cell></row><row><cell>RBFClassifier</cell><cell>59.50 ± 0.25 86.21 ± 0.20</cell><cell>65.90 ± 0.87 87.84 ± 0.11</cell><cell>59.64 ± 0.49 86.26 ± 0.19</cell><cell>60.24 ± 0.56 86.20 ± 0.31</cell><cell>56.82 ± 0.44 84.51 ± 0.26</cell><cell>0.00 ± 0.00 70.11 ± 0.00</cell></row><row><cell>K-SVM</cell><cell>65.61 ± 0.26 87.94 ± 0.12</cell><cell>81.88 ± 0.36 87.72 ± 0.14</cell><cell>56.80 ± 0.31 85.92 ± 0.11</cell><cell>60.29 ± 0.30 86.75 ± 0.10</cell><cell>60.75 ± 0.07 86.84 ± 0.09</cell><cell>0.00 ± 0.00 70.11 ± 0.00</cell></row><row><cell>L-SVM</cell><cell>55.23 ± 0.58 85.72 ± 0.24</cell><cell>63.97 ± 0.27 87.37 ± 0.21</cell><cell>50.32 ± 0.46 84.80 ± 0.17</cell><cell>54.30 ± 0.51 85.50 ± 0.19</cell><cell>52.16 ± 0.50 83.78 ± 0.20</cell><cell>0.60 ± 0.20 70.10 ± 0.05</cell></row><row><cell>LDA</cell><cell>54.04 ± 0.62 85.53 ± 0.24</cell><cell>58.83 ± 0.75 86.49 ± 0.25</cell><cell>53.65 ± 0.56 85.49 ± 0.22</cell><cell>54.38 ± 0.60 85.50 ± 0.22</cell><cell>57.08 ± 0.22 84.36 ± 0.08</cell><cell>15.26 ± 0.23 70.03 ± 0.13</cell></row><row><cell>Logistic</cell><cell>58.85 ± 0.41 86.16 ± 0.13</cell><cell>66.47 ± 0.57 87.60 ± 0.28</cell><cell>58.85 ± 0.41 86.16 ± 0.13</cell><cell>58.65 ± 0.44 86.10 ± 0.20</cell><cell>57.96 ± 0.46 85.56 ± 0.07</cell><cell>1.27 ± 0.19 70.09 ± 0.09</cell></row><row><cell>MLP-H1</cell><cell>60.19 ± 2.00 86.20 ± 0.30</cell><cell>70.14 ± 2.17 88.30 ± 0.33</cell><cell>60.10 ± 2.39 86.15 ± 0.34</cell><cell>61.42 ± 1.68 86.36 ± 0.35</cell><cell>58.29 ± 3.22 84.86 ± 0.17</cell><cell>0.00 ± 0.00 70.11 ± 0.00</cell></row><row><cell>MLP-H3</cell><cell>61.19 ± 3.02 86.14 ± 0.48</cell><cell>70.69 ± 2.57 88.40 ± 0.08</cell><cell>61.42 ± 2.74 86.41 ± 0.59</cell><cell>61.70 ± 2.91 86.33 ± 0.47</cell><cell>60.61 ± 0.71 85.42 ± 0.11</cell><cell>0.00 ± 0.00 70.11 ± 0.00</cell></row><row><cell>MLP-H7</cell><cell>61.71 ± 2.82 86.46 ± 0.40</cell><cell>70.72 ± 2.65 88.52 ± 0.30</cell><cell>62.23 ± 2.59 86.35 ± 0.58</cell><cell>62.45 ± 2.91 86.48 ± 0.43</cell><cell>59.96 ± 2.00 86.06 ± 0.30</cell><cell>0.00 ± 0.00 70.11 ± 0.01</cell></row><row><cell>CNN</cell><cell>59.40 ± 4.33 84.08 ± 1.60</cell><cell>58.82 ± 4.56 85.09 ± 0.13</cell><cell>52.52 ± 5.32 82.79 ± 0.29</cell><cell>32.64 ± 4.77 77.63 ± 0.18</cell><cell>57.70 ± 4.36 81.87 ± 2.12</cell><cell>0.00 ± 0.00 70.11 ± 0.00</cell></row><row><cell>QDA</cell><cell>55.01 ± 0.57 85.43 ± 0.15</cell><cell>65.02 ± 0.25 85.41 ± 0.22</cell><cell>54.32 ± 0.62 85.36 ± 0.16</cell><cell>54.48 ± 0.48 85.36 ± 0.19</cell><cell>59.66 ± 0.33 84.57 ± 0.17</cell><cell>0.59 ±± 0.25 69.83 ± 0.08</cell></row><row><cell>NaïveBayes</cell><cell>64.92 ± 0.83 85.11 ± 0.27</cell><cell>49.82 ± 2.69 80.76 ± 0.69</cell><cell>54.53 ± 0.78 85.11 ± 0.05</cell><cell>28.27 ± 0.70 77.66 ± 0.28</cell><cell>64.83 ± 0.34 80.12 ± 0.23</cell><cell>15.93 ± 0.61 67.51 ± 0.32</cell></row><row><cell>Voted Perceptron</cell><cell>58.47 ± 1.18 85.77 ± 0.16</cell><cell>69.70 ± 0.79 87.29 ± 0.22</cell><cell>61.04 ± 0.43 85.71 ± 0.29</cell><cell>56.59 ± 0.11 85.53 ± 0.21</cell><cell>45.74 ± 0.35 82.00 ± 0.14</cell><cell>0.67 ± 0.51 70.07 ± 0.05</cell></row><row><cell>AdaBoost</cell><cell>51.57 ± 0.57 84.67 ± 0.24</cell><cell>78.47 ± 0.85 83.99 ± 0.24</cell><cell>63.39 ± 0.15 83.28 ± 1.07</cell><cell>57.00 ± 4.89 85.23 ± 0.81</cell><cell>52.76 ± 0.87 83.44 ± 0.80</cell><cell>0.00 ± 0.00 70.11 ± 0.00</cell></row><row><cell>RILB</cell><cell>63.60 ± 3.62 85.94 ± 0.38</cell><cell>71.20 ± 3.04 86.69 ± 0.57</cell><cell>62.31 ± 3.58 85.84 ± 0.21</cell><cell>71.49 ± 2.07 87.71 ± 0.34</cell><cell>58.65 ± 2.35 84.44 ± 0.30</cell><cell>0.12 ± 0.13 70.05 ± 0.05</cell></row><row><cell cols="7">The best performances are denoted in bold; The component number of PCA was set to 15 and determined by keeping 95%</cell></row><row><cell cols="7">of the variances; the component number of FA, ICA, and NMF were the same as PCA; AE had 3 layers in both encoder and</cell></row><row><cell cols="7">decoder, and the first 5 layers used ''ReLU'' activation while the last layer used ''Tanh'' activation; VAE had 2 layers in both</cell></row><row><cell>encoder and decoder.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Training and testing time cost (seconds) of each classifier on different datasets.</figDesc><table><row><cell></cell><cell>Original</cell><cell>SVD</cell><cell>SVDR(25)</cell><cell>NYS</cell><cell>DML</cell><cell>NYS-DML</cell><cell>DML-NYS</cell></row><row><cell>k-NN</cell><cell>30.2 355.6</cell><cell>28.5 362.6</cell><cell>26.3 323.4</cell><cell>173.5 2,893.1</cell><cell>21.2 362.3</cell><cell>38.4 368.3</cell><cell>164.6 2,325.1</cell></row><row><cell>RBF Network</cell><cell>40.7 1.2</cell><cell>43.1 1.4</cell><cell>38.6 1.3</cell><cell>140.6 10.1</cell><cell>13.2 1.7</cell><cell>36.6 3.3</cell><cell>112.6 9.8</cell></row><row><cell>RBF Classifier</cell><cell>140.0 1.2</cell><cell>153.2 1.5</cell><cell>159.7 1.1</cell><cell>552.8 8.3</cell><cell>60.1 1.4</cell><cell>128.4 1.9</cell><cell>502.7 7.6</cell></row><row><cell>K-SVM</cell><cell>6.085E5 1.6</cell><cell>6.434E59 1.6</cell><cell>5.872E5 1.3</cell><cell>\</cell><cell>6.371E5 1.3</cell><cell>\</cell><cell>\</cell></row><row><cell>L-SVM</cell><cell>31.3 0.7</cell><cell>23.0 1.0</cell><cell>22.5 0.8</cell><cell>162.2 1.2</cell><cell>68.9 1.2</cell><cell>34.2 0.9</cell><cell>232.4 1.3</cell></row><row><cell>LDA</cell><cell>12.8 1.5</cell><cell>11.9 1.6</cell><cell>10.5 1.5</cell><cell>895.4 68.4</cell><cell>11.3 1.5</cell><cell>13.8 1.4</cell><cell>860.2 71.0</cell></row><row><cell>Logistic</cell><cell>56.3 0.4</cell><cell>59.7 0.3</cell><cell>46.6 0.0</cell><cell>667.2 0.9</cell><cell>46.2 0.2</cell><cell>56.7 0.4</cell><cell>675.7 0.8</cell></row><row><cell>MLP-H1</cell><cell>118.4 2.5</cell><cell>95.0 2.5</cell><cell>240.2 2.4</cell><cell>9,453.6 17.6</cell><cell>568.1 2.4</cell><cell>450.7 2.7</cell><cell>46,935.1 196.1</cell></row><row><cell>MLP-H3</cell><cell>244.6 2.6</cell><cell>223.2 2.2</cell><cell>135.3 2.7</cell><cell>1,823.6 15.9</cell><cell>217.3 3.1</cell><cell>210.6 2.2</cell><cell>2,078.0 2.8</cell></row><row><cell>MLP-H7</cell><cell>502.3 3.1</cell><cell>499.8 4.6</cell><cell>251.9 1.5</cell><cell>3,489.1 18.7</cell><cell>496.3 1.5</cell><cell>375.9 1.5</cell><cell>3,448.6 15.6</cell></row><row><cell>CNN</cell><cell>16,502.0 3.2</cell><cell>19,402.7 4.1</cell><cell>14,532.8 4.1</cell><cell>\</cell><cell>18,587.9 3.8</cell><cell>24,043.7 4.2</cell><cell>\</cell></row><row><cell>QDA</cell><cell>19.6 1.3</cell><cell>21.8 1.4</cell><cell>19.3 1.3</cell><cell>1,538.1 66.2</cell><cell>19.8 1.5</cell><cell>20.1 1.2</cell><cell>1,566.4 63.2</cell></row><row><cell>NaïveBayes</cell><cell>2.6 2.8</cell><cell>2.7 2.8</cell><cell>21.6 2.6</cell><cell>47.5 33.5</cell><cell>2.6 3.6</cell><cell>3.3 2.4</cell><cell>40.1 35.7</cell></row><row><cell>Voted Perceptron</cell><cell>58.3 1.5</cell><cell>54.2 1.6</cell><cell>42.8 1.3</cell><cell>\</cell><cell>60.3 1.67</cell><cell>55.95 1.55</cell><cell>\</cell></row><row><cell>AdaBoost</cell><cell>69.3 0.3</cell><cell>50.3 0.3</cell><cell>45.4 0.2</cell><cell>836.2 0.4</cell><cell>80.8 0.3</cell><cell>77.6 0.3</cell><cell>842.5 0.4</cell></row><row><cell>RILB</cell><cell>19.6 0.3</cell><cell>20.3 0.3</cell><cell>18.2 0.2</cell><cell>119.1 0.5</cell><cell>12.2 0.3</cell><cell>20.9 0.3</cell><cell>105.5 0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc>Results comparison using multiple measurements.</figDesc><table><row><cell></cell><cell>Training</cell><cell>Testing</cell><cell>F-measure</cell><cell>MCC</cell><cell>AUC</cell><cell>Accuracy (%)</cell><cell>TPR (%)</cell></row><row><cell></cell><cell>time cost</cell><cell>time cost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k-NN(DML)</cell><cell>21.2 s</cell><cell>362.3 s</cell><cell>0.930</cell><cell>0.831</cell><cell>0.974</cell><cell>93.05 ± 0.03</cell><cell>86.40 ± 0.20</cell></row><row><cell>L-SVM(Nys)</cell><cell>162.2 s</cell><cell>1.2 s</cell><cell>0.906</cell><cell>0.773</cell><cell>0.874</cell><cell>90.73 ± 0.22</cell><cell>79.32 ± 6.80</cell></row><row><cell>LDA(Nys)</cell><cell>895.4 s</cell><cell>68.4 s</cell><cell>0.904</cell><cell>0.769</cell><cell>0.935</cell><cell>90.52 ± 0.14</cell><cell>80.75 ± 0.40</cell></row><row><cell>Logistic(Nys)</cell><cell>667.2 s</cell><cell>0.9 s</cell><cell>0.909</cell><cell>0.780</cell><cell>0.949</cell><cell>91.02 ± 0.28</cell><cell>80.89 ± 0.10</cell></row><row><cell>SVM(DML-Nys)</cell><cell>232.4 s</cell><cell>1.3 s</cell><cell>0.912</cell><cell>0.787</cell><cell>0.883</cell><cell>91.28 ± 0.30</cell><cell>80.87 ± 6.50</cell></row><row><cell>LDA(DML-Nys)</cell><cell>860.2 s</cell><cell>71.0 s</cell><cell>0.907</cell><cell>0.776</cell><cell>0.955</cell><cell>90.75 ± 0.11</cell><cell>82.69 ± 0.60</cell></row><row><cell>Logistic(DML-Nys)</cell><cell>675.7 s</cell><cell>0.8 s</cell><cell>0.914</cell><cell>0.793</cell><cell>0.962</cell><cell>91.51 ± 0.32</cell><cell>82.14 ± 0.90</cell></row><row><cell cols="4">The top three best performers were bold lined in each column.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>+ nd + (m 1.5 ∼ m 2 ) + r a-dml k dml 2 + 2k dml n) time,where the subscript dml indicates that they are based on the dataset transformed by DML.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>+</cell></row><row><cell>2kn + r nys n 2</cell><cell>+ d nys n + (m nys</cell><cell>1.5</cell><cell>∼ m nys</cell><cell cols="2">2 )) time, where the subscript</cell></row><row><cell cols="6">nys indicates that they are based on the dataset transformed by</cell></row><row><cell cols="6">Nyström method, rather than the original one. DML-NYS requires</cell></row><row><cell>O(rn 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11</head><label>11</label><figDesc>Textual classification results. 20.94 ± 0.30 98.31 ± 0.00 23.97 ± 1.38 97.80 ± 0.02 RBF Network 0.00 ± 0.00 98.23 ± 0.00 0.00 ± 0.00 97.54 ± 0.00 RBF Classifier 0.00 ± 0.00 98.36 ± 0.00 0.00 ± 0.00 97.54 ± 0.14 ± 0.01 18.90 ± 8.64 97.78 ± 0.03 LDA 15.83 ± 0.49 97.84 ± 0.06 42.43 ± 2.18 96.89 ± 0.09 Logistic 3.58 ± 0.42 98.32 ± 0.01 27.10 ± 2.21 97.72 ± 0.06 MLP-H1 0.00 ± 0.00 98.36 ± 0.01 30.78 ± 3.26 97.80 ± 0.14 MLP-H3 9.71 ± 1.41 98.36 ± 3.31 32.46 ± 3.32 97.69 ± 0.17 MLP-H7 10.06 ± 1.18 98.38 ± 0.03 33.08 ± 2.09 97.73 ± 0.10 ± 1.33 95.67 ± 0.32 29.60 ± 0.57 96.77 ± 0.05 NaïveBayes 58.11 ± 0.86 74.64 ± 0.08 78.56 ± 2.48 36.69 ± 0.05 ± 2.90 97.83 ± 0.31 34.84 ± 3.95 97.37 ± 0.08 The best TPR and accuracy of the classifiers are denoted in bold; K-SVM and CNN cannot find solutions in 7 days; Voted Perceptron ran out of memory on the RoBERT dataset. of QR codes in almost every aspect of life in China, safety is a top concern for QR code payments. Another future research direction is to study the unique features of URLs encoded in graphical QR codes, and improve the detection rate in QR codes frauds.</figDesc><table><row><cell></cell><cell>GloVe</cell><cell></cell><cell>RoBERT</cell><cell></cell></row><row><cell></cell><cell>TPR</cell><cell>Accuracy</cell><cell>TPR</cell><cell>Accuracy</cell></row><row><cell>k-NN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>K-SVM</cell><cell>\</cell><cell>\</cell><cell>\</cell><cell>\</cell></row><row><cell cols="3">L-SVM 98.35 CNN 0.24 ± 0.04 \ \</cell><cell>\</cell><cell>\</cell></row><row><cell cols="2">QDA 35.70 Voted 3.55 ± 0.37</cell><cell cols="2">98.26 ± 2.01 \</cell><cell>\</cell></row><row><cell>Perceptron</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AdaBoost</cell><cell>0.00 ± 0.00</cell><cell cols="2">98.34 ± 0.00 0.00 ± 0.00</cell><cell>97.54 ± 0.00</cell></row><row><cell>RILB</cell><cell>29.58</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Hao123, http://www.hao123.com.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>DMOZ. http://www.dmoz.org/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewer for the valuable suggestions and comments on this work. This work was supported in part by grants from the National Natural Science Foundation of China (#U1811462, #71771037, #71725001, #71971042, #71910107002), and China Postdoctoral Science Foundation (#2019M653388).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 10</head><p>The description of all 62 features used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Feature Description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain domain_length</head><p>The length of the domain. server_name_length</p><p>The length of the ''server name'' section. pure_domain_length</p><p>The length of the pure domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of competing interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Feature list</head><p>All features used in the paper are listed and descripted in Table <ref type="table">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Comparison with content-based methods</head><p>As discussed in Section 2, another approach to identify malicious websites is analyzing the contents of the pages. According to the reviewer's suggestions (after 2 years), we tried to re-fetch the textual contents of all the URLs to evaluate this approach and compare the results with our URL based approach. Unfortunately, many URLs, especially the malicious ones, are not accessible anymore. We grabbed 102,645 valid pages of the URLs, among which only 2,524 were malicious ones. We vectorized the textual contents with two widely used pre-trained deep language neural network models, i.e., GolVe and RoBERT (bert-base-multilingualuncased), and the classification results of the 16 classifiers are summarized in Table <ref type="table">11</ref>.</p><p>As shown in Table <ref type="table">11</ref>, most classifiers had very poor TPR. Though Naïve Bayes had higher TPR, it had the lowest accuracy, which indicates that it had a very high false alarm rate.</p><p>The highest TPR of the classifiers on the two textual content datasets was 42.43% except NaïveBayes, while the highest TPR on the URL-based dataset in Table <ref type="table">5</ref> was 86.40%. The malicious URLs detection rate of content-based approach was substantially lower than the URL-based approach used in this study. These results show that it is very hard, if not impossible, to discriminate malicious websites from benign ones only based on their contents.</p><p>Although transformer-based deep neural networks, such as Bidirectional Encoder Representations from Transformers (BERT) and XLNet, made great progress in recent years, and have been proven to be very effective on numerous textual mining tasks, they do not necessarily apply well to malicious websites detection. As discussed in Section 2, most malicious web pages do not leave clues in their textual contents. In fact, many malicious websites even have counter measures to escape censorship, such as showing different contents based on the client IPs and browser cookies. Therefore, URLs' background information, such as DNS, domains and search engines, are more reliable for malicious websites detection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The social relation key: A new paradigm for security</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The dark side of the internet: Attacks, costs and responses</title>
		<author>
			<persName><forename type="first">Won</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ok-Ran</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chulyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungmin</forename><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="675" to="705" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting fake websites: The contribution of statistical learning theory</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">F</forename><surname>Nunamaker</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zimbra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsinchun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIS Quart</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="435" to="461" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying suspicious URLs: An application of large-scale online learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;S 09</title>
		<meeting><address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Kaggle</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/%7Caccessdate=November" />
		<imprint>
			<date type="published" when="2015-11-08">2015. 2015. Nov. 8, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A few useful things to know about machine learning</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning feature engineering for classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nargesian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samulowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Application of feature engineering for phishing detection</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingshan</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst. E</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1062" to="1070" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An assessment of features related to phishing websites using an automated technique</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thabtah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mccluskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Internet Technology and Secured Transactions</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale automatic classification of phishing pages</title>
		<author>
			<persName><forename type="first">C</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ryner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nazif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NDSS</title>
		<meeting>of NDSS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Google safe browsing API</title>
		<ptr target="http://code.google.com/apis/safebrowsing/" />
		<imprint>
			<date type="published" when="2015-09-12">2015. Accessed Sep. 12, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<title level="m">Beyond blacklists: Learning to detect malicious web sites from suspicious URLs, in: KDD&apos;09</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Execution-based detection of malicious web content</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moshchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bragin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Spyproxy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the USENIX Security Symposium</title>
		<meeting>of the USENIX Security Symposium<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CANTINA: A content-based approach to detecting phishing web sites</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cranor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference (WWW)</title>
		<meeting>the International World Wide Web Conference (WWW)<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Websites classification using web page layout weighting scheme and multi-class support vector machine</title>
		<author>
			<persName><forename type="first">Kailiang</forename><surname>Ko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Why spoofing is serious internet fraud</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dinev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="76" to="82" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intelligent rule based phishing websites classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thabtah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mccluskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Inf. Secur</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="160" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to detect phishing emails</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference (WWW)</title>
		<meeting>the International World Wide Web Conference (WWW)<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A framework for detection and measurement of phishing attacks</title>
		<author>
			<persName><forename type="first">Sujata</forename><surname>Garera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Provos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aviel</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WORM&apos;07: Proceedings of the 2007 ACM workshop on Recurring malcode</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards a programmable semantic extract-transform-load framework for semantic data warehouses</title>
		<author>
			<persName><forename type="first">Rudra</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torben</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="17" to="43" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable subgraph selection with label similarity for big data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dhifli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aridhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mephu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Engelbert</forename><surname>Mr-Simlab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detection of URL based attacks using reduced feature set and modified c4.5 algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Muthurajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Nat. Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A domain-feature enhanced classification model for the detection of chinese phishing e-Business websites</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Manag</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="845" to="853" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Boosting decision stumps for dynamic feature selection on data streams</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Barddal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Enembreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Url classification using non negative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twentieth National Conference on Communications (NCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep learning approach for detecting malicious JavaScript code</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Secur. Commun. Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1520" to="1534" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Menkovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04332</idno>
		<title level="m">Deep learning in information security</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AutoCross: Automatic feature crossing for tabular data in real-world applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuanfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mengshuo</surname></persName>
		</author>
		<author>
			<persName><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multiple-criteria quadratic programming approach to network intrusion detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Academy of Sciences Symposium on Data Mining and Knowledge Management</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-stage optimization for periodic inspection planning of geo-distributed infrastructure systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dzung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">245</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="797" to="804" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Statistical Learning Method</title>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>Tsinghua University Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Classifying with adaptive hyper-spheres: an incremental classifier based on competitive learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMC.2017.2761360</idno>
		<ptr target="http://dx.doi.org/10.1109/TSMC.2017.2761360" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern.: Syst</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning applications and challenges in big data analytics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Najafabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villanustre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Muharemagic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="505" to="512" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incremental support vector learning: analysis implementation and applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1909" to="1936" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large scale online kernel learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Information weighted sampling for detecting rare items in finite populations with a focus on security</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoogstrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">A J</forename><surname>Klaassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="880" to="887" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Kabacoff</surname></persName>
		</author>
		<title level="m">R in Action: Data Analysis and Graphics with R, Manning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">286</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">189</biblScope>
			<biblScope unit="page" from="9" to="1907" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale convex minimization with a low-rank constraint</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalevshwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Projection approximation subspace tracking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalized power method for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Journée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepulchre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="517" to="553" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A boosting approach for supervised mahalanobis distance metric learning</title>
		<author>
			<persName><forename type="first">Chin-Chun</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="844" to="862" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName><forename type="first">Alexandr</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="122" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A generic method for accelerating LSH-based similarity join processing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="712" to="726" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dynamics from noisy data with extreme timing uncertainty</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vendrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramakrishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">532</biblScope>
			<biblScope unit="page" from="471" to="475" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sampling methods for the Nyström method</title>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="981" to="1006" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ARTMAP: Supervised real-time learning and classification of nonstationary data by a self-organizing neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="5" to="588" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spam filtering: how the dimensionality reduction affects the accuracy of naive Bayes classifiers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Internet Serv. Appl</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distributed in-memory processing of all k nearest neighbor queries</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chatzimilioudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeinalipour-Yazti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pitoura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="925" to="938" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>no.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Borgwardt</surname></persName>
		</author>
		<title level="m">The Simplex Method: A Probabilistic Analysis</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Evaluation: From precision, recall and f-measure to roc, informedness, markedness &amp; correlation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
