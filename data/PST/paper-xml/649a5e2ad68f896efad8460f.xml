<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers</title>
				<funder ref="#_ceEU6fG">
					<orgName type="full">IIS</orgName>
				</funder>
				<funder ref="#_CXHEVKf">
					<orgName type="full">INCAS</orgName>
				</funder>
				<funder ref="#_bT9AkXk #_YsVGFf7 #_BhUk6s2 #_Ufp9rAH">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_ywWFbVs">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-24">24 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bowen</forename><surname>Jin</surname></persName>
							<email>bowenj4@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
							<email>xchen@cs.ucla.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yanzhen</forename><surname>Shen</surname></persName>
							<email>yanzhen4@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<email>yumeng5@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><forename type="middle">2023</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Supervised</forename><surname>Weakly</surname></persName>
						</author>
						<author>
							<persName><surname>Multi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">KDD &apos;23</orgName>
								<address>
									<addrLine>August 6-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Label Classification of Full-Text Scientific Papers. In</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">KDD &apos;23</orgName>
								<orgName type="laboratory">P@k Maximum # Tokens used by Longformer</orgName>
								<orgName type="institution">Yu</orgName>
								<address>
									<addrLine>August 6-10, Zhang et al. 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0, P@1 P@3 P@5</addrLine>
									<postCode>2023, 1024 2048, 3072 4096</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="laboratory">P@k Maximum # Tokens used by Longformer</orgName>
								<address>
									<addrLine>35 0.4 0.45 0, P@1 P@3 P@5</addrLine>
									<postCode>1024 2048, 3072 4096</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-24">24 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3580305.3599544</idno>
					<idno type="arXiv">arXiv:2306.14003v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-label text classification</term>
					<term>weak supervision</term>
					<term>scientific paper</term>
					<term>full text</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and</p><p>(2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FuTex, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to classify full-text scientific papers under weak supervision. A network-aware contrastive fine-tuning module and a hierarchyaware aggregation module are designed to leverage the two types of structural signals, respectively. Experiments on two benchmark datasets demonstrate that FuTex significantly outperforms competitive baselines and is on par with fully supervised classifiers that use 1,000 to 60,000 ground-truth training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Information systems ? Data mining; ? Computing methodologies ? Classification and regression trees.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Weakly supervised text classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50]</ref> aims to classify text documents into a set of pre-defined categories without relying on any human-labeled training documents. Instead, the classifier seeks help from various formats of weak supervision such as category names <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50]</ref>, a few category-indicative keywords <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">70]</ref>, and category descriptions <ref type="bibr" target="#b68">[69]</ref>. This setting significantly alleviates the burden of manual annotations, which is particularly helpful in some real applications such as scientific paper classification, where annotations need to be acquired from domain experts.</p><p>Although existing studies on weakly supervised text classification have applied their proposed methods to scientific paper datasets such as arXiv <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b63">64]</ref> and DBLP <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b65">66]</ref>, they are less concerned with the following two challenges in practice.</p><p>A Large and Fine-Grained Label Space. One major goal of scientific paper classification is to help researchers track and analyze academic information and resources. To facilitate this goal, papers should be classified into not only coarse-grained research fields (e.g., "Machine Learning" and "Public Health") but also fine-grained themes (e.g., "Large Language Models" and "Deltacoronavirus"). Note that in a large and fine-grained label space, most papers are naturally relevant to multiple themes. However, most existing studies under the weakly supervised setting focus on classifying papers at a coarse level with 5 to 50 categories and assume each document is relevant to only one category (or a single path from the root to a leaf if categories form a hierarchy). As far as we know, MICoL <ref type="bibr" target="#b68">[69]</ref> is a pioneering work that considers weakly supervised multi-label classification with more than 10,000 categories. Nevertheless, its accuracy is hampered by using limited information such as paper titles and abstracts only, which will be discussed below.</p><p>The Usage of Paper Full Texts. A paper's title and abstract, although summarized to cover its major topics, cannot capture all fine-grained aspects. For example, technique-related labels may be introduced in more detail in the "Method" section; downstream tasks may be explained in the "Experiments" section. To find more Figure <ref type="figure">1</ref>: Weakly supervised classification performance of Longformer <ref type="bibr" target="#b1">[2]</ref> on two datasets of scientific papers, MAG-CS and PubMed, used in <ref type="bibr" target="#b67">[68]</ref>. When Longformer allows a larger input sequence length (i.e., it can take more tokens from paper full texts), the classification precision drops. labels relevant to a paper, it becomes necessary to leverage its full text. Intuitively, when we aim to utilize paper full texts, the first challenge is to deal with long text. Indeed, if we check the two datasets, MAG-CS and PubMed, used by MICoL <ref type="bibr" target="#b68">[69]</ref>, the average full-paper length exceeds 4,000 words. In comparison, most pretrained language models (PLMs) such as BERT <ref type="bibr" target="#b11">[12]</ref> and SciBERT <ref type="bibr" target="#b1">[2]</ref> can take an input sequence with at most 512 tokens. Therefore, those PLM-based weakly supervised text classifiers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b68">69]</ref> cannot be directly applied to full-text scientific papers.</p><p>More importantly, we would like to argue that classifying fulltext papers is beyond the problem of dealing with long text. As shown in Figure <ref type="figure">1</ref>, we adopt Longformer <ref type="bibr" target="#b2">[3]</ref>, which can take at most 4,096 tokens, to classify papers in MAG-CS and PubMed under the weakly supervised setting. (For more experiment details such as how Longformer is used, please refer to Section 4.1.) We control the maximum input sequence length of Longformer from 512 to 4,096. Surprisingly, the more tokens Longformer takes (i.e., the more information from the full text that Longformer considers), the lower the classification precision is. This observation has two implications: First, full texts are noisy and should not be treated the same as abstracts. Abstracts should still play a leading role in classification, while full texts provide auxiliary information <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. Second, to better exploit full text, we need to consider the structures in scientific papers. The major design that enables Longformer to take a longer input sequence is to sparsify the fully connected attention in Transformer <ref type="bibr" target="#b46">[47]</ref>, where each input token only interacts with its neighbor tokens and the first several tokens in the linear sequence. However, the rich structural information prevalently available inside the full text is not fully captured by Longformer. Figure <ref type="figure" target="#fig_0">2</ref> shows two types of such structural information: the cross-paper network structure and the in-paper hierarchy structure. The hierarchy structure organizes sections, subsections, and paragraphs into a tree, where the parent-child relation indicates that a finer text unit is included in a coarser one. Such a structure implies which paragraphs should be jointly considered when aggregating paragraph semantics to the entire paper. Moreover, by parsing the bibliographic entries in paper full texts, one can obtain each paper's references and construct a citation network. This cross-paper network structure indicates the semantic proximity between two papers. To summarize, these two types of structures provide additional semantic signals that are not reflected in a linear text sequence.</p><p>Contributions. Being aware of the two aforementioned challenges, in this paper, we study weakly supervised multi-label text classification of full-text scientific papers. We propose FuTex, the design of which is centered around how to use the cross-paper network structure and the in-paper hierarchy structure to classify scientific papers in a large and fine-grained label space. FuTex has three major modules: (1) Network-aware contrastive fine-tuning aims to leverage the cross-paper network structure to fine-tune a pre-trained language model (PLM) so that it can probe fine-grained label semantics and distinguish among similar categories. (2) Hierarchy-aware aggregation aims to exploit the in-paper hierarchy structure to obtain the entire paper representation by aggregating from its paragraphs. With this aggregation process, a PLM does not need to deal with the full-text sequence at once. (3) Self-training aims to take the initial prediction (i.e., top-ranked categories according to the first two modules) as pseudo labels to train a full-text paper classifier. Then the prediction of the trained classifier can complement the initial prediction to improve the final classification results.</p><p>We conduct experiments on two datasets (both with &gt;10,000 categories) commonly used in previous studies <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>. Results show that FuTex outperforms competitive baselines including scientific PLMs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>, weakly supervised text classifiers <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b68">69]</ref>, and structure-enhanced PLMs <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. Notably, on the MAG-CS dataset, our FuTex model, without any ground-truth training data, is on par with a supervised classifier <ref type="bibr" target="#b39">[40]</ref> trained on 60,000 labeled papers. To summarize, this work makes the following contributions.</p><p>? We study the problem of weakly supervised multi-label classification of full-text scientific papers. Different from most previous studies on weakly supervised text classification, it considers a large, fine-grained label space and paper full texts. ? We propose the FuTex framework which utilizes the cross-paper network structure and the in-paper hierarchy structure associated with scientific papers to improve classification performance. ? We conduct experiments on two datasets and demonstrate the effectiveness of FuTex in comparison with competitive baselines, including those using abstracts only and those using full text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 Cross-Paper and In-Paper Structures</head><p>Cross-Paper Network Structure. Given a collection of scientific papers, we can obtain the references of each paper by parsing the bibliographic entries in its full text. If we view papers as nodes and references as directed edges, a network can be constructed. Formally, we have the definition below. where each node ? ? V is a paper, and (? ? , ? ? ) ? E if and only if ? ? cites ? ? (i.e., ? ? is a bibliographic entry in ? ? ).</p><p>To describe the relationship between two papers in G, we adopt the notation of meta-paths <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. To be specific, if ? ? cites ? ? , we can say the two papers are connected via the meta-path "Paper ? Paper" (or its abbreviation ? ? ?). Similarly, if two papers ? ? and ? ? share a common reference ? ? (i.e., (? ? , ? ? ) ? E and (? ? , ? ? ) ? E), we can say ? ? and ? ? are connected via the meta-path ? ? ? ? ?. Intuitively, if two papers are connected via a certain meta-path, their relevant topics are more likely to overlap. Following <ref type="bibr" target="#b68">[69]</ref>, given a meta-path M, we use ? ? ? M ? ? to denote that ? ? is connected to ? ? via M, and the meta-path-based neighborhood N M (? ? ) is defined as {? ? |? ? ? M ? ? AND ? ? ? ? ? }.</p><p>In-Paper Hierarchy Structure. One unique challenge we are facing in full-text paper classification is that each paper is beyond a plain text sequence (i.e., title+abstract) and contains its internal hierarchical structure of paragraphs. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, nodes representing paragraphs, subsections, sections, and the entire paper form a tree, in which the parent-child relation implies a finer text unit is entailed by a coarser one. Formally, we have the definition below.</p><p>Definition 2.2. (In-Paper Hierarchy Structure) A full-text paper ? contains a hierarchical tree structure T ? . The root of T ? represents the entire paper; the leaves of T ? are ?'s paragraphs P ? = {? ?1 , ..., ? ?? }. The tree can be characterized by a mapping Child(?), where Child(?) is the set of text units that are one level finer than ? and contained in ?.</p><p>Putting the two structures together, we would like to classify the nodes in a network G. Meanwhile, each node ? contains its own subcomponents that form a hierarchy T ? . Given a paper, we need to jointly consider its subcomponents and its proximity with neighbors to infer its categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Definition</head><p>In this paper, we study weakly supervised multi-label text classification. By "weakly supervised", we imply that we do not have any annotated training samples for any label, and the only available supervision to characterize a label is its name and several descriptive sentences <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b68">69]</ref>. Figure <ref type="figure">3</ref> shows the name and description of the label "Deltacoronavirus" as an example. This setting is more challenging than zero-shot multi-label text classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b61">62]</ref> which assumes annotated documents are given for some seen classes and the trained classifier should be generalized to predict unseen classes. Under the weakly supervised setting, all classes are unseen. This setting is also called "wild zero-shot" in some previous studies <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b68">69]</ref>.</p><p>By "multi-label", we mean that each paper can be relevant to more than one label. This is a natural assumption when the label space is fine-grained and multi-faceted. For example, a COVID-19 paper can be labeled as "Infections", "Lung Diseases", "Coronavirus", and "Public Health" at the same time. This assumption makes our task more challenging than weakly supervised single-label classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>To summarize, our task can be defined as follows. each paper ? has its full text and hierarchy structure T ? , and (2) a label space L where each label ? has its name and description, our task is to predict the relevant labels L ? ? L for each ? ? D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head><p>One straightforward solution to our task is to pick a pre-trained language model (e.g., SciBERT <ref type="bibr" target="#b1">[2]</ref>), use it to encode each paper's content and each label's name/description to get their embeddings, and then perform the nearest neighbor search in the embedding space. However, such an approach suffers from two drawbacks: First, unfine-tuned PLMs may not be powerful enough to detect the subtle semantic differences between two papers or two label descriptions, but fine-grained text classification, to a great extent, requires the classifier to distinguish among labels that are close to each other. Second, the entire paper is long (e.g., with ?4,000 words on average in the Semantic Scholar Open Research Corpus (S2ORC) <ref type="bibr" target="#b25">[26]</ref>), which exceeds the maximum sequence length (e.g., 512 tokens) that a PLM can handle in most cases.</p><p>To overcome the aforementioned two drawbacks, we propose to exploit the cross-paper network structure and the in-paper hierarchy structure, which will be introduced in Sections 3.1 and 3.2, respectively. Then, in Section 3.3, we present a self-training strategy, that is, how we use initial predictions as pseudo labels to train a classifier that complements the predictions. The overview of our proposed FuTex framework is shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network-Aware Contrastive Fine-Tuning</head><p>The first module in FuTex aims to utilize the cross-paper network structure to improve the PLM's ability to distinguish among finegrained labels. We follow the intuition of LinkBERT <ref type="bibr" target="#b54">[55]</ref> that if two papers are connected via certain citation-based relationships (e.g., ? ? ? M ? ? ), then a paragraph ? ? ? ? ? and a paragraph ? ? ? ? ? are more likely to share fine-grained topics than two randomly picked paragraphs. In LinkBERT <ref type="bibr" target="#b54">[55]</ref>, Yasunaga et al. propose to concatenate the two "linked" paragraphs together (i.e., [CLS]? ? [SEP]? ? [SEP]) to perform masked token prediction and document relation prediction for language model pre-training <ref type="bibr" target="#b11">[12]</ref>. However, in this paper, we are not aiming at training a generalpurpose PLM. Instead, our model only needs to judge whether two text units are relevant to similar topics or not. Being able to do this, during inference, the model can take a paragraph and a label description as input to predict whether the paragraph is relevant to the label. To achieve this goal, following <ref type="bibr" target="#b68">[69]</ref>, we adopt a contrastive fine-tuning objective to replace the language model pre-training objectives in LinkBERT. To be specific, the PLM should be fine-tuned to distinguish between "linked" and "unlinked" paragraph pairs. As shown in Figure <ref type="figure" target="#fig_3">4</ref> (left), given three papers ?, ? + , and ? -, where ? + ? N M (?) and ? -? N M (?). We randomly sample three paragraphs ?, ? + , and ? -from ?, ? + , and ? -, respectively. The PLM aims to predict the similarity between ? and ? + as well as that between ? and ? -.</p><formula xml:id="formula_0">? pos = PLM([CLS]?[SEP]? + [SEP]), ? neg = PLM([CLS]?[SEP]? -[SEP]).<label>(1)</label></formula><p>Here, ? pos and ? neg are the output representations of the [CLS] token after PLM encoding. A linear layer is then trained to predict the scores given the output representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sim(?, ?</head><formula xml:id="formula_1">+ ) = ? ? ? pos , sim(?, ? -) = ? ? ? neg ,<label>(2)</label></formula><p>where ? is a learnable vector. The PLM is fine-tuned to make sim(?, ? + ) larger than sim(?, ? -), in which case we can adopt the contrastive loss <ref type="bibr" target="#b6">[7]</ref>.</p><formula xml:id="formula_2">J = E ? ?? ? + ?? + ? N M (? ) ? -?? -?N M (? )</formula><p>log exp(sim(?, ? + )) exp(sim(?, ? + )) + exp(sim(?, ? -)) .</p><p>(3) After contrastive fine-tuning, the PLM is used to predict the score between a paragraph and a label. For example, given a paper ? ? D and a label ? ? L, we use ? ? to denote the title+abstract of ?, and we use ? ? to denote the name+description of ?. Then, the score can be calculated as</p><formula xml:id="formula_3">sim(? ? , ? ? ) = ? ? PLM([CLS]? ? [SEP]? ? [SEP]).<label>(4)</label></formula><p>However, the strategy that concatenates each paragraph and each label and feeds them into one PLM (i.e., the Cross-Encoder architecture <ref type="bibr" target="#b35">[36]</ref>) makes the inference computationally expensive because the representation of each text unit cannot be pre-computed. For example, suppose there are ? papers (each of which has ? paragraphs) and ? labels, then we need to call the PLM ? (???) times during inference. Such a cost will prohibit us from applying the model to a large corpus and a large label space. For example, if ? = ? = 10 4 and ? = 30, then ??? = 3 ? 10 9 . To alleviate the cost, we adopt the following two strategies.</p><p>Adding a retrieval stage. Following <ref type="bibr" target="#b68">[69]</ref>, given a paper ?, we first adopt exact name matching to retrieve a small set of candidate labels C(?) from the entire label space L. To be specific, if a label's name appears in the paper's content, it will be added as a candidate. The fine-tuned PLM is then applied as a reranker to score labels from the retrieved candidate pool.</p><p>Using Bi-Encoder for non-abstract paragraphs. As mentioned in the Introduction, the abstract and other paragraphs should not be treated equally in text classification. The abstract is highly summarized to cover the major topics of the paper, while a paragraph in the paper body may capture only one aspect and provides auxiliary topic signals. To use the more powerful tool in the most important case, we only adopt the Cross-Encoder architecture when inferring the labels of paper abstracts (i.e., Eq. ( <ref type="formula" target="#formula_3">4</ref>)). For other paragraphs, we adopt the Bi-Encoder architecture <ref type="bibr" target="#b18">[19]</ref>. To be specific, we encode each paragraph and each label separately.</p><formula xml:id="formula_4">? ? = PLM([CLS]?[SEP]), ? ? = PLM([CLS]? ? [SEP]). (5)</formula><p>Here, ? ? and ? ? are the output representations of the [CLS] token after PLM encoding. Then, the similarity between ? and ? can be computed as cos(? ? , ? ? ). One drawback of this strategy is that the paragraph and the label text cannot serve as each other's context during PLM encoding. However, the efficiency is significantly improved because we can pre-compute ? ? and ? ? for all paragraphs and labels. In fact, if we combine the two proposed strategies, the inference complexity will be reduced to ? (?? + ?? + ?), where ? is the average number of candidate labels picked for each paper in the retrieval stage. For example, if we assume ? = ? = 10 4 and ? = ? = 30, then ?? + ?? + ? = 6.1 ? 10 5 , which is orders of magnitude smaller than ??? = 3 ? 10 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchy-Aware Aggregation</head><p>Although the similarity between each paragraph ? and each label ? can be computed efficiently now, we have not figured out how to calculate the score between an entire paper ? and a label ?. Intuitively, simply averaging all paragraph embeddings may not work well because important signals from the paper abstract and conclusion will then be buried under the great amount of content from other sections. Previously, FullMeSH <ref type="bibr" target="#b10">[11]</ref> proposed to check 5 sections -abstract, introduction, method, result, and summary -in each full paper so as to probe relevant topics from different aspects. Inspired by their idea, we utilize the in-paper hierarchy structure T ? to perform embedding aggregation from paragraphs to sections, and then to the entire paper.</p><p>Given a non-leaf text unit ? ? T ? (e.g., subsections, sections, or the entire paper), we obtain the embedding of ? by aggregating the embeddings from ?'s children. Formally,</p><formula xml:id="formula_5">? ? = 1 |Child(?)| ?? ? ?Child(? ) ? ? .<label>(6)</label></formula><p>After the bottom-up aggregation, the score between the entire paper ? and the label ? can be computed as score ? (?, ?) = cos(? ? , ? ? ).</p><p>Here, the subscript "?" means the score is based on the Bi-Encoder architecture. Recall that we can calculate the score between ? and ? using Eq. ( <ref type="formula" target="#formula_3">4</ref>) based on the Cross-Encoder architecture:</p><formula xml:id="formula_7">score ? (?, ?) = ? ? PLM([CLS]? ? [SEP]? ? [SEP]),<label>(8)</label></formula><p>where "? " stands for Cross-Encoder. Now we adopt an ensemble ranking step to jointly consider the two scores. Given a document ?, we first rank all candidate labels from C(?) in descending order according to score ? (?, ?) and score ? (?, ?), respectively. In this way, each candidate label ? will have two rank positions ? ? (? |?) and ? ? (? |?). Then, we calculate the mean reciprocal rank (MRR) of ?.</p><formula xml:id="formula_8">MRR(? |?) = 1 ? ? (? |?) + 1 ? ? (? |?) .<label>(9)</label></formula><p>Finally, candidate labels are sorted according to MRR(? |?) as the reranking result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Training</head><p>The retrieval stage proposed in Section 3.1 significantly improves the efficiency of FuTex. However, it may filter out labels that do not explicitly appear in a paper but are implicitly relevant to the paper in the latent semantic space. To mitigate this issue, we present a self-training strategy to utilize paper full texts and confident predictions (based on MRR) to train a text classifier ? class (?). The classifier is then used to predict the probability that a paper is relevant to a label (not necessarily selected in the retrieval stage), and those top-ranked labels will complement our initial MRR-based predictions.</p><p>Since the major goal of this paper is not to invent a new fully supervised text classifier, we propose to use an off-the-shelf model. To leverage paper full texts and meanwhile bypass the sequence length problem, we choose a bag-of-words multi-label classifier -Parabel <ref type="bibr" target="#b39">[40]</ref>, which, according to <ref type="bibr" target="#b58">[59]</ref>, has competitive performance even compared with deep learning classifiers on benchmark datasets. Parabel represents each training document ? as a |W D |dimensional feature vector ? ? , where W D is the vocabulary of D. Given a word ? ? W D , its corresponding entry in ? ? is the following tf-idf score:</p><formula xml:id="formula_9">? ?,? = tf (?, ?) ? idf (?, D),<label>(10)</label></formula><p>where tf (?, ?) is the term frequency of ? in ?, and idf (?, D) = log</p><formula xml:id="formula_10">| D | | {? ? ? D |? ?? ? } |</formula><p>is the inverse document frequency of ?. For each paper ?, according to MRR calculated in Eq. ( <ref type="formula" target="#formula_8">9</ref>), we use the top-? predicted labels as pseudo labels to train the Parabel classifier. (If ? has less than ? candidate labels selected in the retrieval stage, we use all of them.) Formally, the pseudo labels of ? is represented as an |L|-dimensional vector ? ? , where</p><formula xml:id="formula_11">? ?,? = 1, ? MRR (? |?) ? ? , 0, otherwise.<label>(11)</label></formula><p>Here, ? MRR (? |?) is the rank position of ? according to MRR(? |?).</p><p>Given ? ? and ? ? of each paper, Parabel learns a tree-based discriminative classifier Pr( ?? |? ? ). (For more technical details, one can refer to <ref type="bibr" target="#b39">[40]</ref>.) Based on the trained classifier, we relabel each paper ? to keep those initial confident predictions while incorporating new highly-ranked labels. To be specific, the top-? predictions according to MRR remain at top-? in our final predictions; the other labels will be sorted according to Parabel's output Pr( ??? = 1|? ? ) and ranked after top-? . We use the following example to explain this process.</p><p>Example 3.1. (Final Prediction after Self-Training) Suppose there are 5 labels L = {?, ?, ?, ?, ?} and ? = 2. Given a paper ?, assume 3 labels ?, ?, and ? are selected in the retrieval stage and their MRR scores are 2.00, 1.00, and 0.67, respectively. Then, ? and ? will be the two pseudo labels of ? used for training Parabel. The trained Parabel is used to classify ? again and get the following scores: In our final prediction, ? and ? will still be the top 2, and the other labels will be ranked according to Parabel's prediction. Therefore, the final rank will be (?, ?, ?, ?, ?). In practice, we find this strategy achieves better classification performance than reranking all labels purely based on Pr( ??? = 1|? ? ), the result of which is (?, ?, ?, ?, ?).</p><p>The entire procedure of FuTex is summarized in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Setup</head><p>Datasets. We use two datasets, MAG-CS <ref type="bibr" target="#b48">[49]</ref> and PubMed <ref type="bibr" target="#b27">[28]</ref>, that are widely adopted in previous studies on scientific paper classification <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>. Originally, MAG-CS contains ?705K papers published at 105 top computer science venues, where each paper is labeled with its related fields-of-study <ref type="bibr" target="#b42">[43]</ref>; PubMed consists of ?899K papers published in 150 top medicine journals, where each paper is labeled with its related MeSH terms <ref type="bibr" target="#b9">[10]</ref>. However, since the two original datasets do not have paper full texts, we try to extract full texts from S2ORC <ref type="bibr" target="#b25">[26]</ref>. S2ORC has segmented each paper into paragraphs, marked the section that each paragraph belongs to, and parsed the bibliographic entries. We remove the paragraphs with less than 10 words from each paper. Note that not all papers in these two datasets can be found in S2ORC, and we finally obtain 96,718 full-text MAG-CS papers and 251,573 full-text PubMed papers. Because FuTex does not require any annotated training data, all these papers are used for testing. More statistics of these two datasets can be found in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Compared Methods. We compare our FuTex model with the following baselines including scientific PLMs, structure-enhanced PLMs, and zero-shot multi-label text classification methods.</p><p>? SciBERT <ref type="bibr" target="#b1">[2]</ref> <ref type="foot" target="#foot_0">1</ref> is a scientific PLM trained on 1.14M scientific papers from Semantic Scholar <ref type="bibr" target="#b0">[1]</ref> using masked language modeling and next sentence prediction tasks. ? OAG-BERT <ref type="bibr" target="#b23">[24]</ref> <ref type="foot" target="#foot_1">2</ref> is a scientific PLM trained on 120M scientific papers from the Open Academic Graph <ref type="bibr" target="#b60">[61]</ref>. It proposes heterogeneous entity type embedding, span-aware entity masking, and Table <ref type="table">2</ref>: P@? and NDCG@? scores of compared methods on MAG-CS and PubMed. Bold: the highest score. *: FuTex is significantly better than this method with p-value &lt; 0.05. **: FuTex is significantly better than this method with p-value &lt; 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method MAG-CS [49]</head><p>PubMed [28] P@1 P@3 P@5 NDCG@3 NDCG@5 P@1 P@3 P@5 NDCG@3 NDCG@5 The four baselines above can be used to classify either abstracts or full-text papers. When applying them to abstracts, we directly encode each abstract and each label description to calculate the cosine similarity between the two embeddings. When applying them to full text, we follow the hierarchy-aware aggregation process in Section 3.2 and ensemble the results of using abstract only and using full text.</p><p>? 0SHOT-TC <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57]</ref> is a zero-shot text classification method.</p><p>It is a natural language inference (NLI) model that predicts to what extent a paper (as the premise) entails the sentence "this document is about {label_name}." (as the hypothesis). Following <ref type="bibr" target="#b41">[42]</ref>, we use RoBERTa-large-mnli <ref type="bibr" target="#b24">[25]</ref> <ref type="foot" target="#foot_5">6</ref> as the NLI model. ? MICoL <ref type="bibr" target="#b68">[69]</ref> <ref type="foot" target="#foot_6">7</ref> is a zero-shot text classification method. It proposes a metadata-induced contrastive learning technique to fine-tune a PLM. MICoL has various configurations with the model architecture and the used meta-path. According to the experimental results in <ref type="bibr" target="#b68">[69]</ref>, we choose the best-performing configuration: (Cross-Encoder, ? ? ? ? ?).</p><p>The two methods above adopt the Cross-Encoder architecture. As mentioned in Section 3.1, it will be too computationally expensive to apply them to all paragraphs. Therefore, we keep their original usage on paper abstracts only.</p><p>? Longformer <ref type="bibr" target="#b2">[3]</ref> <ref type="foot" target="#foot_7">8</ref> is a PLM dealing with long documents. It sparsifies the fully connected attention and can take 4,096 tokens at most. We adopt it to encode full-text papers by setting the maximum number of tokens as 512, 1,024, 2,048, and 4,096. As shown in the Introduction, the 512-token version performs the best, so we use it for performance comparison. ? PLM+GAT <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b53">54]</ref> is a PLM stacked with a Graph Attention Network (GAT) layer. Each paragraph ? ? ? is first encoded by a PLM. Then we use GAT to obtain paragraph embeddings by aggregating PLM representations of its neighbor paragraphs ? ? ? ? + ? N M (?). A link prediction objective (i.e., judging whether two paragraphs are connected via meta-path M) is then adopted to train the PLM and GAT in an end-to-end manner.</p><p>? GraphFormers <ref type="bibr" target="#b53">[54]</ref> 9 is a GNN-nested PLM architecture, in which GNN layers and Transformer layers are alternately stacked. Similar to PLM+GAT, a link prediction objective is adopted to train the model so that the PLM can be enhanced by the crosspaper network structure. The three methods above are naturally suitable for classifying paper full texts.</p><p>According to our experiments, SPECTER performs better than SciBERT, OAG-BERT, and LinkBERT. Therefore, for MICoL, PLM+ GAT, GraphFormers, and FuTex, we all use SPECTER as the base PLM. Also, the meta-path M is set as ? ? ? ? ? for all these models for a fair comparison.</p><p>Implementation and Hyperparameters. Each label from the PubMed dataset may have multiple label names, including one canonical name and 0, 1, or several synonyms (i.e., "entry terms"). Following <ref type="bibr" target="#b68">[69]</ref>, we include ? as a candidate label of ? if any of its names appears in ?'s content. On both MAG-CS and PubMed, during the retrieval stage, we use each paper's title and abstract, instead of the full text, for label name matching because this yields better classification performance. During network-aware contrastive finetuning, when feeding the two paragraphs into a Cross-Encoder, the maximum length of each paragraph is 256 tokens. The training batch size is 8. We use the AdamW optimizer <ref type="bibr" target="#b26">[27]</ref>, warm up the learning rate for the first 100 steps and then linearly decay it. The learning rate is 5e-5, the weight decay is 0.01, and ? = 1e-8. We sample 45,000 and 150,000 tuples of (?, ? + , ? -) from MAG-CS and PubMed, respectively, for PLM fine-tuning. During self-training, we set ? = 5 to get pseudo labels of each paper. For the Parabel classifier 10 , all parameters are set by default. Specifically, the number of trees is 3; the maximum number of labels in a leaf node is 100; the beam search width in prediction is 10. We remove words appearing in less than 5 papers when training Parabel.</p><p>Evaluation Metrics. We use two commonly adopted metrics in multi-label text classification: P@? and NDCG@?, where ? = 1, 3, and 5. Given a paper ?, we use ? ? ? {0, 1} | L | to denote its groundtruth labels and rank(?) to denote the ?-th ranked label in the final prediction. P@? and NDCG@? are then defined as:</p><formula xml:id="formula_12">P@? = 1 ? ? ?? ?=1 ? ?,rank(? ) . DCG@? = ? ?? ?=1 ? ?,rank(? ) log(? + 1) , NDCG@? = DCG@? min(?,||? ? || 0 ) ?=1 1 log(?+1) . (<label>12</label></formula><formula xml:id="formula_13">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison</head><p>Table <ref type="table">2</ref> shows P@? and NDCG@? scores of compared methods on MAG-CS and PubMed. For models with randomness (i.e., MICoL, PLM+GAT, GraphFormers, and FuTex), we run of them 5 times with the average performance reported. Other models (i.e., SciBERT, OAG-BERT, LinkBERT, SPECTER, 0SHOT-TC, and Longformer) are deterministic according to our usage. To show statistical significance, we conduct a two-tailed t-test to compare FuTex with each baseline if the baseline has randomness, and we conduct a twotailed Z-test to compare FuTex with each deterministic baseline. The significance level is also marked in Table <ref type="table">2</ref>.</p><p>From Table <ref type="table">2</ref>, we find that: (1) FuTex consistently and significantly outperforms all baselines. On both datasets, MICoL is a competitive baseline, possibly because it also uses citation links across papers. However, since MICoL only considers text content from paper titles and abstracts, it is not as powerful as FuTex.</p><p>(2) There are four baselines (i.e., SciBERT, OAG-BERT, LinkBERT, and SPECTER) that can be used in both abstract-only and fulltext settings. In most cases, a full-text variant can outperform its abstract-only counterpart. This observation validates our claim that considering the full text is beneficial to paper classification. Besides, unlike Longformer (as shown in Figure <ref type="figure">1</ref>), our proposed hierarchy-aware aggregation strategy, which is also used by the full-text variants of the four baselines, can effectively use paper full texts. Furthermore, in a few cases, a full-text variant underperforms its abstract-only counterpart in terms of P@1 but achieves higher P@3 and P@5. This finding implies that signals extracted by hierarchy-aware aggregation from paper full texts can better help lower-ranked predictions.</p><p>Comparison with a Supervised Model. We further compare Fu-Tex with a fully supervised multi-label text classifier. In accordance with our choice in Section 3.3, we report the performance of Parabel <ref type="bibr" target="#b39">[40]</ref> with ground-truth training data. To be specific, for each dataset, we select 10,000 papers as testing samples and pick different numbers of training samples from the remaining papers. Figure <ref type="figure" target="#fig_6">5</ref> shows the P@5 score of Parabel with different numbers of ground-truth  training samples in comparison with FuTex. We can observe that our FuTex model, without relying on any annotated training data, is on par Parabel that uses 60,000 and 1,000 ground-truth training samples on MAG-CS and PubMed, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>There are three major modules in FuTex: network-aware contrastive fine-tuning, hierarchy-aware aggregation, and self-training. Now we conduct an ablation study to check the contribution of each module. To facilitate this, we create three ablation versions of FuTex:</p><p>? FuTex-NoNetwork does not have the network-aware contrastive fine-tuning module. It directly uses SPECTER in the Bi-Encoder architecture, ensembles the results of using abstract only and using full text, and then performs self-training. ? FuTex-NoHierarchy does not have the hierarchy-aware aggregation module. After contrastive fine-tuning, it applies the Cross-Encoder to paper titles/abstracts only and then performs self-training. ? FuTex-NoSelfTrain does not have the self-training module. It uses the MRR-based ranking list obtained in Section 3.2 as the final prediction.</p><p>Table <ref type="table">3</ref> demonstrates the performance of the full FuTex model and the three ablation versions. We can observe that: (1) The full FuTex model consistently and significantly outperforms the three ablation versions, indicating that all three major modules have a positive contribution to the classification performance. (2) Among the three ablation versions, FuTex-NoNetwork performs the worst in terms of P@1. This finding indicates that the cross-paper network structure is more beneficial to top-ranked predictions. By contrast, FuTex-NoSelfTrain has the lowest P@5 score on both datasets, which means that the self-training module contributes the most to lower-ranked predictions. This observation validates our claim that self-training can find more labels that are semantically relevant to each paper so as to complement the initial top-ranked categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>We now perform a case study to qualitatively demonstrate the effect of considering full text in paper classification. Table <ref type="table" target="#tab_2">4</ref> shows two cases, one of which is from MAG-CS and the other from PubMed. For both papers, we show their text information including the title, Table <ref type="table">3</ref>: P@? and NDCG@? scores of the full FuTex model and three ablation versions on MAG-CS and PubMed. Bold, *, and **: the same meaning as in Table <ref type="table">2</ref>. Method MAG-CS <ref type="bibr" target="#b48">[49]</ref> PubMed [28] P@1 P@3 P@5 NDCG@3 NDCG@5 P@1 P@3 P@5 NDCG@3 NDCG@5 FuTex-NoNetwork 0. Abstract: there is an unmet need for novel blood-based biomarkers that offer timely and accurate diagnostic and prognostic testing in inflammatory bowel diseases (ibd). we aimed to investigate the diagnostic and prognostic utility of serum calprotectin (sc) in ibd ... Full Text: this new challenge has prompted a change in robotics navigation philosophy, where path planning and modeling were always obtained a priori ... in general, in mobile robot navigation, the occupancy-based approach is one of the most commonly used methods ... path planning in large and outdoor environments is a complex task because there are a lot of parameters that define the traversability, for example, as follows ... when the 3-d model is defined (as the one in figs. <ref type="figure" target="#fig_6">5</ref> and<ref type="figure" target="#fig_8">6</ref> , where the terrain considered ta is represented in blue and the nta is represented in red), the free space can be extracted to build a trm for the robot navigation and path planning ... Full Text: inflammatory bowel diseases (ibd), including crohn's disease (cd) and ulcerative colitis (uc), are chronic, debilitating inflammatory disorders of the gastrointestinal tract affecting adults and children ... a recent meta-analysis of 13 studies and 1,041 patients found that fc had a pooled sensitivity and specificity of 0.93 ... to determine the accuracy of blood parameter measurements as a prognostic test capable of diagnosing ibd, receiver operating characteristic (roc) analyses were performed by plotting sensitivity against specificity ... Ground-Truth Labels: robot, voronoi diagram, motion planning, computer vision, mobile robot navigation, mobile robot, scanner, computational geometry Ground-Truth Labels: leukocyte l1 antigen complex, inflammatory bowel diseases, multivariate analysis, colitis ulcerative, prognosis, kaplan meier estimate, sensitivity and specificity, humans, crohn disease, logistic models, proportional hazards models, odds ratio, area under curve MICoL Prediction: voronoi diagram (?), robot (?), scanner (?)</p><p>MICoL Prediction: inflammatory bowel diseases (?), leukocyte l1 antigen complex (?) FuTex Prediction: voronoi diagram (?), robot (?), scanner (?), motion planning (?), mobile robot navigation (?) FuTex Prediction: inflammatory bowel diseases (?), leukocyte l1 antigen complex (?), crohn disease (?), colitis ulcerative (?), sensitivity and specificity (?) abstract, and excerpts from full text. We also show their groundtruth labels, labels predicted by FuTex, and labels predicted by MICoL (which is the most competitive baseline using titles and abstracts only). We mark a label as blue if it (or a semantically similar term) appears in the paper title or abstract; we mark a label as orange if it does not appear in the title/abstract but is mentioned in full text.</p><p>In the MAG-CS case, three labels "voronoi diagram", "robot", and "scanner" explicitly appear in the paper abstract, and they are correctly predicted by both MICoL and FuTex. However, MICoL misses labels such as "motion planning" and "mobile robot navigation", which are not mentioned in the title/abstract. In fact, the term "outdoor robot navigation" in the abstract may imply the paper's relevance to "mobile robot navigation", but MICoL does not build the connection between them. After the paper's full text is exploited, "mobile robot navigation" completely appears in the content, and the term "path planning", which is semantically close to the label "motion planning", is repeatedly mentioned. As a result, both labels are accurately captured by FuTex.</p><p>In the PubMed case, MICoL successfully predicts the groundtruth labels "inflammatory bowel diseases" and "leukocyte l1 antigen complex" (whose synonym is "calprotectin" 11 ) indicated by the title/abstract. However, MICoL fails to predict labels such as "crohn disease", "colitis ulcerative", and "sensitivity and specificity". As 11 According to https://meshb-prev.nlm.nih.gov/record/ui?ui=D039841, "Calprotectin" is an entry term of "Leukocyte L1 Antigen Complex".  shown in Table <ref type="table" target="#tab_2">4</ref>, hints to these labels can be found in the full text.</p><p>Note that these labels are indeed relevant to the paper rather than just being mentioned: Crohn's disease and ulcerative colitis are two types of inflammatory bowel diseases studied in the paper, and the paper extensively discusses the sensitivity and specificity of predicting these diseases. By leveraging the paper's full text, FuTex accurately picks these labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficiency</head><p>We now analyze the training and inference time of FuTex on MAG-CS and PubMed. To be specific, we compare the training time of FuTex (the network-aware contrastive learning step) with that of PLM+GAT and GraphFormers, which also use network signals for training. For a fair comparison, we run each model on an NVIDIA RTX A6000 GPU, and we train the three models for the same number of epochs on each dataset (i.e., 20 epochs on MAG-CS and 5 epochs on PubMed). As for the inference stage, we notice that the major factor which affects each model's inference efficiency is whether it is used for abstracts only or full texts. If we fix this factor, different BERT-based baselines will have similar inference efficiency because they have similar model sizes and architectures. Therefore, we choose SPECTER as a representative for comparison and report its inference time (per testing sample) when used for abstracts only and full texts. The results are demonstrated in Figure <ref type="figure" target="#fig_8">6</ref>.</p><p>From Figure <ref type="figure" target="#fig_8">6</ref>(a), we observe that FuTex has much less training time than PLM+GAT and GraphFormers. From Figure <ref type="figure" target="#fig_8">6</ref>(b), we find that the inference efficiency of FuTex is on par with SPECTER (fulltext). In comparison with SPECTER (abstract), FuTex and SPECTER (full-text) need significantly more time to run because about 30 to 45 times more paragraphs are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Weakly Supervised Text Classification. Weakly supervised text classification aims to assign relevant label(s) to each document without any human-annotated training samples provided. The common formats of weak supervision include label names <ref type="bibr" target="#b32">[33]</ref>, a small set of category-indicative keywords <ref type="bibr" target="#b28">[29]</ref>, and label descriptions <ref type="bibr" target="#b68">[69]</ref>. Technically, earlier methods mainly utilize Explicit Semantic Analysis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">44]</ref>, Latent Dirichlet Allocation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, and context-free word embeddings <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Inspired by the success of BERT <ref type="bibr" target="#b11">[12]</ref> in a wide spectrum of text mining tasks, recent studies start to exploit the power of PLMs in weakly supervised text classification. For example, ConWea <ref type="bibr" target="#b28">[29]</ref> uses BERT to disambiguate the provided keywords and retrieve more category-indicative words for pseudo training data collection; LOTClass <ref type="bibr" target="#b32">[33]</ref> leverages one BERT encoder to perform masked language modeling for finding more indicative words and another BERT to perform classification; X-Class <ref type="bibr" target="#b49">[50]</ref> uses BERT representations of words to perform category-aware clustering and then aligns documents to categories; LIME <ref type="bibr" target="#b37">[38]</ref> adopts BART-large-MNLI <ref type="bibr" target="#b19">[20]</ref> and prompts to predict pseudo labels of each document and then uses BERT for self-training. However, all the aforementioned methods focus on a relatively small label space (? 50 categories in most cases) and assume each document is relevant to one category only (or a single path from the root to a leaf category in the hierarchical classification setting). In contrast, FuTex studies larger and more fine-grained label spaces (e.g., &gt; 10, 000 categories) where each document is relevant to multiple labels in most cases.</p><p>Zero-Shot Multi-Label Text Classification. In general, similar to "weakly supervised", the term "zero-shot" also implies that the classifier does not need any annotated samples. However, in large-scale or extreme multi-label text classification, "zero-shot" is interpreted in different ways in the existing literature. According to <ref type="bibr" target="#b56">[57]</ref>, the restrictive zero-shot setting assumes training samples are given for some seen classes and the classifier aims to predict unseen classes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b61">62]</ref>, which is different from our "weakly supervised" setting; the wild zero-shot setting does not assume any seen classes and the classifier needs to make predictions without relying on any annotations <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b68">69]</ref>. For example, TaxoClass <ref type="bibr" target="#b41">[42]</ref> uses RoBERT-large-MNLI <ref type="bibr" target="#b24">[25]</ref> to convert text classification to an entailment task; MICoL <ref type="bibr" target="#b68">[69]</ref> proposes a metadata-induced contrastive learning method to fine-tune SciBERT <ref type="bibr" target="#b1">[2]</ref>. However, existing wild zero-shot classifiers still view each document as a linear sequence of paragraphs, thus cannot be directly applied to full-text paper classification due to the maximum length limit of PLMs. In comparison, FuTex exploits the cross-paper and in-paper structures of scientific literature. Scientific Paper Classification. Classifying scientific papers is a common evaluation task in both text mining (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b64">65]</ref>) and network mining (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b59">60]</ref>) studies. However, most studies consider coarse-grained paper classification only (e.g., ? 50 categories). To satisfy users' fine-grained interests, Zhang et al. <ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref> and Ye et al. <ref type="bibr" target="#b55">[56]</ref> propose to use paper metadata to perform large-scale multi-label paper classification. In the biomedical domain, MeSH indexing <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53]</ref> can also be cast as a multi-label text classification task to tag PubMed papers with fine-grained medical subject headings. Nevertheless, these studies focus on using the paper title and abstract only. To the best of our knowledge, FullMeSH <ref type="bibr" target="#b10">[11]</ref> and BERTMeSH <ref type="bibr" target="#b57">[58]</ref> are two representative studies making use of paper full texts. However, they adopt a fully supervised setting and are not directly applicable to our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>We present FuTex, a multi-label scientific paper classifier that relies on label names and descriptions as the only supervision and does not require any human-annotated training data. FuTex exploits paper full texts and consists of three modules: the contrastive learning module leverages the cross-paper citation network structure; the semantic aggregation module uses the in-paper hierarchy structure of sections, subsections, and paragraphs; the self-training module trains a full-text classifier using pseudo labels to complement the initial predictions. Experiments on two datasets demonstrate the superiority of FuTex over competitive weakly supervised baselines and show that FuTex is on par with fully supervised classifiers with thousands of ground-truth training samples. An ablation study validates the usefulness of all three proposed modules. A case study shows that FuTex can effectively extract signals from full text to predict labels not indicated by the paper title or abstract.</p><p>Interesting future directions include (1) how to leverage other in-paper structural signals, such as the relationship between paragraphs and figures/tables, to further improve the classification performance and (2) how to leverage large language models (e.g., GPT-4 <ref type="bibr" target="#b36">[37]</ref>) for weakly supervised fine-grained paper classification, where one needs to tackle the maximum input length limit given the paper full text and tens of thousands of label names.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 The Entire Procedure of FuTex</head><p>We summarize the entire procedure of FuTex in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Performance on Infrequent Labels</head><p>A large and fine-grained label space typically implies a long-tailed label distribution, where most categories are associated with only a few documents. In many real applications, it is desirable to predict more tail labels. For example, in scientific paper classification, predicting a paper is relevant to "Lagrangian Support Vector Machine" is more informative than saying the paper the relevant to "Machine Learning". To promote prediction of tail labels, recent studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b68">69]</ref> propose to use propensity-based P@? (i.e., PSP@?) and propensity-based NDCG@? (i.e., PSN@?) as evaluation metrics. PSP@? and PSN@? are formally defined as follows.</p><p>1</p><formula xml:id="formula_14">? ? = 1 + ? (? ? + ?) -? , PSP@? = 1 ? ? ?? ?=1 ? ?,rank(? ) ? rank(? ) . PSDCG@? = ? ?? ?=1 ? ?,rank(? ) ? rank(? ) log(? + 1) , PSN@? = PSDCG@? min(?,||? ? || 0 ) ?=1 1 log(?+1) .<label>(13)</label></formula><p>The intuition behind these two metrics is to give a higher reward to a model if it predicts an infrequent label correctly. In Eq. ( <ref type="formula" target="#formula_14">13</ref>), 1   ? ? is such a reward; ? ? is number of papers relevant to ? in the whole dataset D; ?, ?, ? &gt; 0 are constants. In this way, the less frequent a label is, the higher reward a model can get when predicting it correctly. PSP@? and PSN@? scores can be viewed as a rewardweighted version of P@? and NDCG@?, respectively. Following previous studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b68">69]</ref>, we set ? = 0.55, ? = 1.5, and ? = (log |D | -1) (? + 1) ? . By definition, we have PSP@1 ? PSN@1 if each paper has at least one ground-truth label.</p><p>Table <ref type="table">5</ref> shows PSP@? and PSN@? scores of FuTex and competitive baselines on MAG-CS and PubMed. From Table <ref type="table">5</ref>, we can observe that: (1) FuTex consistently and significantly outperforms all baselines except MICoL. (2) When comparing with MICoL, Fu-Tex has lower PSP@1 and PSN@3 but higher PSP@3, PSP@5, and PSN@5. The only statistically significant gap between FuTex and MICoL is the gap of PSP@5. This echos our finding from Table <ref type="table">2</ref> that considering paper full texts is more beneficial to lower-ranked predictions. One possible reason why FuTex underperforms MICoL in terms of PSP@1 and PSN@3 is that FuTex ensembles the predictions of a Cross-Encoder (Eq. ( <ref type="formula" target="#formula_7">8</ref>)) and a Bi-Encoder (Eq. ( <ref type="formula" target="#formula_6">7</ref>)) while MICoL is solely based on a Cross-Encoder according to our usage. In fact, as shown in <ref type="bibr" target="#b68">[69]</ref>, labels predicted by the Cross-Encoder architecture are more infrequent than those by the Bi-Encoder.</p><p>A.3 Performance on a Small Dataset MAG-CS and PubMed have nearly 97K and 252K papers, respectively, which can provide rich self-supervision during contrastive learning and self-training. We now examine the performance of FuTex on a small dataset and check if it can still outperform competitive baselines. To facilitate this, we adopt the Art dataset from the MAPLE benchmark <ref type="bibr" target="#b66">[67]</ref>. These Art papers are labeled with 1,990 categories at different granularities (e.g., "classics", "popular music", and "rhetorical criticism"), and we manage to find 328 of them from S2ORC <ref type="bibr" target="#b25">[26]</ref> to obtain full texts. The performance of FuTex and competitive baselines on these Art papers are demonstrated in Table <ref type="table" target="#tab_3">6</ref>. We can observe that FuTex performs the best in terms of P@3, P@5, NDCG@3, and NDCG@5. For P@1, FuTex Table <ref type="table">5</ref>: PSP@? and PSN@? scores of compared methods on MAG-CS and PubMed. Bold, *, and **: the same meaning as in Table <ref type="table">2</ref>.</p><p>Method MAG-CS <ref type="bibr" target="#b48">[49]</ref> PubMed [28] PSP@1 PSP@3 PSP@5 PSN@3 PSN@5 PSP@1 PSP@3 PSP@5 PSN@3 PSN@5 SPECTER <ref type="bibr" target="#b8">[9]</ref>    is second to 0SHOT-TC. This observation implies that even if the dataset is small (which may limit the power of contrastive learning and self-training), FuTex still works effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Hyperparameter Study</head><p>We study the effect of two major hyperparameters in FuTex: the number of trees used in the Parabel classifier and the maximum number of pseudo labels per paper used for self-training (i.e., ? ). The P@5 scores of FuTex on MAG-CS with different hyperparameter values in {1, 3, 5, 10} are plotted in Figure <ref type="figure" target="#fig_11">7</ref>. We can find that the performance of FuTex is not quite sensitive to the two hyperparameters. Indeed, all P@5 scores shown in Figure <ref type="figure" target="#fig_11">7</ref> outperform those of all baselines in Table <ref type="table">2</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The cross-paper network structure and in-paper hierarchy structure associated with scientific papers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 . 1 .</head><label>21</label><figDesc>(Cross-Paper Network Structure) A scientific paper corpus D has a cross-paper network structure G = (V, E),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 2 . 3 .Figure 3 :</head><label>233</label><figDesc>Figure 3: Name and description of the label "Deltacoronavirus" from PubMed (https://meshb.nlm.nih.gov/record/ui? ui=D000085686).</figDesc><graphic url="image-7.png" coords="3,360.58,84.23,197.01,83.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of the FuTex framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Pr( ??? = 1|? ? ) 0.80 0.85 0.30 0.60 0.90</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>10 http://manikvarma.org/code/Parabel/download.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The P@5 score of supervised Parabel with different numbers of ground-truth training papers. Our FuTex model, without any ground-truth training samples, is on par with Parabel that uses 60,000 and 1,000 training samples on MAG-CS and PubMed, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Training and inference time of FuTex and representative baselines on MAG-CS and PubMed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1 : 9 for ? ? ? do 10 ? 21 ?</head><label>191021</label><figDesc>FuTexInput: A collection of unlabeled scientific papers D; the cross-paper network structure G; each paper ?'s full text and its in-paper hierarchy structure T ? ; a label space L; each label ?'s name and description. Output: The relevant labels L ? ? L of each paper ? ? D. 1 Retrieve a small set of candidate labels C (? ) ? L for each paper using lexical matching;2 // Network-Aware Contrastive Fine-Tuning;3 Fine-tune a PLM using the contrastive loss in Eq. (3); 4 for ? ? D do 5 for ? ? C (? ) do 6 score ? (?, ? ) ? Eq. (8); 7 // Hierarchy-Aware Aggregation; 8 for ? ? D do ? ? Eq. (5); 11 ? ? ? Eq. (6) according to the hierarchy T ? ; 12 for ? ? C (? ) do 13 ? ? ? Eq. (5); 14 for ? ? D do 15 for ? ? C (? ) do 16 score ? (?, ? ) ? Eq. (7); 17 MRR(? |? ) ? Eq. (9); 18 // Self-Training; 19 for ? ? D do 20 ? ? ? Eq. (10); ? ? Eq. (11); 22 Train a Parabel classifier Pr( ?? |? ? ) using ? ? and ? ? ; 23 Get the final ranking list following Example 3.1; 24 Return L ? = {top-? ranked labels of ? };</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Parameter sensitivity analysis on MAG-CS. (a) Effect of the number of trees used in the Parabel classifier. (b) Effect of the maximum number of pseudo labels per paper used for self-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Papers #Labels</cell><cell>#Words/ Paper</cell><cell>#Paragraphs/ Paper</cell><cell>#Labels/ Paper</cell></row><row><cell cols="2">MAG-CS [49] 96,718</cell><cell>10,909</cell><cell>4071.69</cell><cell>45.91</cell><cell>5.84</cell></row><row><cell cols="3">PubMed [28] 251,573 16,070</cell><cell>4901.42</cell><cell>33.38</cell><cell>8.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>LinkBERT<ref type="bibr" target="#b54">[55]</ref> 3 is a structure-enhanced PLM. It uses linked Wikipedia paragraphs as context to perform masked language modeling and document relation prediction. On the PubMed dataset, we use BioLinkBERT4 , which is pre-trained on PubMed papers and performs better than LinkBERT.? SPECTER<ref type="bibr" target="#b8">[9]</ref> 5 is a structure-enhanced scientific PLM. It continues pre-training SciBERT using a citation prediction objective with 684K pairs of linked papers.</figDesc><table><row><cell></cell><cell>SciBERT [2]</cell><cell cols="3">0.6825** 0.5525** 0.4542**</cell><cell>0.5990**</cell><cell>0.5555**</cell><cell cols="3">0.4503** 0.3712** 0.3193**</cell><cell>0.3916**</cell><cell>0.3595**</cell></row><row><cell>Abstract</cell><cell cols="4">OAG-BERT [24] LinkBERT [55] SPECTER [9] 0SHOT-TC [25, 57] 0.6489** 0.5003** 0.4164** 0.5960** 0.4860** 0.4168** 0.6515** 0.5288** 0.4402** 0.7599** 0.5958** 0.4765**</cell><cell>0.5258** 0.5729** 0.6510** 0.5490**</cell><cell>0.5018** 0.5363** 0.5923** 0.5128**</cell><cell cols="3">0.4142** 0.3340** 0.2902** 0.3689** 0.3331** 0.3014** 0.5419** 0.4124** 0.3388** 0.5504** 0.4280** 0.3532**</cell><cell>0.3539** 0.3434** 0.4441** 0.4583**</cell><cell>0.3265** 0.3270** 0.3946** 0.4085**</cell></row><row><cell></cell><cell>MICoL [69]</cell><cell cols="3">0.7658 0.6005** 0.4797**</cell><cell>0.6562**</cell><cell>0.5965**</cell><cell cols="3">0.5775* 0.4329** 0.3528**</cell><cell>0.4680**</cell><cell>0.4133**</cell></row><row><cell></cell><cell>SciBERT [2]</cell><cell cols="3">0.6881** 0.5573** 0.4574**</cell><cell>0.6041**</cell><cell>0.5594**</cell><cell cols="3">0.4885** 0.4176** 0.3517**</cell><cell>0.4375**</cell><cell>0.3965**</cell></row><row><cell></cell><cell>OAG-BERT [24]</cell><cell cols="3">0.5775** 0.4724** 0.4097**</cell><cell>0.5108**</cell><cell>0.4913**</cell><cell cols="3">0.4245** 0.3867** 0.3296**</cell><cell>0.4002**</cell><cell>0.3660**</cell></row><row><cell>Full Text</cell><cell>LinkBERT [55] SPECTER [9] Longformer [3]</cell><cell cols="3">0.6654** 0.5365** 0.4447** 0.7582** 0.5965** 0.4773** 0.6582** 0.5280** 0.4396**</cell><cell>0.5820** 0.6512** 0.5740**</cell><cell>0.5429** 0.5927** 0.5366**</cell><cell cols="3">0.4241** 0.3919** 0.3380** 0.5400** 0.4335** 0.3575** 0.4392** 0.3580** 0.3070**</cell><cell>0.4041** 0.4610** 0.3786**</cell><cell>0.3727** 0.4112** 0.3466**</cell></row><row><cell></cell><cell cols="4">PLM+GAT [48, 54] 0.7306** 0.5760** 0.4663**</cell><cell>0.6285**</cell><cell>0.5765**</cell><cell cols="3">0.5140** 0.4000** 0.3374**</cell><cell>0.4279**</cell><cell>0.3870**</cell></row><row><cell></cell><cell cols="4">GraphFormers [54] 0.7261** 0.5720** 0.4637**</cell><cell>0.6242**</cell><cell>0.5730**</cell><cell cols="3">0.5213** 0.4046** 0.3380**</cell><cell>0.4334**</cell><cell>0.3895**</cell></row><row><cell></cell><cell>FuTex</cell><cell>0.7692</cell><cell>0.6089</cell><cell>0.4914</cell><cell>0.6648</cell><cell>0.6099</cell><cell>0.5900</cell><cell>0.4533</cell><cell>0.3798</cell><cell>0.4869</cell><cell>0.4388</cell></row><row><cell cols="6">entity-aware two-dimensional position embedding to leverage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">paper metadata information (e.g., venues and authors).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Case study on MAG-CS and PubMed. Blue: Labels indicated by the paper title or abstract. Orange: Labels indicated by the paper's full text but not mentioned in the title or abstract. compact modeling technique for outdoor navigation Title: serum calprotectin: a novel diagnostic and prognostic marker in inflammatory bowel diseases Abstract: abstract-in this paper, a new methodology to build compact local maps in real time for outdoor robot navigation is presented. the environment information is obtained from a 3-d scanner laser. the navigation model, which is called traversable region model, is based on a voronoi diagram technique ...</figDesc><table><row><cell></cell><cell cols="3">7601** 0.6013** 0.4859**</cell><cell>0.6567**</cell><cell>0.6031**</cell><cell cols="3">0.5422** 0.4410** 0.3722**</cell><cell>0.4676**</cell><cell>0.4244**</cell></row><row><cell cols="4">FuTex-NoHierarchy 0.7655** 0.6048** 0.4879**</cell><cell>0.6607**</cell><cell>0.6060**</cell><cell cols="3">0.5802** 0.4421** 0.3681**</cell><cell>0.4760**</cell><cell>0.4272**</cell></row><row><cell cols="4">FuTex-NoSelfTrain 0.7673** 0.6040** 0.4827**</cell><cell>0.6591**</cell><cell>0.5993**</cell><cell cols="3">0.5877** 0.4456** 0.3648**</cell><cell>0.4801**</cell><cell>0.4254**</cell></row><row><cell>FuTex</cell><cell>0.7692</cell><cell>0.6089</cell><cell>0.4914</cell><cell>0.6648</cell><cell>0.6099</cell><cell>0.5900</cell><cell>0.4533</cell><cell>0.3798</cell><cell>0.4869</cell><cell>0.4388</cell></row><row><cell></cell><cell cols="2">MAG-CS [49]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PubMed [28]</cell><cell></cell><cell></cell></row><row><cell>Title:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>(abstract) 0.5222** 0.5529** 0.5511** 0.5448** 0.5450** 0.3712** 0.3577** 0.3427** 0.3617** 0.P@? and NDCG@? scores of compared methods on the Art dataset. Bold, *, and **: the same meaning as in Table2.</figDesc><table><row><cell>3519**</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/allenai/scibert_scivocab_uncased</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/THUDM/OAG-BERT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://huggingface.co/michiyasunaga/LinkBERT-base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/michiyasunaga/BioLinkBERT-base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/allenai/specter</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://huggingface.co/roberta-large-mnli</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/yuzhimanhua/MICoL</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://huggingface.co/allenai/longformer-base-4096</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank anonymous reviewers for their valuable and insightful feedback. Research was supported in part by the <rs type="programName">IBM-Illinois Discovery Accelerator Institute, US DARPA KAIROS Program</rs> No. <rs type="grantNumber">FA8750-19-2-1004</rs> and <rs type="funder">INCAS</rs> Program No. <rs type="grantNumber">HR001121C0165</rs>, <rs type="funder">National Science Foundation</rs> <rs type="grantNumber">IIS-19-56151</rs>, <rs type="grantNumber">IIS-17-41317</rs>, and <rs type="funder">IIS</rs> <rs type="grantNumber">17-04532</rs>, and the <rs type="institution">Molecule Maker Lab Institute</rs>: <rs type="programName">An AI Research Institutes program</rs> supported by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2019897</rs>, and the <rs type="institution">Institute for Geospatial Understanding</rs> through an <rs type="grantName">Integrative Discovery Environment (I-GUIDE</rs>) by <rs type="funder">NSF</rs> under Award No. <rs type="grantNumber">2118329</rs>. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of <rs type="affiliation">DARPA</rs> or the <rs type="institution">U.S. Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CXHEVKf">
					<idno type="grant-number">FA8750-19-2-1004</idno>
					<orgName type="program" subtype="full">IBM-Illinois Discovery Accelerator Institute, US DARPA KAIROS Program</orgName>
				</org>
				<org type="funding" xml:id="_bT9AkXk">
					<idno type="grant-number">HR001121C0165</idno>
				</org>
				<org type="funding" xml:id="_ywWFbVs">
					<idno type="grant-number">IIS-19-56151</idno>
				</org>
				<org type="funding" xml:id="_ceEU6fG">
					<idno type="grant-number">IIS-17-41317</idno>
				</org>
				<org type="funding" xml:id="_YsVGFf7">
					<idno type="grant-number">17-04532</idno>
					<orgName type="program" subtype="full">An AI Research Institutes program</orgName>
				</org>
				<org type="funding" xml:id="_BhUk6s2">
					<idno type="grant-number">2019897</idno>
					<orgName type="grant-name">Integrative Discovery Environment (I-GUIDE</orgName>
				</org>
				<org type="funding" xml:id="_Ufp9rAH">
					<idno type="grant-number">2118329</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Construction of the Literature Graph in Semantic Scholar</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The longdocument transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Description based text classification with reinforcement learning</title>
		<author>
			<persName><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1371" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-Scale Multi-Label Text Classification on EU Legislation</title>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Chalkidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Fergadiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6314" to="6322" />
		</imprint>
	</monogr>
	<note>Prodromos Malakasiotis, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Importance of Semantic Representation: Dataless Classification</title>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="830" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dataless text classification with descriptive LDA</title>
		<author>
			<persName><forename type="first">Xingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<idno>AAAI&apos;15</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SPECTER: Document-level Representation Learning using Citationinformed Transformers</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2270" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Medical subject headings used to search the biomedical literature</title>
		<author>
			<persName><forename type="first">H</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">L</forename><surname>Coletti</surname></persName>
		</author>
		<author>
			<persName><surname>Bleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="317" to="323" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FullMeSH: improving large-scale MeSH indexing with full text</title>
		<author>
			<persName><forename type="first">Suyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1533" to="1541" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized Zero-Shot Extreme Multi-label Learning</title>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakina</forename><surname>Bohra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="527" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In WWW&apos;20. 2704-2710</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extreme multilabel loss functions for recommendation, tagging, ranking &amp; other missing label applications</title>
		<author>
			<persName><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MeSH indexing based on automatically generated summaries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">G</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">R</forename><surname>Mork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName><surname>D?az</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12268</idno>
		<title level="m">Patton: Language Model Pretraining on Text-Rich Networks</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective document labeling with very few seed words: A topic model approach</title>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dataless text classification: A topic modeling approach with document manifold</title>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="973" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MeSHLabeler: improving the accuracy of large-scale MeSH indexing by integrating diverse evidence</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="339" to="347" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingnan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>KDD&apos;22. 3418-3428</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">S2ORC: The Semantic Scholar Open Research Corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR&apos;19</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PubMed and beyond: a survey of web tools for searching biomedical literature</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="volume">2011</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contextualized Weak Supervision for Text Classification</title>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">META: Metadata-Empowered Weak Supervision for Text Classification</title>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weakly-supervised neural text classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="983" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised hierarchical text classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6826" to="6833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Text Classification Using Label Names Only: A Language Model Self-Training Approach</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;20</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9006" to="9017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">All-in text: Learning document, label, and word representations jointly</title>
		<author>
			<persName><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneldo</forename><surname>Loza Menc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>F?rnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1948" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Predicting unseen labels using label hierarchies in large-scale multi-label learning</title>
		<author>
			<persName><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneldo</forename><surname>Loza Menc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>F?rnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14424</idno>
		<title level="m">Multi-stage document ranking with bert</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LIME: Weakly-Supervised Text Classification without Seeds</title>
		<author>
			<persName><forename type="first">Seongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihwa</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1083" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DeepMeSH: deep semantic representation for improving large-scale MeSH indexing</title>
		<author>
			<persName><forename type="first">Shengwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising. In WWW</title>
		<author>
			<persName><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrutendra</forename><surname>Harsola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
	<note>Rahul Agrawal, and Manik Varma</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot and zero-shot multi-label learning for structured label spaces</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;18</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">3132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenda</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4239" to="4249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Web-scale system for scientific knowledge exploration</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;18 System Demonstrations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On dataless hierarchical text classification</title>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;14</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1579" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Co-author relationship prediction in heterogeneous bibliographic networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASONAM&apos;11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>NIPS&apos;17. 5998-6008</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ICLR&apos;18</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">X-Class: Text Classification with Extremely Weak Supervision</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3043" to="3053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards Robust Prediction on Tail Labels</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Ping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1812" to="1820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Extreme Zero-Shot Learning for Extreme Text Classification</title>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5455" to="5468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MeSHProbeNet: a self-attentive probe net for MeSH indexing</title>
		<author>
			<persName><forename type="first">Guangxu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishlay</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3794" to="3802" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">GraphFormers: GNN-nested transformers for representation learning on textual graph</title>
		<author>
			<persName><forename type="first">Junhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS&apos;21. 28798-28810</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">LinkBERT: Pretraining Language Models with Document Links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3162" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3905" to="3914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">BERTMeSH: deep contextual representation learning for large-scale highperformance MeSH indexing with full text</title>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="684" to="692" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification</title>
		<author>
			<persName><forename type="first">Ronghui</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS&apos;</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="5820" to="5830" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Oag: Toward linking large-scale heterogeneous entity graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2585" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Integrating Semantic Knowledge to Tackle Zero-shot Text Classification</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyawat</forename><surname>Lertvittayakumjorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1031" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Weaklysupervised Text Classification Based on Keyword Graph</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2803" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hierarchical Metadata-Aware Document Categorization under Weak Supervision</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14232</idno>
		<title level="m">Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Motifclass: Weakly supervised text classification with higher-order metadata information</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1357" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study. In WWW&apos;23</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1626" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">MATCH: Metadata-Aware Text Classification in A Large Hierarchy</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;21</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3246" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boya</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junheng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. In WWW&apos;22</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3162" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
