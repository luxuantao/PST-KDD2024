<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiobjective Reinforcement Learning: A Comprehensive Overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-02-12">February 12, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chunming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Xin</forename><surname>Xu</surname></persName>
							<email>xinxu@nudt.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Dewen</forename><surname>Hu</surname></persName>
							<email>dwhu@nudt.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Mechatronics and Automation</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of Unmanned Systems</orgName>
								<orgName type="department" key="dep2">College of Mechatronics and Automation</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Automatic Control</orgName>
								<orgName type="department" key="dep2">College of Mechatronics and Automation</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiobjective Reinforcement Learning: A Comprehensive Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-02-12">February 12, 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">B0760663EBEB0B134321FE722D38D298</idno>
					<idno type="DOI">10.1109/TSMC.2014.2358639</idno>
					<note type="submission">received February 16, 2014; revised June 11, 2014; accepted August 15, 2014. Date of publication October 8, 2014; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Markov decision process (MDP)</term>
					<term>multiobjective reinforcement learning (MORL)</term>
					<term>Pareto front</term>
					<term>reinforcement learning (RL)</term>
					<term>sequential decision-making</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reinforcement learning (RL) is a powerful paradigm for sequential decision-making under uncertainties, and most RL algorithms aim to maximize some numerical value which represents only one long-term objective. However, multiple longterm objectives are exhibited in many real-world decision and control systems, so recently there has been growing interest in solving multiobjective reinforcement learning (MORL) problems where there are multiple conflicting objectives. The aim of this paper is to present a comprehensive overview of MORL. The basic architecture, research topics, and naïve solutions of MORL are introduced at first. Then, several representative MORL approaches and some important directions of recent research are comprehensively reviewed. The relationships between MORL and other related research are also discussed, which include multiobjective optimization, hierarchical RL, and multiagent RL. Moreover, research challenges and open problems of MORL techniques are suggested.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>be described by a sequence of states, actions, and rewards. This sequential decision process is usually modeled as a Markov decision process (MDP). The rule or strategy for action selection is called a policy. In RL, the agent learns optimal or near-optimal action policies from such interactions in order to maximize some notion of long-term objectives.</p><p>In the past decades, there has been a large number of works on RL theory and algorithms <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. By focusing on the computational efforts along state transition trajectories and using function approximation techniques for estimating value functions or policies, RL algorithms have produced good results in some challenging real-world problems <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. However, despite of many advances in RL theory and algorithms, one remained challenge is to scale up to larger and more complex problems. The scaling problem for sequential decision-making mainly includes the following aspects <ref type="bibr" target="#b10">[11]</ref>. A problem that has a very large or continuous state or action space, a problem that is best described as a set of hierarchically organized tasks and sub-tasks, and a problem that needs to solve several tasks with different rewards simultaneously. An RL problem in the last aspect is called a multiobjective RL (MORL) problem, which refers to the sequential decision making problem with multiple objectives.</p><p>MORL has been regarded as an important research topic, due to the multiobjective characteristics of many practical sequential decision-making and adaptive optimal control problems in the real world. Compared with conventional RL problems, MORL problems require a learning agent to obtain action policies that can optimize two or more objectives at the same time. In MORL, each objective has its own associated reward signal, so the reward is not a scalar value but a vector. When all the objectives are directly related, a single objective can be derived by combining the multiple objectives together. If all the objectives are completely unrelated, they can be optimized separately and we can find a combined policy to optimize all of them. However, if there are conflicting objectives, any policy can only maximize one of the objectives, or realize a trade-off among the conflicting objectives <ref type="bibr" target="#b11">[12]</ref>. Therefore, MORL can be viewed as the combination 2168-2216 c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.</p><p>See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>of multiobjective optimization (MOO) and RL techniques to solve the sequential decision making problems with multiple conflicting objectives.</p><p>In the MOO domain, there are two common strategies <ref type="bibr" target="#b12">[13]</ref>: one is the multiobjective to single-objective strategy and the other is the Pareto strategy. The former strategy is to optimize a scalar value, including the weighted sum method <ref type="bibr" target="#b13">[14]</ref>, the constraint method <ref type="bibr" target="#b14">[15]</ref>, the sequential method <ref type="bibr" target="#b15">[16]</ref>, and the max-min method, etc. <ref type="bibr" target="#b16">[17]</ref>. In these methods, a scalar value is computed from the multiple objectives for the utility of an action decision, and the conventional single-objective optimization (SOO) techniques can be used. The latter strategy is to use the vector-valued utilities. In this case, it is difficult to order the candidate solutions completely, and the Pareto optimality concept is usually used. The Pareto optimal solutions are defined as noninferior and alternative solutions among the candidate solutions, and they represent the optimal solutions for some possible trade-offs among the multiple conflicting objectives <ref type="bibr" target="#b11">[12]</ref>. All Pareto optimal solutions constitute the Pareto front and one major research issue of MOO is to find or approximate the Pareto front.</p><p>Similar to the MOO domain, MORL algorithms can be divided into two classes based on the number of learned policies. One class comprises single-policy MORL approaches and the other, multiple-policy MORL approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Single-policy approaches aim to find the best single policy which represents the preferences among the multiple objectives as specified by a user or derived from the problem domain. The major difference among single-policy approaches is the way for determining and expressing these preferences. The aim of multiple-policy MORL approaches is to find a set of policies that approximate the Pareto front. The main difference among multiple-policy approaches is the approximation scheme for the Pareto front. Major approaches to MORL will be further discussed in Section IV.</p><p>Although there have been some recent advances in different MORL algorithms, many research challenges still remain in developing MORL theory and algorithms for real-world problems. In addition, according to the authors' knowledge, there is only one related survey paper that has been published in the literature. However, it covers the much broader topic of multiobjective sequential decision-making <ref type="bibr" target="#b17">[18]</ref>. Therefore, the aim of this paper is to provide a comprehensive review of MORL principles, algorithms and some open problems. A representative set of MORL approaches are selected to show the overall framework of the field, to present a summary of major achievements, and to suggest some open problems for future research.</p><p>The remainder of this paper is organized as follows. In Section II, the background of MORL is briefly introduced, including MDP, RL, and MOO. The basic architecture, research topics, and naïve solutions of MORL are described in Section III. A representative set of approaches to MORL are reviewed in Section IV. In Section V, some important directions of recent research on MORL are discussed in detail. The related works are introduced in Section VI. Section VII analyzes the challenges and open problems of MORL. Section VIII concludes this comprehensive overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, the necessary backgrounds on MDP models, RL techniques, and MOO problems are introduced. Firstly, an MDP is characterized as the formulation of a sequential decision-making problem. Then, some basic RL techniques are introduced, where the discussion is restricted to finite state and action spaces, since most MORL results up to now are given for finite spaces. At last, the MOO problem is introduced, as well as the concept of Pareto optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MDP Models</head><p>A sequential decision-making problem can be formulated as an MDP which is defined as a 4-tuple {S, A, R, P}. In this 4-tuple, S is the state space of a finite set of states, A is the action space of a finite set of actions, R is the reward function and P is the matrix of state transition probability. After a state transition from state s to state s' when taking action a, p(s, a, s ) and r(s, a, s ) represent the probability and the reward of the state transition, respectively. An action policy of the MDP is defined as a function π : S→Pr(A), where Pr(A) is a probability distribution in A.</p><p>Due to the different influences of future rewards on the present value, there are two different objective functions of an MDP. One is the discounted reward criteria, which is to estimate the optimal policy π * satisfying the following equation:</p><formula xml:id="formula_0">J π * = max π J π = max π E π ∞ t=0 γ t r t (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where γ (1 &gt; γ &gt; 0) is the discount factor and r t =r(x t , a t ) is the reward at time step t, E π [ • ] stands for the expectation with respect to the policy π and the probability matrix P, and J π is the expected total reward. The other one is called the average reward criteria, which is to estimate the optimal policy π * satisfying the following equation:</p><formula xml:id="formula_2">ρ π * = max π ρ π = max π lim n→∞ 1 n n-1 t=0 E π [r t ]<label>(2)</label></formula><p>where ρ π is the average reward per time step for the policy π .</p><p>For the discounted reward criteria, the state value function and the state-action value function for a policy π are defined by</p><formula xml:id="formula_3">V π (s) = E π ∞ t=0 γ t r t s 0 = s (3) Q π (s, a) = E π ∞ t=0 γ t r t s 0 = s, a 0 = a . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>According to the theory of dynamic programming (DP) <ref type="bibr" target="#b19">[20]</ref>, the following Bellman equations are satisfied:</p><formula xml:id="formula_5">V π (s) = E π r(s, a) + γ V π s (5) Q π (s, a) = R(s, a) + γ E π Q π s , a<label>(6)</label></formula><p>where R(s, a) is the expected reward received after taking action a in state s, s is the successive state of s, and π (s, a) represents the probability of action a taken by policy π in state s.</p><p>The optimal state-action value function is defined as</p><formula xml:id="formula_6">Q * (s, a) = max π Q π (s, a).</formula><p>When Q*(s,a) is obtained, the optimal policy π * can be computed by</p><formula xml:id="formula_7">π * (s) = arg max a Q * (s, a)</formula><p>where the optimal policy π * is a deterministic policy, and it is a projection from S to A.</p><p>For the average reward criteria, let ρ π [see <ref type="bibr" target="#b1">(2)</ref>] be the average reward per time step for a policy π . The relative state value function and the relative state-action value function are defined as <ref type="bibr" target="#b3">[4</ref>]</p><formula xml:id="formula_8">V π (s) = ∞ t=0 E π [ r t -ρ π | s 0 = s] Q π (s, a) = ∞ t=0 E π [ r t -ρ π | s 0 = s, a 0 = a]</formula><p>and the following Bellman equations are satisfied:</p><formula xml:id="formula_9">V π (s) = E π r t + V π s -ρ π Q π (s, a) = R(s, a) -ρ π + E π Q π s , a .<label>(7)</label></formula><p>The optimal relative state-action value function for average reward settings satisfies</p><formula xml:id="formula_10">Q * (s, a)+ρ π * = max π Q π (s, a)+ρ π</formula><p>and the optimal policy π * can also be obtained by</p><formula xml:id="formula_11">π * (s) = arg max a Q * (s, a).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Basic RL Algorithms</head><p>The earlier approach to solve MDPs with model information is to use the dynamic programming (DP) techniques, which compute the optimal policies by estimating the optimal state-action value functions. However, traditional DP algorithms commonly require full model information and large amounts of computation are needed for large state and action spaces. Different from DP, a RL agent learns an optimal or near-optimal policy by interacting with the environment whose dynamic model is assumed to be unknown <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>. As indicated in <ref type="bibr" target="#b3">[4]</ref>, based on the observed state transition data of MDPs, RL algorithms integrate the techniques of Monte Carlo, stochastic approximation, and function approximation to obtain approximate solutions of MDPs. As a central mechanism of RL, temporal-difference (TD) learning <ref type="bibr" target="#b4">[5]</ref> can be viewed as a combination of Monte Carlo and DP. On one hand, like Monte Carlo methods, TD algorithms can learn the value functions using state transition data without model information. On the other hand, similar to DP, before a final outcome is obtained, TD methods can update the current estimation of value functions partially based on previous learned results <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>For the discounted reward criteria, Q-learning and Sarsa are the most widely used tabular RL algorithms. The Q-learning algorithm is shown in Algorithm 1, where α is the learning Algorithm 1 Q-Learning Algorithm <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> \\N: The maximum number of episodes 1: Initialize Q(s, a) arbitrarily; 2: repeat (for each episode i) 3: Initialize s; 4:</p><p>repeat (for each step of episode) 5:</p><p>Choose a from s using policy derived from Q(s, a); 6:</p><p>Take action a, observe r, s ; 7:</p><formula xml:id="formula_12">Q(s, a) ← Q(s, a) + α[r + γ max a Q(s , a ) -Q(s, a)]; 8: s ← s ; 9:</formula><p>until s is terminal 10: until i = N Algorithm 2 R-Learning Algorithm <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref> \\ρ: The average reward \\N: The maximum number of episodes 1: Initialize Q(s, a) and ρ arbitrarily; 2: repeat (for each episode i) 3: s ← current state; 4: Select a from s using policy derived from Q(s, a); 5:</p><p>Take action a, observe r, s ; 6:</p><formula xml:id="formula_13">Q(s, a) ← Q(s, a) + α[r -ρ + γ max a Q(s , a )- Q(s, a)]; 7: if Q(s, a) = max a Q(s, a) then 8: ρ ← ρ + β[r -ρ + max a Q(s , a ) -max a Q(s, a)]; 9:</formula><p>end if 10: until i = N rate parameter, and r is the immediate reward. If in the limit the Q values of all admissible state-action pairs are updated infinitely often, and α decays in a way satisfying the usual stochastic approximation conditions, then the Q values will converge to the optimal value Q* with probability 1 <ref type="bibr" target="#b19">[20]</ref>. For the Sarsa algorithm, if each action is executed infinitely often in every state that is visited infinitely often, the action is greedy with respect to the current Q value in the limit, and the learning rate decays appropriately, then the estimated Q values will also converge to the optimal value Q* with probability 1 <ref type="bibr" target="#b20">[21]</ref>.</p><p>For the average reward criteria, R-learning <ref type="bibr" target="#b21">[22]</ref> is the most widely studied RL algorithm based on TD. The major steps of the R-learning algorithm are illustrated in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MOO Problems</head><p>The MOO problem can be formulated as follows <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_14">max F(X) = f 1 (X), f 2 (X), . . ., f m f (X) s.t. g i (X) ≤ 0, i = 1, . . . , m g</formula><p>where the "max" operator for a vector is defined either in the sense of Pareto optimality or in the sense of maximizing a weighted scalar of all the elements, X = [x 1, x 2, . . . x N ] T ∈ R N is the vector of variables to be optimized, functions g i (X) (i = 1, 2, . . . , m g ) are the constraint functions of this problem, and f i (X) (i = 1, 2, . . . , m f ) are the objective functions.</p><p>The optimal solutions of an MOO problem can be described by two concepts. One is the concept of multiobjective to single objective, in which a synthetic objective function is derived, and the optimal solution of this MOO problem can be obtained by solving a SOO problem. The other one is the concept of Pareto dominance and Pareto front <ref type="bibr" target="#b24">[25]</ref>. When a solution A is better than another solution C for at least one objective, and A is also superior or at least equal to C for all the other objectives, the solution A is said to dominate C. In MOO, it is preferable to find all the dominating solutions instead of the dominated ones <ref type="bibr" target="#b24">[25]</ref>.</p><p>In the case of m f = 2, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), solution C is dominated by A and B, and it is hard to compare the overall performance between A and B. As indicated in <ref type="bibr" target="#b24">[25]</ref>, the Pareto front can be generated by deleting all the dominated solutions from the set of all possible solutions. From Fig. <ref type="figure" target="#fig_1">2</ref>(b), it can be seen that the Pareto front is the set of all the black points, and the solutions corresponding to the gray points are dominated by at least one element of the Pareto front. Since it is difficult to obtain the complete Pareto front for any real-world MOO problem, a simplified way for MOO is to find a set of solutions that approximates the real Pareto front <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MORL PROBLEM</head><p>Before providing insights into the current state of the art, and determining some important directions for future research, it is necessary to characterize the basic architecture, main research topics, and naïve solutions of MORL in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic Architecture</head><p>MORL is different from traditional RL in that there are two or more objectives to be optimized simultaneously by the learning agent, where a reward vector is provided for the learning agent at each step. Fig. <ref type="figure" target="#fig_2">3</ref> shows the basic architecture of MORL, where there are N objectives, and r i (N ≥ i ≥ 1) is the ith feedback signal of the agent's current reward vector which is provided by the environment. Obviously this basic architecture illustrates the case of a single agent that has to optimize its action policies for a set of different objectives simultaneously.</p><p>For each objective i (N ≥ i ≥ 1) and a stationary policy π , there is a corresponding state-action value function Q π i (s, a), which satisfies the Bellman equation ( <ref type="formula" target="#formula_5">6</ref>) or <ref type="bibr" target="#b6">(7)</ref>.</p><p>Let</p><formula xml:id="formula_15">MQ π (s, a) = Q π 1 (s, a), Q π 2 (s, a), . . . Q π N (s, a) T Fig. 3. Basic architecture of MORL.</formula><p>where MQ π (s, a) is the vectored state-action value function, and it also satisfies the Bellman equation. The optimal vectored state-action value function is defined as</p><formula xml:id="formula_16">MQ * (s, a) = max π MQ π (s, a) (8)</formula><p>and the optimal policy π * can also be obtained easily by</p><formula xml:id="formula_17">π * (s) = argmax a MQ * (s, a). (<label>9</label></formula><formula xml:id="formula_18">)</formula><p>In this basic architecture, the optimization problems of ( <ref type="formula">8</ref>) and ( <ref type="formula" target="#formula_17">9</ref>) are both MOO problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Major Research Topics</head><p>MORL is a highly interdisciplinary field and it refers to the integration of MOO methods and RL techniques to solve sequential decision making problems with multiple conflicting objectives. The related disciplines of MORL include artificial intelligence, decision and optimization theory, operations research, control theory, and so on. Research topics of MORL are interplayed by MOO and RL, mainly including the preferences among different objectives (the preferences may vary with time), appropriate representations of preferences, the approximation of the Pareto front and the design of efficient algorithms for specific MORL problems. Therefore, one important task of MORL is to suitably represent the designer's preferences or ensure the optimization priority with some policies in the Pareto front. After appropriately expressing the preferences, the remained task is to design efficient MORL algorithms that can solve the sequential decision making problems based on observed state transition data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Naïve Solutions</head><p>Like MOO problems, MORL approaches can be divided into two groups based on the number of the policies to be learned <ref type="bibr" target="#b11">[12]</ref>: single-policy approaches and multiple-policy approaches.</p><p>The aim of single-policy approaches is to obtain the best policy which simultaneously satisfies the preferences among the multiple objectives as assigned by a user or defined by the application domain. In this case, the naïve approach to solve a MORL problem is to design a synthetic objective function TQ(s, a), which can suitably represent the overall preferences. Similar to the Q-learning algorithm, a naïve solution of singlepolicy approaches to MORL is shown in Algorithm 3.</p><p>The Q-value update rule for each objective can be expressed as repeat (for each step of episode) 5:</p><formula xml:id="formula_19">Q i (s, a) = (1 -α)Q i (s, a) + α r i + max a Q i s , a where N ≥ i ≥ 1,</formula><p>Choose a from s using policy derived from TQ(s, a); 6:</p><p>Take action a, observe r 1 , r 2 ,. . . , s ; 7:</p><p>for i = 1, 2, . . . , N do 8:</p><formula xml:id="formula_20">Q i (s, a) ← Q i (s, a) + α[r i + γ max a Q(s , a )- Q i (s, a)]; 9:</formula><p>end for 10:</p><p>Compute TQ(s, a); 11:</p><p>s ← s ; 12: until s is terminal 13: until j = K which can be derived using the Q-values for all the objectives. The major difference among single-policy approaches is the way in which these preferences are expressed. By making use of the synthetic objective function, the Q-values of every objective can be utilized to fairly distribute control actions.</p><p>In order to ensure the diversity in the policy space for different optimization objectives, multiple-policy approaches have been studied to obtain a set of policies that can approximate the Pareto front. The major difference among multiplepolicy approaches is the manner in which the Pareto front is approximated. However, it is hard to approximate the Pareto front directly in many real-world applications. One naïve solution of multiple-policy approaches is to find policies in the Pareto front by using different synthetic objective functions. Obviously, if a set of parameters can be specified in a synthetic objective function, the optimal policy can be learned for this set of parameters. In <ref type="bibr" target="#b26">[27]</ref>, it was illustrated that by running the scalar Q-learning algorithm independently for different parameter settings, the MORL problem can be solved in a multiple-policy way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. REPRESENTATIVE APPROACHES TO MORL</head><p>According to the different representations of preferences, several typical approaches to MORL have been developed. In this section, seven representative MORL approaches are reviewed and discussed. Among these seven representative approaches, the weighted sum approach, W-learning, the analytic hierarchy process (AHP) approach, the ranking approach, and the geometric approach are single-policy approaches. The convex hull approach and the varying parameter approach belong to multiple-policy approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Weighted Sum Approach</head><p>In <ref type="bibr" target="#b27">[28]</ref>, an algorithm based on "greatest mass" was studied to estimate the combined Q-function. For Q-learning based on the strategy of greatest mass, the synthetic objective function is generated by summing the Q-values for all the objectives</p><formula xml:id="formula_21">TQ (s, a) = N i=1 Q i (s, a). (<label>10</label></formula><formula xml:id="formula_22">)</formula><p>Based on the above synthetic objective function, the action with the maximal summed value is then chosen to be executed. Since Sarsa(0) is an on-policy (the samples used for weight update are generated from the current action policy) RL algorithm and it does not have the problem of positive bias, GM-Sarsa(0) was proposed for MORL in <ref type="bibr" target="#b10">[11]</ref>. The positive bias may be caused by some off-policy RL methods which only use the estimates of greedy actions for learning updates. An advantage of GM-Sarsa(0) is as follows: since the updates are based on the actually selected actions rather than the best action determined by the value function, GM-Sarsa(0) is expected to have smaller errors between the estimated Q-values and the true Q-values.</p><p>A natural extension of the GM-Sarsa(0) approach is the weighted sum approach, which computes a linearly weighted sum of Q-values for all the objectives</p><formula xml:id="formula_23">TQ (s, a) = N i=1 w i Q i (s, a).</formula><p>The weights can make the user have the ability to put more or less emphasis on each objective. In <ref type="bibr" target="#b28">[29]</ref>, the weighted sum approach was employed to combine seven vehicle overtaking objectives, and there are three navigation modes that were used to tune the weights. In <ref type="bibr" target="#b29">[30]</ref>, a similar approach was used for the combination of three objectives, which represent the degree of the crowd in an elevator, the waiting time, the number of start-ends, respectively.</p><p>Although the weighted sum approach is very simple to be implemented, the actions in concave regions of the Pareto front may not be chosen so that the Pareto front cannot be well approximated during the learning process <ref type="bibr" target="#b24">[25]</ref>. For example, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>, if there are two objectives and five candidate actions, it can be seen that actions a 2 , a 3 , and a 4 are in the concave region of the Pareto front, while actions a 1 and a 5 are vertices. For all candidate actions for any positive weights {w i } (N ≥ i ≥ 1), the linear weighted sums of Q-values of actions a 2 , a 3 , and a 4 are not the maximum. Thus, actions a 2 , a 3 and a 4 will never be selected for greedy policies. Instead, one of the two actions a 1 and a 5 will be frequently chosen according to the preset weights. In order to overcome this drawback, some nonlinear functions for weighting Q-values may be used for specific problem domains <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. W-Learning Approach</head><p>In order to ensure that the selected action is optimal for at least one objective, several winner-take-all methods for MORL were studied in <ref type="bibr" target="#b31">[32]</ref>. When the current state is s, a value W i (s) is computed for each objective. Then, the selected action is based on the objective with the maximal W value.</p><p>One simple method to compute W values is called Top-Q <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b31">[32]</ref>, which assigns the W value as the highest Q-value in the current state</p><formula xml:id="formula_24">W i (s) = max a Q i (s, a) 1 ≤ i ≤ N.</formula><p>The largest W value can be obtained as</p><formula xml:id="formula_25">W max (s) = max i W i (s)= max i max a Q i (s, a) W max (s) = max a max i Q i (s, a) 1 ≤ i ≤ N (11)</formula><p>therefore the selected action is</p><formula xml:id="formula_26">ã = arg max a max i Q i (s, a) 1 ≤ i ≤ N. (<label>12</label></formula><formula xml:id="formula_27">)</formula><p>The synthetic objective function for the Top-Q approach can be written as</p><formula xml:id="formula_28">TQ(s, a) = max i Q i (s, a) 1 ≤ i ≤ N. (<label>13</label></formula><formula xml:id="formula_29">)</formula><p>In Top-Q, the selected action is guaranteed to be optimal for at least one objective. However, one drawback of this approach is that the objective with the highest Q-value may have similar priorities for different actions, while other objectives cannot be satisfied due to their low action values. In addition, since the Q-values depend on the scaling of reward functions, a change in reward scaling may influence the results of the winnertake-all contest in (11)- <ref type="bibr" target="#b12">(13)</ref>. Therefore, although the Top-Q approach may obtain good performance in some cases, its behavior will be greatly influenced by the design of reward functions <ref type="bibr" target="#b10">[11]</ref>.</p><p>In order to overcome the above drawback, W-Learning was studied in <ref type="bibr" target="#b31">[32]</ref> to compute the W values based on the following rule:</p><formula xml:id="formula_30">W i (s) = (1 -α)W i (s) + αP i (s) P i (s) = max a Q i (s, a) -r i + γ max a Q i s , a<label>(14)</label></formula><p>where N ≥ i ≥ 1, s is the successive state after action a is executed. At each step, after selecting and executing the action with the highest W value, all the W values, except the highest W value (the winner or the leader), are updated according to the above rule in <ref type="bibr" target="#b13">(14)</ref>. Humphrys <ref type="bibr" target="#b31">[32]</ref> pointed out that it may be not necessary to learn the W values, and instead they can be computed directly from the Q-values in a process called Negotiated W-learning, as shown in Algorithm 4. The negotiated W-learning algorithm can explicitly find that if an objective is not preferred to determine the next action, it may be expected to lose the most long-term reward.</p><p>Algorithm 4 Negotiated W-Learning <ref type="bibr" target="#b31">[32]</ref> \\N: The number of objectives 1: Initialize leader l with a random integer between 1 and N; 2: Observe state s; 3: W l =0; 4: a l = arg max a Q l (s, a); 5: loop: 6:</p><p>for all objectives i except l do 7: C. AHP Approach <ref type="bibr" target="#b33">[34]</ref> Generally, the designer of MORL algorithms may not have enough prior knowledge about the optimization problem. In order to express the information of preferences, some qualitative rules are usually employed, such as "objective B is less important than objective A." The qualitative rules specify the relative importance between two objectives but do not provide a precise mathematical description. Thus, MORL algorithms can make use of the method of AHP to obtain a quantified description of the synthetic objective function TQ(s, a). Compared with the original AHP method in <ref type="bibr" target="#b32">[33]</ref>, the MORL method proposed in <ref type="bibr" target="#b33">[34]</ref> can solve sequential decision-making problems with variable number of objectives.</p><formula xml:id="formula_31">W i = max a Q i (s, a) -Q i (s,</formula><p>Based on the designer's prior knowledge of the problem, the degree of relative importance between two objectives can be quantified by L grades, and a scalar value is defined for each grade. For example, in <ref type="bibr" target="#b33">[34]</ref>, L is set to be 6, and the evaluation of the importance of objective i relative to objective j is supposed to be c ij , where N ≥ i ≥ 1 and N ≥ j ≥ 1. After determining the value of c ij , the relative importance matrix for all objectives C = (c ij ) N ×N can be obtained.</p><p>With matrix C, the importance factor I i (for objective i) can be calculated <ref type="bibr" target="#b33">[34]</ref> </p><formula xml:id="formula_32">I i = SL i N j=1 SL j</formula><p>where</p><formula xml:id="formula_33">SL i = N j=1,j =i c i,j</formula><p>is the importance of objective i relative to all other objectives. Then, for each objective, a fuzzy inference system can be constructed. To compare two candidate actions a p and a q (a p , a q ∈ A), both the importance factor I i and the value of improvement D i (a p , a q ) = Q i (s, a p ) -Q i (s, a q ) are used as the inputs of the fuzzy system, and the output of the fuzzy system is the "goodness" of a p relative to a q . By incorporating the fuzzy subsets and fuzzy inference rules, an action selection strategy was constructed to solve the MORL problem <ref type="bibr" target="#b33">[34]</ref>. The main drawback of this approach is that it requires a lot of prior knowledge of the problem domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ranking Approach</head><p>The ranking approach, also called the sequential approach, or the threshold approach, aims to solve multiobjective decision problems via an ordering or a preference relation among multiple criteria. The idea of using ordinal relations in optimal decision making was studied in the early research work by Mitten <ref type="bibr" target="#b34">[35]</ref> and Sobel <ref type="bibr" target="#b35">[36]</ref>. The synthetic objective function TQ(s, a) was expressed in terms of "partial policies." To ensure the effectiveness of the subordinate objective (the less important objective), multiple solutions need be obtained for the optimization problem of the main objective.</p><p>Inspired by the idea of the ranking approach, an ordering of multiple objectives was established in <ref type="bibr" target="#b36">[37]</ref> for MORL where threshold values were specified for some objectives in order to put the constraints on the objectives. One example for this kind of situations is that an unmanned vehicle performs navigation tasks in an environment while avoiding its fuel level from being empty. The MORL approach in <ref type="bibr" target="#b36">[37]</ref> optimizes one objective while putting constraints on other objectives. The actions are chosen based on the thresholds and a lexicographic ordering (the last objective is maximized at first) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>Let</p><formula xml:id="formula_34">CQ i (s, a) = min {Q i (s, a) , C i }</formula><p>where N ≥ i ≥ 1, C i is the threshold value (the maximum allowable value) for objective i. Since objective N is assumed to be unconstrained, C N = +∞. In the ranking approach for MORL <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b36">[37]</ref>, given a partial ordering of all objectives and their threshold values, actions a and a' can be compared by the action comparison mechanism shown in Algorithm 5, where there is a sub-function Superior() which was recursively defined in <ref type="bibr" target="#b11">[12]</ref>. Based on the action comparison and selection mechanism, the MORL problem can be solved by combining this mechanism with some standard RL algorithms such as Q-learning.</p><p>The performance of the ranking-based MORL approach is mainly dependent on the ordering of the objectives as well as the threshold values. The design of an appropriate lexicographic ordering of all the objectives and their threshold values still requires some prior knowledge of the problem domain, which exhibits the designer's preferences <ref type="bibr" target="#b104">[105]</ref>. Geibel <ref type="bibr" target="#b37">[38]</ref> employed this idea to balance the expected return and risk, where the risk is smaller than some specified threshold, and the problem was formulated as a constrained MDP. In <ref type="bibr" target="#b38">[39]</ref>, the ranking-based MORL approach was applied to the routing problem in cognitive radio networks to address Algorithm 5 Action Comparison Mechanism of the Ranking Approach <ref type="bibr" target="#b11">[12]</ref> Superior(CQ(s i , a ), CQ(s i , a), i);  the challenges of randomness, uncertainty, and multiple metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Geometric Approach</head><p>To deal with dynamic unknown Markovian environments with long-term average reward vectors, Mannor and Shimkin <ref type="bibr" target="#b39">[40]</ref> proposed a geometric approach to MORL. It is assumed that the actions of other agents may influence the dynamics of the environment. Sufficient conditions for state recurrence, i.e., the game is irreducible or ergodic, are also assumed to be satisfied. In <ref type="bibr" target="#b39">[40]</ref>, using the proposed geometric-based idea, two MORL algorithms, called multiple directions RL (MDRL) and single direction RL (SDRL), were presented to approximate a desired target set in a multidimensional objective space, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. This target set can be viewed as the synthetic objective function TQ(s, a), which satisfies some geometric conditions of a dynamic game model. The MDRL and SDRL algorithms are based on the reachability theory for stochastic games, and the main mechanism behind these two algorithms is to steer the average reward vector to the target set.</p><p>When a nested class of target sets is prescribed, Mannor and Shimkin <ref type="bibr" target="#b39">[40]</ref> also provided an extension of the geometric MORL algorithm, where the goal of the learning agent was to approach the target set with the smallest size. A particular example of this case is to solve constrained MDPs with average rewards. In addition, the geometric algorithms also need some prior knowledge of the problem domain to define the target set.</p><p>Algorithm 6 Convex Hull Algorithm <ref type="bibr" target="#b41">[42]</ref> 1: Initialize Q(s, a), for all s, a, arbitrarily; 2: while not converged do 3:</p><p>for all s ∈ S, a ∈ A do 4:</p><p>Q(s, a) ← E r(s, a) + γ hull d Q(s , a )|s, a ; 5: end for 6: end while 7: Return Q;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Convex Hull Approach</head><p>Barrett and Narayanan <ref type="bibr" target="#b41">[42]</ref> presented a multiple-policy algorithm to MORL, which can simultaneously learn optimal policies for all linear preference assignments in the objective space. Two operations on convex hulls are defined as follows.</p><p>Definition 1 <ref type="bibr" target="#b41">[42]</ref>: Translation and scaling operations</p><formula xml:id="formula_35">u + bV ≡ { u + b v| v ∈ V}. (<label>15</label></formula><formula xml:id="formula_36">)</formula><p>Definition 2 <ref type="bibr" target="#b41">[42]</ref>: Summing two convex hulls</p><formula xml:id="formula_37">U + V ≡ hull { u + v| u ∈ U, v ∈ V} (<label>16</label></formula><formula xml:id="formula_38">)</formula><p>where u and v are vectors in convex hulls U and V, respectively. Equation <ref type="bibr" target="#b14">(15)</ref> indicates that the convex hull of a transformed (translation and scaling) convex hull is just itself and ( <ref type="formula" target="#formula_37">16</ref>) indicates that the convex hull of two summed convex hulls requires more computation. Based on the definitions, the convex hull algorithm is illustrated Algorithm 6, where Q is the vertices of the convex hull of all possible Q-value vectors. This algorithm can be viewed as an extension of standard RL algorithms where the expected rewards used to update the value function are optimal for some linear preferences. <ref type="bibr">Barrett and Narayanan [42]</ref> presented this algorithm and proved that the solution can find the optimal policy for any linear preference function. This solution can be simplified as the standard value iteration algorithm when a special weight vector is used.</p><p>This convex-hull technique can be integrated with other RL algorithms. Since multiple policies are learned at once, the integrated RL algorithms should be off-policy algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Varying Parameter Approach</head><p>Generally, a multiple-policy approach can be realized by performing multiple runs with different parameters, objective thresholds, and orderings in any single-policy algorithm. For example, as indicated in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b26">[27]</ref>, scalarized Q-learning can be used in a multiple-policy manner by executing repeated runs of the Q-learning algorithm using different parameters.</p><p>Shelton <ref type="bibr" target="#b42">[43]</ref> applied policy gradient methods and the idea of varying parameters to the MORL domain. In the proposed approach, after estimating multiple policy gradients for each objective, a weighted gradient is obtained and multiple policies can be found by varying the weights of the objective gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I REPRESENTATIVE APPROACHES TO MORL</head><p>In summary, the representative approaches mentioned above to MORL can be briefly described in Table <ref type="table">I</ref>. In addition to these approaches, there are some other MORL approaches proposed recently <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>. The algorithm proposed in <ref type="bibr" target="#b43">[44]</ref> is an extension of multiobjective fitted Q-iteration (FQI) <ref type="bibr" target="#b53">[54]</ref> that can find control policies for all the linear combinations of preferences assigned to the objectives in a single training procedure. By performing single-objective FQI in the state-action space, as well as the weight space, the multiobjective FQI (MOFQI) algorithm realizes function approximation and generalization of the action-value function with vectored rewards. In <ref type="bibr" target="#b44">[45]</ref>, different immediate rewards were given for the visited states by comparing the objective vector of the current state with those of the Pareto optimal solutions that have been computed. After keeping track of the nondominated solutions and constructing the Pareto front at the end of the optimization process, the Pareto optimal solutions can be memorized in an elite list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPORTANT DIRECTIONS OF RECENT RESEARCH ON MORL</head><p>Although the MORL approaches summarized in Section IV are very promising for further applications, there are several important directions in which MORL approaches have been improved recently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Further Development of MORL Approaches</head><p>In order to obtain suitable representations of the preferences and improve the efficiency of MORL algorithms, a lot of efforts have been made recently. Estimation of distribution algorithms (EDA) was used by Handa <ref type="bibr" target="#b45">[46]</ref> to solve MORL problems. By incorporating the notions in evolutionary MOO, the proposed method was able to acquire various strategies by a single run. Studley and Bull <ref type="bibr" target="#b46">[47]</ref> investigated the performance of a learning classifier system for MORL, and the results demonstrated that the choice of action-selection policies can greatly affect the performance of the learning system. Kei et al. <ref type="bibr" target="#b47">[48]</ref> selected effective action candidates by the α-domination strategy and used a goal-directed bias based on the achievement level of each evaluation. Zhang et al. <ref type="bibr" target="#b48">[49]</ref> and Zhao and Zhang <ref type="bibr" target="#b49">[50]</ref> used a parallel genetic algorithm (PGA) to evolve a neuro-controller, and perturbation stochastic approximation (SPSA) was used to improve the convergence of the proposed algorithm. The notion of adaptive margins <ref type="bibr" target="#b50">[51]</ref> was adopted to improve the performance of MORL algorithms <ref type="bibr" target="#b51">[52]</ref>. To obtain Pareto optimal policies in large or continuous spaces, some recent efforts include: Khamis and Gomaa <ref type="bibr" target="#b52">[53]</ref> developed a multiagent framework for MORL in traffic signal control, Castelletti et al. <ref type="bibr" target="#b53">[54]</ref> made use of the FQI for approximating the Pareto front, and Wiering and de Jong <ref type="bibr" target="#b54">[55]</ref> proposed a new value iteration algorithm, called consistency multi objective dynamic programming.</p><p>Most of the MORL algorithms mentioned above belong to single-policy approaches and they were studied and tested independently. The main limitation is that the small scale of previous MORL problems may not verify the algorithm's performance in dealing with a wide range of different problem settings, and the algorithm implementations always require much prior knowledge about the problem domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Preferences in MORL</head><p>Since many real-world optimization and control problems are not stationary, there are growing interests in solving dynamic optimization problems in recent years. Similarly, the policies obtained by MORL approaches usually rely on the preferences on the reward vectors, so it is necessary to develop MORL algorithms that take dynamic preferences into consideration.</p><p>In optimal control with multiple objectives, the designer can use the fixed-weight approach <ref type="bibr" target="#b55">[56]</ref> to determine the optimization direction. However, if there is no exact background knowledge of the problem domain, the fixed-weight approach may find unsatisfactory solutions. For MOO, the randomweight approach <ref type="bibr" target="#b56">[57]</ref> and the adaptive approach <ref type="bibr" target="#b57">[58]</ref> were studied to compute the weights based on the objective data so that the manual selection of objective weights can be simplified. Nevertheless, it is hard for these two approaches to express the preference of the designer.</p><p>An important problem is to automatically derive the weights when the designer is unable to assign the weights. This problem is usually called preference elicitation (PE), and its major related work is inverse RL <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>. In order to predict the future decisions of an agent from its previous decisions, a Bayesian approach was proposed <ref type="bibr" target="#b60">[61]</ref> to learn the utility function. In <ref type="bibr" target="#b61">[62]</ref>, an apprenticeship learning algorithm was presented in which observed behaviors were used to learn the objective weights of the designer.</p><p>Aiming at the time-varying preferences among multiple objectives, Natarajan and Tadepalli <ref type="bibr" target="#b62">[63]</ref> proposed an MORL Algorithm 7 Algorithm Schema for Dynamic MORL <ref type="bibr" target="#b62">[63]</ref> 1: Obtain the current weight vector w new , set δ as a threshold value; 2: π init = arg max π ∈ ( w new ρ π ) 3: Compute the value function vectors of π init 4: Compute the average reward vector of π init 5: Learn the new policy π through vector-based RL; 6: If ( w new • ρ πw new • ρ π init ) &gt; δ, add π to the set of stored policies.</p><p>method that can find and keep a finite number of policies which can be appropriately selected for varying weight vectors. This algorithm is based on the average reward criteria, and its schema is shown in Algorithm 7, where δ is a tunable parameter. The motivation for this algorithm is that despite of infinitely many weight vectors, the set of all optimal policies may be well represented by a small number of optimal policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of MORL Approaches</head><p>As a relatively new field of study, research on MORL has mainly focused on various principles and algorithms to deal with multiple objectives in sequential decision-making problems. Although it is desirable to develop standard benchmark problems and methods for the evaluation of MORL algorithms, there has been little work on this topic except a recent work by Vamplew et al. <ref type="bibr" target="#b11">[12]</ref>. In previous studies on MORL, the algorithms were usually evaluated on different learning problems so that it is necessary to define some standard test problems with certain characteristics for a rigorous performance testing of MORL algorithms. For example, many MORL algorithms were tested separately on some ad hoc problems, including tasks <ref type="bibr" target="#b63">[64]</ref>- <ref type="bibr" target="#b66">[67]</ref>, medical tasks <ref type="bibr" target="#b67">[68]</ref>, robot tasks <ref type="bibr" target="#b68">[69]</ref>- <ref type="bibr" target="#b70">[71]</ref>, network routing tasks <ref type="bibr" target="#b71">[72]</ref>, grids tasks <ref type="bibr" target="#b72">[73]</ref>, and so on. As indicated in <ref type="bibr" target="#b11">[12]</ref>, it is difficult to make comparisons among these algorithms due to the lack of benchmark test problems and methodologies. Furthermore, for ad hoc application problems, the Pareto fronts are usually unknown and it is hard to find absolute performance measures for MORL algorithms. Therefore, a suite of benchmark problems with known Pareto fronts were proposed in <ref type="bibr" target="#b11">[12]</ref>, together with a standard method for performance evaluation which can serve as a basis for future comparative studies. In addition, two classes of MORL algorithms were evaluated in <ref type="bibr" target="#b11">[12]</ref> based on different evaluation metrics. In particular, single policy MORL algorithms were tested via online learning while offline learning performance was tested for multipolicy approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED FIELDS OF STUDY</head><p>MORL is a highly interdisciplinary field, and its related fields mainly include MOO, hierarchical RL (HRL) and multiagent RL (MARL). Their relations to MORL and recent progress are discussed in this section. In addition, by using a keyword-based search in Google scholar, the average number of MORL-related publications was about 90 per year from 1999 to 2001, while this average number becomes 700 per year from 2011 to 2013. Thus, it can be observed that during the past 10 years, there is a significant increase in the number of publications related to MORL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MOO</head><p>MOO problems usually have no unique, optimal solution, which is different from SOO that has a single best solution. For MOO and MORL, a set of noninferior, alternative solutions called the Pareto optimal solutions can be defined instead of a single optimal solution. Nevertheless, the aim of MOO is to solve a parameter optimization problem with multiple objectives, while MORL is to solve sequential decision making problems with multiple objectives.</p><p>There are two common goals for MOO algorithms: one is to find a set of solutions that is close to the Pareto optimal front, and the other is to obtain a diverse set of solutions representing the whole Pareto optimal front. These two goals have also been studied in MORL. In order to achieve these two goals, a variety of algorithms have been presented to solve MOO problems, such as multiobjective particle swarm optimizer (MOPSO) <ref type="bibr" target="#b73">[74]</ref>, multiobjective genetic algorithms (MOGA) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b74">[75]</ref>, etc. These MOO algorithms have improved mathematical characteristics for solving various MOO problems. Nevertheless, when the dimension of MOO problems is high, many of these algorithms usually have decreased performance due to the difficulty of finding a wide range of alternative solutions. Moreover, it is hard for MOGA and MOPSO to solve MOO problems with concave Pareto fronts which are popularly encountered in real-world applications.</p><p>Most MOO problems have different kinds of constraints and they are also called constrained MOO problems. Until now, various constraint handling approaches <ref type="bibr" target="#b75">[76]</ref>- <ref type="bibr" target="#b78">[79]</ref> have been proposed to improve the performance of multi objective evolutionary algorithm and MOPSO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. HRL</head><p>Hierarchical RL (HRL) makes use of a divide-and-conquer strategy to solve complex tasks with large state or decision spaces. Unlike conventional RL, HRL aims to solve sequential decision-making problem that can be best described as a set of hierarchically organized tasks and sub-tasks. MORL differs from HRL in that the MORL problem requires the learning agent to solve several tasks with different objectives at once. The most outstanding advantage of HRL is that it can scale to large and complex problems <ref type="bibr" target="#b79">[80]</ref>. One common feature of HRL and MORL is that there are multiple tasks that need to be solved for the learning agent.</p><p>Earlier HRL algorithms need prior knowledge about the high-level structure of complex MDPs. There have been some HRL approaches or formulations to incorporate prior knowledge: HAMs <ref type="bibr" target="#b80">[81]</ref>, MAXQ <ref type="bibr" target="#b81">[82]</ref>, options <ref type="bibr" target="#b82">[83]</ref>, and ALisp <ref type="bibr" target="#b83">[84]</ref>. The usage of the prior knowledge can simplify the problem decomposition and accelerate the learning process for good policies. Currently, most HRL algorithms are based on the semi-MDP (SMDP) model. In <ref type="bibr" target="#b84">[85]</ref>, the SMDP framework was extended to concurrent activities, multiagent domains, and partially observable states. As discussed in <ref type="bibr" target="#b102">[103]</ref>, although value function approximators can be integrated with HRL, few successful results have been reported in the literature for applying existing HRL approaches to MDPs with large or continuous spaces. Furthermore, to automatically decompose the state space of MDPs or construct options is still a difficult task.</p><p>Recently some new HRL algorithms were proposed. In <ref type="bibr" target="#b85">[86]</ref>, an HRL approach was presented where the state space can be partitioned by critical state. A hierarchical approximate policy iteration (HAPI) algorithm with binary-tree state space decomposition was presented in <ref type="bibr" target="#b86">[87]</ref>. In the HAPI approach, after decomposing the original MDP into multiple sub-MDPs with smaller state spaces, better near-optimal local policies can be found and the final global policy can be derived by combining the local policies in each sub-MDP. For MORL problems, a hierarchical learning architecture with multiple objectives was proposed in <ref type="bibr" target="#b87">[88]</ref>. The major idea of this approach is to make use of a reference network so that an internal reinforcement representation can be generated during the operation of the learning process. Furthermore, internal reinforcement signals from different levels can be provided to represent multilevel objectives for the learning system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MARL</head><p>In MORL, the learning agent aims to solve sequential decision problems with reward vectors, and multiple policies may be obtained separately through decomposition. For multiple policy approaches, the MORL problem can be solved in a distributed manner which is closely related to multiagent RL (MARL) <ref type="bibr" target="#b88">[89]</ref>. In MARL, each agent may also have its own objective and there may be multiple objectives in the learning system. However, most of the efforts in designing MARL systems have been focused on the communication and interaction (cooperation, competition, and mixed strategies) among agents. In MORL, usually there is no explicit sharing of information among objectives. The main task of an MARL system is that autonomous multiple agents explicitly consider other agents and coordinate their action policies so that a coherent joint behavior can be realized.</p><p>In recent years, the research results on MARL can be viewed as a combination of temporal-difference learning, game theory, and direct policy search techniques. There are several major difficulties in the research of MARL algorithms, which are different from single-agent settings. The first difficulty is the existence of multiple equilibriums. In Markov games with a single equilibrium value, the optimal policy can be well defined. But when there are multiple equilibriums, MARL algorithms have to ensure the agents to coordinate their policies for selecting appropriate equilibriums. From an empirical viewpoint, we should carefully consider the influence of multiple equilibriums on MARL algorithms, and undesirable equilibriums may be easily reached due to certain game properties <ref type="bibr" target="#b89">[90]</ref>. In the design of MARL algorithms, one major aim is to make the agents' policies converge to desirable equilibriums. There have been many heuristic exploration strategies proposed in the literature so that the probability for reaching the optimal equilibriums can be improved in identical interest games <ref type="bibr" target="#b88">[89]</ref>- <ref type="bibr" target="#b91">[92]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CHALLENGES AND OPEN PROBLEMS</head><p>In addition to the three important aspects for further development in Section V, there are several challenges and open problems in MORL, which will be discussed in the sequel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. State/Feature Representation in MORL</head><p>The problem of selecting an efficient state or feature representation in a real-world problem has been an important topic in RL <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In the earlier research on RL, there has been some important progress in RL theory and algorithms for discrete-state MDPs. However, most real-world problems have large or continuous state spaces, thus, the huge computational costs will make earlier tabular RL algorithms be impractical for real applications. In the past decade, approximate value functions or policies with feature representations have been widely studied. But one main obstacle is that many RL algorithms and theories with good convergence properties usually rely on manually selected feature representations. Thus, it is difficult to accurately approximate the optimal value functions or policies without carefully selected features. Some recent advances in automatic feature representation include kernel methods and graph Laplacian approaches for RL <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref>. In MORL, there are multiple objectives to be achieved and multiple value functions may need to be approximated simultaneously. The feature representation problem in MORL is more complicated due to the existence of multiple objectives or value functions. Therefore, state or feature representation in MORL is still a big challenge for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Value Function Approximation in MORL</head><p>As discussed above, to solve MDPs with large or continuous state spaces, value function approximation (VFA) is a key technique to realize generalization and improve learning efficiency <ref type="bibr" target="#b94">[95]</ref>. According to the basic properties of function approximators, there are two different kinds of VFA methods, i.e., linear VFA <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b96">[97]</ref> and nonlinear VFA <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref>. In some real-world applications, multilayer neural networks were commonly employed as the nonlinear approximators for VFA. However, the empirical results of successful RL applications using nonlinear VFA commonly lack rigorous theoretical analysis. Some negative results concerning divergence were reported for Q-learning and TD learning based on direct gradient rules <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Hence, many RL approaches with VFA require significant design efforts or problem insights, and it is hard to find a basis function set that is both sufficiently simple and sufficiently reliable. To solve MDPs with large or continuous state spaces, MORL algorithms also require VFA to improve generalization ability and reduce computational costs. The additional representation of the preferences among different objectives is more difficult for developing VFA techniques, especially when the MORL problem has dynamic preferences. Hence, VFA becomes a greater challenge for MORL than that for standard RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence Analysis of MORL Algorithms</head><p>Both the Q-learning algorithm and the Sarsa algorithm have some attractive qualities as basic approaches to RL. The major advantage is the fact that they are guaranteed to converge to the optimal solution for a single MDP with discrete state and action spaces. Suppose there are N objectives in an MORL problem, then this problem can be considered as N sub-MDPs (a sub-MDP is an MDP with one single objective). Hence, the convergence results of the algorithm to solve this MORL problem not only depend on the convergence of all the algorithms to solve these sub-MDPs but also depend on the representations of the preferences among all the objectives. The convergence of single-policy approaches to MORL can be analyzed based on the results of the RL algorithms used to solve sub-MDPs. However, the convergence of multiple-policy approaches has to consider the way how the Pareto front is approximated. In short, for stationary preferences, the convergence analysis of MORL algorithms is mainly dependent on the properties of the learning algorithms to solve MDPs and the representations of the preferences. For dynamic preferences, the dynamic characteristics must be considered additionally. So far, the convergence of MORL algorithms commonly lacks rigorous theoretical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MORL in Multiagent Systems</head><p>A multiagent system (MAS) is a system that has multiple interacting autonomous agents, and there are increasing numbers of application domains that are more suitable to be solved by a multiagent system instead of a centralized single agent <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b99">[100]</ref>. MORL in multiagent systems is a very important research topic, due to the multiobjective nature of many practical multiagent systems. How to extend the algorithms and theories of MORL in single-agent systems to MORL in multiagent systems is an open problem. In particular, to achieve several objectives at once by the cooperation of multiple agents in multiagent systems is a difficult problem to be solved. If there are competitions in multiagent systems, the MORL problem will become more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Applications of MORL</head><p>In recent years, RL has been applied in a variety of fields <ref type="bibr" target="#b100">[101]</ref>- <ref type="bibr" target="#b103">[104]</ref>, but the research on MORL algorithms is a relatively new field of study, and as such there are few realworld applications so far. The following difficult problems should be studied before MORL can be successfully applied in real-time complex systems. One is to develop efficient MORL algorithms that can solve MDPs with large or continuous state spaces. The other is to evaluate the performance of different MORL algorithms in practice and find the gap between theoretical analysis and real performance. The third one is to consider the constraints from real-world applications when developing theoretical models for MORL and to investigate how theoretical results can promote the development of new algorithms and mechanisms.</p><p>Since in many real-world applications, there are multiple conflict objectives to be optimized simultaneously, it can be expected that MORL will find more application domains with the development of new theory and algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, the background, basic architectures, major research topics, and naïve solutions of MORL were introduced at first, then several representative approaches were reviewed and some important directions of recent research were discussed in detail. There are two main advantages of MORL. One is that MORL is very useful to improve the performance of single objective RL by generating highly diverse Pareto-optimal models for constructing policy ensembles in domains with multiple objectives. The second is that MORL algorithms can realize a trade-off between accuracy and interpretability for sequential decision-making tasks.</p><p>For single-policy approaches, the weighted sum and W-learning approaches are very simple to implement, but they cannot express exactly the preferences of the designer. The AHP, ranking, and geometric approaches may express the preferences more exactly, but they need more prior knowledge of the problem domain. For multiple-policy approaches, the convex hull algorithm can learn optimal policies for all linear preference assignments over the objective space at once. The varying parameter approach can be easily implemented by performing multiple runs with different parameters, objective threshold values, and orderings in any single-policy algorithm.</p><p>MORL approaches have been improved recently in three important aspects: enhancing their solution qualities, adapting dynamic preferences and constructing evaluation systems. The main challenges and open problems in MORL include value function approximation, feature representation, convergence analysis of algorithms and the application of MORL to multiagent systems and real world problems. It can be expected that there will be more and more research progress toward these directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Basic RL scenario.</figDesc><graphic coords="1,369.50,178.93,136.56,52.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Concepts of Pareto dominance and Pareto front [25]. (a) Pareto dominance. (b) Pareto front.</figDesc><graphic coords="4,58.49,52.79,232.32,110.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 3</head><label>3</label><figDesc>and α is the learning rate parameter. The overall single policy can be determined based on TQ(s, a), Naïve Solution of Single-Policy Approaches to MORL \\K: The maximum number of episodes \\N: The number of objectives 1: Initialize TQ(s, a) arbitrarily; 2: repeat (for each episode j)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Concave region of the weighted sum approach.</figDesc><graphic coords="5,378.50,53.40,118.08,106.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Predicted target set (two objectives).</figDesc><graphic coords="7,379.00,239.51,117.12,108.12" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the Associate Editors and anonymous reviewers for their valuable comments and suggestions, which greatly improved the quality of this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the Program for New Century Excellent Talents in Universities under Grant NCET-10-0901 and in part by the National Fundamental Research Program of China under Grant 2013CB329401. This paper was recommended by Associate Editor A.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Thorndike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Intelligence</title>
		<imprint>
			<date type="published" when="1911">1911</date>
			<publisher>Halfner</publisher>
			<pubPlace>Darien, CT, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing machinery and intelligence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="433" to="460" />
			<date type="published" when="1950-10">Oct. 1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to predict by the method of temporal differences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming: An introduction</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel based least-squares policy iteration for reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2007-07">Jul. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Elevator group control using multiple reinforcement learning agents</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Crites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="262" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Practical issues in temporal difference learning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="257" to="277" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple-goal reinforcement learning with modular Sarsa(0)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sprague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Joint Conf</title>
		<meeting>18th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1445" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Empirical evaluation methods for multiobjective reinforcement learning algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dazeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Issabekov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dekker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="51" to="80" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Genetic Algorithms and Engineering Optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mitsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Runwei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Tsinghua Univ. Press</publisher>
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive weighted sum method for multiobjective optimization: A new method for Pareto front generation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>De Weck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct. Multidiscipl. Optim</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="116" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-objective optimization using genetic algorithms: A tutorial</title>
		<author>
			<persName><forename type="first">A</forename><surname>Konaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Coitb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab. Eng. Syst. Safety</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="992" to="1007" />
			<date type="published" when="2006-09">Sep. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sequential Approximate Multiobjective Optimization Using Computational Intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On min-norm and min-max methods of multi-objective optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of multi-objective sequential decision-making</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roijers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dazeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="113" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Handbook of Learning and Approximate Dynamic Programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Wiley-IEEE Press</publisher>
			<pubPlace>Hoboken, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the convergence of stochastic iterative dynamic programming algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1185" to="1201" />
			<date type="published" when="1994-11">Nov. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convergence results for single-step on-policy reinforcement learning algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="308" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A reinforcement learning method for maximizing undiscounted rewards</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf. Mach. Learn</title>
		<meeting>10th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-objective optimization by reinforcement learning for power system dispatch and voltage stability</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Innov. Smart Grid Technol. Conf. Eur</title>
		<meeting>Innov. Smart Grid Technol. Conf. Eur<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hybrid evolutionary multi-objective optimization with enhanced convergence and diversity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sindhya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Dept. Math. Inf. Tech., Univ. Jyvaskyla</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Jyvaskyla, Finland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the limitations of scalarisation for multi-objective reinforcement learning of Pareto fronts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yearwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dazeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Aust. Joint Conf</title>
		<meeting>21st Aust. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5360</biblScope>
			<biblScope unit="page" from="372" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Performance assessment of multiobjective optimizers: An analysis and review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laumanns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Da Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="132" />
			<date type="published" when="2003-04">Apr. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reinforcement learning in the operational management of a water system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castelletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rizzolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soncinie-Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFAC Workshop Model</title>
		<meeting>IFAC Workshop Model<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="325" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to solve multiple goals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Rochester</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Rochester, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multiple goal reinforcement learning method for complex vehicle overtaking maneuvers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C K</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H C</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-adaptive multi-objective optimization method design based on agent reinforcement learning for elevator group control systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th World Congr</title>
		<meeting>8th World Congr<address><addrLine>Jinan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2577" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Managing power consumption and performance of computing systems using reinforcement learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1497" to="1504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action selection methods using reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Humphrys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Maes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-A</forename><surname>Mataric</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Meyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Pollack</surname></persName>
		</editor>
		<editor>
			<persName><surname>Wilson</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="134" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A multi-objective optimization genetic algorithm incorporating preference information</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">N</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Control</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="746" to="753" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning algorithm for MOSDMP in unknown environment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th World Congr</title>
		<meeting>8th World Congr</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3190" to="3194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Composition principles for synthesis of optimum multistage processes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Mitten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="610" to="619" />
			<date type="published" when="1964-08">Aug. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ordinal dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manage. Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="967" to="975" />
			<date type="published" when="1975-05">May 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-criteria reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Int. Conf. Mach. Learn</title>
		<meeting>15th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="197" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reinforcement learning with bounded risk</title>
		<author>
			<persName><forename type="first">P</forename><surname>Geibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. Mach. Learn</title>
		<meeting>18th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning based routing in cognitive radio networks: Walking in a random maze</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Netw</title>
		<meeting>Int. Conf. Comput. Netw</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="359" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A geometric approach to multi-criterion reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shimkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="325" to="360" />
			<date type="published" when="2004-01">Jan. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The steering approach for multi-criteria reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shimkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1563" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning all optimal policies with multiple criteria</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. Mach. Learn</title>
		<meeting>25th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Balancing multiple sources of reward in reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shelton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1082" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tree-based fitted Q-iteration for multi-objective Markov decision problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castelletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pianosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-objective optimization by reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Congr</title>
		<meeting>IEEE Congr</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Solving multi-objective reinforcement learning problems by EDA-RL-acquisition of various strategies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Handa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th Int</title>
		<meeting>9th Int</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="426" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Using the XCS classifier system for multiobjective reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Life</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="86" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multicriteria reinforcement learning based on goal-directed exploration and its application to bipedal walking robot</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shigenobu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Inst. Syst. Control Inf. Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="352" to="360" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning algorithm and its application in drive system</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Annu. IEEE Conf. Ind. Electron</title>
		<meeting>34th Annu. IEEE Conf. Ind. Electron<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="274" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning algorithm and its improved convergency method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IEEE Conf</title>
		<meeting>6th IEEE Conf<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2438" to="2445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Parallel reinforcement learning for weighted multi-criteria model with adaptive margin</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Neurodyn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rule driven multi objective dynamic scheduling by data envelopment analysis and reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Logist., Hong Kong</title>
		<meeting>IEEE Int. Conf. Autom. Logist., Hong Kong</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="396" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adaptive multi-objective reinforcement learning with hybrid exploration for traffic signal control based on cooperative multi-agent framework</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gomaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="134" to="151" />
			<date type="published" when="2014-03">Mar. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-objective fitted Q-iteration: Pareto frontier approximation in one single run</title>
		<author>
			<persName><forename type="first">A</forename><surname>Castelletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pianosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Netw. Sens. Control</title>
		<meeting>IEEE Int. Conf. Netw. Sens. Control<address><addrLine>Delft, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="260" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Computing optimal stationary policies for multi-objective Markov decision processes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wiering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp</title>
		<meeting>IEEE Int. Symp<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A fixed-weight RNN dynamic controller for multiple mobile robots</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oubbati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th IASTED Int. Conf. Model. Identif</title>
		<meeting>24th IASTED Int. Conf. Model. Identif</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="457" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The application of hybrid genetic particle swarm optimization algorithm in the distribution network reconfigurations multi-objective optimization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Nat. Comput</title>
		<meeting>3rd Int. Conf. Nat. Comput</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="455" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multiobjective optimization using genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Val. Cot Anal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="310" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int. Conf. Mach. Learn</title>
		<meeting>17th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A POMDP formulation of preference elicitation problems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Nat. Conf</title>
		<meeting>18th Nat. Conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning an agent&apos;s utility function by observing behavior</title>
		<author>
			<persName><forename type="first">U</forename><surname>Chajewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ormoneit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Conf. Mach. Learn</title>
		<meeting>18th Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Int. Conf. Mach. Learn</title>
		<meeting>21st Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynamic preferences in multi-criteria reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int. Conf. Mach. Learn</title>
		<meeting>22nd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adaptive multi-objective reinforcement learning with hybrid exploration for traffic signal control based on cooperative multi-agent framework</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gomaaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="134" to="151" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-objective optimization of freeway traffic flow via a fuzzy reinforcement learning method</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf</title>
		<meeting>3rd Int. Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="530" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Efficiency and equity based freeway traffic network flow control</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf</title>
		<meeting>2nd Int. Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning for traffic signal control using vehicular ad hoc network</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ID 724035</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2010-03">Mar. 2010</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Medical QoS provision based on reinforcement learning in ultrasound streaming over 3.5G wireless systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S H</forename><surname>Istepanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Martini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="566" to="574" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Local episode-based learning of multi-objective behavior coordination for a mobile robot in dynamic environments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kubota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th IEEE Int. Conf. Fuzzy Syst</title>
		<meeting>12th IEEE Int. Conf. Fuzzy Syst</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="307" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Automated vehicle overtaking based on a multiple-goal reinforcement learning framework</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C K</forename><surname>Ngai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H C</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Control Appl</title>
		<meeting>IEEE Int. Conf. Control Appl<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="530" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">An evaluation pattern generation scheme for electric components in hybrid electric vehicles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th IEEE Int. Conf. Int. Syst</title>
		<meeting>5th IEEE Int. Conf. Int. Syst<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multicriteria reinforcement learning based on a Russian doll method for network routing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Petrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aissanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Benyahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th IEEE Int. Conf. Intell. Syst</title>
		<meeting>5th IEEE Int. Conf. Intell. Syst</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multi-objective reinforcement learning for responsive grids</title>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Germain-Renaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Loomis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Grid Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="492" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Towards a more efficient multi-objective particle swarm optimizer</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V S</forename><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multi-Objective Optimization in Computational Intelligence: Theory and Practice</title>
		<meeting><address><addrLine>Hershey, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="76" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">MOEA/D: A multiobjective evolutionary algorithm based on decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="712" to="731" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Multiobjective optimization: Improved FPTAS for shortest paths and non-linear objectives with applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsaggouris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaroliagis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="186" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multiobjective optimization using GAI models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dubus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif. Intell</title>
		<meeting>Int. Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1902" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Near admissible algorithms for multiobjective search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Spanjaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Artif. Intell</title>
		<meeting>Eur. Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="490" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A multiobjective particle swarm optimizer for constrained optimization</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Swarm Intell. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Hierarchical reinforcement learning for adaptive text generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dethlefs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cuayahuitl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf</title>
		<meeting>6th Int. Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Reinforcement learning with hierarchies of machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1043" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Hierarchical reinforcement learning with the MaxQ value function decomposition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="227" to="303" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multi-time models for temporally abstract planning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1050" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">State abstraction for programmable reinforcement learning agents</title>
		<author>
			<persName><forename type="first">D</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Nat. Conf. Artif. Intell</title>
		<meeting>18th Nat. Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="119" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Recent advances in hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Event Dyn. Syst. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="379" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Partitioning the state space by critical states</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Bio-Inspired Comput</title>
		<meeting>4th Int. Conf. Bio-Inspired Comput</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Hierarchial approximate policy iteration with binary-tree state space decomposition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1863" to="1877" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A hierarchical learning architecture with multiple-goal representations based on adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Netw. Sens. Control</title>
		<meeting>Int. Conf. Netw. Sens. Control</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="172" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The dynamics of reinforcement learning in cooperative multiagent systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Nat. Conf. Artif. Intell</title>
		<meeting>15th Nat. Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="746" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Theoretical considerations of potentialbased reward shaping for multi-agent systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kudenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Annu. Int. Conf. Auton. Agents Multiagent Syst</title>
		<meeting>10th Annu. Int. Conf. Auton. Agents Multiagent Syst</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Evolving equilibrium policies for a multiagent reinforcement learning problem with state attractors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Leon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Collect. Intell</title>
		<meeting>Int. Conf. Comput. Collect. Intell<address><addrLine>Gdynia, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Online learning control using adaptive critic designs with sparse kernel machines</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="762" to="775" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2169" to="2231" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A self-learning call admission control scheme for CDMA cellular networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1219" to="1228" />
			<date type="published" when="2005-09">Sep. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Efficient reinforcement learning using recursive least-squares methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="259" to="292" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Technical update: Least-squares temporal difference learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">TD-Gammon, a self-teaching backgammon program, achieves master-level play</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="219" />
			<date type="published" when="1994-03">Mar. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A reinforcement learning approach to job-shop scheduling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Joint Conf</title>
		<meeting>14th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1114" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A novel multi-agent reinforcement learning approach for job scheduling in grid computing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gen. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="430" to="439" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An optimal satellite antenna profile using reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="406" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Intercell interference management in OFDMA networks: A decentralized approach based on reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agustí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pérez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sallent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="968" to="976" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Experience replay for real-time reinforcement learning control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="212" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Autonomous adaptive and active tuning up of the dissolved oxygen setpoint in a wastewater treatment plant using reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernandez-Del-Olmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaudioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nevado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="774" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">An empirical comparison of two common multiobjective reinforcement learning algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Issabekov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vamplew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">respectively, where he is currently pursuing the Ph.D. degree. His current research interests include intelligent systems, machine learning, and autonomous land vehicles</title>
		<meeting><address><addrLine>Changsha, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2012. 2004 and 2006</date>
			<biblScope unit="page" from="626" to="636" />
		</imprint>
	</monogr>
	<note>Proc. 25th Int. Australas. Joint Conf., Sydney, NSW Australia</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">He is currently a Full Professor with the Institute of Unmanned Systems, College of Mechatronics and Automation</title>
		<imprint>
			<date type="published" when="1996">1996. 2002</date>
			<pubPlace>Changsha, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National University of Defense Technology (NUDT) ; NUDT. He has been a Visiting Scientist for Cooperation Research with Hong Kong Polytechnic University, Hong Kong, University of</orgName>
		</respStmt>
	</monogr>
	<note>where he received the Ph.D. degree in control engineering from the College of Mechatronics and Automation</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">He has authored or coauthored over 100 papers in international journals and conferences, and has coauthored four books. He currently serves as an Associate Editor for the Information Sciences journal, and a Guest Editor of the International Journal of Adaptive Control and Signal Processing. Dr. Xu was the recipient of the 2nd Class National Natural Science Award of China in 2012 and the Fork Ying Tong Youth Teacher Fund of China</title>
		<author>
			<persName><forename type="first">Edmonton</forename><surname>Alberta</surname></persName>
		</author>
		<author>
			<persName><surname>Ab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">University</forename><surname>Canada</surname></persName>
		</author>
		<author>
			<persName><surname>Of Guelph</surname></persName>
		</author>
		<author>
			<persName><surname>Guelph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canada</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName><surname>University Of Strathclyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Glasgow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dewen Hu (SM&apos;09) was born in Hunan</title>
		<meeting><address><addrLine>China; Xi&apos;an, China; Changsha, China; Sheffield, U.K.,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1963">2008. 1963. 1983. 1986. 1999. 1986. 1995 to 1996</date>
		</imprint>
		<respStmt>
			<orgName>National University of Defense Technology ; National University of Defense Technology ; University of Sheffield</orgName>
		</respStmt>
	</monogr>
	<note>and was promoted as a Professor in 1996. His current research interests include image processing, system identification and control, neural networks, and cognitive science. Dr. Hu was the recipient of the 2nd Class National Natural Science Award of China in 2012. He is an Action Editor of Neural Networks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
