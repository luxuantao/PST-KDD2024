<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human-Machine Collaborative Systems for Microsurgical Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Autonomous Systems</orgName>
								<address>
									<settlement>Stockhom</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Marayong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Engineering Research Center for Computer Integrated Surgical Systems and Technology</orgName>
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Engineering Research Center for Computer Integrated Surgical Systems and Technology</orgName>
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Engineering Research Center for Computer Integrated Surgical Systems and Technology</orgName>
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Engineering Research Center for Computer Integrated Surgical Systems and Technology</orgName>
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human-Machine Collaborative Systems for Microsurgical Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">346A3AC27A48CD4300EB9409F0BC0531</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our current progress in developing Human-Machine Collaborative Systems (HMCSs) for microsurgical applications such as vitreo-retinal eye surgery. Three specific problems considered here are (1) developing of systems tools for describing and implementing an HMCS, (2) segmentation of complex tasks into logical components given sensor traces of a human performing the task, and (3) measuring HMCS performance. Our goal is to integrate these into a full microsurgical workstation with the ability to automatically "parse" traces of user execution into a task model which is then loaded into the execution environment, providing the user with assistance using online recognition of task state. The major contributions of our work to date include an XML task graph modeling framework and execution engine, an algorithm for real-time segmentation of user actions using continuous Hidden Markov Models, and validation techniques for analyzing the performance of HMCSs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the Human-Machine Collaborative Systems (HMCS) project is to investigate human-machine cooperative execution of small scale, tool-based manipulation activities. Our work on HMCS is specifically aimed at microsurgery <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> and cell manipulation, but the basic principles apply to many other fine scale tasks such as opto-electronic assembly and assembling of LIGA parts <ref type="bibr" target="#b0">[1]</ref>. The motivation for collaborative systems is based on evidence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> suggesting that humans operating in collaboration with robotic mechanisms can take advantage of robotic speed and precision, but avoid the difficulties of full autonomy by retaining the human component "in-the-loop" for essential decision making and/or physical guidance <ref type="bibr" target="#b5">[6]</ref>.</p><p>Our approach to HMCS focuses on three inter-related problems: (1) Synthesis: the development of systems tools necessary for describing implementing an HMCS;</p><p>(2) Modeling: given sensor traces of a human performing a task, segmenting those traces into logical task components and/or measuring the compatibility of a given HMCS structure to that sequence of components; and (3) Validation: measuring HMCS performance.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> depicts the high-level structure of the human-machine collaborative systems we are developing. Logically, there are three components: the human, the augmentation system, and an observer. We assume that a user primarily manipulates the environment using the augmentation system, although unaided manipulation may take place in some settings (dashed line). The user is able to visually observe the tool and the surrounding environment, and directs an augmentation device using force and position commands. The system may also have access to endpoint force data, targeted visual data and/or other application-dependent sensors (e.g., intra-operative imaging). The role of the observer is to assess available sensor data (including haptic feedback from the user) and initiate, modify, or terminate various forms of assistance. Optional direct interaction between the observer and the user may also be used to convey information or otherwise synchronize their interaction.</p><p>The basic notion of HMCS is clearly related to traditional teleoperation, although the goal in HMCS is not to "remotize" the operator <ref type="bibr" target="#b4">[5]</ref> but rather to provide appropriate levels of operator assistance depending on context. At one extreme, shared control <ref type="bibr" target="#b2">[3]</ref> can be viewed as an HMCS for manipulation tasks in which some degrees of freedom are controlled by machine and others by the human. At the other extreme, supervisory control <ref type="bibr" target="#b13">[14]</ref> gives a more discrete, high-level notion of humanmachine interaction. Our notion of HMCS essentially incorporates both views, combining them with broader questions of modeling manipulation activities consisting of multiple steps and varying level of assistance, and validating those models against human performance data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Synthesis</head><p>Our current system development efforts are motivated by the domain of retinal microsurgery where tasks occur near the limit of the manipulation capabilities of the human hand-eye system. A challenging task is retinal vein cannulation: the introduction of a micro-pipette into a vessel of approximately 100 Âµm in diameter <ref type="bibr" target="#b15">[16]</ref>.</p><p>We augment the surgeon's physical abilities using the Johns Hopkins University Steady Hand Robot (JHU SHR, Figure <ref type="figure" target="#fig_0">1</ref>). Briefly, the JHU Steady Hand Robot is an admittance-controlled 7 DOF robot equipped with a force sensing handle at the endpoint. Tools are mounted at the end-effector and "manipulated" by an operator holding a force-sensing handle also attached to the end-effector. The robot responds to the applied force, implementing a means of direct control for the operator. The robot is ergonomically appropriate for minimally invasive microsurgical tasks and provides micron-scale accuracy <ref type="bibr" target="#b14">[15]</ref>. Two general forms of motion assistance have been investigated with the JHU SHR: force scaling <ref type="bibr" target="#b12">[13]</ref> and motion guidance using virtual fixtures <ref type="bibr" target="#b7">[8]</ref>. In this paper, we focus on the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Describing Spatial Motion Constraints</head><p>"Virtual fixtures" in our implementation provide cooperative control of the manipulator by "stiffening" a hand-held guidance mechanism against certain directions of motion or forbidden regions of the workspace. Studies on virtual fixtures for teleoperation have indicated that user performance can increase as much as 70% with the fixture based guidance <ref type="bibr" target="#b10">[11]</ref>. Here, we describe our framework for implementing virtual fixtures on the JHU SHR. In what follows, we model the robot as a purely kinematic Cartesian device with tool tip position x SE Â¡ 3Â¢ and a control input that is endpoint velocity v Â£ áº R 6 , all expressed in the robot base frame. A human operator guides the robot by applying forces and torques f R 6 on the manipulator handle, likewise expressed in robot base coordinates.</p><p>We define SHR guidance geometrically by identifying a space of "preferred" directions of motion. Let us assume that we are given a 6 Â¤ n time-varying matrix D Â£ DÂ¡ t Â¢ Â¦Â¥ 0 Â§ n Â§ 6, representing the instantaneous preferred directions of motion. For example, if n is 1, the preferred direction is along a curve in SE(3); if n is 2 the preferred directions span a surface, and so forth. We define two projection operators, the span and the kernel of the column space, as</p><formula xml:id="formula_0">SpanÂ¡ DÂ¢ Â© D Â£ DÂ¡ D DÂ¢ D Â¥ and KerÂ¡ DÂ¢ Â© D Â©Â£ I ! " D<label>(1)</label></formula><p>where denotes pseudo-inverse for the case where D is (column) rank deficient. By decomposing the input force vector, f, into two components, f D # D f and f Ï f ! f D Â£ D fÂ¥ and introducing a new admittance ratio k Ï 0Â¥ 1 that attenuates the non-preferred component of the force input, we arrive at an admittance control</p><formula xml:id="formula_1">v Â£ k Â¡ f D $ k Ï f Ï Â¢ Â©Â£ k Â¡ D $ k Ï D %Â¢ f<label>(2)</label></formula><p>Thus, the final control law is in the general form of an admittance control with a time-varying gain matrix determined by DÂ¡ t Â¢ . By choosing k, we control the overall admittance of the system. Choosing k Ï low imposes the additional constraint that the robot is stiffer in the non-preferred directions of motion. As noted above, we refer to the case of k Ï Â£ 0 as a hard virtual fixture, since it is not possible to move in any direction other than the preferred direction. All other cases will be referred to as soft virtual fixtures. In the case k Ï Â£ 1Â¥ we have an isotropic admittance.</p><p>The development to this point directly supports motion in a subspace, but it does not allow us to define a fixed desired motion trajectory. If u Â£ f Â¡ xÂ¥ SÂ¢ is the signed distance of the tool tip from a surface S, we can define a new preferred direction as</p><formula xml:id="formula_2">D c Â¡ xÂ¢ &amp;Â£ 'Â¡ 1 ! k d Â¢ ( D f) 10 f 0 $ k d D u 0 Â§ k d Â§ 1Â¥<label>(3)</label></formula><p>combining the two vectors that encode motion in the preferred direction and correcting the tool tip back to S. The constant k d governs how quickly the tool is moved toward the reference surface. We note that the division by 0 f 0 is undefined when no user force is present. As projection is invariant to scale, we write (3) as</p><formula xml:id="formula_3">D c Â¡ xÂ¢ &amp;Â£ Â¡ 1 ! k d Â¢ Â¦ D f $ k d 0 f0 D u 0 Â§ k d Â§ 1 (4)</formula><p>and apply (2) with D Â£ D c . One potential problem with this control law is that when the user applies no force, there is no virtual fixture because there is no defined preferred direction. Thus, there is a discontinuity at the origin. However, in practice the resolution of any force sensing device is usually well below the numerical resolution of the underlying computational hardware computing the pseudo-inverse, so the user will never experience this discontinuity.</p><p>An implementation of this framework in operation is shown in Figure <ref type="figure" target="#fig_0">1</ref> (right). A CCD camera is attached to the robot and views a curve on a task plane. The view is processed using the XVision tracking system <ref type="bibr" target="#b3">[4]</ref> to determine the tangent to the reference path at a point closest to the center of the image in real time. The center is marked with a cross, and a user attempts to move the robot to follow the curve (similar to tracking a blood vessel in preparation for retinal cannulation). In this case, if t x and t y are the components of the tangent to the curve in the image plane, and n x and n y is the vector from the center to the closest point on the curve, then we have u Â£ Â¡ n x Â¥ n y Â¥ 0Â¥ 0Â¥ 0Â¥ 0Â¢ and D Â£ Â¡ t x Â¥ t y Â¥ 0Â¥ 0Â¥ 0Â¥ 0Â¢ Â¡ More details and extensions of this basic task can be found in <ref type="bibr" target="#b8">[9]</ref>. Experimental results with users are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">High-Level Task Specification</head><p>Most microsurgical tasks consist of discrete, serial, quasi-static steps, each with a clearly defined outcome. Hence, models for such procedures can be defined by relatively simple graphs. For example, retinal vein cannulation involves positioning and orienting a needle to the vicinity of the vein, and inserting it when appropriate until contact is made. Upon contact, puncturing is performed, after which the needle can be safely withdrawn, as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>In order to provide appropriate assistance, it must be possible to create models of procedures, and to attach appropriate types or levels of assistance to each node. To this end, we have developed a set of tools for defining task graphs. Task graphs are represented using a specialization of the Extensible Markup Language (XML) 1 . A task graph modeling system allows a user to interactively design a graph for the task to be performed and save the graph as an XML file. A task graph execution engine reads the XML description and coordinates the execution of the graph. The graph itself references a library of basic primitives for providing user assistance at each node, and a set of transition conditions. Thus, at each state the graph executor initiates the appropriate assistance mode and monitors for transition events to subsequent states. Currently, there are three ways of generating a task graph: (1) using the graphical interface, (2) directly writing an XML file, and (3) automating off-line task modeling. We now turn to the latter of these methods.  The black line denotes the virtual fixture reference path, and the path to be followed by the user is dark gray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modeling and Sensing Human Intent</head><p>One problem that arises in HMCS is that of providing the appropriate assistance to a user, based on the intent or context of his or her actions. In order to do so, the system must have a model of the task being performed, and a means for relating the actions of the user to that model. We have investigated continuous Hidden Markov Models (HMMs) <ref type="bibr" target="#b9">[10]</ref> as a means of modeling tasks from sensor traces of user execution. We have also developed a new algorithm for HMM recognition that allows for real-time segmentation of user actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Offline Task Modeling</head><p>The basic hypothesis of offline task modeling is that, for restricted domains such as microsurgery, it is possible to model tasks using a small vocabulary of primitive "gestemes" with an associated assistance mode. We have tested our hypothesis us-ing data acquired from task execution with the JHU SHR and modeled using The Hidden Markov Model Toolkit<ref type="foot" target="#foot_0">2</ref> (HTK). The input data acquired from the robot consisted of seven variables: Cartesian forces and torques expressed in robot-endeffector coordinates, and the magnitude of translation between the last reading and the current one. We assign a linearly sequential left-to-right (SLR) structure to the HMMs of several gestemes. Two tasks were investigated: (1) a peg-in-hole task, and (2) a paint task, which are analogous to retinal vessel cannulation and retinalmembrane peeling, respectively. For the peg-in-hole task, five gestemes were used: place, position, insert, withdraw and remove. For the paint task, the four gestemes were: place, position, paint and remove. Five users participated in the experiment and each user performed ten runs. Each gesteme was trained independently on a training set, then gestemes were run in parallel to segment a test set (Figure <ref type="figure" target="#fig_2">3</ref>). The obtained segmentation accuracy of the system over all users was around 84% for peg-in-hole task and 90% for paint task. In addition, we found that we could use gestemes trained for the paint task in the peg-in-hole task, and the resulting decrease in recognition performance was only 1.08%. These results, although preliminary, suggest that for suitably constrained situations, it is plausible that complex tasks can be modeled using a small set of manipulation primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online Recognition and Assistance</head><p>Given a task model, the question is whether it is possible to determine the appropriate task state online. To do this, we have developed an online continuous HMM recognition algorithm <ref type="bibr" target="#b6">[7]</ref>. The structure of each HMM is SLR and each model operates in parallel. Suppose we have M potential models and each model has N states. For a given model Î», let Ï j Î» Â¡ t Â¢ be the likelihood of observing vector o 1 to o t and being in state j at time t. The partial likelihood of each model can be computed as</p><formula xml:id="formula_4">Ï j Î» Â¡ t Â¢ Â£ Â¡ N â iÂ¢ 1 Ï i Î» Â¡ t ! 1Â¢ a i jÂ£ b j Â¡ o t Â¢ Â¦Â¥ with Ï 1 Î» Â¡ 1Â¢ Â£ 1Â¥ Ï j Î» Â¡ 1Â¢ Â£ 0 Â¤ 1 Â§ j Â¥ N (5)</formula><p>The likelihood of state 1 of a model Î» in time t Â¦ 1 can be defined as</p><formula xml:id="formula_5">Ï 1 Î» Â¡ t Â¢ Â£ 1 M M â mÂ¢ 1 Ï N m Â¡ t ! 1Â¢ for 1 Â§ Î» Â¥ M (6)</formula><p>For each model, we define the sum of the likelihood of all states as the total likelihood at time</p><formula xml:id="formula_6">t as LÂ¡ o 1 Â¥ o 2 Â¥ Â¡ Â¡ Â¡ Â¥ o t Â§ Î»Â¢ &amp;Â£ â N jÂ¢ 1 Ï j Î» Â¡ t Â¢</formula><p>, where the model with the highest total likelihood represents the current hypothesis.</p><p>We tested the online recognition algorithm with the JHU SHR in a planar environment, where the task was to follow a sinusoidal path, but avoid a portion of the path covered by a circle. The real-time HMM recognition output was given every 33ms. In our experiment, we included three models/gestemes (HMMs) of user actions: (1) silence (user is doing nothing), (2) follow curve (user is following the curve); and (3) avoid curve (user is not following the curve). The data used was force in the x and y directions. The force, f , was decomposed into the directions parallel to reference curve Î´ and the normal direction Ï. Â§ Â§</p><formula xml:id="formula_7">f Î´ Â§ Â§ , Â§ Â§ f Ï Â§ Â§ and Â§ Â§ f Â¡ Î´ Â§ Â§</formula><p>were the elements of the observation vector used to train the HMMs.</p><p>The accuracy of recognition and the sensitivity of our algorithm to training in differing environments was examined by an experiment where: (1) the same sine curve was used for both training and recognition, and (2) different sine curves were used for training and recognition. Here, the average accuracy of real-time continuous recognition among all subjects was larger than 90% in both cases. We believe that this improvement in recognition over the offline case is due to the additional context provided by the virtual fixtures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Validation</head><p>During task execution, speed and accuracy are generally used as metrics to evaluate performance. There is typically a trade off between these two metrics, as described by Fitts Law <ref type="bibr" target="#b1">[2]</ref>. In this section, we demonstrate that both speed and accuracy can be improved simultaneously with the aid of virtual fixture guidance. The experimental setup makes use of the curve following virtual fixture described in Section 2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Virtual Fixtures</head><p>We present two experiments that examine the effects of virtual fixture guidance ranging from complete guidance (admittance ratio = 0) to no guidance (admittance ratio = 1). Detailed results can be found in <ref type="bibr" target="#b7">[8]</ref>. The reference path was a 0.39mm thick sine curve with a 35mm amplitude and 70mm wavelength. For Experiment II, we added a circle of radius 10mm with its center located at the midpoint of the sine curve. The subjects were provided with instructions as shown in Figure <ref type="figure" target="#fig_2">3</ref>, and told to move along the path as quickly as possible without sacrificing accuracy, considering accuracy and speed with equal emphasis. Experiment I included five subjects performing the path following task three times with eleven admittance ratios from 0 to 1 (0, 0.1, . . . ). Experiment II included eight subjects performing each task three times with four discrete admittance ratios corresponding to four guidance levels (0=complete, 0.3=medium, 0.6=soft, and 1=none). At the run time, the time and error during motion from Point A to Point B in Figure <ref type="figure" target="#fig_2">3</ref> were recorded. The error represents the deviation from the reference path. For the off-path targeting task, we recorded the time each subject needed to get back on the curve after leaving it. For the avoidance task, we recorded the time needed to avoid the circle, and no error was measured. For the data obtained in Experiment II, we performed ANOVA and multiple pair-wise comparisons using Tukey's method to determine significant differences. Data are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>For Experiment I, the data indicate that improvements in error and time have linear relationships with admittance ratio, as shown in Figure <ref type="figure" target="#fig_3">4</ref>. This is to be expected, since the output velocity of the SHR is linearly proportional to admittance ratio. For Experiment II, the average execution time and error were used to determine the improvement in performance with different guidance levels. For the path following task, a decrease in admittance ratio reduces error, except between medium and complete guidance. However, a decrease in admittance ratio does not improve execution time significantly. We note that none of the subjects performed worse in both time and error when admittance ratio decreased. Complete guidance resulted with the best performance but more guidance resulted in shorter time and/or higher accuracy compared to no guidance. For the targeting task, stronger guidance slowed task execution. However, the execution times for no guidance and soft guidance do not differ significantly. The analysis also shows no difference in error for all guidance levels. In general, reducing guidance reduces the time and error during target acquisition. For the avoidance task, only the execution time was considered. The results indicate that less guidance reduces the execution time.</p><p>The results indicate that the selection of virtual fixture admittance is a taskdependent process. For surgical applications, error reduction is more important than time required to perform the task. For tasks that require significant interaction from the user, such as object avoidance and off-path targeting, strong guidance will reduce accuracy and increase execution time. Therefore, a lower amount of guidance is recommended. Based on the linear relationship between admittance and performance found in Experiment I, we developed admittance ratio selection parameters that can be used to determine an appropriate guidance level <ref type="bibr" target="#b7">[8]</ref>. Using equal weighting for error vs. time, and path following vs. path avoidance, we found that an admittance ratio of approximately 0.6 is optimal. However, online admittance tuning is recommended to achieve the full benefit of virtual fixture guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Online Assistance</head><p>In this experiment, 8 subjects were asked to follow the sine curve (model follow curve) and the edge of the circle (model avoid curve) in three different modes:</p><p>(1) HMM+VF where real-time recognition was used to determine when to apply or remove the virtual fixture, (2) VF: a virtual fixture with constant admittance ratio k Ï = 0.3 was applied, and (3) NG: the virtual fixture was removed (no guidance, k Ï = 1). The error (deviation from the curve) and time were again used to validate the performance. To create a "gold standard" for the correct segmentation, the subjects were asked to press a space bar when they intended to transition from one model to another. The results are presented in Figure <ref type="figure" target="#fig_4">5</ref> showing the good performance of the combined algorithm (HMM+VF). The HMM+VF provides the user with assistance when he/she wants to follow the curve, and removes the assistance otherwise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We have presented the structure, theoretical and experimental results of our HMCS project. We have developed a control method for implementing guidance virtual fixtures with an admittance controlled robot, as well as techniques for automatic recognition of user activities. These components are integrated through a task graph and execution engine specifically designed for serial procedures such as those encountered in microsurgery.</p><p>We are currently integrating these components into a full micro-surgical workstation for vitreo-retinal eye surgery. We expect to shortly be able to automatically "parse" traces of user execution into a task model, load the task model into the execution environment, and thereby provide the user with assistance using online recognition of task state. The system will be tested using ex-vivo and in-vivo experiments involving evaluation of system performance during operation by surgeons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Structure of a Human-Machine Collaborative System (left), and the experimental setup using the Johns Hopkins University Steady Hand Robot (right).</figDesc><graphic coords="2,301.68,65.93,132.60,99.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. GUI for generating task graphs (left). A graph example for vein cannulation (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Network for real-time, continuous HMM recognition (left). One model describes the user motion at any given time. Task descriptions for the human performance experiments (right): (a) path following, (b) off-path targeting, and (c) avoidance. The black line denotes the virtual fixture reference path, and the path to be followed by the user is dark gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Average normalized time and error versus admittance ratio for the path following task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Mean and standard deviation of average error (left) and time (right) for the follow curve section of the task, avoid curve section of the task, and total task. HMM+VF indicates that the virtual fixture was switched off by the HMM when it was detected that the user intended to move away from the curve, VF indicates a constant virtual fixture with admittance ratio k Ï = 0.3, and NG indicates no guidance (k Ï = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experiment II: Experimental results for eight subjects: rows 1-4 for path following, rows 5-7 for off-path targeting, and 8-10 for avoidance.</figDesc><table><row><cell></cell><cell cols="6">Time (Seconds) Admittance Ratio (k Ï ) Guidance Average Standard Deviation Average Standard Deviation Error (Pixels)</cell></row><row><cell>1</cell><cell>0</cell><cell cols="2">Complete 18.710</cell><cell>1.357</cell><cell>0.061</cell><cell>0.013</cell></row><row><cell>2</cell><cell>0.3</cell><cell cols="2">Medium 19.647</cell><cell>1.879</cell><cell>0.078</cell><cell>0.020</cell></row><row><cell>3</cell><cell>0.6</cell><cell>Soft</cell><cell>20.425</cell><cell>2.042</cell><cell>0.121</cell><cell>0.030</cell></row><row><cell>4</cell><cell>1</cell><cell cols="2">None 24.745</cell><cell>5.405</cell><cell>0.229</cell><cell>0.087</cell></row><row><cell>5</cell><cell>0.3</cell><cell cols="2">Medium 9.754</cell><cell>3.664</cell><cell>1.256</cell><cell>0.456</cell></row><row><cell>6</cell><cell>0.6</cell><cell>Soft</cell><cell>7.148</cell><cell>3.178</cell><cell>0.910</cell><cell>0.180</cell></row><row><cell>7</cell><cell>1</cell><cell>None</cell><cell>6.256</cell><cell>2.353</cell><cell>0.702</cell><cell>0.437</cell></row><row><cell>8</cell><cell>0.3</cell><cell cols="2">Medium 14.681</cell><cell>5.629</cell><cell>-</cell><cell>-</cell></row><row><cell>9</cell><cell>0.6</cell><cell>Soft</cell><cell>11.871</cell><cell>4.959</cell><cell>-</cell><cell>-</cell></row><row><cell>10</cell><cell>1</cell><cell>None</cell><cell>9.317</cell><cell>2.954</cell><cell>-</cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://htk.eng.cam.ac.uk</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Russell Taylor and Blake Hannaford for their insights related to this research. This material is based upon work supported by the National Science Foundation, under Grants No. EEC-9731478, IIS-0099770, ITR-0205318 and Swedish Foundation for Strategic Research through the Centre for Autonomous Systems.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parallel Assembly of High Aspect Ratio Microstructures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Feddema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Christenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SPIE Microrobotics and Microassembly</title>
		<imprint>
			<biblScope unit="volume">3834</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The information capacity of the human motor system in controlling the amplitude of movement</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Fitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="381" to="391" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fusion of Human and Machine Intelligence for Telerobotic Systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Tarn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bejczy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Robotics and Automation</title>
		<meeting>Int. Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="3110" to="3115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The XVision system: A general purpose substrate for realtime vision applications</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Sensor-based Telerobotic System for the Space Robot Experiment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heindl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>Int. Symp. on Experimental Robotics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Performance of robotic augmentation in common dextrous surgical motions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Goradia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Whitcomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoianovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Interventions</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognition of Operator Motions for Real-Time Assistance Using Virtual Fixtures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems</title>
		<imprint>
			<publisher>IEEE Virtual Reality</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="125" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effect of Virtual Fixture Compliance on Human-Machine Cooperative Manipulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marayong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bettini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1089" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial Motion Constraints: Theory and Demonstrations for Robot Guidance using Virtual Fixtures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marayong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Okamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2003 IEEE International Conference on Robotics and Automation</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Virtual Fixtures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Mech. Eng., Stanford Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robot-assisted stapedotomy: Micropick fenestration of the stapes footplate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rothbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoianovics</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berkelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Whitcomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niparko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otolaryngology -Head and Neck Surgery</title>
		<imprint>
			<biblScope unit="page" from="417" to="426" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive Force Control of Position/Velocity Controlled Robots:Theory and Experiment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Whitcomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Rob. and Autom</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="137" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human Supervisory Control of Robot Systems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="808" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Steady-Hand Robotic Syatem for Microsurgical Augmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Whitcomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoianovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dejuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kavoussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1201" to="1210" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Injection of tissue plasminogen activator into a branch retinal vein in eyes with central retinal vein occlusion</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opthalmology</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2249" to="2257" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
