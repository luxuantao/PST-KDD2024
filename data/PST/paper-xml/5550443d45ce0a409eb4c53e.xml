<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SmartPhoto: A Resource-Aware Crowdsourcing Approach for Image Sensing with Smartphones</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yibo</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Com-puter Science and Engineering</orgName>
								<orgName type="institution">Yibo Wu</orgName>
								<address>
									<addrLine>Wenjie Hu</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Com-puter Science and Engineering</orgName>
								<orgName type="institution">Yibo Wu</orgName>
								<address>
									<addrLine>Wenjie Hu</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<postCode>16802</postCode>
									<settlement>Uni-versity Park</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjie</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Com-puter Science and Engineering</orgName>
								<orgName type="institution">Yibo Wu</orgName>
								<address>
									<addrLine>Wenjie Hu</addrLine>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guohong</forename><surname>Cao</surname></persName>
							<email>gcao@cse.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Com-puter Science and Engineering</orgName>
								<orgName type="institution">Yibo Wu</orgName>
								<address>
									<addrLine>Wenjie Hu</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SmartPhoto: A Resource-Aware Crowdsourcing Approach for Image Sensing with Smartphones</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A07063ADD1ECE700BBAA0EBCA77C18C1</idno>
					<idno type="DOI">10.1109/TMC.2015.2444379</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMC.2015.2444379, IEEE Transactions on Mobile Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMC.2015.2444379, IEEE Transactions on Mobile Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Crowdsourcing</term>
					<term>image sensing</term>
					<term>photo sharing</term>
					<term>camera sensor</term>
					<term>smartphone</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Photos obtained via crowdsourcing can be used in many critical applications. Due to the limitations of communication bandwidth, storage and processing capability, it is a challenge to transfer the huge amount of crowdsourced photos. To address this problem, we propose a framework, called SmartPhoto, to quantify the quality (utility) of crowdsourced photos based on the accessible geographical and geometrical information (called metadata) including the smartphone's orientation, position and all related parameters of the built-in camera. From the metadata, we can infer where and how the photo is taken, and then only transmit the most useful photos. Four optimization problems regarding the tradeoffs between photo utility and resource constraints, namely Max-Utility, online Max-Utility, Min-Selection and Min-Selection with k-coverage, are studied. Efficient algorithms are proposed and their performance bounds are theoretically proved. We have implemented SmartPhoto in a testbed using Android based smartphones, and proposed techniques to improve the accuracy of the collected metadata by reducing sensor reading errors and solving object occlusion issues. Results based on real implementations and extensive simulations demonstrate the effectiveness of the proposed algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E QUIPPED with GPS, orientation sensors, mega-pixel cameras and advanced mobile operating systems, smartphones not only change the way people communicate with each other, but also the way they interact with the world. The popularity of online photo sharing services such as Flickr and Instagram indicates that people are willing to take photos and share experiences with others. Thanks to the cost efficiency, timeliness and pervasive nature of these data, numerous opportunities have been created for applications based on photo crowdsourcing, such as grassroots journalism <ref type="bibr" target="#b2">[3]</ref>, photo tourism <ref type="bibr" target="#b19">[20]</ref>, and even disaster recovery and emergency management <ref type="bibr" target="#b13">[14]</ref>.</p><p>Consider an example in post-earthquake recovery. First responders survey the damage by taking pictures and then transfer them back to the remote command and control center. As events occur, photos need to be collected and uploaded as quickly as possible. However, there are strict bandwidth constraints, no matter it is based on mobile ad hoc networks, delay tolerant networks, or partly damaged cellular networks. Then, how to make use of the limited bandwidth to upload the most useful photos becomes a challenge.</p><p>Another example can be found in our daily life. A map service provider can enhance user experience by showing photos of interesting objects around the world, for example, landmarks like famous buildings. Data can be obtained from visitors taking photos via their smartphones. Once the photos are uploaded and processed, other map users can have virtual tours. Due to the existence of many useful applications, people are sharing billions of photos taken by smartphones. Photos are often geographically correlated and this correlation can be used to enrich traditional map experience. However, the sheer amount of photos poses big challenges for image processing and storage at the server end. Fully understanding the semantic of each photo by traditional resource intensive image recognition techniques would be a luxury if not impossible. Therefore, how to identify the most relevant data and eliminate redundancy becomes an important issue.</p><p>The major challenges faced by these applications are as follows. The first is how to characterize the quality (usefulness) of crowdsourced photos in a way that is both meaningful and resource friendly. Most content-based image processing techniques such as <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> may demand too much computational and communication resources at both user and server end. On the other hand, existing solutions from description based techniques either categorize photos based on user defined tags, or prioritize them by the GPS location <ref type="bibr" target="#b21">[22]</ref>. Obviously, tagging each photo manually is not convenient and may discourage public participation. GPS location itself may not be sufficient to reveal the real point of interest. Even at the same location, smartphones facing different directions will have different views.</p><p>To address these issues, we propose a framework to quantify the quality of crowdsourced photos based on easily accessible geographical and geometrical information, called metadata, including the orientation and position of the phone, and the field-of-view (FoV) of the camera. Intuitively, a good photo coverage should have multiple views of the target and cover as many aspects as possible. Specifically, given a set of targets and photos, we consider an aspect of a target to be properly covered if it is within a proper range of a photo's viewing direction (defined in Section 2). Then we measure the quality of a photo by utility, which indicates how many aspects are covered. The utility is calculated based on the metadata, which can be practically obtained via various embedded sensors in most off-the-shelf smartphones. They are independent of the image content, and hence the computation is very fast and resource friendly compared to traditional content based approaches.</p><p>With the above model, we address challenges brought by the resource constraint, which is referred to as the Max-Utility problem. Resource constraint of bandwidth, storage and processing capability limits the number of photos that can be uploaded to the server. Given the metadata of the candidate photos, how to find a given number of photos such that the total utility is maximized? Note that this is different from traditional maximization problems on sensor coverage in which a target is covered as long as it is inside the sensing range. Here photos taken at different view points cover different aspects of the target. The total utility depends on how many aspects can be covered and how they are covered, which makes the problem unique and complicated. We also consider online selection/optimization to address the requirements of time critical applications.</p><p>Another challenge to be addressed is how to remove the redundancy and find the most representative photos. In general, the amount of candidate photos is significant and redundancy occurs if multiple photos are taken at similar locations and from similar angles. The less number of photos is selected, the less amount of bandwidth, storage and processing capability is needed. In the Min-Selection problem, given the coverage requirements of the targets, we want to find the minimum set of photos that satisfy the requirements. We also consider having certain level of redundancy in case better coverage is needed.</p><p>Our contributions are summarized as follows. We propose SmartPhoto, a novel framework to evaluate and optimize the selection of crowdsourced photos, based on the collected metadata from the smartphones. We formulate the Max-Utility problem for bandwidth constrained networks, and then extend it into an online optimization problem. We study the Min-Selection problem for redundancy reduction, and also extend it to the case where better coverage (e.g., k-coverage) is desired. Moreover, we propose efficient solutions, and find the performance bounds in terms of approximation or competitive ratios for the proposed algorithms.</p><p>We have implemented SmartPhoto in a testbed using Android based smartphones. We make use of multiple embedded sensors in off-the-shelf smartphones, and propose a series of methods to fuse data, correct errors, and filter out false information, to improve the accuracy of the collected metadata. Finally, the performance of the proposed algorithms are evaluated through real implementations and extensive simulations.</p><p>The rest of the paper is organized as follows. Section 2 introduces the basic concepts and the model. Section 3 studies the Max-Utility problem and Section 4 studies the Min-Selection problem. Section 5 presents the implementation of the testbed. Performance evaluations are presented in Section 6. Section 7 reviews related work and Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Consider a post-disaster scenario in which a set of predefined targets are to be monitored by a group of people or reporters. They use smartphones to take photos and transfer them back to the processing center. However, only a small number of photos can be transferred due to the limited bandwidth caused by damage to base stations or overwhelming cellular traffic. For this reason, reporters first transmit the metadata of the photos, which is extremely lightweight compared to the original images. After that, the server runs optimization algorithms to determine what photos to be actually transferred and notifies the reporters to transmit the photos.</p><p>We first describe the models used in SmartPhoto to characterize targets and photos. Then the concept of utility is introduced. The idea is based on the observation that a good photo should cover as many aspects of the targets as possible. For an aspect to be properly covered, the target should be in a photo whose viewing direction is not too far away from the direction to which the aspect points. This is similar to the face recognition problem in computer vision: as the angle between the object's facing direction (the aspect) and the camera's viewing direction (the vector from the camera to the object) becomes wider, the detection rate of the recognition algorithm will drop dramatically <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>. The utility defined in this section precisely indicates how many aspects of the target are properly covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Targets and Photos</head><p>At the beginning of each event, the application server distributes the information of the interested targets to the public users. The set of targets are denoted by T = {T 1 , . . . , T m }. T i also represents the location of the i-th target if there is no ambiguity. An aspect of the target, denoted by v, is a vector that can be represented by an angle in [0, 2π) with 0 degree indicating the one pointing to the right (east on the map). For ease of presentation, this angle is denoted by arg( v) and is calculated by using arithmetic modulo 2π.</p><p>Given a set of photos: P = {P 1 , . . . , P n }, each photo P j is stored locally and it can be registered to the server with a tuple (l j , r j , ϕ j , d j ), called the metadata of the photo. Here l j is the location where the photo is taken. To simplify notations, we also use P j to represent the location if there is no ambiguity. r j and ϕ j are two internal parameters of the camera used to take the photo. r j is the effective range of the camera, and ϕ j is the field-of-view (FoV, represented in angle) of the camera lens. d j is the orientation of the camera when the photo is taken. Note that d j is the normal vector derived from the camera lens and vertical to the image plane. It can be acquired by using various sensors embedded in the smartphone. Details of obtaining these geographical information on the smartphone will be given in Section 5. As shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), the metadata defines the effective coverage range of the photo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Photo Utility</head><p>For a target T i and a photo P j , T i is said to be covered by P j if P j 's range includes T i . An aspect v of T i is covered if the angle between v and --→ T i P j is smaller or equal to a predefined angle θ called effective angle. Here --→ T i P j is the viewing direction of the camera towards the target when the photo is taken 1 . Further, the utility of a photo P j can be defined based on how many aspects of T i are covered by this photo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1.</head><p>[Utility] Given a target T i and a photo P j covering the target, the utility of P j on T i , denoted by U Pj (T i ), is the portion of aspect that is covered by</p><formula xml:id="formula_0">P j , i.e., U Pj (T i ) = 2π 0 1 Pj (v)dv, where 1 Pj (v) = 1 if v is covered by P j , or 0 otherwise.</formula><p>Accordingly, the utility of a set of photos P ′ = {P j : 1 ≤ j ≤ k} regarding target T i is the total portion of aspect that is covered by the photos of P ′ , i.e., U P ′ (T i ) = 2π 0 1 P ′ (v)dv, where 1 P ′ (v) = 1 if v is covered by any P j from P ′ , or 0 otherwise.</p><p>Finally, the total utility of the photos regarding all targets T = {T 1 , . . . , T m } is the sum of the utility regarding each target. It is normalized by dividing the total number of targets, i.e.,</p><formula xml:id="formula_1">U P ′ (T ) = 1 m m i=1 U P ′ (T i ).</formula><p>For example in Figure <ref type="figure" target="#fig_0">1</ref>(b), target T i is covered by photo P j . Its aspect v 1 is covered by P j but aspect v 2 is not. In fact, P j covers all the aspects in [arg( --→ T i P j ) -θ, arg( --→ T i P j ) + θ] (gray sector in Figure <ref type="figure" target="#fig_0">1(b)</ref>). Thus, P j 's utility is 2θ. If there are multiple photos covering the same target, possible overlap (darker area in Figure <ref type="figure" target="#fig_0">1</ref>(c)) among photos' coverage needs to be identified and removed. In that case, the overlap can only be counted once towards the total utility, which is reflected by the gray area in Figure <ref type="figure" target="#fig_0">1</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MAX-UTILITY WITH BANDWIDTH CONSTRAINT</head><p>In this section, we study the Max-Utility problem and its extension to an online optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Max-Utility Problem</head><p>In the scenario described in Section 2, the bandwidth constraint determines the number of photos that can be selected. The problem is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. [Max-Utility Problem]</head><p>Given a set of m targets with known locations T = {T 1 , . . . , T m } and n photos P = {P 1 , . . . , P n } with known metadata, also given a predefined positive integer B(≤ n), the problem asks for a selection of B photos P ′ out of the n candidates, such that the total utility of the selected photos U P ′ (T ) is maximized.</p><p>1. Intuitively, it should be from P j to T i , but --→ T i P j is used for ease of calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Conversion to Maximum Coverage</head><p>Without loss of generality, we first consider a single target T i and use the coverage interval I i = [0, 2π) to indicate its aspect to be covered. Let P = {P 1 , . . . , P n } be the set of all photos covering T i . Then for each P j , if T i is covered by P j , the coverage of P j on T i (gray sector in Figure <ref type="figure" target="#fig_0">1(b)</ref>) can be represented by a sub-interval of [0, 2π), i.e.,</p><formula xml:id="formula_2">S j [x j , y j ] = [arg( --→ T i P j ) -θ, arg( --→ T i P j ) + θ].<label>(1)</label></formula><p>Note that the angles are always calculated by using arithmetic modulo 2π. Here the two end points x j and y j are called dividing points, which divides I i into two parts: one is S j and the other is I i -S j . If there are more photos by which T i is covered, there would be more dividing points.</p><p>If there are multiple targets, every target corresponds to a coverage interval I i = [0, 2π) and each I i is divided into sub-intervals by the corresponding dividing points. Let U = {e 1 , . . . , e w } be a universe set with each element representing a sub-interval and w being the total number of them. The weight of the element is the length of the subinterval. For each photo P j , a subset of U can be generated based on what sub-intervals are covered by it. Let S j denote this subset. Then we have proved the following lemma: Lemma 1. A solution to the Max-Utility problem can be obtained by solving the following problem: given a universe set U of (non-negative) weighted elements, an integer B and a collection of subsets S = {S 1 , . . . , S n }, find B subsets such that the total weight of the elements covered by the selected subsets is maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Greedy Selection Algorithm</head><p>The general maximum coverage problem is proved to be NP-hard <ref type="bibr" target="#b9">[10]</ref>. A greedy algorithm can be used to find a solution. It works as a multi-round selection process. In each round, the weighted contribution (utility) of every unselected photos is calculated. The photo with the most contribution to the total utility is selected. If there are more than one photos with the most contribution, the one with the lowest index is selected. Once a photo is selected, it will be removed from the selection. The elements (sub-intervals) covered by the selected photo will be removed from future consideration. The selection process runs until B photos have been selected or every aspect of all targets has been covered, whichever comes first. Theorem 1. Let U opt be the optimal value of the total utility that can be achieved by any B photos from P . Let U greedy be the total utility achieved by the greedy selection algorithm. Then</p><formula xml:id="formula_3">U greedy ≥ [1 -(1 - 1 B ) B ] • U opt &gt; (1 - 1 e )U opt .</formula><p>Proof. From Lemma 1, a selection of B subsets implies a valid selection of B photos. Moreover, the total utility of the photos is maximized if and only if the corresponding subsets has the maximum total weight. On the other hand, the subsets selected by the greedy selection can yield a total weight that is at least (1 -1/e) times the optimal value <ref type="bibr" target="#b9">[10]</ref>. Therefore, the total utility of the selected photos is also lower bounded by (1 -1/e) times the maximum total utility.  ) indicates the angle of the viewing direction of the photo (e.g., P 10 ), which has been defined in Section 2. Based on this, each photo's coverage interval is calculated and shown in Figure <ref type="figure" target="#fig_1">2</ref>(c) according to Equation <ref type="bibr" target="#b0">(1)</ref>. Then target T 1 's coverage interval I 1 = [0, 2π) is divided into sub-intervals by the endpoints of all photos' coverage intervals (Figure <ref type="figure" target="#fig_1">2(d)</ref>). This is the universe set U which is composed of weighted elements from e 1 to e 19 , and the weight of each element is reflected by its length. Finally, each photo's coverage interval is converted into a subset S i of elements (Figure <ref type="figure" target="#fig_1">2</ref>(e)).</p><p>We select 3 photos to maximize the total utility. Initially, each S i has a weight of 2θ = 90 • , and hence S 1 is selected due to the smallest index. Elements e 11 , e 12 , e 13 , e 14 are removed from U . Second, the weight of each of S 3 , S 4 , S 5 , S 9 and S 10 is still 90 • , but for the others the weights become: S 2 is 80 • ; S 6 is 50 • ; S 7 is 20 • and S 8 is 40 • . Obviously, S 3 is selected. Then elements e 3 , e 4 , e 5 , e 6 , e 7 are removed from U . Finally, we consider the remaining subsets. The weights of S 5 , S 6 , S 7 , S 8 are unchanged, but S 2 drops to 45 • , S 9 drops to 15 • and S 10 drops to 80 • . Therefore, the last selected photo is S 5 . The final selection is S 1 , S 3 , S 5 , corresponding to P 1 , P 3 , P 5 , and the total achieved utility is 270 • . Theorem 2. For the Max-Utility problem, the worst case time complexity of the greedy algorithm is O(Bn 2 m).</p><p>Proof. Since there are n photos and m targets, calculating the coverage intervals of each photo on each target takes O(mn) time. Then, a target can be covered by at most n photos, so there are O(n) coverage intervals on a target. Sorting the endpoints of those coverage intervals gives us the universe set, which takes O(mn log n) time for all m targets.</p><p>Next, photo selection is done in B steps. There are n candidate photos to consider in the first step, but the number decreases by 1 after each step. With assumption B ≪ n, the number of candidate photos to consider is Θ(n) for any of the B steps. In total, we consider candidate photos for Θ(Bn) times. Now let's look at how much time it takes to handle one candidate photo. For each of the m targets, we need to add the weights of the elements covered by that photo. The number of those elements is proportional to the total number of elements on the target, which is in the order of O(n). For example, given effective angle θ = 45 • , a photo always covers 1/4 of the total aspects, so the elements it covers are 1/4 of the total number of elements. This means calculating the utility of one candidate photo takes O(mn). Getting these together, the selection process takes</p><formula xml:id="formula_4">O(Bn • mn) = O(Bn 2 m) time.</formula><p>To summarize, the worse case time complexity is</p><formula xml:id="formula_5">O(mn + mn log n + Bn 2 m) = O(Bn 2 m).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online Max-Utility Problem</head><p>For applications like crisis management, due to the urgency, the server should not wait for the metadata of all photos to come in before it begins the selection. Instead, it should start selecting photos from the beginning based on available metadata, and then gradually improve the photo coverage by continuously and periodically selecting photos when new ones become available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Problem Statement</head><p>Let time be divided into transmission periods. At the beginning of each period, based on the available metadata and the available bandwidth in this period, the server makes decision on what photos to be uploaded in this period, and then notify the users to transfer the photos. Let t i be the i-th period and B i be the number of photos that can be uploaded in the i-th period. Then let A i be the set of available photos (but not being uploaded yet) at the beginning of t i . Finally, the selected photos to be uploaded in t i is denoted by C i . The problem is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3. [Online Max-Utility Problem]</head><p>Given a set of m targets with known locations T = {T 1 , . . . , T m }, and the set of available photos A i at the beginning of each period t i , and suppose the event happens at period t 0 , how does one select the set of photo C i for each period in an online manner, such that C i ⊆ A i and |C i | ≤ B i , and at the end of each period t i , the total utility of all the selected photos up to t i , i.e., U C0∪...∪Ci (T ) (defined in Definition 1), is maximized?</p><p>Note that the length of the period is a parameter determined by the application, e.g., how urgent the event is and how often new photos should be collected, etc. The bandwidth constraint B i can vary from one period to another and is not necessarily a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Online Selection Algorithm and Analysis</head><p>In each period, all the photos available up to present are considered. Finding the ones that can maximize the increase of total utility is easy when the number of photos is small, and an enumeration of all possible combinations can always deliver the optimal solution. However, as the process continues and more and more photos are available, computation cost would become prohibitively high.</p><p>Our solution is to use the approximation algorithm proposed for the original Max-Utility problem. At the beginning of each period, the server selects photos one by one greedily such that each one maximizes the increase of total utility, until it reaches the number imposed by B i . Note that the conversion into a weighted set system is the same as before except that the aspects covered by photos selected in previous periods should be excluded. After that, the selected photos will be transferred immediately during the period.</p><p>Then we prove the competitive ratio <ref type="bibr" target="#b4">[5]</ref> of the above online selection algorithm. Theorem 3. For any given integer s, let U online be the total utility of the selected photos up to period t s by the online selection algorithm, and let U opt be the total utility of the optimal set of selected photos, subject to the constraints in Definition 3, then</p><formula xml:id="formula_6">1 2 (1 -(1 -1/B min ) Bmin ) • U opt ≤ U online , where B min = min{B 1 , . . . , B t }, and 1-(1-1/Bmin) B min 2 ≥ (1-1/e) 2 ≈ 0.32.</formula><p>Proof. To simplify the presentation, we first define some notations used in the proof. Let C i denote the photos selected by our algorithm in t i and C opt i denote the photos selected by an optimal algorithm including a possible offline algorithm. Meanwhile, let</p><formula xml:id="formula_7">SC i = i-1 j=1 C j and SC opt i = i-1 j=1 C opt j .</formula><p>Let a(C i ) denote the aspects (of the targets in T ) that are covered by C i but not covered by any photos in SC i . And the size of the set a(</p><formula xml:id="formula_8">C i ) is |a(C i )| = U SCi∪Ci (T ) -U SCi (T ),</formula><p>i.e., the amount of total utility increased by adding C i to the existing selection. Similarly, we can define a(C opt i ) and its size in regarding to the selection by the optimal algorithm. From this definition, we have</p><formula xml:id="formula_9">U opt = |a( s i=1 C opt i )| and U online = |a( s i=1 C i )|. For any period t k , let I k = a( k i=1 C i ) ∩ a( k i=1 C opt i ), i.e.</formula><p>, aspects that are covered by both our selection of photos and the optimal selection. Then let α = 1 -(1 -1/B min ) Bmin , where B min = min{B 1 , . . . , B t }. We will show by induction that, for any period t k , |a(</p><formula xml:id="formula_10">k i=1 C opt i ) \ I k | &lt; 1 α |a( k i=1 C i )|. (<label>2</label></formula><formula xml:id="formula_11">)</formula><p>Clearly this is true for t 0 . Suppose for t k-1 , where</p><formula xml:id="formula_12">2 ≤ k ≤ s, (2) is correct. Then consider a( k i=1 C opt i ) = a(SC opt k ) ∪ a(C opt k ). Notice that a(SC opt k ) \ I k ⊆ a(SC opt k ) \ I k-1 , as I k-1 ⊆ I k . Thus, according to induction assump- tion, |a(SC opt k ) \ I k | ≤ 1 α |a(SC k )|. Also notice that a(C opt k )\I k ⊆ a(C opt k )\A(SC k ).</formula><p>Because C k is selected by using greedy algorithm maximizing the increase of total utility, from Theorem 1, we have (1</p><formula xml:id="formula_13">-(1 - 1/B k ) B k )|a(C opt k ) \ a(SC k )| ≤ |a(C k )|. And as α ≤ 1 -(1 - 1/B k ) B k , thus, |a(C opt k ) \ I k | ≤ 1 α |a(C k )|.</formula><p>Combining the results from above two paragraphs, we have proved <ref type="bibr" target="#b1">(2)</ref>. From (2), and the fact that</p><formula xml:id="formula_14">U opt = |a( s i=1 C opt i ) \ I s | + |I s \ a( s i=1 C opt i )|</formula><p>, and the later is smaller than</p><formula xml:id="formula_15">|I s | ≤ |a( s i=1 C i )| = U online &lt; 1 α U online , we have U opt ≤ 2</formula><p>α U online , which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ACHIEVING REQUIRED UTILITY WITH MIN-SELECTION</head><p>In this section we first study the Min-Selection problem, and then extend it to consider better coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Min-Selection Problem</head><p>We consider a different scenario from the Max-Utility problem: the number of photos is minimized while the total utility is to be above a required level. In many practical applications such as virtual tours in map services, the major obstacle is to deal with the sheer amount of raw data (photos) obtained via crowdsourcing. Thus, it is desirable to remove redundancy and only keep the minimum selection of photos that satisfies the coverage requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Problem Statement</head><p>Each target T i is associated with a coverage requirement, represented by a coverage interval Note that in this problem if the requirement can not be met due to the insufficiency of the original set of photos, the best achievable utility will be used as the criteria. Here the best achievable utility on a target is the utility of all photos on it, and the best achievable total utility on all targets is the sum on each targets normalized by the number of targets.</p><formula xml:id="formula_16">I i = [a i , b i ], 0 ≤ a i , b i &lt; 2π.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Min-Selection Algorithm</head><p>In the following description, it is assumed that the coverage requirement of each target can be satisfied by the whole set of photos. Then the following theorem shows the main result of our findings. Theorem 4. Suppose the targets' coverage requirements can be satisfied by all photos in the pool and let N opt be the minimum number of photos to satisfy the requirement. There exists N approx photos that can be found in polynomial time such that each target's requirement can be met by these photos and moreover, N approx ≤ O(log mn)N opt .</p><p>Proof. We prove this by constructing the selection using a greedy algorithm.</p><p>First, we use a conversion process that is similar to Section 3.1.1. Here each target T i 's coverage requirement I i is partitioned into sub-intervals by the dividing points, and the dividing points are the end points of the coverage intervals (sub-intervals) of the photos like before. After this preparation, all the sub-intervals are numbered, and can be represented by elements that altogether form an universe set U = {e 1 , . . . , e w }, where w is the total number of sub-intervals. Then for each P j , there is a subset S j ⊂ U which is comprised of the elements corresponding to the sub-intervals covered by P j . Based on this, the problem of finding the minimum photo selection can be converted to the following problem:</p><p>Given a universe set U and a collection of subsets of U : S = {S 1 , . . . , S n }, and assume ∪ n j S j = U , how to find a subset S ′ of S such that ∪ Sj ∈S ′ S j = U and |S ′ | is minimum?</p><p>This is an instance of the set cover problem, which has been proved NP-hard <ref type="bibr" target="#b9">[10]</ref>. Thus for the Min-Selection problem, we can solve it by an approximation algorithm based on the greedy selection.</p><p>Specifically, the algorithm begins by selecting the photo (some S j ) that covers the most number of sub-intervals (elements). Once a photo is selected, it will not be removed. The sub-intervals covered will not be considered in the future. Photos are selected one by one based on how many new sub-intervals can be covered. Each time, the photo covering the most number of new sub-intervals is selected. Ties can be broken arbitrarily, e.g., by giving priority to the one with smaller index. The process stops if all sub-intervals is covered or no more photos can be selected (i.e., either photos are all selected or no more benefit can be achieved).</p><p>Once the photos are found, it is obvious all the elements in U is covered which implies the requirement of all targets are satisfied. By using similar argument from Theorem 3.1 in <ref type="bibr" target="#b9">[10]</ref>, it is easy to see the number of selected photos is upper bounded as shown in the theorem.</p><p>An Example: Again, we use Figure <ref type="figure" target="#fig_1">2</ref> to illustrate the above idea. Consider the problem settings in Figure <ref type="figure" target="#fig_1">2</ref>(a) and suppose the required coverage for T 1 is [0, 2pi). The construction of the universe set and all the subsets are shown in Figure <ref type="figure" target="#fig_1">2</ref>(b)-(e). The universe set U consists of 19 elements. The selection works on the subsets S i . First, photo S 2 is selected as it covers 5 new elements {e 7 , e 8 , e 9 , e 10 , e 11 }. It has the most elements covered and the smallest index. Then S 5 can be selected as it covers 5, the most number of new elements {e 1 , e 16 , e 17 , e 18 , e 19 }. In the third round, S 3 can be selected, as it covers 4 new elements {e 3 , e 4 , e 5 , e 6 }. After that, S 1 , which covers 3 new elements {e 12 , e 13 , e 14 }, is selected. Up to now, 17 out of the total 19 elements have been covered. The remaining two are e 2 and e 15 . To cover e 2 , S 4 is selected. Then S 6 is selected to cover e 15 . The final selection is S 1 , . . . , S 6 , which correspond to the following 6 photos: P 1 , P 2 , P 3 , P 4 , P 5 , P 6 .</p><p>The above discussion can be easily applied to the scenario of multiple targets. In that case, each target corresponds to a set U i of elements (sub-intervals). Elements of all U i will be considered to determine if a particular S j can yield the most coverage. The algorithm stops if elements of all U i are covered or no more progress can be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 5. For the Min-Selection problem, the worst case time complexity of the greedy algorithm is O(n 3 m).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. The conversion process takes time O(mn+mn log n).</head><p>For the selection process, we highlight two differences between the Max-Utility problem and the Min-Selection problem. First, in Max-Utility we select the photo that covers maximum weight of new elements, while in Min-Selection we select the photo that covers maximum number of new elements. Although weight and cardinality are different, they can be calculated by the same number of additions. Thus they are the same with regard to asymptotic running time. The second difference is that Max-Utility finishes with exactly B steps, while Min-Selection may finish in any steps between 1 and n. Considering the worst case, if Min-Selection finishes with n steps, the number of candidate photos it considers in those n steps is n + (n -1) + • • • + 1 = O(n 2 ). Since handling each candidate photo requires time O(mn), the time complexity of the entire algorithm is O(mn + mn log n + n 2 mn) = O(n 3 m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Min-Selection with k-Coverage</head><p>Some crowdsourced photos may contain inaccurate information. This may happen in an emergency situation where photos have to be taken in a very short time, leaving not much time for users to contemplate. Even if metadata can help understand how and where the photo was taken, some real photos may still miss our expectations. The inaccuracy can be caused by various reasons such as image blur due to vibration of the phone, occlusion, color abberation, or simply inaccurate metadata. To reduce the possibility that an important aspect of the object is missed, applications may require some degree of fault-tolerance, which can be achieved with k-coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">k-coverage</head><p>In this problem, an aspect is required to be covered k (k ≥ 2) times. Each target T i has a coverage requirement</p><formula xml:id="formula_17">I i = [a i , b i ], 0 ≤ a i , b i &lt; 2π</formula><p>, but now I i needs to be covered k times by the selected photos. The problem is formally defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5. [Min-Selection with k-Coverage]</head><p>Given m targets with known locations T = {T 1 , . . . , T m } and n photos P = {P 1 , . . . , P n } with known metadata, also given the coverage requirements for the targets: I = {I 1 , . . . , I m } and an integer k ≥ 2, the problem asks for a minimum number of photos out of the n candidates, such that the coverage requirement for each target is covered at least k times.</p><p>As the original Min-Selection problem can be converted to the set cover problem, the k-coverage problem can be converted in the same way to the set multicover problem. The set multicover problem, as its name suggests, differs from the set cover problem that each element e in the universe set U must appear at least k e times in the selected subsets of U , where k e is a positive integer for element e. Note that in our case, k e = k for any element e.</p><p>The greedy algorithm for the original Min-Selection problem can be naturally extended to the case of k-coverage. Specifically, an element is alive until it is covered k times. In each step, we select the photo that covers as many alive elements as possible, and then update the aliveness of the elements covered by this photo as appropriate. The selection proceeds until all elements are not alive or no more photos can be selected (either photos are all selected or no more benefit can be achieved).</p><p>Suppose the overall coverage requirements can be satisfied if all photos are selected. Dobson <ref type="bibr" target="#b7">[8]</ref> proved that the above algorithm achieves an approximation ratio of O(log mn), which means the number of photos selected by the greedy algorithm does not exceed O(log mn) times the minimum possible number. Note that this approximation ratio is the same as the ratio achieved by the Min-Selection algorithm (Theorem 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">2-coverage+</head><p>Although the k-coverage model looks good in theory, it may not work well in reality. Consider a scenario shown in Figure <ref type="figure" target="#fig_2">3</ref>. An aspect v of the target T is covered by three photos, P 1 , P 2 and P 3 . There is an obstacle between the target and the photos. By the k-coverage model, if k = 2, any combination of two photos among P 1 , P 2 and P 3 satisfies the coverage requirement on aspect v. However, choosing P 1 and P 2 will actually leave aspect v uncovered because the target is blocked in photo P 1 and P 2 .</p><p>The key observation here is that the existence of obstacles affects our choice of photos. Ideally, the server should avoid P 1 and P 2 since they are blocked by an object and do not cover aspect v. Although we will propose techniques in Section 5 to detect occlusion and filter out photos with occlusion, these techniques are not guaranteed to detect all obstacles. Hence, when selecting photos, the server should take into consideration the potential existence of obstacles. Instead of choosing k photos arbitrarily, the photos with fairly different views should be chosen, so that even one of them is blocked by an obstacle, the other still covers the aspect. Specifically, in the 2-coverage+ model, for each aspect covered by two photos, the angle between their viewing directions should be larger than a predefined threshold α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 6. [Min-Selection with 2-Coverage+]</head><p>Given m targets with known locations T = {T 1 , . . . , T m } and n photos P = {P 1 , . . . , P n } with known metadata, also given the coverage requirements for the targets: I = {I 1 , . . . , I m } and a threshold α ∈ [0, 2θ], the problem asks for a minimum number of photos such that each aspect in the coverage requirements is covered by at least 2 photos, and the maximum angle between the viewing directions of those photos is greater than α.</p><p>In this problem, the value of α determines the coverage resistance to obstacles. The larger α is, the farther the two photos are separated, and the less likely they are blocked by the same obstacle. However, α cannot be arbitrarily large. If a photo covers a given aspect v, its viewing direction must be within a 2θ range, [arg( v) -θ, arg( v) + θ] (light gray area in Figure <ref type="figure" target="#fig_2">3</ref>). This means α has an upper bound 2θ. As α becomes closer to 2θ, it becomes harder to find two photos satisfying the requirement, because the two photos have to be positioned more precisely so as to both cover the aspect and have far enough viewing directions.</p><p>The 2-coverage+ problem is equivalent to the 2-coverage problem when α = 0, so it is at least as hard as the kcoverage problem, and thus there is no polynomial time algorithm to find its optimal solution. Similar to the kcoverage problem, we can use a greedy algorithm to find a solution, but with some adjustment to the notion of aliveness. Specifically, the aliveness value of an element can be 2, 1 or 0. alive = 2 means that the element has never been covered; alive = 1 means that it has been covered at least once, and among the photos covering it, the maximum angle between their viewing directions is less than α; alive = 0 means that it has been covered at least twice, and among the photos covering it, the maximum angle between their viewing directions is no less than α.</p><p>In each step of the greedy algorithm, for each photo, we count the number of elements whose aliveness value would decrease if the photo were selected. Then we pick the photo with the largest count, and update the aliveness values accordingly. Here the decrease of an aliveness value means previously alive = 2 and after selecting the photo alive = 1, or previously alive = 1 and after selecting the photo alive = 0. This selection process continues until all alive = 0 or no more photo can be selected (either photos are all selected or no more benefit can be achieved). The performance of the greedy algorithm will be evaluated in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TESTBED IMPLEMENTATION</head><p>A prototype of SmartPhoto has been implemented in a testbed using Samsung Nexus S running Android 2.3.6, Samsung Galaxy S III running Android 4.0.4, and Google (LG) Nexus 4 running Android 4.2.</p><p>In the testbed, the smartphones take photos with the metadata automatically recorded. The metadata is a tuple comprised of a GPS location, a range indicating how far the photo can cover, a field-of-view (FoV) angle of the camera taking the photo and an orientation vector indicating the facing direction of the camera lens. After the photo has been taken, the smartphone uploads the metadata of the photo to a centralized server, which is a PC in our lab running the photo selection algorithm. Then the server notifies the smartphones to upload the selected photos. In this section, we present the technical details on how to obtain the metadata, how to improve the accuracy of orientation measurement, and how to deal with occlusion and out-offocus issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Metadata Acquisition</head><p>One critical issue is how to get the metadata from off-theshelf smartphones. The location can be directly acquired via the built-in GPS receiver. The camera's field-of-view is accessible via the Android camera API <ref type="bibr" target="#b0">[1]</ref>. The range is a little trickier as it depends on the resolution of the lens (and the image sensor), the zooming level (or focal length) and the requirement of the application. Applications requiring a survey of large scale buildings may find the photos useful even if they are taken a hundred meters away by a lower resolution camera, while others may require closer look at the object and hence may exclude photos taken more than a few meters away. In our experiment, as the subjects are buildings on campus, 50 meter is used as a reference range. We find that for our purpose, objects in photos taken within this range are generally recognizable.</p><p>Orientation is a key factor that has not yet been fully taken advantage of in previous works. The way used to characterize the orientation in the Android system is to first define a local and a world coordinate system 2 , and represent the orientation as a rotation matrix. The rotation matrix is used to transform a local coordinate tuple into a global one. Another way to represent the rotation is to use a three tuple called azimuth, pitch, and roll, which respectively indicate the phone's rotation around the Z, X and Y axes <ref type="bibr" target="#b14">[15]</ref>. The two representations are equivalent and the orientation tuple (i.e., the angles) can be derived from the rotation matrix. In the following description, we use R to denote the rotation matrix.</p><p>In Android system, the rotation matrix can be directly obtained based on accelerometer and magnetic field sensor readings. The accelerometer measures the phone's proper acceleration along the three axes in the phone's local coordinate system, including the influence of the gravity. The magnetic field sensor provides the readings measuring the ambient magnetic field along the three axes in the local coordinate system. The coordinates of both the gravity and the ambient magnetic field are known in the world coordinate system. Thus, by combining the above readings and facts, the orientation of the phone can be obtained. Let us call this the "basic" method, and let the result be denoted by R basic .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Techniques to Improve Accuracy</head><p>The rotation matrix R basic is susceptible to noise and errors. It fluctuates quickly due to the vibration of accelerometer's reading. Also, the magnet field sensor's reading is easily affected by nearby magnet objects. Even worse, R basic responses slowly to quick rotation of the phone. Thus, we propose several techniques to improve the accuracy of the orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Hybrid Method</head><p>Apart from the accelerometer and the magnetic field sensor, gyroscope is now available in most smartphones, and it can also be used to measure the rotation matrix.</p><p>Gyroscope measures the angular rotation speeds along all three axes in the phone's local coordinate system. By integrating (multiplying) the angular speed with the time interval between two consecutive sensor readings, we can obtain the rotation vector, which indicates the change of orientation in terms of rotation angles around the three axes. It can also be used to obtain the rotation matrix (denoted by ∆R gy ). Given an initial rotation matrix, which can be obtained from R basic , we can get the new rotation matrix, denoted as R gy , by R gy = R gy × ∆R gy .</p><p>However, the cumulative error caused by the integration in R gy can become greater and the result would drift as time goes by. In fact, the orientation derived from R gy alone usually drifts over 10 degrees in about 20 seconds in our lab test.</p><p>2. In a world coordinate system, Z axis is perpendicular to ground and points to the sky; Y is tangential to the ground and points towards the magnetic north pole; X is the vector product of Y and Z. In the phone's local coordinate system, Z is perpendicular to the phone screen and points outward; the X axis is along the width of the phone and the Y axis is along the length <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Thus, we propose a hybrid method which combines the readings from the above sensors to improve the accuracy of orientation, as shown in Figure <ref type="figure" target="#fig_3">4</ref> and explained as below.</p><p>First, a simple Infinite Impulse Response (IIR) low pass filter is used on R basic to remove the short term vibration, i.e.,</p><formula xml:id="formula_18">R ′ basic = R basic + µ • (R prev basic -R basic )</formula><p>, where R basic is the current reading and R prev basic is the previous reading from the basic method, and µ ∈ [0, 1] is an adjustable parameter balancing the current and previous values. In practice, we find µ = 0.3 is good for our purpose.</p><p>Second, we combine R ′ basic and R gy to take advantage of both values; that is,</p><formula xml:id="formula_19">R hybrid = ν × R gy + (1 -ν) × R ′ basic</formula><p>We find ν = 0.9 works well.</p><p>Third, R hybrid is the output, and it will also be used as the initial matrix input for the computation of a new R gy .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Enhancement by Orthonormalization</head><p>We exploit the orthonormal property of the rotation matrix to further improve the accuracy of orientation. In a valid rotation matrix, any pair of columns (or rows) of the rotation matrix are orthogonal, i.e., with unit length and vertical to each other. However, this property may be violated as errors occur. Thus, the rotation matrix R hybrid obtained from the above method can be further calibrated by an orthonormalization process (e.g., the Gram-Schmidt process <ref type="bibr" target="#b25">[26]</ref>) to get an enhanced rotation matrix R enhanced .</p><p>Specifically, consider a 3 × 3 rotation matrix: R hybrid = [α 1 , α 2 , α 3 ], with α i being a column vector. Let the inner product of the two vectors α and β be &lt; α, β &gt;= n i=1 α i β j , where n = 3 is the dimension. First, R hybrid is orthogonalized by</p><formula xml:id="formula_20">ξ 1 = α 1 , ξ 2 = α 2 - &lt; α 2 , ξ 1 &gt; &lt; ξ 1 , ξ 1 &gt; ξ 1 , ξ 3 = α 3 - &lt; α 3 , ξ 1 &gt; &lt; ξ 1 , ξ 1 &gt; ξ 1 - &lt; α 3 , ξ 2 &gt; &lt; ξ 2 , ξ 2 &gt; ξ 2 .</formula><p>Second, the above ξ i 's are normalized by</p><formula xml:id="formula_21">β i = ξ i √ &lt; ξ i , ξ i &gt; , i = 1, 2, 3.</formula><p>Then, the final rotation matrix is</p><formula xml:id="formula_22">R enhanced = [β 1 , β 2 , β 3 ].</formula><p>Comparisons: To verify the effectiveness of the optimization techniques, we measure the orientation using three different methods: the "basic" method, the "hybrid" method, and the "enhanced" method, and compare their results. We place the phone in a horizontal plane, so the orientation is reflected by the azimuth value. Then we rotate the phone 30 degrees and measure its azimuth reading against a commercial compass. Each measurement is repeated 50 times and the statistics are calculated. Figure <ref type="figure" target="#fig_4">5</ref>(a) compares the measurement errors (in degree) by these three methods. The short bar in the middle of each box is the median value of the azimuth reading error, and the lower and upper side of the box are the first (25%) and third (75%) quartile, which is denoted by Q1 and Q3.</p><p>Then the lower limit is calculated by Q1-1.5 * (Q3-Q1) and the upper limit is Q3 + 1.5 * (Q3 -Q1). More details about the average error and standard variance of each method are listed in Table <ref type="table">1</ref>. We find that the hybrid method can reduce the average measurement error by 37% compared to the basic method, and the enhanced method can further reduce the measurement error by more than 40% compared to the hybrid method. Also, new phones (e.g., Nexus 4), with more advanced hardware and OS, are more accurate with less variance. For all these phones, with our enhanced method, the average azimuth reading error is under 3.5 degrees, and the error can be reduced to 1.3 degree with Nexus 4.</p><p>To understand the effectiveness of these techniques clearly, we show the measurement results of these methods when the phone is turned to 90 degree, as illustrated in Figure <ref type="figure" target="#fig_4">5(b)</ref>. As can be seen, the basic method oscillates frequently. The hybrid method improves the accuracy compared to the basic method but still carries the reading errors. With orthonormalization, the enhanced method can significantly improve the accuracy of orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Occlusion and Out-of-focus</head><p>After a photo is taken, we assume that the user will visually check if the object appears in the photo, as most people do. However, if the user does not check the photo, and the object is blocked by unexpected obstacles such as a moving vehicle, the photo will not be useful to the server. Even if the user checks the photo and the object is clear, it may be different from what the server is expecting. For example, the server may expect the photo to be about a building, but the user may be looking at a tree in front of the building. Although in the two scenarios, the smartphone may produce the same metadata (e.g., the same facing direction), the content could be very different, and the one focusing on (blocked by) the tree is useless for the server's task. Besides this problem, there are many other occasions that the interested targets are out-of-focus. Uploading these photos will waste lots of bandwidths and storage spaces. We use a feature called focus distance, which is provided by many new smartphones with focusing capability, to solve the problem. The focus distance is the distance between the camera and the object perfectly focused in the photo. Note that the real distance between the camera and our interested target can be calculated by GPS locations. Thus in an ideal case, if the two distances do not match, the target is out-offocus and the photo should be excluded from consideration.</p><p>The measurement of the focus distance is sensible to errors. A slight offset does not necessarily mean the target is out-of-focus. In fact, in photography the distance between the nearest and farthest objects that appear acceptable sharp in a photo is called the Depth-Of-Field (DOF). DOF is determined by four parameters: focal length (f ), focus distance (v o ), lens aperture (A), and circle of confusion (CoC). Among these parameters, focal length and lens aperture are builtin and readable from the Android API. CoC (denoted by c) is a predefined number which determines the resolution limit for our application. Focus distance changes from case to case but obtainable from Android API. Therefore, we can calculate the near/far limit of DOF (Figure <ref type="figure" target="#fig_5">6(a)</ref>) by</p><formula xml:id="formula_23">D near = vo(H-f ) H+vo-2f , D f ar = vo(H-f ) H-vo</formula><p>, where H = f 2 Ac + f is the hyperfocal distance <ref type="bibr" target="#b16">[17]</ref>.</p><p>After a photo is taken, the distance between the target and the camera (phone) is compared with the above two values. If the target falls into the DOF, the photo is considered valid; otherwise, it will be dropped. This filtering is done at the user side and the metadata of unqualified photos will not be sent to the server. As an example, consider the two photos in Figure <ref type="figure" target="#fig_5">6(b)</ref>. The dictionary is the interested target. In the left photo, the near and far limit of DOF is 85cm and 105cm respectively. In the right photo, the near and far limit of DOF is 5cm and 10cm respectively. The distance between the camera and the dictionary is 100cm. Based on these parameters, it is clear that the target falls into the DOF in the left photo. From the figure we can see, the dictionary is clear in the left photo but blocked by another object in the right photo. Note that this method can detect most obstacles but not all. If the obstacle is very close to the target (their distances to the camera are almost the same), it is possible that the target is blocked but still in the DOF. Discussions: Energy is an important issue for mobile devices, especially in post-disaster scenarios. Although various built-in sensors are used to collect metadata, they do not consume too much energy due to the following two reasons. First, metadata are collected only when users are taking photos, and the sensors are inactive most of time.</p><p>Second, crowdsourcing relies on large number of users to obtain information. A single user does not need to take many photos and thus does not spend too much energy.</p><p>Photos can be of low quality due to various reasons. Over-exposure or under-exposure causes images to be too bright or too dark; camera movement and shutter speed affect how severe the image is blurred; the quality of lens and digital sensors is also important. These factors can only be analyzed by image processing. Thus, before photo selection, some efficient image processing techniques <ref type="bibr" target="#b8">[9]</ref> may be applied at the user end to filter out low quality photos. However, existing image processing techniques are computationally expensive, and thus should be carefully adapted considering the resource limitations of mobile devices. Note that our approach is not meant to replace the role played by image processing algorithms, but to serve as an important complement to improve the utility of collected photos, especially when there are resource constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PERFORMANCE EVALUATIONS</head><p>In this section, we first show a real world demo using the smartphone testbed, and then evaluate the performance of the photo selection algorithms by extensive simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Demo in a Real-World Example</head><p>The testbed in Section 5 is used in a real-world example to demonstrate the effectiveness of the proposed photo selection algorithm. In this demonstration, a landmark (a bell tower) is the target. Photos are taken by using the reprogrammed smartphones around the target with the metadata automatically recorded. The metadata of all photos are later uploaded into a centralized desktop server. There are 30 photos in total. Although most of them are taken around the target, some are not facing the target, and some are blocked by trees or other objects. Also, the distribution is not uniform, due to the reality that people are likely to take pictures of the front (more attractive) side of the building.</p><p>After the metadata is retrieved, the Max-Utility problem is solved by choosing 4 photos. The photos selected by our algorithm are shown in Figure <ref type="figure" target="#fig_6">7(a)</ref>. The positions and orientations of the photos are shown in Figure <ref type="figure" target="#fig_6">7(d)</ref>, where the original 30 photos are marked as dotted "V" shapes, and the selected photos are marked by bold lines. As a comparison, we randomly choose 4 photos as shown in Figure <ref type="figure" target="#fig_6">7</ref>(b), and randomly choose another 4 photos as shown in Figure <ref type="figure" target="#fig_6">7(c</ref>). It can be seen that the 4 photos chosen by our algorithm cover the target from 4 different locations well separated from each other, with each one from a totally different angle. The bell tower is viewable from all 4 photos. In contrast, only 2 photos in the first random selection cover the target. The third photo faces away from the target, which is because random selection does not consider the orientation. In the fourth photo, the target is blocked by a flagpole. This photo is not considered by our algorithm because the target is out of focus according to the DOF information. For the second randomly selected 4 photos, two of them cover the target but they are very similar and contain redundant information. The other two do not cover the target.  We also demonstrate the effectiveness of our algorithm for the Min-Selection problem where the coverage requirement is from 0 • to 360 • , i.e., all aspects of the target. As shown in Figure <ref type="figure" target="#fig_7">8</ref>(a), our algorithm selects 6 photos to meet the coverage requirement. The angle between any two adjacent viewing directions (dashed lines connecting the photos and the target) is less than 90 • . Since the effective angle is set to 45 • , all aspects are covered. We also compare the performance with a random selection approach, which randomly selects photos one by one until the coverage requirement is achieved. As shown in Figure <ref type="figure" target="#fig_7">8</ref>(b), the random approach has to select 15 photos to meet the same coverage requirement. The experiment based on the random selection approach is repeated 100 times, and on average 21 photos are selected to meet the same coverage requirement. This demonstrates that our algorithm can significantly reduce the number of photos selected to achieve the required coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Simulation Results</head><p>In this section, we evaluate the photo selection algorithms through simulations. Targets are randomly distributed in a 100m by 100m square area. We generate photo metadata to represent real photos taken by users as follows. The photo locations are uniformly distributed in a 200m by 200m square, with the target area in the center. The orientations are randomly distributed from 0 to 2π. The field-of-view is set to 120 • , and the range is set to 50m as discussed in Section 5.1.</p><p>During the simulation, we compare our algorithms with a random selection algorithm that randomly selects photos at each step, until the bandwidth constraint is reached or the coverage requirement is satisfied. For a fair comparison, the random selection excludes any photos that have no target covered, but only consider photos that cover at least one target, i.e., relevant photos. Note that a more naive selection could be blindly selecting photos without considering this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Results on Max-Utility Problem</head><p>In the first part, we evaluate the performance of our algorithm on addressing the Max-Utility problem. Intuitively, with more bandwidths, better coverage of the targets can be achieved. As shown in Figure <ref type="figure">9</ref>(a), both our algorithm ("ours") and the random selection ("random") achieve more utility as the bandwidth B increases. The total utility achieved by all photos ("best achievable") is also shown to provide an upper bound. The difference between our selection and the random selection is significant and the advantage of our algorithm is obvious especially when B is smaller, i.e., bandwidth is more constrained. Although the performance of both algorithms converges to the bestachievable utility as B becomes larger, the convergence of ours is much faster.</p><p>Figure <ref type="figure">9</ref>(b) shows how the total utility changes as the number of candidate photos increases while other factors including bandwidth (B = 20) remain unchanged. The advantage of our algorithm is significant across the range. Considering the bandwidth limitation (only 20 photos can be selected to cover 30 targets), the difference between the utility achieved by ours and the best achievable level is small. Moreover, our algorithm can take advantage of the increasing density of photos, and improve its performance as the number of photos increases.</p><p>Figure <ref type="figure">9</ref>(c) plots the total achieved utility against the effective angle θ, with all other factors unchanged. As the effective angle increases, the coverage intervals of photos grow accordingly, so the total utility of both our algorithm and random selection increases as expected. Moreover, when the effective angle is between 10 to 40 degrees, the utility achieved by ours increase at a faster rate (the red circle line has larger slope than the blue triangle line). This is because our algorithm tries its best to avoid coverage overlap and thus benefits more from the coverage growth of each single photo. Note that as the effective angle increases, photos have more coverage but also have more coverage overlap. When the effective angle is more than 40 degrees, the utility obtained by our algorithm approaches the best achievable, so its increasing rate cannot be as high as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Results on Online Max-Utility Problem</head><p>In this part, the algorithm for the online Max-Utility problem is evaluated. We first observe how the total utility changes as the number of periods increases in Figure <ref type="figure" target="#fig_0">10(a)</ref>. Here, targets are randomly distributed as mentioned at the beginning of Section 6.2, and there are 100 new photos available in every period. Photo parameters are shown in the figure. The normalized total utility at the end of each period is recorded. As can be seen, for both our algorithm and the random algorithm, the total utility increases as the number of periods increases. However, for our algorithm, it quickly approaches 360 • . It is actually above 350 • after t 7 , which means by that time, almost all aspects of the targets are covered by the selected photos. The random algorithm takes much longer (after t 25 ) to reach that level of coverage. Thus, our algorithm is more responsive and effective.</p><p>Next, we vary the number of new photos from 50 to 100, with other parameters the same as above except the number of periods which is now fixed to be 5. The total utility of the selected photos after t 5 is shown in Figure <ref type="figure" target="#fig_0">10(b)</ref>. As the number of available photos increases, the selection algorithm has more choices. As a result, the total utility improves and approaches 360 • in our algorithm. In comparison, the performance of the random algorithm is flat (a little fluctuated due to randomness) and very low. Given the same time period, the total utility of our algorithm is much higher than the random algorithm across the range.</p><p>We also compare the original version and the online version of the Max-Utility problem. Figure <ref type="figure" target="#fig_0">10(c)</ref> shows the utility after each photo is selected in a process of selecting 20 photos. The four lines are four greedy selections with different number of periods but the same number (1000) of total available photos. The red circle line has 1 period, so it directly selects 20 photos from 1000 candidates, which represents the algorithm for the original Max-Utility problem. The other three lines have more than 1 period, so they represent algorithms for the online problem. For example, the black square line has 4 periods, so in each period it selects 20/4=5 photos from 1000/4=250 candidates. Our first observation is that for each line, the utility gain of selecting a photo is always positive and non-increasing. This complies with the features of greedy algorithms. Secondly, the performance becomes worse if there are more periods. This is because the online selection algorithm picks a fixed number of photos from each period, even though there are fewer good photos in one period and more in another. Finally, after selecting 20 photos, the utility achieved by the 20-period line is about half the utility achieved by the 1-period line. Note that 20 periods is the most periods possible for selecting 20 photos, so it represents the worst performance of the online algorithm. Therefore, this simulation result matches what we proved in Theorem 3, i.e., the performance bound of the online algorithm is half the performance bound of the original greedy algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Results on Min-Selection Problem</head><p>In this part the Min-Selection problem is studied. In reality, the given pool of photos can be very large and the number of relevant photos (i.e., photos covering at least one target) can increase very fast as the total number of randomly taken photos increases. Then, a careful selection of photos can greatly reduce the redundancy. Figure <ref type="figure" target="#fig_0">11(a)</ref> shows the effectiveness of our selection algorithm on reducing the redundancy. There are 20 targets, and all the aspects from 0 • to 360 • are required to be covered. As the total number of photos varies from 500 to 2000, the number of related photos ("related") increases linearly. However, the number of photos selected by our algorithm ("selected by ours") to achieve the same coverage does not increase. It actually decreases slightly since our algorithm takes advantage of the increased density of the photos and improves its efficiency.</p><p>The algorithms are also evaluated under the situation that the number of targets (m) varies from 5 to 50, while the total number of photos is fixed to be 1000 and all other factors remain the same. As shown in Figure <ref type="figure" target="#fig_0">11</ref>(b), the algorithms have to select more photos to cover the increased number of targets. However, the number of photos selected by our algorithm remains very low, and the increasing speed is much slower as the number of targets increases, which is much better than the random algorithm.</p><p>In Figure <ref type="figure" target="#fig_0">11</ref>(c) we fix the number of targets as 30 and change the amount of aspects to be covered on each target. As expected, the number of photos needed to achieve the required coverage increases as more aspects are to be covered. Interestingly, the increasing rates of the lines rise on the left half but drop on the right half. The reason behind this can be explained as follows. Let us denote the amount of aspects to be covered on a target as x. The coverage requirement of target i is I i = [0, x]. When x is small (e.g. 30 • ), the photos selected to cover [0, x] also cover more than [0, x], since by definition a photo always covers a 2θ range of aspects on a target (here θ = 45 • ). Then if we increase x a little bit, not many new photos are needed to achieve the new coverage, since it is almost achieved by previous photos. As x keeps increasing, this advantage becomes weaker because the new coverage requirement [0, x] is no longer within the 2θ range. Then many new photos have to be selected to complete the coverage. That is why the increasing rate rises for small x. On the other hand, when x is large, this advantage becomes stronger as x increases, because the remaining uncovered aspects are now within the 2θ range, and they are more likely to be covered by the selected photos. This explains the drop of the increasing rate for large x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Results on Min-Selection with k-Coverage</head><p>In this part, we study the Min-Selection problem with kcoverage by comparing five different coverage settings. Three of them are based on the k-coverage model, with k = 1, 2, 3, respectively. The other two are based on the 2-coverage+ model, with α = 15 and 30 degrees. Figure <ref type="figure" target="#fig_1">12</ref>(a) plots the number of selected photos as a function of the total number of available photos. The other parameters are fixed and shown in the figure. First, it is clear that all algorithms are able to take advantage of the increased number of photos, make better choices, and reduce the number of selected photos. When comparing 1-cover, 2-cover and 3cover, the number of selected photos is almost proportional to the degree (k) of coverage. This shows that k-coverage does not fundamentally differ from single coverage. It just repeats single coverage for k times. We also compare 2cover (i.e. 2-cover+ with α = 0), 2-cover+ with α = 15, and 2-cover+ with α = 30. The α = 15 line is pretty close to the α = 0 line. However, α = 30 requires at least 25% more photos than α = 15. This suggests that the difficulty of achieving 2-coverage+ and the value of α are not linearly related. Once α becomes large, it requires much more photos to achieve the desired coverage.</p><p>In Figure <ref type="figure" target="#fig_1">12</ref>(b), the number of targets varies from 5 to 50 while the number of available photos is fixed at 1000. For a target, all the aspects from 0 • to 360 • should satisfy the required level of coverage, either k-coverage or 2-coverage+. We have similar observations as previous figures. The relationships between the lines are similar to those in Figure <ref type="figure" target="#fig_1">12</ref>(a), and the increasing trend of the lines is similar to that in Figure <ref type="figure" target="#fig_0">11(b)</ref>.</p><p>In Figure <ref type="figure" target="#fig_1">12</ref>(c), we fix the number of targets but change the amount of aspects that should satisfy the required coverage. The relationships between the lines are similar to those in Figure <ref type="figure" target="#fig_1">12</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>The mass adoption of camera sensors and other position sensors on smartphones makes photo taking and sharing via online social networks much easier and more enjoyable. It creates opportunities for many applications based on camera sensor networks, which have received much attention recently in research <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>. One basic problem is how to characterize the usefulness of the image data and how to optimize the network to achieve better quality of information. However, very little effort has been devoted to this field. One problem studied is called pan and scan <ref type="bibr" target="#b11">[12]</ref>, which is proposed to maximize the total coverage in a camera sensor networks. For camera sensor placement, various optimization models and heuristics are studied in <ref type="bibr" target="#b10">[11]</ref>. However, the coverage model is relatively simple, depending only on the distance between the target and the object, which does not consider the uniqueness of photo coverage.</p><p>Our work is inspired by the full-view coverage model which was originally proposed in <ref type="bibr" target="#b23">[24]</ref> and later extended in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>. An object is considered to be full-view covered if no matter which direction the object faces, there is always a sensor whose sensing range includes the object and that sensor's viewing direction is sufficiently close to the object's facing direction. Although our work is based on the fullview coverage model, our model is more general and we study various optimization problems on the tradeoffs between resource constraints and photo coverage.</p><p>Another interesting work is PhotoNet <ref type="bibr" target="#b21">[22]</ref>, which is a picture delivery service that prioritizes the transmission of photos by considering the location, time stamp, and color difference, with the goal of maximizing the "diversity" of the photos. Compared to their model, we consider direction and angle information, and develop techniques to obtain them from off-the-shelf smartphones. These are very important and unique features for photos and enable us to develop much finer optimization models. Moreover, the solutions to our optimization problems are rigorously analyzed.</p><p>It is also worth mentioning that there has been much progress in content-based image retrieval techniques (see <ref type="bibr" target="#b6">[7]</ref> for a good survey). These techniques have also been used for images obtained from mobile users. One example is to build photo annotated world maps and create 3D models of the objects from 2D photos via online social networks <ref type="bibr" target="#b5">[6]</ref>. Some other interesting works have been done for image retrieval/search on smartphones <ref type="bibr" target="#b26">[27]</ref>. However, most of these works involve power-intensive computation at both user and server end, and some demands human validation to be included into the cycle <ref type="bibr" target="#b26">[27]</ref>. These techniques are challenged by the content diversity and the resource constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>We proposed a resource-aware framework, called Smart-Photo, to optimize the selection of crowdsourced photos based on the accessible metadata of the smartphone including GPS location, phone orientation, etc. With this model, a remote server can efficiently evaluate and select photos from mobile users under severely constrained resources such as bandwidth, storage, computational power and device energy. Four optimization problems regarding the tradeoffs between photo coverage and resource constraints, namely Max-Utility, online Max-Utility, Min-Selection and Min-Selection with k-coverage, have been studied. The approximation bounds of the proposed algorithms are theoretically proved. We have implemented SmartPhoto in a testbed using Android based smartphones, and proposed techniques to improve the accuracy of the collected metadata and mitigate the occlusion and out-of-focus issues. Results based on real implementations and extensive simulations validated the effectiveness of the proposed algorithms.</p><p>As future work, we will consider assigning more weights to targets and aspects that are more important. For example, after a disaster, a building with more potential survivors (e.g. schools, malls) is more important than others, and its possible entrance is more important than other aspects of the building. Hence, photos covering that building and its entrance should have higher priority to be selected. We will also address other research issues such as extending the current photo model to 3D and identifying other photo selection problems based on the requirements of crowdsourcing applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>photo range</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>photos' viewing directions (degree)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The idea of 2-coverage+: two photos covering an aspect should have fairly different viewing directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Hybrid method combines the results of the basic method with gyroscope readings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Orientation errors of the three proposed methods. TABLE 1 Average error in azimuth (degree) Nexus S Nexus 4 Galaxy S III Basic 9.1(±2.0) 8.2(±1.5) 9.6(±2.4) Hybrid 5.7(±1.9) 5.1(±1.3) 7.3(±1.7) Enhanced 3.4(±1.4) 1.3(±0.7) 3.4(±1.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Using DOF to detect occlusion and out-of-focus.</figDesc><graphic coords="9,456.29,44.69,97.62,65.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Demo results based on Max-Utility. (a) Photos selected by our algorithm. (b)(c) Photos selected randomly. (d)(e)(f) The locations and orientations of the photos are marked as "V" shapes on the map.</figDesc><graphic coords="10,436.36,353.69,110.06,83.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Demo results based on Min-Selection. To cover all aspects of the target, our algorithm uses 6 photos, and a random selection uses 15 photos.</figDesc><graphic coords="10,330.07,354.21,110.06,83.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Simulation results on Max-Utility problem.Total utility / num. targets in degree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) and (b), and the increasing trend of the lines is similar to that in Figure11(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig. 11. Simulation results on Min-Selection problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>P 1 : [180, 270] P 2 : [100, 190] P 3 : [45, 135] P 4 : [0, 90] P 5 : [280, 360) È [0, 10] P 6 : [230, 320] P 7 : [200, 290] P 8 : [140, 230] P 9 : [60, 150] P 10 : [325, 360) È [0, 55]</head><label></label><figDesc></figDesc><table><row><cell>0</cell><cell>10</cell><cell>45</cell><cell>55</cell><cell>60</cell><cell>90</cell><cell>100</cell><cell>135</cell><cell>140</cell><cell>150</cell><cell>180 190</cell><cell>200</cell><cell>230</cell><cell>270</cell><cell>280</cell><cell>290</cell><cell>320</cell><cell>325</cell><cell>360</cell></row><row><cell>e</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1 e 2 e 3 e 4 e 5 e 6 e 7 e 8 e 9 e 10 e 11 e 12 e 13 e 14 e 15 e 16 e 17 e18 e 19 S 1 : {e 11 e 12 e 13 e 14 } S 2 : {e 7 e 8 e 9 e 10 e 11 } S 3 : {e 3 e 4 e 5 e 6 e 7 } S 4 : {e 1 e 2 e 3 e 4 e 5 } S 5 : {e 1 e 16 e 17 e 18 e 19 } S 6 : {e 14 e 15 e 16 e 17 } S 7 : {e 13 e 14 e 15 e 16 } S 8 : {e 9 e 10 e 11 e 12 e 13 } S 9 : {e 5 e 6 e 7 e 8 e 9 } S 10 : {e 1 e 2 e 3 e 19</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMC.2015.2444379, IEEE Transactions on Mobile Computing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>1536-1233 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Science Foundation (NSF) under grant CNS-1421578.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sensormanager -Android</forename><surname>Developer</surname></persName>
		</author>
		<ptr target="http://developer.android.com/reference/android/hardware/SensorManager.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on wireless multimedia sensor networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Akyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Melodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Netw</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="921" to="960" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">We the Media: Grassroots Journalism by the People, for the People</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Spencer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>O&apos;Reilly Media, Sebastopol, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition based on frontal views generated from non-frontal images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<title level="m">Online Computation and Competitive Analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling people and places with internet photo collections</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Worst-case analysis of greedy heuristics for integer programming with nonnegative data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="515" to="531" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blind image quality assessment through anisotropy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gabarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="42" to="B51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Approximation Algorithms for NP-Hard Problems</title>
		<editor>D. S. Hochbaum</editor>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>PWS Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the optimal placement of multiple visual sensors</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Workshop on Video Surveillance and Sensor Networks (VSSN)</title>
		<meeting>ACM International Workshop on Video Surveillance and Sensor Networks (VSSN)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pan and scan: Configuring cameras for coverage</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Noy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Information processing for live photo mosaic with a group of wireless image sensors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barabas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE Conference on Information Processing in Sensor Networks (IPSN)</title>
		<meeting>ACM/IEEE Conference on Information essing in Sensor Networks (IPSN)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">In search of the bigger picture: The emergent role of on-line photo-sharing in times of disaster</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Palen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vieweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCRAM</title>
		<meeting>ISCRAM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Professional Android 2 Application Development, 2nd Ed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Wiley Publishing, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FRVT 2006 and ICE 2006 large-scale results</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7408</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Applied Photographic Optics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Focal Press</publisher>
			<pubPlace>Oxford, UK</pubPlace>
		</imprint>
	</monogr>
	<note>3 edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A bright future for distributed smart cameras</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1562" to="1564" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient background subtraction for tracking in embedded camera networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE IPSN</title>
		<meeting>ACM/IEEE IPSN</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photo tourism: Exploring photo collections in 3d</title>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey of visual sensor networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heinzelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photonet: A similarity-aware picture delivery service for situation awareness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y S</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Real-Time Systems Symposium (RTSS)</title>
		<meeting>IEEE Real-Time Systems Symposium (RTSS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Barrier coverage in camera sensor networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Mobihoc</title>
		<meeting>ACM Mobihoc</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On full-view coverage in camera sensor networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Achieving full-view coverage in camera sensor networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Sensor Networks (ToSN)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Linear Algebra With Applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Jones &amp; Bartlett Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowdsearch: exploiting crowds for accurate real-time image search on mobile phones</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MobiSys</title>
		<meeting>ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">iScope: personalized multi-modality image search for mobile devices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MobiSys</title>
		<meeting>ACM MobiSys</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
