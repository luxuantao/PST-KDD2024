<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multitask Learning Strengthens Adversarial Robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-14">14 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amogh</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
							<email>vikram.nitin@columbia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
							<email>vondrick@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multitask Learning Strengthens Adversarial Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-14">14 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2007.07236v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multi-task Learning, Adversarial Robustness Equal Contribution. Order is alphabetical</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although deep networks achieve strong accuracy on a range of computer vision benchmarks, they remain vulnerable to adversarial attacks, where imperceptible input perturbations fool the network. We present both theoretical and empirical analyses that connect the adversarial robustness of a model to the number of tasks that it is trained on. Experiments on two datasets show that attack difficulty increases as the number of target tasks increase. Moreover, our results suggest that when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks. While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep networks obtain high performance in many computer vision tasks <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b16">18]</ref>, yet they remain brittle to adversarial examples. A large body of work has demonstrated that images with human-imperceptible noise <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b38">40]</ref> can be crafted to cause the model to mispredict. This pervasiveness of adversarial examples exposes key limitations of deep networks, and hampers their deployment in safety-critical applications, such as autonomous driving.</p><p>A growing body of research has been dedicated to answering what causes deep networks to be fragile to adversarial examples and how to improve robustness <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b47">49]</ref>. The investigations center around two factors: the training data and the optimization procedure. For instance, more training data -both labeled and unlabeled -improves robustness <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b50">52]</ref>. It has been theoretically shown that decreasing the input dimensionality of data improves robustness <ref type="bibr" target="#b45">[47]</ref>. Adversarial training <ref type="bibr" target="#b33">[35]</ref> improves robustness by dynamically augmenting the training data using generated adversarial examples. Similarly, optimization procedures that regularize the learning specifically with robustness Fig. <ref type="figure">1</ref>: We find that multitask models are more robust against adversarial attacks. Training a model to solve multiple tasks improves the robustness when one task is attacked. The middle and right column show predictions for single-task and multitask models when one task is adversarially attacked.</p><p>losses have been proposed <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b5">7]</ref>. This body of work suggests that the fragility of deep networks may stem from the training data and optimization procedure.</p><p>In this paper, we pursue a new line of investigation: how learning on multiple tasks affects adversarial robustness. While previous work shows that multitask learning can improve the performance of specific tasks <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b46">48]</ref>, we show that it increases robustness too. See Figure <ref type="figure">1</ref>. Unlike prior work that trades off performance between natural and adversarial examples <ref type="bibr" target="#b49">[51]</ref>, our work improves adversarial robustness while also maintaining performance on natural examples.</p><p>Using the first order vulnerability of neural networks <ref type="bibr" target="#b45">[47]</ref>, we theoretically show that increasing output dimensionality -treating each output dimension as an individual task -improves the robustness of the entire model. Perturbations needed to attack multiple output dimensions cancel each other out. We formally quantify and upper bound how much robustness a multitask model gains against a multitask attack with increasing output dimensionality.</p><p>We further empirically show that multitask learning improves the model robustness for two classes of attack: both when a single task is attacked or several tasks are simultaneously attacked. We experiment with up to 11 vision tasks on two natural image datasets, Cityscapes <ref type="bibr" target="#b6">[8]</ref> and Taskonomy <ref type="bibr" target="#b58">[60]</ref>. When all tasks are under attack, multitask learning increases segmentation robustness by up to 7 points and reduces the error of other tasks up to 60% over baselines. We compare the robustness of a model trained for a main task with and without an auxiliary task. Results show that, when the main task is under attack, multitask learning improves segmentation overlap by up to 6 points and reduces the error of the other tasks by up to 23%. Moreover, multitask training is a complementary defense to adversarial training, and it improves both the clean and adversarial performance of the state-of-the-art, adversarially trained, single-task models.</p><p>Code is available at https://github.com/columbia/MTRobust.</p><p>Overall, our experiments show that multitask learning improves adversarial robustness while maintaining most of the the state-of-the-art single-task model performance. While defending against adversarial attacks remains an open problem, our results suggest that current deep networks are vulnerable partly because they are trained for too few tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We briefly review related work in multitask learning and adversarial attacks.</p><p>Multitask Learning: Multitask learning <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b46">48]</ref> aims to solve several tasks at once, and has been used to learn better models for semantic segmentation <ref type="bibr" target="#b26">[28]</ref>, depth estimation <ref type="bibr" target="#b51">[53]</ref>, key-point prediction <ref type="bibr" target="#b21">[23]</ref>, and object detection <ref type="bibr" target="#b27">[29]</ref>. It is hypothesized that multitask learning improves the performance of select tasks by introducing a knowledge-based inductive bias <ref type="bibr" target="#b2">[4]</ref>. However, multi-objective functions are hard to optimize, where researchers design architectures <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b28">30]</ref> and optimization procedures <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b57">59]</ref> for learning better multitask models. Our work complements this body of work by linking multitask learning to adversarial robustness.</p><p>Adversarial Attacks: Current adversarial attacks manipulate the input <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b35">37]</ref> to fool target models. While attacking single output models <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b24">26]</ref> is straightforward, Arnab et. al. <ref type="bibr" target="#b0">[2]</ref> empirically shows the inherent hardness of attacking segmentation models with dense output. Theoretical insight of this robustness gain, however, is missing in the literature. While past theoretical work showed the hardness of multi-objective optimization <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b42">44]</ref>, we leverage this motivation and prove that multitask models are robust when tasks are simultaneously attacked. Our work contributes both theoretical and empirical insights on adversarial attacks through the lens of multitask learning.</p><p>Adversarial Robustness: Adversarial training improves models' robustness against attacks, where the training data is augmented using adversarial samples <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b33">35]</ref>. In combination with adversarial training, later works <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b53">55]</ref> achieve improved robustness by regularizing the feature representations with additional loss, which can be viewed as adding additional tasks. Despite the improvement of robustness, adversarially trained models lose significant accuracy on clean (unperturbed) examples <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b59">61,</ref><ref type="bibr" target="#b49">51]</ref>. Moreover, generating adversarial samples slows down training several-fold, which makes it hard to scale adversarial training to large datasets.</p><p>Past work revealed that model robustness is strongly connected to the gradient of the input, where models' robustness is improved by regularizing the gradient norm <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b5">7]</ref>. Parseval <ref type="bibr" target="#b5">[7]</ref> regularizes the Lipschitz constant-the maximum norm of gradient-of the neural network to produce a robust classifier, but it fails in the presence of batch-normalization layers. <ref type="bibr" target="#b39">[41]</ref> decreases the input gradients norm. These methods can improve the model's robustness without compromising clean accuracy. Simon-Gabriel et al. <ref type="bibr" target="#b45">[47]</ref> conducted a theoretical analysis of the vulnerability of neural network classifiers, and connected gradient norm and adversarial robustness. Our method enhances robustness by training a multitask model, which complements both adversarial training <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b59">61]</ref> and existing regularization methods <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b5">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial Setting</head><p>The goal of an adversary is to "fool" the target model by adding humanimperceptible perturbations to its input. We focus on untargeted attacks, which are harder to defend against than targeted attacks <ref type="bibr" target="#b12">[14]</ref>. We classify adversarial attacks for a multitask prediction model into two categories: adversarial attacks that fool more than one task at once (multitask attacks), and adversarial attacks that fool a specific task (single-task attacks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multitask Learning Objective</head><p>Notations. Let x denote an input example, and y c denote the corresponding ground-truth label for task c. In this work, we focus on multitask learning with shared parameters <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b25">27]</ref>, where all the tasks share the same "backbone network" F (•) as a feature extractor with task-specific decoder networks D c (•). The task-specific loss is formulated as:</p><formula xml:id="formula_0">L c (x, y c ) = (D c (F (x)), y c ),<label>(1)</label></formula><p>where is any appropriate loss function. For simplicity, we denote (y 1 , ..., y M ) as y, where M is the number of tasks. The total loss for multitask learning is a weighted sum of all the individual losses:</p><formula xml:id="formula_1">L all (x, y) = M c=1 λ c L c (x, y c )<label>(2)</label></formula><p>For the simplicity of theoretical analysis, we set λ c = 1 M for all c = 1, ..., M , such that M c=1 λ c = 1. In our experiments on real-world datasets, we will adjust the λ c accordingly, following standard practice <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b25">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Multitask Attack Objective</head><p>The goal of a multitask attack is to change multiple output predictions together. For example, to fool an autonomous driving model, the attacker may need to deceive both the object classification and depth estimation tasks. Moreover, if we regard each output pixel of a semantic segmentation task as an individual task, adversarial attacks on segmentation models need to flip multiple output pixels, so we consider them as multitask attacks. We also consider other dense output tasks as a variant of multitask, such as depth estimation, keypoints estimation, and texture prediction.</p><p>In general, given an input example x, the objective function for multitask attacks against models with multiple outputs is the following:</p><formula xml:id="formula_2">argmax x adv L all (x adv , y) s.t. ||x adv − x|| p ≤ r<label>(3)</label></formula><p>where the attacker aims to maximize the joint loss function by finding small perturbations within a p-norm bounded distance r of the input example. Intuitively, a multitask attack is not easy to perform because the attacker needs to optimize the perturbation to fool each individual task simultaneously. The robustness of the overall model can be a useful property -for instance, consider an autonomous-driving model trained for both classification and depth estimation. If either of the two tasks is attacked, the other can still be relied on to prevent accidents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial Single-Task Attack Objective</head><p>In contrast to a multitask attack, a single-task attack focuses on a selected target task. Compared with attacking all tasks at once, this type of attack is more effective for the target task, since the perturbation can be designed solely for this task without being limited by other considerations. It is another realistic type of attack because some tasks are more important than the others for the attacker. For example, if the attacker successfully subverts the color prediction for a traffic light, the attacker may cause an accident even if the other tasks predict correctly. The objective function for single-task attack is formulated as:</p><formula xml:id="formula_3">argmax x adv L c (x adv , y c ), s.t.||x adv − x|| p ≤ r<label>(4)</label></formula><p>For any given task, this single-task attack is more effective than jointly attacking the other tasks. We will empirically demonstrate that multitask learning also improves model robustness against this type of attack in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>We present theoretical insights into the robustness of multitask models. A prevalent formulation of multitask learning work uses shared backbone network with task-specific branches <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b25">27]</ref>. We denote the multitask predictor as F and each individual task predictor as F c . Prior work <ref type="bibr" target="#b45">[47]</ref> showed that the norm of gradients captures the vulnerability of the model. We thus measure the multitask models' vulnerability with the same metric. Since we are working with deep networks, we assume all the functions here are differentiable. Details of all proofs are in the supplementary material.</p><p>Definition 1. Given classifier F , input x, output target y, and loss L(x, y) = (F (x), y), the feasible adversarial examples lie in a p-norm bounded ball with radius r, B(x, r) := {x adv , ||x adv − x|| p &lt; r}. Then adversarial vulnerability of a classifier over the whole dataset is</p><formula xml:id="formula_4">E x [∆L(x, y, r)] = E x [ max ||δ||p&lt;r |L(x, y) − L(x + δ, y)|]</formula><p>∆L captures the maximum change of the output loss from arbitrary input change δ inside the p-norm ball. Intuitively, a robust model should have smaller change of the loss given any perturbation of the input. Given the adversarial noise is imperceptible, i.e., r → 0, we can approximate ∆L with a first-order Taylor expansion <ref type="bibr" target="#b45">[47]</ref>.</p><p>Lemma 1. For a given neural network F that predicts multiple tasks, the adversarial vulnerability is</p><formula xml:id="formula_5">E x [∆L(x, y, r)] ≈ E x [||∂ x L all (x, y)|| q ] • ||δ|| p ∝ E x [||∂ x L all (x, y)|| q ]</formula><p>where q is the dual norm of p, which satisfies 1 p + 1 q = 1 and 1 ≤ p ≤ ∞. Without loss of generality, let p = 2 and q = 2. Note that from equation 2, we get L all (x, y) = M c=1 1 M L c (x, y c ). Thus we get the following equation:</p><formula xml:id="formula_6">∂ x L all (x, y) = ∂ x M c=1 1 M L c (x, y c ) = 1 M M c=1 ∂ x L c (x, y c )<label>(5)</label></formula><p>We denote the gradient for task c as r c , i.e., r c = ∂ x L c (x, y c ). We propose the following theory for robustness of different numbers of randomly selected tasks.</p><p>Theorem 1. (Adversarial Vulnerability of Model for Multiple Correlated Tasks) If the selected output tasks are correlated with each other such that the covariance between the gradient of task i and task j is Cov(r i , r j ), and the gradient for each task is i.i.d. with zero mean (because the model is converged), then adversarial vulnerability of the given model is proportional to</p><formula xml:id="formula_7">1 + 2 M M i=1 i−1 j=1 Cov(ri,rj ) Cov(ri,ri) M</formula><p>where M is the number of output tasks selected.</p><p>The idea is that when we select more tasks as attack targets, the gradients for each of the individual tasks on average cancels out with each other. We define the joint gradient vector R as follows:</p><formula xml:id="formula_8">R = ∂ x L all (x, y) = 1 M M c=1 ∂ x L c (x, y c )</formula><p>The joint gradient is the sum of gradients from each individual task. We then obtain the expectation of the L 2 norm of the joint gradient:</p><formula xml:id="formula_9">E( R 2 2 ) = E 1 M M i=1 r i 2 2 = 1 M 2 E   M i=1 r i 2 + 2 M i=1 i j=1 r i r j   = 1 M 2   M i=1 E[Cov(r i , r i )] + 2 M i=1 i j=1 E[Cov(r i , r j )]  </formula><p>The last equation holds due to the 0 mean assumption of the gradient. For further details of the proof, please see the supplementary material.</p><p>Corollary 1. (Adversarial Vulnerability of Model for Multiple Independent Tasks) If the output tasks selected are independent of each other, and the gradient for each task is i.i.d. with zero mean, then the adversarial vulnerability of given model is proportional to 1 √ M , where M is the number of independent output tasks selected.</p><p>Based on the independence assumption, all covariances becomes zero. Thus Theorem 1 can be simplified as:</p><formula xml:id="formula_10">E[ ∂ x L all (x, y) 2 2 ] = E( R 2 2 ) = 1 M E r i 2 = σ 2 M ∝ 1 M<label>(6)</label></formula><p>Remark 1. By increasing the number of output tasks M , the first order vulnerability <ref type="bibr" target="#b45">[47]</ref> of network decreases. In the ideal case, if the model has an infinite number of uncorrelated tasks, then it is impossible to find an adversarial examples that fools all the tasks. Remark 2. Theorem 1 studies the robustness for multiple correlated tasks, which is true for most computer vision tasks. The independent tasks assumption in Corollary 1 is a simplified, idealistic instance of Theorem 1 that upperbounds the robustness of models under multitask attacks. Together, Theorem 1 and Corollary 1.1 demonstrate that unless the tasks are 100% correlated (the same task), multiple tasks together are more robust than each individual one.</p><p>Our theoretical analysis shows that more outputs, especially if they are less correlated, improve the model's robustness against multitask attacks. Past work shows that segmentation is inherently robust <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b4">6]</ref> compared to classification. Our analysis provides a formal explanation to this inherent robustness because a segmentation model can be viewed as a multitask model (one task per pixel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We validate our analysis with empirical results on the Cityscapes and the Taskonomy datasets. We evaluate the robustness of multitask models against two types of attack: multitask attack (Section 5.3) and single-task attacks (Section 5.4). We also conduct multitask learning experiments on adversarial training and show that they are complementary (Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Cityscapes. The Cityscapes dataset <ref type="bibr" target="#b6">[8]</ref> consists of images of urban driving scenes. We study three tasks: semantic segmentation, depth estimation, and image reconstruction. We use the full resolution (2048 × 1024) for analyzing pretrained state-of-the-art models. We resize the image to 680 × 340 to train our single task (baseline) and multitask models.<ref type="foot" target="#foot_0">1</ref> Taskonomy. The Taskonomy dataset <ref type="bibr" target="#b58">[60]</ref> consists of images of indoor scenes. We train on up to 11 tasks: semantic segmentation (s), depth euclidean estimation (D), depth zbuffer estimation (d), normal (n), edge texture (e), edge occlusion (E), keypoints 2D (k), keypoints 3D (K), principal curvature (p), reshading (r), and image reconstruction (A). We use the "tiny" version of their dataset splits <ref type="bibr">[1]</ref>. We resize the images to 256 × 256.  We do not use the DAG <ref type="bibr" target="#b52">[54]</ref> attack for segmentation because it is an unrestricted attack without controlling L ∞ bound. For all the iterative attacks, the step size is 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multitask Models Against Multitask Attack</head><p>High Output Dimensionality as Multitask. Our experiment first studies the effect of a higher number of output dimensions on adversarial robustness. As an example, we use semantic segmentation. The experiment uses a pre-trained Dilated Residual Network (DRN-105) <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref> model on the Cityscapes dataset. To obtain the given output dimensionality, we randomly select a subset of pixels from the model output. We mitigate the randomness of the sampling by averaging the results over 20 random samples. Random sampling is a general dimension reduction method, which preserves the correlation and structure for high dimensional, structured data <ref type="bibr" target="#b20">[22]</ref>. Figure <ref type="figure">4a</ref> shows that the model's vulnerability (as measured by the norm of the gradients) decreases as the number of output dimension increases, which validates Theorem 1.</p><p>Besides the norm of gradient, we measure the performance under FGSM <ref type="bibr" target="#b15">[17]</ref> and PGD <ref type="bibr" target="#b33">[35]</ref> adversarial attacks, and show that it improves as output dimensionality increases (Figure <ref type="figure">4b</ref>). Notice when few pixels are selected, the robustness gains are faster. This is because with fewer pixels: (1) the marginal gain of the inverse function is larger; and (2) the select pixels are sparse and tend to be far away and uncorrelated to each other. The correlation between the output pixels compounds as more nearby pixels are selected, which slows down the improvements to robustness. The results demonstrate that models with higher output dimension/diversity are inherently more robust against adversarial attacks, consistent with the observation in <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b4">6]</ref> and our Theorem 1.</p><formula xml:id="formula_11">Clean =1 =2 =4 =8 =16</formula><p>Attack Strength  x-axis is the attack strength, ranging from no attack (clean) to the strongest attack ( = 16 PGD). For each subfigure, the y-axis shows the performance of one task under multitask attack. ↑ means the higher, the better. ↓ means the lower, the better. The multitask model names are in the legend, we refer to the task by their initials, e.g., 'sde' means the model is trained on segmentation, depth, and edge simultaneously. The blue line is the single-task baseline performance, the other lines are multitask performance. The figures show that it is hard to attack all the tasks in a multitask model simultaneously. Thus multitask models are more robust against multitask attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Tasks.</head><p>We now consider the case where the number of tasks increases, which is a second factor that increases output dimensionality. We evaluate the robustness of multitask models on the Cityscapes and Taskonomy datasets. We equally train all the tasks with the shared backbone architecture mentioned in Section 3. On Cityscapes, we use DRN-105 model as the architecture for encoder and decoder; on Taskonomy, we use Resnet-18 <ref type="bibr" target="#b58">[60]</ref>. Each task has its own decoder. For the Cityscapes dataset, we start with training only the semantic segmentation task, then add the depth estimation and input reconstruction task. For the Taskonomy dataset, following the setup in <ref type="bibr" target="#b46">[48]</ref>, we start with only semantic segmentation, and add depth estimation, normal, keypoints 2D, edge texture, and reshading tasks to the model one by one. In our figures and tables, we refer to these tasks by the task's first letter.</p><p>Figure <ref type="figure">4c</ref> shows the L2 norm of the joint gradient for many tasks, which measures the adversarial vulnerability. Overall, as we add more tasks, the norm of the joint gradient decreases, indicating improvement to robustness <ref type="bibr" target="#b45">[47]</ref>. The only exception is the depth estimation task, which we believe is due to the large range of values (0 to +∞) that its outputs take. Empirically, a larger output range leads to a larger loss, which implies a larger gradient value. We additionally measure the robust performance on different multitask models under multitask attacks. Following the setup in <ref type="bibr" target="#b0">[2]</ref>, we enumerate the of the L ∞ attack from 1 to 16. Figure <ref type="figure" target="#fig_1">5</ref> shows the robustness of multitask models using Taskonomy, where the adversarial robustness of multitask models are better than single-task models, even if the clean performance of multitask models may be lower. We also observe some tasks gain more robustness compared to other tasks when they are attacked together, which suggests some tasks are inherently harder to attack. Overall, the attacker cannot simultaneously attack all the tasks successfully, which results in improved overall robustness of multitask models. In Table <ref type="table" target="#tab_2">1</ref>, we observe the same improvement on Cityscapes. Qualitative results are shown in Figure <ref type="figure">2</ref> and Figure <ref type="figure">3</ref>. Please see the supplemental material for additional results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multitask Models Against Single-Task Attacks</head><p>Following the setup for multitask learning in <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b25">27]</ref>, we train the multitask models using a main task and auxiliary tasks, where we use λ = 1 for the main task and λ a for the auxiliary tasks. We then evaluate the robustness of the main task under single-task attacks. On Cityscapes, the main and the auxiliary tasks share 16 layers of an encoding backbone network. The decoding network for  each individual task has 6 layers. For all the models, we train for 200 epochs. For adversarial robustness evaluation, we use strong attacks including PGD100 and MIM100 for attacking the segmentation accuracy 2 , and use 100 steps Houdini <ref type="bibr" target="#b4">[6]</ref> to attack the non-differentiable mIoU of the Segmentation model directly.</p><p>We do not use Houdini to attack the depth because the L1 loss for depth is differentiable and does not need any surrogate loss. The results in Table <ref type="table" target="#tab_3">2</ref> show that multitask learning improves the segmentation mIoU by 1.2 points and the performance of depth estimation by 11% under attack, while maintaining the performance on most of the clean examples. Qualitative results are in Figure <ref type="figure" target="#fig_2">6</ref>.</p><p>On the Taskonomy dataset, we conduct experiments on 11 tasks. Following the setup in <ref type="bibr" target="#b46">[48]</ref>, we use ResNet-18 <ref type="bibr" target="#b17">[19]</ref> as the shared encoding network, where each individual task has its own prediction network using the encoded representation. We train single-task models for each of the 11 tasks as baselines. We train a total of 110 multitask models -each main task combined with 10 different auxilliary tasks -for 11 main tasks. We evaluate both the clean performance and adversarial performance. λ a is either 0.1 or 0.01 based on the tasks. We use PGD attacks bounded with L ∞ = 4 with 50 steps, where the step size is 1. The attack performance plateaus for more steps. Figure <ref type="figure" target="#fig_4">7</ref> shows the performance of the main task on both clean and adversarial examples. While maintaining the performance on clean examples (average improvement of 4.7%), multitask learning improves 90/110 the models' performance under attacks, by an average of 10.23% relative improvement. Our results show that one major advantage of   multitask learning, which to our knowledge is previously unknown, is that it improves the model's robustness under adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Multitask Learning Complements Adversarial Training</head><p>We study whether multitask learning helps adversarial robust training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The widening deployment of machine learning in real-world applications calls for versatile models that solve multiple tasks or produce high-dimensional outputs.</p><p>Our theoretical analysis explains that versatile models are inherently more robust than models with fewer output dimensions. Our experiments on real-world datasets and common computer vision tasks measure improvements in adversarial robustness under attacks. Our work is the first to connect this vulnerability with multitask learning and hint towards a new direction of research to understand and mitigate this fragility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 3 :Fig. 4 :</head><label>234</label><figDesc>Fig. 2: We show model predictions on Cityscapes under multitask attack. The single-task segmentation model misclassifies the 'road' as 'sidewalk' under attack, while the multitask model can still segment it correctly. The multitask models are more robust than the single-task trained model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Adversarial robustness against multitask attack on Taskonomy dataset. The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Performance of single-task attack for multitask models trained on Cityscapes.We show segmentation under attack for single-task and three multitask models. The multitask trained model out-performs the single-task trained model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: We consider models trained on two tasks. In each matrix, the rows show the first training task and the testing task. The columns show the auxiliary training task. The first column without color shows the absolute value for the baseline model (single-task). The middle colored columns show the relative improvement of multitask models over the single-task model in percentage. The last colored column shows the average relative improvement. We show results for both (a) adversarial and (b) clean performance. Multitask learning improves the performance on clean examples for 70/110 cases, and the performance on adversarial examples for 90/110 cases. While multitask training does not always improve clean performance, we show multitask learning provides more gains for adversarial performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The</figDesc><table><row><cell></cell><cell cols="3">Baseline Multitask</cell><cell></cell><cell cols="3">Baseline Multitask</cell></row><row><cell>Training Tasks</cell><cell>s</cell><cell>sd</cell><cell>sdA</cell><cell>Training Tasks</cell><cell>d</cell><cell>sd</cell><cell>sdA</cell></row><row><cell cols="4">Clean SemSeg ↑ 44.77 46.53 45.82</cell><cell>Clean Depth ↓</cell><cell cols="3">1.82 1.780 1.96</cell></row><row><cell>PGD SemSeg ↑</cell><cell cols="3">15.75 16.01 16.36</cell><cell>PGD Depth ↓</cell><cell>6.81</cell><cell cols="2">6.08 5.81</cell></row></table><note>models' performances under multitask PGD attack and clean images on Cityscapes using DRN-D-105<ref type="bibr" target="#b56">[58]</ref> . The bold demonstrate the better performance for each row, underline shows inferior results of multitask learning. The results show that multitask models are overall more robust under multitask attack.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Model's robust performance under L∞ = 4 bounded single-task attacks on Cityscapes. Each column is a DRN-22 model trained on a different combination of tasks, where "s,""d,"and"A"denote segmentation, depth, and auto-encoder, respectively. ↑ means the higher, the better. ↓ means the lower, the better. Bold in each row, shows the best performance under the same attack. Multitask learning models out-perform single-task models except for the underlined ones. While nearly maintaining the performance on clean examples, multitask models are consistently more robust under strong adversarial attacks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>We use DRN-22 on the Cityscapes dataset, and train both single-task and multitask models for 200 epoch under the same setup. The single-task model follows the</figDesc><table><row><cell></cell><cell></cell><cell cols="4">SemSeg mIoU Score ↑</cell><cell cols="3">Depth Abs Error ↓</cell></row><row><cell></cell><cell></cell><cell cols="7">Baseline Multitask Learning Baseline Multitask Learning</cell></row><row><cell></cell><cell>Training Tasks − →</cell><cell>s</cell><cell>sd</cell><cell>sA</cell><cell>sdA</cell><cell>d</cell><cell>ds dA</cell><cell>dAs</cell></row><row><cell></cell><cell>Clean</cell><cell cols="4">41.95 43.27 43.65 43.26</cell><cell cols="2">2.24 2.07 2.15</cell><cell>2.15</cell></row><row><cell>Attacks</cell><cell>PGD50 PGD100 MIM100 Houdini100</cell><cell cols="4">19.73 22.08 20.45 21.93 19.63 21.96 20.31 21.83 19.54 21.89 20.20 21.74 17.05 19.45 17.36 19.16</cell><cell cols="2">2.85 2.61 2.75 2.85 2.61 2.75 2.85 2.61 2.75 ---</cell><cell>2.67 2.67 2.67 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Adversarial robustness of adversarial training models under L∞ = 4 bounded attacks on Cityscapes. Each column is a model trained on a different combination of tasks. "s,""d,"and"A"denote segmentation, depth, and auto-encoder respectively. ↑ indicates the higher, the better. The ↓ indicates the lower, the better. Bold shows the best performance of the same task for each row. Multitask learning improves both the clean performance and robustness upon single-task learning.standard adversarial training algorithm, where we train the model on the generated single-task (segmentation) adversarial attacks. For the multitask adversarial training, we train it on the generated multitask attack images for both semantic segmentation and the auxiliary task. Details are in the supplementary material. Table3shows that multitask learning improves the robust performance of both clean examples and adversarial examples, where segmentation mIoU improves by 2.40 points and depth improves by 8.4%.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the same dimension for baselines and ours during comparison because input dimension impacts robustness<ref type="bibr" target="#b45">[47]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We evaluate the model robustness with L ∞ bounded adversarial attacks, which is a standard evaluation metric for adversarial robustness <ref type="bibr" target="#b33">[35]</ref>. We evaluate with four different attacks:</p><p>FGSM: We evaluate on the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b15">[17]</ref>, which generates adversarial examples x adv by x adv = x + • sign(∇ x (F (x), y)). It is a single step, non-iterative attack.</p><p>PGD: Following the attack setup for segmentation in <ref type="bibr" target="#b0">[2]</ref>, we use the widely used attack PGD (Iteratively FGSM with random start <ref type="bibr" target="#b33">[35]</ref>), set the number of iterations of attacks to min( + 4, 1.25 ) and step-size α = 1. We choose the L ∞ bound from {1, 2, 4, 8, 16} where noise is almost imperceptible. Under = 4, we also evaluate the robustness using PGD attacks with {10, 20, 50, 100} steps, which is a stronger attack compared to 5 steps attack used in <ref type="bibr" target="#b0">[2]</ref>.</p><p>MIM: We also evaluate on MIM attack <ref type="bibr" target="#b9">[11]</ref>, which adds momentum to iterative attacks to escape local minima and won the NeurIPS 2017 Adversarial Attack Competition.</p><p>Houdini: We evaluate the semantic segmentation task with the state-of-theart Houdini attack <ref type="bibr" target="#b4">[6]</ref>, which directly attacks the evaluation metric, such as the non-differentiable mIoU criterion (mean Intersection over Union). Acknowledgements: This work was in part supported by Amazon Research Award; NSF grant CNS-15-64055; NSF-CCF 1845893; NSF-IIS 1850069; ONR grants N00014-16-1-2263 and N00014-17-1-2788; a JP Morgan Faculty Research Award; a DiDi Faculty Research Award; a Google Cloud grant; an Amazon Web Services grant. The authors thank Vaggelis Atlidakis, Augustine Cha, Dídac Surís, Lovish Chum, Justin Wong, and Shunhua Jiang for valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the robustness of semantic segmentation models to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997-07">Jul 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Houdini: Fooling deep structured prediction models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Costales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Norwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Live trojan attacks on deep neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07860</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple-gradient descent algorithm (mgda) for multiobjective optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Désidéri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathematique</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2012">03 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A discussion of &quot;adversarial examples are not bugs, they are features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019-08">Aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Evaluating and understanding the robustness of adversarial logit pairing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Regularized multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-08">08 2004</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximability and hardness in multi-objective optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glaßer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reitwießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Witek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programs, Proofs, Processes</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image depth estimation trained via depth from defocus cues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv 1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<title level="m">One model to learn them all</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<title level="m">Adversarial logit pairing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matrix completion from noisy entries</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Ubernet: Training a &apos;universal&apos; convolutional neural network for low-,mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02132</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
		<title level="m">Learning task grouping and overlap in multi-task learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ndirango</surname></persName>
		</author>
		<title level="m">Generalization in multitask deep neural classifiers: a statistical physics approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10704</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Ssd: Single shot multibox detector p</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-07">07 2018</date>
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Metric learning for adversarial robustness</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations against semantic image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<title level="m">Cross-stitch networks for multitask learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08846</idno>
		<title level="m">Improving adversarial robustness via promoting ensemble diversity</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07528</idno>
		<title level="m">The limitations of deep learning in adversarial settings</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09404</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06605</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5019" to="5031" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the influence of the number of objectives on the hardness of a multiobjective optimization problem</title>
		<author>
			<persName><forename type="first">O</forename><surname>Schutze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="444" to="455" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<title level="m">Multi-task learning as multi-objective optimization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Advspade: Realistic unrestricted attacks for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Firstorder adversarial vulnerability of neural networks and input dimension</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5809" to="5817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Which tasks should be learned together in multi-task learning?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07553</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Are labels required for improving adversarial robustness? CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Feature denoising for improving adversarial robustness. CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep defense: Training dnns with improved adversarial robustness</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32Nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>arXiv abs/1901.08573</idno>
		<title level="m">Theoretically principled trade-off between robustness and accuracy</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
