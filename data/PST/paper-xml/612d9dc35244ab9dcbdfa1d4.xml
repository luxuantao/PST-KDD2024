<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-27">27 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pierre</forename><forename type="middle">L</forename><surname>Dognin</surname></persName>
							<email>pdognin@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Melnyk</surname></persName>
							<email>igor.melnyk@ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Payel</forename><surname>Das</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-27">27 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.12472v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TEKGEN datasets.</p><p>Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph representation of knowledge is a powerful tool to capture real-world information where complex relationships between node entities can be simply encoded. Automatic generation of Knowledge Bases (KBs) from free-form text and its counterpart of generating semantically relevant text from KBs are both active and challenging research topics.</p><p>Recently, there has been an increased interest in leveraging Pretrained Language Models (PLMs) to improve performance for text generation from graph, or graph-to-text (G2T) task <ref type="bibr" target="#b16">(Ribeiro et al., 2020)</ref>. Indeed, large PLMs like T5 <ref type="bibr" target="#b13">(Raffel et al., 2020)</ref> and BART <ref type="bibr" target="#b9">(Lewis et al., 2020)</ref> that have been pretrained on vast amount of diverse and variedly structured data, are particularly good candidates for generating natural looking text from graph data.</p><p>BART-and T5-related models have been employed by top performers in public challenges such as the WebNLG+ 2020 Challenge <ref type="bibr">(Castro Ferreira et al., 2020b)</ref> where both graph-to-text and textto-graph (T2G) tasks are offered, under the names RDF-to-Text and Text-to-RDF (semantic parsing) respectively (RDF stands for Resource Description Framework, a standard for describing web resources). One can notice that more teams entered the competition for the G2T task than for T2G as the latter is a much harder task. Best models generally use PLMs and fine-tune them for the target modality at hand (either graph or text). This is possible by re-framing the T2G and G2T generations as a sequence to sequence (Seq2Seq) generation problem, which suits fine-tuning PLMs well. One can therefore hope to leverage the large pretraining of PLMs to improve the overall quality of generation.</p><p>The Seq2Seq formulation requires a linearization of any input graph, which is not unique. This creates an opportunity for data augmentation where multiple linearizations are provided to the model at training time so the model learns the content represented by the graph, not the order of its sequential representation.</p><p>In this work, we are interested in leveraging the power of PLMs for both G2T and T2G generation tasks, and will demonstrate the strength of our approach by improving upon the best results of the WebNLG+ 2020 Challenge (rev 3.0) as reported by <ref type="bibr" target="#b2">Castro Ferreira et al. (2020a)</ref> for both T2G (Semantic Parsing) and G2T (Data-to-Text) tasks. We will also present results for the TEKGEN Corpus <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> to show performance on a different, much larger dataset. To illustrate the task of generation, Fig. <ref type="figure">1</ref> provides examples of G2T and T2G outputs obtained using the proposed generation framework. The first two sentences of the abstract of this paper were used as input for T2G using our best model. The model generates a graph Figure <ref type="figure">1</ref>: Actual examples of generation for Text-to-Graph and Graph-to-Text tasks using our best RL models. The first two sentences of the abstract were processed through our best models. First, a graph was created capturing the facts from the input sentences. Then, this graph was used as input to generate text. Despite a strong domain mismatch between input data and models, the generated paragraph is capturing most of the original sentences content. Both models were trained using RL, specifically Self-Critical Sequence Training (SCST).</p><p>from the input text by simultaneously extracting relevant nodes and linking them coherently. For the G2T task, another model starts from the generated graph and generates semantically relevant text from it. As one can appreciate, the final text is quite readable and captures most facts from the original abstract sentences despite a strong domain mismatch between input data and training data, which both models were built on.</p><p>Since both T2G and G2T generative tasks can be formulated as a Seq2Seq problem, we propose to use Reinforcement Learning (RL) as part of the PLMs fine-tuning on the target domain data. For both G2T and T2G tasks, a differentiable function such as the cross-entropy (CE) loss function is often used, since minimizing it results in maximizing the probability of generating the correct token/word. However, when it comes to evaluating a model's performance, benchmarks often use BLEU <ref type="bibr" target="#b11">(Pa Pa Aung et al., 2020)</ref>, METEOR <ref type="bibr" target="#b8">(Lavie and Agarwal, 2007)</ref>, chrF++ <ref type="bibr" target="#b12">(Popović, 2017)</ref> for G2T, or simply F1, Precision, and Recall scores for T2G, none of which being differentiable. During training, one hopes that by minimizing the CE loss, the model will tend towards better prediction of the target tokens, hence improving on evaluation metrics as a beneficial by-product. Thankfully, RL provides a framework where we can update our model parameters so to improve evaluation metrics directly. Mixed Incremental Cross-Entropy Reinforce (MIXER) from Ranzato et al. ( <ref type="formula">2016</ref>) introduced using REINFORCE (Williams, 1992) for sequence training. We propose to use one of its variant known as Self-Critical Sequence Training (SCST) <ref type="bibr" target="#b15">(Rennie et al., 2017)</ref> for both T2G and G2T training.</p><p>To summarize, our main contributions are: • We propose to use RL-based sequence training, specifically SCST, for both G2T and T2G tasks. This is the first time that RL based training is proposed to the bi-directional generation of text and graph. To the best of our knowledge, the present work is the first time it is introduced for a T2G task.</p><p>• We demonstrate that our approach provides better performance than the best systems reported for the WebNLG 2020+ Challenge.</p><p>• We provide a thorough investigation of SCSTbased training for both T2G and G2T tasks, including best rewards combination.</p><p>• We constructed subject and relation-object boundaries from TEKGEN sentence-triples pairs and showed performance of our approach for both T2G and G2T tasks.</p><p>• We adapted the large-scale TEKGEN corpus <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> for T2G and G2T tasks and confirmed the benefit of SCST-based fine-tuning approach over CE-trained baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In the WebNLG+ 2020 Challenge, most top performing models relied on fine-tuning of PLMs. Interestingly, all four top teams in the Challenge proposed quite different approaches while leveraging PLMs. 1 st place Amazon AI <ref type="bibr" target="#b6">(Guo et al., 2020a)</ref> pipelined a relational graph convolutional network (R-GCN) and a T5 PLM with some canonicalization rules. 2 nd place OSU Neural NLG <ref type="bibr" target="#b10">(Li et al., 2020)</ref>, the closest to our approach in spirit, used T5 and mBART PLMs to fine-tune after special data preprocessing. 3 rd place FBConvAI <ref type="bibr" target="#b20">(Yang et al., 2020)</ref> used BART PLM and multiple strategies to model input RDFs. 4 th place bt5 employed a T5 PLM trained in a bi-lingual approach on English and Russian, even using WMT English/Russian parallel corpus.</p><p>Recently, <ref type="bibr" target="#b5">Dognin et al. (2020)</ref>; <ref type="bibr" target="#b7">Guo et al. (2020b)</ref> proposed models trained to generate in both T2G and G2T directions, with consistency cycles created to enable the use of unsupervised datasets. In contrast, our approach of fine-tuning a T5 PLM is fully supervised but can produce either the specialized models for T2G and G2T tasks alone, or a hybrid model that can handle both T/G inputs simultaneously to generate the corresponding translated outputs G/T.</p><p>Note that in contrast to many WebNLG+ 2020 Challenge participants, e.g. <ref type="bibr" target="#b10">(Li et al., 2020)</ref>, no preprocessing of the data is performed for text, while for graph triples, we add tokens to mark subject, predicate, and object positions in their linearized sequence representation. Moreover, data augmentation is performed by allowing random shuffling of triples order in graph linearization to avoid a model to learn the exact order of triples, especially for the T2G task.</p><p>While the use of RL training in PLM has been explored in many works, the approach of <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> is closest to ours. However, their work focuses on the improved text generation in the context of natural question generation, while in our algorithm we use it for graph-to-text and text-tograph generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>Models are trained on a dataset D composed of a set of (x T , x G ) i samples, where x T is made of text (one or more sentences), and x G is a graph represented as a list of triples x G = [(s 1 , p 1 , o 1 ), . . . , (s K , p K , o K )], where the k-th triple is composed of a subject s k , predicate (relationship) p k , and object o k . The superscript i denotes the i-th sample in D. For G2T, the model is given x G as input and must generate xT . A crossentropy loss is computed as an expectation:</p><formula xml:id="formula_0">L T CE = E xT∼D log p G2T θ (x T ) ,<label>(1)</label></formula><p>where p G2T θ (x T ) is the distribution of the generated sequence xT = T G2T (x G ), T G2T (x G ) being the transformation from graph to text, and where our model is parameterized by θ. xT = [ ŵ1 , ŵ2 , . . . , ŵT ] is a sequence of generated tokens/words. Similarly, for training a T2G model, the cross-entropy loss used in training is simply</p><formula xml:id="formula_1">L G CE = E xG∼D log p T2G θ (x G ) ,<label>(2)</label></formula><p>where p T2G θ (x G ) is the distribution of the generated graph xG = T T2G (x T ), T T2G (x T ) being the transformation from text to graph.</p><p>In both Eq. ( <ref type="formula" target="#formula_0">1</ref>) and Eq. ( <ref type="formula" target="#formula_1">2</ref>), x G must be expressed as a sequence of tokens t j such that a list of triples x G turns into a list of tokens</p><formula xml:id="formula_2">[t 1 , t 2 , • • • , t M ]</formula><p>. This is simply done by adding tokens marking the subject, predicate, and object locations in the sequence such that each triple</p><formula xml:id="formula_3">(s k , p k , o k ) is turned into a sequence such as [&lt;S&gt;, w s 1 , &lt;P&gt;, w p 1 , w p 2 , &lt;O&gt;, w o 1 , w o 2 , w o 3 ]</formula><p>, assuming our subject is made of 1 token, our predicate of 2 tokens, and our object of 3 tokens in this example. &lt;S&gt;,&lt;P&gt;, and &lt;O&gt; are just special marker tokens to help the model know where subject, predicate and objects are located in the sequence.</p><p>We start from a pretrained encoder-decoder M model that we fine-tune on either T2G to get M T , or G2T task to get M G . We also propose a third kind of model M T+G to be fine-tuned on both T2G and G2T samples, i.e. the model will learn to generate in any direction, by supplying an input sample x = [x T ; x G ] and corresponding target for it. Input from each modality is prefixed by a task specific string to distinguish samples ("Text to Graph:" for x T and "Graph to Text:" for x G ). For M T+G models, the cross-entropy loss is similarly defined as for Eq. ( <ref type="formula" target="#formula_0">1</ref>) and Eq. ( <ref type="formula" target="#formula_1">2</ref>) such that</p><formula xml:id="formula_4">L T+G CE = E x∼D [ log p θ (x)].</formula><p>All models are shown in Fig. <ref type="figure">2</ref>. By convention, we refer to models in this paper by their input modality T, G, or T+G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reinforcement Learning</head><p>A sequence generation task can be re-framed as a model picking the best word within a vocabulary to react to its environment and accounting for past predictions, we can then reformulate Seq2Seq generation into the Reinforcement Learning framework. A model is an agent that defines a policy resulting in the action of selecting each word during generation, as first introduced by Ranzato et al. ( <ref type="formula">2016</ref>). REINFORCE, presented by Williams (1992), allows the optimization of a model's parameters θ by maximizing the expected value of the reward R(x T ) of generated sequence xT = [ ŵ1 , . . . , ŵT ]. To match usual Deep Learning conventions, we can minimize a loss expressed as its negative value:</p><formula xml:id="formula_5">L RL = ŵ1 ,..., ŵT p θ ( ŵ1 , . . . , ŵT )r( ŵ1 , . . . , ŵT ) = E [ ŵ1 ,..., ŵT ]∼p θ R( ŵ1 , . . . , ŵT ), = E xT∼pθ R(x T ).</formula><p>(3) R(x T ) is the reward for the generated text which is often associated with non-differentiable metrics</p><formula xml:id="formula_6">Encoder x t Decoder x tg L ce (x tg , x g ) L rl (x tg , x g ) M t Encoder x g Decoder x gt L ce (x gt , x t ) L rl (x gt , x t ) M g Specialized models M t , M g x t x g Encoder Decoder x tg x gt L ce x tg x gt , x g x t L rl x tg x gt , x g x t</formula><p>Hybrid model M t+g Notations:</p><p>x .t : text x .g : graph</p><p>Figure <ref type="figure">2</ref>: Specialized and hybrid models rely on the same losses for fine-tuning. However, specialized models are dedicated to a particular generation task while hybrid models can handle both generation directions.</p><p>such as BLEU, METEOR, chrF, etc. We circumvent this issue by using the REINFORCE policy gradient method:</p><formula xml:id="formula_7">∇ θ L RL ∝ (R(x T ) b) ∇ θ log p θ (x T ), (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where b is a baseline used to reduce the variance of our gradient estimate. b can be any function, even a random variable, as long as it is independent of the actions taken to generate xT , as described in Chapter 13.4 from <ref type="bibr" target="#b17">Sutton and Barto (2018)</ref>. In Self-Critical Sequence Training (SCST) <ref type="bibr" target="#b15">(Rennie et al., 2017)</ref>, b is chosen to be the reward of x * T , the output generated by the model by greedy max generation, hence the model serving as its own critic:</p><formula xml:id="formula_9">∇ θ L SCST ∝ (R(x T ) R(x * T )) ∇ θ log p θ (x T ),<label>(5</label></formula><p>) where xT is sampled from our model and x * T is generated by greedy max. An interesting property of the baseline is that if R(x T ) &gt; R(x * T ), sampled xT has higher reward than x * T , then the model is updated to reinforce the choices made by this generation. In the opposite case where R(x T ) &lt; R(x * T ), the model update will take the negative gradient or the opposite model update to subdue such generation. When R(x T ) = R(x * T ), no update is performed on the model since the gradient is effectively zeroed out, regardless of the individual values R(x T ) and R(x * T ). This happens when xT and x * T are identical (greedy-max and sampled sequences are identical). In that case the sample is lost for RL as no update to the model will result from this sample. Basically, REINFORCE is a Monte Carlo method of learning where a gradient update is applied in the direction decided by how R(x T ) compares to baseline b, the role of b being to reduce the variance of the gradient estimate. Variations around REINFORCE exist on how to apply the gradients, such as MIXER <ref type="bibr" target="#b14">(Ranzato et al., 2016)</ref>, or on how to evaluate the baseline (Luo, 2020) to minimize the gradient variance.</p><p>In our training, PLMs are first fine-tuned using L CE loss. Once they reach a good generation quality, the training is switched to RL fine-tuning by minimizing L SCST .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this Section, we present the experimental setup used for all the results reported in this paper. Models We used T5 PLMs (from Wolf et al. ( <ref type="formula">2020</ref>)) for our experiments for two distinct models, t5-large [770M parameters] and t5-base [220M parameters], with a special focus on t5-large as it is the best performing of the two on various NLP tasks. Models were fine-tuned to be either specialized on T2G (M T ) or G2T (M G ) task, or to accommodate both directions of generation (M T+G ). Data processing Graphs are often represented as list of triples. However our model expects a sequence of input words/tokens to work on. The linearization of graph triples is obviously ambiguous as there are many ways to traverse a graph <ref type="bibr">(Breadth First Search, Depth First Search, random walk, etc.)</ref>. In practice, we linearize the triples in the order of the list provided by the dataset, but use this inherent linearization ambiguity as an opportunity to do data-augmentation. Indeed, models are first fine-tuned using cross-entropy loss that strongly penalizes generation if it is in any different order than the ground truth order. To avoid the model to overfit to our data and memorize observed triples order, we augment the data by including a few permutations of the graph triples.</p><p>During graph linearization, we encode the subject, predicate, and object positions by using &lt;S&gt;,&lt;P&gt;,&lt;O&gt; tokens. In practice, we expand the model vocabulary with these special indivisible tokens that are not split during tokenization. No other preprocessing is done on the data for training. We explored masked and span-masked LM fine-tuning to match T5 pretraining <ref type="bibr" target="#b13">(Raffel et al., 2020)</ref> which WebNLG G2T BLEU↑ BLEU↑ NLTK METEOR↑ chrF++↑ Team/model Amazon AI (Shanghai) <ref type="bibr" target="#b6">(Guo et al., 2020a)</ref> 0.540 0.535 0.417 0.690 OSU Neural NLG <ref type="bibr" target="#b10">(Li et al., 2020)</ref> 0.535 0.532 0.414 0.688 FBConvAI <ref type="bibr" target="#b20">(Yang et al., 2020)</ref> 0.527 0.523 0.413 0.686 bt5 <ref type="bibr" target="#b1">(Agarwal et al., 2020)</ref> 0 did not lead to any noticeable improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>WebNLG+ 2020 We report results on WebNLG+ 2020 (v3.0) used in the WebNLG 2020 Challenge <ref type="bibr">(Castro Ferreira et al., 2020b)</ref>. The Challenge comprises of two tasks: RDF-to-text generation (G2T), and Text-to-RDF semantic parsing (T2G which loosely correspond to different levels of relaxation of how close a match of an entity must be to the ground truth in content and position in a triple. Note that when generating graphs/RDFs, scoring metrics explore all possible permutations of a graph edges. For TEKGEN, we use the same metrics as for WebNLG+ 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>For all experiments, PLMs were first exposed to the target datasets (WebNLG+, TEKGEN) by finetuning using L CE loss.  <ref type="bibr" target="#b14">(Ranzato et al., 2016;</ref><ref type="bibr" target="#b15">Rennie et al., 2017)</ref>. Therefore, we followed the subsequent simple approach: During fine-tuning, the evaluations are conducted on the validation set. From the CE phase, the best performing model iteration is selected based on the METEOR and F1 score for the G2T and T2G tasks, respectively, to pursue RL fine-tuning. In case of G2T, potential ties in METEOR scores among candidate models, are resolved by using BLEU_NLTK, followed by the chrF++ metric. Note that early stopping selection of CE models led to good performance for t5-base models as well. During the SCST phase, the best model iteration on the validation set is selected and its performance numbers on the test set are reported in our tables.</p><p>WebNLG+ 2020 G2T For the WebNLG+ 2020 Challenge, the results of the top four systems for RDF-to-text task can be found in Tab. 1 for all categories (results for seen and unseen categories are given in Tab. 5 in the Appendix), while descriptions the top teams' systems were given in Section 2. We report our G2T results for both t5-large and t5base models as well. For t5-large, ReGen G2T Even in averaging results of various seeded models, we see a sustained gain from SCST for CE models in all metrics. Multiple reward candidates were investigated (BLEU, BLEU_NLTK, METEOR, chrF) as well as some linear combinations of pairs of them, as can be seen in Tab. 7 in Appendix. In Tab. 7, for t5-large, METEOR is consistently the best SCST reward, and improves all the other metrics scores as well. However, for 'smaller' models such as t5-base, BLEU_NLTK is revealed to be the best reward for improving BLEU performance as expected. Again, SCST brings significant gains across all the metrics in that case. Note that for t5-base model, selecting a METEOR reward improves METEOR results significantly as reported in Tab. 9 in Appendix.</p><p>Another interesting fact is that early stopping of CE model G2T.CE.ES (at 5 epochs) leads to the best SCST model G2T.RL.ES for t5-base, while selecting the best CE model G2T.CE.best (at 11 epochs) still showed some gains from SCST model G2T.RL.best. SCST needs a good starting point, but a better CE model that has seen a lot more epochs of our dataset maybe harder for SCST to stir in a better solution in the parameter space. Moreover, the test split contains unseen categories not present in the validation dataset which render choices based on validation sub-optimal for the test dataset. The best models we report in this work are specialized models M G . Early in our investigation, hybrid models were the best performing model for G2T reaching 0.547 BLEU, 0.543 BLEU_NLTK and 0.417 METEOR, and first to beat the Challenge winning team. However, when batch size became larger (20-24 samples), the specialized models took the lead and retain it still.</p><p>For training, we optimized all our models using AdamW (Loshchilov and Hutter, 2017), variant of the Adam optimizer with default values of β = [0.9, 0.999] and weight decay of 10 −2 . For learning rate, we used 5.10 −6 for all our experiments as it was better than 10 −5 and 10 −6 as seen in Tab. 8 in Appendix. All our models were trained with 20-24 minibatch size on WebNLG. Further details on our experimental setup are provided in the Appendix in Section A. WebNLG+ 2020 T2G Results for the Text-to-RDF task are reported in Tab. 2 for all categories. Results for our best model on seen and unseen categories are given in Tab. 6 in Appendix. AmazonAI and bt5 are the top performing teams. Again, the proposed ReGen T2G.CE model shows strong results that are better in term of all metrics, for all matching categories. In themselves, these numbers are a defacto new state-of-the-art for this dataset, as far as we know. SCST model T2G.RL fails to improve on this model though. The exact F1 metric was used as reward, but the model could never pull ahead of the CE model in our experiments. The exact F1 metric may not be a strong enough reward to really capture the dynamics of graph generation properly for WebNLG+ as it is very rigid in its measure (one must have an exact match), although the same reward gave good results on our second dataset TEKGEN. A more sensitive metric could possibly help. We even tried to use n-gram based metrics (like BLEU) but to no avail. We further address this issue at the end on this Section. TEKGEN G2T For the TEKGEN dataset, we present our results on Graph-to-Text generation in Tab. 3. Similar to the experiments in WebNLG+, we pick the best model during the CE fine-tuning based on the METEOR score and proceed with the RL fine-tuning. We observe that the RL fine-tuning step helps boost the test split scores on all metrics. It is worth noting that the scores are slightly underestimating the potential of our system because of the nature of the sentences in the TEKGEN dataset. Unlike WebNLG+, in a paired text-graph sample in TEKGEN, the linearized graph does not usually cover all the concepts described in the correspond-ing text. This leads to underestimating when the hypothesis is scored against the reference using n-gram metrics.</p><p>TEKGEN T2G Results for the Text-to-Graph for TEKGEN are reported in Tab. 4. Once the CE finetuning is done, we continue with the RL fine-tuning using exact F1 as reward. The performance is consistent with what we observe in G2T task for TEK-GEN, where SCST step boosts the performance of the model. Since, we reformulate this dataset (refer Section 4.1) to offer as T2G and G2T tasks, our approach is the first attempt in understanding the nature of TEKGEN dataset and our methods provide a baseline for future research. Please note that for both T2G and G2T tasks in TEKGEN, we only start a t5-large PLM.  Summary Results on WebNLG+ 2020 and TEK-GEN demonstrated that RL fine-tuning of models leads to significant improvements of results for T2G and G2T, establishing new state-of-the-art results for both tasks. For WebNLG+, T2G was a challenging task for RL fine-tuning. In further work, we plan to address this issue by investigating two points: First, look into a more sensible graphdependent sampling for graph structures, rather than the current multinomial sampling of the best tokens at each generation step. Second, try a different reward schemes where the reward is more attuned to the challenges of graph generation as well as graph structure, allowing for some curriculum learning, or increasing the harshness of rewards gradually during training. Results on TEKGEN showed that RL fine-tuning is a viable option even on large-scale datasets. To enrich this quantitative study of ReGen, we provide a few qualitative cherry picked results in Tab. 11 and Tab. 12 in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed to use RL for improving upon current generation for text-to-graph and graph-to-text tasks for the WebNLG+ 2020 Challenge dataset using pre-trained LMs. We not only defined a novel Seq2Seq training of models in T2G and G2T generation tasks, but we established stateof-the-art results for WebNLG+ for both tasks, significantly improving on the previously published results. We provided extensive analyses of our results and of the steps taken to reach these improvements. We then expanded our approach to large scale training by means of TEKGEN where we demonstrated that RL fine-tuning provides a robust way to improve upon regular model finetuning within a dataset that is orders of magnitude larger than the WebNLG+ starting point. We established gains despite a weaker content overlap in text-graph data pairs for TEKGEN. Along the way, we constructed subject, and relation-object boundaries from TEKGEN sentence-triples pairs that we plan on releasing to benefit the research community. Future work will focus on developing a variant of SCST that leverages the unique structure of graph by either performing of more sensible graphdependent sampling, or by investigating different reward schemes more attuned to integrating the content and structure of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Setup</head><p>All our experiments were run using NVIDIA V100 GPUs for training and validation, some trainings were done on A100. We distributed our training to 2-4 GPUs depending on availability. Each training epoch for CE ranged from 30 minutes to 1 hour depending on number of GPUs utilized.</p><p>Validation and testing (1,779 and 2,155 samples for testA and testB of WebNLG+ 2020) lasted from 40 minutes to 1 hour depending on machines. Computation was dominated by beam search generation as we used beam search with beam size of 5 and a max sequence length of 192 (since linearized graph sequence can be quite long). We used the official scoring scripts released by WebNLG+ 2020 Challenge to score all our experiments. The evaluation of graph being the most computationally expensive as all possible matching combinations are tested in what looks like a factorial complexity, taking scoring of set of triples larger than 8 from impractical to not feasible.</p><p>All our models were built using PyTorch. Total effective batch sizes were set to either 20 or 24 samples for our distributed training. We adjusted the batch size on each worker to ensure consistent global batch size of 20 or 24.</p><p>We did some search on learning rates for t5large training and SCST rewards, see discussion and results in Section C.</p><p>All our trainings have a seeded random number generator for reproducibility. We also report results on WebNLG+ 2020 G2T tasks for each training setup by showing results for 3 models from different seeds, and provide means and standard deviations of these results in Tab. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B WebNLG+ 2020 Results per</head><p>Categories for Best G2T and T2G Models</p><p>In Tab. 5, we are reporting results for all WebNLG+ 2020 categories for our best CE and RL models. While results for unseen categories are much worse than for seen categories, RL fine-tuning manages to improve on both seen and unseen categories.</p><p>Tab. 6 provides results for seen, unseen and all categories for our best CE model ReGen T2G.CE which established state-of-the-art results on T2G task of WebNLG+ 2020 Challenge dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Studies</head><p>In Tables <ref type="table">7 and 8</ref> we present ablation studies of different optimized metrics and learning rates for SCST training. As can be seen from Table <ref type="table">7</ref>, when METEOR is used as a reward, we get the best performance across all the metrics. We also tried using a combination of multiple rewards with different scaling but did not get any gain over the single metric rewards. In Table <ref type="table">8</ref>. we also show the effect of learning rate on SCST performance. Using lr = 5 • 10 −6 gave us the best performance, while higher rates, such as 10 −4 , led to unstable training and collapse of SCST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D G2T Results t5-base models for SCST with METEOR Reward</head><p>Results for SCST fine-tuning of t5-base models using a METEOR reward are compiled in Tab. 9. Clearly, these models achieve better METEOR results as expected since they are RL optimzed on this metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E G2T Results for Models from Multiple Random Seeds</head><p>All our training have a seeded random number generator for reproducibility. We also report the mean and standard deviations for all our G2T models. Each model setup was run 3 times using three independent and distinct seeds, following the same exact process. This is to ensure that our results are not just the product of a lucky system configuration or otherwise advantageous random shuffling of our training dataset. The gain reported between CE and RL for our t5-large models are clearly still showing after average of all 3 models from distinct random seeds. For t5-base, gains between CE and RL are still present, albeit smaller than for our best systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Processed TEKGEN Dataset</head><p>In Fig. <ref type="figure">3</ref> we show an example of our processing of TEKGEN dataset in establishing subject, relation, object boundaries. This enables both training and evaluating systems for T2G and G2T tasks.</p><p>Table <ref type="table" target="#tab_0">12</ref>: Few cherry-picked generation for G2T task for WebNLG+ 2020 (top three) and TEKGEN (bottom three).</p><p>For each source (Graph), we show the ground truth <ref type="bibr">(Gold)</ref> and system generated hypothesis from the best CE (Hyp-CE) and SCST models (Hyp-SCST). Note that the set of triples in WebNLG+ 2020 takes the form x G = [(s 1 ♦p 1 ♦o 1 ), . . . , (s K ♦p K ♦o K )], whereas the same for TEKGEN is of form x G = [s♦(p 1 ♦o 1 ), . . . , (p K ♦o K )]</p></div><figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="13,70.87,514.41,453.55,194.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>G2T Best results on WebNLG 2020 Challenge (v3.0) dataset. The first four rows were the top performers of the Challenge. Results for CE and RL models are presented for our ReGen systems so to show gains from using SCST. Our G2T.RL is the best system overall, fine-tuning a t5-large model using METEOR reward. G2T.RL.ES and G2T.RL.best show the impact of using early stopping (ES) or best CE selection for starting SCST fine-tuning on a t5-base smaller model while using BLEU_NLTK reward.</figDesc><table><row><cell>.517</cell><cell>0.517</cell><cell>0.411</cell><cell>0.679</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>G2T Results for TEKGEN dataset. ReGen-CE establishes a baseline on this dataset. ReGen-SCST consistently improve on the baseline on all metrics, for validation and test sets.</figDesc><table><row><cell>.CE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>T2G TEKGEN Results: ReGen-CE establishes a baseline of the dataset. ReGen-SCST improves results on the test set compared to ReGen-CE.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training</title>
		<author>
			<persName><forename type="first">Heming</forename><surname>References Oshin Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><surname>Al-Rfou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Machine translation aided bilingual data-to-text generation and semantic parsing</title>
		<author>
			<persName><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="125" to="130" />
		</imprint>
	</monogr>
	<note>Siamak Shakeri, and Rami Al-Rfou</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Ilinykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020a. 2020</date>
			<biblScope unit="page" from="55" to="76" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Ilinykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<title level="m">2020b. Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<editor>
			<persName><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>Virtual</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforcement learning based graph-to-sequence model for natural question generation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Du-alTKB: A Dual Learning Bridge between Text and Knowledge Base</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.694</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8605" to="8616" />
		</imprint>
	</monogr>
	<note>Cicero Nogueira dos Santos, and Payel Das</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">√ 2 : A plan-and-pretrain approach for knowledge graph-to-text generation</title>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="100" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CycleGT: Unsupervised graph-to-text and text-to-graph generation via cycle training</title>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
				<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Czech Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics. Ilya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandre</forename><surname>Maskharashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jory</forename><surname>Symon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stevens-Guille</surname></persName>
		</author>
		<author>
			<persName><surname>White</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
	<note>Leveraging large pretrained models for WebNLG. Ruotian Luo. 2020. A better variant of self-critical sequence training</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic Myanmar image captioning using CNN and LSTM-based language model</title>
		<author>
			<persName><forename type="first">San</forename><surname>Pa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pa</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Win</forename><forename type="middle">Pa</forename><surname>Pa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tin</forename><surname>Lay Nwe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)</title>
				<meeting>the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="139" to="143" />
		</imprint>
	</monogr>
	<note>European Language Resources association</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">chrF++: words helping character n-grams</title>
		<author>
			<persName><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4770</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
				<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Steven J Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Investigating pretrained language models for graph-to-text generation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Schmitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Hinrich Schütze, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>A Bradford Book</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Type Sentence / Graph Source The Pontiac Rageous began and ended its production in 1997 on an assembly line in Detroit, a city in Michigan</title>
		<author>
			<persName><forename type="first">Zixiaofan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Einolghozati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Diedrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gold Pontiac_Rageous ♦ productionStartYear ♦ 1997 ♦ Pontiac_Rageous ♦ assembly ♦ Michigan ♦ Pontiac_Rageous ♦ assembly ♦ Detroit ♦ Pontiac_Rageous ♦ productionEndYear ♦ 1997 ♦ Detroit ♦ type ♦ City</title>
				<meeting><address><addrLine>Dublin, Ireland (Virtual</addrLine></address></meeting>
		<imprint>
			<publisher>Michigan</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
	<note>Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m">Hyp-CE Pontiac_Rageous ♦ assembly ♦ Detroit ♦ Pontiac_Rageous ♦ modelYears ♦ 1997 ♦ Pontiac_Rageous ♦ modelYears ♦ 1997 ♦ Detroit ♦ isPartOf ♦ Michigan Hyp-SCST Pontiac_Rageous ♦ assembly ♦ Detroit ♦ Pontiac_Rageous ♦ modelYears ♦ 1997 ♦ Pontiac_Rageous ♦ assembly ♦ Michigan Source In the United States, where Abraham A, Ribicoff was born, African Americans are one of the ethnic groups. Abraham A. Ribicoff was married to Ruth Ribicoff</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Gold</forename><surname>Abraham_A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">♦</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Ribicoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Abraham_A</surname></persName>
		</author>
		<title level="m">_Ribicoff ♦ birthPlace ♦ United_States ♦ United_States ♦ ethnicGroup ♦ African_Americans ♦ Abraham_A._Ribicoff ♦ nationality ♦ United_States Hyp-CE Abraham_A._Ribicoff ♦ birthPlace ♦ United_States ♦ Abraham_A._Ribicoff ♦ spouse ♦ &quot;Ruth Ribicoff&quot; ♦ United_States ♦ ethnicGroup ♦ African_Americans Hyp-SCST Abraham_A._Ribicoff ♦ birthPlace ♦ United_States ♦ Abraham_A._Ribicoff ♦ spouse ♦ &quot;Ruth Ribicoff&quot; ♦ Abraham_A._Ribicoff ♦ nationality ♦ American ♦ United_States ♦ ethnicGroup ♦ African_Americans Source Super Capers</title>
				<editor>
			<persName><forename type="first">Stacy</forename><surname>Katzman</surname></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>is a 98 minute film starring Michael Rooker and Tom Sizemore</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m">Gold Super_Capers ♦ editing ♦ Stacy_Katzman ♦ Super_Capers ♦ starring ♦ Michael_Rooker ♦ Su-per_Capers ♦ starring ♦ Tom_Sizemore ♦ Super_Capers ♦ runtime | 98</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m">Hyp-CE Super_Capers ♦ starring ♦ Tom_Sizemore ♦ Super_Capers ♦ timeOut ♦ &quot;980.0&quot;(minutes) ♦ Su-per_Capers ♦ starring ♦ Michael_Rooker ♦ Super_Capers ♦ editor ♦ Stacy_Katzman Hyp-SCST Super_Capers ♦ starring ♦ Tom_Sizemore ♦ Super_Capers ♦ length ♦ 98.0 (minutes) ♦ Super_Capers ♦ starring ♦ Michael_Rooker ♦ Super_Capers ♦ editor ♦ Stacy_Katzman Source Doctor George Cary</title>
				<imprint>
			<biblScope unit="page" from="1611" to="1680" />
		</imprint>
	</monogr>
	<note>Professor of Sacred Theology, lord of the manor of Clovelly, Devon, was Dean of Exeter between 1663 and 1680. amongst other duties responsible for the maintenance and decoration of Exeter Cathedral</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">1611-1680) ♦ position held ♦ Dean of Exeter ♦ start time ♦ 01 January 1663 ♦ date of birth ♦ 00 1611 ♦ date of</title>
		<author>
			<persName><forename type="first">Gold</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cary</forename></persName>
		</author>
		<idno>death ♦ 00 1680</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">) ♦ date of birth ♦ 01 January 1611 ♦ date of death ♦ 01</title>
		<author>
			<persName><forename type="first">-Ce George</forename><surname>Hyp</surname></persName>
		</author>
		<author>
			<persName><surname>Cary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1680-01">January 1680</date>
		</imprint>
	</monogr>
	<note>priest</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">position held ♦ Dean of Exeter ♦ date of birth ♦ 01 January 1611 ♦ date of death ♦ 01</title>
		<author>
			<persName><forename type="first">-Scst George</forename><surname>Hyp</surname></persName>
		</author>
		<author>
			<persName><surname>Cary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1680-01">January 1680</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bahamian general election ♦ point in time ♦ 10 April 1968 ♦ country ♦ The Bahamas ♦ applies to jurisdiction ♦ The Bahamas Hyp-CE 1968 Bahamian general election ♦ point in time</title>
		<imprint>
			<date type="published" when="1968-04-10">10 April 1968. 1968. 10 April 1968</date>
		</imprint>
	</monogr>
	<note>Source Early general elections were held in the Bahamas on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">♦ country ♦ The Bahamas Source The school was established on 6 January 1930, by former education minister, CWW Kannangara, who additionally founded two other colleges located in central Ceylon</title>
		<author>
			<persName><forename type="first">-Scst</forename><surname>Hyp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968-04-10">1968. 10 April 1968</date>
		</imprint>
	</monogr>
	<note>Bahamian general election ♦ point in time ♦</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">Gold Kattankudy Central College ♦ instance of ♦ School Hyp-CE Government Polytechnic , Colombo ♦ inception ♦ 00 1930</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Note that the set of triples in WebNLG+ takes the form x G = [(s 1 ♦p 1 ♦o 1 ), . . . , (s K ♦p K ♦o K )], whereas the same for TEKGEN is of form x G =</title>
		<author>
			<persName><forename type="first">-Scst Government</forename><surname>Hyp</surname></persName>
		</author>
		<author>
			<persName><surname>Polytechnic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Colombo ♦ inception ♦ 00 1930 ♦ instance of ♦ School Table 11: Few cherry-picked generation for T2G task for WebNLG+ 2020 (top three) and TEKGEN (bottom three)</title>
				<imprint/>
	</monogr>
	<note>For each source (Text), we show the ground truth (Gold) and system generated hypothesis from the best CE (Hyp-CE) and SCST models (Hyp-SCST). s♦(p 1 ♦o 1 ), . . . , (p K ♦o K )] Type Graph / Sentence</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harry Carey later grew up to write and star in the movie McVeagh of the South Seas. Harry Carey, born in 1878, wrote and appeared in the movie McVeagh of the South Seas</title>
	</analytic>
	<monogr>
		<title level="m">Source McVeagh_of_the_South_Seas ♦ starring ♦ Harry_Carey_(actor_born_1878) ♦ McVeagh_of_the_South_Seas ♦ writer ♦ Harry_Carey_(actor_born_1878) Gold Born in 1878</title>
				<imprint>
			<publisher>Harry Carey</publisher>
		</imprint>
	</monogr>
	<note>who was born in 1878, wrote and starred the film of McVeagh of the South Seas</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m">Hyp-CE McVeagh of the South Seas was written by Harry Carey, who was born in 1878</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Source Aleksandr_Prudnikov ♦ height ♦ 185.0 (centimetres) ♦ Aleksandr_Prudnikov ♦ youthclub ♦ FC_Spartak_Moscow ♦ FC_Spartak_Moscow ♦ ground ♦ Otkrytiye_Arena Gold Aleksandr Prudnikov, 185cm tall played for FC Spartak Moscow&apos;s youth team. FC Spartak Moscow is based in the Otkrytiye Arena. Aleksandr Prudnikov who is 185 cm tall is a member of the youth side of FC Spartak Moscow. The home ground of FC Spartak Moscow is Otkrytiye Arena</title>
		<author>
			<persName><forename type="first">-Scst</forename><surname>Hyp</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>McVeagh of the South Seas was written by Harry Carey and starred the actor Harry Carey who was born in 1878. Aleksandr Prudnikov is 185.0 cm tall and played for the FC Spartak Moscow at the Otkrytiye Arena</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Aleksandr Prudnikov is 185 cm tall and played for FC Spartak Moscow&apos;s youth team at the Otkrytiye Arena. Hyp-SCST Aleksandr Prudnikov is 185 cm tall and played for the youth team of FC Spartak Moscow whose home ground is the Otkrytiye Arena</title>
		<author>
			<persName><forename type="first">-Ce</forename><surname>Hyp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Source Baku_Turkish_Martyrs&apos;_Memorial ♦ nativeName ♦ &quot;Türk Şehitleri Anıtı&quot; ♦ Baku_Turkish_Martyrs&apos;_Memorial ♦ location ♦ Azerbaijan Gold The Native name of the Baku Turkish Martyrs&apos; Memorial is &quot;Türk Şehitleri Anıtı&quot; which is located in Azerbaijan. The native name of the Baku Turkish Martyrs&apos; Memorial is Türk Şehitleri Anıtı located in Azerbaijan. The native name for the Baku Turkish Martyrs&apos; Memorial is Türk Şehitleri Anıtı</title>
		<imprint>
			<pubPlace>Azerbaijan</pubPlace>
		</imprint>
	</monogr>
	<note>which is located in Baku</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The native name of the Baku Turkish Martyrs&apos; Memorial in Azerbaijan is Türk Şehitleri Anıtı. Hyp-SCST The Baku Turkish Martyrs&apos; Memorial is located in Azerbaijan and is known locally as Türk Şehitleri Anıtı</title>
		<author>
			<persName><forename type="first">-Ce</forename><surname>Hyp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Source John Banister (anatomist) ♦ occupation ♦ Surgeon ♦ date of birth ♦ 01 January 1533 ♦ date of death ♦ 01</title>
		<imprint>
			<date type="published" when="1610-01">January 1610</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><surname>Banister</surname></persName>
		</author>
		<title level="m">1533-1610) was an English anatomist, surgeon and teacher</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m">Hyp-CE John Banister (1533-1610) was an English surgeon</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m">Hyp-SCST John Banister (1533-1610) was an English surgeon and anatomist</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Source WNPT (TV) ♦ country ♦ United States ♦ instance of ♦ Television station Gold WNPT, virtual channel 8 (VHF digital channel 7), is a PBS member television station licensed to Nashville</title>
		<imprint>
			<pubPlace>Tennessee, United States</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">virtual channel 3 (UHF digital channel 15), is a Fox-affiliated television station licensed to Portland</title>
		<author>
			<persName><forename type="first">Hyp-Ce</forename><surname>Wnpt</surname></persName>
		</author>
		<imprint>
			<pubPlace>Oregon, United States</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">virtual channel 4 (UHF digital channel 16), is a Public Broadcasting Service (PBS) member television station licensed to Portland</title>
		<author>
			<persName><forename type="first">Hyp-Scst</forename><surname>Wnpt</surname></persName>
		</author>
		<imprint>
			<pubPlace>Oregon, United States</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Source Our Lady of the Presentation Cathedral, Natal ♦ inception</title>
		<imprint>
			<date type="published" when="1988-11-21">21 November 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Gold Our Lady of the Presentation Cathedral, Natal was inaugurated on November 21, 1988, and is located in the district of Cidade Alta in Natal, capital of the Brazilian state of</title>
		<imprint>
			<pubPlace>Rio Grande do Norte</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Our Lady of the Presentation Cathedral, Natal was built in 1988</title>
		<author>
			<persName><forename type="first">-Ce</forename><surname>Hyp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Our Lady of the Presentation Cathedral, Natal was consecrated on</title>
		<author>
			<persName><forename type="first">-Scst</forename><surname>Hyp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988-11-21">21 November 1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
